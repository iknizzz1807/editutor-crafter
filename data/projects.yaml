domains:
- id: app-dev
  name: Application Development
  icon: ðŸŒ
  subdomains:
  - name: Web Frontend
  - name: Web Backend
  - name: Full-stack
  - name: Mobile
  - name: Desktop & CLI
  projects:
    beginner:
    - id: rest-api-design
      name: Production REST API
      detailed: true
    intermediate:
    - id: blog-platform
      name: Blog Platform
      description: Full CRUD, auth, markdown
      detailed: true
    - id: chat-app
      name: Real-time Chat
      description: WebSocket, real-time updates
      detailed: true
    - id: ecommerce-basic
      name: E-commerce (Basic)
      description: Cart, checkout flow
      detailed: true
    - id: grpc-service
      name: gRPC Microservice
      detailed: true
    - id: circuit-breaker
      name: Circuit Breaker Pattern
      detailed: true
    - id: file-upload-service
      name: File Upload Service
      description: Chunked uploads, resumable, virus scanning
      detailed: true
    - id: websocket-server
      name: WebSocket Server
      description: Real-time bidirectional communication
      detailed: true
    - id: notification-service
      name: Notification Service
      description: Push notifications, email, SMS delivery
      detailed: true
    - id: graphql-server
      name: GraphQL Server
      description: Schema-first API with resolvers, dataloaders, subscriptions
      detailed: true
    - id: background-job-processor
      name: Background Job Processor
      description: Async task queue like Sidekiq/Celery with retries, scheduling
      detailed: true
    advanced:
    - id: social-network
      name: Social Network
      description: Feed, followers, notifications
      detailed: true
    - id: video-streaming
      name: Video Streaming
      description: HLS, adaptive bitrate
      detailed: true
    - id: api-gateway
      name: API Gateway
      detailed: true
    - id: distributed-tracing
      name: Distributed Tracing System
      detailed: true
    - id: collaborative-editor
      name: Collaborative Editor
      description: Real-time collaborative editing like Google Docs
      detailed: true
    - id: media-processing
      name: Media Processing Pipeline
      description: Image/video transcoding, thumbnails, CDN
      detailed: true
    - id: multi-tenant-saas
      name: Multi-tenant SaaS Backend
      description: Tenant isolation, row-level security, billing integration
      detailed: true
    expert:
    - id: build-react
      name: Build Your Own React
      description: Virtual DOM, reconciliation, hooks, fiber
      detailed: true
    - id: build-bundler
      name: Build Your Own Bundler
      description: Module resolution, tree shaking, code splitting
      detailed: true
    - id: build-spreadsheet
      name: Build Your Own Spreadsheet
      description: Excel-like app with formulas
      detailed: true
    - id: build-web-framework
      name: Build Your Own Web Framework
      description: Express/Django clone
      detailed: true
    - id: build-graphql-engine
      name: Build Your Own GraphQL Engine
      description: Query parsing, execution, schema stitching like Hasura
      detailed: true
- id: systems
  name: Systems & Low-Level
  icon: âš™ï¸
  subdomains:
  - name: Systems Programming
  - name: Networking
  - name: Operating Systems
  projects:
    beginner: []
    intermediate:
    - id: http-server-basic
      name: HTTP Server (Basic)
      description: Static file serving
      detailed: true
    - id: memory-pool
      name: Memory Pool Allocator
      description: Fixed-size block allocation
      detailed: true
    - id: process-spawner
      name: Process Spawner
      description: fork/exec, process lifecycle
      detailed: true
      bridge: true
    - id: signal-handler
      name: Signal Handler
      description: SIGINT, SIGTERM, signal masks
      detailed: true
      bridge: true
    advanced:
    - id: http2-server
      name: HTTP/2 Server
      description: Multiplexing, HPACK
      detailed: true
    - id: container-basic
      name: Container (Basic)
      description: Namespaces isolation
      detailed: true
    - id: mini-shell
      name: Mini Shell
      description: Job control, background processes
      detailed: true
      bridge: true
    - id: virtual-memory-sim
      name: Virtual Memory Simulator
      description: Page tables, TLB
      detailed: true
      bridge: true
    - id: filesystem
      name: Filesystem Implementation
      description: Simple filesystem with inodes, directories, journaling
      detailed: true
    - id: reverse-proxy
      name: Reverse Proxy
      description: Nginx-like proxy with load balancing, caching, SSL termination
      detailed: true
    expert:
    - id: build-docker
      name: Build Your Own Docker
      description: Container runtime with namespaces, cgroups
      detailed: true
      languages:
      - Go
      - Rust
      - C
    - id: build-shell
      name: Build Your Own Shell
      description: Full Unix shell with job control
      detailed: true
      languages:
      - C
      - Rust
      - Go
    - id: build-allocator
      name: Build Your Own Memory Allocator
      description: malloc/free implementation
      detailed: true
      languages:
      - C
      - Rust
      - Zig
    - id: build-os
      name: Build Your Own OS
      description: Operating system kernel
      detailed: true
      languages:
      - C
      - Rust
      - Zig
    - id: build-tcp-stack
      name: Build Your Own TCP/IP Stack
      description: Network stack implementation
      detailed: true
      languages:
      - C
      - Rust
      - Go
- id: data-storage
  name: Data & Storage
  icon: ðŸ—„ï¸
  subdomains:
  - name: Databases
  - name: Data Engineering
  projects:
    beginner: []
    intermediate:
    - id: btree-impl
      name: B-tree Implementation
      description: Insert, delete, rebalance
      detailed: true
    - id: sql-parser
      name: SQL Parser
      description: SELECT, WHERE, JOIN
      detailed: true
    - id: etl-pipeline
      name: ETL Pipeline
      description: Extract, transform, load data processing
      detailed: true
    - id: cdc-system
      name: Change Data Capture
      description: Database change streaming and replication
      detailed: true
    - id: data-quality-framework
      name: Data Quality Framework
      description: Schema validation, anomaly detection, data profiling
      detailed: true
    advanced:
    - id: query-optimizer
      name: Query Optimizer
      description: Cost estimation
      detailed: true
    - id: wal-impl
      name: WAL Implementation
      description: Write-ahead logging
      detailed: true
    - id: search-engine
      name: Search Engine
      description: Full-text search with inverted index, ranking
      detailed: true
    - id: event-sourcing
      name: Event Sourcing System
      description: Event store with projections and snapshots
      detailed: true
    - id: workflow-orchestrator
      name: Workflow Orchestrator
      description: DAG-based task scheduling like Airflow with dependencies
      detailed: true
    - id: vector-database
      name: Vector Database
      description: Similarity search with HNSW/IVF indexes for embeddings
      detailed: true
    expert:
    - id: build-redis
      name: Build Your Own Redis
      description: In-memory data store with RESP protocol
      detailed: true
    - id: build-sqlite
      name: Build Your Own SQLite
      description: Embedded SQL database
      detailed: true
    - id: build-kafka
      name: Build Your Own Kafka
      description: Distributed message queue
      detailed: true
    - id: time-series-db
      name: Time-Series Database
      description: Optimized for time-stamped data with compression
      detailed: true
    - id: stream-processing-engine
      name: Stream Processing Engine
      description: Real-time data processing like Flink with windowing, state
      detailed: true
- id: distributed
  name: Distributed & Cloud
  icon: â˜ï¸
  subdomains:
  - name: Distributed Systems
  - name: Cloud & DevOps
  - name: Consensus
  - name: Blockchain
  projects:
    beginner:
    - id: service-discovery
      name: Service Discovery
      description: Registry, health checks
      detailed: true
    - id: rpc-basic
      name: RPC Framework (Basic)
      description: Remote procedure calls
      detailed: true
      bridge: true
    intermediate:
    - id: load-balancer-basic
      name: Load Balancer (Basic)
      description: Round-robin
      detailed: true
    - id: rate-limiter
      name: Rate Limiter
      description: Token bucket
      detailed: true
    - id: leader-election
      name: Leader Election
      description: Bully algorithm, ring election
      detailed: true
      bridge: true
    - id: replicated-log
      name: Replicated Log
      description: Append-only log, basic replication
      detailed: true
      bridge: true
    - id: vector-clocks
      name: Vector Clocks
      description: Logical time, causality
      detailed: true
      bridge: true
    - id: feature-flags
      name: Feature Flag System
      description: Dynamic feature toggles with targeting rules
      detailed: true
    - id: job-scheduler
      name: Job Scheduler
      description: Distributed task scheduling with retries
      detailed: true
    advanced:
    - id: distributed-cache
      name: Distributed Cache
      description: Consistent hashing
      detailed: true
    - id: gossip-protocol
      name: Gossip Protocol
      description: Epidemic broadcast
      detailed: true
      bridge: true
    - id: 2pc-impl
      name: Two-Phase Commit
      description: Distributed transactions
      detailed: true
      bridge: true
    - id: service-mesh
      name: Service Mesh
      description: Sidecar proxy for service-to-service communication
      detailed: true
    - id: rate-limiter-distributed
      name: Distributed Rate Limiter
      description: Rate limiting across multiple nodes
      detailed: true
    - id: saga-orchestrator
      name: Saga Orchestrator
      description: Distributed transactions with compensating actions
      detailed: true
    - id: chaos-engineering
      name: Chaos Engineering Framework
      description: Fault injection and resilience testing
      detailed: true
    - id: container-runtime
      name: Container Runtime
      description: OCI-compliant container execution
      detailed: true
    - id: kubernetes-operator
      name: Kubernetes Operator
      description: Custom controller with CRDs for automated app management
      detailed: true
    - id: gitops-deployment
      name: GitOps Deployment System
      description: Git-driven deployments like ArgoCD with sync, rollback
      detailed: true
    expert:
    - id: build-raft
      name: Build Your Own Raft
      description: Consensus algorithm
      detailed: true
      languages:
      - Go
      - Rust
      - Java
    - id: build-distributed-kv
      name: Build Your Own Distributed KV Store
      description: Partitioning, replication
      detailed: true
    - id: infrastructure-as-code
      name: Infrastructure as Code Engine
      description: Terraform-like resource provisioning
      detailed: true
    - id: serverless-runtime
      name: Serverless Function Runtime
      description: Function-as-a-Service with cold start optimization, scaling
      detailed: true
- id: ai-ml
  name: AI & Machine Learning
  icon: ðŸ§ 
  subdomains:
  - name: Classical ML
  - name: Deep Learning
  - name: NLP
  projects:
    beginner:
    - id: linear-regression
      name: Linear Regression
      description: Gradient descent
      detailed: true
    - id: knn
      name: KNN Classifier
      description: Distance metrics
      detailed: true
    - id: chatbot-intent
      name: Intent-Based Chatbot
      detailed: true
    intermediate:
    - id: neural-network-basic
      name: Neural Network (micrograd)
      description: Forward/backward pass
      detailed: true
    - id: word2vec
      name: Word Embeddings
      description: Skip-gram, CBOW
      detailed: true
    - id: rag-system
      name: RAG System (Retrieval Augmented Generation)
      detailed: true
    - id: semantic-search
      name: Semantic Search Engine
      detailed: true
    - id: recommendation-engine
      name: Recommendation Engine
      detailed: true
    - id: ml-model-serving
      name: ML Model Serving API
      description: Model inference service with batching, versioning, A/B testing
      detailed: true
    advanced:
    - id: transformer-scratch
      name: Transformer from Scratch
      description: Attention mechanism
      detailed: true
    - id: ai-agent-framework
      name: AI Agent Framework
      detailed: true
    - id: llm-eval-framework
      name: LLM Evaluation Framework
      detailed: true
    - id: llm-finetuning-pipeline
      name: LLM Fine-tuning Pipeline
      description: LoRA/QLoRA fine-tuning with dataset preparation, evaluation
      detailed: true
    - id: mlops-platform
      name: MLOps Platform
      description: 'End-to-end ML lifecycle: training, versioning, deployment, monitoring'
      detailed: true
    expert:
    - id: build-nn-framework
      name: Build Your Own Neural Network Framework
      description: PyTorch/TensorFlow clone
      detailed: true
    - id: build-transformer
      name: Build Your Own Transformer
      description: Full GPT implementation
      detailed: true
- id: game-dev
  name: Game Development
  icon: ðŸŽ®
  subdomains:
  - name: Game Programming
  - name: Graphics
  - name: Engine Development
  projects:
    beginner:
    - id: tetris
      name: Tetris
      description: Rotation, line clearing
      detailed: true
    intermediate:
    - id: platformer
      name: Platformer
      description: Gravity, jumping
      detailed: true
    - id: topdown-shooter
      name: Top-down Shooter
      description: Enemies, projectiles
      detailed: true
    advanced:
    - id: software-3d
      name: Software 3D Renderer
      description: No GPU, pure math
      detailed: true
    - id: ecs-arch
      name: ECS Architecture
      description: Entity-component-system
      detailed: true
    expert:
    - id: build-game-engine
      name: Build Your Own Game Engine
      description: Full 2D/3D engine
      detailed: true
    - id: build-raytracer
      name: Build Your Own Ray Tracer
      description: Path tracing renderer
      detailed: true
- id: compilers
  name: Languages & Compilers
  icon: ðŸ“
  subdomains:
  - name: Parsing & Lexing
  - name: Interpreters
  - name: Compilers
  - name: Runtime Systems
  projects:
    beginner:
    - id: calculator-parser
      name: Calculator Parser
      description: Arithmetic expressions
      detailed: true
    - id: json-parser
      name: JSON Parser
      description: Recursive descent
      detailed: true
    - id: tokenizer
      name: Tokenizer/Lexer
      description: Token types, state machine
      detailed: true
      bridge: true
    intermediate:
    - id: lisp-interp
      name: Lisp Interpreter
      description: S-expressions, eval
      detailed: true
    - id: bytecode-vm
      name: Bytecode VM
      description: Stack-based
      detailed: true
    - id: ast-builder
      name: AST Builder
      description: Parse expressions to AST
      detailed: true
      bridge: true
    - id: ast-interpreter
      name: AST Tree-Walking Interpreter
      description: Evaluate AST directly
      detailed: true
      bridge: true
    advanced:
    - id: type-checker
      name: Type Checker
      description: Type inference
      detailed: true
    - id: simple-gc
      name: Simple GC
      description: Mark-sweep
      detailed: true
    - id: bytecode-compiler
      name: Bytecode Compiler
      description: AST to bytecode
      detailed: true
      bridge: true
    - id: wasm-emitter
      name: WebAssembly Emitter
      description: Emit .wasm from AST
      detailed: true
      bridge: true
    expert:
    - id: build-interpreter
      name: Build Your Own Interpreter (Lox)
      description: Crafting Interpreters
      detailed: true
      languages:
      - Java
      - C
      - Rust
      - Go
    - id: build-gc
      name: Build Your Own Garbage Collector
      description: Memory management
      detailed: true
      languages:
      - C
      - Rust
      - Zig
    - id: build-regex
      name: Build Your Own Regex Engine
      description: NFA/DFA, Thompson construction
      detailed: true
      languages:
      - C
      - Rust
      - Go
      - Python
- id: security
  name: Security
  icon: ðŸ”
  subdomains:
  - name: Cryptography
  - name: Web Security
  projects:
    beginner:
    - id: hash-impl
      name: Hash Function
      description: SHA-256 from spec
      detailed: true
    - id: password-hashing
      name: Password Hashing
      description: bcrypt, salt
      detailed: true
    intermediate:
    - id: aes-impl
      name: AES Implementation
      description: Block cipher
      detailed: true
    - id: jwt-impl
      name: JWT Library
      description: Sign, verify
      detailed: true
    - id: session-management
      name: Session Management
      description: Secure session handling with tokens and cookies
      detailed: true
    - id: audit-logging
      name: Audit Logging System
      description: Immutable audit trail for compliance
      detailed: true
    - id: sandbox
      name: Process Sandbox
      description: Sandboxing with seccomp, namespaces, capabilities
      detailed: true
    advanced:
    - id: https-client
      name: HTTPS Client
      description: TLS handshake
      detailed: true
    - id: oauth2-provider
      name: OAuth2/OIDC Provider
      description: Identity provider with authorization code flow, PKCE, JWT
      detailed: true
    - id: rbac-system
      name: RBAC/ABAC Authorization
      description: Role and attribute based access control system
      detailed: true
    - id: secret-management
      name: Secret Management Vault
      description: Secure storage for API keys, credentials, certificates
      detailed: true
    - id: fuzzer
      name: Fuzzing Framework
      description: Coverage-guided fuzzer like AFL with mutation strategies
      detailed: true
    - id: vulnerability-scanner
      name: Vulnerability Scanner
      description: Network/web vulnerability scanner with CVE detection
      detailed: true
    expert:
    - id: build-tls
      name: Build Your Own TLS
      description: TLS 1.3 implementation
      detailed: true
- id: specialized
  name: Specialized
  icon: ðŸ”§
  subdomains:
  - name: Developer Tools
  - name: Advanced Networking
  - name: Embedded & Emulation
  projects:
    beginner:
    - id: markdown-renderer
      name: Markdown Renderer
      description: Markdown to HTML converter with CommonMark spec
      detailed: true
    - id: hexdump
      name: Hexdump Utility
      description: Binary file viewer with hex/ASCII display
      detailed: true
    - id: diff-tool
      name: Diff Tool
      description: Text diff using LCS algorithm
      detailed: true
    intermediate:
    - id: config-parser
      name: Config File Parser
      description: Parse INI, TOML, and YAML formats
      detailed: true
    - id: packet-sniffer
      name: Packet Sniffer
      description: Network packet capture and protocol parsing
      detailed: true
    - id: profiler
      name: CPU/Memory Profiler
      description: Sampling profiler with flame graphs, memory tracking
      detailed: true
    advanced:
    - id: disassembler
      name: x86 Disassembler
      description: Instruction decoder for x86/x64 binaries
      detailed: true
    - id: terminal-multiplexer
      name: Terminal Multiplexer
      description: Simple tmux-like terminal manager
      detailed: true
    - id: protocol-buffer
      name: Protocol Buffer
      description: Binary serialization format implementation
      detailed: true
    - id: payment-gateway
      name: Payment Gateway
      description: Payment processing with PCI compliance
      detailed: true
    - id: subscription-billing
      name: Subscription Billing
      description: Recurring payments, invoicing, dunning
      detailed: true
    - id: webhook-delivery
      name: Webhook Delivery System
      description: Reliable webhook dispatch with retries
      detailed: true
    - id: cdn-implementation
      name: CDN Implementation
      description: Content delivery with edge caching
      detailed: true
    - id: order-matching-engine
      name: Order Matching Engine
      description: Low-latency trading engine with order book, price-time priority
      detailed: true
    - id: ledger-system
      name: Double-entry Ledger System
      description: Accounting system with journal entries, balance sheets, audit trail
      detailed: true
    - id: lock-free-structures
      name: Lock-free Data Structures
      description: Lock-free queue, stack, hashmap with CAS operations
      detailed: true
    - id: cache-optimized-structures
      name: Cache-Optimized Data Structures
      description: Cache-oblivious algorithms, B+ trees, memory layouts
      detailed: true
    expert:
    - id: build-git
      name: Build Your Own Git
      description: Version control
      detailed: true
    - id: build-text-editor
      name: Build Your Own Text Editor
      description: Vim-like editor
      detailed: true
    - id: build-bittorrent
      name: Build Your Own BitTorrent
      description: P2P file sharing
      detailed: true
    - id: build-dns
      name: Build Your Own DNS Server
      description: Recursive resolver
      detailed: true
    - id: build-debugger
      name: Build Your Own Debugger
      description: GDB-like
      detailed: true
    - id: build-lsp
      name: Build Your Own LSP Server
      description: Language server protocol
      detailed: true
    - id: build-emulator
      name: Build Your Own Emulator
      description: NES/GameBoy/CHIP-8
      detailed: true
    - id: build-browser
      name: Build Your Own Browser
      description: Browser engine
      detailed: true
    - id: multiplayer-game-server
      name: Multiplayer Game Server
      description: Real-time game state synchronization
      detailed: true
    - id: build-vpn
      name: Build Your Own VPN
      description: VPN with TUN/TAP, encryption, key exchange
      detailed: true
- id: software-engineering
  name: Software Engineering Practices
  icon: âœ…
  subdomains:
  - name: Testing & Quality
  - name: CI/CD
  - name: Observability
  - name: Engineering Practices
  projects:
    beginner:
    - id: unit-testing-basics
      name: Unit Testing Fundamentals
      description: pytest/jest basics
      detailed: true
    intermediate:
    - id: logging-structured
      name: Structured Logging
      description: JSON logs, aggregation
      detailed: true
    - id: ci-cd-pipeline
      name: CI/CD Pipeline Builder
      description: Automated build, test, deploy workflows
      detailed: true
    advanced:
    - id: integration-testing
      name: Integration Testing Suite
      description: Test containers, mocking
      detailed: true
    - id: cd-deployment
      name: CD with Blue-Green Deployment
      description: Zero-downtime deploys
      detailed: true
    - id: metrics-dashboard
      name: Metrics & Alerting Dashboard
      description: Prometheus, Grafana
      detailed: true
    - id: load-testing-framework
      name: Load Testing Framework
      description: Performance testing with distributed load
      detailed: true
    - id: log-aggregator
      name: Log Aggregation System
      description: Centralized logging with search and alerts
      detailed: true
    - id: alerting-system
      name: Alerting System
      description: Metric-based alerts with escalation
      detailed: true
    - id: apm-system
      name: APM System
      description: Application performance monitoring
      detailed: true
    - id: metrics-collector
      name: Metrics Collector
      description: Time-series metrics collection and aggregation
      detailed: true
    expert:
    - id: build-test-framework
      name: Build Your Own Test Framework
      description: pytest/jest clone
      detailed: true
    - id: build-ci-system
      name: Build Your Own CI System
      description: Pipeline executor
      detailed: true
    - id: build-observability-platform
      name: Build Your Own Observability Platform
      description: Logs/metrics/traces
      detailed: true
expert_projects:
  2pc-impl:
    id: 2pc-impl
    name: Two-Phase Commit
    description: Implement 2PC for distributed transactions. Learn atomic commitment and coordinator recovery.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Distributed systems
    - Transaction concepts
    - Failure handling
    languages:
      recommended:
      - Go
      - Java
      - Python
      also_possible:
      - Rust
      - Erlang
    resources:
    - name: 2PC Paper
      url: https://www.cs.princeton.edu/courses/archive/fall16/cos418/papers/bernstein-ch7.pdf
      type: paper
    - name: Distributed Transactions
      url: https://www.the-paper-trail.org/post/2014-10-16-consensus-protocols-two-phase-commit/
      type: article
    milestones:
    - id: 1
      name: Transaction Log
      description: Implement durable transaction log for recovery.
      acceptance_criteria:
      - Write-ahead log persists every state transition to disk before the coordinator acts on it
      - Log records capture PREPARE, COMMIT, and ABORT states with associated participant identifiers
      - Coordinator process survives kill-9 and recovers correct transaction state from the log on restart
      - Recovery procedure replays the log and resumes any in-progress two-phase commit protocol rounds
      hints:
        level1: Log must be durable (fsync). Write before sending messages.
        level2: 'Log format: [tx_id, state, participants]. State = PREPARE/COMMIT/ABORT.'
        level3: "import json\nimport os\n\nclass TransactionLog:\n    def __init__(self, path: str):\n        self.path =\
          \ path\n        self.file = open(path, 'a+')\n    \n    def log(self, tx_id: str, state: str, data: dict = None):\n\
          \        record = {'tx_id': tx_id, 'state': state, 'data': data or {}}\n        self.file.write(json.dumps(record)\
          \ + '\\n')\n        self.file.flush()\n        os.fsync(self.file.fileno())\n    \n    def recover(self) -> dict:\n\
          \        '''Return {tx_id: last_state}'''\n        self.file.seek(0)\n        transactions = {}\n        for line\
          \ in self.file:\n            if line.strip():\n                record = json.loads(line)\n                transactions[record['tx_id']]\
          \ = record\n        return transactions\n\nclass Coordinator:\n    def __init__(self, log_path: str):\n        self.log\
          \ = TransactionLog(log_path)\n        self.pending = {}  # tx_id -> {participants, votes}\n    \n    def recover(self):\n\
          \        '''Recover after crash'''\n        transactions = self.log.recover()\n        for tx_id, record in transactions.items():\n\
          \            if record['state'] == 'PREPARE':\n                # Was in prepare phase, need to abort or retry\n\
          \                self.abort_transaction(tx_id, record['data']['participants'])\n            elif record['state']\
          \ == 'COMMIT':\n                # Committed but may not have notified all\n                self.send_decision(tx_id,\
          \ 'COMMIT', record['data']['participants'])"
      pitfalls:
      - Incomplete writes
      - Recovery order
      - Log truncation
      concepts:
      - Write-ahead logging
      - Durability
      - Crash recovery
      estimated_hours: 3-4
      deliverables:
      - Write-ahead log with fsync durability guaranteeing persistence before acknowledgment
      - Log record format containing tx_id, state, and participant list for each transaction
      - Recovery function that replays the log on restart to restore last-known transaction states
      - Log compaction and truncation that removes committed entries while preserving incomplete ones
    - id: 2
      name: Prepare Phase
      description: Implement the voting/prepare phase.
      acceptance_criteria:
      - Coordinator broadcasts PREPARE to every participant listed in the transaction record
      - Each participant votes YES only after successfully acquiring all required resource locks
      - Participant persists its vote to its local log before sending the vote response to the coordinator
      - Coordinator aborts the transaction if any participant vote is not received within the configured timeout
      hints:
        level1: 'PREPARE asks: can you commit? Participant checks and votes.'
        level2: Participant must log YES vote before responding (promise to commit if asked).
        level3: "class Coordinator:\n    def begin_2pc(self, tx_id: str, participants: list):\n        # Log prepare intent\n\
          \        self.log.log(tx_id, 'PREPARE', {'participants': participants})\n        \n        self.pending[tx_id] =\
          \ {\n            'participants': participants,\n            'votes': {},\n            'state': 'PREPARING'\n   \
          \     }\n        \n        # Send PREPARE to all participants\n        for p in participants:\n            self.send_to(p,\
          \ 'PREPARE', {'tx_id': tx_id})\n        \n        # Wait for votes (with timeout)\n        threading.Timer(5.0,\
          \ lambda: self.check_votes(tx_id)).start()\n    \n    def on_vote(self, participant: str, tx_id: str, vote: str):\n\
          \        if tx_id not in self.pending:\n            return\n        \n        self.pending[tx_id]['votes'][participant]\
          \ = vote\n        \n        # Check if all votes received\n        if len(self.pending[tx_id]['votes']) == len(self.pending[tx_id]['participants']):\n\
          \            self.decide(tx_id)\n\nclass Participant:\n    def on_prepare(self, tx_id: str):\n        if self.can_commit(tx_id):\n\
          \            # Log YES vote (promise)\n            self.log.log(tx_id, 'VOTE_YES')\n            self.send_to_coordinator('VOTE',\
          \ {'tx_id': tx_id, 'vote': 'YES'})\n        else:\n            self.log.log(tx_id, 'VOTE_NO')\n            self.send_to_coordinator('VOTE',\
          \ {'tx_id': tx_id, 'vote': 'NO'})"
      pitfalls:
      - Logging before response
      - Vote timeout
      - Participant crash after voting
      concepts:
      - Voting protocol
      - Prepare phase
      - Participant state
      estimated_hours: 4-5
      deliverables:
      - Coordinator sends PREPARE message to all registered participants for a given transaction
      - Participants acquire necessary locks and respond with VOTE_COMMIT or VOTE_ABORT
      - Timeout handling that aborts the transaction when a participant fails to respond within deadline
      - Prepare state persisted to the write-ahead log before any vote response is sent
    - id: 3
      name: Commit Phase
      description: Implement the decision/commit phase.
      acceptance_criteria:
      - All YES votes result in COMMIT decision; any single NO vote results in global ABORT
      - Coordinator logs the final decision to stable storage before sending it to any participant
      - Coordinator sends the decision message to all participants and waits for acknowledgments
      - Transaction completes only after all participant acknowledgments are received or timed out
      hints:
        level1: Decision is final once logged. Must eventually reach all participants.
        level2: Retry sending decision to failed participants.
        level3: "class Coordinator:\n    def decide(self, tx_id: str):\n        votes = self.pending[tx_id]['votes']\n   \
          \     participants = self.pending[tx_id]['participants']\n        \n        # All YES -> COMMIT, otherwise ABORT\n\
          \        all_yes = all(v == 'YES' for v in votes.values())\n        decision = 'COMMIT' if all_yes else 'ABORT'\n\
          \        \n        # Log decision BEFORE sending\n        self.log.log(tx_id, decision, {'participants': participants})\n\
          \        \n        # Send decision to all participants\n        self.send_decision(tx_id, decision, participants)\n\
          \    \n    def send_decision(self, tx_id: str, decision: str, participants: list):\n        pending_acks = set(participants)\n\
          \        \n        while pending_acks:\n            for p in list(pending_acks):\n                if self.send_to(p,\
          \ decision, {'tx_id': tx_id}):\n                    # Wait for ACK\n                    pass\n            time.sleep(1)\
          \  # Retry failed participants\n    \n    def on_ack(self, participant: str, tx_id: str):\n        # Remove from\
          \ pending, eventually log COMPLETE\n        pass\n\nclass Participant:\n    def on_commit(self, tx_id: str):\n \
          \       self.log.log(tx_id, 'COMMIT')\n        self.apply_transaction(tx_id)\n        self.send_ack(tx_id)\n   \
          \ \n    def on_abort(self, tx_id: str):\n        self.log.log(tx_id, 'ABORT')\n        self.rollback_transaction(tx_id)\n\
          \        self.send_ack(tx_id)"
      pitfalls:
      - Decision before logging
      - Lost messages
      - Participant uncertainty
      concepts:
      - Commit protocol
      - Atomic commitment
      - Decision logging
      estimated_hours: 4-5
      deliverables:
      - Coordinator decides COMMIT if all participants voted yes, ABORT otherwise
      - COMMIT or ABORT message broadcast reliably to all participants in the transaction
      - Participants apply committed changes and release all acquired locks upon receiving COMMIT
      - Acknowledgment collection from all participants confirming they have applied the decision
    - id: 4
      name: Failure Recovery
      description: Handle coordinator and participant failures.
      acceptance_criteria:
      - Coordinator restarts after a crash and correctly re-drives any in-progress transactions to completion
      - Participant that times out waiting for a decision queries the coordinator or other participants for the outcome
      - Participant recovers after crash by consulting its local log and contacting the coordinator if needed
      - Blocking scenario is detected and handled when coordinator is down and participants are in uncertain state
      hints:
        level1: 'On recovery: check log. If COMMIT logged, re-send. If only PREPARE, can abort.'
        level2: 'Participant in doubt (voted YES, no decision): must wait or ask coordinator.'
        level3: "class Participant:\n    def recover(self):\n        '''Recovery after crash'''\n        transactions = self.log.recover()\n\
          \        for tx_id, record in transactions.items():\n            state = record['state']\n            \n       \
          \     if state == 'VOTE_YES':\n                # I voted YES but don't know decision\n                # Must ask\
          \ coordinator\n                self.query_coordinator(tx_id)\n            elif state == 'COMMIT':\n            \
          \    # Was committed, ensure applied\n                if not self.is_applied(tx_id):\n                    self.apply_transaction(tx_id)\n\
          \            elif state == 'ABORT':\n                # Was aborted, ensure rolled back\n                if not self.is_rolled_back(tx_id):\n\
          \                    self.rollback_transaction(tx_id)\n    \n    def query_coordinator(self, tx_id: str):\n    \
          \    '''Ask coordinator for decision (may block if coordinator down)'''\n        response = self.send_to_coordinator('QUERY',\
          \ {'tx_id': tx_id})\n        if response:\n            if response['decision'] == 'COMMIT':\n                self.on_commit(tx_id)\n\
          \            else:\n                self.on_abort(tx_id)\n        else:\n            # Coordinator unreachable -\
          \ must wait\n            # This is the blocking problem of 2PC\n            self.in_doubt.add(tx_id)\n\nclass Coordinator:\n\
          \    def on_query(self, tx_id: str):\n        '''Participant asking for decision'''\n        transactions = self.log.recover()\n\
          \        if tx_id in transactions:\n            return {'decision': transactions[tx_id]['state']}\n        else:\n\
          \            # Never heard of it, must have aborted\n            return {'decision': 'ABORT'}"
      pitfalls:
      - In-doubt blocking
      - Coordinator single point of failure
      - Network partitions
      concepts:
      - Crash recovery
      - Blocking protocols
      - Failure handling
      estimated_hours: 4-6
      deliverables:
      - Coordinator crash recovery that re-reads the log and resumes the protocol from last known state
      - Participant crash recovery that checks its local log and queries the coordinator for unknown outcomes
      - Blocking protocol handling that detects and manages scenarios where the coordinator is unreachable
      - Presumed abort optimization that reduces log writes by assuming abort for missing decisions
  aes-impl:
    id: aes-impl
    name: AES Encryption Implementation
    description: Implement AES encryption from scratch. Learn symmetric cryptography, block ciphers, and modes of operation.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Binary operations
    - Modular arithmetic
    - Basic cryptography concepts
    languages:
      recommended:
      - C
      - Rust
      - Python
      also_possible:
      - Go
      - Java
    resources:
    - name: FIPS 197 (AES Spec)
      url: https://csrc.nist.gov/publications/detail/fips/197/final
      type: paper
    - name: A Stick Figure Guide to AES
      url: http://www.moserware.com/2009/09/stick-figure-guide-to-advanced.html
      type: article
    milestones:
    - id: 1
      name: Galois Field Arithmetic
      description: Implement GF(2^8) operations used in AES.
      acceptance_criteria:
      - Field addition correctly computes a XOR b for any two elements in GF(2^8)
      - Multiplication in GF(2^8) produces correct results verified against known test vectors
      - Multiplicative inverse lookup returns the correct inverse for every non-zero element in the field
      - Pre-computed S-box and inverse S-box tables match the values defined in the AES specification (FIPS 197)
      hints:
        level1: GF(2^8) addition = XOR. Multiplication involves polynomial math.
        level2: Use irreducible polynomial x^8 + x^4 + x^3 + x + 1 (0x11B).
        level3: |-
          def gf_add(a, b):
              '''Addition in GF(2^8) is XOR'''
              return a ^ b

          def gf_mul(a, b):
              '''Multiplication in GF(2^8) with irreducible polynomial 0x11B'''
              p = 0
              for _ in range(8):
                  if b & 1:
                      p ^= a
                  hi_bit = a & 0x80
                  a = (a << 1) & 0xFF
                  if hi_bit:
                      a ^= 0x1B  # Reduce by x^8 + x^4 + x^3 + x + 1
                  b >>= 1
              return p

          # Pre-compute lookup tables for efficiency
          MUL2 = [gf_mul(i, 2) for i in range(256)]
          MUL3 = [gf_mul(i, 3) for i in range(256)]

          def gf_inverse(a):
              '''Find multiplicative inverse using extended Euclidean algorithm'''
              if a == 0:
                  return 0
              # Or use: a^254 = a^(-1) in GF(2^8)
              result = a
              for _ in range(6):
                  result = gf_mul(result, result)
                  result = gf_mul(result, a)
              return gf_mul(result, result)
      pitfalls:
      - Overflow handling
      - Wrong irreducible polynomial
      - Off-by-one in tables
      concepts:
      - Galois fields
      - Polynomial arithmetic
      - Lookup tables
      estimated_hours: 3-4
      deliverables:
      - GF(2^8) multiplication using the irreducible polynomial x^8+x^4+x^3+x+1 (0x11B)
      - Lookup tables for the S-box and inverse S-box precomputed from the multiplicative inverse in GF(2^8)
      - XOR-based field addition that operates as bitwise exclusive-or on byte operands
      - MixColumns multiplication implementation using xtime and reduction modulo the irreducible polynomial
    - id: 2
      name: AES Core Operations
      description: Implement the four AES round operations.
      acceptance_criteria:
      - SubBytes replaces every byte in the 4x4 state matrix with its S-box substitution value
      - ShiftRows cyclically left-shifts row i by i positions in the state matrix
      - MixColumns multiplies each column of the state by the polynomial {03}x^3+{01}x^2+{01}x+{02} in GF(2^8)
      - AddRoundKey XORs each byte of the state with the corresponding byte of the expanded round key
      hints:
        level1: AES state is 4x4 byte matrix. Each round applies 4 transformations.
        level2: 'SubBytes: apply S-box to each byte. ShiftRows: rotate rows.'
        level3: |-
          # S-box (pre-computed)
          SBOX = [
              0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5,
              # ... (256 values total)
          ]

          def sub_bytes(state):
              '''Apply S-box to each byte'''
              for i in range(4):
                  for j in range(4):
                      state[i][j] = SBOX[state[i][j]]

          def shift_rows(state):
              '''Rotate rows: row 0 by 0, row 1 by 1, row 2 by 2, row 3 by 3'''
              state[1] = state[1][1:] + state[1][:1]
              state[2] = state[2][2:] + state[2][:2]
              state[3] = state[3][3:] + state[3][:3]

          def mix_columns(state):
              '''Mix each column using GF(2^8) matrix multiplication'''
              for j in range(4):
                  a = [state[i][j] for i in range(4)]
                  state[0][j] = MUL2[a[0]] ^ MUL3[a[1]] ^ a[2] ^ a[3]
                  state[1][j] = a[0] ^ MUL2[a[1]] ^ MUL3[a[2]] ^ a[3]
                  state[2][j] = a[0] ^ a[1] ^ MUL2[a[2]] ^ MUL3[a[3]]
                  state[3][j] = MUL3[a[0]] ^ a[1] ^ a[2] ^ MUL2[a[3]]

          def add_round_key(state, round_key):
              '''XOR state with round key'''
              for i in range(4):
                  for j in range(4):
                      state[i][j] ^= round_key[i][j]
      pitfalls:
      - State matrix orientation
      - ShiftRows direction
      - MixColumns coefficients
      concepts:
      - Substitution
      - Permutation
      - Diffusion
      estimated_hours: 4-6
      deliverables:
      - SubBytes transformation that substitutes each byte using the precomputed S-box lookup table
      - ShiftRows transformation that cyclically left-shifts each row of the state matrix
      - MixColumns transformation that multiplies each column by a fixed polynomial in GF(2^8)
      - AddRoundKey transformation that XORs the state matrix with the current round key
    - id: 3
      name: Key Expansion
      description: Implement the key schedule for AES-128/192/256.
      acceptance_criteria:
      - Expanding a 128-bit key produces exactly 44 words (11 round keys of 4 words each)
      - RotWord and SubWord functions produce correct intermediate values verified against FIPS 197 examples
      - Round constants (Rcon) are correctly computed as successive powers of 2 in GF(2^8)
      - Key expansion supports AES-128, AES-192, and AES-256 key sizes with correct output lengths
      hints:
        level1: Key expansion creates 11 round keys (128-bit) from original key.
        level2: Each round key derived from previous. Special transform every 4th word.
        level3: "RCON = [0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80, 0x1B, 0x36]\n\ndef rot_word(word):\n    '''Rotate\
          \ 4-byte word left by 1'''\n    return word[1:] + word[:1]\n\ndef sub_word(word):\n    '''Apply S-box to each byte\
          \ of word'''\n    return [SBOX[b] for b in word]\n\ndef key_expansion(key):\n    '''Expand 16-byte key to 44 words\
          \ (11 round keys)'''\n    # Key is 16 bytes = 4 words\n    w = [list(key[i:i+4]) for i in range(0, 16, 4)]\n   \
          \ \n    for i in range(4, 44):  # Generate 40 more words\n        temp = w[i-1].copy()\n        if i % 4 == 0:\n\
          \            temp = sub_word(rot_word(temp))\n            temp[0] ^= RCON[i // 4 - 1]\n        w.append([w[i-4][j]\
          \ ^ temp[j] for j in range(4)])\n    \n    # Convert to round keys (4 words each)\n    round_keys = []\n    for\
          \ r in range(11):\n        round_key = [[w[r*4 + j][i] for j in range(4)] for i in range(4)]\n        round_keys.append(round_key)\n\
          \    \n    return round_keys"
      pitfalls:
      - Rcon indexing
      - Word vs byte ordering
      - Key length handling
      concepts:
      - Key schedule
      - Round constants
      - Key expansion
      estimated_hours: 3-4
      deliverables:
      - Round constant (Rcon) generation producing the correct powers of x in GF(2^8) for each round
      - Key schedule algorithm that expands 128/192/256-bit keys into the required number of round keys
      - 'Correct number of round keys generated: 11 for AES-128, 13 for AES-192, and 15 for AES-256'
      - Word rotation (RotWord) and S-box application (SubWord) used within the key expansion process
    - id: 4
      name: Encryption & Modes
      description: Complete encryption and implement cipher modes.
      acceptance_criteria:
      - Full AES encrypt function produces correct ciphertext matching NIST FIPS 197 test vectors
      - ECB mode encrypts each 16-byte block independently and produces deterministic output for same input
      - CBC mode XORs each plaintext block with the previous ciphertext block using the provided IV
      - PKCS7 padding is correctly applied so plaintext is extended to a multiple of 16 bytes before encryption
      hints:
        level1: 10 rounds for AES-128. Last round skips MixColumns.
        level2: 'CBC: each block XORed with previous ciphertext before encryption.'
        level3: "def aes_encrypt_block(plaintext, key):\n    '''Encrypt 16-byte block'''\n    # Convert to state matrix (column-major)\n\
          \    state = [[plaintext[i + 4*j] for j in range(4)] for i in range(4)]\n    round_keys = key_expansion(key)\n \
          \   \n    # Initial round\n    add_round_key(state, round_keys[0])\n    \n    # Main rounds\n    for r in range(1,\
          \ 10):\n        sub_bytes(state)\n        shift_rows(state)\n        mix_columns(state)\n        add_round_key(state,\
          \ round_keys[r])\n    \n    # Final round (no MixColumns)\n    sub_bytes(state)\n    shift_rows(state)\n    add_round_key(state,\
          \ round_keys[10])\n    \n    # Convert back to bytes\n    return bytes([state[i][j] for j in range(4) for i in range(4)])\n\
          \ndef pkcs7_pad(data, block_size=16):\n    pad_len = block_size - (len(data) % block_size)\n    return data + bytes([pad_len]\
          \ * pad_len)\n\ndef aes_cbc_encrypt(plaintext, key, iv):\n    '''CBC mode encryption'''\n    plaintext = pkcs7_pad(plaintext)\n\
          \    ciphertext = b''\n    prev_block = iv\n    \n    for i in range(0, len(plaintext), 16):\n        block = bytes(a\
          \ ^ b for a, b in zip(plaintext[i:i+16], prev_block))\n        encrypted = aes_encrypt_block(block, key)\n     \
          \   ciphertext += encrypted\n        prev_block = encrypted\n    \n    return ciphertext"
      pitfalls:
      - ECB mode vulnerabilities
      - IV reuse
      - Padding oracle attacks
      concepts:
      - Block cipher modes
      - CBC
      - Padding
      estimated_hours: 4-6
      deliverables:
      - Full AES encryption and decryption performing the correct number of rounds for the key size
      - ECB mode implementation that encrypts each block independently for reference and testing
      - CBC mode implementation with initialization vector XOR chaining between consecutive blocks
      - CTR mode implementation that encrypts a counter to produce a keystream for streaming encryption
  ast-builder:
    id: ast-builder
    name: AST Builder (Parser)
    description: Build a parser that converts tokens into an Abstract Syntax Tree. Learn grammar rules and tree structures.
    difficulty: intermediate
    estimated_hours: 12-20
    prerequisites:
    - Tokenizer/Lexer
    - Recursion
    - Tree data structures
    languages:
      recommended:
      - Python
      - JavaScript
      - Java
      also_possible:
      - Go
      - Rust
      - C
    resources:
    - name: Crafting Interpreters - Parsing
      url: https://craftinginterpreters.com/parsing-expressions.html
      type: book
    - name: Pratt Parsing
      url: https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html
      type: article
    milestones:
    - id: 1
      name: AST Node Definitions
      description: Define AST node types for your language.
      acceptance_criteria:
      - Expression nodes represent literals, binary operators, unary operators, and identifiers as distinct types
      - Statement nodes represent if-else, while, return, and block constructs with correct child references
      - Visitor pattern or tagged union dispatch enables processing each node type without type-casting
      - Every AST node includes source location data (file, line, column) for error reporting and debugging
      hints:
        level1: AST = tree of nodes, each representing a language construct.
        level2: Use inheritance or tagged unions. Visitor pattern for traversal.
        level3: |-
          from dataclasses import dataclass
          from typing import List, Optional, Any

          @dataclass
          class Expr:
              '''Base expression class'''
              pass

          @dataclass
          class Literal(Expr):
              value: Any

          @dataclass
          class Identifier(Expr):
              name: str

          @dataclass
          class Binary(Expr):
              left: Expr
              operator: str
              right: Expr

          @dataclass
          class Unary(Expr):
              operator: str
              operand: Expr

          @dataclass
          class Call(Expr):
              callee: Expr
              arguments: List[Expr]

          @dataclass
          class Stmt:
              '''Base statement class'''
              pass

          @dataclass
          class ExprStmt(Stmt):
              expression: Expr

          @dataclass
          class VarDecl(Stmt):
              name: str
              initializer: Optional[Expr]

          @dataclass
          class If(Stmt):
              condition: Expr
              then_branch: Stmt
              else_branch: Optional[Stmt]

          @dataclass
          class While(Stmt):
              condition: Expr
              body: Stmt

          @dataclass
          class Block(Stmt):
              statements: List[Stmt]

          @dataclass
          class Function(Stmt):
              name: str
              params: List[str]
              body: List[Stmt]
      pitfalls:
      - Missing node types
      - No location info
      - Mutable vs immutable nodes
      concepts:
      - AST nodes
      - Expression vs statement
      - Tree representation
      estimated_hours: 2-3
      deliverables:
      - Base Node class with source location tracking including file name, line number, and column offset
      - Expression node types covering literal values, binary operations, unary operations, and identifier references
      - Statement node types covering if-else branches, while loops, return statements, and block groupings
      - Visitor pattern implementation enabling type-safe tree traversal without modifying node classes
    - id: 2
      name: Recursive Descent Parser
      description: Implement recursive descent parsing for expressions.
      acceptance_criteria:
      - Each grammar production rule is implemented as a separate recursive parsing function
      - Operator precedence is respected so multiplication binds tighter than addition in the resulting AST
      - Function call expressions are parsed with the callee followed by a parenthesized argument list
      - Token consumption advances the stream correctly and reports errors on unexpected token types
      hints:
        level1: Each grammar rule = one parsing function. Lower precedence = called first.
        level2: expression -> equality -> comparison -> term -> factor -> unary -> primary
        level3: "class Parser:\n    def __init__(self, tokens):\n        self.tokens = tokens\n        self.current = 0\n\
          \    \n    def peek(self):\n        return self.tokens[self.current]\n    \n    def is_at_end(self):\n        return\
          \ self.peek().type == TokenType.EOF\n    \n    def advance(self):\n        if not self.is_at_end():\n          \
          \  self.current += 1\n        return self.tokens[self.current - 1]\n    \n    def check(self, type):\n        if\
          \ self.is_at_end():\n            return False\n        return self.peek().type == type\n    \n    def match(self,\
          \ *types):\n        for type in types:\n            if self.check(type):\n                self.advance()\n     \
          \           return True\n        return False\n    \n    def consume(self, type, message):\n        if self.check(type):\n\
          \            return self.advance()\n        raise ParseError(message, self.peek())\n    \n    # Grammar: expression\
          \ -> equality\n    def expression(self):\n        return self.equality()\n    \n    # equality -> comparison (('=='\
          \ | '!=') comparison)*\n    def equality(self):\n        expr = self.comparison()\n        while self.match(TokenType.EQUAL_EQUAL,\
          \ TokenType.BANG_EQUAL):\n            op = self.tokens[self.current - 1].value\n            right = self.comparison()\n\
          \            expr = Binary(expr, op, right)\n        return expr\n    \n    # comparison -> term (('<' | '>' | '<='\
          \ | '>=') term)*\n    def comparison(self):\n        expr = self.term()\n        while self.match(TokenType.LESS,\
          \ TokenType.GREATER, TokenType.LESS_EQUAL, TokenType.GREATER_EQUAL):\n            op = self.tokens[self.current\
          \ - 1].value\n            right = self.term()\n            expr = Binary(expr, op, right)\n        return expr\n\
          \    \n    # term -> factor (('+' | '-') factor)*\n    def term(self):\n        expr = self.factor()\n        while\
          \ self.match(TokenType.PLUS, TokenType.MINUS):\n            op = self.tokens[self.current - 1].value\n         \
          \   right = self.factor()\n            expr = Binary(expr, op, right)\n        return expr"
      pitfalls:
      - Wrong precedence order
      - Left recursion
      - Infinite loops
      concepts:
      - Recursive descent
      - Operator precedence
      - Grammar rules
      estimated_hours: 4-6
      deliverables:
      - Token stream consumer with single-token lookahead for predictive parsing decisions
      - Precedence climbing algorithm that correctly handles operator precedence levels and grouping
      - Recursive parsing functions with one function per grammar production rule
      - Operator associativity handling that correctly nests left-associative and right-associative operators
    - id: 3
      name: Statement Parsing
      description: Parse statements and declarations.
      acceptance_criteria:
      - Parser correctly handles let/var/const declarations with optional initializer expressions
      - If/else statements are parsed with condition, then-branch, and optional else-branch as child nodes
      - While loops are parsed with a condition expression and a body statement or block
      - Block statements group zero or more statements within braces and establish lexical scope boundaries
      hints:
        level1: statement = exprStmt | ifStmt | whileStmt | block | declaration
        level2: 'Block: ''{'' statement* ''}''. Declaration: ''var'' NAME (''='' expr)? '';'''
        level3: "def statement(self):\n    if self.match(TokenType.IF):\n        return self.if_statement()\n    if self.match(TokenType.WHILE):\n\
          \        return self.while_statement()\n    if self.match(TokenType.LBRACE):\n        return Block(self.block())\n\
          \    if self.match(TokenType.RETURN):\n        return self.return_statement()\n    return self.expression_statement()\n\
          \ndef if_statement(self):\n    self.consume(TokenType.LPAREN, \"Expect '(' after 'if'.\")\n    condition = self.expression()\n\
          \    self.consume(TokenType.RPAREN, \"Expect ')' after condition.\")\n    \n    then_branch = self.statement()\n\
          \    else_branch = None\n    if self.match(TokenType.ELSE):\n        else_branch = self.statement()\n    \n    return\
          \ If(condition, then_branch, else_branch)\n\ndef while_statement(self):\n    self.consume(TokenType.LPAREN, \"Expect\
          \ '(' after 'while'.\")\n    condition = self.expression()\n    self.consume(TokenType.RPAREN, \"Expect ')' after\
          \ condition.\")\n    body = self.statement()\n    return While(condition, body)\n\ndef block(self):\n    statements\
          \ = []\n    while not self.check(TokenType.RBRACE) and not self.is_at_end():\n        statements.append(self.declaration())\n\
          \    self.consume(TokenType.RBRACE, \"Expect '}' after block.\")\n    return statements\n\ndef declaration(self):\n\
          \    if self.match(TokenType.VAR):\n        return self.var_declaration()\n    if self.match(TokenType.FUN):\n \
          \       return self.function('function')\n    return self.statement()\n\ndef var_declaration(self):\n    name =\
          \ self.consume(TokenType.IDENTIFIER, 'Expect variable name.').value\n    initializer = None\n    if self.match(TokenType.EQUAL):\n\
          \        initializer = self.expression()\n    self.consume(TokenType.SEMICOLON, \"Expect ';' after variable declaration.\"\
          )\n    return VarDecl(name, initializer)"
      pitfalls:
      - Dangling else ambiguity
      - Missing semicolons
      - Block scope boundaries
      concepts:
      - Statement parsing
      - Control flow
      - Declarations
      estimated_hours: 4-5
      deliverables:
      - Variable declaration parsing supporting let, var, and const with optional initializer expressions
      - Function definition parsing including parameter list, return type annotation, and body block
      - Control flow statement parsing for if/else, while, and for constructs with correct body nesting
      - Block statement parsing that groups multiple statements within braces and establishes a new scope
    - id: 4
      name: Error Recovery
      description: Implement error handling and recovery.
      acceptance_criteria:
      - Parser reports multiple syntax errors in a single pass instead of stopping at the first error
      - After an error, the parser synchronizes to the next statement boundary and continues parsing
      - Error messages include the file name, line number, column, and a description of what was expected
      - Source locations in error messages accurately point to the token where the error was detected
      hints:
        level1: 'Panic mode: on error, skip tokens until synchronization point.'
        level2: 'Sync points: statement boundaries (semicolons, keywords like if/while/class).'
        level3: "class ParseError(Exception):\n    def __init__(self, message, token):\n        self.message = message\n \
          \       self.token = token\n        super().__init__(f'[line {token.line}] Error at {repr(token.value)}: {message}')\n\
          \nclass Parser:\n    def __init__(self, tokens):\n        self.tokens = tokens\n        self.current = 0\n     \
          \   self.errors = []\n        self.had_error = False\n    \n    def error(self, message, token=None):\n        token\
          \ = token or self.peek()\n        error = ParseError(message, token)\n        self.errors.append(error)\n      \
          \  self.had_error = True\n        return error\n    \n    def synchronize(self):\n        '''Skip tokens until we\
          \ reach a statement boundary'''\n        self.advance()\n        \n        while not self.is_at_end():\n       \
          \     # After semicolon, we're at statement boundary\n            if self.tokens[self.current - 1].type == TokenType.SEMICOLON:\n\
          \                return\n            \n            # These keywords start statements\n            if self.peek().type\
          \ in (\n                TokenType.CLASS, TokenType.FUN, TokenType.VAR,\n                TokenType.FOR, TokenType.IF,\
          \ TokenType.WHILE,\n                TokenType.RETURN\n            ):\n                return\n            \n   \
          \         self.advance()\n    \n    def declaration(self):\n        try:\n            if self.match(TokenType.VAR):\n\
          \                return self.var_declaration()\n            return self.statement()\n        except ParseError:\n\
          \            self.synchronize()\n            return None"
      pitfalls:
      - Stopping at first error
      - Bad sync points
      - Cascading errors
      concepts:
      - Error recovery
      - Panic mode
      - Synchronization
      estimated_hours: 2-3
      deliverables:
      - Synchronization points that advance the token stream to a known recovery position after an error
      - Multiple error collection that continues parsing after the first error to report additional issues
      - Meaningful error messages that include the expected token, actual token, and source location
      - Panic mode recovery that discards tokens until a statement boundary is found to resume parsing
  ast-interpreter:
    id: ast-interpreter
    name: AST Tree-Walking Interpreter
    description: Build an interpreter that directly evaluates the AST. Learn environments, scoping, and evaluation.
    difficulty: intermediate
    estimated_hours: 12-20
    prerequisites:
    - AST Builder
    - Recursion
    - Environment/scope concepts
    languages:
      recommended:
      - Python
      - JavaScript
      - Java
      also_possible:
      - Go
      - Rust
    resources:
    - name: Crafting Interpreters - Evaluating
      url: https://craftinginterpreters.com/evaluating-expressions.html
      type: book
    - name: SICP - Metacircular Evaluator
      url: https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-26.html
      type: book
    milestones:
    - id: 1
      name: Expression Evaluation
      description: Evaluate arithmetic and comparison expressions.
      acceptance_criteria:
      - Literal nodes evaluate to their corresponding runtime values (numbers, strings, booleans, nil)
      - Binary operators (+, -, *, /, <, >, ==, !=) produce correct results for valid operand types
      - Unary minus negates numeric values and logical-not inverts boolean truthiness
      - Parenthesized expressions are evaluated first, correctly overriding default operator precedence
      hints:
        level1: 'Recursive evaluate: check node type, recursively evaluate children, apply operation.'
        level2: Use visitor pattern or isinstance checks for different node types.
        level3: "class Interpreter:\n    def evaluate(self, node):\n        method_name = f'eval_{type(node).__name__}'\n\
          \        method = getattr(self, method_name, self.generic_eval)\n        return method(node)\n    \n    def generic_eval(self,\
          \ node):\n        raise RuntimeError(f'No eval method for {type(node).__name__}')\n    \n    def eval_Literal(self,\
          \ node):\n        return node.value\n    \n    def eval_Binary(self, node):\n        left = self.evaluate(node.left)\n\
          \        right = self.evaluate(node.right)\n        \n        ops = {\n            '+': lambda a, b: a + b,\n  \
          \          '-': lambda a, b: a - b,\n            '*': lambda a, b: a * b,\n            '/': lambda a, b: a / b,\n\
          \            '<': lambda a, b: a < b,\n            '>': lambda a, b: a > b,\n            '==': lambda a, b: a ==\
          \ b,\n            '!=': lambda a, b: a != b,\n            'and': lambda a, b: a and b,\n            'or': lambda\
          \ a, b: a or b,\n        }\n        \n        if node.operator not in ops:\n            raise RuntimeError(f'Unknown\
          \ operator: {node.operator}')\n        \n        return ops[node.operator](left, right)\n    \n    def eval_Unary(self,\
          \ node):\n        operand = self.evaluate(node.operand)\n        if node.operator == '-':\n            return -operand\n\
          \        elif node.operator == '!':\n            return not operand"
      pitfalls:
      - Type errors at runtime
      - Division by zero
      - Short-circuit evaluation
      concepts:
      - Tree-walking
      - Evaluation
      - Operator semantics
      estimated_hours: 3-4
      deliverables:
      - Literal value evaluation that returns numbers, strings, and booleans as runtime values
      - Binary operator evaluation with runtime type checking for arithmetic, comparison, and equality
      - Unary operator evaluation supporting numeric negation and logical-not operations
      - Short-circuit evaluation for logical AND and OR that skips the right operand when result is determined
    - id: 2
      name: Variables and Environment
      description: Implement variable binding and lookup.
      acceptance_criteria:
      - Environment stores name-to-value mappings and supports get, set, and define operations
      - Variable declaration with var or let creates a new binding in the current scope's environment
      - Variable assignment updates the binding in the nearest enclosing scope where the name is defined
      - Nested scopes look up variables by walking the parent environment chain until the name is found
      hints:
        level1: Environment = dict + parent pointer. Lookup checks local then parent.
        level2: New scope = new environment with current as parent.
        level3: "class Environment:\n    def __init__(self, parent=None):\n        self.values = {}\n        self.parent =\
          \ parent\n    \n    def define(self, name, value):\n        self.values[name] = value\n    \n    def get(self, name):\n\
          \        if name in self.values:\n            return self.values[name]\n        if self.parent:\n            return\
          \ self.parent.get(name)\n        raise RuntimeError(f'Undefined variable: {name}')\n    \n    def assign(self, name,\
          \ value):\n        if name in self.values:\n            self.values[name] = value\n            return\n        if\
          \ self.parent:\n            self.parent.assign(name, value)\n            return\n        raise RuntimeError(f'Undefined\
          \ variable: {name}')\n\nclass Interpreter:\n    def __init__(self):\n        self.environment = Environment()\n\
          \    \n    def eval_Identifier(self, node):\n        return self.environment.get(node.name)\n    \n    def eval_VarDecl(self,\
          \ node):\n        value = None\n        if node.initializer:\n            value = self.evaluate(node.initializer)\n\
          \        self.environment.define(node.name, value)\n    \n    def eval_Assignment(self, node):\n        value =\
          \ self.evaluate(node.value)\n        self.environment.assign(node.name, value)\n        return value\n    \n   \
          \ def eval_Block(self, node):\n        # New scope\n        previous = self.environment\n        self.environment\
          \ = Environment(parent=previous)\n        \n        try:\n            result = None\n            for stmt in node.statements:\n\
          \                result = self.evaluate(stmt)\n            return result\n        finally:\n            self.environment\
          \ = previous"
      pitfalls:
      - Scope restoration
      - Shadowing
      - Assignment vs declaration
      concepts:
      - Environments
      - Scoping
      - Variable binding
      estimated_hours: 3-4
      deliverables:
      - Environment class that maps variable names to their current runtime values
      - Nested scopes with a parent environment chain enabling lexical scope variable lookup
      - Variable declaration that binds a name to an initial value in the current environment
      - Undefined variable error handling that raises a clear runtime error with the variable name
    - id: 3
      name: Control Flow
      description: Implement if/else and loops.
      acceptance_criteria:
      - If/else statements correctly evaluate the condition and execute only the matching branch
      - While loops repeat the body as long as the condition evaluates to a truthy value
      - For loops execute the initializer once, check the condition each iteration, and run the increment
      - Break and continue statements correctly exit or restart the nearest enclosing loop iteration
      hints:
        level1: Evaluate condition, then evaluate appropriate branch.
        level2: 'Loops: evaluate condition, if true evaluate body, repeat.'
        level3: "def eval_If(self, node):\n    condition = self.evaluate(node.condition)\n    \n    if self.is_truthy(condition):\n\
          \        return self.evaluate(node.then_branch)\n    elif node.else_branch:\n        return self.evaluate(node.else_branch)\n\
          \    return None\n\ndef eval_While(self, node):\n    result = None\n    while self.is_truthy(self.evaluate(node.condition)):\n\
          \        try:\n            result = self.evaluate(node.body)\n        except BreakException:\n            break\n\
          \        except ContinueException:\n            continue\n    return result\n\ndef eval_For(self, node):\n    #\
          \ for (init; condition; increment) body\n    # Desugar to: { init; while (condition) { body; increment; } }\n  \
          \  previous = self.environment\n    self.environment = Environment(parent=previous)\n    \n    try:\n        if\
          \ node.initializer:\n            self.evaluate(node.initializer)\n        \n        while True:\n            if\
          \ node.condition:\n                if not self.is_truthy(self.evaluate(node.condition)):\n                    break\n\
          \            \n            try:\n                self.evaluate(node.body)\n            except BreakException:\n\
          \                break\n            except ContinueException:\n                pass\n            \n            if\
          \ node.increment:\n                self.evaluate(node.increment)\n    finally:\n        self.environment = previous\n\
          \ndef is_truthy(self, value):\n    if value is None:\n        return False\n    if isinstance(value, bool):\n  \
          \      return value\n    return True"
      pitfalls:
      - Infinite loops
      - Break outside loop
      - Truthiness rules
      concepts:
      - Control flow
      - Loops
      - Conditionals
      estimated_hours: 3-4
      deliverables:
      - If/else statement execution that evaluates the condition and runs the appropriate branch
      - While loop execution with support for break and continue control flow statements
      - For loop desugaring that translates for-loops into equivalent while-loop AST structures
      - Return statement execution with value propagation back to the enclosing function call
    - id: 4
      name: Functions
      description: Implement function definitions and calls.
      acceptance_criteria:
      - Function declarations create a callable value stored in the current environment by name
      - Function calls bind each argument to the corresponding parameter in a new local environment
      - Return statements unwind execution and deliver the return value to the call site
      - Closures correctly capture and access variables from the enclosing scope even after it exits
      hints:
        level1: Function = (params, body, closure_env). Call = new env with args bound.
        level2: Closure captures defining environment, not calling environment.
        level3: "class LoxFunction:\n    def __init__(self, declaration, closure):\n        self.declaration = declaration\n\
          \        self.closure = closure  # Environment when defined\n    \n    def call(self, interpreter, arguments):\n\
          \        # Create new environment for this call\n        environment = Environment(parent=self.closure)\n      \
          \  \n        # Bind parameters to arguments\n        for i, param in enumerate(self.declaration.params):\n     \
          \       environment.define(param, arguments[i])\n        \n        try:\n            interpreter.execute_block(self.declaration.body,\
          \ environment)\n        except ReturnException as ret:\n            return ret.value\n        \n        return None\n\
          \    \n    def arity(self):\n        return len(self.declaration.params)\n\nclass Interpreter:\n    def eval_FunctionDecl(self,\
          \ node):\n        function = LoxFunction(node, self.environment)\n        self.environment.define(node.name, function)\n\
          \    \n    def eval_Call(self, node):\n        callee = self.evaluate(node.callee)\n        arguments = [self.evaluate(arg)\
          \ for arg in node.arguments]\n        \n        if not hasattr(callee, 'call'):\n            raise RuntimeError('Can\
          \ only call functions')\n        \n        if len(arguments) != callee.arity():\n            raise RuntimeError(f'Expected\
          \ {callee.arity()} arguments but got {len(arguments)}')\n        \n        return callee.call(self, arguments)\n\
          \    \n    def eval_Return(self, node):\n        value = None\n        if node.value:\n            value = self.evaluate(node.value)\n\
          \        raise ReturnException(value)"
      pitfalls:
      - Closure capture
      - Return from nested function
      - Argument count
      concepts:
      - Functions
      - Closures
      - Call stack
      estimated_hours: 4-5
      deliverables:
      - Function declaration that stores the function name, parameters, and body in the environment
      - Function call execution that binds arguments to parameters and evaluates the body in a new scope
      - Closure capture that preserves the enclosing environment at the time of function definition
      - Recursion support allowing a function to call itself by name within its own body
  blog-platform:
    id: blog-platform
    name: Blog Platform
    description: Build a full-featured blog platform with authentication, markdown support, and CRUD operations. Learn full-stack
      web development fundamentals.
    difficulty: intermediate
    estimated_hours: 25-35
    prerequisites:
    - HTML/CSS/JavaScript
    - Basic backend knowledge (Node.js or Python)
    - Database basics (SQL or MongoDB)
    languages:
      recommended:
      - JavaScript
      - Python
      - TypeScript
      also_possible:
      - Go
      - Ruby
      - PHP
    resources:
    - name: Build a Blog with Next.js
      url: https://nextjs.org/learn/basics/create-nextjs-app
      type: tutorial
    - name: Flask Mega-Tutorial
      url: https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world
      type: tutorial
    milestones:
    - id: 1
      name: Project Setup & Database Schema
      description: Set up the project structure and design the database.
      acceptance_criteria:
      - Project is initialized with the chosen web framework and all dependencies are installable
      - Database connection is established and verified with a health check query on startup
      - User table includes columns for id, email, password_hash, name, and created_at with unique email constraint
      - Post table includes columns for id, title, content, author_id foreign key, created_at, and updated_at
      - Migration system supports running pending migrations forward and rolling back the most recent migration
      hints:
        level1: Start with SQLite for development, switch to PostgreSQL for production.
        level2: Use an ORM (Prisma, SQLAlchemy, TypeORM) for database operations.
        level3: |-
          // Prisma schema example
          model User {
            id       Int    @id @default(autoincrement())
            email    String @unique
            name     String
            password String
            posts    Post[]
          }
          model Post {
            id        Int      @id @default(autoincrement())
            title     String
            content   String
            author    User     @relation(fields: [authorId], references: [id])
            authorId  Int
            createdAt DateTime @default(now())
          }
      pitfalls:
      - Not indexing frequently queried columns
      - Storing passwords in plain text
      - Not using foreign key constraints
      concepts:
      - Database design
      - ORM usage
      - Migrations
      estimated_hours: 3-4
      deliverables:
      - Database schema for users, posts, and comments with foreign key relationships and indexes
      - Migration files for schema versioning that support forward and rollback migrations
      - Database connection pooling setup with configurable pool size and connection timeout
      - Basic project structure with route definitions, middleware setup, and configuration files
    - id: 2
      name: User Authentication
      description: Implement user registration and login with secure password handling.
      acceptance_criteria:
      - Registration accepts email and password, validates format, and creates a new user if email is unique
      - Password is hashed with bcrypt using a cost factor of at least 10 before storing in the database
      - Login verifies credentials and returns a signed JWT or session cookie with configurable expiration
      - Protected routes return 401 Unauthorized when the request lacks a valid authentication token
      - Logout invalidates the session or token so that subsequent requests with that credential are rejected
      hints:
        level1: Use bcrypt with cost factor 10-12 for password hashing.
        level2: 'JWT: sign with secret, include user id and expiry.'
        level3: |-
          app.post('/login', async (req, res) => {
            const user = await User.findByEmail(req.body.email);
            if (!user || !await bcrypt.compare(req.body.password, user.password)) {
              return res.status(401).json({ error: 'Invalid credentials' });
            }
            const token = jwt.sign({ userId: user.id }, SECRET, { expiresIn: '7d' });
            res.json({ token });
          });
      pitfalls:
      - Storing JWT in localStorage (XSS vulnerable)
      - Not validating email format
      - Exposing whether email exists on failed login
      concepts:
      - Password hashing
      - JWT authentication
      - HTTP-only cookies
      - CSRF protection
      estimated_hours: 5-6
      deliverables:
      - User registration endpoint that validates input, hashes the password, and creates the user record
      - Login endpoint that verifies credentials and returns a JWT or session token for authenticated access
      - Password reset functionality with time-limited token sent via email for secure credential recovery
      - Protected route middleware that validates the auth token and rejects unauthenticated requests with 401
    - id: 3
      name: Blog CRUD Operations
      description: Implement create, read, update, delete for blog posts.
      acceptance_criteria:
      - Create post stores a new post with title, content, and the authenticated user as author
      - List posts endpoint returns paginated results with configurable page size and total count
      - Single post endpoint returns the full post content including author name and comment count
      - Edit endpoint returns 403 Forbidden when a user attempts to modify another user's post
      - Delete endpoint returns 403 Forbidden when a user attempts to delete another user's post
      - Markdown content is rendered to HTML for display while preserving the raw markdown in storage
      hints:
        level1: Use marked or markdown-it library for markdown rendering.
        level2: 'Pagination: OFFSET/LIMIT or cursor-based for large datasets.'
        level3: |-
          app.get('/posts', async (req, res) => {
            const page = parseInt(req.query.page) || 1;
            const limit = 10;
            const offset = (page - 1) * limit;
            const [posts, total] = await Promise.all([
              Post.findMany({ skip: offset, take: limit, orderBy: { createdAt: 'desc' } }),
              Post.count()
            ]);
            res.json({ posts, pagination: { page, limit, total, pages: Math.ceil(total / limit) } });
          });
      pitfalls:
      - XSS from rendering user markdown (sanitize HTML)
      - N+1 queries when loading posts with authors
      - Not checking ownership on edit/delete
      concepts:
      - CRUD operations
      - Authorization
      - Markdown rendering
      - Pagination
      estimated_hours: 6-8
      deliverables:
      - Create post endpoint accepting title, content, and tags from the authenticated author
      - Read posts endpoint with cursor-based pagination, sorting, and optional tag filtering
      - Update post endpoint that allows only the original author to modify title and content
      - Delete post endpoint with soft-delete option that marks posts as deleted without removing data
    - id: 4
      name: Frontend UI
      description: Build the frontend interface for the blog.
      acceptance_criteria:
      - Homepage displays a paginated list of posts sorted by newest first with title and excerpt
      - Post detail page renders the full markdown content as HTML with author name and publish date
      - Login and registration forms validate input client-side and display server error messages
      - Post editor provides a split-pane with markdown input on the left and live preview on the right
      - Responsive layout adjusts to mobile screen widths without horizontal scrolling or overlapping elements
      hints:
        level1: Use a component library or build minimal components.
        level2: Split editor and preview into side-by-side panels.
        level3: |-
          function PostEditor({ value, onChange }) {
            return (
              <div className="editor-container">
                <textarea value={value} onChange={(e) => onChange(e.target.value)} className="editor" />
                <div className="preview" dangerouslySetInnerHTML={{ __html: marked(value) }} />
              </div>
            );
          }
      pitfalls:
      - Not handling loading states
      - No error boundaries
      - SEO issues with client-side rendering
      concepts:
      - Component architecture
      - Form handling
      - State management
      estimated_hours: 6-8
      deliverables:
      - Post listing page with pagination controls, post previews, and author attribution
      - Single post view page showing full content, author info, and threaded comments section
      - Post editor with a markdown input area and live HTML preview panel for authoring
      - Responsive design using CSS grid or flexbox that adapts layout for desktop and mobile viewports
  btree-impl:
    id: btree-impl
    name: B-tree Implementation
    description: Implement a B-tree data structure. Learn disk-friendly tree structures used in databases and file systems.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Trees basics
    - Algorithm complexity
    languages:
      recommended:
      - C
      - Python
      - Rust
      also_possible:
      - Go
      - Java
    resources:
    - name: B-tree Wikipedia
      url: https://en.wikipedia.org/wiki/B-tree
      type: article
    - name: B-tree Visualization
      url: https://www.cs.usfca.edu/~galles/visualization/BTree.html
      type: interactive
    milestones:
    - id: 1
      name: B-tree Node Structure
      description: Define B-tree node with keys and children.
      acceptance_criteria:
      - Each node holds at most 2t-1 keys and at least t-1 keys (except the root which may have fewer)
      - Each internal node has exactly one more child pointer than it has keys (up to 2t children)
      - Keys within every node are maintained in sorted ascending order after every operation
      - Leaf indicator flag is set correctly and differentiates leaf nodes from internal nodes during traversal
      hints:
        level1: t = minimum degree. Node has t-1 to 2t-1 keys (except root).
        level2: Children array has one more element than keys array.
        level3: "class BTreeNode:\n    def __init__(self, t, is_leaf=True):\n        self.t = t  # Minimum degree\n      \
          \  self.keys = []  # List of keys\n        self.children = []  # List of child nodes\n        self.is_leaf = is_leaf\n\
          \    \n    @property\n    def is_full(self):\n        return len(self.keys) == 2 * self.t - 1\n    \n    @property\n\
          \    def is_underflow(self):\n        return len(self.keys) < self.t - 1\n\nclass BTree:\n    def __init__(self,\
          \ t):\n        self.t = t  # Minimum degree\n        self.root = BTreeNode(t)"
      pitfalls:
      - Off-by-one in capacity
      - Not tracking leaf status
      - Children count mismatch
      concepts:
      - B-tree properties
      - Node capacity
      - Tree structure
      estimated_hours: 2-3
      deliverables:
      - Node class with sorted keys array and children pointer array for internal and leaf nodes
      - Configurable minimum degree t that determines the maximum and minimum key capacity per node
      - Leaf versus internal node distinction flag used to select insertion and traversal behavior
      - Disk page representation mapping each node to a fixed-size page for persistent storage
    - id: 2
      name: Search
      description: Implement search operation.
      acceptance_criteria:
      - Binary search within a node identifies the correct key position or child pointer in O(log t) time
      - Recursive traversal descends to the appropriate child node when the key is not found at the current level
      - Search returns the matching key and its associated value when the key exists in the tree
      - Search returns a not-found indicator without error when the key does not exist in the tree
      hints:
        level1: Find position where key would be, then check or descend.
        level2: Use bisect for binary search in Python.
        level3: "import bisect\n\ndef search(self, key, node=None):\n    if node is None:\n        node = self.root\n    \n\
          \    # Find position where key would be\n    i = bisect.bisect_left(node.keys, key)\n    \n    # Check if found\n\
          \    if i < len(node.keys) and node.keys[i] == key:\n        return (node, i)\n    \n    # If leaf, key not in tree\n\
          \    if node.is_leaf:\n        return None\n    \n    # Recurse to appropriate child\n    return self.search(key,\
          \ node.children[i])"
      pitfalls:
      - Index bounds
      - Leaf check
      - Key comparison
      concepts:
      - Binary search
      - Tree traversal
      - Key lookup
      estimated_hours: 2-3
      deliverables:
      - Binary search within a node's key array to locate the target key or correct child index
      - Recursive descent from the root through internal nodes to the correct child for the search key
      - Search result returning the found value and a boolean indicating whether the key was present
      - O(log n) time complexity with at most O(log_t n) node accesses for a tree of n keys
    - id: 3
      name: Insert with Split
      description: Implement insertion with node splitting.
      acceptance_criteria:
      - Insertion into a non-full leaf places the key in sorted position without any structural changes
      - Full nodes are split proactively during the downward traversal, before the actual insertion
      - Splitting the root creates a new root with one key and two children, increasing tree height by one
      - Sorted order invariant is maintained across all nodes after every insertion operation
      hints:
        level1: Split before descending if child is full (proactive split).
        level2: 'Split: median goes to parent, left/right halves become children.'
        level3: "def insert(self, key):\n    root = self.root\n    if root.is_full:\n        # Create new root\n        new_root\
          \ = BTreeNode(self.t, is_leaf=False)\n        new_root.children.append(self.root)\n        self._split_child(new_root,\
          \ 0)\n        self.root = new_root\n    self._insert_non_full(self.root, key)\n\ndef _split_child(self, parent,\
          \ i):\n    t = self.t\n    child = parent.children[i]\n    new_node = BTreeNode(t, child.is_leaf)\n    \n    # Middle\
          \ key goes to parent\n    mid = t - 1\n    parent.keys.insert(i, child.keys[mid])\n    \n    # Right half goes to\
          \ new node\n    new_node.keys = child.keys[mid+1:]\n    child.keys = child.keys[:mid]\n    \n    if not child.is_leaf:\n\
          \        new_node.children = child.children[t:]\n        child.children = child.children[:t]\n    \n    parent.children.insert(i\
          \ + 1, new_node)\n\ndef _insert_non_full(self, node, key):\n    i = bisect.bisect_left(node.keys, key)\n    if node.is_leaf:\n\
          \        node.keys.insert(i, key)\n    else:\n        if node.children[i].is_full:\n            self._split_child(node,\
          \ i)\n            if key > node.keys[i]:\n                i += 1\n        self._insert_non_full(node.children[i],\
          \ key)"
      pitfalls:
      - Split at wrong position
      - Not updating root
      - Child index after split
      concepts:
      - Node splitting
      - Proactive split
      - Tree growth
      estimated_hours: 4-6
      deliverables:
      - Leaf location finder that traverses the tree to identify the correct leaf for the new key
      - Proactive node splitting during descent that splits full nodes before inserting into them
      - Median key promotion that moves the middle key of a split node up into the parent node
      - Root split handler that creates a new root when the current root is full and needs splitting
    - id: 4
      name: Delete with Rebalancing
      description: Implement deletion with borrowing and merging.
      acceptance_criteria:
      - Deleting a key from a leaf node with more than t-1 keys succeeds without restructuring
      - Deleting from an internal node replaces the key with its in-order predecessor or successor and recurses
      - Borrowing rotates a key from a sibling through the parent to restore the minimum occupancy invariant
      - Merging combines two siblings and their parent separator key into one node when borrowing is not possible
      hints:
        level1: 'Internal delete: replace with predecessor/successor, delete from leaf.'
        level2: Before descending, ensure child has >= t keys.
        level3: "def delete(self, key):\n    self._delete(self.root, key)\n    # Shrink tree if root is empty\n    if len(self.root.keys)\
          \ == 0 and not self.root.is_leaf:\n        self.root = self.root.children[0]\n\ndef _delete(self, node, key):\n\
          \    i = bisect.bisect_left(node.keys, key)\n    \n    if i < len(node.keys) and node.keys[i] == key:\n        if\
          \ node.is_leaf:\n            node.keys.pop(i)\n        else:\n            self._delete_internal(node, i)\n    elif\
          \ not node.is_leaf:\n        self._ensure_child_not_minimal(node, i)\n        # Recompute i after potential restructuring\n\
          \        i = bisect.bisect_left(node.keys, key)\n        if i > len(node.keys):\n            i = len(node.keys)\n\
          \        self._delete(node.children[i], key)\n\ndef _ensure_child_not_minimal(self, parent, i):\n    child = parent.children[i]\n\
          \    if len(child.keys) >= self.t:\n        return\n    # Try borrowing from siblings or merge\n    # ... (borrow\
          \ from left, borrow from right, or merge)"
      pitfalls:
      - Merge vs borrow decision
      - Predecessor vs successor choice
      - Root shrinking
      concepts:
      - Node merging
      - Sibling borrowing
      - Tree shrinking
      estimated_hours: 5-8
      deliverables:
      - Delete from leaf node by removing the key and checking for minimum occupancy underflow
      - Delete from internal node by swapping with the in-order predecessor or successor, then deleting from leaf
      - Borrow from sibling operation that transfers a key through the parent to fix an underflowing node
      - Merge with sibling operation that combines two underflowing nodes and their shared parent key
  build-allocator:
    id: build-allocator
    name: Build Your Own Memory Allocator
    description: Implement malloc/free with various allocation strategies.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - C programming
    - Pointers and memory
    - Data structures
    - System calls (sbrk, mmap)
    languages:
      recommended:
      - C
      - Rust
      - Zig
      also_possible: []
    resources:
    - type: article
      name: Implementing malloc
      url: https://moss.cs.iit.edu/cs351/slides/slides-malloc.pdf
    - type: book
      name: The C Programming Language
      url: https://en.wikipedia.org/wiki/The_C_Programming_Language
    - type: article
      name: Writing a Memory Allocator
      url: http://dmitrysoshnikov.com/compilers/writing-a-memory-allocator/
    milestones:
    - id: 1
      name: Basic Allocator with sbrk
      description: Implement simple bump allocator using sbrk.
      acceptance_criteria:
      - Allocator requests memory from the OS via sbrk and returns a usable pointer to the caller
      - Block headers track allocation size and status so free can identify block boundaries
      - Basic malloc returns correctly aligned memory (at least 8-byte aligned on 64-bit systems)
      - Free marks a block as available and subsequent malloc calls can reuse that freed memory
      hints:
        level1: Use sbrk(0) to get current break, sbrk(size) to extend heap.
        level2: Store metadata (size, free flag) before each block.
        level3: "## Basic Allocator\n\n```c\n#include <unistd.h>\n#include <stdint.h>\n\n// Block header\ntypedef struct block_header\
          \ {\n    size_t size;          // Size of block (not including header)\n    int is_free;          // 1 if free,\
          \ 0 if allocated\n    struct block_header *next;  // Next block in list\n} block_header_t;\n\n#define HEADER_SIZE\
          \ sizeof(block_header_t)\n#define ALIGN(size) (((size) + 7) & ~7)  // 8-byte alignment\n\nstatic block_header_t\
          \ *head = NULL;\nstatic block_header_t *tail = NULL;\n\n// Request memory from OS\nstatic block_header_t *request_space(size_t\
          \ size) {\n    block_header_t *block = sbrk(0);\n    void *request = sbrk(HEADER_SIZE + size);\n    \n    if (request\
          \ == (void*)-1) {\n        return NULL;  // sbrk failed\n    }\n    \n    block->size = size;\n    block->is_free\
          \ = 0;\n    block->next = NULL;\n    \n    if (tail) {\n        tail->next = block;\n    }\n    tail = block;\n\
          \    \n    if (!head) {\n        head = block;\n    }\n    \n    return block;\n}\n\nvoid *my_malloc(size_t size)\
          \ {\n    if (size == 0) return NULL;\n    \n    size = ALIGN(size);\n    \n    // First fit: find first free block\
          \ that fits\n    block_header_t *current = head;\n    while (current) {\n        if (current->is_free && current->size\
          \ >= size) {\n            current->is_free = 0;\n            return (void*)(current + 1);  // Return pointer after\
          \ header\n        }\n        current = current->next;\n    }\n    \n    // No free block found, request more space\n\
          \    block_header_t *block = request_space(size);\n    if (!block) return NULL;\n    \n    return (void*)(block\
          \ + 1);\n}\n\nvoid my_free(void *ptr) {\n    if (!ptr) return;\n    \n    block_header_t *block = (block_header_t*)ptr\
          \ - 1;\n    block->is_free = 1;\n}\n```"
      pitfalls:
      - Forgetting alignment
      - Memory leaks in metadata
      - Not handling sbrk failure
      concepts:
      - Heap management
      - Memory alignment
      - System calls
      estimated_hours: 8-12
      deliverables:
      - sbrk() system call wrapper that extends the heap by the requested number of bytes
      - Block header structure containing the allocation size, in-use flag, and alignment padding
      - First-fit allocation strategy that scans the free list and returns the first sufficiently large block
      - Simple free() implementation that marks a block as unused and returns it to the free list
    - id: 2
      name: Free List Management
      description: Implement efficient free block tracking.
      acceptance_criteria:
      - Explicit free list links free blocks together for O(free-blocks) allocation scanning
      - Block splitting divides a large free block into an allocated portion and a smaller free remainder
      - Block coalescing merges physically adjacent free blocks to reduce external fragmentation
      - First-fit, best-fit, and worst-fit strategies are all implemented and selectable at compile time
      hints:
        level1: Maintain a linked list of only free blocks for faster allocation.
        level2: When freeing, merge adjacent free blocks to reduce fragmentation.
        level3: "## Free List with Coalescing\n\n```c\n// Split block if it's too large\nstatic void split_block(block_header_t\
          \ *block, size_t size) {\n    if (block->size >= size + HEADER_SIZE + 16) {  // Only split if remainder is useful\n\
          \        block_header_t *new_block = (block_header_t*)((char*)(block + 1) + size);\n        new_block->size = block->size\
          \ - size - HEADER_SIZE;\n        new_block->is_free = 1;\n        new_block->next = block->next;\n        \n   \
          \     block->size = size;\n        block->next = new_block;\n    }\n}\n\n// Coalesce adjacent free blocks\nstatic\
          \ void coalesce() {\n    block_header_t *current = head;\n    while (current && current->next) {\n        if (current->is_free\
          \ && current->next->is_free) {\n            // Merge with next block\n            current->size += HEADER_SIZE +\
          \ current->next->size;\n            current->next = current->next->next;\n            // Don't advance - check if\
          \ we can merge more\n        } else {\n            current = current->next;\n        }\n    }\n}\n\n// Best-fit\
          \ allocation\nvoid *my_malloc_bestfit(size_t size) {\n    size = ALIGN(size);\n    \n    block_header_t *best =\
          \ NULL;\n    block_header_t *current = head;\n    \n    while (current) {\n        if (current->is_free && current->size\
          \ >= size) {\n            if (!best || current->size < best->size) {\n                best = current;\n        \
          \        if (best->size == size) break;  // Perfect fit\n            }\n        }\n        current = current->next;\n\
          \    }\n    \n    if (best) {\n        split_block(best, size);\n        best->is_free = 0;\n        return (void*)(best\
          \ + 1);\n    }\n    \n    // Request new space...\n    return NULL;\n}\n\nvoid my_free_coalesce(void *ptr) {\n \
          \   if (!ptr) return;\n    \n    block_header_t *block = (block_header_t*)ptr - 1;\n    block->is_free = 1;\n  \
          \  \n    coalesce();\n}\n```"
      pitfalls:
      - Fragmentation from not coalescing
      - Splitting blocks too small
      - Corrupting free list pointers
      concepts:
      - Free lists
      - Fragmentation
      - Allocation strategies
      estimated_hours: 10-15
      deliverables:
      - Explicit free list with next and previous pointers embedded in free block payloads
      - Coalescing logic that merges adjacent free blocks into a single larger free block
      - Best-fit and next-fit allocation strategies selectable as alternatives to first-fit
      - Boundary tags at both ends of each block enabling efficient bidirectional coalescing
    - id: 3
      name: Segregated Free Lists
      description: Implement size-class based allocation for better performance.
      acceptance_criteria:
      - Multiple segregated free lists are maintained, one per size class, for fast allocation lookup
      - Common allocation sizes (16, 32, 64, 128 bytes) are served from their dedicated free list in O(1)
      - External fragmentation is reduced compared to a single free list by grouping similar-sized blocks
      - Small allocations (under 64 bytes) are served efficiently without scanning large-block free lists
      hints:
        level1: Maintain separate free lists for different size ranges (16, 32, 64, 128... bytes).
        level2: Round up allocation size to next size class. Search only appropriate list.
        level3: "## Segregated Free Lists\n\n```c\n#define NUM_SIZE_CLASSES 10\n// Size classes: 16, 32, 64, 128, 256, 512,\
          \ 1024, 2048, 4096, larger\n\nstatic block_header_t *free_lists[NUM_SIZE_CLASSES] = {NULL};\n\nstatic int get_size_class(size_t\
          \ size) {\n    if (size <= 16) return 0;\n    if (size <= 32) return 1;\n    if (size <= 64) return 2;\n    if (size\
          \ <= 128) return 3;\n    if (size <= 256) return 4;\n    if (size <= 512) return 5;\n    if (size <= 1024) return\
          \ 6;\n    if (size <= 2048) return 7;\n    if (size <= 4096) return 8;\n    return 9;  // Larger allocations\n}\n\
          \nstatic size_t get_class_size(int class) {\n    static size_t sizes[] = {16, 32, 64, 128, 256, 512, 1024, 2048,\
          \ 4096, 0};\n    return sizes[class];\n}\n\n// Remove block from its free list\nstatic void remove_from_freelist(block_header_t\
          \ *block) {\n    int class = get_size_class(block->size);\n    \n    if (free_lists[class] == block) {\n       \
          \ free_lists[class] = block->next;\n    } else {\n        block_header_t *prev = free_lists[class];\n        while\
          \ (prev && prev->next != block) {\n            prev = prev->next;\n        }\n        if (prev) {\n            prev->next\
          \ = block->next;\n        }\n    }\n}\n\n// Add block to appropriate free list\nstatic void add_to_freelist(block_header_t\
          \ *block) {\n    int class = get_size_class(block->size);\n    block->next = free_lists[class];\n    free_lists[class]\
          \ = block;\n}\n\nvoid *segregated_malloc(size_t size) {\n    size = ALIGN(size);\n    int class = get_size_class(size);\n\
          \    \n    // Search in this class and larger\n    for (int i = class; i < NUM_SIZE_CLASSES; i++) {\n        block_header_t\
          \ *block = free_lists[i];\n        while (block) {\n            if (block->size >= size) {\n                remove_from_freelist(block);\n\
          \                block->is_free = 0;\n                // Could split if much larger\n                return (void*)(block\
          \ + 1);\n            }\n            block = block->next;\n        }\n    }\n    \n    // Request new space\n   \
          \ size_t alloc_size = (class < 9) ? get_class_size(class) : size;\n    return request_and_return(alloc_size);\n\
          }\n```"
      pitfalls:
      - Internal fragmentation from size classes
      - Complex bookkeeping
      - Cache unfriendly access patterns
      concepts:
      - Size classes
      - Segregated storage
      - Performance optimization
      estimated_hours: 10-15
      deliverables:
      - Size class definitions with power-of-two boundaries (e.g., 16, 32, 64, 128, 256 bytes)
      - Separate free list maintained for each size class for constant-time lookup of fitting blocks
      - Fast allocation from the matching size class that returns a block without scanning other lists
      - Block splitting when a larger class block is used to satisfy a smaller class request
    - id: 4
      name: Thread Safety & mmap
      description: Add thread safety and large allocation support.
      acceptance_criteria:
      - Thread-safe allocation and free operations work correctly under concurrent access from multiple threads
      - Large allocations above a configurable threshold (e.g., 128KB) use mmap instead of sbrk
      - Per-thread caches reduce cross-thread contention and improve allocation throughput in multithreaded programs
      - Memory debugging support detects double-free, use-after-free, and buffer overflow when enabled
      hints:
        level1: Use mutex to protect allocator state. For large allocs, use mmap directly.
        level2: mmap allocations can be munmapped directly, avoiding fragmentation.
        level3: "## Thread-Safe Allocator\n\n```c\n#include <pthread.h>\n#include <sys/mman.h>\n\n#define MMAP_THRESHOLD 128\
          \ * 1024  // Use mmap for > 128KB\n\nstatic pthread_mutex_t alloc_mutex = PTHREAD_MUTEX_INITIALIZER;\n\nvoid *thread_safe_malloc(size_t\
          \ size) {\n    if (size == 0) return NULL;\n    \n    // Large allocations: use mmap directly\n    if (size >= MMAP_THRESHOLD)\
          \ {\n        size_t total = size + HEADER_SIZE;\n        void *ptr = mmap(NULL, total, PROT_READ | PROT_WRITE,\n\
          \                        MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n        if (ptr == MAP_FAILED) return NULL;\n   \
          \     \n        block_header_t *block = ptr;\n        block->size = size;\n        block->is_free = 0;\n       \
          \ block->next = NULL;  // Mark as mmap'd (could use flag)\n        \n        return (void*)(block + 1);\n    }\n\
          \    \n    // Small allocations: use segregated lists with mutex\n    pthread_mutex_lock(&alloc_mutex);\n    void\
          \ *result = segregated_malloc(size);\n    pthread_mutex_unlock(&alloc_mutex);\n    \n    return result;\n}\n\nvoid\
          \ thread_safe_free(void *ptr) {\n    if (!ptr) return;\n    \n    block_header_t *block = (block_header_t*)ptr -\
          \ 1;\n    \n    // Check if this was mmap'd (large allocation)\n    if (block->size >= MMAP_THRESHOLD) {\n     \
          \   munmap(block, block->size + HEADER_SIZE);\n        return;\n    }\n    \n    pthread_mutex_lock(&alloc_mutex);\n\
          \    block->is_free = 1;\n    add_to_freelist(block);\n    pthread_mutex_unlock(&alloc_mutex);\n}\n\n// Debug: check\
          \ for memory leaks\nvoid debug_print_stats() {\n    pthread_mutex_lock(&alloc_mutex);\n    \n    size_t total_allocated\
          \ = 0;\n    size_t total_free = 0;\n    int num_blocks = 0;\n    \n    block_header_t *current = head;\n    while\
          \ (current) {\n        num_blocks++;\n        if (current->is_free) {\n            total_free += current->size;\n\
          \        } else {\n            total_allocated += current->size;\n        }\n        current = current->next;\n\
          \    }\n    \n    printf(\"Blocks: %d, Allocated: %zu, Free: %zu\\n\",\n           num_blocks, total_allocated,\
          \ total_free);\n    \n    pthread_mutex_unlock(&alloc_mutex);\n}\n```"
      pitfalls:
      - Lock contention
      - Deadlocks
      - Memory leaks from lost blocks
      concepts:
      - Thread safety
      - Virtual memory
      - Memory debugging
      estimated_hours: 12-18
      deliverables:
      - Per-thread arenas that give each thread its own heap region to reduce lock contention
      - Lock-free fast path for small allocations using thread-local free lists without mutex acquisition
      - mmap() fallback for large allocations that maps memory directly from the OS instead of using sbrk
      - Thread-local caching that buffers recently freed blocks for fast same-thread reallocation
  build-bittorrent:
    id: build-bittorrent
    name: Build Your Own BitTorrent
    description: Build a BitTorrent client with P2P file sharing.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Networking (TCP/UDP)
    - Concurrency
    - File I/O
    - Bencode format
    languages:
      recommended:
      - Go
      - Rust
      - Python
      also_possible:
      - JavaScript
      - Java
    resources:
    - type: specification
      name: BitTorrent Protocol
      url: https://www.bittorrent.org/beps/bep_0003.html
    - type: tutorial
      name: Building a BitTorrent Client
      url: https://blog.jse.li/posts/torrent/
    - type: interactive
      name: CodeCrafters BitTorrent
      url: https://app.codecrafters.io/courses/bittorrent/overview
    milestones:
    - id: 1
      name: Torrent File Parsing
      description: Parse .torrent files and extract metadata.
      acceptance_criteria:
      - Bencode decoder correctly parses strings, integers, lists, and nested dictionaries from raw bytes
      - Announce URL is extracted from the torrent metainfo and is a valid HTTP or UDP tracker URL
      - File info including name, length, and piece length is correctly extracted from the info dictionary
      - Info hash is a 20-byte SHA1 digest computed from the exact bencoded bytes of the info dictionary
      hints:
        level1: 'Bencode: i42e=int, 4:spam=string, l...e=list, d...e=dict'
        level2: Info hash = SHA1 of bencoded info dict. Used to identify torrent.
        level3: "## Torrent Parsing\n\n```python\nimport hashlib\n\ndef decode_bencode(data: bytes, pos=0):\n    if data[pos:pos+1]\
          \ == b'i':  # Integer\n        end = data.index(b'e', pos)\n        return int(data[pos+1:end]), end + 1\n    \n\
          \    elif data[pos:pos+1] == b'l':  # List\n        result = []\n        pos += 1\n        while data[pos:pos+1]\
          \ != b'e':\n            item, pos = decode_bencode(data, pos)\n            result.append(item)\n        return result,\
          \ pos + 1\n    \n    elif data[pos:pos+1] == b'd':  # Dict\n        result = {}\n        pos += 1\n        while\
          \ data[pos:pos+1] != b'e':\n            key, pos = decode_bencode(data, pos)\n            value, pos = decode_bencode(data,\
          \ pos)\n            result[key] = value\n        return result, pos + 1\n    \n    else:  # String (length:content)\n\
          \        colon = data.index(b':', pos)\n        length = int(data[pos:colon])\n        start = colon + 1\n     \
          \   return data[start:start+length], start + length\n\nclass Torrent:\n    def __init__(self, filepath):\n     \
          \   with open(filepath, 'rb') as f:\n            data = f.read()\n        \n        self.meta, _ = decode_bencode(data)\n\
          \        self.announce = self.meta[b'announce'].decode()\n        \n        info = self.meta[b'info']\n        self.name\
          \ = info[b'name'].decode()\n        self.piece_length = info[b'piece length']\n        self.pieces = info[b'pieces']\
          \  # Concatenated SHA1 hashes\n        \n        if b'files' in info:  # Multi-file torrent\n            self.files\
          \ = [\n                {'path': '/'.join(p.decode() for p in f[b'path']),\n                 'length': f[b'length']}\n\
          \                for f in info[b'files']\n            ]\n            self.total_length = sum(f['length'] for f in\
          \ self.files)\n        else:  # Single file\n            self.files = [{'path': self.name, 'length': info[b'length']}]\n\
          \            self.total_length = info[b'length']\n        \n        # Calculate info hash\n        info_start =\
          \ data.index(b'4:info') + 6\n        info_end = len(data) - 1  # Before final 'e'\n        self.info_hash = hashlib.sha1(data[info_start:info_end]).digest()\n\
          \    \n    @property\n    def num_pieces(self):\n        return len(self.pieces) // 20\n    \n    def piece_hash(self,\
          \ index):\n        return self.pieces[index*20:(index+1)*20]\n```"
      pitfalls:
      - Bencode parsing edge cases
      - Wrong info dict boundaries
      - Binary vs text strings
      concepts:
      - Bencode format
      - Hashing
      - Torrent metadata
      estimated_hours: 8-12
      deliverables:
      - 'Bencode decoder supporting all four types: byte strings, integers, lists, and dictionaries'
      - Metainfo extraction that reads the announce URL and info dictionary from the decoded torrent file
      - Info hash calculation by computing the SHA1 digest of the bencoded info dictionary bytes
      - Piece length and piece hashes extraction from the info dictionary for download verification
    - id: 2
      name: Tracker Communication
      description: Communicate with tracker to get peer list.
      acceptance_criteria:
      - HTTP tracker announce request includes all required parameters (info_hash, peer_id, port, uploaded, downloaded, left)
      - Announce request is correctly URL-encoded and sent as an HTTP GET to the tracker
      - Compact peer list is parsed into a list of (IP address, port) pairs for peer connection
      - Client re-announces to the tracker at the interval specified in the tracker response
      hints:
        level1: GET request with info_hash, peer_id, port, uploaded, downloaded, left.
        level2: Response is bencoded dict with 'peers' (compact or dict format).
        level3: "## Tracker Client\n\n```python\nimport requests\nimport struct\nimport random\nimport string\n\nclass TrackerClient:\n\
          \    def __init__(self, torrent):\n        self.torrent = torrent\n        self.peer_id = self.generate_peer_id()\n\
          \        self.port = 6881\n        self.uploaded = 0\n        self.downloaded = 0\n    \n    def generate_peer_id(self):\n\
          \        # Format: -XX0000-xxxxxxxxxxxx\n        return f\"-PY0001-{''.join(random.choices(string.digits, k=12))}\"\
          .encode()\n    \n    def announce(self, event=None):\n        params = {\n            'info_hash': self.torrent.info_hash,\n\
          \            'peer_id': self.peer_id,\n            'port': self.port,\n            'uploaded': self.uploaded,\n\
          \            'downloaded': self.downloaded,\n            'left': self.torrent.total_length - self.downloaded,\n\
          \            'compact': 1,\n        }\n        if event:\n            params['event'] = event\n        \n      \
          \  response = requests.get(self.torrent.announce, params=params)\n        return self.parse_response(response.content)\n\
          \    \n    def parse_response(self, data):\n        response, _ = decode_bencode(data)\n        \n        if b'failure\
          \ reason' in response:\n            raise Exception(response[b'failure reason'].decode())\n        \n        interval\
          \ = response.get(b'interval', 1800)\n        peers = self.parse_peers(response[b'peers'])\n        \n        return\
          \ {'interval': interval, 'peers': peers}\n    \n    def parse_peers(self, peers_data):\n        if isinstance(peers_data,\
          \ list):  # Dictionary format\n            return [\n                (p[b'ip'].decode(), p[b'port'])\n         \
          \       for p in peers_data\n            ]\n        else:  # Compact format: 6 bytes per peer (4 IP + 2 port)\n\
          \            peers = []\n            for i in range(0, len(peers_data), 6):\n                ip = '.'.join(str(b)\
          \ for b in peers_data[i:i+4])\n                port = struct.unpack('>H', peers_data[i+4:i+6])[0]\n            \
          \    peers.append((ip, port))\n            return peers\n```"
      pitfalls:
      - URL encoding info_hash
      - Compact vs dict peer format
      - Handling tracker errors
      concepts:
      - HTTP protocol
      - Tracker protocol
      - Peer discovery
      estimated_hours: 8-12
      deliverables:
      - HTTP GET request to the announce URL with info_hash, peer_id, port, and progress parameters
      - Compact peer list parser that decodes 6-byte (IP + port) entries from the tracker response
      - Periodic re-announce that sends updated uploaded/downloaded/left byte counts at the tracker's interval
      - Tracker response handler that parses the interval, complete, incomplete, and peers fields
    - id: 3
      name: Peer Protocol
      description: Implement BitTorrent peer wire protocol.
      acceptance_criteria:
      - Handshake sends and receives the 68-byte message with protocol identifier, info hash, and peer ID
      - Message framing correctly reads the 4-byte length prefix and 1-byte message ID for each peer message
      - Bitfield exchange correctly sends and parses the peer's available-pieces bitmap after the handshake
      - Request and piece messages correctly transfer 16KB blocks with piece index, block offset, and data
      - Choking and unchoking state transitions are handled so downloads only proceed when the peer is unchoked
      hints:
        level1: 'Handshake: pstrlen + pstr + reserved + info_hash + peer_id'
        level2: 'Messages: 4-byte length + 1-byte type + payload'
        level3: "## Peer Protocol\n\n```python\nimport struct\nimport socket\n\nclass PeerConnection:\n    CHOKE = 0\n   \
          \ UNCHOKE = 1\n    INTERESTED = 2\n    NOT_INTERESTED = 3\n    HAVE = 4\n    BITFIELD = 5\n    REQUEST = 6\n   \
          \ PIECE = 7\n    CANCEL = 8\n    \n    def __init__(self, ip, port, info_hash, peer_id):\n        self.socket =\
          \ socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.connect((ip, port))\n        self.info_hash\
          \ = info_hash\n        self.peer_id = peer_id\n        self.bitfield = None\n        self.choked = True\n      \
          \  self.interested = False\n    \n    def handshake(self):\n        pstr = b'BitTorrent protocol'\n        handshake\
          \ = bytes([len(pstr)]) + pstr + bytes(8) + self.info_hash + self.peer_id\n        self.socket.send(handshake)\n\
          \        \n        response = self.socket.recv(68)\n        if response[28:48] != self.info_hash:\n            raise\
          \ Exception(\"Info hash mismatch\")\n        \n        return response[48:68]  # Peer's peer_id\n    \n    def recv_message(self):\n\
          \        length_bytes = self.socket.recv(4)\n        if not length_bytes:\n            return None\n        \n \
          \       length = struct.unpack('>I', length_bytes)[0]\n        if length == 0:  # Keep-alive\n            return\
          \ ('keep-alive', None)\n        \n        msg_id = self.socket.recv(1)[0]\n        payload = self.socket.recv(length\
          \ - 1) if length > 1 else b''\n        \n        return (msg_id, payload)\n    \n    def send_interested(self):\n\
          \        self.socket.send(struct.pack('>IB', 1, self.INTERESTED))\n        self.interested = True\n    \n    def\
          \ send_request(self, index, begin, length):\n        payload = struct.pack('>III', index, begin, length)\n     \
          \   self.socket.send(struct.pack('>IB', 13, self.REQUEST) + payload)\n    \n    def parse_bitfield(self, payload):\n\
          \        self.bitfield = []\n        for byte in payload:\n            for i in range(8):\n                self.bitfield.append(bool(byte\
          \ & (128 >> i)))\n    \n    def parse_piece(self, payload):\n        index = struct.unpack('>I', payload[:4])[0]\n\
          \        begin = struct.unpack('>I', payload[4:8])[0]\n        data = payload[8:]\n        return index, begin,\
          \ data\n    \n    def download_piece(self, piece_index, piece_length):\n        BLOCK_SIZE = 16384  # 16 KB\n  \
          \      data = bytearray(piece_length)\n        \n        # Send all requests\n        for offset in range(0, piece_length,\
          \ BLOCK_SIZE):\n            length = min(BLOCK_SIZE, piece_length - offset)\n            self.send_request(piece_index,\
          \ offset, length)\n        \n        # Receive all blocks\n        received = 0\n        while received < piece_length:\n\
          \            msg_id, payload = self.recv_message()\n            if msg_id == self.PIECE:\n                idx, begin,\
          \ block = self.parse_piece(payload)\n                if idx == piece_index:\n                    data[begin:begin+len(block)]\
          \ = block\n                    received += len(block)\n        \n        return bytes(data)\n```"
      pitfalls:
      - Endianness
      - Partial message reads
      - Blocking on choked peer
      concepts:
      - Wire protocol
      - Message framing
      - State machines
      estimated_hours: 15-20
      deliverables:
      - TCP connection establishment and BitTorrent handshake with protocol string and info hash verification
      - 'Message parser for all peer protocol messages: choke, unchoke, interested, have, bitfield, request, and piece'
      - Peer state machine tracking am_choking, am_interested, peer_choking, and peer_interested flags per connection
      - Request pipelining that sends multiple outstanding block requests to maximize download throughput
    - id: 4
      name: Piece Management & Seeding
      description: Manage pieces, verify hashes, and seed to other peers.
      acceptance_criteria:
      - Piece verification rejects corrupt pieces whose SHA1 hash does not match the torrent metadata and re-requests them
      - Rarest-first selection downloads pieces in order of ascending availability across connected peers
      - Completed pieces are assembled into the output file at the correct byte offsets and the full file hash matches
      - Seeding mode serves requested blocks to connected peers and reports uploaded bytes to the tracker
      - Client maintains connections to multiple peers simultaneously and downloads different pieces from each
      hints:
        level1: 'Verify each piece''s SHA1 hash. Rarest-first: download pieces others have least.'
        level2: Run multiple peer connections concurrently. Balance downloading vs uploading.
        level3: "## Piece Manager\n\n```python\nimport asyncio\nimport hashlib\nfrom collections import defaultdict\n\nclass\
          \ PieceManager:\n    def __init__(self, torrent):\n        self.torrent = torrent\n        self.have = [False] *\
          \ torrent.num_pieces\n        self.pending = set()  # Pieces being downloaded\n        self.piece_availability =\
          \ defaultdict(int)  # piece -> count of peers\n    \n    def update_availability(self, peer_bitfield):\n       \
          \ for i, has in enumerate(peer_bitfield):\n            if has:\n                self.piece_availability[i] += 1\n\
          \    \n    def select_piece(self, peer_bitfield):\n        # Rarest first\n        candidates = [\n            (self.piece_availability[i],\
          \ i)\n            for i in range(self.torrent.num_pieces)\n            if peer_bitfield[i] and not self.have[i]\
          \ and i not in self.pending\n        ]\n        if not candidates:\n            return None\n        \n        candidates.sort()\
          \  # Sort by availability\n        piece_index = candidates[0][1]\n        self.pending.add(piece_index)\n     \
          \   return piece_index\n    \n    def verify_piece(self, index, data):\n        expected_hash = self.torrent.piece_hash(index)\n\
          \        actual_hash = hashlib.sha1(data).digest()\n        return expected_hash == actual_hash\n    \n    def piece_received(self,\
          \ index, data):\n        if self.verify_piece(index, data):\n            self.have[index] = True\n            self.pending.discard(index)\n\
          \            self.write_piece(index, data)\n            return True\n        else:\n            self.pending.discard(index)\n\
          \            return False\n    \n    def write_piece(self, index, data):\n        offset = index * self.torrent.piece_length\n\
          \        # Handle multi-file torrents...\n        with open(self.torrent.name, 'r+b') as f:\n            f.seek(offset)\n\
          \            f.write(data)\n\nclass BitTorrentClient:\n    def __init__(self, torrent_path):\n        self.torrent\
          \ = Torrent(torrent_path)\n        self.tracker = TrackerClient(self.torrent)\n        self.piece_manager = PieceManager(self.torrent)\n\
          \        self.peers = []\n    \n    async def download(self):\n        # Get peers from tracker\n        response\
          \ = self.tracker.announce('started')\n        \n        # Connect to peers\n        tasks = []\n        for ip,\
          \ port in response['peers'][:50]:  # Limit connections\n            task = asyncio.create_task(self.connect_peer(ip,\
          \ port))\n            tasks.append(task)\n        \n        await asyncio.gather(*tasks, return_exceptions=True)\n\
          \    \n    async def connect_peer(self, ip, port):\n        try:\n            peer = PeerConnection(ip, port, self.torrent.info_hash,\
          \ \n                                  self.tracker.peer_id)\n            peer.handshake()\n            \n      \
          \      # Get bitfield\n            msg_id, payload = peer.recv_message()\n            if msg_id == PeerConnection.BITFIELD:\n\
          \                peer.parse_bitfield(payload)\n                self.piece_manager.update_availability(peer.bitfield)\n\
          \            \n            peer.send_interested()\n            \n            # Wait for unchoke\n            while\
          \ peer.choked:\n                msg_id, _ = peer.recv_message()\n                if msg_id == PeerConnection.UNCHOKE:\n\
          \                    peer.choked = False\n            \n            # Download pieces\n            while not all(self.piece_manager.have):\n\
          \                piece_index = self.piece_manager.select_piece(peer.bitfield)\n                if piece_index is\
          \ None:\n                    break\n                \n                piece_length = self.torrent.piece_length\n\
          \                if piece_index == self.torrent.num_pieces - 1:\n                    piece_length = self.torrent.total_length\
          \ % self.torrent.piece_length\n                \n                data = peer.download_piece(piece_index, piece_length)\n\
          \                self.piece_manager.piece_received(piece_index, data)\n                \n        except Exception\
          \ as e:\n            print(f\"Peer {ip}:{port} error: {e}\")\n```"
      pitfalls:
      - Hash verification failures
      - Race conditions in piece selection
      - Connection management
      concepts:
      - Content verification
      - Scheduling algorithms
      - Concurrent downloads
      estimated_hours: 15-20
      deliverables:
      - Piece verification that computes SHA1 of each downloaded piece and compares it to the expected hash
      - Rarest-first piece selection strategy that prioritizes downloading pieces held by the fewest peers
      - Endgame mode that requests remaining blocks from all peers to avoid stalling on the last pieces
      - Upload capability that serves piece data to requesting peers for seeding after download completes
  build-browser:
    id: build-browser
    name: Build Your Own Browser
    description: Build a simple web browser engine. Learn HTML/CSS parsing, layout, and rendering.
    difficulty: expert
    estimated_hours: 80-150
    prerequisites:
    - HTML/CSS parsing
    - Tree data structures
    - Graphics basics
    languages:
      recommended:
      - Rust
      - C++
      - Go
      also_possible:
      - Python
      - TypeScript
    resources:
    - type: book
      name: Web Browser Engineering
      url: https://browser.engineering/
    - type: tutorial
      name: Let's build a browser engine
      url: https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html
    milestones:
    - id: 1
      name: HTML Parser
      description: Parse HTML into a DOM tree.
      acceptance_criteria:
      - Tokenizer correctly identifies start tags, end tags, attributes, self-closing tags, and text content
      - DOM tree is built with correct parent-child relationships reflecting the HTML nesting structure
      - Nested elements produce the correct tree depth with proper parent and sibling node references
      - Self-closing and void tags (br, hr, img, input) are handled without expecting a matching end tag
      hints:
        level1: 'State machine tokenizer: data state, tag open, tag name, attribute states.'
        level2: Build tree with stack. Push on open tag, pop on close. Handle implicit closes.
        level3: "from dataclasses import dataclass, field\nfrom typing import List, Dict, Optional\n\n@dataclass\nclass Node:\n\
          \    pass\n\n@dataclass\nclass Element(Node):\n    tag_name: str\n    attributes: Dict[str, str] = field(default_factory=dict)\n\
          \    children: List[Node] = field(default_factory=list)\n\n@dataclass\nclass Text(Node):\n    data: str\n\nclass\
          \ HTMLParser:\n    SELF_CLOSING = {'area', 'base', 'br', 'col', 'embed', 'hr', 'img', 'input', 'link', 'meta', 'param',\
          \ 'source', 'track', 'wbr'}\n    \n    def __init__(self, html: str):\n        self.html = html\n        self.pos\
          \ = 0\n    \n    def parse(self) -> Element:\n        # Create implicit root\n        root = Element('html')\n \
          \       self._parse_nodes(root)\n        return root\n    \n    def _parse_nodes(self, parent: Element):\n     \
          \   while self.pos < len(self.html):\n            if self._peek() == '<':\n                if self._peek(1) == '/':\n\
          \                    return  # Close tag - handled by caller\n                elif self._peek(1) == '!':\n     \
          \               self._skip_comment()\n                else:\n                    elem = self._parse_element()\n\
          \                    if elem:\n                        parent.children.append(elem)\n            else:\n       \
          \         text = self._parse_text()\n                if text.strip():\n                    parent.children.append(Text(text))\n\
          \    \n    def _parse_element(self) -> Optional[Element]:\n        self._consume('<')\n        tag_name = self._parse_tag_name()\n\
          \        attrs = self._parse_attributes()\n        \n        self._skip_whitespace()\n        \n        # Self-closing\
          \ syntax or void element\n        if self._peek() == '/' or tag_name.lower() in self.SELF_CLOSING:\n           \
          \ if self._peek() == '/':\n                self._consume('/')\n            self._consume('>')\n            return\
          \ Element(tag_name.lower(), attrs)\n        \n        self._consume('>')\n        \n        elem = Element(tag_name.lower(),\
          \ attrs)\n        self._parse_nodes(elem)\n        \n        # Consume close tag\n        if self._peek() == '<'\
          \ and self._peek(1) == '/':\n            self._consume('</')\n            self._parse_tag_name()  # Should match\n\
          \            self._consume('>')\n        \n        return elem\n    \n    def _parse_attributes(self) -> Dict[str,\
          \ str]:\n        attrs = {}\n        while True:\n            self._skip_whitespace()\n            if self._peek()\
          \ in ('>', '/', ''):\n                break\n            \n            name = self._parse_attr_name()\n        \
          \    self._skip_whitespace()\n            \n            if self._peek() == '=':\n                self._consume('=')\n\
          \                value = self._parse_attr_value()\n            else:\n                value = ''\n            \n\
          \            attrs[name.lower()] = value\n        return attrs"
      pitfalls:
      - Malformed HTML handling
      - Entity decoding
      - Case sensitivity
      concepts:
      - DOM
      - Tokenization
      - Tree construction
      estimated_hours: 12-20
      deliverables:
      - Tokenizer that splits raw HTML into start tags, end tags, self-closing tags, and text content tokens
      - DOM tree construction algorithm that builds a tree of element and text nodes from the token stream
      - Handler for self-closing and void elements (br, img, input) that do not require closing tags
      - Basic error recovery for malformed HTML such as missing closing tags or improperly nested elements
    - id: 2
      name: CSS Parser
      description: Parse CSS into a stylesheet.
      acceptance_criteria:
      - Selector parser handles tag selectors, class selectors, ID selectors, and combinators correctly
      - Property parser extracts property names and values including shorthand properties and units
      - 'Specificity is correctly calculated as (inline, #ids, .classes+[attrs], tags) for any selector'
      - 'Cascade applies rules in the correct order: user-agent defaults, then author styles ordered by specificity'
      hints:
        level1: 'Selectors: element, class, id, combinators. Properties are key: value pairs.'
        level2: 'Specificity: (inline, id, class, element). Higher wins. Later wins on tie.'
        level3: "@dataclass\nclass Selector:\n    tag: Optional[str] = None\n    id: Optional[str] = None\n    classes: List[str]\
          \ = field(default_factory=list)\n    \n    def specificity(self) -> tuple:\n        # (inline, ids, classes, elements)\n\
          \        return (\n            0,\n            1 if self.id else 0,\n            len(self.classes),\n          \
          \  1 if self.tag else 0\n        )\n    \n    def matches(self, elem: Element) -> bool:\n        if self.tag and\
          \ self.tag != elem.tag_name:\n            return False\n        if self.id and self.id != elem.attributes.get('id'):\n\
          \            return False\n        elem_classes = set(elem.attributes.get('class', '').split())\n        if not\
          \ all(c in elem_classes for c in self.classes):\n            return False\n        return True\n\n@dataclass\nclass\
          \ Rule:\n    selectors: List[Selector]\n    declarations: Dict[str, str]\n\nclass CSSParser:\n    def __init__(self,\
          \ css: str):\n        self.css = css\n        self.pos = 0\n    \n    def parse(self) -> List[Rule]:\n        rules\
          \ = []\n        while self.pos < len(self.css):\n            self._skip_whitespace_and_comments()\n            if\
          \ self.pos >= len(self.css):\n                break\n            rules.append(self._parse_rule())\n        return\
          \ rules\n    \n    def _parse_selector(self) -> Selector:\n        sel = Selector()\n        while self._peek()\
          \ not in (',', '{', ''):\n            if self._peek() == '#':\n                self._consume('#')\n            \
          \    sel.id = self._parse_identifier()\n            elif self._peek() == '.':\n                self._consume('.')\n\
          \                sel.classes.append(self._parse_identifier())\n            elif self._peek().isalpha():\n      \
          \          sel.tag = self._parse_identifier()\n            else:\n                break\n        return sel\n  \
          \  \n    def _parse_declarations(self) -> Dict[str, str]:\n        decls = {}\n        self._consume('{')\n    \
          \    while self._peek() != '}':\n            self._skip_whitespace()\n            name = self._parse_identifier()\n\
          \            self._skip_whitespace()\n            self._consume(':')\n            self._skip_whitespace()\n    \
          \        value = self._parse_value()\n            decls[name] = value\n            self._skip_whitespace()\n   \
          \         if self._peek() == ';':\n                self._consume(';')\n        self._consume('}')\n        return\
          \ decls\n\n# Style computation\ndef compute_style(elem: Element, stylesheet: List[Rule]) -> Dict[str, str]:\n  \
          \  # Collect matching rules with specificity\n    matched = []\n    for rule in stylesheet:\n        for selector\
          \ in rule.selectors:\n            if selector.matches(elem):\n                matched.append((selector.specificity(),\
          \ rule.declarations))\n    \n    # Sort by specificity\n    matched.sort(key=lambda x: x[0])\n    \n    # Apply\
          \ in order (later/higher specificity wins)\n    style = {}\n    for _, decls in matched:\n        style.update(decls)\n\
          \    \n    return style"
      pitfalls:
      - Selector combinators
      - Shorthand properties
      - '!important'
      concepts:
      - CSS parsing
      - Specificity
      - Cascade
      estimated_hours: 15-25
      deliverables:
      - CSS tokenizer that extracts selectors, property names, values, and delimiters from a stylesheet string
      - Selector parser supporting tag, class, ID, descendant, child, and sibling combinators
      - Specificity calculator that computes (inline, id, class, tag) weights for each selector
      - Style rule storage structure that maps selectors to their declared property-value pairs
    - id: 3
      name: Layout
      description: Calculate box positions and sizes.
      acceptance_criteria:
      - Box model correctly computes total element dimensions as content + padding + border + margin
      - Block layout stacks child elements vertically with correct margin collapsing between adjacent blocks
      - Inline layout flows text and inline elements horizontally, wrapping to a new line at the container width
      - Width and height calculations respect explicit CSS values and default to auto-sizing based on content
      hints:
        level1: Layout tree mirrors DOM but has computed styles. Block = vertical, Inline = horizontal.
        level2: Width flows down (parent constrains child). Height flows up (content determines parent).
        level3: "@dataclass\nclass Dimensions:\n    content: Rect = field(default_factory=Rect)\n    padding: EdgeSizes =\
          \ field(default_factory=EdgeSizes)\n    border: EdgeSizes = field(default_factory=EdgeSizes)\n    margin: EdgeSizes\
          \ = field(default_factory=EdgeSizes)\n    \n    def padding_box(self) -> Rect:\n        return self.content.expanded_by(self.padding)\n\
          \    \n    def border_box(self) -> Rect:\n        return self.padding_box().expanded_by(self.border)\n    \n   \
          \ def margin_box(self) -> Rect:\n        return self.border_box().expanded_by(self.margin)\n\n@dataclass\nclass\
          \ LayoutBox:\n    box_type: str  # 'block', 'inline', 'anonymous'\n    dimensions: Dimensions = field(default_factory=Dimensions)\n\
          \    children: List['LayoutBox'] = field(default_factory=list)\n    style: Dict[str, str] = field(default_factory=dict)\n\
          \    node: Optional[Element] = None\n\ndef layout_block(box: LayoutBox, containing_block: Dimensions):\n    # Calculate\
          \ width based on containing block\n    calculate_block_width(box, containing_block)\n    \n    # Position box below\
          \ previous siblings\n    calculate_block_position(box, containing_block)\n    \n    # Layout children\n    layout_block_children(box)\n\
          \    \n    # Height is determined by children\n    calculate_block_height(box)\n\ndef calculate_block_width(box:\
          \ LayoutBox, container: Dimensions):\n    style = box.style\n    \n    # Default to auto\n    width = style.get('width',\
          \ 'auto')\n    \n    # Get margin, padding, border values\n    margin_left = to_px(style.get('margin-left', '0'))\n\
          \    margin_right = to_px(style.get('margin-right', '0'))\n    padding_left = to_px(style.get('padding-left', '0'))\n\
          \    padding_right = to_px(style.get('padding-right', '0'))\n    border_left = to_px(style.get('border-left-width',\
          \ '0'))\n    border_right = to_px(style.get('border-right-width', '0'))\n    \n    total = margin_left + border_left\
          \ + padding_left + padding_right + border_right + margin_right\n    \n    if width != 'auto':\n        total +=\
          \ to_px(width)\n    \n    # Adjust for containing block\n    underflow = container.content.width - total\n    \n\
          \    if width == 'auto':\n        # Expand to fill\n        box.dimensions.content.width = underflow\n    else:\n\
          \        box.dimensions.content.width = to_px(width)\n        # Adjust auto margins\n        if style.get('margin-left')\
          \ == 'auto' and style.get('margin-right') == 'auto':\n            margin_left = underflow / 2\n            margin_right\
          \ = underflow / 2\n    \n    box.dimensions.padding.left = padding_left\n    box.dimensions.padding.right = padding_right\n\
          \    box.dimensions.margin.left = margin_left\n    box.dimensions.margin.right = margin_right"
      pitfalls:
      - Auto margins
      - Percentage units
      - Collapsing margins
      concepts:
      - Box model
      - Block formatting
      - Layout algorithms
      estimated_hours: 20-35
      deliverables:
      - Box model calculation that computes margin, border, padding, and content dimensions for each element
      - Block layout algorithm that stacks child boxes vertically and computes their y-positions
      - Inline layout algorithm that places inline boxes horizontally with line breaking at container boundaries
      - Layout tree construction from the styled DOM that pairs each DOM node with its computed box geometry
    - id: 4
      name: Rendering
      description: Paint the layout tree to a canvas.
      acceptance_criteria:
      - Background colors are painted as filled rectangles at the correct position and size for each box
      - Borders are rendered with the correct width, color, and style on all four sides of the box
      - Text is rendered at the correct baseline position using the computed font, size, and color properties
      - Z-ordering paints elements in the correct stacking order so later elements overlap earlier ones
      hints:
        level1: Walk layout tree. Draw backgrounds, then borders, then content (painter's algorithm).
        level2: Text needs font metrics. Use a graphics library (SDL, Cairo, etc.).
        level3: "@dataclass\nclass DisplayCommand:\n    pass\n\n@dataclass\nclass SolidColor(DisplayCommand):\n    color:\
          \ Color\n    rect: Rect\n\n@dataclass\nclass DrawText(DisplayCommand):\n    text: str\n    x: float\n    y: float\n\
          \    color: Color\n    font_size: float\n\ndef build_display_list(layout_root: LayoutBox) -> List[DisplayCommand]:\n\
          \    commands = []\n    render_layout_box(commands, layout_root)\n    return commands\n\ndef render_layout_box(commands:\
          \ List, box: LayoutBox):\n    render_background(commands, box)\n    render_borders(commands, box)\n    \n    if\
          \ box.box_type == 'block':\n        for child in box.children:\n            render_layout_box(commands, child)\n\
          \    elif box.box_type == 'inline':\n        render_text(commands, box)\n\ndef render_background(commands: List,\
          \ box: LayoutBox):\n    color = box.style.get('background-color')\n    if color and color != 'transparent':\n  \
          \      commands.append(SolidColor(\n            color=parse_color(color),\n            rect=box.dimensions.border_box()\n\
          \        ))\n\ndef render_borders(commands: List, box: LayoutBox):\n    color = box.style.get('border-color')\n\
          \    if not color:\n        return\n    \n    d = box.dimensions\n    border_box = d.border_box()\n    \n    # Top\
          \ border\n    if d.border.top > 0:\n        commands.append(SolidColor(\n            color=parse_color(color),\n\
          \            rect=Rect(\n                x=border_box.x,\n                y=border_box.y,\n                width=border_box.width,\n\
          \                height=d.border.top\n            )\n        ))\n    \n    # ... similar for other sides\n\ndef\
          \ render_text(commands: List, box: LayoutBox):\n    if box.node and isinstance(box.node, Text):\n        color =\
          \ box.style.get('color', 'black')\n        font_size = to_px(box.style.get('font-size', '16px'))\n        \n   \
          \     commands.append(DrawText(\n            text=box.node.data,\n            x=box.dimensions.content.x,\n    \
          \        y=box.dimensions.content.y + font_size,  # Baseline\n            color=parse_color(color),\n          \
          \  font_size=font_size\n        ))\n\n# Execute display list\ndef paint(commands: List[DisplayCommand], canvas):\n\
          \    for cmd in commands:\n        if isinstance(cmd, SolidColor):\n            canvas.fill_rect(cmd.rect, cmd.color)\n\
          \        elif isinstance(cmd, DrawText):\n            canvas.draw_text(cmd.text, cmd.x, cmd.y, cmd.color, cmd.font_size)"
      pitfalls:
      - Subpixel rendering
      - Font fallbacks
      - Clipping
      concepts:
      - Rendering pipeline
      - Display lists
      - Graphics
      estimated_hours: 33-70
      deliverables:
      - Paint list generator that traverses the layout tree and emits draw commands for each visible box
      - Rectangle and text drawing commands that specify coordinates, colors, fonts, and dimensions
      - Basic display list optimization that clips invisible elements and sorts commands by paint order
      - Render target that outputs the display list to a canvas, window surface, or image buffer
  build-bundler:
    id: build-bundler
    name: Build Your Own Bundler
    description: Build a JavaScript bundler like webpack/rollup with module resolution, tree shaking, and code splitting.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - JavaScript AST
    - Module systems (CommonJS, ESM)
    - File system operations
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible:
      - Go
      - Rust
    resources:
    - type: article
      name: Minipack - Simple bundler
      url: https://github.com/ronami/minipack
    - type: docs
      name: Rollup Plugin Development
      url: https://rollupjs.org/plugin-development/
    milestones:
    - id: 1
      name: Module Parsing
      description: Parse JavaScript files and extract dependencies.
      acceptance_criteria:
      - ES module import and export statements are correctly parsed and their specifiers extracted
      - Dependency list is built from all import, export-from, and require statements in each source file
      - Relative paths (./foo) and bare specifiers (lodash) are both recognized as module dependencies
      - Module dependency graph is constructed with nodes for each module and edges for each import relationship
      hints:
        level1: Use a parser like @babel/parser to get AST. Walk AST to find ImportDeclaration nodes.
        level2: Track both static imports and dynamic imports. Resolve paths relative to current file.
        level3: "const parser = require('@babel/parser');\nconst traverse = require('@babel/traverse').default;\nconst path\
          \ = require('path');\nconst fs = require('fs');\n\nfunction parseModule(filePath) {\n    const content = fs.readFileSync(filePath,\
          \ 'utf-8');\n    \n    const ast = parser.parse(content, {\n        sourceType: 'module',\n        plugins: ['jsx']\n\
          \    });\n    \n    const dependencies = [];\n    \n    traverse(ast, {\n        ImportDeclaration({ node }) {\n\
          \            dependencies.push({\n                source: node.source.value,\n                specifiers: node.specifiers.map(s\
          \ => ({\n                    type: s.type,\n                    imported: s.imported?.name,\n                  \
          \  local: s.local.name\n                }))\n            });\n        },\n        \n        ExportNamedDeclaration({\
          \ node }) {\n            if (node.source) {\n                dependencies.push({\n                    source: node.source.value,\n\
          \                    isReExport: true\n                });\n            }\n        },\n        \n        CallExpression({\
          \ node }) {\n            if (node.callee.type === 'Import') {\n                // Dynamic import\n             \
          \   if (node.arguments[0].type === 'StringLiteral') {\n                    dependencies.push({\n               \
          \         source: node.arguments[0].value,\n                        isDynamic: true\n                    });\n \
          \               }\n            }\n        }\n    });\n    \n    return {\n        filePath,\n        ast,\n    \
          \    content,\n        dependencies\n    };\n}\n\nfunction buildModuleGraph(entryPath) {\n    const modules = new\
          \ Map();\n    const queue = [path.resolve(entryPath)];\n    \n    while (queue.length > 0) {\n        const filePath\
          \ = queue.shift();\n        \n        if (modules.has(filePath)) continue;\n        \n        const module = parseModule(filePath);\n\
          \        modules.set(filePath, module);\n        \n        for (const dep of module.dependencies) {\n          \
          \  const resolvedPath = resolvePath(dep.source, filePath);\n            module.dependencies.find(d => d.source ===\
          \ dep.source).resolved = resolvedPath;\n            queue.push(resolvedPath);\n        }\n    }\n    \n    return\
          \ modules;\n}"
      pitfalls:
      - Circular dependencies
      - Node modules resolution
      - File extensions
      concepts:
      - AST parsing
      - Dependency analysis
      - Module graph
      estimated_hours: 10-15
      deliverables:
      - JavaScript and TypeScript parser integration that produces an AST from source files
      - Import and export statement extraction that identifies all static module dependencies
      - Dynamic import() call detection for identifying code-split points in the module graph
      - CommonJS require() call handling for backward compatibility with non-ESM modules
    - id: 2
      name: Module Resolution
      description: Implement Node.js-style module resolution.
      acceptance_criteria:
      - Relative path resolution correctly resolves ./file and ../file imports to absolute filesystem paths
      - Node modules lookup traverses parent directories checking node_modules/package until found or root is reached
      - Package.json main, module, and exports fields are consulted in the correct priority order for entry resolution
      - Index file fallback resolves a bare directory import to index.js or index.ts within that directory
      hints:
        level1: 'Relative: resolve against current file. Bare specifiers: walk up node_modules.'
        level2: Check package.json for main field. Try .js, .json, /index.js extensions.
        level3: "function resolvePath(specifier, fromPath) {\n    const fromDir = path.dirname(fromPath);\n    \n    // Relative\
          \ or absolute path\n    if (specifier.startsWith('.') || specifier.startsWith('/')) {\n        return resolveFile(path.resolve(fromDir,\
          \ specifier));\n    }\n    \n    // Bare specifier - node_modules\n    return resolveNodeModule(specifier, fromDir);\n\
          }\n\nfunction resolveFile(filePath) {\n    // Try exact path\n    if (fs.existsSync(filePath) && fs.statSync(filePath).isFile())\
          \ {\n        return filePath;\n    }\n    \n    // Try extensions\n    const extensions = ['.js', '.jsx', '.ts',\
          \ '.tsx', '.json'];\n    for (const ext of extensions) {\n        if (fs.existsSync(filePath + ext)) {\n       \
          \     return filePath + ext;\n        }\n    }\n    \n    // Try as directory\n    if (fs.existsSync(filePath) &&\
          \ fs.statSync(filePath).isDirectory()) {\n        // Check package.json\n        const pkgPath = path.join(filePath,\
          \ 'package.json');\n        if (fs.existsSync(pkgPath)) {\n            const pkg = JSON.parse(fs.readFileSync(pkgPath,\
          \ 'utf-8'));\n            if (pkg.main) {\n                return resolveFile(path.join(filePath, pkg.main));\n\
          \            }\n        }\n        \n        // Try index\n        return resolveFile(path.join(filePath, 'index'));\n\
          \    }\n    \n    throw new Error(`Cannot resolve: ${filePath}`);\n}\n\nfunction resolveNodeModule(specifier, fromDir)\
          \ {\n    // Handle scoped packages\n    const parts = specifier.split('/');\n    const packageName = specifier.startsWith('@')\
          \ \n        ? parts.slice(0, 2).join('/') \n        : parts[0];\n    const subpath = specifier.startsWith('@')\n\
          \        ? parts.slice(2).join('/')\n        : parts.slice(1).join('/');\n    \n    // Walk up directory tree\n\
          \    let dir = fromDir;\n    while (dir !== path.dirname(dir)) {\n        const nodeModules = path.join(dir, 'node_modules',\
          \ packageName);\n        if (fs.existsSync(nodeModules)) {\n            if (subpath) {\n                return resolveFile(path.join(nodeModules,\
          \ subpath));\n            }\n            return resolveFile(nodeModules);\n        }\n        dir = path.dirname(dir);\n\
          \    }\n    \n    throw new Error(`Cannot find module: ${specifier}`);\n}"
      pitfalls:
      - Symlinks
      - Package exports field
      - Conditional exports
      concepts:
      - Module resolution
      - Package management
      - Path resolution
      estimated_hours: 8-12
      deliverables:
      - Relative path resolver that converts ./foo and ../bar specifiers to absolute file system paths
      - Node modules resolution algorithm that walks up the directory tree checking node_modules folders
      - Package.json field handler that reads main, module, exports, and browser fields for entry point resolution
      - Dependency graph constructor that assembles the full transitive closure of all module dependencies
    - id: 3
      name: Bundle Generation
      description: Generate a single JavaScript bundle from module graph.
      acceptance_criteria:
      - Each module is wrapped in a function scope so its top-level variables do not pollute the global namespace
      - Module registry tracks loaded modules by ID and returns cached exports on subsequent require calls
      - Import and export bindings are correctly rewritten to use the runtime module loader's API
      - Source maps are generated in v3 format and correctly map bundled positions to original file and line numbers
      hints:
        level1: Each module becomes a function. Registry maps IDs to modules. Require loads and caches.
        level2: Transform import/export to require/exports. Assign numeric IDs to modules.
        level3: "const generate = require('@babel/generator').default;\nconst t = require('@babel/types');\n\nfunction generateBundle(moduleGraph,\
          \ entryPath) {\n    const modules = [];\n    const moduleIds = new Map();\n    let nextId = 0;\n    \n    // Assign\
          \ IDs\n    for (const [filePath] of moduleGraph) {\n        moduleIds.set(filePath, nextId++);\n    }\n    \n  \
          \  // Transform each module\n    for (const [filePath, module] of moduleGraph) {\n        const id = moduleIds.get(filePath);\n\
          \        const transformedCode = transformModule(module, moduleIds);\n        \n        modules.push(`\n       \
          \     ${id}: function(module, exports, require) {\n                ${transformedCode}\n            }`);\n    }\n\
          \    \n    const entryId = moduleIds.get(path.resolve(entryPath));\n    \n    return `\n        (function(modules)\
          \ {\n            const installedModules = {};\n            \n            function require(moduleId) {\n        \
          \        if (installedModules[moduleId]) {\n                    return installedModules[moduleId].exports;\n   \
          \             }\n                \n                const module = installedModules[moduleId] = {\n             \
          \       exports: {}\n                };\n                \n                modules[moduleId](module, module.exports,\
          \ require);\n                \n                return module.exports;\n            }\n            \n           \
          \ return require(${entryId});\n        })({${modules.join(',')}});\n    `;\n}\n\nfunction transformModule(module,\
          \ moduleIds) {\n    traverse(module.ast, {\n        ImportDeclaration(path) {\n            const depId = moduleIds.get(path.node._resolved);\n\
          \            // Transform to require\n            // import { foo } from './bar' -> const { foo } = require(1)\n\
          \            const requireCall = t.callExpression(\n                t.identifier('require'),\n                [t.numericLiteral(depId)]\n\
          \            );\n            // ... handle different import types\n            path.replaceWith(/* ... */);\n  \
          \      },\n        \n        ExportNamedDeclaration(path) {\n            // exports.foo = foo;\n        },\n   \
          \     \n        ExportDefaultDeclaration(path) {\n            // exports.default = ...;\n        }\n    });\n  \
          \  \n    return generate(module.ast).code;\n}"
      pitfalls:
      - Circular dependencies
      - Live bindings
      - Default export handling
      concepts:
      - Code generation
      - Module wrapping
      - Runtime
      estimated_hours: 15-25
      deliverables:
      - Module wrapping that encloses each module's code in a function scope to prevent variable leaks
      - Runtime module loader code that manages a module registry, executes modules on demand, and caches exports
      - Topological sort of the module dependency graph to determine correct initialization order
      - Source map generation that maps bundled output positions back to original source file locations
    - id: 4
      name: Tree Shaking
      description: Eliminate unused code from the bundle.
      acceptance_criteria:
      - Used exports are tracked across the module graph and only reachable code is included in the output
      - Unused exports and their associated code are removed from the final bundle output
      - 'Modules marked with sideEffects: false in package.json are fully removed when none of their exports are used'
      - Code with side effects (e.g., top-level function calls, global assignments) is preserved even if exports are unused
      hints:
        level1: Build usage graph from imports. Mark used exports. Remove unused declarations.
        level2: Side effects (top-level code) must be preserved unless marked pure.
        level3: "function treeShake(moduleGraph, entryPath) {\n    // Track what's used\n    const usedExports = new Map();\
          \  // filePath -> Set of export names\n    const usedModules = new Set();\n    \n    // Start from entry\n    function\
          \ markUsed(filePath, importedNames) {\n        usedModules.add(filePath);\n        \n        const module = moduleGraph.get(filePath);\n\
          \        if (!usedExports.has(filePath)) {\n            usedExports.set(filePath, new Set());\n        }\n     \
          \   \n        const used = usedExports.get(filePath);\n        \n        if (importedNames === '*') {\n        \
          \    // Namespace import - mark all\n            module.exports.forEach(exp => used.add(exp));\n        } else {\n\
          \            importedNames.forEach(name => used.add(name));\n        }\n        \n        // Follow dependencies\n\
          \        for (const dep of module.dependencies) {\n            const names = dep.specifiers?.map(s => s.imported\
          \ || 'default') || ['*'];\n            markUsed(dep.resolved, names);\n        }\n    }\n    \n    markUsed(path.resolve(entryPath),\
          \ ['*']);\n    \n    // Remove unused\n    for (const [filePath, module] of moduleGraph) {\n        if (!usedModules.has(filePath))\
          \ {\n            moduleGraph.delete(filePath);\n            continue;\n        }\n        \n        const used =\
          \ usedExports.get(filePath);\n        \n        traverse(module.ast, {\n            ExportNamedDeclaration(path)\
          \ {\n                if (path.node.declaration) {\n                    const name = path.node.declaration.id?.name;\n\
          \                    if (name && !used.has(name)) {\n                        // Check for side effects\n       \
          \                 if (!hasSideEffects(path.node.declaration)) {\n                            path.remove();\n  \
          \                      }\n                    }\n                }\n            },\n            \n            //\
          \ Remove unused top-level declarations\n            FunctionDeclaration(path) {\n                if (path.parent.type\
          \ === 'Program') {\n                    const name = path.node.id.name;\n                    if (!isUsed(name, module,\
          \ used)) {\n                        path.remove();\n                    }\n                }\n            }\n  \
          \      });\n    }\n    \n    return moduleGraph;\n}\n\nfunction hasSideEffects(node) {\n    // Conservative: assume\
          \ functions are pure\n    // But check for top-level calls, assignments to globals, etc.\n    let found = false;\n\
          \    traverse(node, {\n        CallExpression() { found = true; },\n        AssignmentExpression({ node }) {\n \
          \           if (node.left.type === 'MemberExpression') found = true;\n        }\n    });\n    return found;\n}"
      pitfalls:
      - Side effect detection
      - Re-exports
      - Dynamic access patterns
      concepts:
      - Dead code elimination
      - Static analysis
      - Purity
      estimated_hours: 17-28
      deliverables:
      - Dead code detection via static analysis of export usage across the entire module graph
      - Side effect annotation handling that reads sideEffects field from package.json to identify pure modules
      - Unused export removal that eliminates exported symbols not imported by any other module in the bundle
      - Bundle size optimization reporting that shows bytes saved by tree-shaking per module
  build-ci-system:
    id: build-ci-system
    name: Build Your Own CI System
    description: Build a continuous integration system that runs pipelines based on code changes.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Docker/containers
    - Git hooks/webhooks
    - Process management
    - Queue systems
    languages:
      recommended:
      - Go
      - Python
      - Rust
      also_possible:
      - JavaScript
      - Java
    resources:
    - type: article
      name: How GitHub Actions Works
      url: https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions
    - type: article
      name: Building a CI Server
      url: https://blog.boot.dev/education/build-ci-cd-server/
    - type: documentation
      name: Jenkins Architecture
      url: https://www.jenkins.io/doc/developer/architecture/
    milestones:
    - id: 1
      name: Pipeline Configuration Parser
      description: Parse and validate pipeline configuration files (YAML).
      acceptance_criteria:
      - YAML pipeline files are parsed into a structured pipeline object with stages, jobs, and step definitions
      - Step and stage structure correctly captures sequential steps within a job and parallel stages in the pipeline
      - Environment variables from the pipeline config, system, and secrets are substituted into step commands
      - Conditional execution uses if-expressions to skip or run steps based on branch name, event type, or custom conditions
      - Matrix builds expand multiple axis values into the cartesian product of job configurations for parallel testing
      hints:
        level1: Define a schema for pipelines with stages, jobs, and steps.
        level2: Support variables, conditionals (if), and matrix for parallel variants.
        level3: "## Pipeline Configuration\n\n```yaml\n# Example: .ci/pipeline.yaml\nname: Build and Test\n\non:\n  push:\n\
          \    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\nenv:\n  GO_VERSION: \"1.21\"\n\njobs:\n\
          \  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        go-version: [\"1.20\", \"1.21\"]\n \
          \   steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        \n      - name: Setup Go\n      \
          \  run: |\n          wget https://go.dev/dl/go${{ matrix.go-version }}.linux-amd64.tar.gz\n          tar -xzf go${{\
          \ matrix.go-version }}.linux-amd64.tar.gz\n          \n      - name: Run Tests\n        run: go test ./...\n   \
          \     \n  build:\n    needs: test\n    if: github.ref == 'refs/heads/main'\n    steps:\n      - name: Build\n  \
          \      run: go build -o app ./cmd/app\n        \n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n\
          \        with:\n          name: app-binary\n          path: app\n```\n\n```go\npackage pipeline\n\nimport (\n  \
          \  \"gopkg.in/yaml.v3\"\n)\n\ntype Pipeline struct {\n    Name string            `yaml:\"name\"`\n    On   Trigger\
          \           `yaml:\"on\"`\n    Env  map[string]string `yaml:\"env\"`\n    Jobs map[string]*Job   `yaml:\"jobs\"\
          `\n}\n\ntype Trigger struct {\n    Push        *PushTrigger        `yaml:\"push,omitempty\"`\n    PullRequest *PullRequestTrigger\
          \ `yaml:\"pull_request,omitempty\"`\n    Schedule    []CronTrigger       `yaml:\"schedule,omitempty\"`\n}\n\ntype\
          \ Job struct {\n    RunsOn   string            `yaml:\"runs-on\"`\n    Needs    []string          `yaml:\"needs,omitempty\"\
          `\n    If       string            `yaml:\"if,omitempty\"`\n    Strategy *Strategy         `yaml:\"strategy,omitempty\"\
          `\n    Env      map[string]string `yaml:\"env,omitempty\"`\n    Steps    []Step            `yaml:\"steps\"`\n}\n\
          \ntype Strategy struct {\n    Matrix    map[string][]interface{} `yaml:\"matrix\"`\n    FailFast  bool         \
          \            `yaml:\"fail-fast\"`\n    MaxParallel int                    `yaml:\"max-parallel\"`\n}\n\ntype Step\
          \ struct {\n    Name    string            `yaml:\"name\"`\n    Uses    string            `yaml:\"uses,omitempty\"\
          `\n    Run     string            `yaml:\"run,omitempty\"`\n    With    map[string]string `yaml:\"with,omitempty\"\
          `\n    Env     map[string]string `yaml:\"env,omitempty\"`\n    If      string            `yaml:\"if,omitempty\"\
          `\n}\n\nfunc ParsePipeline(data []byte) (*Pipeline, error) {\n    var p Pipeline\n    if err := yaml.Unmarshal(data,\
          \ &p); err != nil {\n        return nil, err\n    }\n    return &p, p.Validate()\n}\n\nfunc (p *Pipeline) Validate()\
          \ error {\n    // Check for circular dependencies\n    // Validate step configurations\n    // Verify matrix combinations\n\
          \    return nil\n}\n```"
      pitfalls:
      - Circular job dependencies
      - Invalid matrix combinations
      - Missing required fields
      concepts:
      - YAML parsing
      - Configuration validation
      - DAG construction
      estimated_hours: 8-12
      deliverables:
      - YAML pipeline definition parser that reads job definitions, steps, and configuration from a CI config file
      - Stage and job dependency graph builder that determines execution order and parallelism from declared dependencies
      - Environment variable substitution engine that resolves $VAR and ${VAR} references in pipeline configuration
      - Matrix build configuration that expands axis definitions into multiple job instances with different parameter combinations
    - id: 2
      name: Job Execution Engine
      description: Execute pipeline jobs in isolated containers.
      acceptance_criteria:
      - Container-based isolation ensures each job runs in a fresh Docker container with no state from previous jobs
      - Step execution runs each shell command in sequence and stops the job on the first non-zero exit code
      - Environment variables including secrets are injected into the container without leaking to logs
      - Stdout and stderr output from each step is captured in real-time and stored for later retrieval
      - Artifacts specified by glob patterns are collected from the job workspace and uploaded to artifact storage
      hints:
        level1: Use Docker to run each job in isolation. Mount workspace as volume.
        level2: Capture stdout/stderr, handle exit codes, support timeouts.
        level3: "## Job Execution\n\n```go\ntype JobExecutor struct {\n    dockerClient *docker.Client\n    workspace    string\n\
          }\n\ntype JobResult struct {\n    Status    string\n    StartTime time.Time\n    EndTime   time.Time\n    Steps\
          \     []StepResult\n    Artifacts []string\n}\n\ntype StepResult struct {\n    Name     string\n    Status   string\n\
          \    Output   string\n    Duration time.Duration\n    ExitCode int\n}\n\nfunc (e *JobExecutor) Execute(ctx context.Context,\
          \ job *Job, matrix map[string]interface{}) (*JobResult, error) {\n    result := &JobResult{\n        Status:   \
          \ \"running\",\n        StartTime: time.Now(),\n    }\n    \n    // Create container\n    containerConfig := &container.Config{\n\
          \        Image:      job.RunsOn,\n        WorkingDir: \"/workspace\",\n        Env:        buildEnv(job.Env, matrix),\n\
          \    }\n    \n    hostConfig := &container.HostConfig{\n        Binds: []string{\n            fmt.Sprintf(\"%s:/workspace\"\
          , e.workspace),\n        },\n    }\n    \n    resp, err := e.dockerClient.ContainerCreate(ctx, containerConfig,\
          \ hostConfig, nil, nil, \"\")\n    if err != nil {\n        return nil, err\n    }\n    defer e.dockerClient.ContainerRemove(ctx,\
          \ resp.ID, types.ContainerRemoveOptions{Force: true})\n    \n    if err := e.dockerClient.ContainerStart(ctx, resp.ID,\
          \ types.ContainerStartOptions{}); err != nil {\n        return nil, err\n    }\n    \n    // Execute steps\n   \
          \ for _, step := range job.Steps {\n        stepResult := e.executeStep(ctx, resp.ID, step, matrix)\n        result.Steps\
          \ = append(result.Steps, stepResult)\n        \n        if stepResult.Status == \"failed\" {\n            result.Status\
          \ = \"failed\"\n            break\n        }\n    }\n    \n    if result.Status == \"running\" {\n        result.Status\
          \ = \"success\"\n    }\n    result.EndTime = time.Now()\n    \n    return result, nil\n}\n\nfunc (e *JobExecutor)\
          \ executeStep(ctx context.Context, containerID string, step Step, matrix map[string]interface{}) StepResult {\n\
          \    result := StepResult{\n        Name: step.Name,\n    }\n    start := time.Now()\n    \n    // Interpolate variables\n\
          \    script := interpolateVariables(step.Run, matrix)\n    \n    // Create exec\n    execConfig := types.ExecConfig{\n\
          \        Cmd:          []string{\"/bin/sh\", \"-c\", script},\n        AttachStdout: true,\n        AttachStderr:\
          \ true,\n    }\n    \n    execResp, err := e.dockerClient.ContainerExecCreate(ctx, containerID, execConfig)\n  \
          \  if err != nil {\n        result.Status = \"failed\"\n        result.Output = err.Error()\n        return result\n\
          \    }\n    \n    // Attach and capture output\n    attachResp, _ := e.dockerClient.ContainerExecAttach(ctx, execResp.ID,\
          \ types.ExecStartCheck{})\n    defer attachResp.Close()\n    \n    var output bytes.Buffer\n    io.Copy(&output,\
          \ attachResp.Reader)\n    \n    // Get exit code\n    inspectResp, _ := e.dockerClient.ContainerExecInspect(ctx,\
          \ execResp.ID)\n    \n    result.Duration = time.Since(start)\n    result.Output = output.String()\n    result.ExitCode\
          \ = inspectResp.ExitCode\n    \n    if result.ExitCode == 0 {\n        result.Status = \"success\"\n    } else {\n\
          \        result.Status = \"failed\"\n    }\n    \n    return result\n}\n```"
      pitfalls:
      - Container cleanup on failure
      - Timeout handling
      - Large output handling
      concepts:
      - Container isolation
      - Process execution
      - Resource management
      estimated_hours: 12-18
      deliverables:
      - Docker container job execution that runs each job in an isolated container with the specified image
      - Script step runner that executes shell commands sequentially and captures stdout/stderr with exit codes
      - Artifact collection that copies specified output files from the job container to persistent storage
      - Parallel job execution engine that runs independent jobs concurrently up to the configured worker limit
    - id: 3
      name: Webhook & Queue System
      description: Handle webhooks and queue jobs for execution.
      acceptance_criteria:
      - Git webhook handler validates the webhook signature and parses push, PR, and tag event payloads
      - Job queue reliably stores pending jobs and delivers each job to exactly one worker for execution
      - Multiple workers execute jobs concurrently with the maximum concurrency enforced by the pool configuration
      - Rate limiting prevents a burst of webhook events from overwhelming the system by throttling queue intake
      - Priority scheduling allows critical pipelines (e.g., production deploys) to be processed before lower-priority jobs
      hints:
        level1: Parse webhook payloads to determine which pipelines to trigger.
        level2: Use a work queue with configurable parallelism. Track job state.
        level3: "## Webhook & Queue\n\n```go\n// Webhook handler\nfunc (s *CIServer) handleWebhook(w http.ResponseWriter,\
          \ r *http.Request) {\n    // Verify signature\n    signature := r.Header.Get(\"X-Hub-Signature-256\")\n    if !verifySignature(r.Body,\
          \ signature, s.webhookSecret) {\n        http.Error(w, \"Invalid signature\", http.StatusUnauthorized)\n       \
          \ return\n    }\n    \n    var payload WebhookPayload\n    if err := json.NewDecoder(r.Body).Decode(&payload); err\
          \ != nil {\n        http.Error(w, err.Error(), http.StatusBadRequest)\n        return\n    }\n    \n    // Determine\
          \ event type\n    eventType := r.Header.Get(\"X-GitHub-Event\")\n    \n    // Find matching pipelines\n    pipelines\
          \ := s.findMatchingPipelines(payload.Repository, eventType, payload)\n    \n    // Queue jobs\n    for _, pipeline\
          \ := range pipelines {\n        build := &Build{\n            ID:         uuid.New().String(),\n            Pipeline:\
          \   pipeline,\n            Commit:     payload.After,\n            Branch:     payload.Ref,\n            Author:\
          \     payload.Pusher.Name,\n            Status:     \"queued\",\n            QueuedAt:   time.Now(),\n        }\n\
          \        s.queue.Enqueue(build)\n    }\n    \n    w.WriteHeader(http.StatusAccepted)\n}\n\n// Job Queue\ntype JobQueue\
          \ struct {\n    mu          sync.Mutex\n    pending     []*Build\n    running     map[string]*Build\n    maxParallel\
          \ int\n    workers     chan struct{}\n}\n\nfunc NewJobQueue(maxParallel int) *JobQueue {\n    return &JobQueue{\n\
          \        pending:     make([]*Build, 0),\n        running:     make(map[string]*Build),\n        maxParallel: maxParallel,\n\
          \        workers:     make(chan struct{}, maxParallel),\n    }\n}\n\nfunc (q *JobQueue) Enqueue(build *Build) {\n\
          \    q.mu.Lock()\n    // Priority: main branch > PRs > other branches\n    insertIdx := len(q.pending)\n    for\
          \ i, b := range q.pending {\n        if build.Priority() > b.Priority() {\n            insertIdx = i\n         \
          \   break\n        }\n    }\n    q.pending = append(q.pending[:insertIdx], append([]*Build{build}, q.pending[insertIdx:]...)...)\n\
          \    q.mu.Unlock()\n    \n    q.tryDispatch()\n}\n\nfunc (q *JobQueue) tryDispatch() {\n    q.mu.Lock()\n    defer\
          \ q.mu.Unlock()\n    \n    for len(q.pending) > 0 && len(q.running) < q.maxParallel {\n        build := q.pending[0]\n\
          \        q.pending = q.pending[1:]\n        q.running[build.ID] = build\n        \n        go func(b *Build) {\n\
          \            q.workers <- struct{}{}  // Acquire worker slot\n            defer func() { <-q.workers }()\n     \
          \       \n            b.Run()\n            \n            q.mu.Lock()\n            delete(q.running, b.ID)\n    \
          \        q.mu.Unlock()\n            \n            q.tryDispatch()  // Try to dispatch next job\n        }(build)\n\
          \    }\n}\n```"
      pitfalls:
      - Webhook replay attacks
      - Queue starvation
      - Resource exhaustion
      concepts:
      - Webhook security
      - Work queues
      - Concurrency control
      estimated_hours: 10-15
      deliverables:
      - GitHub and GitLab webhook handler that receives push, pull-request, and tag events via HTTP POST
      - Job queue backed by Redis or a database that stores pending pipeline runs with their trigger context
      - Worker process pool that dequeues and executes pipeline jobs with configurable concurrency limits
      - Pipeline triggering logic that matches webhook events to pipeline configurations and creates pipeline runs
    - id: 4
      name: Web Dashboard
      description: Build a dashboard for viewing builds and logs.
      acceptance_criteria:
      - Build list view shows recent pipeline runs with their status, trigger, branch, and duration at a glance
      - Real-time log streaming displays job output in the browser within seconds of it being produced
      - Build status badges return SVG images showing pass/fail status embeddable in repository README files
      - Pipeline DAG visualization renders stages and jobs as nodes with arrows showing dependency relationships
      hints:
        level1: Use WebSockets for real-time log streaming.
        level2: Show pipeline as a graph with job nodes and dependency edges.
        level3: "## Dashboard Implementation\n\n```go\n// Log streaming via WebSocket\nfunc (s *CIServer) handleLogStream(w\
          \ http.ResponseWriter, r *http.Request) {\n    buildID := r.URL.Query().Get(\"build\")\n    \n    upgrader := websocket.Upgrader{}\n\
          \    conn, err := upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        return\n    }\n    defer conn.Close()\n\
          \    \n    // Subscribe to build logs\n    logChan := s.logBroker.Subscribe(buildID)\n    defer s.logBroker.Unsubscribe(buildID,\
          \ logChan)\n    \n    // Send existing logs\n    existingLogs := s.logStore.GetLogs(buildID)\n    for _, log :=\
          \ range existingLogs {\n        conn.WriteJSON(log)\n    }\n    \n    // Stream new logs\n    for {\n        select\
          \ {\n        case log, ok := <-logChan:\n            if !ok {\n                return\n            }\n         \
          \   if err := conn.WriteJSON(log); err != nil {\n                return\n            }\n        case <-r.Context().Done():\n\
          \            return\n        }\n    }\n}\n```\n\n```javascript\n// Frontend Log Viewer\nfunction LogViewer({ buildId\
          \ }) {\n  const [logs, setLogs] = useState([]);\n  const logEndRef = useRef(null);\n  \n  useEffect(() => {\n  \
          \  const ws = new WebSocket(`ws://localhost:8080/logs?build=${buildId}`);\n    \n    ws.onmessage = (event) => {\n\
          \      const log = JSON.parse(event.data);\n      setLogs(prev => [...prev, log]);\n    };\n    \n    return ()\
          \ => ws.close();\n  }, [buildId]);\n  \n  useEffect(() => {\n    logEndRef.current?.scrollIntoView({ behavior: 'smooth'\
          \ });\n  }, [logs]);\n  \n  return (\n    <div className=\"log-viewer\">\n      {logs.map((log, i) => (\n      \
          \  <div key={i} className={`log-line ${log.level}`}>\n          <span className=\"timestamp\">{log.timestamp}</span>\n\
          \          <span className=\"step\">[{log.step}]</span>\n          <span className=\"message\">{log.message}</span>\n\
          \        </div>\n      ))}\n      <div ref={logEndRef} />\n    </div>\n  );\n}\n\n// Pipeline Graph\nfunction PipelineGraph({\
          \ pipeline, buildStatus }) {\n  return (\n    <div className=\"pipeline-graph\">\n      {Object.entries(pipeline.jobs).map(([name,\
          \ job]) => (\n        <JobNode\n          key={name}\n          name={name}\n          job={job}\n          status={buildStatus.jobs[name]?.status\
          \ || 'pending'}\n          needs={job.needs || []}\n        />\n      ))}\n    </div>\n  );\n}\n```\n\n```html\n\
          <!-- Status Badge endpoint -->\n/api/badge/:repo/:branch\n\nReturns SVG:\n<svg xmlns=\"http://www.w3.org/2000/svg\"\
          \ width=\"100\" height=\"20\">\n  <rect width=\"100\" height=\"20\" fill=\"#555\"/>\n  <rect x=\"50\" width=\"50\"\
          \ height=\"20\" fill=\"#4c1\"/>  <!-- green for passing -->\n  <text x=\"25\" y=\"14\" fill=\"#fff\" text-anchor=\"\
          middle\">build</text>\n  <text x=\"75\" y=\"14\" fill=\"#fff\" text-anchor=\"middle\">passing</text>\n</svg>\n```"
      pitfalls:
      - WebSocket connection management
      - Large log performance
      - Badge caching
      concepts:
      - Real-time streaming
      - Data visualization
      - SVG generation
      estimated_hours: 10-15
      deliverables:
      - Build history page displaying all pipeline runs with status, branch, commit, and duration information
      - Real-time log streaming that pushes step output to the browser as the job executes via WebSocket or SSE
      - Pipeline visualization as a directed acyclic graph showing stages, jobs, and their dependency relationships
      - Build artifact download page listing available artifacts with download links for each pipeline run
  build-debugger:
    id: build-debugger
    name: Build Your Own Debugger
    description: Build a debugger like GDB. Learn ptrace, breakpoints, and symbol tables.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Unix processes
    - Assembly basics
    - ELF format
    languages:
      recommended:
      - C
      - Rust
      - C++
      also_possible:
      - Go
    resources:
    - type: blog
      name: Writing a Linux Debugger
      url: https://blog.tartanllama.xyz/writing-a-linux-debugger-setup/
    - type: book
      name: The Linux Programming Interface - Ch 26
      url: https://man7.org/tlpi/
    milestones:
    - id: 1
      name: Process Control
      description: Use ptrace to control debugee execution.
      acceptance_criteria:
      - Debugger forks a child process and attaches to it with ptrace before the child executes the target program
      - Single-step execution advances exactly one instruction and returns control to the debugger after each step
      - Continue execution resumes the tracee and blocks the debugger until the next signal or breakpoint is hit
      - Debugger correctly waits for and handles signals from the tracee using waitpid with status inspection
      hints:
        level1: ptrace(PTRACE_TRACEME) in child, parent uses PTRACE_CONT, PTRACE_SINGLESTEP.
        level2: Child stops on exec. Use waitpid to detect stops. Check WIFSTOPPED.
        level3: "#include <sys/ptrace.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <stdio.h>\n\nstruct debugger\
          \ {\n    pid_t pid;\n    int running;\n};\n\nvoid run_debugee(const char* prog) {\n    ptrace(PTRACE_TRACEME, 0,\
          \ NULL, NULL);\n    execl(prog, prog, NULL);\n}\n\nstruct debugger* create_debugger(const char* prog) {\n    struct\
          \ debugger* dbg = malloc(sizeof(struct debugger));\n    \n    pid_t pid = fork();\n    if (pid == 0) {\n       \
          \ // Child\n        run_debugee(prog);\n    }\n    \n    // Parent\n    dbg->pid = pid;\n    dbg->running = 0;\n\
          \    \n    // Wait for child to stop at exec\n    int status;\n    waitpid(pid, &status, 0);\n    \n    // Set options\n\
          \    ptrace(PTRACE_SETOPTIONS, pid, NULL, PTRACE_O_EXITKILL);\n    \n    return dbg;\n}\n\nvoid continue_execution(struct\
          \ debugger* dbg) {\n    ptrace(PTRACE_CONT, dbg->pid, NULL, NULL);\n    dbg->running = 1;\n    \n    int status;\n\
          \    waitpid(dbg->pid, &status, 0);\n    dbg->running = 0;\n    \n    if (WIFSTOPPED(status)) {\n        printf(\"\
          Stopped with signal %d\\n\", WSTOPSIG(status));\n    } else if (WIFEXITED(status)) {\n        printf(\"Exited with\
          \ code %d\\n\", WEXITSTATUS(status));\n    }\n}\n\nvoid single_step(struct debugger* dbg) {\n    ptrace(PTRACE_SINGLESTEP,\
          \ dbg->pid, NULL, NULL);\n    int status;\n    waitpid(dbg->pid, &status, 0);\n}"
      pitfalls:
      - Signal handling in debugger
      - Race conditions
      - Zombie processes
      concepts:
      - ptrace
      - Process control
      - Unix signals
      estimated_hours: 8-12
      deliverables:
      - ptrace() attachment that connects the debugger to a target process for inspection and control
      - Process start and stop control using SIGSTOP and SIGCONT signals to pause and resume execution
      - Single-step execution using PTRACE_SINGLESTEP to advance exactly one machine instruction at a time
      - Continue execution using PTRACE_CONT to resume the process until the next breakpoint or signal
    - id: 2
      name: Breakpoints
      description: Implement software breakpoints using int3.
      acceptance_criteria:
      - Setting a breakpoint at a given address replaces the first byte of the instruction with 0xCC (INT3)
      - Original instruction byte is saved and correctly restored when the breakpoint is hit so execution can continue
      - Breakpoint hit is detected by the debugger via SIGTRAP, and the instruction pointer is adjusted back by one byte
      - Multiple breakpoints can be set simultaneously and each fires independently when its address is reached
      hints:
        level1: Read original byte with PTRACE_PEEKTEXT, write 0xCC. On hit, restore and rewind PC.
        level2: Breakpoint hit causes SIGTRAP. PC points past int3, so decrement by 1.
        level3: "#include <stdint.h>\n#include <sys/ptrace.h>\n\nstruct breakpoint {\n    uint64_t addr;\n    uint8_t saved_byte;\n\
          \    int enabled;\n};\n\nstruct breakpoint* set_breakpoint(struct debugger* dbg, uint64_t addr) {\n    struct breakpoint*\
          \ bp = malloc(sizeof(struct breakpoint));\n    bp->addr = addr;\n    bp->enabled = 0;\n    \n    // Read original\
          \ instruction\n    long data = ptrace(PTRACE_PEEKTEXT, dbg->pid, addr, NULL);\n    bp->saved_byte = (uint8_t)(data\
          \ & 0xFF);\n    \n    // Replace with int3 (0xCC)\n    long modified = (data & ~0xFF) | 0xCC;\n    ptrace(PTRACE_POKETEXT,\
          \ dbg->pid, addr, modified);\n    \n    bp->enabled = 1;\n    return bp;\n}\n\nvoid disable_breakpoint(struct debugger*\
          \ dbg, struct breakpoint* bp) {\n    if (!bp->enabled) return;\n    \n    long data = ptrace(PTRACE_PEEKTEXT, dbg->pid,\
          \ bp->addr, NULL);\n    long restored = (data & ~0xFF) | bp->saved_byte;\n    ptrace(PTRACE_POKETEXT, dbg->pid,\
          \ bp->addr, restored);\n    \n    bp->enabled = 0;\n}\n\nvoid handle_breakpoint_hit(struct debugger* dbg, struct\
          \ breakpoint* bp) {\n    // Get registers\n    struct user_regs_struct regs;\n    ptrace(PTRACE_GETREGS, dbg->pid,\
          \ NULL, &regs);\n    \n    // PC is now past int3, rewind\n    regs.rip = bp->addr;\n    ptrace(PTRACE_SETREGS,\
          \ dbg->pid, NULL, &regs);\n    \n    // Disable breakpoint, single step, re-enable\n    disable_breakpoint(dbg,\
          \ bp);\n    single_step(dbg);\n    enable_breakpoint(dbg, bp);\n}"
      pitfalls:
      - Multi-byte instruction boundaries
      - Breakpoint in loop
      - Thread safety
      concepts:
      - Software breakpoints
      - Instruction patching
      - Program counter
      estimated_hours: 10-15
      deliverables:
      - Software breakpoint implementation that writes an INT3 (0xCC) byte at the target instruction address
      - Original instruction preservation that saves the overwritten byte for restoration when the breakpoint is hit
      - Breakpoint enable and disable toggling that inserts or restores the original byte without losing the breakpoint
      - Hit count tracking that increments a counter each time a breakpoint is triggered during execution
    - id: 3
      name: Symbol Tables
      description: Parse DWARF debug info for source-level debugging.
      acceptance_criteria:
      - ELF section headers are parsed to locate .debug_info, .debug_line, .debug_abbrev, and .symtab sections
      - DWARF Debug Information Entries (DIEs) are decoded including tags, attributes, and tree structure
      - Address-to-line mapping converts a program counter value to the corresponding source file and line number
      - Name-to-address mapping resolves a function name to its entry point address for setting breakpoints by name
      hints:
        level1: ELF has .debug_info, .debug_line sections. Use libdwarf or parse manually.
        level2: DWARF uses DIEs (Debug Info Entries) in tree structure. Line table maps PC to source.
        level3: "// Using libdwarf\n#include <libdwarf/libdwarf.h>\n#include <libdwarf/dwarf.h>\n\nstruct line_info {\n  \
          \  uint64_t addr;\n    unsigned int line;\n    const char* file;\n};\n\nstruct symbol_table {\n    Dwarf_Debug dbg;\n\
          \    struct line_info* lines;\n    size_t num_lines;\n    // function name -> address map\n};\n\nstruct symbol_table*\
          \ load_symbols(const char* binary) {\n    struct symbol_table* st = calloc(1, sizeof(struct symbol_table));\n  \
          \  \n    int fd = open(binary, O_RDONLY);\n    Dwarf_Error err;\n    \n    if (dwarf_init(fd, DW_DLC_READ, NULL,\
          \ NULL, &st->dbg, &err) != DW_DLV_OK) {\n        return NULL;\n    }\n    \n    // Iterate compilation units\n \
          \   Dwarf_Unsigned cu_header_length;\n    while (dwarf_next_cu_header(st->dbg, &cu_header_length, NULL, NULL, NULL,\
          \ NULL, &err) == DW_DLV_OK) {\n        Dwarf_Die cu_die;\n        dwarf_siblingof(st->dbg, NULL, &cu_die, &err);\n\
          \        \n        // Get line table\n        Dwarf_Line* lines;\n        Dwarf_Signed num_lines;\n        dwarf_srclines(cu_die,\
          \ &lines, &num_lines, &err);\n        \n        for (int i = 0; i < num_lines; i++) {\n            Dwarf_Addr addr;\n\
          \            Dwarf_Unsigned lineno;\n            char* file;\n            \n            dwarf_lineaddr(lines[i],\
          \ &addr, &err);\n            dwarf_lineno(lines[i], &lineno, &err);\n            dwarf_linesrc(lines[i], &file,\
          \ &err);\n            \n            // Store mapping\n            add_line_info(st, addr, lineno, file);\n     \
          \   }\n    }\n    \n    return st;\n}\n\nuint64_t symbol_to_addr(struct symbol_table* st, const char* name) {\n\
          \    // Search DIEs for function with matching name\n    // Return low_pc attribute\n}\n\nstruct line_info* addr_to_line(struct\
          \ symbol_table* st, uint64_t addr) {\n    // Binary search in sorted line table\n    // Return entry with largest\
          \ addr <= target\n}"
      pitfalls:
      - DWARF version differences
      - Inlined functions
      - Optimized code mapping
      concepts:
      - Debug information
      - Symbol tables
      - ELF/DWARF
      estimated_hours: 15-25
      deliverables:
      - DWARF debug information parser that reads compilation unit entries from the .debug_info ELF section
      - Function name to address mapping built from DWARF subprogram DIEs for breakpoint-by-name support
      - Source file and line number mapping from the .debug_line section for source-level stepping and display
      - Variable location information extracted from DWARF location lists for inspecting local and global variables
    - id: 4
      name: Variable Inspection
      description: Read and display variable values using debug info.
      acceptance_criteria:
      - Variable location is resolved from DWARF info to either a register, stack offset, or global memory address
      - Register and memory values are read correctly using ptrace and displayed as the appropriate data type
      - Different data types (int, float, char, pointer, array) are formatted and displayed in human-readable notation
      - Struct members are accessible by name, displaying each field with its type and current value
      hints:
        level1: DWARF location expressions describe where variable lives (register, stack, etc.).
        level2: Use DW_AT_type to get variable type. Recursively handle pointers, arrays, structs.
        level3: "// Variable location from DWARF\nenum loc_type { LOC_REG, LOC_ADDR, LOC_EXPR };\n\nstruct var_location {\n\
          \    enum loc_type type;\n    union {\n        int reg;           // Register number\n        uint64_t addr;   \
          \  // Memory address\n        Dwarf_Loc* expr;   // Location expression\n    };\n};\n\nstruct var_location get_var_location(struct\
          \ symbol_table* st, const char* var_name, uint64_t pc) {\n    // Find variable DIE\n    Dwarf_Die var_die = find_variable_die(st,\
          \ var_name, pc);\n    \n    // Get location attribute\n    Dwarf_Attribute loc_attr;\n    dwarf_attr(var_die, DW_AT_location,\
          \ &loc_attr, NULL);\n    \n    Dwarf_Loc* locs;\n    Dwarf_Signed num_locs;\n    dwarf_loclist(loc_attr, &locs,\
          \ &num_locs, NULL);\n    \n    // Interpret first location atom\n    struct var_location result;\n    \n    switch\
          \ (locs[0].lr_atom) {\n        case DW_OP_reg0 ... DW_OP_reg31:\n            result.type = LOC_REG;\n          \
          \  result.reg = locs[0].lr_atom - DW_OP_reg0;\n            break;\n        case DW_OP_fbreg:  // Offset from frame\
          \ base\n            result.type = LOC_ADDR;\n            result.addr = get_frame_base(dbg) + locs[0].lr_number;\n\
          \            break;\n        // ... handle other cases\n    }\n    \n    return result;\n}\n\nchar* read_variable(struct\
          \ debugger* dbg, struct symbol_table* st, const char* name) {\n    uint64_t pc = get_pc(dbg);\n    struct var_location\
          \ loc = get_var_location(st, name, pc);\n    Dwarf_Die type_die = get_var_type(st, name, pc);\n    \n    uint64_t\
          \ value;\n    if (loc.type == LOC_REG) {\n        value = get_register_value(dbg, loc.reg);\n    } else {\n    \
          \    value = ptrace(PTRACE_PEEKDATA, dbg->pid, loc.addr, NULL);\n    }\n    \n    // Format based on type\n    return\
          \ format_value(value, type_die);\n}"
      pitfalls:
      - Optimized-out variables
      - Complex location expressions
      - Type alignment
      concepts:
      - Variable location
      - Type information
      - Register access
      estimated_hours: 17-28
      deliverables:
      - Memory reader that fetches bytes at a variable's address using PTRACE_PEEKDATA for value inspection
      - Type-aware value formatter that interprets raw bytes as int, float, char, or pointer based on DWARF type info
      - Struct and array member access that navigates composite types using field offsets and element strides
      - Register value reader that retrieves CPU register contents using PTRACE_GETREGS for register-allocated variables
  build-distributed-kv:
    id: build-distributed-kv
    name: Build Your Own Distributed KV Store
    description: Build a distributed key-value store with partitioning and replication. Learn consistent hashing, shard management,
      and distributed transactions.
    difficulty: expert
    estimated_hours: 70-120
    prerequisites:
    - Build Raft or 2PC
    - Consistent hashing
    - RPC framework
    languages:
      recommended:
      - Go
      - Rust
      - Java
      also_possible:
      - C++
      - Scala
    resources:
    - type: paper
      name: 'Dynamo: Amazon''s Key-Value Store'
      url: https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf
    - type: course
      name: MIT 6.824 Distributed Systems
      url: https://pdos.csail.mit.edu/6.824/
    milestones:
    - id: 1
      name: Consistent Hashing
      description: Implement consistent hashing for key distribution across nodes.
      acceptance_criteria:
      - Hash ring with virtual nodes distributes keys uniformly across nodes with less than 10% standard deviation
      - Key lookup returns the correct responsible node by walking clockwise to the next token on the ring
      - Adding a node causes only keys between the new node and its predecessor to be redistributed
      - Removing a node causes only that node's keys to be redistributed to the next node on the ring
      hints:
        level1: Hash both keys and nodes onto ring. Key belongs to first node clockwise.
        level2: Virtual nodes improve balance. Each physical node has multiple positions.
        level3: "import hashlib\nfrom bisect import bisect_right\nfrom typing import List, Dict, Any\n\nclass ConsistentHash:\n\
          \    def __init__(self, virtual_nodes: int = 150):\n        self.virtual_nodes = virtual_nodes\n        self.ring\
          \ = []  # Sorted list of (hash, node_id)\n        self.nodes = {}  # node_id -> node_info\n    \n    def _hash(self,\
          \ key: str) -> int:\n        return int(hashlib.sha256(key.encode()).hexdigest(), 16)\n    \n    def add_node(self,\
          \ node_id: str, node_info: Any):\n        self.nodes[node_id] = node_info\n        \n        for i in range(self.virtual_nodes):\n\
          \            virtual_key = f'{node_id}:{i}'\n            hash_val = self._hash(virtual_key)\n            # Insert\
          \ maintaining sorted order\n            idx = bisect_right([h for h, _ in self.ring], hash_val)\n            self.ring.insert(idx,\
          \ (hash_val, node_id))\n    \n    def remove_node(self, node_id: str):\n        if node_id not in self.nodes:\n\
          \            return\n        \n        del self.nodes[node_id]\n        self.ring = [(h, n) for h, n in self.ring\
          \ if n != node_id]\n    \n    def get_node(self, key: str) -> str:\n        if not self.ring:\n            raise\
          \ NoNodesError()\n        \n        hash_val = self._hash(key)\n        idx = bisect_right([h for h, _ in self.ring],\
          \ hash_val)\n        \n        # Wrap around to first node if past end\n        if idx >= len(self.ring):\n    \
          \        idx = 0\n        \n        return self.ring[idx][1]\n    \n    def get_nodes(self, key: str, n: int) ->\
          \ List[str]:\n        \"\"\"Get n distinct nodes for replication\"\"\"\n        if len(self.nodes) < n:\n      \
          \      raise InsufficientNodes()\n        \n        hash_val = self._hash(key)\n        idx = bisect_right([h for\
          \ h, _ in self.ring], hash_val)\n        \n        result = []\n        seen = set()\n        \n        for i in\
          \ range(len(self.ring)):\n            _, node_id = self.ring[(idx + i) % len(self.ring)]\n            if node_id\
          \ not in seen:\n                result.append(node_id)\n                seen.add(node_id)\n                if len(result)\
          \ == n:\n                    break\n        \n        return result"
      pitfalls:
      - Hot spots with few nodes
      - Virtual node count tuning
      - Rebalancing overhead
      concepts:
      - Consistent hashing
      - Load balancing
      - Data partitioning
      estimated_hours: 10-15
      deliverables:
      - Hash ring implementation that maps keys to nodes using consistent hashing on a circular key space
      - Virtual node support that places multiple hash ring tokens per physical node for balanced load distribution
      - Node addition and removal with minimal key rehashing, only moving keys between adjacent ring positions
      - Key-to-node mapping function that locates the responsible node by walking clockwise from the key's hash position
    - id: 2
      name: Replication
      description: Implement replication with configurable consistency levels.
      acceptance_criteria:
      - Each key is replicated to N nodes, where N is the configurable replication factor per keyspace
      - Quorum operations enforce R + W > N so that reads and writes overlap on at least one up-to-date replica
      - Sloppy quorum and hinted handoff allow writes to proceed when a preferred replica is temporarily unavailable
      - Conflict resolution using vector clocks or last-write-wins timestamps handles concurrent writes to the same key
      hints:
        level1: For N=3, R=2, W=2 gives strong consistency. R=1, W=1 for availability.
        level2: 'Hinted handoff: if replica down, store hint at another node for later delivery.'
        level3: "from dataclasses import dataclass\nfrom typing import Optional\nimport time\n\n@dataclass\nclass VersionedValue:\n\
          \    value: bytes\n    version: int  # Vector clock or timestamp\n    timestamp: float\n\nclass ReplicatedStore:\n\
          \    def __init__(self, ring: ConsistentHash, n: int = 3, r: int = 2, w: int = 2):\n        self.ring = ring\n \
          \       self.n = n  # Replication factor\n        self.r = r  # Read quorum\n        self.w = w  # Write quorum\n\
          \        self.hints = {}  # target_node -> [(key, value)]\n    \n    async def put(self, key: str, value: bytes)\
          \ -> bool:\n        nodes = self.ring.get_nodes(key, self.n)\n        version = int(time.time() * 1000000)\n   \
          \     \n        versioned = VersionedValue(value, version, time.time())\n        \n        # Write to replicas\n\
          \        successes = 0\n        for node in nodes:\n            try:\n                await self._write_to_node(node,\
          \ key, versioned)\n                successes += 1\n            except NodeUnavailable:\n                # Hinted\
          \ handoff\n                hint_node = self._find_hint_node(nodes)\n                if hint_node:\n            \
          \        self._store_hint(hint_node, node, key, versioned)\n        \n        return successes >= self.w\n    \n\
          \    async def get(self, key: str) -> Optional[bytes]:\n        nodes = self.ring.get_nodes(key, self.n)\n     \
          \   \n        # Read from replicas\n        responses = []\n        for node in nodes:\n            try:\n     \
          \           val = await self._read_from_node(node, key)\n                if val:\n                    responses.append(val)\n\
          \                if len(responses) >= self.r:\n                    break\n            except NodeUnavailable:\n\
          \                continue\n        \n        if len(responses) < self.r:\n            raise ReadQuorumNotMet()\n\
          \        \n        # Return most recent version\n        latest = max(responses, key=lambda v: v.version)\n    \
          \    \n        # Read repair: update stale replicas\n        asyncio.create_task(self._read_repair(key, latest,\
          \ nodes))\n        \n        return latest.value\n    \n    async def _read_repair(self, key, latest, nodes):\n\
          \        for node in nodes:\n            try:\n                current = await self._read_from_node(node, key)\n\
          \                if not current or current.version < latest.version:\n                    await self._write_to_node(node,\
          \ key, latest)\n            except NodeUnavailable:\n                pass"
      pitfalls:
      - Quorum calculation
      - Read repair races
      - Hint delivery ordering
      concepts:
      - Quorum consensus
      - Eventual consistency
      - Hinted handoff
      estimated_hours: 15-25
      deliverables:
      - Configurable replication factor N that stores each key-value pair on N consecutive ring nodes
      - Quorum read and write operations where reads require R replies and writes require W acks (R + W > N)
      - Replica synchronization protocol that propagates updates to all replicas eventually
      - Anti-entropy repair using Merkle tree comparison to detect and fix divergent replicas
    - id: 3
      name: Cluster Management
      description: Implement cluster membership and failure detection.
      acceptance_criteria:
      - Gossip protocol disseminates membership changes to all nodes within a bounded number of communication rounds
      - Failure detection identifies unresponsive nodes within a configurable timeout and marks them as suspect or down
      - Automatic rebalancing redistributes key ownership when nodes join or leave the cluster
      - Cluster state converges to a consistent view across all nodes within a bounded time after any membership change
      hints:
        level1: 'Gossip: periodically exchange state with random peers. State converges.'
        level2: Phi accrual failure detector adapts to network conditions.
        level3: "import random\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Set\n\n@dataclass\n\
          class NodeState:\n    status: str  # 'alive', 'suspect', 'dead'\n    heartbeat: int\n    last_seen: float = field(default_factory=time.time)\n\
          \nclass GossipProtocol:\n    def __init__(self, node_id: str, seeds: List[str]):\n        self.node_id = node_id\n\
          \        self.seeds = seeds\n        self.members: Dict[str, NodeState] = {}\n        self.heartbeat = 0\n     \
          \   self.suspect_timeout = 5.0\n        self.dead_timeout = 30.0\n    \n    async def run(self, interval: float\
          \ = 1.0):\n        while True:\n            await asyncio.sleep(interval)\n            self.heartbeat += 1\n   \
          \         \n            # Update own state\n            self.members[self.node_id] = NodeState('alive', self.heartbeat)\n\
          \            \n            # Gossip to random peer\n            peers = [n for n in self.members if n != self.node_id]\n\
          \            if not peers:\n                peers = self.seeds\n            \n            target = random.choice(peers)\n\
          \            await self._gossip_to(target)\n            \n            # Check for failures\n            self._detect_failures()\n\
          \    \n    async def _gossip_to(self, target: str):\n        try:\n            # Send our view, receive theirs\n\
          \            their_state = await rpc_call(target, 'gossip', self.members)\n            self._merge_state(their_state)\n\
          \        except Exception:\n            pass  # Target unreachable\n    \n    def _merge_state(self, remote: Dict[str,\
          \ NodeState]):\n        for node_id, state in remote.items():\n            if node_id not in self.members:\n   \
          \             self.members[node_id] = state\n            elif state.heartbeat > self.members[node_id].heartbeat:\n\
          \                self.members[node_id] = state\n                self.members[node_id].last_seen = time.time()\n\
          \    \n    def _detect_failures(self):\n        now = time.time()\n        for node_id, state in self.members.items():\n\
          \            if node_id == self.node_id:\n                continue\n            \n            age = now - state.last_seen\n\
          \            \n            if state.status == 'alive' and age > self.suspect_timeout:\n                state.status\
          \ = 'suspect'\n                self._on_node_suspect(node_id)\n            elif state.status == 'suspect' and age\
          \ > self.dead_timeout:\n                state.status = 'dead'\n                self._on_node_dead(node_id)\n   \
          \ \n    def _on_node_dead(self, node_id: str):\n        # Trigger rebalancing\n        self.ring.remove_node(node_id)\n\
          \        # Move data from failed node's range\n        asyncio.create_task(self._rebalance_for_failure(node_id))"
      pitfalls:
      - Gossip message size
      - Split brain
      - Cascade failures
      concepts:
      - Gossip protocols
      - Failure detection
      - Cluster membership
      estimated_hours: 18-30
      deliverables:
      - Node discovery and membership service that maintains the list of active cluster members
      - Gossip protocol that propagates cluster state changes (joins, leaves, failures) across all nodes
      - Failure detection using periodic heartbeats and phi-accrual or timeout-based suspicion mechanisms
      - Hinted handoff for failed nodes that temporarily stores writes and replays them when the node recovers
    - id: 4
      name: Transactions
      description: Implement distributed transactions across shards.
      acceptance_criteria:
      - Single-key operations provide linearizability so reads always return the latest committed write
      - Multi-key transactions using two-phase commit atomically commit or abort across all involved keys and nodes
      - Optimistic concurrency control detects conflicting writes at commit time and aborts the losing transaction
      - Deadlock detection or prevention mechanism avoids indefinite blocking when two transactions wait on each other
      hints:
        level1: 'Single-key: use Raft per shard. Multi-key: coordinate with 2PC.'
        level2: 'Optimistic: validate reads at commit time. Abort on conflict.'
        level3: "from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Set\nimport uuid\n\
          \nclass TxnState(Enum):\n    ACTIVE = 'active'\n    PREPARED = 'prepared'\n    COMMITTED = 'committed'\n    ABORTED\
          \ = 'aborted'\n\n@dataclass\nclass Transaction:\n    id: str\n    read_set: Dict[str, int]  # key -> version read\n\
          \    write_set: Dict[str, bytes]  # key -> new value\n    state: TxnState = TxnState.ACTIVE\n\nclass TransactionCoordinator:\n\
          \    def __init__(self, store: ReplicatedStore):\n        self.store = store\n        self.transactions: Dict[str,\
          \ Transaction] = {}\n    \n    def begin(self) -> str:\n        txn_id = str(uuid.uuid4())\n        self.transactions[txn_id]\
          \ = Transaction(id=txn_id, read_set={}, write_set={})\n        return txn_id\n    \n    async def read(self, txn_id:\
          \ str, key: str) -> bytes:\n        txn = self.transactions[txn_id]\n        \n        # Check write set first (read\
          \ your writes)\n        if key in txn.write_set:\n            return txn.write_set[key]\n        \n        # Read\
          \ from store\n        value, version = await self.store.get_versioned(key)\n        txn.read_set[key] = version\n\
          \        return value\n    \n    def write(self, txn_id: str, key: str, value: bytes):\n        txn = self.transactions[txn_id]\n\
          \        txn.write_set[key] = value\n    \n    async def commit(self, txn_id: str) -> bool:\n        txn = self.transactions[txn_id]\n\
          \        \n        # Group keys by shard\n        shards = self._group_by_shard(set(txn.read_set) | set(txn.write_set))\n\
          \        \n        # Phase 1: Prepare\n        prepared = []\n        for shard, keys in shards.items():\n     \
          \       try:\n                # Lock keys, validate read set\n                success = await self._prepare_shard(shard,\
          \ txn, keys)\n                if not success:\n                    await self._abort_prepared(prepared, txn)\n \
          \                   return False\n                prepared.append(shard)\n            except Exception:\n      \
          \          await self._abort_prepared(prepared, txn)\n                return False\n        \n        # Phase 2:\
          \ Commit\n        for shard in prepared:\n            await self._commit_shard(shard, txn)\n        \n        txn.state\
          \ = TxnState.COMMITTED\n        return True\n    \n    async def _prepare_shard(self, shard, txn, keys) -> bool:\n\
          \        # Validate read versions haven't changed\n        for key in keys:\n            if key in txn.read_set:\n\
          \                current_version = await self.store.get_version(key)\n                if current_version != txn.read_set[key]:\n\
          \                    return False  # Conflict\n        \n        # Acquire locks\n        locked = await self.store.try_lock_keys(shard,\
          \ keys, txn.id)\n        return locked"
      pitfalls:
      - Coordinator failure during 2PC
      - Lock timeout tuning
      - Phantom reads
      concepts:
      - Distributed transactions
      - 2PC
      - Serializability
      estimated_hours: 27-50
      deliverables:
      - Single-key transactions using compare-and-swap (CAS) for atomic read-modify-write operations
      - Multi-key transactions using two-phase commit or Percolator-style protocol for cross-key atomicity
      - Multi-version concurrency control (MVCC) that maintains versioned values for snapshot isolation reads
      - Conflict resolution strategy supporting last-write-wins timestamps or CRDT-based automatic merge
  build-dns:
    id: build-dns
    name: Build Your Own DNS Server
    description: Build a DNS server with recursive resolution.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - UDP networking
    - DNS protocol basics
    - Caching strategies
    languages:
      recommended:
      - Go
      - Rust
      - C
      also_possible:
      - Python
      - JavaScript
    resources:
    - type: rfc
      name: RFC 1035 - DNS
      url: https://tools.ietf.org/html/rfc1035
    - type: interactive
      name: CodeCrafters DNS
      url: https://app.codecrafters.io/courses/dns-server/overview
    - type: article
      name: How DNS Works
      url: https://howdns.works/
    milestones:
    - id: 1
      name: DNS Message Parsing
      description: Parse and construct DNS messages.
      acceptance_criteria:
      - Header parser correctly extracts the 16-bit ID, QR flag, opcode, rcode, and all section counts
      - Question section parser decodes domain names with label-length encoding and extracts QTYPE and QCLASS
      - Answer section parser reads resource records with name, type, class, TTL, and type-specific RDATA
      - Name compression using DNS pointer labels (0xC0 prefix) is correctly decoded during parsing
      - Message construction serializes a DNS response into the correct wire-format byte sequence
      hints:
        level1: 'DNS message: header (12 bytes) + question + answer + authority + additional'
        level2: Name compression uses pointers (0xC0 prefix) to reduce message size.
        level3: "## DNS Message Parsing\n\n```go\npackage dns\n\nimport (\n    \"bytes\"\n    \"encoding/binary\"\n)\n\ntype\
          \ Header struct {\n    ID      uint16\n    Flags   uint16\n    QDCount uint16  // Question count\n    ANCount uint16\
          \  // Answer count\n    NSCount uint16  // Authority count\n    ARCount uint16  // Additional count\n}\n\ntype Question\
          \ struct {\n    Name  string\n    Type  uint16\n    Class uint16\n}\n\ntype ResourceRecord struct {\n    Name  \
          \   string\n    Type     uint16\n    Class    uint16\n    TTL      uint32\n    RDLength uint16\n    RData    []byte\n\
          }\n\ntype Message struct {\n    Header    Header\n    Questions []Question\n    Answers   []ResourceRecord\n   \
          \ Authority []ResourceRecord\n    Additional []ResourceRecord\n}\n\nfunc ParseMessage(data []byte) (*Message, error)\
          \ {\n    reader := bytes.NewReader(data)\n    msg := &Message{}\n    \n    // Parse header\n    binary.Read(reader,\
          \ binary.BigEndian, &msg.Header)\n    \n    // Parse questions\n    for i := 0; i < int(msg.Header.QDCount); i++\
          \ {\n        q := Question{}\n        q.Name = parseName(data, reader)\n        binary.Read(reader, binary.BigEndian,\
          \ &q.Type)\n        binary.Read(reader, binary.BigEndian, &q.Class)\n        msg.Questions = append(msg.Questions,\
          \ q)\n    }\n    \n    // Parse answers\n    for i := 0; i < int(msg.Header.ANCount); i++ {\n        rr := parseRR(data,\
          \ reader)\n        msg.Answers = append(msg.Answers, rr)\n    }\n    \n    return msg, nil\n}\n\nfunc parseName(data\
          \ []byte, reader *bytes.Reader) string {\n    var name []string\n    for {\n        length, _ := reader.ReadByte()\n\
          \        \n        if length == 0 {\n            break\n        }\n        \n        // Check for compression pointer\n\
          \        if length&0xC0 == 0xC0 {\n            nextByte, _ := reader.ReadByte()\n            offset := int(length&0x3F)<<8\
          \ | int(nextByte)\n            // Follow pointer\n            ptrReader := bytes.NewReader(data[offset:])\n    \
          \        name = append(name, parseName(data, ptrReader))\n            break\n        }\n        \n        label\
          \ := make([]byte, length)\n        reader.Read(label)\n        name = append(name, string(label))\n    }\n    return\
          \ strings.Join(name, \".\")\n}\n\nfunc (m *Message) Serialize() []byte {\n    buf := new(bytes.Buffer)\n    \n \
          \   // Write header\n    binary.Write(buf, binary.BigEndian, m.Header)\n    \n    // Write questions\n    for _,\
          \ q := range m.Questions {\n        writeName(buf, q.Name)\n        binary.Write(buf, binary.BigEndian, q.Type)\n\
          \        binary.Write(buf, binary.BigEndian, q.Class)\n    }\n    \n    // Write answers\n    for _, rr := range\
          \ m.Answers {\n        writeRR(buf, rr)\n    }\n    \n    return buf.Bytes()\n}\n\nfunc writeName(buf *bytes.Buffer,\
          \ name string) {\n    parts := strings.Split(name, \".\")\n    for _, part := range parts {\n        buf.WriteByte(byte(len(part)))\n\
          \        buf.WriteString(part)\n    }\n    buf.WriteByte(0)  // Null terminator\n}\n```"
      pitfalls:
      - Compression pointer loops
      - Wrong byte order
      - Name not null-terminated
      concepts:
      - Binary protocols
      - Name compression
      - Message format
      estimated_hours: 10-15
      deliverables:
      - DNS header parser that reads the 12-byte header including ID, flags, and section counts
      - Question section parser that extracts the queried domain name, record type, and class
      - Resource record parser supporting A, AAAA, CNAME, MX, NS, SOA, and TXT record types
      - DNS message serializer that encodes header, questions, and resource records back into wire format bytes
    - id: 2
      name: Authoritative Server
      description: Respond to queries from local zone data.
      acceptance_criteria:
      - Zone file parser reads standard BIND-format zone files and loads records into an in-memory data structure
      - Query matching returns the correct record set for a given domain name and record type from the zone
      - SOA records are returned in the authority section for NXDOMAIN and negative responses
      - NS records are correctly included in the authority section for delegated subdomains
      - A and AAAA records are returned in the answer section when the queried name and type match a zone record
      hints:
        level1: Load zone data from file. Match queries against records.
        level2: Return NXDOMAIN for non-existent names. Set authority flag.
        level3: "## Authoritative Server\n\n```go\ntype Zone struct {\n    Origin  string\n    TTL     uint32\n    Records\
          \ map[string][]ResourceRecord\n}\n\nfunc LoadZone(filename string) (*Zone, error) {\n    zone := &Zone{\n      \
          \  Records: make(map[string][]ResourceRecord),\n    }\n    \n    // Parse zone file (simplified)\n    file, _ :=\
          \ os.Open(filename)\n    scanner := bufio.NewScanner(file)\n    \n    for scanner.Scan() {\n        line := scanner.Text()\n\
          \        if strings.HasPrefix(line, \";\") || line == \"\" {\n            continue\n        }\n        \n      \
          \  parts := strings.Fields(line)\n        name := parts[0]\n        if name == \"@\" {\n            name = zone.Origin\n\
          \        }\n        \n        rr := ResourceRecord{\n            Name:  name,\n            Class: 1,  // IN\n  \
          \      }\n        \n        // Parse type and data\n        switch parts[len(parts)-2] {\n        case \"A\":\n\
          \            rr.Type = 1\n            rr.RData = net.ParseIP(parts[len(parts)-1]).To4()\n        case \"AAAA\":\n\
          \            rr.Type = 28\n            rr.RData = net.ParseIP(parts[len(parts)-1]).To16()\n        case \"NS\":\n\
          \            rr.Type = 2\n            // Encode name\n        case \"MX\":\n            rr.Type = 15\n         \
          \   // Encode preference + exchange\n        }\n        \n        zone.Records[name] = append(zone.Records[name],\
          \ rr)\n    }\n    \n    return zone, nil\n}\n\ntype AuthServer struct {\n    zones map[string]*Zone\n}\n\nfunc (s\
          \ *AuthServer) HandleQuery(query *Message) *Message {\n    response := &Message{\n        Header: Header{\n    \
          \        ID:      query.Header.ID,\n            Flags:   0x8400,  // Response + Authoritative\n            QDCount:\
          \ query.Header.QDCount,\n        },\n        Questions: query.Questions,\n    }\n    \n    for _, q := range query.Questions\
          \ {\n        // Find matching zone\n        zone := s.findZone(q.Name)\n        if zone == nil {\n            response.Header.Flags\
          \ |= 0x0003  // NXDOMAIN\n            continue\n        }\n        \n        // Find matching records\n        records\
          \ := zone.Records[q.Name]\n        for _, rr := range records {\n            if rr.Type == q.Type || q.Type == 255\
          \ {  // 255 = ANY\n                response.Answers = append(response.Answers, rr)\n            }\n        }\n \
          \   }\n    \n    response.Header.ANCount = uint16(len(response.Answers))\n    return response\n}\n```"
      pitfalls:
      - Wildcard matching
      - CNAME chasing
      - Case insensitivity
      concepts:
      - Zone files
      - Record types
      - Authoritative responses
      estimated_hours: 10-15
      deliverables:
      - Zone file parser that reads BIND-format zone files with SOA, NS, A, AAAA, CNAME, and MX records
      - Query matching engine that finds the best-matching records in the zone data for a given question
      - SOA and NS record handling that populates the authority section for authoritative responses
      - Authority and additional section population that includes glue records for NS referrals
    - id: 3
      name: Recursive Resolver
      description: Implement recursive resolution from root servers.
      acceptance_criteria:
      - Iterative queries start at root servers and follow referrals down the DNS hierarchy to the authoritative server
      - NS referrals are followed correctly by querying the referred nameserver for the next level of the domain
      - Root hints bootstrap the resolver with the IP addresses of the root nameservers for initial queries
      - Glue records (A records for NS names in the additional section) are used to avoid circular resolution dependencies
      hints:
        level1: Start from root servers, follow NS referrals until authoritative answer.
        level2: Glue records provide IP addresses for nameservers in referrals.
        level3: "## Recursive Resolver\n\n```go\nvar RootServers = []string{\n    \"198.41.0.4\",   // a.root-servers.net\n\
          \    \"199.9.14.201\", // b.root-servers.net\n    // ... more root servers\n}\n\ntype Resolver struct {\n    cache\
          \ *Cache\n}\n\nfunc (r *Resolver) Resolve(name string, qtype uint16) ([]ResourceRecord, error) {\n    // Check cache\
          \ first\n    if cached := r.cache.Get(name, qtype); cached != nil {\n        return cached, nil\n    }\n    \n \
          \   // Start from root\n    nameservers := RootServers\n    \n    for {\n        // Query one of the nameservers\n\
          \        response, err := r.queryNS(nameservers[0], name, qtype)\n        if err != nil {\n            // Try next\
          \ nameserver\n            nameservers = nameservers[1:]\n            continue\n        }\n        \n        // Got\
          \ authoritative answer\n        if response.Header.Flags&0x0400 != 0 {  // AA flag\n            r.cache.Set(name,\
          \ qtype, response.Answers)\n            return response.Answers, nil\n        }\n        \n        // Got referral\
          \ - follow it\n        if len(response.Authority) > 0 {\n            nameservers = r.extractNS(response)\n     \
          \       continue\n        }\n        \n        // No answer, no referral\n        return nil, fmt.Errorf(\"resolution\
          \ failed\")\n    }\n}\n\nfunc (r *Resolver) queryNS(server, name string, qtype uint16) (*Message, error) {\n   \
          \ query := &Message{\n        Header: Header{\n            ID:      uint16(rand.Int()),\n            Flags:   0x0100,\
          \  // RD (Recursion Desired) = 0 for iterative\n            QDCount: 1,\n        },\n        Questions: []Question{{\n\
          \            Name:  name,\n            Type:  qtype,\n            Class: 1,\n        }},\n    }\n    \n    conn,\
          \ _ := net.Dial(\"udp\", server+\":53\")\n    defer conn.Close()\n    \n    conn.Write(query.Serialize())\n    \n\
          \    buf := make([]byte, 512)\n    n, _ := conn.Read(buf)\n    \n    return ParseMessage(buf[:n])\n}\n\nfunc (r\
          \ *Resolver) extractNS(response *Message) []string {\n    var servers []string\n    \n    // Get NS names from authority\
          \ section\n    nsNames := make(map[string]bool)\n    for _, rr := range response.Authority {\n        if rr.Type\
          \ == 2 {  // NS\n            nsNames[string(rr.RData)] = true\n        }\n    }\n    \n    // Look for glue records\
          \ (A records for NS)\n    for _, rr := range response.Additional {\n        if rr.Type == 1 {  // A record\n   \
          \         if nsNames[rr.Name] {\n                ip := net.IP(rr.RData)\n                servers = append(servers,\
          \ ip.String())\n            }\n        }\n    }\n    \n    // If no glue, need to resolve NS names separately\n\
          \    if len(servers) == 0 {\n        for name := range nsNames {\n            addrs, _ := r.Resolve(name, 1)  //\
          \ Resolve A record\n            for _, rr := range addrs {\n                servers = append(servers, net.IP(rr.RData).String())\n\
          \            }\n        }\n    }\n    \n    return servers\n}\n```"
      pitfalls:
      - Infinite referral loops
      - Missing glue records
      - CNAME handling
      concepts:
      - Iterative resolution
      - Referrals
      - DNS hierarchy
      estimated_hours: 12-18
      deliverables:
      - Root server hints file containing the IP addresses of the 13 DNS root servers for bootstrapping resolution
      - Iterative query engine that follows NS referrals from root servers down to the authoritative server
      - CNAME following logic that transparently resolves CNAME chains to the final target record
      - Response construction that assembles the final answer from the results of the iterative resolution process
    - id: 4
      name: Caching & Performance
      description: Implement caching with TTL and negative caching.
      acceptance_criteria:
      - Cached records are returned immediately and evicted automatically when their TTL countdown reaches zero
      - Negative caching stores NXDOMAIN responses to avoid repeated queries for non-existent domains
      - Cache poisoning is prevented by validating that response records match the queried domain and ignoring out-of-bailiwick
        data
      - UDP server handles concurrent queries from multiple clients without blocking on any single resolution
      hints:
        level1: Cache answers with TTL countdown. Negative cache NXDOMAIN responses.
        level2: Validate response matches query. Use random query IDs.
        level3: "## DNS Cache\n\n```go\ntype CacheEntry struct {\n    Records    []ResourceRecord\n    Expiration time.Time\n\
          \    Negative   bool  // NXDOMAIN cache\n}\n\ntype Cache struct {\n    mu      sync.RWMutex\n    entries map[string]*CacheEntry\
          \  // key: name+type\n}\n\nfunc (c *Cache) makeKey(name string, qtype uint16) string {\n    return fmt.Sprintf(\"\
          %s:%d\", strings.ToLower(name), qtype)\n}\n\nfunc (c *Cache) Get(name string, qtype uint16) []ResourceRecord {\n\
          \    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    entry, ok := c.entries[c.makeKey(name, qtype)]\n    if !ok\
          \ {\n        return nil\n    }\n    \n    if time.Now().After(entry.Expiration) {\n        return nil  // Expired\n\
          \    }\n    \n    if entry.Negative {\n        return []ResourceRecord{}  // Negative cache hit\n    }\n    \n \
          \   // Adjust TTLs in returned records\n    remaining := uint32(entry.Expiration.Sub(time.Now()).Seconds())\n  \
          \  records := make([]ResourceRecord, len(entry.Records))\n    for i, rr := range entry.Records {\n        records[i]\
          \ = rr\n        records[i].TTL = remaining\n    }\n    \n    return records\n}\n\nfunc (c *Cache) Set(name string,\
          \ qtype uint16, records []ResourceRecord) {\n    if len(records) == 0 {\n        return\n    }\n    \n    // Use\
          \ minimum TTL\n    minTTL := records[0].TTL\n    for _, rr := range records[1:] {\n        if rr.TTL < minTTL {\n\
          \            minTTL = rr.TTL\n        }\n    }\n    \n    c.mu.Lock()\n    c.entries[c.makeKey(name, qtype)] = &CacheEntry{\n\
          \        Records:    records,\n        Expiration: time.Now().Add(time.Duration(minTTL) * time.Second),\n    }\n\
          \    c.mu.Unlock()\n}\n\nfunc (c *Cache) SetNegative(name string, qtype uint16, ttl uint32) {\n    c.mu.Lock()\n\
          \    c.entries[c.makeKey(name, qtype)] = &CacheEntry{\n        Negative:   true,\n        Expiration: time.Now().Add(time.Duration(ttl)\
          \ * time.Second),\n    }\n    c.mu.Unlock()\n}\n\n// Server with validation\nfunc (s *Server) handleQuery(addr *net.UDPAddr,\
          \ query *Message) {\n    response := s.resolver.Resolve(query)\n    \n    // Validate response\n    if response.Header.ID\
          \ != query.Header.ID {\n        return  // Ignore mismatched response\n    }\n    \n    s.conn.WriteToUDP(response.Serialize(),\
          \ addr)\n}\n```"
      pitfalls:
      - TTL underflow
      - Cache poisoning
      - Memory exhaustion
      concepts:
      - Caching strategies
      - Security
      - Concurrency
      estimated_hours: 10-15
      deliverables:
      - TTL-based cache that stores resolved records and evicts them when their time-to-live expires
      - Negative caching that stores NXDOMAIN and NODATA responses with the SOA minimum TTL for the zone
      - Cache lookup that checks for cached answers before initiating a recursive resolution query
      - Concurrent query handler that processes multiple DNS requests simultaneously using async I/O or threading
  build-docker:
    id: build-docker
    name: Build Your Own Docker
    description: Build a container runtime that can run isolated processes using Linux primitives. You'll learn about namespaces,
      cgroups, and filesystem isolation.
    difficulty: expert
    estimated_hours: 30-50
    prerequisites:
    - Linux system administration
    - Process management (fork, exec)
    - Filesystem concepts
    - Basic networking
    languages:
      recommended:
      - Go
      - C
      - Rust
      also_possible: []
    resources:
    - name: Containers from Scratch
      url: https://ericchiang.github.io/post/containers-from-scratch/
      type: blog
    - name: Containers from Scratch (Video) - Liz Rice
      url: https://www.youtube.com/watch?v=8fi7uSYlOdc
      type: video
    - name: Linux Namespaces man page
      url: https://man7.org/linux/man-pages/man7/namespaces.7.html
      type: documentation
    - name: OCI Runtime Specification
      url: https://github.com/opencontainers/runtime-spec
      type: documentation
    milestones:
    - id: 1
      name: Process Isolation (Namespaces)
      description: Isolate a process using Linux namespaces (PID, UTS, mount).
      acceptance_criteria:
      - Process runs inside its own PID namespace and sees itself as PID 1 with no visibility of host processes
      - Process has its own hostname via UTS namespace that can be set independently of the host's hostname
      - Process has its own mount namespace so mount and unmount operations do not affect the host filesystem
      - Namespaces are created using clone() with appropriate flags or unshare() after fork for each namespace type
      hints:
        level1: Use CLONE_NEWPID, CLONE_NEWUTS, CLONE_NEWNS flags with clone().
        level2: 'unshare command in bash: unshare --pid --uts --mount --fork /bin/bash'
        level3: |-
          Linux namespace types:
          - PID: Process IDs
          - UTS: Hostname
          - Mount: Filesystem mounts
          - Net: Network stack
          - User: User/group IDs
          - IPC: Inter-process comm
          - Cgroup: Cgroup root
      pitfalls:
      - Not using CLONE_NEWPID correctly
      - /proc still showing host PIDs
      - Permission issues (usually need root)
      concepts:
      - Linux namespaces
      - Process isolation
      - clone() syscall
      estimated_hours: 4-6
      deliverables:
      - PID namespace creation that gives the container process its own process ID space starting at PID 1
      - Network namespace setup that provides an isolated network stack with its own interfaces and routing table
      - Mount namespace for filesystem isolation ensuring the container has its own mount point hierarchy
      - UTS namespace that allows the container to have its own hostname independent of the host system
    - id: 2
      name: Resource Limits (cgroups)
      description: Limit container resources using cgroups (CPU, memory).
      acceptance_criteria:
      - Memory limit enforcement causes the container process to be OOM-killed if it exceeds the configured limit
      - CPU shares or quota limit the container's CPU consumption to the configured percentage of available cycles
      - Cgroup filesystem hierarchy is correctly set up with the container's limits written before the process starts
      - Container process is added to the cgroup before exec so all resource limits are in effect from the start
      hints:
        level1: Create directory in /sys/fs/cgroup/memory/mycontainer. Write PID to tasks file.
        level2: Set memory.limit_in_bytes for memory limit. Use cpu.cfs_quota_us for CPU.
        level3: |-
          Cgroup resource limits in C (cgroup v2):

          #include <stdio.h>
          #include <fcntl.h>
          #include <unistd.h>

          #define CGROUP_ROOT "/sys/fs/cgroup"

          void write_file(const char *path, const char *content) {
              int fd = open(path, O_WRONLY);
              if (fd < 0) { perror("open"); return; }
              write(fd, content, strlen(content));
              close(fd);
          }

          void setup_cgroup(const char *container_id, pid_t pid,
                            long memory_limit_bytes, int cpu_quota_percent) {
              char path[256], buf[64];

              // Create cgroup directory
              snprintf(path, sizeof(path), "%s/%s", CGROUP_ROOT, container_id);
              mkdir(path, 0755);

              // Set memory limit (e.g., 512MB)
              snprintf(path, sizeof(path), "%s/%s/memory.max",
                       CGROUP_ROOT, container_id);
              snprintf(buf, sizeof(buf), "%ld", memory_limit_bytes);
              write_file(path, buf);

              // Set CPU quota (e.g., 50% = "50000 100000")
              snprintf(path, sizeof(path), "%s/%s/cpu.max",
                       CGROUP_ROOT, container_id);
              snprintf(buf, sizeof(buf), "%d 100000", cpu_quota_percent * 1000);
              write_file(path, buf);

              // Set PIDs limit (prevent fork bombs)
              snprintf(path, sizeof(path), "%s/%s/pids.max",
                       CGROUP_ROOT, container_id);
              write_file(path, "100");

              // Add process to cgroup
              snprintf(path, sizeof(path), "%s/%s/cgroup.procs",
                       CGROUP_ROOT, container_id);
              snprintf(buf, sizeof(buf), "%d", pid);
              write_file(path, buf);
          }
      pitfalls:
      - cgroup v1 vs v2 differences
      - Not cleaning up cgroups on exit
      - Memory limits not accounting kernel memory
      concepts:
      - Control groups
      - Resource limiting
      - OOM killer
      estimated_hours: 3-4
      deliverables:
      - Memory limit cgroup configuration that caps the container's resident memory usage at a specified value
      - CPU shares and quota cgroup settings that control the container's share of CPU time
      - PIDs limit cgroup that restricts the number of processes a container can create to prevent fork bombs
      - Cgroup filesystem interaction that writes limit values to the correct cgroup v1 or v2 control files
    - id: 3
      name: Filesystem Isolation (chroot/pivot_root)
      description: Give container its own root filesystem using chroot or pivot_root.
      acceptance_criteria:
      - Container sees only its own filesystem tree and cannot access any host filesystem paths
      - Host filesystem is completely inaccessible from within the container after pivot_root and old root unmount
      - Proc filesystem is mounted at /proc inside the container so tools like ps and top work correctly
      - Container works with any base root filesystem such as Alpine, Ubuntu, or Debian extracted from an image
      hints:
        level1: chroot /path/to/rootfs /bin/sh - simplest form
        level2: pivot_root is more secure than chroot. Requires mount namespace.
        level3: |-
          Filesystem isolation with pivot_root in C:

          #define _GNU_SOURCE
          #include <sched.h>
          #include <sys/mount.h>
          #include <sys/syscall.h>

          int pivot_root(const char *new_root, const char *put_old) {
              return syscall(SYS_pivot_root, new_root, put_old);
          }

          void setup_rootfs(const char *rootfs_path) {
              // Make new root a mount point
              mount(rootfs_path, rootfs_path, NULL, MS_BIND | MS_REC, NULL);

              // Create directory for old root
              char old_root[256];
              snprintf(old_root, sizeof(old_root), "%s/.old_root", rootfs_path);
              mkdir(old_root, 0755);

              // Pivot root
              if (pivot_root(rootfs_path, old_root) != 0) {
                  perror("pivot_root failed");
                  exit(1);
              }

              // Change to new root
              chdir("/");

              // Unmount old root
              umount2("/.old_root", MNT_DETACH);
              rmdir("/.old_root");

              // Mount essential filesystems
              mount("proc", "/proc", "proc", 0, NULL);
              mount("sysfs", "/sys", "sysfs", 0, NULL);
              mount("tmpfs", "/tmp", "tmpfs", 0, NULL);
          }
      pitfalls:
      - chroot is escapable without mount namespace
      - Forgetting to mount /proc
      - Not having required binaries in rootfs
      concepts:
      - chroot jails
      - pivot_root
      - Root filesystem
      estimated_hours: 3-4
      deliverables:
      - Root filesystem extraction that unpacks a tarball or image layers into a directory to serve as the container root
      - pivot_root() call that atomically swaps the container's root filesystem and isolates it from the host
      - Mounting /proc and /sys inside the container so process and system information is available within the namespace
      - Old root unmount that removes access to the host filesystem from within the container after pivot_root
    - id: 4
      name: Layered Filesystem (OverlayFS)
      description: Implement layered filesystem for efficient image storage.
      acceptance_criteria:
      - Multiple read-only lower layers are merged into a single unified view visible to the container process
      - All filesystem writes are captured in the upper (writable) layer without modifying any lower layer
      - Copy-on-write semantics allow reading from lower layers and writing modifications only to the upper layer
      - Common image layers are shared between containers so disk space is not duplicated for identical layers
      hints:
        level1: 'OverlayFS: mount -t overlay overlay -o lowerdir=base,upperdir=changes,workdir=work target'
        level2: 'Multiple lower dirs: lowerdir=layer3:layer2:layer1 (later = higher priority)'
        level3: |-
          Docker layer structure:
          /var/lib/docker/overlay2/
            <layer-id>/
              diff/    # Layer contents
              lower    # Link to parent
              work/    # OverlayFS work dir
      pitfalls:
      - OverlayFS doesn't support all filesystems
      - Renaming directories has copy-up overhead
      - Open file handles survive layer changes
      concepts:
      - Union filesystems
      - Copy-on-write
      - Docker image layers
      estimated_hours: 4-6
      deliverables:
      - OverlayFS mount configuration with lower (read-only), upper (writable), and work directory paths
      - Layer stacking logic that merges multiple read-only image layers into a unified filesystem view
      - Copy-on-write behavior where modifications to lower-layer files are written to the upper layer only
      - Layer caching and reuse mechanism that shares common layers across multiple container instances
    - id: 5
      name: Container Networking
      description: Set up network namespace with virtual ethernet pair.
      acceptance_criteria:
      - Container has its own isolated network stack with its own loopback, IP addresses, and routing table
      - A veth pair connects the container namespace to the host bridge so traffic can flow between them
      - Container can reach external internet addresses via NAT masquerading through the host's network interface
      - Containers on the same bridge network can communicate with each other using their assigned IP addresses
      hints:
        level1: ip netns add container; ip link add veth0 type veth peer name veth1
        level2: Move one end of veth into container namespace. Connect other to bridge.
        level3: |-
          Network namespace and veth setup in C:

          #include <sched.h>
          #include <net/if.h>
          #include <linux/rtnetlink.h>

          void setup_container_network(pid_t container_pid) {
              // Create veth pair
              system("ip link add veth0 type veth peer name veth1");

              // Move veth1 into container's network namespace
              char cmd[256];
              snprintf(cmd, sizeof(cmd),
                  "ip link set veth1 netns /proc/%d/ns/net", container_pid);
              system(cmd);

              // Configure host side
              system("ip link set veth0 up");
              system("ip addr add 172.17.0.1/16 dev veth0");

              // Configure container side (run in container ns)
              // ip link set veth1 up
              // ip addr add 172.17.0.2/16 dev veth1
              // ip route add default via 172.17.0.1

              // Enable NAT for outbound traffic
              system("iptables -t nat -A POSTROUTING -s 172.17.0.0/16 "
                     "-o eth0 -j MASQUERADE");
              system("echo 1 > /proc/sys/net/ipv4/ip_forward");
          }
      pitfalls:
      - DNS resolution (need /etc/resolv.conf)
      - iptables rules for masquerading
      - Bridge vs host network
      concepts:
      - Network namespaces
      - Virtual ethernet
      - Linux bridge networking
      estimated_hours: 5-8
      deliverables:
      - Virtual ethernet pair (veth) creation that connects the container's network namespace to the host
      - Bridge network setup that connects multiple container veth endpoints on a shared virtual switch
      - NAT and masquerade rules using iptables that allow containers to reach external networks through the host
      - Port forwarding rules that map host ports to container ports for accepting inbound connections
    - id: 6
      name: Image Format and CLI
      description: Implement OCI image format and Docker-compatible CLI.
      acceptance_criteria:
      - Image pull downloads all layers and the manifest from Docker Hub or any OCI-compliant registry
      - OCI image manifest is parsed to extract the layer digests, config, and entry point command
      - Filesystem layers are extracted and stacked using OverlayFS to create the container root filesystem
      - Container run command equivalent creates all isolation primitives and launches the specified process
      hints:
        level1: OCI image is tarball with manifest.json pointing to layer tarballs.
        level2: 'Docker Hub API: GET /v2/<name>/manifests/<tag>'
        level3: |-
          OCI image pulling and container CLI in Python:

          import json
          import requests
          import tarfile
          from pathlib import Path

          class ContainerRuntime:
              def __init__(self, root_dir="/var/lib/mycontainer"):
                  self.root_dir = Path(root_dir)
                  self.images_dir = self.root_dir / "images"
                  self.containers_dir = self.root_dir / "containers"

              def pull(self, image_name, tag="latest"):
                  # Get auth token
                  auth_url = f"https://auth.docker.io/token?service=registry.docker.io&scope=repository:library/{image_name}:pull"
                  token = requests.get(auth_url).json()["token"]
                  headers = {"Authorization": f"Bearer {token}"}

                  # Get manifest
                  manifest_url = f"https://registry-1.docker.io/v2/library/{image_name}/manifests/{tag}"
                  headers["Accept"] = "application/vnd.docker.distribution.manifest.v2+json"
                  manifest = requests.get(manifest_url, headers=headers).json()

                  # Download and extract layers
                  image_dir = self.images_dir / f"{image_name}_{tag}"
                  image_dir.mkdir(parents=True, exist_ok=True)

                  for layer in manifest["layers"]:
                      digest = layer["digest"]
                      blob_url = f"https://registry-1.docker.io/v2/library/{image_name}/blobs/{digest}"
                      response = requests.get(blob_url, headers=headers, stream=True)

                      with tarfile.open(fileobj=response.raw) as tar:
                          tar.extractall(image_dir)

                  return image_dir

              def run(self, image_name, command):
                  # Create container directory with overlayfs
                  container_id = os.urandom(8).hex()
                  container_dir = self.containers_dir / container_id
                  rootfs = container_dir / "rootfs"

                  # Setup namespaces, cgroups, rootfs, then exec
                  # ... (calls to unshare, pivot_root, cgroup setup)
                  os.execvp(command[0], command)
      pitfalls:
      - Image manifest v1 vs v2
      - Content-addressable storage
      - Image and layer deduplication
      concepts:
      - OCI specification
      - Container registry protocol
      - CLI design
      estimated_hours: 6-10
      deliverables:
      - OCI image specification parser that reads manifests, config, and layer descriptors from an image archive
      - Image pull client that downloads image layers and manifests from a Docker registry over HTTP
      - Container run command that creates namespaces, applies cgroups, mounts the filesystem, and executes the entry point
      - Container lifecycle management with start, stop, and remove commands for managing running containers
  build-emulator:
    id: build-emulator
    name: Build Your Own Emulator
    description: Build a NES, GameBoy, or CHIP-8 emulator. Learn CPU emulation, memory mapping, and timing.
    difficulty: expert
    estimated_hours: 60-120
    prerequisites:
    - Binary/hex
    - Assembly basics
    - Graphics basics
    languages:
      recommended:
      - C
      - Rust
      - C++
      also_possible:
      - Go
      - TypeScript
    resources:
    - type: guide
      name: Writing a CHIP-8 Emulator
      url: https://tobiasvl.github.io/blog/write-a-chip-8-emulator/
    - type: docs
      name: Pan Docs (Game Boy)
      url: https://gbdev.io/pandocs/
    - type: wiki
      name: NesDev Wiki
      url: https://www.nesdev.org/wiki/Nesdev_Wiki
    milestones:
    - id: 1
      name: CPU Emulation
      description: Implement the instruction set and registers.
      acceptance_criteria:
      - All CPU registers are implemented with correct bit widths matching the target architecture specification
      - Fetch-decode-execute cycle reads the opcode at the program counter, decodes it, and dispatches the correct handler
      - All documented opcodes of the target ISA are implemented and produce correct register and memory side effects
      - CPU flags (zero, carry, half-carry, negative) are updated correctly after each arithmetic and logic instruction
      hints:
        level1: CHIP-8 has 35 opcodes. Game Boy has ~500. Start simple.
        level2: Use a big switch statement or function pointer table for opcodes.
        level3: "// CHIP-8 CPU example\nstruct CPU {\n    uint8_t V[16];      // General registers V0-VF\n    uint16_t I;\
          \         // Index register\n    uint16_t PC;        // Program counter\n    uint8_t SP;         // Stack pointer\n\
          \    uint16_t stack[16]; // Call stack\n    uint8_t delay_timer;\n    uint8_t sound_timer;\n};\n\nvoid cpu_init(struct\
          \ CPU* cpu) {\n    memset(cpu, 0, sizeof(struct CPU));\n    cpu->PC = 0x200;  // Programs start at 0x200\n}\n\n\
          void cpu_cycle(struct CPU* cpu, uint8_t* memory) {\n    // Fetch: 2-byte opcode\n    uint16_t opcode = (memory[cpu->PC]\
          \ << 8) | memory[cpu->PC + 1];\n    cpu->PC += 2;\n    \n    // Decode and execute\n    uint8_t x = (opcode >> 8)\
          \ & 0x0F;  // Second nibble\n    uint8_t y = (opcode >> 4) & 0x0F;  // Third nibble\n    uint8_t n = opcode & 0x0F;\
          \         // Fourth nibble\n    uint8_t nn = opcode & 0xFF;        // Second byte\n    uint16_t nnn = opcode & 0x0FFF;\
          \    // Last 12 bits\n    \n    switch (opcode & 0xF000) {\n        case 0x0000:\n            switch (opcode) {\n\
          \                case 0x00E0: // CLS - Clear screen\n                    clear_display();\n                    break;\n\
          \                case 0x00EE: // RET - Return\n                    cpu->SP--;\n                    cpu->PC = cpu->stack[cpu->SP];\n\
          \                    break;\n            }\n            break;\n        \n        case 0x1000: // JP addr - Jump\n\
          \            cpu->PC = nnn;\n            break;\n        \n        case 0x2000: // CALL addr\n            cpu->stack[cpu->SP]\
          \ = cpu->PC;\n            cpu->SP++;\n            cpu->PC = nnn;\n            break;\n        \n        case 0x3000:\
          \ // SE Vx, byte - Skip if equal\n            if (cpu->V[x] == nn) cpu->PC += 2;\n            break;\n        \n\
          \        case 0x6000: // LD Vx, byte\n            cpu->V[x] = nn;\n            break;\n        \n        case 0x7000:\
          \ // ADD Vx, byte\n            cpu->V[x] += nn;\n            break;\n        \n        case 0x8000: // Arithmetic\n\
          \            switch (n) {\n                case 0x0: cpu->V[x] = cpu->V[y]; break;  // LD\n                case\
          \ 0x1: cpu->V[x] |= cpu->V[y]; break; // OR\n                case 0x2: cpu->V[x] &= cpu->V[y]; break; // AND\n \
          \               case 0x4: // ADD with carry\n                    cpu->V[0xF] = (cpu->V[x] + cpu->V[y]) > 255 ? 1\
          \ : 0;\n                    cpu->V[x] += cpu->V[y];\n                    break;\n                // ... more arithmetic\
          \ ops\n            }\n            break;\n        \n        case 0xD000: // DRW - Draw sprite\n            draw_sprite(cpu,\
          \ memory, x, y, n);\n            break;\n        // ... remaining opcodes\n    }\n}"
      pitfalls:
      - Endianness
      - Flag edge cases
      - Undocumented opcodes
      concepts:
      - CPU architecture
      - Instruction sets
      - Machine code
      estimated_hours: 15-30
      deliverables:
      - Instruction fetch-decode-execute loop that reads opcodes from memory and dispatches to handlers
      - Register file implementation with all general-purpose, flag, and special-purpose registers of the target CPU
      - ALU operations implementing arithmetic, logic, shift, and rotate instructions for the target ISA
      - Program counter management and CPU flags handling (zero, carry, negative, overflow) updated per instruction
    - id: 2
      name: Memory System
      description: Implement memory mapping and bank switching.
      acceptance_criteria:
      - Memory map implements the correct address ranges for ROM, RAM, and I/O regions per the target platform spec
      - ROM loading places the program binary at the correct start address and enforces read-only access
      - RAM regions support read and write operations with correct byte-level addressing and initial state
      - Memory-mapped I/O reads and writes trigger the correct peripheral handler rather than accessing raw memory
      hints:
        level1: 'CHIP-8 is simple: 4KB flat memory. Game Boy has ROM banking.'
        level2: 'Memory-mapped I/O: writes to certain addresses control hardware.'
        level3: "// Game Boy memory map\nstruct MMU {\n    uint8_t* rom;           // Cartridge ROM\n    uint8_t* rom_banks;\
          \     // Switchable ROM banks\n    uint8_t wram[8192];     // Work RAM\n    uint8_t vram[8192];     // Video RAM\n\
          \    uint8_t oam[160];       // Sprite attribute memory\n    uint8_t io[128];        // I/O registers\n    uint8_t\
          \ hram[127];      // High RAM\n    uint8_t ie;             // Interrupt enable\n    \n    int rom_bank;        \
          \   // Current ROM bank number\n    int mbc_type;           // Memory Bank Controller type\n};\n\nuint8_t mmu_read(struct\
          \ MMU* mmu, uint16_t addr) {\n    switch (addr & 0xF000) {\n        case 0x0000:\n        case 0x1000:\n       \
          \ case 0x2000:\n        case 0x3000:\n            // ROM bank 0 (fixed)\n            return mmu->rom[addr];\n  \
          \      \n        case 0x4000:\n        case 0x5000:\n        case 0x6000:\n        case 0x7000:\n            //\
          \ Switchable ROM bank\n            return mmu->rom_banks[(mmu->rom_bank - 1) * 0x4000 + (addr - 0x4000)];\n    \
          \    \n        case 0x8000:\n        case 0x9000:\n            // Video RAM\n            return mmu->vram[addr -\
          \ 0x8000];\n        \n        case 0xC000:\n        case 0xD000:\n            // Work RAM\n            return mmu->wram[addr\
          \ - 0xC000];\n        \n        case 0xF000:\n            if (addr >= 0xFF00 && addr <= 0xFF7F) {\n            \
          \    // I/O registers\n                return read_io(mmu, addr);\n            }\n            if (addr >= 0xFF80\
          \ && addr <= 0xFFFE) {\n                return mmu->hram[addr - 0xFF80];\n            }\n            if (addr ==\
          \ 0xFFFF) {\n                return mmu->ie;\n            }\n            break;\n    }\n    return 0xFF;  // Unmapped\n\
          }\n\nvoid mmu_write(struct MMU* mmu, uint16_t addr, uint8_t value) {\n    // Handle MBC bank switching\n    if (addr\
          \ >= 0x2000 && addr <= 0x3FFF) {\n        // ROM bank select\n        mmu->rom_bank = value & 0x1F;\n        if\
          \ (mmu->rom_bank == 0) mmu->rom_bank = 1;\n        return;\n    }\n    // ... handle other regions\n}"
      pitfalls:
      - Bank 0 special cases
      - Echo RAM
      - Write-only registers
      concepts:
      - Memory mapping
      - Bank switching
      - MMIO
      estimated_hours: 12-20
      deliverables:
      - Memory address space emulation providing a byte-addressable array covering the full address range of the target
      - Memory-mapped I/O regions that route reads and writes to peripheral device emulation handlers
      - ROM and RAM region definitions that enforce read-only access for ROM and read-write for RAM areas
      - Bank switching mechanism (if applicable) that swaps ROM or RAM banks based on mapper register writes
    - id: 3
      name: Graphics
      description: Implement the display/PPU (Picture Processing Unit).
      acceptance_criteria:
      - Screen rendering produces a pixel-accurate display output matching the target platform's resolution and palette
      - Sprites are rendered with correct positioning, priority, flipping, and transparency per the target hardware behavior
      - Background tiles are rendered using the tile map and tile data with correct scrolling offsets
      - Rendering timing matches the target hardware's scanline and frame timing to avoid visual artifacts
      hints:
        level1: 'CHIP-8: 64x32 monochrome, XOR drawing. Game Boy: tile-based with layers.'
        level2: Render line by line (scanline rendering). Check sprite priority.
        level3: "// Game Boy PPU (simplified)\nstruct PPU {\n    uint8_t lcdc;           // LCD Control\n    uint8_t stat;\
          \           // LCD Status\n    uint8_t scy, scx;       // Scroll Y/X\n    uint8_t ly;             // Current scanline\n\
          \    uint8_t lyc;            // LY Compare\n    uint8_t bgp;            // Background palette\n    uint8_t obp0,\
          \ obp1;     // Sprite palettes\n    \n    uint8_t* vram;\n    uint8_t* oam;\n    uint32_t framebuffer[160 * 144];\n\
          \    \n    int mode;               // 0=HBlank, 1=VBlank, 2=OAM, 3=Transfer\n    int cycles;\n};\n\nvoid ppu_step(struct\
          \ PPU* ppu, int cpu_cycles) {\n    ppu->cycles += cpu_cycles;\n    \n    switch (ppu->mode) {\n        case 2: \
          \ // OAM Scan (80 cycles)\n            if (ppu->cycles >= 80) {\n                ppu->cycles -= 80;\n          \
          \      ppu->mode = 3;\n            }\n            break;\n        \n        case 3:  // Pixel Transfer (172 cycles\
          \ avg)\n            if (ppu->cycles >= 172) {\n                ppu->cycles -= 172;\n                render_scanline(ppu);\n\
          \                ppu->mode = 0;\n            }\n            break;\n        \n        case 0:  // HBlank (204 cycles)\n\
          \            if (ppu->cycles >= 204) {\n                ppu->cycles -= 204;\n                ppu->ly++;\n      \
          \          \n                if (ppu->ly == 144) {\n                    ppu->mode = 1;  // VBlank\n            \
          \        request_interrupt(INT_VBLANK);\n                } else {\n                    ppu->mode = 2;\n        \
          \        }\n            }\n            break;\n        \n        case 1:  // VBlank (10 lines)\n            if (ppu->cycles\
          \ >= 456) {\n                ppu->cycles -= 456;\n                ppu->ly++;\n                \n               \
          \ if (ppu->ly > 153) {\n                    ppu->ly = 0;\n                    ppu->mode = 2;\n                }\n\
          \            }\n            break;\n    }\n}\n\nvoid render_scanline(struct PPU* ppu) {\n    if (!(ppu->lcdc & 0x80))\
          \ return;  // LCD off\n    \n    int y = ppu->ly;\n    \n    // Render background\n    if (ppu->lcdc & 0x01) {\n\
          \        for (int x = 0; x < 160; x++) {\n            int scrolled_x = (x + ppu->scx) & 255;\n            int scrolled_y\
          \ = (y + ppu->scy) & 255;\n            \n            // Get tile from tilemap\n            int tile_x = scrolled_x\
          \ / 8;\n            int tile_y = scrolled_y / 8;\n            int tile_idx = get_bg_tile(ppu, tile_x, tile_y);\n\
          \            \n            // Get pixel from tile\n            int pixel_x = scrolled_x % 8;\n            int pixel_y\
          \ = scrolled_y % 8;\n            int color = get_tile_pixel(ppu, tile_idx, pixel_x, pixel_y);\n            \n  \
          \          ppu->framebuffer[y * 160 + x] = apply_palette(color, ppu->bgp);\n        }\n    }\n    \n    // Render\
          \ sprites on top\n    if (ppu->lcdc & 0x02) {\n        render_sprites(ppu, y);\n    }\n}"
      pitfalls:
      - Mid-frame register changes
      - Sprite limit per line
      - Priority rules
      concepts:
      - Tile graphics
      - Scanline rendering
      - Sprite systems
      estimated_hours: 18-35
      deliverables:
      - Display buffer (framebuffer) storing pixel data at the target platform's native resolution and color depth
      - Sprite and tile rendering engine that composites sprite and background layers per the target graphics hardware
      - Scanline timing emulation that renders each horizontal line at the correct cycle count per the display timing spec
      - Screen output module that blits the framebuffer to a host window surface at the target refresh rate
    - id: 4
      name: Timing and Input
      description: Implement accurate timing and controller input.
      acceptance_criteria:
      - Cycle-accurate timing ensures CPU instructions consume the correct number of machine cycles per the ISA specification
      - Timer interrupts fire at the correct interval based on the timer register configuration and clock divider
      - Joypad input reads return the correct button state reflecting the currently pressed host keyboard or gamepad buttons
      - Audio output produces sound at the correct pitch and timing relative to the emulated CPU clock speed
      hints:
        level1: Each instruction takes specific cycles. Run PPU/timers in sync.
        level2: 'Game Boy: 4.19 MHz CPU, timer can fire at different rates.'
        level3: "// Main emulation loop with timing\nstruct Emulator {\n    struct CPU cpu;\n    struct MMU mmu;\n    struct\
          \ PPU ppu;\n    struct Timer timer;\n    uint8_t joypad;\n    \n    uint64_t total_cycles;\n};\n\nvoid emulator_frame(struct\
          \ Emulator* emu) {\n    // One frame = 70224 cycles (at 59.7 Hz)\n    const int CYCLES_PER_FRAME = 70224;\n    int\
          \ cycles_this_frame = 0;\n    \n    while (cycles_this_frame < CYCLES_PER_FRAME) {\n        // Handle interrupts\n\
          \        int interrupt_cycles = handle_interrupts(&emu->cpu, &emu->mmu);\n        \n        // Execute one instruction\n\
          \        int cpu_cycles = cpu_step(&emu->cpu, &emu->mmu);\n        \n        int total = interrupt_cycles + cpu_cycles;\n\
          \        \n        // Step other components\n        timer_step(&emu->timer, total);\n        ppu_step(&emu->ppu,\
          \ total);\n        \n        cycles_this_frame += total;\n        emu->total_cycles += total;\n    }\n}\n\n// Timer\
          \ implementation\nstruct Timer {\n    uint8_t div;    // Divider (increments at 16384 Hz)\n    uint8_t tima;   //\
          \ Timer counter\n    uint8_t tma;    // Timer modulo (reload value)\n    uint8_t tac;    // Timer control\n    \n\
          \    int div_counter;\n    int timer_counter;\n};\n\nvoid timer_step(struct Timer* t, int cycles) {\n    // DIV\
          \ increments every 256 cycles\n    t->div_counter += cycles;\n    while (t->div_counter >= 256) {\n        t->div_counter\
          \ -= 256;\n        t->div++;\n    }\n    \n    // TIMA if enabled\n    if (t->tac & 0x04) {\n        t->timer_counter\
          \ += cycles;\n        \n        int threshold;\n        switch (t->tac & 0x03) {\n            case 0: threshold\
          \ = 1024; break;  // 4096 Hz\n            case 1: threshold = 16; break;    // 262144 Hz\n            case 2: threshold\
          \ = 64; break;    // 65536 Hz\n            case 3: threshold = 256; break;   // 16384 Hz\n        }\n        \n\
          \        while (t->timer_counter >= threshold) {\n            t->timer_counter -= threshold;\n            t->tima++;\n\
          \            \n            if (t->tima == 0) {  // Overflow\n                t->tima = t->tma;  // Reload\n    \
          \            request_interrupt(INT_TIMER);\n            }\n        }\n    }\n}\n\n// Joypad\nuint8_t read_joypad(struct\
          \ Emulator* emu) {\n    uint8_t select = emu->mmu.io[0x00];  // P1 register\n    uint8_t result = select | 0x0F;\
          \  // Upper bits from select, lower bits high\n    \n    if (!(select & 0x10)) {  // Direction keys selected\n \
          \       if (emu->joypad & KEY_RIGHT) result &= ~0x01;\n        if (emu->joypad & KEY_LEFT)  result &= ~0x02;\n \
          \       if (emu->joypad & KEY_UP)    result &= ~0x04;\n        if (emu->joypad & KEY_DOWN)  result &= ~0x08;\n \
          \   }\n    if (!(select & 0x20)) {  // Button keys selected\n        if (emu->joypad & KEY_A)      result &= ~0x01;\n\
          \        if (emu->joypad & KEY_B)      result &= ~0x02;\n        if (emu->joypad & KEY_SELECT) result &= ~0x04;\n\
          \        if (emu->joypad & KEY_START)  result &= ~0x08;\n    }\n    \n    return result;\n}"
      pitfalls:
      - Cycle counting accuracy
      - Timer edge cases
      - VBlank timing
      concepts:
      - Cycle accuracy
      - Hardware timers
      - Input handling
      estimated_hours: 15-35
      deliverables:
      - Cycle-accurate timing system that tracks CPU cycles per instruction and synchronizes subsystems accordingly
      - Input device mapping that translates host keyboard or gamepad events to the target platform's input register format
      - Interrupt handling that triggers the correct CPU interrupt vector on timer overflow, VBlank, and I/O events
      - Audio timing synchronization that generates sound samples at the correct rate relative to CPU cycle execution
  build-game-engine:
    id: build-game-engine
    name: Build Your Own Game Engine
    description: Build a 2D/3D game engine with rendering, physics, and entity management.
    difficulty: expert
    estimated_hours: 100-200
    prerequisites:
    - Graphics programming basics
    - Linear algebra
    - C/C++ or Rust
    - Game architecture patterns
    languages:
      recommended:
      - C++
      - Rust
      - C
      also_possible:
      - Zig
    resources:
    - type: book
      name: Game Engine Architecture
      url: https://www.gameenginebook.com/
    - type: video
      name: Handmade Hero
      url: https://handmadehero.org/
    - type: tutorial
      name: Learn OpenGL
      url: https://learnopengl.com/
    milestones:
    - id: 1
      name: Window & Rendering Foundation
      description: Create window and basic rendering pipeline.
      acceptance_criteria:
      - Window is created with the requested resolution and title, and processes input events from the OS
      - OpenGL or Vulkan context is initialized and can clear the screen to a solid color each frame
      - Basic 2D sprite rendering draws textured rectangles at arbitrary positions, scales, and rotations
      - Texture loading reads PNG or JPG images from disk and uploads them to GPU texture objects
      - Shader system compiles vertex and fragment shaders from source and links them into a usable program
      hints:
        level1: Use SDL2 or GLFW for cross-platform windowing. Initialize OpenGL context.
        level2: Create vertex buffer for quad, write basic vertex/fragment shaders.
        level3: "## Rendering Foundation\n\n```cpp\n// Window and OpenGL setup\n#include <SDL2/SDL.h>\n#include <glad/glad.h>\n\
          \nclass Window {\npublic:\n    SDL_Window* window;\n    SDL_GLContext glContext;\n    int width, height;\n    \n\
          \    bool init(const char* title, int w, int h) {\n        SDL_Init(SDL_INIT_VIDEO);\n        \n        SDL_GL_SetAttribute(SDL_GL_CONTEXT_MAJOR_VERSION,\
          \ 4);\n        SDL_GL_SetAttribute(SDL_GL_CONTEXT_MINOR_VERSION, 1);\n        SDL_GL_SetAttribute(SDL_GL_CONTEXT_PROFILE_MASK,\
          \ SDL_GL_CONTEXT_PROFILE_CORE);\n        \n        window = SDL_CreateWindow(title, SDL_WINDOWPOS_CENTERED, SDL_WINDOWPOS_CENTERED,\n\
          \                                  w, h, SDL_WINDOW_OPENGL);\n        glContext = SDL_GL_CreateContext(window);\n\
          \        gladLoadGLLoader((GLADloadproc)SDL_GL_GetProcAddress);\n        \n        width = w; height = h;\n    \
          \    return true;\n    }\n    \n    void swap() { SDL_GL_SwapWindow(window); }\n};\n\n// Shader\nclass Shader {\n\
          public:\n    GLuint program;\n    \n    void compile(const char* vertSrc, const char* fragSrc) {\n        GLuint\
          \ vert = glCreateShader(GL_VERTEX_SHADER);\n        glShaderSource(vert, 1, &vertSrc, NULL);\n        glCompileShader(vert);\n\
          \        \n        GLuint frag = glCreateShader(GL_FRAGMENT_SHADER);\n        glShaderSource(frag, 1, &fragSrc,\
          \ NULL);\n        glCompileShader(frag);\n        \n        program = glCreateProgram();\n        glAttachShader(program,\
          \ vert);\n        glAttachShader(program, frag);\n        glLinkProgram(program);\n        \n        glDeleteShader(vert);\n\
          \        glDeleteShader(frag);\n    }\n    \n    void use() { glUseProgram(program); }\n    void setMat4(const char*\
          \ name, const float* mat) {\n        glUniformMatrix4fv(glGetUniformLocation(program, name), 1, GL_FALSE, mat);\n\
          \    }\n};\n\n// Sprite Batch for efficient 2D rendering\nclass SpriteBatch {\n    GLuint vao, vbo, ebo;\n    std::vector<Vertex>\
          \ vertices;\n    Shader* shader;\n    \npublic:\n    void begin() { vertices.clear(); }\n    \n    void draw(Texture*\
          \ tex, float x, float y, float w, float h) {\n        // Add 4 vertices for quad\n        vertices.push_back({{x,\
          \ y}, {0, 0}});\n        vertices.push_back({{x+w, y}, {1, 0}});\n        vertices.push_back({{x+w, y+h}, {1, 1}});\n\
          \        vertices.push_back({{x, y+h}, {0, 1}});\n    }\n    \n    void end() {\n        glBindBuffer(GL_ARRAY_BUFFER,\
          \ vbo);\n        glBufferData(GL_ARRAY_BUFFER, vertices.size() * sizeof(Vertex),\n                    vertices.data(),\
          \ GL_DYNAMIC_DRAW);\n        \n        shader->use();\n        glBindVertexArray(vao);\n        glDrawElements(GL_TRIANGLES,\
          \ (vertices.size() / 4) * 6, GL_UNSIGNED_INT, 0);\n    }\n};\n```"
      pitfalls:
      - OpenGL state leaks
      - Shader compilation errors not checked
      - Wrong vertex attribute setup
      concepts:
      - Graphics APIs
      - Shader programming
      - Batch rendering
      estimated_hours: 20-30
      deliverables:
      - Window creation and event loop using SDL2 or GLFW with configurable resolution and title
      - OpenGL or Vulkan graphics context setup with swap chain and basic render pipeline initialization
      - Game render loop with delta time calculation for frame-rate-independent updates and rendering
      - Basic sprite and mesh rendering pipeline that draws textured quads or 3D meshes to the screen
    - id: 2
      name: Entity Component System
      description: Implement ECS architecture for game objects.
      acceptance_criteria:
      - Entity creation returns a unique ID and entity destruction recycles the ID for future reuse
      - Component storage supports adding, removing, and retrieving components by entity ID in amortized O(1) time
      - System execution iterates over all entities with the required components and calls the system's update function
      - Component queries efficiently select entities matching a multi-component filter without scanning all entities
      - Entity destruction removes all associated components and marks the entity ID as available for recycling
      hints:
        level1: Entity is just an ID. Components are data. Systems process entities with specific components.
        level2: Use archetypes or sparse sets for efficient component storage and iteration.
        level3: "## ECS Implementation\n\n```cpp\nusing Entity = uint32_t;\nusing ComponentType = uint32_t;\n\n// Component\
          \ storage using sparse set\ntemplate<typename T>\nclass ComponentArray {\n    std::vector<T> dense;\n    std::vector<Entity>\
          \ denseToEntity;\n    std::unordered_map<Entity, size_t> entityToIndex;\n    \npublic:\n    void insert(Entity entity,\
          \ T component) {\n        size_t index = dense.size();\n        dense.push_back(component);\n        denseToEntity.push_back(entity);\n\
          \        entityToIndex[entity] = index;\n    }\n    \n    void remove(Entity entity) {\n        size_t index = entityToIndex[entity];\n\
          \        size_t lastIndex = dense.size() - 1;\n        \n        // Swap with last\n        dense[index] = dense[lastIndex];\n\
          \        denseToEntity[index] = denseToEntity[lastIndex];\n        entityToIndex[denseToEntity[index]] = index;\n\
          \        \n        dense.pop_back();\n        denseToEntity.pop_back();\n        entityToIndex.erase(entity);\n\
          \    }\n    \n    T& get(Entity entity) { return dense[entityToIndex[entity]]; }\n    bool has(Entity entity) {\
          \ return entityToIndex.count(entity) > 0; }\n    \n    auto begin() { return dense.begin(); }\n    auto end() {\
          \ return dense.end(); }\n};\n\nclass World {\n    Entity nextEntity = 0;\n    std::unordered_map<ComponentType,\
          \ void*> componentArrays;\n    std::set<Entity> entities;\n    \npublic:\n    Entity createEntity() {\n        Entity\
          \ e = nextEntity++;\n        entities.insert(e);\n        return e;\n    }\n    \n    void destroyEntity(Entity\
          \ e) {\n        entities.erase(e);\n        // Remove from all component arrays...\n    }\n    \n    template<typename\
          \ T>\n    void addComponent(Entity e, T component) {\n        getComponentArray<T>()->insert(e, component);\n  \
          \  }\n    \n    template<typename T>\n    T& getComponent(Entity e) {\n        return getComponentArray<T>()->get(e);\n\
          \    }\n    \n    template<typename... Components>\n    void forEach(std::function<void(Entity, Components&...)>\
          \ func) {\n        for (Entity e : entities) {\n            if ((getComponentArray<Components>()->has(e) && ...))\
          \ {\n                func(e, getComponent<Components>(e)...);\n            }\n        }\n    }\n};\n\n// Components\n\
          struct Transform { float x, y, rotation, scaleX, scaleY; };\nstruct Velocity { float vx, vy; };\nstruct Sprite {\
          \ Texture* texture; float width, height; };\n\n// Systems\nvoid MovementSystem(World& world, float dt) {\n    world.forEach<Transform,\
          \ Velocity>([dt](Entity e, Transform& t, Velocity& v) {\n        t.x += v.vx * dt;\n        t.y += v.vy * dt;\n\
          \    });\n}\n\nvoid RenderSystem(World& world, SpriteBatch& batch) {\n    world.forEach<Transform, Sprite>([&batch](Entity\
          \ e, Transform& t, Sprite& s) {\n        batch.draw(s.texture, t.x, t.y, s.width * t.scaleX, s.height * t.scaleY);\n\
          \    });\n}\n```"
      pitfalls:
      - Component iteration invalidation
      - Memory fragmentation
      - System ordering dependencies
      concepts:
      - ECS architecture
      - Data-oriented design
      - Cache efficiency
      estimated_hours: 20-30
      deliverables:
      - Entity ID manager that creates, tracks, and recycles integer entity identifiers with generation counters
      - Component storage using dense arrays (struct-of-arrays) for cache-friendly iteration over component data
      - System execution framework that iterates over entities matching a component signature and applies logic
      - Entity query API that selects entities possessing a specific combination of component types for processing
    - id: 3
      name: Physics & Collision
      description: Implement 2D physics and collision detection.
      acceptance_criteria:
      - Rigid body dynamics correctly apply gravity, velocity, and acceleration to update entity positions each frame
      - Collision detection identifies overlapping AABB and circle collider pairs in the broad phase and narrow phase
      - Collision response applies impulse forces and position corrections to separate overlapping bodies realistically
      - Spatial partitioning reduces collision pair checks from O(n^2) to O(n log n) or better for large entity counts
      - Physics timestep uses a fixed delta time with accumulator to ensure deterministic simulation independent of frame
        rate
      hints:
        level1: Semi-implicit Euler for integration. AABB for broad phase, then narrow phase.
        level2: Fixed timestep with accumulator for deterministic physics.
        level3: "## 2D Physics\n\n```cpp\nstruct RigidBody {\n    float mass, invMass;\n    float vx, vy;\n    float ax, ay;\n\
          \    float restitution;  // Bounciness\n};\n\nstruct Collider {\n    enum Type { AABB, Circle } type;\n    union\
          \ {\n        struct { float halfW, halfH; } aabb;\n        struct { float radius; } circle;\n    };\n};\n\nclass\
          \ PhysicsWorld {\n    float gravity = -9.81f;\n    float fixedDeltaTime = 1.0f / 60.0f;\n    float accumulator =\
          \ 0.0f;\n    \npublic:\n    void update(World& world, float dt) {\n        accumulator += dt;\n        \n      \
          \  while (accumulator >= fixedDeltaTime) {\n            fixedUpdate(world, fixedDeltaTime);\n            accumulator\
          \ -= fixedDeltaTime;\n        }\n    }\n    \n    void fixedUpdate(World& world, float dt) {\n        // Apply forces\n\
          \        world.forEach<Transform, RigidBody>([this, dt](Entity e, Transform& t, RigidBody& rb) {\n            if\
          \ (rb.invMass > 0) {\n                rb.ay += gravity;\n            }\n            \n            // Semi-implicit\
          \ Euler\n            rb.vx += rb.ax * dt;\n            rb.vy += rb.ay * dt;\n            t.x += rb.vx * dt;\n  \
          \          t.y += rb.vy * dt;\n            \n            rb.ax = rb.ay = 0;\n        });\n        \n        // Collision\
          \ detection & response\n        detectAndResolveCollisions(world);\n    }\n    \n    void detectAndResolveCollisions(World&\
          \ world) {\n        std::vector<std::tuple<Entity, Entity>> pairs;\n        \n        // Broad phase: find potential\
          \ collisions\n        world.forEach<Transform, Collider>([&](Entity e1, Transform& t1, Collider& c1) {\n       \
          \     world.forEach<Transform, Collider>([&](Entity e2, Transform& t2, Collider& c2) {\n                if (e1 >=\
          \ e2) return;\n                if (broadPhaseCheck(t1, c1, t2, c2)) {\n                    pairs.emplace_back(e1,\
          \ e2);\n                }\n            });\n        });\n        \n        // Narrow phase: resolve collisions\n\
          \        for (auto [e1, e2] : pairs) {\n            auto& t1 = world.getComponent<Transform>(e1);\n            auto&\
          \ t2 = world.getComponent<Transform>(e2);\n            auto& c1 = world.getComponent<Collider>(e1);\n          \
          \  auto& c2 = world.getComponent<Collider>(e2);\n            \n            CollisionInfo info;\n            if (narrowPhaseCheck(t1,\
          \ c1, t2, c2, info)) {\n                resolveCollision(world, e1, e2, info);\n            }\n        }\n    }\n\
          \    \n    void resolveCollision(World& world, Entity e1, Entity e2, CollisionInfo& info) {\n        auto& rb1 =\
          \ world.getComponent<RigidBody>(e1);\n        auto& rb2 = world.getComponent<RigidBody>(e2);\n        auto& t1 =\
          \ world.getComponent<Transform>(e1);\n        auto& t2 = world.getComponent<Transform>(e2);\n        \n        //\
          \ Separate objects\n        float totalInvMass = rb1.invMass + rb2.invMass;\n        t1.x -= info.normal.x * info.depth\
          \ * (rb1.invMass / totalInvMass);\n        t1.y -= info.normal.y * info.depth * (rb1.invMass / totalInvMass);\n\
          \        t2.x += info.normal.x * info.depth * (rb2.invMass / totalInvMass);\n        t2.y += info.normal.y * info.depth\
          \ * (rb2.invMass / totalInvMass);\n        \n        // Impulse-based response\n        float relVelN = (rb2.vx\
          \ - rb1.vx) * info.normal.x + (rb2.vy - rb1.vy) * info.normal.y;\n        if (relVelN > 0) return;  // Already separating\n\
          \        \n        float e = std::min(rb1.restitution, rb2.restitution);\n        float j = -(1 + e) * relVelN /\
          \ totalInvMass;\n        \n        rb1.vx -= j * rb1.invMass * info.normal.x;\n        rb1.vy -= j * rb1.invMass\
          \ * info.normal.y;\n        rb2.vx += j * rb2.invMass * info.normal.x;\n        rb2.vy += j * rb2.invMass * info.normal.y;\n\
          \    }\n};\n```"
      pitfalls:
      - Variable timestep physics
      - Tunneling (fast objects passing through)
      - Collision jitter
      concepts:
      - Physics simulation
      - Collision detection
      - Impulse resolution
      estimated_hours: 25-40
      deliverables:
      - AABB (axis-aligned bounding box) collision detection that tests overlap between rectangular bounds
      - Spatial partitioning structure (grid or quadtree) that reduces collision checks to nearby entity pairs
      - Rigid body physics integration using semi-implicit Euler for velocity and position updates each frame
      - Collision response and resolution that applies impulses and separates overlapping bodies after detection
    - id: 4
      name: Resource & Scene Management
      description: Implement asset loading and scene system.
      acceptance_criteria:
      - Asset loader reads textures, audio clips, and data files from disk and returns typed resource handles
      - Resource cache returns the same handle for duplicate load requests and tracks reference counts for cleanup
      - Scene serialization writes the current entity-component state to a file that can be loaded to restore the scene
      - Scene transitions unload the current scene's resources and load the new scene's assets without memory leaks
      - Game loop runs update and render phases in sequence at a stable frame rate with delta-time propagation
      hints:
        level1: Load resources once, cache by path. Scenes contain entities and their components.
        level2: Use JSON/binary for scene serialization. Implement proper cleanup on scene switch.
        level3: "## Resource & Scene Management\n\n```cpp\nclass ResourceManager {\n    std::unordered_map<std::string, std::shared_ptr<Texture>>\
          \ textures;\n    std::unordered_map<std::string, std::shared_ptr<Sound>> sounds;\n    \npublic:\n    std::shared_ptr<Texture>\
          \ loadTexture(const std::string& path) {\n        if (textures.count(path)) return textures[path];\n        \n \
          \       auto tex = std::make_shared<Texture>();\n        tex->loadFromFile(path);\n        textures[path] = tex;\n\
          \        return tex;\n    }\n    \n    void unloadUnused() {\n        for (auto it = textures.begin(); it != textures.end();)\
          \ {\n            if (it->second.use_count() == 1) {\n                it = textures.erase(it);\n            } else\
          \ {\n                ++it;\n            }\n        }\n    }\n};\n\nclass Scene {\npublic:\n    std::string name;\n\
          \    World world;\n    \n    virtual void onEnter() = 0;\n    virtual void onExit() = 0;\n    virtual void update(float\
          \ dt) = 0;\n    virtual void render() = 0;\n    \n    void saveToFile(const std::string& path) {\n        json j;\n\
          \        j[\"name\"] = name;\n        j[\"entities\"] = json::array();\n        \n        world.forEach<Transform>([&](Entity\
          \ e, Transform& t) {\n            json entity;\n            entity[\"id\"] = e;\n            entity[\"transform\"\
          ] = {t.x, t.y, t.rotation, t.scaleX, t.scaleY};\n            // Serialize other components...\n            j[\"\
          entities\"].push_back(entity);\n        });\n        \n        std::ofstream file(path);\n        file << j.dump(2);\n\
          \    }\n    \n    void loadFromFile(const std::string& path) {\n        std::ifstream file(path);\n        json\
          \ j = json::parse(file);\n        \n        name = j[\"name\"];\n        for (auto& entity : j[\"entities\"]) {\n\
          \            Entity e = world.createEntity();\n            auto& t = entity[\"transform\"];\n            world.addComponent(e,\
          \ Transform{t[0], t[1], t[2], t[3], t[4]});\n            // Deserialize other components...\n        }\n    }\n\
          };\n\nclass Engine {\n    Window window;\n    ResourceManager resources;\n    std::unique_ptr<Scene> currentScene;\n\
          \    std::unique_ptr<Scene> nextScene;\n    bool running = true;\n    \npublic:\n    void run() {\n        float\
          \ lastTime = SDL_GetTicks() / 1000.0f;\n        \n        while (running) {\n            float currentTime = SDL_GetTicks()\
          \ / 1000.0f;\n            float dt = currentTime - lastTime;\n            lastTime = currentTime;\n            \n\
          \            processInput();\n            \n            if (nextScene) {\n                if (currentScene) currentScene->onExit();\n\
          \                currentScene = std::move(nextScene);\n                currentScene->onEnter();\n            }\n\
          \            \n            if (currentScene) {\n                currentScene->update(dt);\n                currentScene->render();\n\
          \            }\n            \n            window.swap();\n        }\n    }\n    \n    void changeScene(std::unique_ptr<Scene>\
          \ scene) {\n        nextScene = std::move(scene);\n    }\n};\n```"
      pitfalls:
      - Resource leaks
      - Scene state corruption during transition
      - Serialization version incompatibility
      concepts:
      - Resource management
      - Scene graphs
      - Game architecture
      estimated_hours: 20-30
      deliverables:
      - Asset loading system that reads textures, models, audio files, and data from disk into memory
      - Resource caching with reference counting that shares loaded assets and frees them when no longer referenced
      - Scene graph or level structure that organizes entities into hierarchical groups with transform inheritance
      - Scene serialization and deserialization that saves and loads scene state to and from JSON or binary files
  build-gc:
    id: build-gc
    name: Build Your Own Garbage Collector
    description: Implement a garbage collector with mark-sweep and generational collection.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - Memory management
    - C/Rust
    - Data structures
    - Graph algorithms
    languages:
      recommended:
      - C
      - Rust
      - Zig
      also_possible: []
    resources:
    - type: book
      name: The Garbage Collection Handbook
      url: https://gchandbook.org/
    - type: article
      name: Baby's First Garbage Collector
      url: https://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/
    - type: article
      name: Writing a Simple GC in C
      url: https://maplant.com/gc.html
    milestones:
    - id: 1
      name: Mark-Sweep Collector
      description: Implement basic mark-sweep garbage collection.
      acceptance_criteria:
      - Object allocation returns memory from the managed heap and registers the object for garbage collection tracking
      - Root set identification correctly enumerates all pointers from the stack, global variables, and CPU registers
      - Mark phase performs a depth-first traversal and marks every object reachable from the root set as live
      - Sweep phase iterates the entire heap and reclaims all unmarked objects by adding them to the free list
      - Total memory reclaimed after a collection equals the sum of all unreachable object sizes in the heap
      hints:
        level1: Mark all reachable objects from roots, then sweep unmarked ones.
        level2: Roots are global variables and stack. Mark with DFS to find all reachable.
        level3: "## Mark-Sweep GC\n\n```c\n#include <stdlib.h>\n#include <stdbool.h>\n\ntypedef struct Object {\n    bool\
          \ marked;\n    struct Object* next;  // Intrusive list of all objects\n    // Object type info for tracing\n   \
          \ enum { OBJ_INT, OBJ_PAIR } type;\n    union {\n        int value;\n        struct { struct Object* head; struct\
          \ Object* tail; } pair;\n    };\n} Object;\n\ntypedef struct {\n    Object* firstObject;  // All allocated objects\n\
          \    Object** stack;       // VM stack (roots)\n    int stackSize;\n    int numObjects;\n    int maxObjects;\n}\
          \ VM;\n\nVM* newVM() {\n    VM* vm = malloc(sizeof(VM));\n    vm->firstObject = NULL;\n    vm->stack = malloc(sizeof(Object*)\
          \ * 256);\n    vm->stackSize = 0;\n    vm->numObjects = 0;\n    vm->maxObjects = 8;  // Initial threshold\n    return\
          \ vm;\n}\n\nObject* allocate(VM* vm, int type) {\n    if (vm->numObjects >= vm->maxObjects) {\n        gc(vm);\n\
          \    }\n    \n    Object* obj = malloc(sizeof(Object));\n    obj->marked = false;\n    obj->type = type;\n    obj->next\
          \ = vm->firstObject;\n    vm->firstObject = obj;\n    vm->numObjects++;\n    return obj;\n}\n\nvoid mark(Object*\
          \ obj) {\n    if (obj == NULL || obj->marked) return;\n    \n    obj->marked = true;\n    \n    if (obj->type ==\
          \ OBJ_PAIR) {\n        mark(obj->pair.head);\n        mark(obj->pair.tail);\n    }\n}\n\nvoid markAll(VM* vm) {\n\
          \    for (int i = 0; i < vm->stackSize; i++) {\n        mark(vm->stack[i]);\n    }\n}\n\nvoid sweep(VM* vm) {\n\
          \    Object** obj = &vm->firstObject;\n    while (*obj) {\n        if (!(*obj)->marked) {\n            Object* unreached\
          \ = *obj;\n            *obj = unreached->next;\n            free(unreached);\n            vm->numObjects--;\n  \
          \      } else {\n            (*obj)->marked = false;  // Reset for next GC\n            obj = &(*obj)->next;\n \
          \       }\n    }\n}\n\nvoid gc(VM* vm) {\n    int numObjects = vm->numObjects;\n    \n    markAll(vm);\n    sweep(vm);\n\
          \    \n    vm->maxObjects = vm->numObjects * 2;  // Grow threshold\n    \n    printf(\"Collected %d objects, %d\
          \ remaining.\\n\",\n           numObjects - vm->numObjects, vm->numObjects);\n}\n```"
      pitfalls:
      - Missing roots
      - Marking already marked objects (infinite loop)
      - Not resetting marks
      concepts:
      - Reachability
      - Graph traversal
      - Memory reclamation
      estimated_hours: 10-15
      deliverables:
      - Root set identification that locates all GC roots from the stack, global variables, and registers
      - Mark phase that performs depth-first traversal of the object graph starting from all identified roots
      - Sweep phase that scans the heap and collects all unmarked objects into the free list for reuse
      - Free list management that maintains a linked list of reclaimed memory blocks for future allocations
    - id: 2
      name: Tri-color Marking
      description: Implement tri-color invariant for incremental GC.
      acceptance_criteria:
      - Objects transition correctly through white (unvisited), gray (discovered), and black (fully scanned) states
      - Worklist-based marking processes gray objects one at a time and transitions them to black after scanning their fields
      - Incremental marking can pause and resume between mutator execution phases without missing live objects
      - Write barriers intercept pointer updates and re-gray the affected objects to prevent premature collection
      hints:
        level1: White=unmarked, gray=marked but children not scanned, black=fully scanned.
        level2: Process gray objects incrementally. Write barrier adds to gray set.
        level3: "## Tri-color Marking\n\n```c\ntypedef enum { WHITE, GRAY, BLACK } Color;\n\ntypedef struct Object {\n   \
          \ Color color;\n    // ... other fields ...\n} Object;\n\ntypedef struct {\n    Object** grayList;\n    int grayCount;\n\
          \    int grayCapacity;\n} GC;\n\nvoid makeGray(GC* gc, Object* obj) {\n    if (obj == NULL || obj->color != WHITE)\
          \ return;\n    \n    obj->color = GRAY;\n    if (gc->grayCount >= gc->grayCapacity) {\n        gc->grayCapacity\
          \ *= 2;\n        gc->grayList = realloc(gc->grayList, gc->grayCapacity * sizeof(Object*));\n    }\n    gc->grayList[gc->grayCount++]\
          \ = obj;\n}\n\nvoid markRoots(VM* vm, GC* gc) {\n    for (int i = 0; i < vm->stackSize; i++) {\n        makeGray(gc,\
          \ vm->stack[i]);\n    }\n}\n\n// Process some gray objects (incremental)\nbool markSome(GC* gc, int workAmount)\
          \ {\n    int work = 0;\n    while (gc->grayCount > 0 && work < workAmount) {\n        Object* obj = gc->grayList[--gc->grayCount];\n\
          \        \n        // Scan children\n        if (obj->type == OBJ_PAIR) {\n            makeGray(gc, obj->pair.head);\n\
          \            makeGray(gc, obj->pair.tail);\n        }\n        \n        obj->color = BLACK;\n        work++;\n\
          \    }\n    return gc->grayCount == 0;  // Done?\n}\n\n// Write barrier: when mutator changes a pointer in a black\
          \ object\nvoid writeBarrier(GC* gc, Object* parent, Object* child) {\n    if (parent->color == BLACK && child->color\
          \ == WHITE) {\n        // Maintain tri-color invariant\n        makeGray(gc, child);  // or makeGray(gc, parent)\
          \ for snapshot-at-the-beginning\n    }\n}\n\n// Incremental GC cycle\nvoid incrementalGC(VM* vm, GC* gc) {\n   \
          \ static enum { IDLE, MARKING, SWEEPING } phase = IDLE;\n    static Object** sweepPtr = NULL;\n    \n    switch\
          \ (phase) {\n        case IDLE:\n            // All objects start white\n            markRoots(vm, gc);\n      \
          \      phase = MARKING;\n            break;\n            \n        case MARKING:\n            if (markSome(gc, 10))\
          \ {  // Process 10 objects per step\n                sweepPtr = &vm->firstObject;\n                phase = SWEEPING;\n\
          \            }\n            break;\n            \n        case SWEEPING:\n            // Sweep a few objects\n \
          \           for (int i = 0; i < 10 && *sweepPtr; i++) {\n                if ((*sweepPtr)->color == WHITE) {\n  \
          \                  Object* unreached = *sweepPtr;\n                    *sweepPtr = unreached->next;\n          \
          \          free(unreached);\n                } else {\n                    (*sweepPtr)->color = WHITE;  // Reset\n\
          \                    sweepPtr = &(*sweepPtr)->next;\n                }\n            }\n            if (*sweepPtr\
          \ == NULL) {\n                phase = IDLE;\n            }\n            break;\n    }\n}\n```"
      pitfalls:
      - Write barrier missing
      - Violating tri-color invariant
      - Race conditions
      concepts:
      - Incremental GC
      - Write barriers
      - Tri-color abstraction
      estimated_hours: 12-18
      deliverables:
      - White/gray/black tri-color abstraction where white is unvisited, gray is discovered, and black is fully scanned
      - Worklist-based marking that processes gray objects by scanning their references and turning them black
      - Write barrier that intercepts pointer stores during incremental marking to maintain the tri-color invariant
      - Termination condition that halts marking when the gray worklist is empty and all live objects are black
    - id: 3
      name: Generational Collection
      description: Implement generational GC with nursery and old generation.
      acceptance_criteria:
      - Young generation (nursery) allocates new objects and is collected frequently with low pause times
      - Old generation holds long-lived objects and is collected less frequently using a full mark-sweep or mark-compact
      - Promotion policy moves surviving objects to the old generation after a configurable tenuring threshold
      - Remembered set records cross-generational pointers so minor collections can find young-gen objects referenced from
        old-gen
      - Minor collections run significantly faster than major collections by only scanning the young generation plus remembered
        set
      hints:
        level1: Most objects die young. Collect nursery frequently, old gen rarely.
        level2: Remembered set tracks old->young pointers so we don't scan all old objects.
        level3: "## Generational GC\n\n```c\ntypedef struct {\n    // Young generation (bump allocation)\n    char* nurseryStart;\n\
          \    char* nurseryEnd;\n    char* nurseryAlloc;\n    \n    // Old generation\n    Object* oldGenList;\n    \n  \
          \  // Remembered set: old objects pointing to young\n    Object** rememberedSet;\n    int rememberedCount;\n   \
          \ int rememberedCapacity;\n    \n    int youngCollections;\n    int promotionAge;  // Survive this many collections\
          \ to promote\n} GenerationalGC;\n\nObject* allocYoung(GenerationalGC* gc, size_t size) {\n    size = ALIGN(size);\n\
          \    \n    if (gc->nurseryAlloc + size > gc->nurseryEnd) {\n        minorCollection(gc);\n        if (gc->nurseryAlloc\
          \ + size > gc->nurseryEnd) {\n            // Still no space, trigger major collection\n            majorCollection(gc);\n\
          \        }\n    }\n    \n    Object* obj = (Object*)gc->nurseryAlloc;\n    gc->nurseryAlloc += size;\n    obj->age\
          \ = 0;\n    obj->isOld = false;\n    return obj;\n}\n\nvoid addToRememberedSet(GenerationalGC* gc, Object* oldObj)\
          \ {\n    if (gc->rememberedCount >= gc->rememberedCapacity) {\n        gc->rememberedCapacity *= 2;\n        gc->rememberedSet\
          \ = realloc(gc->rememberedSet,\n                                    gc->rememberedCapacity * sizeof(Object*));\n\
          \    }\n    gc->rememberedSet[gc->rememberedCount++] = oldObj;\n}\n\n// Write barrier for generational GC\nvoid\
          \ generationalWriteBarrier(GenerationalGC* gc, Object* parent, Object* child) {\n    if (parent->isOld && !child->isOld)\
          \ {\n        addToRememberedSet(gc, parent);\n    }\n}\n\nvoid minorCollection(GenerationalGC* gc) {\n    // Roots:\
          \ stack + remembered set\n    // Copy live young objects to either:\n    // 1. Survivor space (if young)\n    //\
          \ 2. Old generation (if survived enough)\n    \n    char* toSpace = allocateToSpace();\n    char* toPtr = toSpace;\n\
          \    \n    // Process roots\n    for (int i = 0; i < vm->stackSize; i++) {\n        if (!vm->stack[i]->isOld) {\n\
          \            vm->stack[i] = copyObject(vm->stack[i], &toPtr, gc);\n        }\n    }\n    \n    // Process remembered\
          \ set\n    for (int i = 0; i < gc->rememberedCount; i++) {\n        Object* old = gc->rememberedSet[i];\n      \
          \  // Scan old object's children\n        scanChildren(old, &toPtr, gc);\n    }\n    \n    // Process copied objects\
          \ (Cheney's algorithm)\n    char* scanPtr = toSpace;\n    while (scanPtr < toPtr) {\n        Object* obj = (Object*)scanPtr;\n\
          \        scanChildren(obj, &toPtr, gc);\n        scanPtr += objectSize(obj);\n    }\n    \n    // Swap spaces\n\
          \    gc->nurseryAlloc = gc->nurseryStart;\n    gc->rememberedCount = 0;\n    gc->youngCollections++;\n}\n\nObject*\
          \ copyObject(Object* obj, char** toPtr, GenerationalGC* gc) {\n    if (obj->forwarding) return obj->forwarding;\
          \  // Already copied\n    \n    obj->age++;\n    \n    if (obj->age >= gc->promotionAge) {\n        // Promote to\
          \ old generation\n        Object* copy = allocOld(gc, objectSize(obj));\n        memcpy(copy, obj, objectSize(obj));\n\
          \        copy->isOld = true;\n        obj->forwarding = copy;\n        return copy;\n    } else {\n        // Copy\
          \ to to-space\n        Object* copy = (Object*)*toPtr;\n        memcpy(copy, obj, objectSize(obj));\n        *toPtr\
          \ += objectSize(obj);\n        obj->forwarding = copy;\n        return copy;\n    }\n}\n```"
      pitfalls:
      - Missing remembered set entries
      - Promoting too early/late
      - Write barrier overhead
      concepts:
      - Generational hypothesis
      - Copying collection
      - Inter-generational pointers
      estimated_hours: 15-20
      deliverables:
      - Young and old generation heap separation with independently collectible memory regions
      - Minor collection that scans only the young generation for fast, frequent garbage collection cycles
      - Object promotion policy that moves objects surviving a configurable number of minor collections to the old generation
      - Remembered set that tracks pointers from old-generation objects into the young generation for minor collection roots
    - id: 4
      name: Concurrent Collection
      description: Add concurrent marking for reduced pause times.
      acceptance_criteria:
      - Concurrent marking thread traces live objects in parallel with application execution without corruption
      - GC handshakes and safepoints synchronize the collector and mutator threads at well-defined safe points
      - SATB or incremental update write barrier ensures no live object is missed during concurrent marking
      - Concurrent sweep reclaims dead objects while the application continues running with minimal pause time
      hints:
        level1: GC thread marks concurrently while mutator runs. Need synchronization.
        level2: Safepoints where mutator checks for GC requests. SATB logs overwritten pointers.
        level3: "## Concurrent GC\n\n```c\n#include <pthread.h>\n#include <stdatomic.h>\n\ntypedef struct {\n    atomic_bool\
          \ gcRequested;\n    atomic_bool gcInProgress;\n    \n    // SATB (Snapshot-At-The-Beginning) buffer\n    Object**\
          \ satbBuffer;\n    atomic_int satbCount;\n    \n    pthread_t gcThread;\n    pthread_mutex_t mutex;\n    pthread_cond_t\
          \ cond;\n} ConcurrentGC;\n\n// Mutator safepoint - called periodically\nvoid safepoint(ConcurrentGC* gc) {\n   \
          \ if (atomic_load(&gc->gcRequested)) {\n        // Wait for GC to complete initial marking\n        pthread_mutex_lock(&gc->mutex);\n\
          \        while (atomic_load(&gc->gcInProgress)) {\n            pthread_cond_wait(&gc->cond, &gc->mutex);\n     \
          \   }\n        pthread_mutex_unlock(&gc->mutex);\n    }\n}\n\n// SATB write barrier - log overwritten pointer\n\
          void satbWriteBarrier(ConcurrentGC* gc, Object** slot) {\n    if (atomic_load(&gc->gcInProgress)) {\n        Object*\
          \ old = *slot;\n        if (old != NULL) {\n            int idx = atomic_fetch_add(&gc->satbCount, 1);\n       \
          \     gc->satbBuffer[idx] = old;  // Log old value\n        }\n    }\n}\n\n// GC thread\nvoid* gcThreadFunc(void*\
          \ arg) {\n    ConcurrentGC* gc = (ConcurrentGC*)arg;\n    VM* vm = gc->vm;\n    \n    while (1) {\n        // Wait\
          \ for GC request\n        pthread_mutex_lock(&gc->mutex);\n        while (!atomic_load(&gc->gcRequested)) {\n  \
          \          pthread_cond_wait(&gc->cond, &gc->mutex);\n        }\n        pthread_mutex_unlock(&gc->mutex);\n   \
          \     \n        atomic_store(&gc->gcInProgress, true);\n        \n        // Phase 1: Initial mark (stop-the-world,\
          \ mark roots)\n        stopTheWorld(vm);\n        markRoots(vm, gc);\n        resumeTheWorld(vm);\n        \n  \
          \      // Phase 2: Concurrent mark\n        while (gc->grayCount > 0) {\n            Object* obj = gc->grayList[--gc->grayCount];\n\
          \            scanAndMarkChildren(gc, obj);\n        }\n        \n        // Phase 3: Remark (stop-the-world, process\
          \ SATB buffer)\n        stopTheWorld(vm);\n        processSATBBuffer(gc);\n        while (gc->grayCount > 0) {\n\
          \            Object* obj = gc->grayList[--gc->grayCount];\n            scanAndMarkChildren(gc, obj);\n        }\n\
          \        resumeTheWorld(vm);\n        \n        // Phase 4: Concurrent sweep\n        concurrentSweep(gc);\n   \
          \     \n        atomic_store(&gc->gcInProgress, false);\n        atomic_store(&gc->gcRequested, false);\n      \
          \  \n        pthread_mutex_lock(&gc->mutex);\n        pthread_cond_broadcast(&gc->cond);\n        pthread_mutex_unlock(&gc->mutex);\n\
          \    }\n}\n\nvoid processSATBBuffer(ConcurrentGC* gc) {\n    int count = atomic_exchange(&gc->satbCount, 0);\n \
          \   for (int i = 0; i < count; i++) {\n        makeGray(gc, gc->satbBuffer[i]);\n    }\n}\n```"
      pitfalls:
      - Data races
      - Missing SATB entries
      - Long pauses during STW phases
      concepts:
      - Concurrent algorithms
      - SATB/incremental update
      - Safepoints
      estimated_hours: 15-20
      deliverables:
      - Concurrent marking thread that traces the object graph while the application (mutator) continues to run
      - Snapshot-at-the-beginning (SATB) or incremental update barrier to maintain correctness during concurrent tracing
      - Safe point mechanism that coordinates between the GC thread and mutator threads for handshake synchronization
      - Low-pause concurrent sweep that reclaims memory without stopping the mutator threads for extended periods
  build-git:
    id: build-git
    name: Build Your Own Git
    description: Build a version control system that implements core Git operations. Understand content-addressable storage
      and the Git object model.
    difficulty: expert
    estimated_hours: 30-50
    prerequisites:
    - File I/O and hashing
    - Tree data structures
    - Basic compression (zlib)
    - Graph algorithms
    languages:
      recommended:
      - Python
      - Rust
      - Go
      - C
      also_possible: []
    resources:
    - name: Write yourself a Git!
      url: https://wyag.thb.lt/
      type: tutorial
    - name: CodeCrafters Git Challenge
      url: https://app.codecrafters.io/courses/git/overview
      type: interactive
    - name: Git Internals - Git Objects
      url: https://git-scm.com/book/en/v2/Git-Internals-Git-Objects
      type: documentation
    milestones:
    - id: 1
      name: Repository Initialization
      description: Implement git init - create .git directory structure.
      acceptance_criteria:
      - Running init creates a .git directory in the current working directory with the correct structure
      - The .git/objects directory is created with subdirectories for object storage and packs
      - The .git/refs/heads directory is created for storing branch reference files
      - 'The .git/HEAD file is created with content ''ref: refs/heads/master'' pointing to the default branch'
      hints:
        level1: 'mkdir -p for nested directories. HEAD contains ''ref: refs/heads/master'''
        level2: .git/objects stores all content. .git/refs stores branch pointers.
        level3: |-
          .git structure:
          .git/
            HEAD           # ref: refs/heads/master
            objects/       # blob, tree, commit objects
            refs/
              heads/       # branch refs
              tags/        # tag refs
      concepts:
      - Git repository structure
      - References (HEAD, branches)
      estimated_hours: 1-2
      pitfalls:
      - Wrong permissions on .git directory (should be 0755)
      - Missing directories cause later commands to fail silently
      - 'HEAD file must have exact format ''ref: refs/heads/master'' with newline'
      - Creating .git inside existing repo causes confusion
      deliverables:
      - .git directory structure creation with all required subdirectories for a valid git repository
      - HEAD file creation containing a symbolic reference pointing to refs/heads/master as the default branch
      - Objects and refs directories creation for storing git objects and branch/tag references respectively
      - Initial config file with repository format version and default settings for the new repository
    - id: 2
      name: Object Storage (Blobs)
      description: Implement hash-object and cat-file for blob objects.
      acceptance_criteria:
      - hash-object computes the SHA-1 hash of 'blob {size}\0{content}' matching real git's output for the same input
      - Compressed object is stored at .git/objects/xx/yyyyyyyy where xx is the first two hex characters of the hash
      - cat-file retrieves and decompresses the stored object, outputting the original content without the header
      - 'Object format follows git''s specification: type tag, space, decimal size, null byte, then raw content bytes'
      hints:
        level1: SHA-1 hash of 'blob {size}\0{content}'. Store zlib-compressed.
        level2: 'Object path: first 2 chars of hash = directory, rest = filename.'
        level3: |-
          def hash_object(data, type='blob'):
            header = f'{type} {len(data)}\0'.encode()
            store = header + data
            sha = hashlib.sha1(store).hexdigest()
            path = f'.git/objects/{sha[:2]}/{sha[2:]}'
            compressed = zlib.compress(store)
            write_file(path, compressed)
            return sha
      pitfalls:
      - Forgetting null byte between header and content
      - Not compressing before storage
      - Binary vs text content
      concepts:
      - Content-addressable storage
      - SHA-1 hashing
      - Zlib compression
      estimated_hours: 2-3
      deliverables:
      - SHA1 hash computation that produces a 40-character hex digest from the formatted object content
      - Zlib compression of the full object (header + content) before writing to the object store
      - Object file path derivation that splits the hash into a 2-character directory and 38-character filename
      - Blob object format implementation using the 'blob <size>\0<content>' header-content structure
    - id: 3
      name: Tree Objects
      description: Implement tree objects that represent directory structure.
      acceptance_criteria:
      - Tree object stores a sorted list of (mode, name, SHA-1 hash) entries for files and subdirectories
      - ls-tree displays tree contents showing mode, object type, hash, and filename for each entry
      - write-tree creates a tree object from the current index reflecting the staged directory structure
      - Nested subdirectories produce nested tree objects with the parent tree referencing child tree hashes
      hints:
        level1: 'Tree entries: mode (100644 for file, 40000 for dir), name, 20-byte hash.'
        level2: 'Entries sorted by name. Each entry is binary: mode + space + name + null + hash.'
        level3: |-
          # Tree object format and implementation
          def write_tree(directory):
              entries = []
              for name in sorted(os.listdir(directory)):
                  path = os.path.join(directory, name)
                  if name == '.git':
                      continue
                  if os.path.isfile(path):
                      mode = b'100644'
                      sha = hash_object(open(path, 'rb').read(), 'blob')
                  else:
                      mode = b'40000'
                      sha = write_tree(path)  # Recurse for subdirectories
                  entries.append((mode, name.encode(), bytes.fromhex(sha)))

              # Build tree content: mode + space + name + null + 20-byte SHA
              data = b''
              for mode, name, sha_bytes in entries:
                  data += mode + b' ' + name + b'\x00' + sha_bytes

              return hash_object(data, 'tree')

          def ls_tree(tree_sha):
              content = read_object(tree_sha)
              entries = []
              while content:
                  # Find space after mode
                  space_idx = content.index(b' ')
                  mode = content[:space_idx].decode()
                  content = content[space_idx + 1:]
                  # Find null after name
                  null_idx = content.index(b'\x00')
                  name = content[:null_idx].decode()
                  content = content[null_idx + 1:]
                  # Next 20 bytes are SHA
                  sha = content[:20].hex()
                  content = content[20:]
                  entries.append((mode, sha, name))
              return entries
      pitfalls:
      - Hash stored as binary (20 bytes), not hex (40 chars)
      - Sorting rules
      - Mode formatting
      concepts:
      - Tree data structure
      - Directory representation
      - Binary formats
      estimated_hours: 3-4
      deliverables:
      - Tree object format implementation encoding entries as 'mode name\0<20-byte-hash>' sequences
      - Directory-to-tree conversion that recursively builds tree objects from the working directory structure
      - Tree hash calculation by hashing the formatted tree object content with the 'tree <size>' header
      - Recursive tree building that creates nested tree objects for subdirectories and references them by hash
    - id: 4
      name: Commit Objects
      description: Implement commit objects with tree, parent, author, message.
      acceptance_criteria:
      - commit-tree creates a commit object referencing the given tree hash and optional parent commit hashes
      - Commit object contains tree hash, parent hashes, author, committer with timestamps, and the commit message
      - Timestamps are stored in Unix epoch format with a timezone offset (e.g., +0000 or -0500)
      - Chained commits form a linked history where each commit references its parent, creating a DAG of revisions
      hints:
        level1: Commit points to tree. First commit has no parent.
        level2: 'Format: tree, parent (optional), author, committer, blank line, message.'
        level3: |-
          Commit object creation in Python:

          import hashlib
          import zlib
          import time

          def create_commit(tree_sha, parent_sha, message, author_name, author_email):
              timestamp = int(time.time())
              timezone = "+0000"

              # Build commit content
              lines = [f"tree {tree_sha}"]
              if parent_sha:
                  lines.append(f"parent {parent_sha}")
              lines.append(f"author {author_name} <{author_email}> {timestamp} {timezone}")
              lines.append(f"committer {author_name} <{author_email}> {timestamp} {timezone}")
              lines.append("")  # Blank line before message
              lines.append(message)

              content = "\n".join(lines).encode()

              # Create commit object (type + size + null + content)
              header = f"commit {len(content)}\0".encode()
              store = header + content

              # Calculate SHA-1 hash
              sha = hashlib.sha1(store).hexdigest()

              # Write compressed object to .git/objects/xx/yy...
              path = f".git/objects/{sha[:2]}/{sha[2:]}"
              os.makedirs(os.path.dirname(path), exist_ok=True)
              with open(path, 'wb') as f:
                  f.write(zlib.compress(store))

              return sha

          # Usage: sha = create_commit(tree_sha, parent_sha, "Initial commit",
          #                            "Alice", "alice@example.com")
      pitfalls:
      - Timestamp format
      - Handling merge commits
      - Message can be multi-line
      concepts:
      - Commit graph
      - Directed acyclic graph (DAG)
      - Immutable history
      estimated_hours: 2-3
      deliverables:
      - Commit object format containing tree hash, parent commit(s), author, committer, and message fields
      - Parent commit linking that records one or more parent hashes for building the commit history graph
      - Timestamp and timezone handling that records author and committer times in Unix epoch with UTC offset
      - Commit hash calculation by hashing the complete commit object with the 'commit <size>' header
    - id: 5
      name: References and Branches
      description: Implement branches as references to commits.
      acceptance_criteria:
      - Branch creation writes a new file at .git/refs/heads/<name> containing the target commit hash
      - Branch deletion removes the corresponding ref file from .git/refs/heads/ after safety checks
      - All branches are stored as plain text files in .git/refs/heads/ containing a single commit hash
      - HEAD contains a symbolic reference to the current branch and updates when branches are switched
      - Detached HEAD state writes a raw commit hash to the HEAD file when checking out a specific commit
      hints:
        level1: 'Branch = file containing commit hash. HEAD = file containing ''ref: refs/heads/xxx''.'
        level2: update-ref command safely updates ref files.
        level3: |-
          Branch and reference management in Python:

          import os

          class RefManager:
              def __init__(self, git_dir=".git"):
                  self.git_dir = git_dir

              def resolve_ref(self, ref):
                  """Resolve a ref (branch name, HEAD, etc.) to a commit SHA."""
                  # Check if it's a direct SHA
                  if len(ref) == 40 and all(c in '0123456789abcdef' for c in ref):
                      return ref

                  # Try reading as a file reference
                  for path in [
                      f"{self.git_dir}/{ref}",
                      f"{self.git_dir}/refs/{ref}",
                      f"{self.git_dir}/refs/heads/{ref}",
                      f"{self.git_dir}/refs/tags/{ref}",
                  ]:
                      if os.path.isfile(path):
                          content = open(path).read().strip()
                          # Handle symbolic refs (e.g., "ref: refs/heads/main")
                          if content.startswith("ref: "):
                              return self.resolve_ref(content[5:])
                          return content
                  return None

              def create_branch(self, name, commit_sha=None):
                  """Create a new branch pointing to commit."""
                  if commit_sha is None:
                      commit_sha = self.resolve_ref("HEAD")

                  ref_path = f"{self.git_dir}/refs/heads/{name}"
                  os.makedirs(os.path.dirname(ref_path), exist_ok=True)
                  with open(ref_path, 'w') as f:
                      f.write(commit_sha + "\n")

              def update_head(self, ref_or_sha, symbolic=True):
                  """Update HEAD to point to a branch or commit."""
                  head_path = f"{self.git_dir}/HEAD"
                  if symbolic and not ref_or_sha.startswith("refs/"):
                      ref_or_sha = f"refs/heads/{ref_or_sha}"

                  with open(head_path, 'w') as f:
                      if symbolic:
                          f.write(f"ref: {ref_or_sha}\n")
                      else:
                          f.write(ref_or_sha + "\n")  # Detached HEAD
      pitfalls:
      - Symbolic refs vs direct refs
      - Deleting branch you're on
      - Ref file locking
      concepts:
      - Git references
      - Symbolic references
      - Branch management
      estimated_hours: 2-3
      deliverables:
      - Reference files that store a 40-character commit hash pointing to the tip of a branch
      - Branch creation that writes a new ref file under .git/refs/heads/ with the target commit hash
      - 'HEAD as a symbolic reference file containing ''ref: refs/heads/<branch>'' for the current branch'
      - Detached HEAD handling that writes a raw commit hash to HEAD instead of a symbolic reference
    - id: 6
      name: Index (Staging Area)
      description: Implement the staging area for preparing commits.
      acceptance_criteria:
      - Add command stages files to the index by creating blob objects and recording their hash in the index entry
      - Status command shows staged, unstaged, and untracked changes by comparing the index to HEAD and the working tree
      - Index is stored as a binary file at .git/index with a header, sorted entries, and SHA-1 checksum
      - Each index entry stores the file mode, SHA-1 hash, flags, and path name for the staged file
      hints:
        level1: Index caches file info for quick status checks.
        level2: Binary format with header, entries, and optional extensions.
        level3: |-
          Git index file parsing and writing in Python:

          import struct
          import hashlib
          import os

          class IndexEntry:
              def __init__(self, path, sha, mode=0o100644):
                  self.ctime = self.mtime = int(time.time())
                  self.dev = self.ino = 0
                  self.mode = mode
                  self.uid = self.gid = os.getuid()
                  self.size = 0
                  self.sha = bytes.fromhex(sha)
                  self.flags = len(path.encode()) & 0xFFF
                  self.path = path

          def read_index():
              """Parse .git/index file."""
              try:
                  with open('.git/index', 'rb') as f:
                      data = f.read()
              except FileNotFoundError:
                  return []

              # Header: DIRC + version (4 bytes) + entry count (4 bytes)
              signature = data[:4]
              assert signature == b'DIRC', "Invalid index file"
              version = struct.unpack('>I', data[4:8])[0]
              entry_count = struct.unpack('>I', data[8:12])[0]

              entries = []
              offset = 12
              for _ in range(entry_count):
                  # Parse 62-byte fixed portion + variable path
                  ctime_s, ctime_n, mtime_s, mtime_n = struct.unpack('>IIII', data[offset:offset+16])
                  dev, ino, mode, uid, gid, size = struct.unpack('>IIIIII', data[offset+16:offset+40])
                  sha = data[offset+40:offset+60].hex()
                  flags = struct.unpack('>H', data[offset+60:offset+62])[0]
                  path_len = flags & 0xFFF
                  path = data[offset+62:offset+62+path_len].decode()

                  entry = IndexEntry(path, sha, mode)
                  entries.append(entry)

                  # Entries are padded to 8-byte alignment
                  entry_len = 62 + path_len + 1  # +1 for null
                  offset += (entry_len + 7) & ~7

              return entries

          def write_index(entries):
              """Write entries to .git/index."""
              # ... serialize entries and compute SHA-1 checksum
      pitfalls:
      - Index is binary, not text
      - Stat info for detecting changes
      - Path encoding and sorting
      concepts:
      - Staging area concept
      - Binary file formats
      - File metadata caching
      estimated_hours: 4-6
      deliverables:
      - Index file binary format parser that reads the header, entries, and checksum from .git/index
      - Add operation that stages a file by computing its blob hash and inserting an entry into the index
      - Remove operation that unstages a file by deleting its entry from the index data structure
      - Index-to-tree conversion that builds tree objects from the sorted index entries for commit creation
    - id: 7
      name: Diff Algorithm
      description: Implement diff to show changes between versions.
      acceptance_criteria:
      - Diff shows line-by-line differences with + for additions and - for deletions in the correct order
      - Unified diff format includes @@ hunk headers with line number ranges and surrounding context lines
      - Diff between the working tree and the index shows unstaged modifications to tracked files
      - Diff between two commits compares their tree objects and reports all file-level changes between them
      hints:
        level1: Use Myers diff algorithm or Longest Common Subsequence.
        level2: 'Unified diff: @@ -start,count +start,count @@ then - and + lines.'
        level3: |-
          Myers diff algorithm implementation in Python:

          def myers_diff(a, b):
              """Myers diff algorithm - finds shortest edit script."""
              n, m = len(a), len(b)
              max_d = n + m
              v = {1: 0}  # Furthest reaching d-path endpoints
              trace = []  # For backtracking

              for d in range(max_d + 1):
                  trace.append(dict(v))
                  for k in range(-d, d + 1, 2):
                      # Choose whether to go down or right
                      if k == -d or (k != d and v.get(k - 1, 0) < v.get(k + 1, 0)):
                          x = v.get(k + 1, 0)  # Move down (insert)
                      else:
                          x = v.get(k - 1, 0) + 1  # Move right (delete)
                      y = x - k

                      # Extend diagonal (match)
                      while x < n and y < m and a[x] == b[y]:
                          x, y = x + 1, y + 1

                      v[k] = x
                      if x >= n and y >= m:
                          # Found the end - backtrack to get edits
                          return backtrack(trace, a, b)
              return []

          def backtrack(trace, a, b):
              edits = []
              x, y = len(a), len(b)

              for d in range(len(trace) - 1, -1, -1):
                  v = trace[d]
                  k = x - y

                  if k == -d or (k != d and v.get(k - 1, 0) < v.get(k + 1, 0)):
                      prev_k = k + 1
                  else:
                      prev_k = k - 1

                  prev_x = v.get(prev_k, 0)
                  prev_y = prev_x - prev_k

                  while x > prev_x and y > prev_y:
                      edits.append(('=', a[x - 1]))
                      x, y = x - 1, y - 1

                  if d > 0:
                      if x == prev_x:
                          edits.append(('+', b[y - 1]))
                      else:
                          edits.append(('-', a[x - 1]))
                  x, y = prev_x, prev_y

              return list(reversed(edits))
      pitfalls:
      - Binary files
      - Line ending differences
      - Large files / long diffs
      concepts:
      - Diff algorithms (Myers, LCS)
      - Edit distance
      - Unified diff format
      estimated_hours: 4-6
      deliverables:
      - Myers diff algorithm implementation that finds the shortest edit script between two sequences of lines
      - Line-based diff output that identifies added, removed, and unchanged lines between two file versions
      - Unified diff format output with @@ hunk headers showing line numbers and context lines around changes
      - Diff between trees that compares two tree objects and reports changed, added, and deleted file entries
    - id: 8
      name: Merge (Three-way)
      description: Implement three-way merge with conflict detection.
      acceptance_criteria:
      - Merge base algorithm finds the lowest common ancestor commit of the two branches being merged
      - Non-conflicting changes from both branches are applied automatically to produce the merged result
      - Conflicting changes are marked with <<<<<<< ======= >>>>>>> markers in the affected files for manual resolution
      - Merge commit is created with two parent hashes, preserving the full branch topology in the commit graph
      hints:
        level1: Merge base = lowest common ancestor in commit graph.
        level2: 'Three-way: if only one side changed, take that. If both changed same, conflict.'
        level3: |-
          Three-way merge algorithm in Python:

          def three_way_merge(base, ours, theirs):
              """Three-way merge of file contents."""
              base_lines = base.splitlines(keepends=True)
              ours_lines = ours.splitlines(keepends=True)
              theirs_lines = theirs.splitlines(keepends=True)

              # Get diffs from base to each branch
              ours_diff = list(myers_diff(base_lines, ours_lines))
              theirs_diff = list(myers_diff(base_lines, theirs_lines))

              result = []
              conflict = False
              i = j = k = 0  # Indices into base, ours, theirs

              while i < len(base_lines) or j < len(ours_lines) or k < len(theirs_lines):
                  ours_changed = (j < len(ours_lines) and
                                 (i >= len(base_lines) or ours_lines[j] != base_lines[i]))
                  theirs_changed = (k < len(theirs_lines) and
                                   (i >= len(base_lines) or theirs_lines[k] != base_lines[i]))

                  if not ours_changed and not theirs_changed:
                      # No changes - keep base
                      result.append(base_lines[i])
                      i += 1; j += 1; k += 1
                  elif ours_changed and not theirs_changed:
                      # Only ours changed - take ours
                      result.append(ours_lines[j])
                      j += 1; i += 1; k += 1
                  elif theirs_changed and not ours_changed:
                      # Only theirs changed - take theirs
                      result.append(theirs_lines[k])
                      k += 1; i += 1; j += 1
                  elif ours_lines[j] == theirs_lines[k]:
                      # Both changed the same way - take either
                      result.append(ours_lines[j])
                      i += 1; j += 1; k += 1
                  else:
                      # Conflict!
                      conflict = True
                      result.append("<<<<<<< ours\n")
                      result.append(ours_lines[j])
                      result.append("=======\n")
                      result.append(theirs_lines[k])
                      result.append(">>>>>>> theirs\n")
                      i += 1; j += 1; k += 1

              return ''.join(result), conflict
      pitfalls:
      - Finding merge base in complex history
      - Handling renames during merge
      - Nested conflicts
      concepts:
      - Three-way merge
      - Merge base calculation
      - Conflict resolution
      estimated_hours: 6-10
      deliverables:
      - Common ancestor finding algorithm that locates the merge base commit between two branch tips
      - Three-way merge algorithm that combines changes from both branches relative to their common ancestor
      - Conflict detection and marking that identifies overlapping edits and inserts conflict markers in the file
      - Merge commit creation with two parent hashes recording both branch tips in the commit history
  build-interpreter:
    id: build-interpreter
    name: Build Your Own Interpreter (Lox)
    description: Build a complete interpreter for the Lox programming language. Based on the excellent 'Crafting Interpreters'
      book by Bob Nystrom.
    difficulty: expert
    estimated_hours: 40-80
    prerequisites:
    - Basic programming language concepts
    - Recursion
    - Tree data structures
    - Object-oriented programming
    languages:
      recommended:
      - Java
      - C
      - Rust
      also_possible:
      - Python
      - Go
      - TypeScript
    resources:
    - name: Crafting Interpreters (Free Online Book)
      url: https://craftinginterpreters.com
      type: book
    - name: Writing An Interpreter In Go
      url: https://interpreterbook.com
      type: book
    - name: CodeCrafters Interpreter Challenge
      url: https://app.codecrafters.io/courses/interpreter/overview
      type: interactive
    milestones:
    - id: 1
      name: Scanner (Lexer)
      description: Build a scanner that converts Lox source code into tokens. Chapter 4 of Crafting Interpreters.
      acceptance_criteria:
      - Scanner recognizes all language tokens including keywords (var, fun, if, else, while, for, return, class, etc.)
      - String literals with double quotes and number literals with optional decimal points are correctly tokenized
      - Line numbers are tracked and reported accurately for each token to support meaningful error messages
      - Whitespace and comments (single-line // and optionally block /* */) are consumed and ignored between tokens
      - Lexical errors such as unterminated strings or unrecognized characters are reported with their source location
      hints:
        level1: Consume characters one by one. Match against known patterns.
        level2: |-
          Token types: ( ) { } , . - + ; / * ! != = == > >= < <=
          Keywords: and class else false for fun if nil or print return super this true var while
        level3: |-
          class Scanner:
              def __init__(self, source):
                  self.source = source
                  self.tokens = []
                  self.start = 0
                  self.current = 0
                  self.line = 1

              def scan_tokens(self):
                  while not self.is_at_end():
                      self.start = self.current
                      self.scan_token()
                  self.tokens.append(Token(TokenType.EOF, "", None, self.line))
                  return self.tokens

              def scan_token(self):
                  c = self.advance()
                  match c:
                      case '(': self.add_token(TokenType.LEFT_PAREN)
                      case ')': self.add_token(TokenType.RIGHT_PAREN)
                      case '-': self.add_token(TokenType.MINUS)
                      case '+': self.add_token(TokenType.PLUS)
                      case '!':
                          self.add_token(TokenType.BANG_EQUAL if self.match('=') else TokenType.BANG)
                      case '=':
                          self.add_token(TokenType.EQUAL_EQUAL if self.match('=') else TokenType.EQUAL)
                      case '<':
                          self.add_token(TokenType.LESS_EQUAL if self.match('=') else TokenType.LESS)
                      case '"': self.string()
                      case c if c.isdigit(): self.number()
                      case c if c.isalpha() or c == '_': self.identifier()
                      case ' ' | '\r' | '\t': pass
                      case '\n': self.line += 1
                      case _: self.error(f"Unexpected character: {c}")

              def match(self, expected):
                  if self.is_at_end() or self.source[self.current] != expected:
                      return False
                  self.current += 1
                  return True
      pitfalls:
      - Confusing = and ==
      - Not handling unterminated strings
      - Newlines inside strings
      concepts:
      - Lexical analysis
      - Token representation
      - Error handling
      estimated_hours: 2-3
      deliverables:
      - Character stream to token stream converter that reads source code and emits a sequence of typed tokens
      - Token type definitions for keywords, identifiers, string literals, number literals, and operators
      - Line and column tracking that records the source position of each token for error reporting
      - String and number literal scanner that handles quoted strings with escape sequences and decimal numbers
    - id: 2
      name: Representing Code (AST)
      description: Define the abstract syntax tree classes for Lox. Chapter 5.
      acceptance_criteria:
      - Expression node classes cover binary, unary, grouping, and literal expressions with correct field definitions
      - Pretty-printer using the visitor pattern outputs a readable S-expression like (* (+ 1 2) 3) for nested expressions
      - Statement node classes cover print, expression-statement, and variable declaration with correct child references
      hints:
        level1: Use the Visitor pattern for operations on AST nodes.
        level2: Generate classes from grammar rules. Expr -> Binary, Unary, Literal, Grouping...
        level3: |-
          AST node classes with visitor pattern in Python:

          from dataclasses import dataclass
          from typing import Any, List
          from abc import ABC, abstractmethod

          # Expression AST nodes
          @dataclass
          class Expr(ABC):
              pass

          @dataclass
          class Binary(Expr):
              left: Expr
              operator: 'Token'
              right: Expr

          @dataclass
          class Unary(Expr):
              operator: 'Token'
              right: Expr

          @dataclass
          class Literal(Expr):
              value: Any

          @dataclass
          class Grouping(Expr):
              expression: Expr

          @dataclass
          class Variable(Expr):
              name: 'Token'

          @dataclass
          class Assign(Expr):
              name: 'Token'
              value: Expr

          # Statement AST nodes
          @dataclass
          class Stmt(ABC):
              pass

          @dataclass
          class Print(Stmt):
              expression: Expr

          @dataclass
          class VarDecl(Stmt):
              name: 'Token'
              initializer: Expr = None

          @dataclass
          class Block(Stmt):
              statements: List[Stmt]

          # Visitor pattern
          class Visitor(ABC):
              @abstractmethod
              def visit_binary(self, expr: Binary): pass
              @abstractmethod
              def visit_literal(self, expr: Literal): pass
              # ... other visit methods

          class ASTPrinter(Visitor):
              def print(self, expr):
                  return expr.accept(self)

              def visit_binary(self, expr):
                  return f"({expr.operator.lexeme} {expr.left.accept(self)} {expr.right.accept(self)})"

              def visit_literal(self, expr):
                  return str(expr.value)
      pitfalls:
      - Visitor pattern boilerplate
      - Mutable vs immutable AST nodes
      - Parent/child references creating cycles
      concepts:
      - Abstract Syntax Trees
      - Visitor pattern
      - Expression vs Statement
      estimated_hours: 2-3
      deliverables:
      - Expression AST node classes for binary, unary, grouping, literal, variable, assign, call, and logical expressions
      - Statement AST node classes for print, expression, variable declaration, block, if, while, function, return, and class
      - Visitor pattern implementation that dispatches to type-specific visit methods for each AST node kind
      - Pretty printer that formats the AST as a parenthesized S-expression string for debugging and testing
    - id: 3
      name: Parsing Expressions
      description: Build a recursive descent parser for expressions. Chapter 6.
      acceptance_criteria:
      - 'Arithmetic expressions are parsed with correct precedence: * and / bind tighter than + and -'
      - Parenthesized sub-expressions override default precedence and are correctly nested in the AST
      - Comparison (<, >, <=, >=) and equality (==, !=) operators are parsed at their correct precedence levels
      - Syntax errors report the unexpected token, its line number, and a description of what was expected
      hints:
        level1: One function per precedence level. Lower precedence = higher in call stack.
        level2: |-
          Precedence (low to high):
          equality (== !=)
          comparison (< > <= >=)
          term (+ -)
          factor (* /)
          unary (! -)
          primary (literals, grouping)
        level3: |-
          # Recursive descent parser with precedence handling
          class Parser:
              def __init__(self, tokens):
                  self.tokens = tokens
                  self.current = 0

              def expression(self):
                  return self.equality()

              def equality(self):  # == !=
                  expr = self.comparison()
                  while self.match(TokenType.BANG_EQUAL, TokenType.EQUAL_EQUAL):
                      operator = self.previous()
                      right = self.comparison()
                      expr = Binary(expr, operator, right)
                  return expr

              def comparison(self):  # > >= < <=
                  expr = self.term()
                  while self.match(TokenType.GREATER, TokenType.GREATER_EQUAL,
                                   TokenType.LESS, TokenType.LESS_EQUAL):
                      operator = self.previous()
                      right = self.term()
                      expr = Binary(expr, operator, right)
                  return expr

              def term(self):  # + -
                  expr = self.factor()
                  while self.match(TokenType.MINUS, TokenType.PLUS):
                      operator = self.previous()
                      right = self.factor()
                      expr = Binary(expr, operator, right)
                  return expr

              def factor(self):  # * /
                  expr = self.unary()
                  while self.match(TokenType.SLASH, TokenType.STAR):
                      operator = self.previous()
                      right = self.unary()
                      expr = Binary(expr, operator, right)
                  return expr

              def unary(self):  # ! -
                  if self.match(TokenType.BANG, TokenType.MINUS):
                      operator = self.previous()
                      right = self.unary()
                      return Unary(operator, right)
                  return self.primary()

              def primary(self):
                  if self.match(TokenType.FALSE): return Literal(False)
                  if self.match(TokenType.TRUE): return Literal(True)
                  if self.match(TokenType.NIL): return Literal(None)
                  if self.match(TokenType.NUMBER, TokenType.STRING):
                      return Literal(self.previous().literal)
                  if self.match(TokenType.LEFT_PAREN):
                      expr = self.expression()
                      self.consume(TokenType.RIGHT_PAREN, "Expect ')' after expression.")
                      return Grouping(expr)
                  raise self.error(self.peek(), "Expect expression.")
      pitfalls:
      - Left recursion causes infinite loop
      - Forgetting closing paren
      - Error recovery
      concepts:
      - Recursive descent parsing
      - Operator precedence
      - Error recovery
      estimated_hours: 3-4
      deliverables:
      - Recursive descent expression parser with one function per precedence level of the grammar
      - Operator precedence handling that correctly nests binary operations according to language rules
      - Grouping expression parsing that handles parenthesized sub-expressions for explicit precedence control
      - Unary and binary expression node construction with correct operator token and operand references
    - id: 4
      name: Evaluating Expressions
      description: Build a tree-walking interpreter to evaluate expressions. Chapter 7.
      acceptance_criteria:
      - Arithmetic operations (+, -, *, /) evaluate correctly on numeric operands and return numeric results
      - Unary minus negates a number and unary ! inverts a boolean truthiness value
      - 'Truthiness rules are implemented: nil and false are falsy, all other values are truthy'
      - String concatenation with the + operator joins two string operands into a new combined string
      - Runtime errors are raised with a message when operators are applied to incompatible types (e.g., 'hello' - 1)
      hints:
        level1: Implement Visitor that returns evaluated value. Recursively evaluate children.
        level2: Lox is dynamically typed. Operations check types at runtime.
        level3: |-
          Tree-walking expression evaluator in Python:

          class Interpreter:
              def __init__(self):
                  self.environment = {}

              def evaluate(self, node):
                  method = getattr(self, f'eval_{type(node).__name__}')
                  return method(node)

              def eval_Literal(self, node):
                  return node.value

              def eval_Binary(self, node):
                  left = self.evaluate(node.left)
                  right = self.evaluate(node.right)

                  op = node.operator.type
                  if op == TokenType.PLUS:
                      if isinstance(left, str) and isinstance(right, str):
                          return left + right  # String concatenation
                      return left + right
                  elif op == TokenType.MINUS:
                      return left - right
                  elif op == TokenType.STAR:
                      return left * right
                  elif op == TokenType.SLASH:
                      if right == 0:
                          raise RuntimeError("Division by zero")
                      return left / right
                  elif op == TokenType.EQUAL_EQUAL:
                      return left == right
                  elif op == TokenType.BANG_EQUAL:
                      return left != right
                  elif op == TokenType.LESS:
                      return left < right
                  elif op == TokenType.GREATER:
                      return left > right

              def eval_Unary(self, node):
                  right = self.evaluate(node.right)
                  if node.operator.type == TokenType.MINUS:
                      return -right
                  elif node.operator.type == TokenType.BANG:
                      return not self.is_truthy(right)

              def eval_Grouping(self, node):
                  return self.evaluate(node.expression)

              def is_truthy(self, value):
                  if value is None: return False
                  if isinstance(value, bool): return value
                  return True
      pitfalls:
      - Division by zero
      - String + non-string
      - Java null vs Lox nil
      concepts:
      - Tree-walking interpretation
      - Dynamic typing
      - Runtime type checking
      estimated_hours: 2-3
      deliverables:
      - Tree-walking evaluator that recursively visits each AST node and computes a runtime value
      - Runtime value representation supporting numbers, strings, booleans, nil, and callable objects
      - Runtime type checking that validates operand types before performing each operation
      - Operator semantics implementation defining behavior for arithmetic, comparison, equality, and string operations
    - id: 5
      name: Statements and State
      description: Add statements, variables, and assignment. Chapter 8.
      acceptance_criteria:
      - Print statement evaluates its expression and writes the string representation to stdout
      - Variable declarations with var create a new binding initialized to the expression value or nil if omitted
      - Assignment expressions update the value of an existing variable binding in the nearest enclosing scope
      - Expression statements evaluate their expression for side effects and discard the result
      - Environment stores variable bindings and supports nested scopes with parent-chain lookup for variables
      hints:
        level1: Environment is a map from variable name to value.
        level2: var a = 1; creates binding. a = 2; updates existing binding.
        level3: |-
          # Environment for variable storage
          class Environment:
              def __init__(self, enclosing=None):
                  self.values = {}
                  self.enclosing = enclosing  # For nested scopes

              def define(self, name, value):
                  self.values[name] = value

              def get(self, name):
                  if name.lexeme in self.values:
                      return self.values[name.lexeme]
                  if self.enclosing:
                      return self.enclosing.get(name)
                  raise RuntimeError(f"Undefined variable '{name.lexeme}'.")

              def assign(self, name, value):
                  if name.lexeme in self.values:
                      self.values[name.lexeme] = value
                      return
                  if self.enclosing:
                      self.enclosing.assign(name, value)
                      return
                  raise RuntimeError(f"Undefined variable '{name.lexeme}'.")

          # Parsing statements
          def statement(self):
              if self.match(TokenType.PRINT):
                  return self.print_statement()
              if self.match(TokenType.VAR):
                  return self.var_declaration()
              return self.expression_statement()

          def var_declaration(self):
              name = self.consume(TokenType.IDENTIFIER, "Expect variable name.")
              initializer = None
              if self.match(TokenType.EQUAL):
                  initializer = self.expression()
              self.consume(TokenType.SEMICOLON, "Expect ';' after variable declaration.")
              return Var(name, initializer)
      pitfalls:
      - Using undeclared variable
      - Assignment to non-variable
      - Scoping issues
      concepts:
      - Statement vs Expression
      - Environment and bindings
      - Assignment as expression
      estimated_hours: 2-3
      deliverables:
      - Print statement that evaluates its expression argument and outputs the result to stdout with a newline
      - Variable declaration statement that binds a name to an optional initializer expression value
      - Environment data structure that stores variable name-to-value bindings for the current scope
      - Block statement that creates a new nested environment scope for the enclosed statements
    - id: 6
      name: Control Flow
      description: Add if, while, for, and logical operators. Chapter 9.
      acceptance_criteria:
      - If/else executes the then-branch when the condition is truthy and the else-branch (if present) otherwise
      - While loops repeat the body until the condition evaluates to a falsy value and then continue past the loop
      - For loops are desugared to a block containing the initializer, a while loop with the condition, and the increment
      - Logical and returns the left operand if falsy (short-circuit), otherwise evaluates and returns the right operand
      hints:
        level1: for (init; cond; incr) body -> { init; while (cond) { body; incr; } }
        level2: 'Short-circuit: a and b returns a if falsy, else b. a or b returns a if truthy, else b.'
        level3: |-
          # Control flow implementation
          class Interpreter:
              def visit_if_stmt(self, stmt):
                  if self.is_truthy(self.evaluate(stmt.condition)):
                      self.execute(stmt.then_branch)
                  elif stmt.else_branch:
                      self.execute(stmt.else_branch)

              def visit_while_stmt(self, stmt):
                  while self.is_truthy(self.evaluate(stmt.condition)):
                      self.execute(stmt.body)

              def visit_logical_expr(self, expr):
                  left = self.evaluate(expr.left)
                  # Short-circuit evaluation
                  if expr.operator.type == TokenType.OR:
                      if self.is_truthy(left):
                          return left
                  else:  # AND
                      if not self.is_truthy(left):
                          return left
                  return self.evaluate(expr.right)

          # For loop desugaring in parser:
          def for_statement(self):
              self.consume(TokenType.LEFT_PAREN, "Expect '(' after 'for'.")

              initializer = None if self.match(TokenType.SEMICOLON) else \
                            self.var_declaration() if self.match(TokenType.VAR) else \
                            self.expression_statement()

              condition = Literal(True) if self.check(TokenType.SEMICOLON) else self.expression()
              self.consume(TokenType.SEMICOLON, "Expect ';' after loop condition.")

              increment = None if self.check(TokenType.RIGHT_PAREN) else self.expression()
              self.consume(TokenType.RIGHT_PAREN, "Expect ')' after for clauses.")

              body = self.statement()

              # Desugar to while loop
              if increment:
                  body = Block([body, Expression(increment)])
              body = While(condition, body)
              if initializer:
                  body = Block([initializer, body])
              return body
      pitfalls:
      - Dangling else
      - Forgetting short-circuit
      - Infinite loops without timeout
      concepts:
      - Conditional execution
      - Loop desugaring
      - Short-circuit evaluation
      estimated_hours: 2-3
      deliverables:
      - If/else statement execution that evaluates the condition and runs the matching branch body
      - While loop execution that repeats the body statement as long as the condition evaluates to truthy
      - For loop desugaring that transforms for(init; cond; incr) into an equivalent while loop structure
      - Logical operators (and, or) with short-circuit evaluation that skip the right operand when determined
    - id: 7
      name: Functions
      description: Add function declarations and calls. Chapter 10.
      acceptance_criteria:
      - The fun keyword declares a named function and stores it as a callable value in the enclosing environment
      - Parameters are bound to argument values positionally and accessible as local variables within the function body
      - Return statement exits the function immediately and the returned value becomes the call expression's result
      - Functions are first-class values that can be stored in variables, passed as arguments, and returned from other functions
      - Recursive calls work correctly with each invocation getting its own environment and returning to the correct caller
      hints:
        level1: 'Function value stores: name, parameters, body AST, and defining environment.'
        level2: 'Call: create new environment for local vars, execute body, return result.'
        level3: |-
          # Function declaration and calling
          class LoxFunction:
              def __init__(self, declaration, closure):
                  self.declaration = declaration
                  self.closure = closure  # Environment where function was defined

              def call(self, interpreter, arguments):
                  # Create new environment for function scope
                  environment = Environment(self.closure)

                  # Bind parameters to arguments
                  for i, param in enumerate(self.declaration.params):
                      environment.define(param.lexeme, arguments[i])

                  try:
                      interpreter.execute_block(self.declaration.body, environment)
                  except Return as return_value:
                      return return_value.value
                  return None

              def arity(self):
                  return len(self.declaration.params)

          class Return(Exception):
              def __init__(self, value):
                  super().__init__()
                  self.value = value

          # In interpreter:
          def visit_return_stmt(self, stmt):
              value = None
              if stmt.value:
                  value = self.evaluate(stmt.value)
              raise Return(value)

          def visit_call_expr(self, expr):
              callee = self.evaluate(expr.callee)
              arguments = [self.evaluate(arg) for arg in expr.arguments]

              if not hasattr(callee, 'call'):
                  raise RuntimeError("Can only call functions and classes.")
              if len(arguments) != callee.arity():
                  raise RuntimeError(f"Expected {callee.arity()} arguments but got {len(arguments)}.")

              return callee.call(self, arguments)
      pitfalls:
      - Stack overflow from infinite recursion
      - Not restoring environment after call
      - Return outside function
      concepts:
      - First-class functions
      - Call frames
      - Return values
      estimated_hours: 3-4
      deliverables:
      - Function declaration using the fun keyword that binds a callable object to a name in the current scope
      - Function call execution that evaluates arguments, binds them to parameters, and runs the body in a new scope
      - Return statement that immediately exits the function body and delivers a value to the call site
      - Local variable scoping so each function call gets its own environment for parameter and local variable bindings
    - id: 8
      name: Closures
      description: Implement lexical scoping and closures. Chapter 11.
      acceptance_criteria:
      - Functions capture the enclosing environment at definition time and retain access to outer variables
      - Nested functions correctly access and modify variables from their enclosing function's scope
      - Closures persist and remain usable after the enclosing function has returned and its scope would normally be gone
      - Variable resolution at compile time determines the correct scope depth for each variable reference in closures
      hints:
        level1: Closure = function + captured environment. Environment chains to enclosing scopes.
        level2: 'Resolver pass: determine which declaration each variable refers to.'
        level3: |-
          Example:
          fun makeCounter() {
            var i = 0;
            fun count() {
              i = i + 1;
              return i;
            }
            return count;
          }
          var counter = makeCounter();
          counter(); // 1
          counter(); // 2
      pitfalls:
      - Capturing variable vs capturing value
      - Variable resolution with shadowing
      - This in closures
      concepts:
      - Lexical scoping
      - Closures
      - Environment chains
      estimated_hours: 4-6
      deliverables:
      - Closure implementation that captures a reference to the enclosing environment at function definition time
      - Free variable resolution that binds references to variables defined in outer scopes at compile or definition time
      - Closure as a first-class value that can be passed to functions, returned, and stored in data structures
      - Nested function support where inner functions close over variables from any enclosing function scope
    - id: 9
      name: Classes
      description: Add class declarations, instances, and methods. Chapter 12.
      acceptance_criteria:
      - The class keyword declares a named class and stores it as a callable class object in the environment
      - Calling ClassName() creates a new instance of the class and invokes the init() initializer if defined
      - Properties are accessed and assigned using dot notation (instance.field) on class instances
      - Methods defined inside the class body are callable on instances with 'this' bound to the receiver object
      - The init() initializer method is automatically called on instance creation and receives constructor arguments
      hints:
        level1: Class is a map of method names to functions. Instance has class + fields map.
        level2: this bound when method is accessed. Method call binds this to instance.
        level3: |-
          # Class and instance implementation
          class LoxClass:
              def __init__(self, name, superclass, methods):
                  self.name = name
                  self.superclass = superclass
                  self.methods = methods

              def call(self, interpreter, arguments):
                  instance = LoxInstance(self)
                  # Call initializer if present
                  initializer = self.find_method("init")
                  if initializer:
                      initializer.bind(instance).call(interpreter, arguments)
                  return instance

              def arity(self):
                  initializer = self.find_method("init")
                  return 0 if not initializer else initializer.arity()

              def find_method(self, name):
                  if name in self.methods:
                      return self.methods[name]
                  if self.superclass:
                      return self.superclass.find_method(name)
                  return None

          class LoxInstance:
              def __init__(self, klass):
                  self.klass = klass
                  self.fields = {}

              def get(self, name):
                  if name.lexeme in self.fields:
                      return self.fields[name.lexeme]
                  method = self.klass.find_method(name.lexeme)
                  if method:
                      return method.bind(self)  # Bind 'this'
                  raise RuntimeError(f"Undefined property '{name.lexeme}'.")

              def set(self, name, value):
                  self.fields[name.lexeme] = value

          # Bind 'this' to method
          def bind(self, instance):
              environment = Environment(self.closure)
              environment.define("this", instance)
              return LoxFunction(self.declaration, environment)
      pitfalls:
      - this outside method
      - Returning from init()
      - Method vs function
      concepts:
      - Classes and instances
      - This binding
      - Constructors
      estimated_hours: 4-5
      deliverables:
      - Class declaration that creates a class object with a name, methods, and optional initializer
      - Instance creation by calling the class name as a constructor function that returns a new object
      - Property get and set operations using dot notation on instance objects for field access and assignment
      - Method definition with implicit 'this' binding that provides access to the instance within method bodies
    - id: 10
      name: Inheritance
      description: Add class inheritance and super calls. Chapter 13.
      acceptance_criteria:
      - Syntax 'class Derived < Base' creates a subclass that inherits all methods from the superclass
      - Methods defined on the superclass are callable on instances of the derived class without redefinition
      - super.method() inside a subclass method calls the superclass version of that method, not the overridden one
      - Method resolution looks up the class hierarchy from the instance's class to its superclass chain in order
      hints:
        level1: Subclass stores reference to superclass. Method lookup chains upward.
        level2: super is looked up in enclosing class, not dynamically.
        level3: |-
          class Doughnut {
            cook() { print "Fry"; }
          }
          class BostonCream < Doughnut {
            cook() {
              super.cook();
              print "Pipe cream";
            }
          }
      pitfalls:
      - super outside class
      - Inheriting from non-class
      - Diamond inheritance
      concepts:
      - Single inheritance
      - Super calls
      - Method resolution
      estimated_hours: 3-4
      deliverables:
      - Superclass specification using 'class Derived < Base' syntax in the class declaration
      - Method inheritance that allows instances of the derived class to call methods defined on the base class
      - Super keyword that invokes a method on the superclass, bypassing the subclass override if present
      - Initializer chaining where the derived class init calls super.init() to run the base class constructor
  build-kafka:
    id: build-kafka
    name: Build Your Own Kafka
    description: Build a distributed message queue with partitions and consumer groups. Learn pub/sub, ordering, and scalability.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - Replicated log
    - Distributed systems
    - Binary protocols
    languages:
      recommended:
      - Go
      - Java
      - Rust
      also_possible:
      - Scala
      - C++
    resources:
    - type: book
      name: 'Kafka: The Definitive Guide'
      url: https://www.confluent.io/resources/kafka-the-definitive-guide/
    - type: paper
      name: 'Kafka: a Distributed Messaging System'
      url: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf
    milestones:
    - id: 1
      name: Topic and Partitions
      description: Implement topic management with multiple partitions.
      acceptance_criteria:
      - Topics are created with a user-specified partition count and each partition is initialized as an empty log
      - Each partition functions as an append-only log where messages are assigned sequential offsets starting from zero
      - Message key hashing consistently routes messages with the same key to the same partition for ordering guarantees
      - Consumer offset tracking stores the last consumed offset per partition per consumer group for resume-on-restart
      hints:
        level1: Topic = name + list of partitions. Each partition is an independent log.
        level2: Hash message key to determine partition. Null key = round-robin.
        level3: "import hashlib\nfrom dataclasses import dataclass\nfrom typing import Optional, List\n\n@dataclass\nclass\
          \ Message:\n    key: Optional[bytes]\n    value: bytes\n    timestamp: int\n    offset: int = -1  # Set by partition\n\
          \nclass Partition:\n    def __init__(self, topic: str, partition_id: int, log_dir: str):\n        self.topic = topic\n\
          \        self.id = partition_id\n        self.log_path = f'{log_dir}/{topic}-{partition_id}'\n        self.messages\
          \ = []  # In production, this is file-backed\n        self.next_offset = 0\n        self._load_log()\n    \n   \
          \ def append(self, msg: Message) -> int:\n        msg.offset = self.next_offset\n        self.next_offset += 1\n\
          \        self.messages.append(msg)\n        self._persist(msg)\n        return msg.offset\n    \n    def read(self,\
          \ start_offset: int, max_messages: int) -> List[Message]:\n        if start_offset >= self.next_offset:\n      \
          \      return []\n        end = min(start_offset + max_messages, self.next_offset)\n        return self.messages[start_offset:end]\n\
          \nclass Topic:\n    def __init__(self, name: str, num_partitions: int, log_dir: str):\n        self.name = name\n\
          \        self.partitions = [\n            Partition(name, i, log_dir)\n            for i in range(num_partitions)\n\
          \        ]\n    \n    def get_partition(self, key: Optional[bytes]) -> Partition:\n        if key is None:\n   \
          \         # Round-robin for null keys\n            return self.partitions[self._rr_counter() % len(self.partitions)]\n\
          \        \n        # Hash-based assignment\n        hash_val = int(hashlib.md5(key).hexdigest(), 16)\n        return\
          \ self.partitions[hash_val % len(self.partitions)]\n    \n    def produce(self, key: Optional[bytes], value: bytes)\
          \ -> tuple:\n        partition = self.get_partition(key)\n        msg = Message(key=key, value=value, timestamp=int(time.time()\
          \ * 1000))\n        offset = partition.append(msg)\n        return partition.id, offset"
      pitfalls:
      - Partition assignment consistency
      - Offset gaps
      - Key null handling
      concepts:
      - Partitioning
      - Ordered logs
      - Horizontal scaling
      estimated_hours: 10-15
      deliverables:
      - Topic creation API that registers a new topic with a specified name, partition count, and replication factor
      - Partition management module that represents each partition as an independent append-only log segment
      - Partition-to-broker assignment logic that distributes partition leadership across brokers for load balancing
      - Topic metadata storage that records topic configuration, partition list, and leader assignments for cluster discovery
    - id: 2
      name: Producer
      description: Implement producer with batching and acknowledgments.
      acceptance_criteria:
      - Batch accumulation groups multiple messages and sends them in a single produce request for throughput
      - Configurable acks (0=fire-and-forget, 1=leader-ack, all=ISR-ack) control durability guarantees per produce request
      - Retry logic re-sends failed produce requests with exponential backoff up to a configurable maximum attempt count
      - Idempotent producer assigns sequence numbers to detect and discard duplicate retries at the broker (optional)
      hints:
        level1: Batch messages by partition. Send when batch full or timeout.
        level2: 'acks=0: fire and forget. acks=1: leader ack. acks=all: all replicas.'
        level3: "from collections import defaultdict\nimport asyncio\n\nclass Producer:\n    def __init__(self, bootstrap_servers,\
          \ acks='1', batch_size=16384, linger_ms=5):\n        self.brokers = bootstrap_servers\n        self.acks = acks\n\
          \        self.batch_size = batch_size\n        self.linger_ms = linger_ms\n        \n        self.batches = defaultdict(list)\
          \  # (topic, partition) -> [messages]\n        self.batch_sizes = defaultdict(int)\n        self.pending = {}  #\
          \ Future results\n        \n        self._sender_task = asyncio.create_task(self._sender_loop())\n    \n    async\
          \ def send(self, topic: str, key: bytes, value: bytes) -> asyncio.Future:\n        partition = self._partition_for(topic,\
          \ key)\n        \n        future = asyncio.Future()\n        msg = ProducerRecord(topic, partition, key, value,\
          \ future)\n        \n        batch_key = (topic, partition)\n        self.batches[batch_key].append(msg)\n     \
          \   self.batch_sizes[batch_key] += len(value)\n        \n        # Check if batch is full\n        if self.batch_sizes[batch_key]\
          \ >= self.batch_size:\n            await self._send_batch(batch_key)\n        \n        return future\n    \n  \
          \  async def _sender_loop(self):\n        while True:\n            await asyncio.sleep(self.linger_ms / 1000)\n\
          \            \n            for batch_key in list(self.batches.keys()):\n                if self.batches[batch_key]:\n\
          \                    await self._send_batch(batch_key)\n    \n    async def _send_batch(self, batch_key):\n    \
          \    topic, partition = batch_key\n        messages = self.batches.pop(batch_key, [])\n        self.batch_sizes.pop(batch_key,\
          \ 0)\n        \n        if not messages:\n            return\n        \n        try:\n            broker = await\
          \ self._get_leader(topic, partition)\n            response = await broker.produce(\n                topic=topic,\n\
          \                partition=partition,\n                messages=[(m.key, m.value) for m in messages],\n        \
          \        acks=self.acks\n            )\n            \n            # Resolve futures\n            for i, msg in enumerate(messages):\n\
          \                msg.future.set_result(RecordMetadata(\n                    topic=topic,\n                    partition=partition,\n\
          \                    offset=response.base_offset + i\n                ))\n        except Exception as e:\n     \
          \       for msg in messages:\n                msg.future.set_exception(e)"
      pitfalls:
      - Batch timeout handling
      - Leader failover during send
      - Duplicate messages
      concepts:
      - Batching
      - Acknowledgments
      - At-least-once delivery
      estimated_hours: 12-18
      deliverables:
      - Message serialization that encodes key, value, headers, and timestamp into the record batch wire format
      - Partition selection logic using key hash, round-robin, or custom partitioner to choose the target partition
      - Batch accumulation buffer that groups multiple messages into a single produce request for network efficiency
      - Delivery acknowledgment handler that confirms successful writes based on the configured acks level
    - id: 3
      name: Consumer Groups
      description: Implement consumer groups with partition assignment and rebalancing.
      acceptance_criteria:
      - Group membership protocol handles consumer join, heartbeat, and leave requests through the group coordinator
      - Partition assignment strategies (range, round-robin) distribute partitions evenly among active group members
      - Offset commits persist the consumer's position per partition so it can resume after restart without reprocessing
      - Rebalancing redistributes partitions when a consumer joins, leaves, or fails its heartbeat within the session timeout
      hints:
        level1: Each partition assigned to exactly one consumer in group. Rebalance on membership change.
        level2: Coordinator manages group. Consumers send heartbeats. Leader assigns partitions.
        level3: "class ConsumerGroup:\n    def __init__(self, group_id: str, coordinator):\n        self.group_id = group_id\n\
          \        self.coordinator = coordinator\n        self.members = {}  # member_id -> ConsumerMember\n        self.generation\
          \ = 0\n        self.leader = None\n        self.assignment = {}  # member_id -> [partitions]\n    \n    def join(self,\
          \ member_id: str, subscriptions: List[str]) -> JoinGroupResponse:\n        self.members[member_id] = ConsumerMember(member_id,\
          \ subscriptions)\n        \n        # Trigger rebalance\n        self.generation += 1\n        \n        # Elect\
          \ leader (first member)\n        if self.leader is None or self.leader not in self.members:\n            self.leader\
          \ = member_id\n        \n        return JoinGroupResponse(\n            generation=self.generation,\n          \
          \  leader=self.leader,\n            member_id=member_id,\n            members=list(self.members.keys()) if member_id\
          \ == self.leader else []\n        )\n    \n    def sync(self, member_id: str, generation: int, assignment: dict)\
          \ -> SyncGroupResponse:\n        if generation != self.generation:\n            raise RebalanceInProgress()\n  \
          \      \n        if member_id == self.leader:\n            self.assignment = assignment\n        \n        return\
          \ SyncGroupResponse(\n            assignment=self.assignment.get(member_id, [])\n        )\n\nclass Consumer:\n\
          \    def __init__(self, group_id: str, bootstrap_servers):\n        self.group_id = group_id\n        self.member_id\
          \ = None\n        self.generation = None\n        self.assignment = []\n        self.offsets = {}  # (topic, partition)\
          \ -> offset\n    \n    async def subscribe(self, topics: List[str]):\n        self.subscriptions = topics\n    \
          \    await self._join_group()\n    \n    async def poll(self, timeout_ms: int) -> List[ConsumerRecord]:\n      \
          \  records = []\n        for topic, partition in self.assignment:\n            offset = self.offsets.get((topic,\
          \ partition), 0)\n            batch = await self._fetch(topic, partition, offset)\n            records.extend(batch)\n\
          \            if batch:\n                self.offsets[(topic, partition)] = batch[-1].offset + 1\n        return\
          \ records\n    \n    async def commit(self):\n        await self.coordinator.commit_offsets(\n            self.group_id,\n\
          \            self.member_id,\n            self.generation,\n            self.offsets\n        )\n\n# Partition assignment\
          \ (Range strategy)\ndef range_assignment(members: List[str], partitions: List[tuple]) -> dict:\n    assignment =\
          \ {m: [] for m in members}\n    \n    # Group partitions by topic\n    by_topic = defaultdict(list)\n    for topic,\
          \ partition in partitions:\n        by_topic[topic].append(partition)\n    \n    for topic, parts in by_topic.items():\n\
          \        parts.sort()\n        per_consumer = len(parts) // len(members)\n        extra = len(parts) % len(members)\n\
          \        \n        idx = 0\n        for i, member in enumerate(sorted(members)):\n            count = per_consumer\
          \ + (1 if i < extra else 0)\n            for p in parts[idx:idx+count]:\n                assignment[member].append((topic,\
          \ p))\n            idx += count\n    \n    return assignment"
      pitfalls:
      - Rebalance storms
      - Stuck rebalances
      - Duplicate processing during rebalance
      concepts:
      - Consumer groups
      - Partition assignment
      - Rebalancing
      estimated_hours: 18-30
      deliverables:
      - Consumer group membership protocol that registers consumers with the group coordinator broker
      - Partition assignment strategies (range and round-robin) that distribute partitions among group members
      - Offset commit and fetch operations that persist consumer progress and resume from the last committed position
      - Rebalancing protocol that triggers partition reassignment when consumers join or leave the group
    - id: 4
      name: Replication
      description: Implement partition replication for fault tolerance.
      acceptance_criteria:
      - Leader-follower replication continuously copies new records from the partition leader to all assigned followers
      - ISR set is maintained by removing followers that fall behind the leader by more than the configured lag threshold
      - Leader election selects a new leader from the current ISR set within seconds when the existing leader becomes unavailable
      - High watermark advances only after all ISR members acknowledge the offset, ensuring consumers never read uncommitted
        data
      hints:
        level1: Each partition has leader and followers. Leader handles reads/writes. Followers fetch from leader.
        level2: ISR = replicas within lag threshold. Only commit when all ISR have message.
        level3: "class ReplicatedPartition:\n    def __init__(self, topic, partition_id, replicas, leader_id):\n        self.topic\
          \ = topic\n        self.id = partition_id\n        self.replicas = replicas  # [broker_id, ...]\n        self.leader\
          \ = leader_id\n        self.isr = set(replicas)  # In-sync replicas\n        \n        self.log = []  # Local log\n\
          \        self.high_watermark = -1  # Last committed offset\n        self.leo = -1  # Log end offset\n        \n\
          \        # Leader state\n        self.replica_leo = {}  # replica_id -> their LEO\n    \n    def append_as_leader(self,\
          \ messages: List) -> int:\n        base_offset = self.leo + 1\n        \n        for i, msg in enumerate(messages):\n\
          \            msg.offset = base_offset + i\n            self.log.append(msg)\n        \n        self.leo = self.log[-1].offset\n\
          \        self._update_high_watermark()\n        return base_offset\n    \n    def fetch_as_follower(self, fetch_offset:\
          \ int, max_bytes: int) -> FetchResponse:\n        messages = []\n        size = 0\n        \n        for msg in\
          \ self.log[fetch_offset:]:\n            if size + len(msg.value) > max_bytes:\n                break\n         \
          \   messages.append(msg)\n            size += len(msg.value)\n        \n        return FetchResponse(\n        \
          \    messages=messages,\n            high_watermark=self.high_watermark\n        )\n    \n    def update_follower_state(self,\
          \ follower_id: int, follower_leo: int):\n        self.replica_leo[follower_id] = follower_leo\n        \n      \
          \  # Check if follower is in sync\n        if follower_leo >= self.leo - self.max_lag:\n            self.isr.add(follower_id)\n\
          \        else:\n            self.isr.discard(follower_id)\n        \n        self._update_high_watermark()\n   \
          \ \n    def _update_high_watermark(self):\n        # HW = min LEO of all ISR\n        if not self.isr:\n       \
          \     return\n        \n        isr_leos = [self.replica_leo.get(r, self.leo) for r in self.isr]\n        new_hw\
          \ = min(isr_leos)\n        \n        if new_hw > self.high_watermark:\n            self.high_watermark = new_hw\n\
          \nclass ReplicaFetcher:\n    \"\"\"Follower thread that fetches from leader\"\"\"\n    \n    def __init__(self,\
          \ partition, leader_broker):\n        self.partition = partition\n        self.leader = leader_broker\n    \n  \
          \  async def run(self):\n        while True:\n            try:\n                response = await self.leader.fetch(\n\
          \                    self.partition.topic,\n                    self.partition.id,\n                    fetch_offset=self.partition.leo\
          \ + 1\n                )\n                \n                for msg in response.messages:\n                    self.partition.log.append(msg)\n\
          \                    self.partition.leo = msg.offset\n                \n                self.partition.high_watermark\
          \ = min(\n                    self.partition.high_watermark,\n                    response.high_watermark\n    \
          \            )\n            except LeaderNotAvailable:\n                await asyncio.sleep(1)\n               \
          \ await self._find_new_leader()"
      pitfalls:
      - ISR shrinking to empty
      - Unclean leader election
      - Data loss on failover
      concepts:
      - Replication
      - ISR
      - High watermark
      - Exactly-once semantics
      estimated_hours: 20-37
      deliverables:
      - Leader election per partition that selects a new leader from the in-sync replica set when the current leader fails
      - In-sync replica (ISR) tracking that maintains the set of followers whose logs are fully caught up with the leader
      - Follower fetch protocol that replicates new records from the leader to keep follower logs synchronized
      - High watermark advancement that tracks the offset up to which all ISR members have replicated for safe consumer reads
  build-lsp:
    id: build-lsp
    name: Build Your Own LSP Server
    description: Build a Language Server Protocol server for IDE features.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - JSON-RPC
    - AST/parsing
    - IDE concepts
    - Concurrency
    languages:
      recommended:
      - TypeScript
      - Rust
      - Go
      also_possible:
      - Python
      - C#
    resources:
    - type: specification
      name: LSP Specification
      url: https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/
    - type: article
      name: LSP Tutorial
      url: https://code.visualstudio.com/api/language-extensions/language-server-extension-guide
    milestones:
    - id: 1
      name: JSON-RPC & Initialization
      description: Implement JSON-RPC transport and LSP initialization.
      acceptance_criteria:
      - JSON-RPC messages are correctly framed with Content-Length headers and parsed from the byte stream
      - Initialize/initialized handshake completes successfully and the server transitions to the ready state
      - Capability negotiation returns a ServerCapabilities object listing all features the server supports
      - Shutdown request returns success and the subsequent exit notification causes the server process to terminate with
        exit code 0
      hints:
        level1: LSP uses JSON-RPC 2.0 over stdio. Messages have Content-Length header.
        level2: Server advertises capabilities in initialize response.
        level3: "## LSP Transport\n\n```typescript\nimport * as readline from 'readline';\n\ninterface Message {\n  jsonrpc:\
          \ '2.0';\n  id?: number | string;\n  method?: string;\n  params?: any;\n  result?: any;\n  error?: { code: number;\
          \ message: string };\n}\n\nclass LSPServer {\n  private pendingData = '';\n  private contentLength = -1;\n  \n \
          \ constructor() {\n    process.stdin.on('data', (data) => this.handleData(data.toString()));\n  }\n  \n  private\
          \ handleData(data: string) {\n    this.pendingData += data;\n    \n    while (true) {\n      if (this.contentLength\
          \ === -1) {\n        const headerEnd = this.pendingData.indexOf('\\r\\n\\r\\n');\n        if (headerEnd === -1)\
          \ return;\n        \n        const header = this.pendingData.slice(0, headerEnd);\n        const match = header.match(/Content-Length:\
          \ (\\d+)/);\n        if (!match) throw new Error('Invalid header');\n        \n        this.contentLength = parseInt(match[1]);\n\
          \        this.pendingData = this.pendingData.slice(headerEnd + 4);\n      }\n      \n      if (this.pendingData.length\
          \ < this.contentLength) return;\n      \n      const content = this.pendingData.slice(0, this.contentLength);\n\
          \      this.pendingData = this.pendingData.slice(this.contentLength);\n      this.contentLength = -1;\n      \n\
          \      this.handleMessage(JSON.parse(content));\n    }\n  }\n  \n  private handleMessage(msg: Message) {\n    if\
          \ (msg.method) {\n      this.handleRequest(msg);\n    } else if (msg.id !== undefined) {\n      // Response to our\
          \ request (not common for servers)\n    }\n  }\n  \n  private handleRequest(msg: Message) {\n    switch (msg.method)\
          \ {\n      case 'initialize':\n        this.sendResponse(msg.id, {\n          capabilities: {\n            textDocumentSync:\
          \ 1,  // Full sync\n            completionProvider: { triggerCharacters: ['.'] },\n            hoverProvider: true,\n\
          \            definitionProvider: true,\n          }\n        });\n        break;\n        \n      case 'initialized':\n\
          \        // Client is ready\n        break;\n        \n      case 'shutdown':\n        this.sendResponse(msg.id,\
          \ null);\n        break;\n        \n      case 'exit':\n        process.exit(0);\n    }\n  }\n  \n  private sendResponse(id:\
          \ number | string, result: any) {\n    this.send({ jsonrpc: '2.0', id, result });\n  }\n  \n  private send(msg:\
          \ Message) {\n    const content = JSON.stringify(msg);\n    const header = `Content-Length: ${Buffer.byteLength(content)}\\\
          r\\n\\r\\n`;\n    process.stdout.write(header + content);\n  }\n}\n```"
      pitfalls:
      - Content-Length calculation
      - Partial message handling
      - Encoding issues
      concepts:
      - JSON-RPC
      - Protocol negotiation
      - Streaming
      estimated_hours: 8-12
      deliverables:
      - JSON-RPC message parser and serializer handling Content-Length framing over stdin/stdout transport
      - Initialize request handler that receives client capabilities and returns the server's supported capabilities
      - Server capabilities negotiation that advertises supported features (completion, hover, diagnostics, etc.)
      - Shutdown and exit handling that cleanly terminates the server process when the client disconnects
    - id: 2
      name: Document Synchronization
      description: Track document changes from the editor.
      acceptance_criteria:
      - textDocument/didOpen notification stores the document content and triggers initial analysis
      - textDocument/didChange notification applies edits and updates the stored document to the latest version
      - textDocument/didClose notification removes the document from the server's in-memory store
      - Incremental sync applies range-based text changes correctly, updating only the modified portion of the document
      hints:
        level1: Track open documents in a map. Update on didChange.
        level2: 'Incremental sync: apply text edits to stored content.'
        level3: "## Document Sync\n\n```typescript\ninterface TextDocument {\n  uri: string;\n  languageId: string;\n  version:\
          \ number;\n  content: string;\n}\n\nclass DocumentManager {\n  private documents = new Map<string, TextDocument>();\n\
          \  \n  open(uri: string, languageId: string, version: number, content: string) {\n    this.documents.set(uri, {\
          \ uri, languageId, version, content });\n  }\n  \n  close(uri: string) {\n    this.documents.delete(uri);\n  }\n\
          \  \n  get(uri: string): TextDocument | undefined {\n    return this.documents.get(uri);\n  }\n  \n  update(uri:\
          \ string, version: number, changes: TextDocumentContentChangeEvent[]) {\n    const doc = this.documents.get(uri);\n\
          \    if (!doc) return;\n    \n    for (const change of changes) {\n      if ('range' in change) {\n        // Incremental\
          \ change\n        const start = this.offsetAt(doc.content, change.range.start);\n        const end = this.offsetAt(doc.content,\
          \ change.range.end);\n        doc.content = doc.content.slice(0, start) + change.text + doc.content.slice(end);\n\
          \      } else {\n        // Full content\n        doc.content = change.text;\n      }\n    }\n    doc.version =\
          \ version;\n  }\n  \n  private offsetAt(content: string, position: Position): number {\n    const lines = content.split('\\\
          n');\n    let offset = 0;\n    for (let i = 0; i < position.line; i++) {\n      offset += lines[i].length + 1; \
          \ // +1 for newline\n    }\n    return offset + position.character;\n  }\n  \n  positionAt(content: string, offset:\
          \ number): Position {\n    const lines = content.slice(0, offset).split('\\n');\n    return {\n      line: lines.length\
          \ - 1,\n      character: lines[lines.length - 1].length\n    };\n  }\n}\n\n// In LSPServer\ncase 'textDocument/didOpen':\n\
          \  this.documents.open(\n    msg.params.textDocument.uri,\n    msg.params.textDocument.languageId,\n    msg.params.textDocument.version,\n\
          \    msg.params.textDocument.text\n  );\n  this.validate(msg.params.textDocument.uri);\n  break;\n\ncase 'textDocument/didChange':\n\
          \  this.documents.update(\n    msg.params.textDocument.uri,\n    msg.params.textDocument.version,\n    msg.params.contentChanges\n\
          \  );\n  this.validate(msg.params.textDocument.uri);\n  break;\n```"
      pitfalls:
      - Version mismatch
      - Offset calculation errors
      - Unicode handling
      concepts:
      - Document tracking
      - Incremental updates
      - Positions
      estimated_hours: 8-12
      deliverables:
      - didOpen handler that stores the full document text when a file is opened in the editor
      - didClose handler that removes the document from the server's in-memory document store
      - didChange handler that applies incremental or full text edits to the stored document content
      - Document state manager that tracks document URIs, version numbers, and current text content
    - id: 3
      name: Language Features
      description: Implement completion, hover, and go-to-definition.
      acceptance_criteria:
      - textDocument/completion returns a list of relevant completion items based on the cursor position and context
      - textDocument/hover returns a MarkupContent response with type signature and documentation for the hovered symbol
      - textDocument/definition returns the source location (file URI, line, column) of the symbol's declaration
      - Symbol resolution correctly maps identifier references to their declaring scope and definition location
      hints:
        level1: Parse document to AST. Find symbol at cursor position.
        level2: Build symbol table for quick lookups. Handle imports/includes.
        level3: "## Language Features\n\n```typescript\ninterface Symbol {\n  name: string;\n  kind: SymbolKind;\n  location:\
          \ Location;\n  type?: string;\n  documentation?: string;\n}\n\nclass LanguageService {\n  private symbolTable =\
          \ new Map<string, Symbol[]>();  // uri -> symbols\n  \n  analyze(uri: string, content: string) {\n    const ast\
          \ = this.parse(content);\n    const symbols: Symbol[] = [];\n    \n    // Walk AST to collect symbols\n    this.walk(ast,\
          \ (node) => {\n      if (node.type === 'FunctionDeclaration') {\n        symbols.push({\n          name: node.name,\n\
          \          kind: SymbolKind.Function,\n          location: { uri, range: node.range },\n          type: this.formatFunctionType(node),\n\
          \          documentation: node.docComment\n        });\n      } else if (node.type === 'VariableDeclaration') {\n\
          \        symbols.push({\n          name: node.name,\n          kind: SymbolKind.Variable,\n          location: {\
          \ uri, range: node.range },\n          type: node.declaredType\n        });\n      }\n    });\n    \n    this.symbolTable.set(uri,\
          \ symbols);\n  }\n  \n  getCompletions(uri: string, position: Position): CompletionItem[] {\n    const doc = this.documents.get(uri);\n\
          \    const symbols = this.symbolTable.get(uri) || [];\n    const context = this.getContextAtPosition(doc.content,\
          \ position);\n    \n    // Filter symbols based on context\n    return symbols\n      .filter(s => this.isVisible(s,\
          \ position))\n      .map(s => ({\n        label: s.name,\n        kind: s.kind,\n        detail: s.type,\n     \
          \   documentation: s.documentation\n      }));\n  }\n  \n  getHover(uri: string, position: Position): Hover | null\
          \ {\n    const symbol = this.findSymbolAtPosition(uri, position);\n    if (!symbol) return null;\n    \n    return\
          \ {\n      contents: {\n        kind: 'markdown',\n        value: `**${symbol.name}**: ${symbol.type || 'unknown'}\\\
          n\\n${symbol.documentation || ''}`\n      }\n    };\n  }\n  \n  getDefinition(uri: string, position: Position):\
          \ Location | null {\n    const word = this.getWordAtPosition(uri, position);\n    \n    // Search all documents\
          \ for definition\n    for (const [docUri, symbols] of this.symbolTable) {\n      const symbol = symbols.find(s =>\
          \ s.name === word);\n      if (symbol) {\n        return symbol.location;\n      }\n    }\n    \n    return null;\n\
          \  }\n  \n  private findSymbolAtPosition(uri: string, position: Position): Symbol | null {\n    const word = this.getWordAtPosition(uri,\
          \ position);\n    const symbols = this.symbolTable.get(uri) || [];\n    return symbols.find(s => s.name === word)\
          \ || null;\n  }\n}\n```"
      pitfalls:
      - Stale symbol table
      - Scope visibility
      - Performance on large files
      concepts:
      - Symbol resolution
      - AST analysis
      - IDE features
      estimated_hours: 15-20
      deliverables:
      - Completion provider that returns context-appropriate completion items with labels, kinds, and documentation
      - Hover provider that returns type information and documentation for the symbol under the cursor
      - Go-to-definition implementation that resolves a symbol reference to its declaration source location
      - Find-all-references implementation that locates every usage of a symbol across the open documents
    - id: 4
      name: Diagnostics & Code Actions
      description: Report errors and suggest fixes.
      acceptance_criteria:
      - textDocument/publishDiagnostics pushes diagnostics to the client whenever the document content changes
      - textDocument/codeAction returns applicable code actions (quick fixes, refactors) for the given range or diagnostic
      - Diagnostic severity is correctly set to Error, Warning, Information, or Hint for each reported issue
      - Quick fix code actions include text edits that, when applied, resolve the associated diagnostic
      hints:
        level1: Push diagnostics to client on document change.
        level2: Code actions provide fixes for diagnostics at a location.
        level3: "## Diagnostics & Code Actions\n\n```typescript\ninterface Diagnostic {\n  range: Range;\n  severity: DiagnosticSeverity;\n\
          \  code?: string | number;\n  source?: string;\n  message: string;\n  relatedInformation?: DiagnosticRelatedInformation[];\n\
          }\n\nclass DiagnosticProvider {\n  validate(uri: string, content: string): Diagnostic[] {\n    const diagnostics:\
          \ Diagnostic[] = [];\n    const ast = this.parse(content);\n    \n    // Check for errors\n    this.walk(ast, (node)\
          \ => {\n      // Undefined variable\n      if (node.type === 'Identifier' && !this.isDefined(node.name)) {\n   \
          \     diagnostics.push({\n          range: node.range,\n          severity: DiagnosticSeverity.Error,\n        \
          \  code: 'undefined-variable',\n          source: 'my-lsp',\n          message: `'${node.name}' is not defined`\n\
          \        });\n      }\n      \n      // Type mismatch\n      if (node.type === 'Assignment') {\n        const leftType\
          \ = this.getType(node.left);\n        const rightType = this.getType(node.right);\n        if (leftType && rightType\
          \ && !this.isAssignable(leftType, rightType)) {\n          diagnostics.push({\n            range: node.range,\n\
          \            severity: DiagnosticSeverity.Error,\n            code: 'type-mismatch',\n            source: 'my-lsp',\n\
          \            message: `Type '${rightType}' is not assignable to type '${leftType}'`\n          });\n        }\n\
          \      }\n      \n      // Unused variable (warning)\n      if (node.type === 'VariableDeclaration' && !this.isUsed(node.name))\
          \ {\n        diagnostics.push({\n          range: node.range,\n          severity: DiagnosticSeverity.Warning,\n\
          \          code: 'unused-variable',\n          source: 'my-lsp',\n          message: `'${node.name}' is declared\
          \ but never used`\n        });\n      }\n    });\n    \n    return diagnostics;\n  }\n}\n\nclass CodeActionProvider\
          \ {\n  getCodeActions(uri: string, range: Range, diagnostics: Diagnostic[]): CodeAction[] {\n    const actions:\
          \ CodeAction[] = [];\n    \n    for (const diag of diagnostics) {\n      switch (diag.code) {\n        case 'undefined-variable':\n\
          \          // Suggest declaring the variable\n          const varName = this.extractVarName(diag.message);\n   \
          \       actions.push({\n            title: `Declare variable '${varName}'`,\n            kind: CodeActionKind.QuickFix,\n\
          \            diagnostics: [diag],\n            edit: {\n              changes: {\n                [uri]: [{\n  \
          \                range: { start: { line: range.start.line, character: 0 }, end: { line: range.start.line, character:\
          \ 0 } },\n                  newText: `let ${varName};\\n`\n                }]\n              }\n            }\n\
          \          });\n          break;\n          \n        case 'unused-variable':\n          // Suggest removing or\
          \ prefixing with _\n          actions.push({\n            title: 'Remove unused variable',\n            kind: CodeActionKind.QuickFix,\n\
          \            diagnostics: [diag],\n            edit: {\n              changes: {\n                [uri]: [{ range:\
          \ diag.range, newText: '' }]\n              }\n            }\n          });\n          break;\n      }\n    }\n\
          \    \n    return actions;\n  }\n}\n\n// Publish diagnostics\nprivate validate(uri: string) {\n  const doc = this.documents.get(uri);\n\
          \  if (!doc) return;\n  \n  const diagnostics = this.diagnosticProvider.validate(uri, doc.content);\n  \n  this.sendNotification('textDocument/publishDiagnostics',\
          \ {\n    uri,\n    diagnostics\n  });\n}\n```"
      pitfalls:
      - Diagnostic spam
      - Outdated diagnostics
      - Large workspace performance
      concepts:
      - Static analysis
      - Quick fixes
      - Editor integration
      estimated_hours: 15-20
      deliverables:
      - Diagnostic publisher that sends error and warning diagnostics to the editor after each document change
      - Error and warning reporting with severity levels, source ranges, and descriptive messages per diagnostic
      - Code action provider that suggests fixes and refactorings applicable to the current selection or diagnostic
      - Quick fix implementations that generate text edits to automatically resolve common diagnostic issues
  build-nn-framework:
    id: build-nn-framework
    name: Build Your Own Neural Network Framework
    description: Build a PyTorch/TensorFlow-like deep learning framework with automatic differentiation.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - Linear algebra
    - Calculus (chain rule)
    - Python/NumPy
    - Basic neural networks
    languages:
      recommended:
      - Python
      - Rust
      - C++
      also_possible:
      - Julia
    resources:
    - type: video
      name: Micrograd by Karpathy
      url: https://www.youtube.com/watch?v=VMj-3S1tku0
    - type: article
      name: Autodiff from Scratch
      url: https://sidsite.com/posts/autodiff/
    - type: code
      name: Tinygrad
      url: https://github.com/tinygrad/tinygrad
    milestones:
    - id: 1
      name: Tensor & Operations
      description: Implement tensor data structure with basic operations.
      acceptance_criteria:
      - Tensor stores N-dimensional array data with shape and requires_grad attributes
      - Element-wise add, subtract, multiply, and divide produce correct results
      - Matrix multiplication yields correct output for 2D and batched inputs
      - Broadcasting automatically expands dimensions following NumPy rules
      - Optional GPU support offloads computation to CUDA device
      hints:
        level1: Wrap NumPy arrays, track shape and data. Implement __add__, __mul__, etc.
        level2: Support broadcasting rules. Matrix multiply with @ operator.
        level3: "## Tensor Implementation\n\n```python\nimport numpy as np\n\nclass Tensor:\n    def __init__(self, data,\
          \ requires_grad=False):\n        self.data = np.array(data, dtype=np.float32)\n        self.requires_grad = requires_grad\n\
          \        self.grad = None\n        self._backward = lambda: None\n        self._prev = set()\n    \n    @property\n\
          \    def shape(self):\n        return self.data.shape\n    \n    def __repr__(self):\n        return f\"Tensor({self.data},\
          \ requires_grad={self.requires_grad})\"\n    \n    def __add__(self, other):\n        other = other if isinstance(other,\
          \ Tensor) else Tensor(other)\n        out = Tensor(self.data + other.data, requires_grad=self.requires_grad or other.requires_grad)\n\
          \        \n        def _backward():\n            if self.requires_grad:\n                self.grad = self.grad +\
          \ out.grad if self.grad is not None else out.grad.copy()\n            if other.requires_grad:\n                other.grad\
          \ = other.grad + out.grad if other.grad is not None else out.grad.copy()\n        \n        out._backward = _backward\n\
          \        out._prev = {self, other}\n        return out\n    \n    def __mul__(self, other):\n        other = other\
          \ if isinstance(other, Tensor) else Tensor(other)\n        out = Tensor(self.data * other.data, requires_grad=self.requires_grad\
          \ or other.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n            \
          \    grad = other.data * out.grad\n                self.grad = self.grad + grad if self.grad is not None else grad\n\
          \            if other.requires_grad:\n                grad = self.data * out.grad\n                other.grad =\
          \ other.grad + grad if other.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev\
          \ = {self, other}\n        return out\n    \n    def __matmul__(self, other):\n        out = Tensor(self.data @\
          \ other.data, requires_grad=self.requires_grad or other.requires_grad)\n        \n        def _backward():\n   \
          \         if self.requires_grad:\n                grad = out.grad @ other.data.T\n                self.grad = self.grad\
          \ + grad if self.grad is not None else grad\n            if other.requires_grad:\n                grad = self.data.T\
          \ @ out.grad\n                other.grad = other.grad + grad if other.grad is not None else grad\n        \n   \
          \     out._backward = _backward\n        out._prev = {self, other}\n        return out\n    \n    def sum(self):\n\
          \        out = Tensor(self.data.sum(), requires_grad=self.requires_grad)\n        \n        def _backward():\n \
          \           if self.requires_grad:\n                grad = np.ones_like(self.data) * out.grad\n                self.grad\
          \ = self.grad + grad if self.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev\
          \ = {self}\n        return out\n```"
      pitfalls:
      - Broadcasting gradient shapes
      - In-place operations breaking grad
      - Memory management
      concepts:
      - Tensor operations
      - Broadcasting
      - Memory layout
      estimated_hours: 12-18
      deliverables:
      - N-dimensional tensor class with shape and dtype tracking
      - Element-wise arithmetic operations with operator overloading
      - Matrix multiplication supporting batched dimensions
      - NumPy-style broadcasting for mismatched tensor shapes
    - id: 2
      name: Automatic Differentiation
      description: Implement reverse-mode autodiff (backpropagation).
      acceptance_criteria:
      - Computation graph correctly tracks parent-child operation relationships
      - Reverse-mode autodiff computes gradients matching numerical differentiation
      - Gradient accumulation sums contributions when tensor used multiple times
      - Topological sort ensures correct backward pass ordering without missing nodes
      hints:
        level1: Build computation graph as operations happen. Backward traverses in reverse.
        level2: Topological sort ensures gradients flow correctly. Accumulate, don't replace.
        level3: "## Backpropagation\n\n```python\nclass Tensor:\n    # ... previous code ...\n    \n    def backward(self):\n\
          \        # Build topological order\n        topo = []\n        visited = set()\n        \n        def build_topo(v):\n\
          \            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n     \
          \               build_topo(child)\n                topo.append(v)\n        \n        build_topo(self)\n        \n\
          \        # Initialize gradient of output\n        self.grad = np.ones_like(self.data)\n        \n        # Backpropagate\n\
          \        for node in reversed(topo):\n            node._backward()\n    \n    def relu(self):\n        out = Tensor(np.maximum(0,\
          \ self.data), requires_grad=self.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n\
          \                grad = (self.data > 0) * out.grad\n                self.grad = self.grad + grad if self.grad is\
          \ not None else grad\n        \n        out._backward = _backward\n        out._prev = {self}\n        return out\n\
          \    \n    def exp(self):\n        out = Tensor(np.exp(self.data), requires_grad=self.requires_grad)\n        \n\
          \        def _backward():\n            if self.requires_grad:\n                grad = out.data * out.grad\n    \
          \            self.grad = self.grad + grad if self.grad is not None else grad\n        \n        out._backward =\
          \ _backward\n        out._prev = {self}\n        return out\n    \n    def log(self):\n        out = Tensor(np.log(self.data),\
          \ requires_grad=self.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n  \
          \              grad = out.grad / self.data\n                self.grad = self.grad + grad if self.grad is not None\
          \ else grad\n        \n        out._backward = _backward\n        out._prev = {self}\n        return out\n\ndef\
          \ softmax(x):\n    exp_x = (x - Tensor(x.data.max(axis=-1, keepdims=True))).exp()\n    return exp_x * Tensor(1.0\
          \ / exp_x.data.sum(axis=-1, keepdims=True))\n\ndef cross_entropy(pred, target):\n    # pred: (batch, classes), target:\
          \ (batch,) indices\n    batch_size = pred.shape[0]\n    log_probs = pred.log()\n    # Select log prob of correct\
          \ class\n    loss = Tensor(0.0, requires_grad=True)\n    for i in range(batch_size):\n        loss = loss - log_probs.data[i,\
          \ target[i]]\n    return loss * Tensor(1.0 / batch_size)\n```"
      pitfalls:
      - Gradient not accumulated
      - Wrong topological order
      - Vanishing/exploding gradients
      concepts:
      - Computational graphs
      - Chain rule
      - Backpropagation
      estimated_hours: 15-20
      deliverables:
      - Computation graph constructed during forward pass operations
      - Backward pass traversal using topological sort ordering
      - Gradient accumulation across multiple backward contributions
      - Gradient tape implementation recording operation history
    - id: 3
      name: Layers & Modules
      description: Implement neural network layers and module system.
      acceptance_criteria:
      - Linear layer produces correct output shape for given input and weight dimensions
      - Activation functions apply element-wise and propagate gradients correctly
      - Module.parameters() recursively collects all trainable tensors from submodules
      - Sequential container chains layers and forwards input through each in order
      hints:
        level1: 'Module stores parameters. Linear layer: y = Wx + b.'
        level2: Implement parameters() method to collect all trainable tensors.
        level3: "## Module System\n\n```python\nclass Module:\n    def __init__(self):\n        self._parameters = {}\n  \
          \      self._modules = {}\n    \n    def __setattr__(self, name, value):\n        if isinstance(value, Tensor) and\
          \ value.requires_grad:\n            self._parameters[name] = value\n        elif isinstance(value, Module):\n  \
          \          self._modules[name] = value\n        super().__setattr__(name, value)\n    \n    def parameters(self):\n\
          \        params = list(self._parameters.values())\n        for module in self._modules.values():\n            params.extend(module.parameters())\n\
          \        return params\n    \n    def zero_grad(self):\n        for p in self.parameters():\n            p.grad\
          \ = None\n    \n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\nclass\
          \ Linear(Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        # Xavier\
          \ initialization\n        k = np.sqrt(1 / in_features)\n        self.weight = Tensor(np.random.uniform(-k, k, (in_features,\
          \ out_features)), requires_grad=True)\n        self.bias = Tensor(np.zeros(out_features), requires_grad=True)\n\
          \    \n    def forward(self, x):\n        return x @ self.weight + self.bias\n\nclass ReLU(Module):\n    def forward(self,\
          \ x):\n        return x.relu()\n\nclass Sequential(Module):\n    def __init__(self, *layers):\n        super().__init__()\n\
          \        for i, layer in enumerate(layers):\n            setattr(self, f'layer_{i}', layer)\n        self.layers\
          \ = layers\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n     \
          \   return x\n\n# Example: MLP\nclass MLP(Module):\n    def __init__(self, in_dim, hidden_dim, out_dim):\n     \
          \   super().__init__()\n        self.fc1 = Linear(in_dim, hidden_dim)\n        self.fc2 = Linear(hidden_dim, out_dim)\n\
          \    \n    def forward(self, x):\n        x = self.fc1(x).relu()\n        x = self.fc2(x)\n        return x\n```"
      pitfalls:
      - Parameter not tracked
      - In-place modifications
      - Wrong initialization
      concepts:
      - Module pattern
      - Parameter management
      - Initialization
      estimated_hours: 12-18
      deliverables:
      - Linear layer implementing y = Wx + b with weight initialization
      - Activation functions including ReLU, sigmoid, and tanh
      - Module base class with recursive parameter collection
      - Parameter registration system tracking all trainable tensors
    - id: 4
      name: Optimizers & Training
      description: Implement optimizers and training loop.
      acceptance_criteria:
      - SGD updates parameters by subtracting learning rate times gradient
      - Adam optimizer converges faster than SGD on non-convex loss landscapes
      - Learning rate scheduling reduces rate according to configurable strategy
      - Mini-batch training shuffles data and processes fixed-size batches per epoch
      hints:
        level1: 'SGD: param -= lr * grad. Adam adds momentum and adaptive learning.'
        level2: Track running averages for Adam. Clip gradients to prevent explosion.
        level3: "## Optimizers\n\n```python\nclass Optimizer:\n    def __init__(self, parameters, lr=0.01):\n        self.parameters\
          \ = parameters\n        self.lr = lr\n    \n    def zero_grad(self):\n        for p in self.parameters:\n      \
          \      p.grad = None\n    \n    def step(self):\n        raise NotImplementedError\n\nclass SGD(Optimizer):\n  \
          \  def __init__(self, parameters, lr=0.01, momentum=0.0):\n        super().__init__(parameters, lr)\n        self.momentum\
          \ = momentum\n        self.velocities = [np.zeros_like(p.data) for p in parameters]\n    \n    def step(self):\n\
          \        for i, p in enumerate(self.parameters):\n            if p.grad is None:\n                continue\n   \
          \         self.velocities[i] = self.momentum * self.velocities[i] - self.lr * p.grad\n            p.data += self.velocities[i]\n\
          \nclass Adam(Optimizer):\n    def __init__(self, parameters, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n    \
          \    super().__init__(parameters, lr)\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps\
          \ = eps\n        self.t = 0\n        self.m = [np.zeros_like(p.data) for p in parameters]\n        self.v = [np.zeros_like(p.data)\
          \ for p in parameters]\n    \n    def step(self):\n        self.t += 1\n        for i, p in enumerate(self.parameters):\n\
          \            if p.grad is None:\n                continue\n            \n            # Update biased first moment\
          \ estimate\n            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * p.grad\n            # Update biased\
          \ second raw moment estimate\n            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (p.grad ** 2)\n\
          \            \n            # Compute bias-corrected estimates\n            m_hat = self.m[i] / (1 - self.beta1 **\
          \ self.t)\n            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n            \n            # Update parameters\n\
          \            p.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n\n# Training loop\ndef train(model, X, y,\
          \ epochs=100, batch_size=32, lr=0.01):\n    optimizer = Adam(model.parameters(), lr=lr)\n    n_samples = len(X)\n\
          \    \n    for epoch in range(epochs):\n        # Shuffle data\n        indices = np.random.permutation(n_samples)\n\
          \        total_loss = 0\n        \n        for i in range(0, n_samples, batch_size):\n            batch_idx = indices[i:i+batch_size]\n\
          \            x_batch = Tensor(X[batch_idx], requires_grad=False)\n            y_batch = y[batch_idx]\n         \
          \   \n            # Forward\n            pred = model(x_batch)\n            loss = cross_entropy(softmax(pred),\
          \ y_batch)\n            \n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n\
          \            optimizer.step()\n            \n            total_loss += loss.data\n        \n        if epoch % 10\
          \ == 0:\n            print(f\"Epoch {epoch}, Loss: {total_loss / (n_samples / batch_size):.4f}\")\n```"
      pitfalls:
      - Bias correction in Adam
      - Learning rate too high/low
      - Not shuffling data
      concepts:
      - Optimization algorithms
      - Momentum
      - Adaptive learning rates
      estimated_hours: 15-20
      deliverables:
      - SGD optimizer with configurable momentum and learning rate
      - Adam optimizer with bias-corrected first and second moment estimates
      - Training step abstraction managing forward, backward, and update
      - Loss functions including cross-entropy and mean squared error
  build-observability-platform:
    id: build-observability-platform
    name: Build Your Own Observability Platform
    description: Build a unified observability platform combining logs, metrics, and traces.
    difficulty: expert
    estimated_hours: 80-120
    prerequisites:
    - Distributed tracing
    - Metrics systems
    - Log aggregation
    - Time-series databases
    - Full-text search
    languages:
      recommended:
      - Go
      - Rust
      also_possible:
      - Java
      - Python
    resources:
    - type: book
      name: Observability Engineering
      url: https://www.oreilly.com/library/view/observability-engineering/9781492076438/
    - type: article
      name: Three Pillars of Observability
      url: https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html
    - type: documentation
      name: OpenTelemetry Collector
      url: https://opentelemetry.io/docs/collector/
    milestones:
    - id: 1
      name: Unified Data Model
      description: Design a unified data model that correlates logs, metrics, and traces.
      acceptance_criteria:
      - All telemetry signals share common resource attributes like service name and version
      - Logs can be queried by trace_id to find all logs for a request
      - Metrics include exemplar trace IDs linking to representative traces
      - Resource model identifies service name, version, instance, and environment
      - Data model is compatible with OpenTelemetry semantic conventions
      hints:
        level1: 'All signals share: timestamp, service, resource attributes, trace context.'
        level2: Use trace_id/span_id to correlate logs with traces. Use exemplars for metrics.
        level3: |-
          ## Unified Data Model

          ```go
          // Common resource attributes
          type Resource struct {
              ServiceName      string            `json:"service.name"`
              ServiceVersion   string            `json:"service.version"`
              ServiceInstance  string            `json:"service.instance.id"`
              HostName         string            `json:"host.name"`
              Environment      string            `json:"deployment.environment"`
              Attributes       map[string]string `json:"attributes"`
          }

          // Trace context for correlation
          type TraceContext struct {
              TraceID string `json:"trace_id,omitempty"`
              SpanID  string `json:"span_id,omitempty"`
          }

          // Log entry
          type LogRecord struct {
              Timestamp     time.Time         `json:"timestamp"`
              Resource      Resource          `json:"resource"`
              TraceContext  TraceContext      `json:"trace_context,omitempty"`
              SeverityText  string            `json:"severity_text"`
              SeverityNum   int               `json:"severity_number"`
              Body          string            `json:"body"`
              Attributes    map[string]string `json:"attributes"`
          }

          // Metric data point
          type MetricDataPoint struct {
              Timestamp   time.Time         `json:"timestamp"`
              Resource    Resource          `json:"resource"`
              Name        string            `json:"name"`
              Type        string            `json:"type"` // counter, gauge, histogram
              Value       float64           `json:"value"`
              Labels      map[string]string `json:"labels"`
              Exemplars   []Exemplar        `json:"exemplars,omitempty"`
          }

          // Exemplar links metric to trace
          type Exemplar struct {
              TraceID   string    `json:"trace_id"`
              SpanID    string    `json:"span_id"`
              Value     float64   `json:"value"`
              Timestamp time.Time `json:"timestamp"`
          }

          // Span (trace segment)
          type Span struct {
              TraceID      string            `json:"trace_id"`
              SpanID       string            `json:"span_id"`
              ParentSpanID string            `json:"parent_span_id,omitempty"`
              Resource     Resource          `json:"resource"`
              Name         string            `json:"name"`
              Kind         string            `json:"kind"` // client, server, internal
              StartTime    time.Time         `json:"start_time"`
              EndTime      time.Time         `json:"end_time"`
              Status       string            `json:"status"`
              Attributes   map[string]string `json:"attributes"`
              Events       []SpanEvent       `json:"events"`
              Links        []SpanLink        `json:"links"`
          }

          // Correlation helpers
          func (m *MetricDataPoint) GetExemplarTraces() []string {
              traceIDs := make([]string, len(m.Exemplars))
              for i, e := range m.Exemplars {
                  traceIDs[i] = e.TraceID
              }
              return traceIDs
          }

          func (db *ObservabilityDB) GetLogsForTrace(traceID string) ([]LogRecord, error) {
              return db.logs.Query(LogQuery{TraceID: traceID})
          }

          func (db *ObservabilityDB) GetMetricsWithExemplar(traceID string) ([]MetricDataPoint, error) {
              return db.metrics.QueryByExemplar(traceID)
          }
          ```
      pitfalls:
      - Schema drift across signals
      - Missing correlation IDs
      - Cardinality explosion
      concepts:
      - Data modeling
      - Signal correlation
      - OpenTelemetry semantics
      estimated_hours: 10-15
      deliverables:
      - Common attribute schema shared across logs, metrics, and traces
      - Trace-log correlation linking log records to trace and span IDs
      - Metric-trace correlation using exemplars to connect metrics to traces
      - Resource attribution model mapping telemetry to service instances
    - id: 2
      name: Data Ingestion Pipeline
      description: Build a high-throughput ingestion pipeline for all signal types.
      acceptance_criteria:
      - OTLP receiver accepts logs, metrics, and traces over gRPC protocol
      - Batch ingestion buffers up to configurable size before flushing to storage
      - Pipeline normalizes timestamps, resource attributes, and field names consistently
      - Backpressure returns appropriate error codes when ingestion capacity is exceeded
      - Pipeline processes at least 10,000 events per second on commodity hardware
      hints:
        level1: Accept OTLP (gRPC and HTTP). Buffer incoming data before writing to storage.
        level2: 'Implement pipelines: receivers -> processors -> exporters.'
        level3: "## Ingestion Pipeline\n\n```go\n// Pipeline architecture\ntype Pipeline struct {\n    receivers  []Receiver\n\
          \    processors []Processor\n    exporters  []Exporter\n    bufferSize int\n    dataChan   chan Signal\n}\n\ntype\
          \ Signal interface {\n    Type() string  // \"logs\", \"metrics\", \"traces\"\n    Resource() Resource\n}\n\n//\
          \ OTLP Receiver\ntype OTLPReceiver struct {\n    grpcServer *grpc.Server\n    httpServer *http.Server\n    output\
          \     chan<- Signal\n}\n\nfunc (r *OTLPReceiver) ReceiveTraces(ctx context.Context, req *otlp.TracesRequest) (*otlp.TracesResponse,\
          \ error) {\n    for _, resourceSpan := range req.ResourceSpans {\n        resource := convertResource(resourceSpan.Resource)\n\
          \        for _, scopeSpan := range resourceSpan.ScopeSpans {\n            for _, span := range scopeSpan.Spans {\n\
          \                r.output <- &Span{\n                    Resource:  resource,\n                    TraceID:   hex.EncodeToString(span.TraceId),\n\
          \                    SpanID:    hex.EncodeToString(span.SpanId),\n                    Name:      span.Name,\n  \
          \                  StartTime: time.Unix(0, int64(span.StartTimeUnixNano)),\n                    EndTime:   time.Unix(0,\
          \ int64(span.EndTimeUnixNano)),\n                    // ... convert other fields\n                }\n          \
          \  }\n        }\n    }\n    return &otlp.TracesResponse{}, nil\n}\n\n// Processors\ntype FilterProcessor struct\
          \ {\n    rules []FilterRule\n}\n\nfunc (p *FilterProcessor) Process(signal Signal) (Signal, bool) {\n    for _,\
          \ rule := range p.rules {\n        if rule.Matches(signal) {\n            if rule.Action == \"drop\" {\n       \
          \         return nil, false\n            }\n        }\n    }\n    return signal, true\n}\n\ntype AttributeProcessor\
          \ struct {\n    actions []AttributeAction\n}\n\nfunc (p *AttributeProcessor) Process(signal Signal) (Signal, bool)\
          \ {\n    for _, action := range p.actions {\n        switch action.Type {\n        case \"insert\":\n          \
          \  signal.SetAttribute(action.Key, action.Value)\n        case \"delete\":\n            signal.DeleteAttribute(action.Key)\n\
          \        case \"hash\":\n            // Hash PII\n            val := signal.GetAttribute(action.Key)\n         \
          \   signal.SetAttribute(action.Key, hash(val))\n        }\n    }\n    return signal, true\n}\n\n// Backpressure\
          \ handling\nfunc (p *Pipeline) Run(ctx context.Context) {\n    // Bounded buffer with backpressure\n    buffer :=\
          \ make(chan Signal, p.bufferSize)\n    \n    // Receiver goroutines write to buffer\n    for _, receiver := range\
          \ p.receivers {\n        go receiver.Start(ctx, buffer)\n    }\n    \n    // Worker pool processes signals\n   \
          \ var wg sync.WaitGroup\n    for i := 0; i < runtime.NumCPU(); i++ {\n        wg.Add(1)\n        go func() {\n \
          \           defer wg.Done()\n            for signal := range buffer {\n                // Run through processors\n\
          \                var ok bool\n                for _, proc := range p.processors {\n                    signal, ok\
          \ = proc.Process(signal)\n                    if !ok {\n                        break\n                    }\n \
          \               }\n                if !ok {\n                    continue\n                }\n                \n\
          \                // Send to exporters\n                for _, exp := range p.exporters {\n                    exp.Export(signal)\n\
          \                }\n            }\n        }()\n    }\n    \n    <-ctx.Done()\n    close(buffer)\n    wg.Wait()\n\
          }\n```"
      pitfalls:
      - Data loss under load
      - Memory exhaustion
      - Head-of-line blocking
      concepts:
      - Data pipelines
      - Backpressure
      - Protocol buffers
      estimated_hours: 15-20
      deliverables:
      - OTLP protocol receiver accepting gRPC and HTTP telemetry data
      - Batch ingestion pipeline buffering and flushing incoming data
      - Data normalization layer converting varied formats to unified model
      - Backpressure mechanism throttling producers when pipeline is saturated
    - id: 3
      name: Multi-Signal Storage
      description: Implement storage backends optimized for each signal type.
      acceptance_criteria:
      - Log storage supports full-text search returning results within 500ms for 1M logs
      - Metric storage handles high-cardinality time series with configurable retention
      - Trace storage reconstructs full span trees from individual span records
      - Cross-signal queries retrieve correlated logs and traces for a given time range
      - Storage engines support configurable retention policies with automatic cleanup
      hints:
        level1: 'Use different storage strategies: inverted index for logs, TSDB for metrics, span storage for traces.'
        level2: 'Implement tiered storage: hot (recent, fast) -> warm -> cold (archived, slow).'
        level3: "## Multi-Signal Storage\n\n```go\n// Storage interface per signal type\ntype LogStorage interface {\n   \
          \ Insert(log *LogRecord) error\n    Search(query LogQuery) ([]LogRecord, error)\n    Aggregate(query AggregationQuery)\
          \ (map[string]int64, error)\n}\n\ntype MetricStorage interface {\n    Insert(metric *MetricDataPoint) error\n  \
          \  Query(query MetricQuery) ([]MetricDataPoint, error)\n    Downsample(resolution time.Duration) error\n}\n\ntype\
          \ TraceStorage interface {\n    InsertSpan(span *Span) error\n    GetTrace(traceID string) (*Trace, error)\n   \
          \ SearchTraces(query TraceQuery) ([]*TraceSummary, error)\n    GetServiceGraph(start, end time.Time) (*ServiceGraph,\
          \ error)\n}\n\n// Log storage with inverted index\ntype LogStore struct {\n    index    *InvertedIndex\n    segments\
          \ []*LogSegment\n    mu       sync.RWMutex\n}\n\ntype InvertedIndex struct {\n    terms map[string]*PostingList\
          \  // term -> doc IDs\n    fields map[string]*FieldIndex  // field name -> field values -> doc IDs\n}\n\nfunc (s\
          \ *LogStore) Search(query LogQuery) ([]LogRecord, error) {\n    // Parse query into terms\n    terms := tokenize(query.Query)\n\
          \    \n    // Intersect posting lists\n    var docIDs *roaring.Bitmap\n    for _, term := range terms {\n      \
          \  if posting := s.index.terms[term]; posting != nil {\n            if docIDs == nil {\n                docIDs =\
          \ posting.bitmap.Clone()\n            } else {\n                docIDs.And(posting.bitmap)\n            }\n    \
          \    }\n    }\n    \n    // Apply field filters\n    if query.Service != \"\" {\n        fieldBitmap := s.index.fields[\"\
          service\"].Get(query.Service)\n        docIDs.And(fieldBitmap)\n    }\n    \n    // Fetch documents\n    results\
          \ := make([]LogRecord, 0)\n    it := docIDs.Iterator()\n    for it.HasNext() {\n        docID := it.Next()\n   \
          \     log := s.getDocument(docID)\n        results = append(results, log)\n    }\n    \n    return results, nil\n\
          }\n\n// Metric storage with time-series optimization\ntype MetricStore struct {\n    series map[string]*TimeSeries\
          \  // metric+labels hash -> series\n    wal    *WAL\n}\n\ntype TimeSeries struct {\n    chunks []*Chunk\n    current\
          \ *Chunk\n}\n\ntype Chunk struct {\n    startTime  int64\n    endTime    int64\n    timestamps []int64\n    values\
          \     []float64\n    // Use delta encoding and compression\n}\n\n// Tiered storage\ntype TieredStorage struct {\n\
          \    hot   Storage  // Recent data, fast SSD\n    warm  Storage  // Older data, cheaper storage\n    cold  Storage\
          \  // Archive, object storage\n    \n    hotRetention  time.Duration  // e.g., 24h\n    warmRetention time.Duration\
          \  // e.g., 7d\n}\n\nfunc (t *TieredStorage) Compact() {\n    now := time.Now()\n    \n    // Move hot -> warm\n\
          \    hotCutoff := now.Add(-t.hotRetention)\n    t.hot.MoveOlderThan(hotCutoff, t.warm)\n    \n    // Move warm ->\
          \ cold\n    warmCutoff := now.Add(-t.warmRetention)\n    t.warm.MoveOlderThan(warmCutoff, t.cold)\n}\n```"
      pitfalls:
      - Query performance at scale
      - Storage costs
      - Data consistency across stores
      concepts:
      - Inverted indexes
      - Time-series compression
      - Tiered storage
      estimated_hours: 20-30
      deliverables:
      - Log storage engine with full-text search indexing capability
      - Time-series database for metric storage with downsampling support
      - Trace storage with span tree reconstruction and querying
      - Cross-signal index enabling correlation queries across data types
    - id: 4
      name: Unified Query Interface
      description: Build a query interface that can correlate across all signal types.
      acceptance_criteria:
      - Query language supports filtering by time range, service, and attributes
      - Cross-signal queries join traces with associated logs in a single request
      - Query API returns paginated results with cursor-based navigation
      - Aggregate queries compute percentiles, rates, and histograms over metrics
      - Query execution completes within acceptable latency for interactive exploration
      hints:
        level1: Allow jumping from metric anomaly -> exemplar traces -> related logs.
        level2: Implement a query language that can express correlations explicitly.
        level3: "## Unified Query Interface\n\n```go\n// Query language supporting cross-signal correlation\ntype Query struct\
          \ {\n    // Base query\n    Signal    string    // \"logs\", \"metrics\", \"traces\"\n    Filter    string    //\
          \ Signal-specific filter\n    TimeRange TimeRange\n    \n    // Correlation\n    Correlate *CorrelationQuery `json:\"\
          correlate,omitempty\"`\n}\n\ntype CorrelationQuery struct {\n    // Follow trace context\n    FollowTrace bool\n\
          \    // Get exemplars from metrics\n    GetExemplars bool\n    // Include related signals\n    Include []string\
          \  // [\"logs\", \"metrics\"]\n}\n\n// Example queries:\n// 1. \"Show me all logs for traces with errors\"\n// {\n\
          //   \"signal\": \"traces\",\n//   \"filter\": \"status = 'ERROR'\",\n//   \"correlate\": { \"include\": [\"logs\"\
          ] }\n// }\n\n// 2. \"Show me traces for this metric spike\"\n// {\n//   \"signal\": \"metrics\",\n//   \"filter\"\
          : \"http_latency_p99 > 1s\",\n//   \"correlate\": { \"getExemplars\": true }\n// }\n\nfunc (db *ObservabilityDB)\
          \ ExecuteQuery(q Query) (*QueryResult, error) {\n    result := &QueryResult{\n        Signal: q.Signal,\n    }\n\
          \    \n    // Execute base query\n    switch q.Signal {\n    case \"logs\":\n        logs, _ := db.logs.Search(parseLogFilter(q.Filter),\
          \ q.TimeRange)\n        result.Logs = logs\n        \n    case \"metrics\":\n        metrics, _ := db.metrics.Query(parseMetricFilter(q.Filter),\
          \ q.TimeRange)\n        result.Metrics = metrics\n        \n    case \"traces\":\n        traces, _ := db.traces.Search(parseTraceFilter(q.Filter),\
          \ q.TimeRange)\n        result.Traces = traces\n    }\n    \n    // Handle correlations\n    if q.Correlate != nil\
          \ {\n        result.Correlated = db.correlate(result, q.Correlate)\n    }\n    \n    return result, nil\n}\n\nfunc\
          \ (db *ObservabilityDB) correlate(result *QueryResult, corr *CorrelationQuery) *CorrelatedData {\n    data := &CorrelatedData{}\n\
          \    \n    // Collect trace IDs from results\n    traceIDs := make(map[string]bool)\n    \n    if corr.GetExemplars\
          \ && len(result.Metrics) > 0 {\n        for _, m := range result.Metrics {\n            for _, e := range m.Exemplars\
          \ {\n                traceIDs[e.TraceID] = true\n            }\n        }\n    }\n    \n    if len(result.Traces)\
          \ > 0 {\n        for _, t := range result.Traces {\n            traceIDs[t.TraceID] = true\n        }\n    }\n \
          \   \n    // Fetch correlated data\n    for _, include := range corr.Include {\n        switch include {\n     \
          \   case \"logs\":\n            for traceID := range traceIDs {\n                logs, _ := db.logs.Search(LogQuery{TraceID:\
          \ traceID})\n                data.Logs = append(data.Logs, logs...)\n            }\n        case \"traces\":\n \
          \           for traceID := range traceIDs {\n                trace, _ := db.traces.GetTrace(traceID)\n         \
          \       data.Traces = append(data.Traces, trace)\n            }\n        }\n    }\n    \n    return data\n}\n```\n\
          \n```javascript\n// UI for correlated exploration\nfunction ObservabilityExplorer() {\n  const [query, setQuery]\
          \ = useState('');\n  const [results, setResults] = useState(null);\n  const [selectedTrace, setSelectedTrace] =\
          \ useState(null);\n  \n  const explore = async (q) => {\n    const res = await fetch('/api/query', {\n      method:\
          \ 'POST',\n      body: JSON.stringify(q)\n    }).then(r => r.json());\n    setResults(res);\n  };\n  \n  return\
          \ (\n    <div className=\"explorer\">\n      <QueryEditor value={query} onChange={setQuery} onRun={explore} />\n\
          \      \n      {results && (\n        <>\n          <SignalTabs>\n            {results.metrics && <MetricsPanel\
          \ data={results.metrics} onExemplarClick={traceId => {\n              explore({ signal: 'traces', filter: `traceId\
          \ = '${traceId}'`, correlate: { include: ['logs'] }});\n            }} />}\n            {results.traces && <TracesPanel\
          \ data={results.traces} onSelect={setSelectedTrace} />}\n            {results.logs && <LogsPanel data={results.logs}\
          \ />}\n          </SignalTabs>\n          \n          {selectedTrace && (\n            <CorrelatedView\n       \
          \       trace={selectedTrace}\n              logs={results.correlated?.logs}\n              metrics={results.correlated?.metrics}\n\
          \            />\n          )}\n        </>\n      )}\n    </div>\n  );\n}\n```"
      pitfalls:
      - Query complexity explosion
      - Slow correlation queries
      - Missing correlation data
      concepts:
      - Query planning
      - Data correlation
      - Unified observability
      estimated_hours: 15-25
      deliverables:
      - Query language supporting filters across logs, metrics, and traces
      - Cross-signal query engine joining data from multiple storage backends
      - Query API returning paginated results with streaming support
      - Query execution plan optimizer selecting efficient access paths
    - id: 5
      name: Alerting & Anomaly Detection
      description: Implement intelligent alerting that correlates anomalies across signals.
      acceptance_criteria:
      - Alert rules trigger when configured threshold conditions are met
      - Anomaly detection identifies deviations from learned baseline patterns
      - Notifications route to correct channels based on alert severity and team
      - Alert deduplication suppresses duplicate notifications for the same incident
      hints:
        level1: 'Trigger alerts based on combinations: error logs + latency spike + trace errors.'
        level2: Use statistical methods (z-score, MAD) for anomaly detection on metrics.
        level3: "## Multi-Signal Alerting\n\n```go\n// Multi-signal alert rule\ntype AlertRule struct {\n    Name        string\n\
          \    Description string\n    \n    // Conditions across signals\n    Conditions []AlertCondition\n    Logic    \
          \  string  // \"all\" or \"any\"\n    \n    For         time.Duration\n    Annotations map[string]string\n}\n\n\
          type AlertCondition struct {\n    Signal    string  // \"logs\", \"metrics\", \"traces\"\n    Query     string\n\
          \    Threshold string  // e.g., \"> 100\", \"exists\"\n}\n\n// Example: Alert when high error rate AND error logs\
          \ appear\n// {\n//   \"name\": \"ServiceDegraded\",\n//   \"conditions\": [\n//     { \"signal\": \"metrics\", \"\
          query\": \"rate(http_errors_total[5m])\", \"threshold\": \"> 0.01\" },\n//     { \"signal\": \"logs\", \"query\"\
          : \"level=error\", \"threshold\": \"count > 10\" },\n//     { \"signal\": \"traces\", \"query\": \"status=ERROR\"\
          , \"threshold\": \"ratio > 0.05\" }\n//   ],\n//   \"logic\": \"any\",\n//   \"for\": \"5m\"\n// }\n\n// Anomaly\
          \ detection\ntype AnomalyDetector struct {\n    history map[string]*MetricHistory\n}\n\ntype MetricHistory struct\
          \ {\n    values []float64\n    times  []time.Time\n}\n\nfunc (d *AnomalyDetector) Detect(metric string, value float64)\
          \ *Anomaly {\n    hist := d.history[metric]\n    if hist == nil || len(hist.values) < 100 {\n        return nil\
          \  // Not enough history\n    }\n    \n    // Calculate statistics\n    mean := stat.Mean(hist.values, nil)\n  \
          \  stddev := stat.StdDev(hist.values, nil)\n    \n    // Z-score\n    zscore := (value - mean) / stddev\n    \n\
          \    if math.Abs(zscore) > 3 {\n        return &Anomaly{\n            Metric:    metric,\n            Value:   \
          \  value,\n            Expected:  mean,\n            ZScore:    zscore,\n            Timestamp: time.Now(),\n  \
          \      }\n    }\n    return nil\n}\n\n// Alert correlation - group related alerts\ntype AlertCorrelator struct {\n\
          \    window   time.Duration\n    alerts   []*Alert\n    groups   map[string]*AlertGroup\n}\n\nfunc (c *AlertCorrelator)\
          \ Correlate(alert *Alert) *AlertGroup {\n    // Find existing group by:\n    // 1. Same service\n    // 2. Related\
          \ trace IDs\n    // 3. Close in time\n    \n    for _, group := range c.groups {\n        if c.isRelated(alert,\
          \ group) {\n            group.Alerts = append(group.Alerts, alert)\n            return group\n        }\n    }\n\
          \    \n    // Create new group\n    group := &AlertGroup{\n        ID:        uuid.New().String(),\n        Alerts:\
          \    []*Alert{alert},\n        CreatedAt: time.Now(),\n    }\n    c.groups[group.ID] = group\n    return group\n\
          }\n\n// Root cause analysis\nfunc (db *ObservabilityDB) SuggestRootCause(alertGroup *AlertGroup) []RootCauseSuggestion\
          \ {\n    suggestions := []RootCauseSuggestion{}\n    \n    // Collect all trace IDs from alerts\n    traceIDs :=\
          \ collectTraceIDs(alertGroup)\n    \n    // Find common error spans\n    errorSpans := map[string]int{}  // operation\
          \ -> count\n    for _, traceID := range traceIDs {\n        trace, _ := db.traces.GetTrace(traceID)\n        for\
          \ _, span := range trace.Spans {\n            if span.Status == \"ERROR\" {\n                key := span.Service\
          \ + \"/\" + span.Name\n                errorSpans[key]++\n            }\n        }\n    }\n    \n    // Sort by\
          \ frequency\n    for op, count := range errorSpans {\n        if count > len(traceIDs)/2 {\n            suggestions\
          \ = append(suggestions, RootCauseSuggestion{\n                Type:       \"common_error_operation\",\n        \
          \        Operation:  op,\n                Confidence: float64(count) / float64(len(traceIDs)),\n               \
          \ Evidence:   fmt.Sprintf(\"%d/%d traces have errors in %s\", count, len(traceIDs), op),\n            })\n     \
          \   }\n    }\n    \n    return suggestions\n}\n```"
      pitfalls:
      - Alert fatigue
      - False positives
      - Missing root causes
      concepts:
      - Multi-signal correlation
      - Statistical anomaly detection
      - Root cause analysis
      estimated_hours: 20-30
      deliverables:
      - Multi-signal alert rules evaluating conditions across logs, metrics, and traces
      - Anomaly detection engine identifying unusual patterns in metric streams
      - Alert notification system with configurable channels and routing
      - Alert silencing and deduplication preventing notification fatigue
  build-os:
    id: build-os
    name: Build Your Own OS
    description: Build a minimal operating system kernel from scratch.
    difficulty: expert
    estimated_hours: 100-200
    prerequisites:
    - Assembly language
    - C programming
    - Computer architecture
    - Memory management
    languages:
      recommended:
      - C
      - Rust
      - Zig
      also_possible:
      - Assembly
    resources:
    - type: book
      name: 'Operating Systems: Three Easy Pieces'
      url: https://pages.cs.wisc.edu/~remzi/OSTEP/
    - type: tutorial
      name: Writing an OS in Rust
      url: https://os.phil-opp.com/
    - type: wiki
      name: OSDev Wiki
      url: https://wiki.osdev.org/
    milestones:
    - id: 1
      name: Bootloader & Kernel Entry
      description: Boot from BIOS/UEFI and enter kernel code.
      acceptance_criteria:
      - Bootloader loads kernel binary from disk into memory at correct address
      - GDT configures code and data segments for 32-bit protected mode operation
      - Kernel entry point zeroes BSS section and transfers control to main
      - VGA driver prints text characters with color attributes to screen
      hints:
        level1: Use GRUB or write a minimal bootloader. Kernel starts in real mode on x86.
        level2: Set up Global Descriptor Table (GDT) to enter 32/64-bit protected mode.
        level3: "## Bootloader Basics\n\n```asm\n; boot.asm - Minimal bootloader\n[BITS 16]\n[ORG 0x7C00]\n\nstart:\n    ;\
          \ Set up segments\n    xor ax, ax\n    mov ds, ax\n    mov es, ax\n    mov ss, ax\n    mov sp, 0x7C00\n    \n  \
          \  ; Load kernel from disk\n    mov ah, 0x02    ; BIOS read sectors\n    mov al, 10      ; Number of sectors\n \
          \   mov ch, 0       ; Cylinder\n    mov cl, 2       ; Sector (1-indexed, 1 is boot)\n    mov dh, 0       ; Head\n\
          \    mov bx, 0x1000  ; Load address\n    int 0x13\n    \n    ; Switch to protected mode\n    cli\n    lgdt [gdt_descriptor]\n\
          \    \n    mov eax, cr0\n    or eax, 1\n    mov cr0, eax\n    \n    jmp 0x08:protected_mode\n\n[BITS 32]\nprotected_mode:\n\
          \    ; Set up segment registers\n    mov ax, 0x10\n    mov ds, ax\n    mov es, ax\n    mov fs, ax\n    mov gs, ax\n\
          \    mov ss, ax\n    mov esp, 0x90000\n    \n    ; Jump to kernel\n    jmp 0x1000\n\n; GDT\ngdt_start:\n    dq 0\
          \                    ; Null descriptor\ngdt_code:\n    dw 0xFFFF, 0x0000      ; Code segment\n    db 0x00, 0x9A,\
          \ 0xCF, 0x00\ngdt_data:\n    dw 0xFFFF, 0x0000      ; Data segment\n    db 0x00, 0x92, 0xCF, 0x00\ngdt_end:\n\n\
          gdt_descriptor:\n    dw gdt_end - gdt_start - 1\n    dd gdt_start\n\ntimes 510-($-$$) db 0\ndw 0xAA55          \
          \     ; Boot signature\n```"
      pitfalls:
      - Wrong memory addresses
      - Forgetting to disable interrupts
      - GDT misconfiguration
      concepts:
      - Boot process
      - CPU modes
      - Memory layout
      estimated_hours: 15-25
      deliverables:
      - Bootloader loading kernel from disk into protected mode
      - GDT setup transitioning processor to 32-bit protected mode
      - Kernel entry point initializing BSS and calling main function
      - VGA text mode driver printing characters to screen buffer
    - id: 2
      name: Interrupts & Keyboard
      description: Handle hardware interrupts and keyboard input.
      acceptance_criteria:
      - IDT contains valid entries for at least CPU exception vectors 0-31
      - Interrupt handlers save and restore registers before returning with iret
      - PIC remapping moves IRQs to non-conflicting interrupt vector range
      - Keyboard driver converts PS/2 scancodes to printable ASCII characters
      hints:
        level1: Set up IDT with handlers for each interrupt. Remap PIC to avoid conflicts.
        level2: Timer (IRQ0) drives preemptive scheduling. Keyboard (IRQ1) for input.
        level3: "## Interrupt Handling\n\n```c\n// idt.c\n#include <stdint.h>\n\nstruct idt_entry {\n    uint16_t base_low;\n\
          \    uint16_t selector;\n    uint8_t  zero;\n    uint8_t  flags;\n    uint16_t base_high;\n} __attribute__((packed));\n\
          \nstruct idt_ptr {\n    uint16_t limit;\n    uint32_t base;\n} __attribute__((packed));\n\nstruct idt_entry idt[256];\n\
          struct idt_ptr idtp;\n\nvoid idt_set_gate(int num, uint32_t base, uint16_t sel, uint8_t flags) {\n    idt[num].base_low\
          \ = base & 0xFFFF;\n    idt[num].base_high = (base >> 16) & 0xFFFF;\n    idt[num].selector = sel;\n    idt[num].zero\
          \ = 0;\n    idt[num].flags = flags;\n}\n\n// Remap PIC\nvoid pic_remap() {\n    outb(0x20, 0x11);  // Init PIC1\n\
          \    outb(0xA0, 0x11);  // Init PIC2\n    outb(0x21, 0x20);  // PIC1 offset (IRQ 0-7 -> INT 32-39)\n    outb(0xA1,\
          \ 0x28);  // PIC2 offset (IRQ 8-15 -> INT 40-47)\n    outb(0x21, 0x04);  // Tell PIC1 about PIC2\n    outb(0xA1,\
          \ 0x02);  // Tell PIC2 its cascade identity\n    outb(0x21, 0x01);  // 8086 mode\n    outb(0xA1, 0x01);\n    outb(0x21,\
          \ 0x0);   // Unmask all\n    outb(0xA1, 0x0);\n}\n\n// Keyboard handler\nvoid keyboard_handler() {\n    uint8_t\
          \ scancode = inb(0x60);\n    \n    if (!(scancode & 0x80)) {  // Key press (not release)\n        char c = scancode_to_ascii(scancode);\n\
          \        if (c) {\n            terminal_putchar(c);\n        }\n    }\n    \n    // Send EOI\n    outb(0x20, 0x20);\n\
          }\n```"
      pitfalls:
      - Not sending EOI
      - Wrong PIC remapping
      - Stack corruption in handlers
      concepts:
      - Interrupts
      - Hardware I/O
      - Device drivers
      estimated_hours: 15-25
      deliverables:
      - IDT setup registering interrupt handler entries for all vectors
      - Interrupt handlers for CPU exceptions and hardware IRQs
      - PIC or APIC configuration mapping hardware interrupts to vectors
      - Keyboard driver reading scancodes and converting to ASCII characters
    - id: 3
      name: Memory Management
      description: Implement physical and virtual memory management.
      acceptance_criteria:
      - Physical frame allocator allocates and frees 4KB page frames without leaking
      - Page tables correctly map virtual addresses to physical frames with proper flags
      - Kernel heap allocator handles malloc and free without fragmentation issues
      - Memory-mapped I/O regions are marked as uncacheable in page table entries
      hints:
        level1: Track physical pages with a bitmap or free list. Set up page tables.
        level2: Enable paging in CR0. Handle page faults to implement demand paging.
        level3: "## Paging\n\n```c\n// Physical memory manager (bitmap)\n#define PAGE_SIZE 4096\n#define BITMAP_SIZE (total_memory\
          \ / PAGE_SIZE / 8)\n\nuint8_t *page_bitmap;\nsize_t total_pages;\n\nvoid pmm_init(size_t mem_size) {\n    total_pages\
          \ = mem_size / PAGE_SIZE;\n    page_bitmap = (uint8_t*)BITMAP_ADDR;\n    memset(page_bitmap, 0xFF, BITMAP_SIZE);\
          \  // All used initially\n    \n    // Mark available memory as free\n    // (from memory map provided by bootloader)\n\
          }\n\nvoid *pmm_alloc_page() {\n    for (size_t i = 0; i < total_pages; i++) {\n        if (!(page_bitmap[i / 8]\
          \ & (1 << (i % 8)))) {\n            page_bitmap[i / 8] |= (1 << (i % 8));  // Mark used\n            return (void*)(i\
          \ * PAGE_SIZE);\n        }\n    }\n    return NULL;  // Out of memory\n}\n\nvoid pmm_free_page(void *page) {\n \
          \   size_t idx = (size_t)page / PAGE_SIZE;\n    page_bitmap[idx / 8] &= ~(1 << (idx % 8));\n}\n\n// Page table setup\
          \ (x86)\ntypedef uint32_t page_entry_t;\n\npage_entry_t *page_directory;\npage_entry_t *page_tables[1024];\n\nvoid\
          \ paging_init() {\n    page_directory = pmm_alloc_page();\n    memset(page_directory, 0, PAGE_SIZE);\n    \n   \
          \ // Identity map first 4MB\n    page_entry_t *first_table = pmm_alloc_page();\n    for (int i = 0; i < 1024; i++)\
          \ {\n        first_table[i] = (i * PAGE_SIZE) | 3;  // Present, R/W\n    }\n    page_directory[0] = ((uint32_t)first_table)\
          \ | 3;\n    \n    // Enable paging\n    asm volatile(\n        \"mov %0, %%cr3\\n\"\n        \"mov %%cr0, %%eax\\\
          n\"\n        \"or $0x80000000, %%eax\\n\"\n        \"mov %%eax, %%cr0\\n\"\n        : : \"r\"(page_directory) :\
          \ \"eax\"\n    );\n}\n\nvoid map_page(uint32_t virt, uint32_t phys, uint32_t flags) {\n    uint32_t pd_idx = virt\
          \ >> 22;\n    uint32_t pt_idx = (virt >> 12) & 0x3FF;\n    \n    if (!(page_directory[pd_idx] & 1)) {\n        page_entry_t\
          \ *new_table = pmm_alloc_page();\n        memset(new_table, 0, PAGE_SIZE);\n        page_directory[pd_idx] = ((uint32_t)new_table)\
          \ | 3;\n    }\n    \n    page_entry_t *table = (page_entry_t*)(page_directory[pd_idx] & ~0xFFF);\n    table[pt_idx]\
          \ = phys | flags;\n}\n```"
      pitfalls:
      - TLB not flushed
      - Wrong page table structure
      - Kernel unmapped after enabling paging
      concepts:
      - Virtual memory
      - Paging
      - Memory protection
      estimated_hours: 25-40
      deliverables:
      - Physical frame allocator tracking free and used memory pages
      - Page table setup enabling virtual-to-physical address translation
      - Kernel heap allocator providing dynamic memory allocation
      - Memory-mapped I/O support for hardware device registers
    - id: 4
      name: Process Management
      description: Implement processes and basic scheduling.
      acceptance_criteria:
      - Process control block stores PID, register state, page tables, and status
      - Context switch correctly saves current and restores next process register state
      - Scheduler runs multiple processes in round-robin order with time slicing
      - System calls transition from user mode to kernel mode via interrupt mechanism
      hints:
        level1: Store process state in PCB. Timer interrupt triggers scheduler.
        level2: Context switch saves/restores registers. Each process has own page table.
        level3: "## Process Management\n\n```c\ntypedef struct {\n    uint32_t eax, ebx, ecx, edx;\n    uint32_t esi, edi,\
          \ ebp, esp;\n    uint32_t eip, eflags;\n    uint32_t cr3;  // Page directory\n} cpu_state_t;\n\ntypedef struct process\
          \ {\n    int pid;\n    enum { READY, RUNNING, BLOCKED, ZOMBIE } state;\n    cpu_state_t regs;\n    uint32_t *page_directory;\n\
          \    struct process *next;\n} process_t;\n\nprocess_t *current_process;\nprocess_t *ready_queue;\n\nvoid schedule()\
          \ {\n    if (!ready_queue) return;\n    \n    // Save current process state\n    if (current_process && current_process->state\
          \ == RUNNING) {\n        current_process->state = READY;\n        // Add to end of ready queue\n        process_t\
          \ *p = ready_queue;\n        while (p->next) p = p->next;\n        p->next = current_process;\n        current_process->next\
          \ = NULL;\n    }\n    \n    // Get next process\n    current_process = ready_queue;\n    ready_queue = ready_queue->next;\n\
          \    current_process->state = RUNNING;\n    \n    // Switch to process\n    switch_context(&current_process->regs);\n\
          }\n\n// Context switch (assembly)\nextern void switch_context(cpu_state_t *state);\n\n// Timer interrupt calls scheduler\n\
          void timer_handler() {\n    // ... update time ...\n    schedule();\n    outb(0x20, 0x20);  // EOI\n}\n\nint sys_fork()\
          \ {\n    process_t *child = kmalloc(sizeof(process_t));\n    child->pid = next_pid++;\n    child->state = READY;\n\
          \    \n    // Copy page directory (copy-on-write would be better)\n    child->page_directory = clone_page_directory(current_process->page_directory);\n\
          \    \n    // Copy registers, set child return value to 0\n    child->regs = current_process->regs;\n    child->regs.eax\
          \ = 0;\n    \n    // Add to ready queue\n    child->next = ready_queue;\n    ready_queue = child;\n    \n    return\
          \ child->pid;  // Parent returns child PID\n}\n```"
      pitfalls:
      - Stack corruption during switch
      - Not saving all registers
      - Deadlocks in scheduling
      concepts:
      - Context switching
      - Scheduling algorithms
      - Process isolation
      estimated_hours: 30-50
      deliverables:
      - Process control block storing process state and metadata
      - Context switching saving and restoring register state between processes
      - Basic round-robin scheduler selecting next process to run
      - System call interface dispatching user-mode requests to kernel functions
  build-raft:
    id: build-raft
    name: Build Your Own Raft
    description: Implement the Raft consensus algorithm for distributed agreement. Learn leader election, log replication,
      and safety.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - Replicated log
    - Leader election
    - Distributed systems basics
    languages:
      recommended:
      - Go
      - Rust
      - Java
      also_possible:
      - Python
      - C++
    resources:
    - type: paper
      name: In Search of an Understandable Consensus Algorithm
      url: https://raft.github.io/raft.pdf
    - type: visualization
      name: Raft Visualization
      url: https://raft.github.io/
    milestones:
    - id: 1
      name: Leader Election
      description: Implement Raft leader election with terms and voting.
      acceptance_criteria:
      - Nodes increment term and transition to candidate after election timeout expires
      - Election timeouts are randomized within configured range to reduce split votes
      - RequestVote RPC includes candidate term, log index, and log term fields
      - Nodes grant vote only if candidate log is at least as up-to-date as own
      hints:
        level1: Follower times out -> becomes candidate -> requests votes. Majority wins.
        level2: Random timeout (150-300ms) prevents split votes. Only vote once per term.
        level3: "import random\nimport asyncio\nfrom enum import Enum\n\nclass State(Enum):\n    FOLLOWER = 'follower'\n \
          \   CANDIDATE = 'candidate'\n    LEADER = 'leader'\n\nclass RaftNode:\n    def __init__(self, node_id, peers):\n\
          \        self.id = node_id\n        self.peers = peers\n        \n        # Persistent state\n        self.current_term\
          \ = 0\n        self.voted_for = None\n        self.log = []  # List of (term, command)\n        \n        # Volatile\
          \ state\n        self.state = State.FOLLOWER\n        self.commit_index = 0\n        self.last_applied = 0\n   \
          \     \n        # Leader state\n        self.next_index = {}   # peer -> next log index to send\n        self.match_index\
          \ = {}  # peer -> highest replicated index\n        \n        self.election_timeout = self._random_timeout()\n \
          \       self.last_heartbeat = time.time()\n    \n    def _random_timeout(self):\n        return random.uniform(0.15,\
          \ 0.3)  # 150-300ms\n    \n    async def election_timer(self):\n        while True:\n            await asyncio.sleep(0.05)\
          \  # Check every 50ms\n            \n            if self.state == State.LEADER:\n                continue\n    \
          \        \n            if time.time() - self.last_heartbeat > self.election_timeout:\n                await self.start_election()\n\
          \    \n    async def start_election(self):\n        self.state = State.CANDIDATE\n        self.current_term += 1\n\
          \        self.voted_for = self.id\n        self.election_timeout = self._random_timeout()\n        self.last_heartbeat\
          \ = time.time()\n        \n        votes = 1  # Vote for self\n        last_log_index = len(self.log) - 1\n    \
          \    last_log_term = self.log[-1][0] if self.log else 0\n        \n        # Request votes from all peers\n    \
          \    tasks = []\n        for peer in self.peers:\n            tasks.append(self.send_request_vote(peer, last_log_index,\
          \ last_log_term))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        votes\
          \ += sum(1 for r in results if r is True)\n        \n        # Check if won\n        if votes > (len(self.peers)\
          \ + 1) // 2:\n            self.become_leader()\n    \n    def handle_request_vote(self, term, candidate_id, last_log_index,\
          \ last_log_term):\n        if term < self.current_term:\n            return False, self.current_term\n        \n\
          \        if term > self.current_term:\n            self.current_term = term\n            self.voted_for = None\n\
          \            self.state = State.FOLLOWER\n        \n        # Check if log is up-to-date\n        my_last_term =\
          \ self.log[-1][0] if self.log else 0\n        my_last_index = len(self.log) - 1\n        \n        log_ok = (last_log_term\
          \ > my_last_term or \n                  (last_log_term == my_last_term and last_log_index >= my_last_index))\n \
          \       \n        if (self.voted_for is None or self.voted_for == candidate_id) and log_ok:\n            self.voted_for\
          \ = candidate_id\n            self.last_heartbeat = time.time()\n            return True, self.current_term\n  \
          \      \n        return False, self.current_term"
      pitfalls:
      - Split vote scenarios
      - Term number handling
      - Log up-to-date check
      concepts:
      - Leader election
      - Distributed voting
      - Failure detection
      estimated_hours: 12-18
      deliverables:
      - Term management tracking current term and voted-for state
      - Election timeout randomization preventing simultaneous candidate elections
      - RequestVote RPC requesting votes from peer nodes in cluster
      - Vote granting rules ensuring at most one leader per term
    - id: 2
      name: Log Replication
      description: Implement log replication from leader to followers.
      acceptance_criteria:
      - AppendEntries RPC sends log entries with prevLogIndex and prevLogTerm for consistency
      - Followers reject AppendEntries if their log does not match at prevLogIndex
      - Leader advances commit index when entry is replicated to majority of cluster
      - Committed entries are applied to state machine in log order without gaps
      hints:
        level1: Leader sends heartbeats and log entries. Include prev log index/term for consistency.
        level2: On conflict, follower rejects. Leader decrements nextIndex and retries.
        level3: "async def send_append_entries(self, peer):\n    prev_log_index = self.next_index[peer] - 1\n    prev_log_term\
          \ = self.log[prev_log_index][0] if prev_log_index >= 0 else 0\n    \n    entries = self.log[self.next_index[peer]:]\n\
          \    \n    success, term = await peer.append_entries(\n        term=self.current_term,\n        leader_id=self.id,\n\
          \        prev_log_index=prev_log_index,\n        prev_log_term=prev_log_term,\n        entries=entries,\n      \
          \  leader_commit=self.commit_index\n    )\n    \n    if term > self.current_term:\n        self.current_term = term\n\
          \        self.state = State.FOLLOWER\n        return\n    \n    if success:\n        self.next_index[peer] = len(self.log)\n\
          \        self.match_index[peer] = len(self.log) - 1\n        self.advance_commit_index()\n    else:\n        # Decrement\
          \ and retry\n        self.next_index[peer] = max(0, self.next_index[peer] - 1)\n\ndef handle_append_entries(self,\
          \ term, leader_id, prev_log_index, prev_log_term, entries, leader_commit):\n    self.last_heartbeat = time.time()\n\
          \    \n    if term < self.current_term:\n        return False, self.current_term\n    \n    if term > self.current_term:\n\
          \        self.current_term = term\n        self.voted_for = None\n    \n    self.state = State.FOLLOWER\n    \n\
          \    # Check log consistency\n    if prev_log_index >= 0:\n        if prev_log_index >= len(self.log):\n       \
          \     return False, self.current_term\n        if self.log[prev_log_index][0] != prev_log_term:\n            # Delete\
          \ conflicting entries\n            self.log = self.log[:prev_log_index]\n            return False, self.current_term\n\
          \    \n    # Append new entries\n    for i, entry in enumerate(entries):\n        idx = prev_log_index + 1 + i\n\
          \        if idx < len(self.log):\n            if self.log[idx][0] != entry[0]:\n                self.log = self.log[:idx]\n\
          \                self.log.append(entry)\n        else:\n            self.log.append(entry)\n    \n    # Update commit\
          \ index\n    if leader_commit > self.commit_index:\n        self.commit_index = min(leader_commit, len(self.log)\
          \ - 1)\n    \n    return True, self.current_term\n\ndef advance_commit_index(self):\n    # Find N such that majority\
          \ have match_index >= N\n    for n in range(len(self.log) - 1, self.commit_index, -1):\n        if self.log[n][0]\
          \ != self.current_term:\n            continue\n        \n        count = 1  # Self\n        for peer in self.peers:\n\
          \            if self.match_index.get(peer, 0) >= n:\n                count += 1\n        \n        if count > (len(self.peers)\
          \ + 1) // 2:\n            self.commit_index = n\n            break"
      pitfalls:
      - Log index off-by-one
      - Commit index from previous term
      - Handling gaps
      concepts:
      - Log replication
      - Consistency
      - Quorum
      estimated_hours: 15-25
      deliverables:
      - AppendEntries RPC replicating log entries from leader to followers
      - Log consistency check ensuring follower log matches leader prefix
      - Commit index advancement after majority replication confirmation
      - Log entry application to state machine after commit
    - id: 3
      name: Safety Properties
      description: Ensure Raft safety guarantees are maintained.
      acceptance_criteria:
      - Split vote scenario resolves correctly with no two leaders in same term
      - Newly elected leader contains all previously committed log entries
      - If two logs contain an entry with same index and term the logs are identical up to that index
      - All non-failed nodes apply exactly the same sequence of state machine commands
      hints:
        level1: Never commit entries from previous terms directly. Only commit by replicating current term entry.
        level2: Leader never overwrites its log. Followers may truncate conflicting entries.
        level3: "# Safety invariant checks\n\nclass SafetyChecker:\n    \"\"\"Debug helper to verify Raft safety properties\"\
          \"\"\n    \n    @staticmethod\n    def check_election_safety(nodes):\n        \"\"\"At most one leader per term\"\
          \"\"\n        leaders_by_term = {}\n        for node in nodes:\n            if node.state == State.LEADER:\n   \
          \             term = node.current_term\n                if term in leaders_by_term:\n                    raise SafetyViolation(\n\
          \                        f'Multiple leaders in term {term}: '\n                        f'{leaders_by_term[term]}\
          \ and {node.id}'\n                    )\n                leaders_by_term[term] = node.id\n    \n    @staticmethod\n\
          \    def check_log_matching(nodes):\n        \"\"\"If two logs contain entry with same index and term,\n       \
          \    logs are identical up to that index\"\"\"\n        for i, n1 in enumerate(nodes):\n            for n2 in nodes[i+1:]:\n\
          \                for idx in range(min(len(n1.log), len(n2.log))):\n                    if n1.log[idx][0] == n2.log[idx][0]:\
          \  # Same term\n                        # All previous entries must match\n                        for j in range(idx):\n\
          \                            if n1.log[j] != n2.log[j]:\n                                raise SafetyViolation(\n\
          \                                    f'Log mismatch at {j} between {n1.id} and {n2.id}'\n                      \
          \          )\n    \n    @staticmethod\n    def check_leader_completeness(committed_entries, leader):\n        \"\
          \"\"Any committed entry must be in leader's log\"\"\"\n        for entry in committed_entries:\n            if entry\
          \ not in leader.log:\n                raise SafetyViolation(\n                    f'Committed entry {entry} not\
          \ in leader log'\n                )\n\n# In RaftNode, ensure we only commit from current term\ndef advance_commit_index(self):\n\
          \    for n in range(len(self.log) - 1, self.commit_index, -1):\n        # CRITICAL: Only commit entries from current\
          \ term\n        # This ensures Leader Completeness property\n        if self.log[n][0] != self.current_term:\n \
          \           continue  # Can't directly commit old term entries\n        \n        count = 1\n        for peer in\
          \ self.peers:\n            if self.match_index.get(peer, 0) >= n:\n                count += 1\n        \n      \
          \  if count > (len(self.peers) + 1) // 2:\n            self.commit_index = n\n            break"
      pitfalls:
      - Committing old term entries
      - Figure 8 scenario
      - Network partition handling
      concepts:
      - Safety properties
      - Formal verification
      - Invariants
      estimated_hours: 15-25
      deliverables:
      - Election safety ensuring at most one leader per term
      - Leader completeness guaranteeing committed entries survive leader changes
      - Log matching property verifying consistent logs across nodes
      - State machine safety ensuring all nodes apply same entries in same order
    - id: 4
      name: Cluster Membership Changes
      description: Implement dynamic cluster membership changes.
      acceptance_criteria:
      - Joint consensus requires majority of both old and new configurations for decisions
      - Configuration changes are committed as special log entries through Raft protocol
      - New nodes catch up on log before being counted in majority calculations
      - Removed nodes stop participating in elections after configuration change commits
      hints:
        level1: 'Single-server changes are simpler: add/remove one at a time.'
        level2: New server must catch up before counting for quorum. Leader steps down if removed.
        level3: "class RaftNode:\n    def __init__(self, ...):\n        # ...\n        self.config = set(peers) | {self.id}\n\
          \        self.pending_config = None\n    \n    async def add_server(self, new_server):\n        if self.state !=\
          \ State.LEADER:\n            raise NotLeaderError()\n        \n        if self.pending_config:\n            raise\
          \ ConfigChangeInProgress()\n        \n        # First, catch up new server\n        await self.catch_up_server(new_server)\n\
          \        \n        # Append config change to log\n        new_config = self.config | {new_server}\n        entry\
          \ = (self.current_term, ConfigChange(new_config))\n        self.log.append(entry)\n        \n        self.pending_config\
          \ = new_config\n        self.peers.add(new_server)\n        self.next_index[new_server] = len(self.log)\n      \
          \  self.match_index[new_server] = 0\n        \n        # Replicate and wait for commit\n        await self.replicate_and_commit()\n\
          \        \n        self.config = new_config\n        self.pending_config = None\n    \n    async def remove_server(self,\
          \ server):\n        if self.state != State.LEADER:\n            raise NotLeaderError()\n        \n        if server\
          \ not in self.config:\n            raise ServerNotInConfig()\n        \n        new_config = self.config - {server}\n\
          \        entry = (self.current_term, ConfigChange(new_config))\n        self.log.append(entry)\n        \n     \
          \   self.pending_config = new_config\n        \n        await self.replicate_and_commit()\n        \n        self.config\
          \ = new_config\n        self.pending_config = None\n        \n        if server == self.id:\n            # Leader\
          \ is being removed\n            self.state = State.FOLLOWER\n            # Transfer leadership\n            await\
          \ self.transfer_leadership()\n    \n    async def catch_up_server(self, server, timeout=30):\n        \"\"\"Replicate\
          \ log to new server until caught up\"\"\"\n        start = time.time()\n        while time.time() - start < timeout:\n\
          \            await self.send_append_entries(server)\n            if self.match_index.get(server, 0) >= len(self.log)\
          \ - 1:\n                return\n            await asyncio.sleep(0.1)\n        raise CatchUpTimeout()"
      pitfalls:
      - Disjoint majorities
      - Leader in transition
      - Stuck configurations
      concepts:
      - Membership changes
      - Joint consensus
      - Availability during changes
      estimated_hours: 18-32
      deliverables:
      - Joint consensus protocol transitioning between old and new configurations
      - Configuration change log entries replicated through Raft protocol
      - Node addition procedure safely introducing new members to cluster
      - Node removal procedure safely removing members without losing availability
  build-raytracer:
    id: build-raytracer
    name: Build Your Own Ray Tracer
    description: Build a path tracing renderer following Ray Tracing in One Weekend. Renders photorealistic images with reflections,
      refractions, and soft shadows.
    difficulty: advanced
    estimated_hours: 20-40
    prerequisites:
    - Linear algebra (vectors, matrices)
    - Basic geometry
    - Some physics (light, optics)
    languages:
      recommended:
      - C++
      - Rust
      - Go
      also_possible:
      - Python
      - JavaScript
    resources:
    - name: Ray Tracing in One Weekend
      url: https://raytracing.github.io/books/RayTracingInOneWeekend.html
      type: book
    - name: 'Ray Tracing: The Next Week'
      url: https://raytracing.github.io/books/RayTracingTheNextWeek.html
      type: book
    - name: Physically Based Rendering (PBRT)
      url: https://pbr-book.org/
      type: reference
    milestones:
    - id: 1
      name: Output an Image
      description: Generate a simple PPM image with gradient colors.
      acceptance_criteria:
      - PPM writer outputs valid P3 format file openable by image viewers
      - Color class stores red, green, blue components as floating point values
      - Image buffer correctly maps x,y coordinates to linear pixel array
      - Gradient image shows smooth color transition from top to bottom
      hints:
        level1: 'PPM format: P3 header, width height, max color, then R G B values.'
        level2: Iterate y from top to bottom, x from left to right.
        level3: |-
          # PPM image output - simple format for raytracing
          class Color:
              def __init__(self, r, g, b):
                  self.r = r
                  self.g = g
                  self.b = b

              def __add__(self, other):
                  return Color(self.r + other.r, self.g + other.g, self.b + other.b)

              def __mul__(self, scalar):
                  return Color(self.r * scalar, self.g * scalar, self.b * scalar)

              def clamp(self):
                  return Color(
                      max(0.0, min(1.0, self.r)),
                      max(0.0, min(1.0, self.g)),
                      max(0.0, min(1.0, self.b))
                  )

              def to_ppm(self, samples_per_pixel=1):
                  scale = 1.0 / samples_per_pixel
                  r = int(256 * max(0.0, min(0.999, self.r * scale)))
                  g = int(256 * max(0.0, min(0.999, self.g * scale)))
                  b = int(256 * max(0.0, min(0.999, self.b * scale)))
                  return f"{r} {g} {b}"

          def write_ppm(filename, width, height, pixels):
              with open(filename, 'w') as f:
                  f.write(f"P3\n{width} {height}\n255\n")
                  for row in pixels:
                      for color in row:
                          f.write(color.to_ppm() + "\n")

          # Example: gradient image
          width, height = 256, 256
          pixels = []
          for j in range(height):
              row = []
              for i in range(width):
                  r = i / (width - 1)
                  g = (height - 1 - j) / (height - 1)
                  b = 0.25
                  row.append(Color(r, g, b))
              pixels.append(row)
          write_ppm("gradient.ppm", width, height, pixels)
      concepts:
      - Image file formats
      - Color representation
      - Coordinate systems
      estimated_hours: '1'
      pitfalls:
      - RGB values must be clamped to 0-255 range
      - Forgetting newline between pixel values in PPM
      - Integer overflow when color values exceed 1.0
      - 'Y-axis direction: most formats have origin at top-left, not bottom-left'
      deliverables:
      - PPM image file writer outputting RGB pixel data
      - Color class representing RGB values with clamping support
      - Image buffer storing pixel data for width by height grid
      - Gradient rendering demonstrating color interpolation across image
    - id: 2
      name: Ray Class and Background
      description: Define ray class and render background gradient.
      acceptance_criteria:
      - Ray evaluates point at parameter t as origin plus t times direction
      - Camera generates rays from eye through each viewport pixel position
      - Background returns blue-to-white gradient based on ray y-direction component
      - Vector operations produce mathematically correct results for unit tests
      hints:
        level1: 'Ray: P(t) = origin + t * direction. t=0 at origin, t=1 at origin+direction.'
        level2: 'Background: lerp between white and blue based on y component of unit direction.'
        level3: |-
          Color ray_color(Ray& r) {
            Vec3 unit_dir = normalize(r.direction);
            float t = 0.5 * (unit_dir.y + 1.0);
            return (1-t)*white + t*blue;
          }
      concepts:
      - Ray representation
      - Linear interpolation (lerp)
      - Camera model basics
      estimated_hours: 1-2
      pitfalls:
      - Rays with zero-length direction vectors cause NaN
      - Forgetting to normalize direction vector affects calculations
      - Using int instead of float for coordinates loses precision
      - Background color calculation sensitive to ray direction normalization
      deliverables:
      - Ray class storing origin point and direction vector
      - Camera model defining viewport and ray generation per pixel
      - Background color function interpolating based on ray direction
      - Vector utility functions for dot product, cross product, and normalization
    - id: 3
      name: Sphere Intersection
      description: Add a sphere and test ray-sphere intersection.
      acceptance_criteria:
      - Ray-sphere intersection correctly solves quadratic equation for hit points
      - Hit record contains intersection point, surface normal, and parameter t
      - Negative t values are rejected to avoid intersections behind camera
      - Rendered sphere appears as colored circle centered in the output image
      hints:
        level1: 'Sphere equation: |P - C|^2 = r^2. Substitute ray equation, solve quadratic.'
        level2: at^2 + bt + c = 0 where a=d.d, b=2*d.(o-c), c=(o-c).(o-c)-r^2
        level3: |-
          discriminant = b*b - 4*a*c
          if (discriminant < 0) return false;
          t = (-b - sqrt(discriminant)) / (2*a);
          if (t < 0) t = (-b + sqrt(discriminant)) / (2*a);
      pitfalls:
      - Choosing correct root (closest positive)
      - Numeric precision issues
      - Sphere behind camera
      concepts:
      - Ray-sphere intersection
      - Quadratic formula
      - Hit detection
      estimated_hours: 1-2
      deliverables:
      - Sphere geometry class with center position and radius
      - Ray-sphere intersection test using quadratic discriminant formula
      - Hit record storing intersection point, normal, and distance parameter
      - Scene rendering showing red sphere against gradient background
    - id: 4
      name: Surface Normals and Multiple Objects
      description: Compute normals and support multiple objects with closest hit.
      acceptance_criteria:
      - Surface normals point outward and are unit length at intersection points
      - Normal-to-color mapping produces visually distinct shading across sphere surface
      - Hittable list iterates all objects and selects closest valid intersection
      - Scene with ground plane and sphere renders both objects with correct occlusion
      hints:
        level1: 'Normal at hit point on sphere: (hit_point - center) / radius'
        level2: Hittable interface with hit() method. Scene is list of hittables.
        level3: |-
          # Hit detection with surface normals
          import math
          from dataclasses import dataclass

          @dataclass
          class Vec3:
              x: float
              y: float
              z: float

              def __neg__(self): return Vec3(-self.x, -self.y, -self.z)
              def __add__(self, o): return Vec3(self.x+o.x, self.y+o.y, self.z+o.z)
              def __sub__(self, o): return Vec3(self.x-o.x, self.y-o.y, self.z-o.z)
              def __mul__(self, t): return Vec3(self.x*t, self.y*t, self.z*t)
              def dot(self, o): return self.x*o.x + self.y*o.y + self.z*o.z
              def length(self): return math.sqrt(self.dot(self))
              def unit(self): return self * (1.0 / self.length())

          @dataclass
          class HitRecord:
              p: Vec3          # Hit point
              normal: Vec3     # Surface normal (always outward)
              t: float         # Ray parameter
              front_face: bool # Are we hitting front or back?

          class Sphere:
              def __init__(self, center, radius, material):
                  self.center = center
                  self.radius = radius
                  self.material = material

              def hit(self, ray, t_min, t_max):
                  oc = ray.origin - self.center
                  a = ray.direction.dot(ray.direction)
                  half_b = oc.dot(ray.direction)
                  c = oc.dot(oc) - self.radius * self.radius
                  discriminant = half_b*half_b - a*c

                  if discriminant < 0:
                      return None

                  sqrtd = math.sqrt(discriminant)
                  root = (-half_b - sqrtd) / a
                  if root < t_min or root > t_max:
                      root = (-half_b + sqrtd) / a
                      if root < t_min or root > t_max:
                          return None

                  p = ray.at(root)
                  outward_normal = (p - self.center) * (1.0/self.radius)
                  front_face = ray.direction.dot(outward_normal) < 0
                  normal = outward_normal if front_face else -outward_normal

                  return HitRecord(p, normal, root, front_face)

          class HittableList:
              def __init__(self):
                  self.objects = []

              def hit(self, ray, t_min, t_max):
                  closest = None
                  closest_t = t_max
                  for obj in self.objects:
                      rec = obj.hit(ray, t_min, closest_t)
                      if rec:
                          closest = rec
                          closest_t = rec.t
                  return closest
      concepts:
      - Surface normals
      - Object-oriented design
      - Closest hit algorithm
      estimated_hours: 2-3
      pitfalls:
      - Normal must point outward from surface (flip if inside)
      - t_min should be small positive (0.001) not zero to avoid self-intersection
      - 'Floating point precision: comparing t == 0 fails'
      - Forgetting to return closest hit, not first hit
      deliverables:
      - Surface normal computation at sphere intersection points
      - Normal-based coloring mapping normal direction to RGB values
      - Hittable list managing collection of scene objects
      - Closest hit selection returning nearest intersection from all objects
    - id: 5
      name: Antialiasing
      description: Add antialiasing by shooting multiple rays per pixel.
      acceptance_criteria:
      - Multiple samples per pixel are cast with random sub-pixel offsets
      - Averaged samples produce smoother edges compared to single-sample rendering
      - Sample count is configurable and higher counts produce visibly smoother results
      - Random jitter is uniformly distributed within each pixel boundary
      hints:
        level1: For each pixel, shoot N rays with random offset. Average results.
        level2: 'Offset: (x + random()) / width instead of x / width'
        level3: |-
          for (int s = 0; s < samples_per_pixel; s++) {
            float u = (x + random_float()) / (width - 1);
            float v = (y + random_float()) / (height - 1);
            color += ray_color(camera.get_ray(u, v));
          }
          color /= samples_per_pixel;
      concepts:
      - Antialiasing
      - Monte Carlo sampling
      - Random number generation
      estimated_hours: 1-2
      pitfalls:
      - Not using random offsets within pixel causes visible patterns
      - Too few samples causes noisy images
      - Forgetting to average samples produces overly bright images
      - Random number generator state affects reproducibility
      deliverables:
      - Multi-sample per pixel casting multiple rays with random jitter
      - Random number generator producing uniform samples in unit square
      - Sample averaging combining multiple ray colors into final pixel color
      - Configurable samples-per-pixel controlling quality versus render time
    - id: 6
      name: Diffuse Materials
      description: Implement Lambertian (diffuse) material.
      acceptance_criteria:
      - Diffuse surfaces scatter rays randomly with cosine-weighted distribution
      - Recursive tracing terminates at configurable maximum bounce depth limit
      - Shadow regions appear naturally where bounced rays receive less light
      - Gamma correction with power 1/2.2 produces visually correct brightness levels
      hints:
        level1: On hit, spawn new ray in random direction in hemisphere around normal.
        level2: 'Lambertian: direction = normal + random_unit_vector()'
        level3: |-
          Color ray_color(Ray& r, int depth) {
            if (depth <= 0) return black;
            if (world.hit(r, hit_record)) {
              Vec3 target = hit_point + normal + random_unit_vector();
              return 0.5 * ray_color(Ray(hit_point, target - hit_point), depth-1);
            }
            return background;
          }
      pitfalls:
      - Infinite recursion without depth limit
      - Shadow acne (self-intersection)
      - Correct hemisphere sampling
      concepts:
      - Lambertian reflection
      - Recursive ray tracing
      - Material scattering
      estimated_hours: 2-3
      deliverables:
      - Lambertian material scattering rays in random hemisphere directions
      - Random point generation on unit sphere for diffuse bounce direction
      - Recursive ray tracing following scattered rays up to depth limit
      - Gamma correction transforming linear color space to display color space
    - id: 7
      name: Metal and Reflections
      description: Implement metallic (reflective) materials with fuzz.
      acceptance_criteria:
      - Metal surfaces produce mirror-like reflections of surrounding scene objects
      - Reflection formula computes v - 2*dot(v,n)*n for incident vector v and normal n
      - Fuzz parameter adds random perturbation making reflections appear rougher
      - Different materials can be assigned to different objects in the same scene
      hints:
        level1: Reflect direction around normal. Incident = incoming ray direction.
        level2: 'Fuzz: add random vector scaled by fuzziness to reflected direction.'
        level3: |-
          Vec3 reflect(Vec3& v, Vec3& n) {
            return v - 2*dot(v, n)*n;
          }
          Vec3 scattered = reflect(ray.dir, normal) + fuzz*random_in_unit_sphere();
      concepts:
      - Specular reflection
      - Roughness/fuzz
      - Material system
      estimated_hours: 2-3
      pitfalls:
      - Reflected ray pointing into surface (dot product check)
      - Infinite recursion without max depth limit
      - Fuzz parameter > 1 causes rays to pass through surface
      - Not attenuating color with each bounce produces unrealistic brightness
      deliverables:
      - Metal material reflecting rays about surface normal vector
      - Reflection vector computation using incident ray and normal
      - Fuzz parameter controlling roughness of metallic reflections
      - Material assignment system associating materials with scene objects
    - id: 8
      name: Dielectrics (Glass)
      description: Implement glass with refraction and Fresnel effects.
      acceptance_criteria:
      - Glass spheres both refract and reflect light based on angle of incidence
      - Snell's law correctly computes refracted direction given refractive index ratio
      - Schlick approximation produces higher reflectance at shallow viewing angles
      - Total internal reflection occurs when light exits dense medium at steep angles
      hints:
        level1: 'Snell''s law: n1*sin(theta1) = n2*sin(theta2). Glass IOR ~ 1.5'
        level2: Total internal reflection when sin(theta2) > 1. Just reflect.
        level3: |-
          Dielectric (glass) material with Snell's law in C++:

          class Dielectric : public Material {
              float refraction_index;  // Glass ~ 1.5
          public:
              Dielectric(float ri) : refraction_index(ri) {}

              bool scatter(const Ray& r_in, const HitRecord& rec,
                           Color& attenuation, Ray& scattered) const override {
                  attenuation = Color(1.0, 1.0, 1.0);  // Glass doesn't absorb
                  float ri = rec.front_face ? (1.0 / refraction_index) : refraction_index;

                  Vec3 unit_direction = normalize(r_in.direction);
                  float cos_theta = fmin(dot(-unit_direction, rec.normal), 1.0);
                  float sin_theta = sqrt(1.0 - cos_theta * cos_theta);

                  bool cannot_refract = ri * sin_theta > 1.0;  // Total internal reflection
                  Vec3 direction;

                  if (cannot_refract || reflectance(cos_theta, ri) > random_float()) {
                      direction = reflect(unit_direction, rec.normal);
                  } else {
                      direction = refract(unit_direction, rec.normal, ri);
                  }

                  scattered = Ray(rec.p, direction);
                  return true;
              }

              static float reflectance(float cosine, float ri) {
                  // Schlick's approximation
                  float r0 = (1 - ri) / (1 + ri);
                  r0 = r0 * r0;
                  return r0 + (1 - r0) * pow(1 - cosine, 5);
              }
          };

          Vec3 refract(const Vec3& uv, const Vec3& n, float etai_over_etat) {
              float cos_theta = fmin(dot(-uv, n), 1.0);
              Vec3 r_out_perp = etai_over_etat * (uv + cos_theta * n);
              Vec3 r_out_parallel = -sqrt(fabs(1.0 - r_out_perp.length_squared())) * n;
              return r_out_perp + r_out_parallel;
          }
      pitfalls:
      - Getting the IOR ratio right
      - Hollow glass spheres (negative radius trick)
      - Total internal reflection check
      concepts:
      - Refraction (Snell's law)
      - Total internal reflection
      - Fresnel equations
      estimated_hours: 3-4
      deliverables:
      - Dielectric material implementing both refraction and reflection
      - Snell's law computation determining refracted ray direction
      - Schlick approximation estimating reflectance at grazing angles
      - Total internal reflection handling when refraction is impossible
    - id: 9
      name: Positionable Camera
      description: Implement camera with position, look-at, and field of view.
      acceptance_criteria:
      - Camera can be positioned at arbitrary location pointing at specified target
      - Field of view parameter visibly changes the zoom level of rendered scene
      - Camera up vector determines roll orientation of the rendered view
      - Aspect ratio matches image width-to-height ratio without distortion
      hints:
        level1: 'Camera basis vectors: w (back), u (right), v (up).'
        level2: vfov = vertical field of view. viewport_height = 2 * tan(vfov/2)
        level3: |-
          Positionable camera with lookfrom/lookat in C++:

          class Camera {
              Vec3 origin;
              Vec3 lower_left_corner;
              Vec3 horizontal;
              Vec3 vertical;
              Vec3 u, v, w;  // Camera basis vectors
              float lens_radius;

          public:
              Camera(Vec3 lookfrom, Vec3 lookat, Vec3 vup,
                     float vfov, float aspect_ratio, float aperture, float focus_dist) {

                  float theta = degrees_to_radians(vfov);
                  float h = tan(theta / 2);
                  float viewport_height = 2.0 * h;
                  float viewport_width = aspect_ratio * viewport_height;

                  // Camera coordinate system
                  w = normalize(lookfrom - lookat);    // Points away from target
                  u = normalize(cross(vup, w));         // Points right
                  v = cross(w, u);                      // Points up

                  origin = lookfrom;
                  horizontal = focus_dist * viewport_width * u;
                  vertical = focus_dist * viewport_height * v;
                  lower_left_corner = origin - horizontal/2 - vertical/2 - focus_dist*w;

                  lens_radius = aperture / 2;
              }

              Ray get_ray(float s, float t) const {
                  // Random point on lens for depth of field
                  Vec3 rd = lens_radius * random_in_unit_disk();
                  Vec3 offset = u * rd.x + v * rd.y;

                  return Ray(
                      origin + offset,
                      lower_left_corner + s*horizontal + t*vertical - origin - offset
                  );
              }
          };

          // Usage:
          // Camera cam(Vec3(13,2,3), Vec3(0,0,0), Vec3(0,1,0), 20, 16.0/9.0, 0.1, 10.0);
      concepts:
      - Camera coordinate system
      - Field of view
      - Look-at transformation
      estimated_hours: 2-3
      pitfalls:
      - Field of view in radians vs degrees confusion
      - Up vector parallel to look direction crashes
      - Aspect ratio calculation wrong leads to stretched images
      - Viewport height/width off by one pixel
      deliverables:
      - Camera positioning with lookfrom and lookat point parameters
      - Field of view parameter controlling viewport angular width
      - Camera orientation basis vectors computed from view direction and up vector
      - Aspect ratio configuration matching output image dimensions
    - id: 10
      name: Depth of Field
      description: Add defocus blur (depth of field) effect.
      acceptance_criteria:
      - Objects at focus distance appear sharp while others appear blurred
      - Larger aperture produces more pronounced depth-of-field blur effect
      - Ray origins are randomly distributed within circular lens aperture disk
      - Final scene demonstrates multiple materials, camera positioning, and defocus blur
      hints:
        level1: Random origin on lens disk, ray still goes through focus point.
        level2: Larger aperture = more blur. Aperture = 0 = pinhole camera.
        level3: |-
          Vec3 rd = lens_radius * random_in_unit_disk();
          Vec3 offset = u * rd.x + v * rd.y;
          return Ray(origin + offset, lower_left + s*horizontal + t*vertical - origin - offset);
      concepts:
      - Thin lens model
      - Depth of field
      - Aperture and focus
      estimated_hours: 2-3
      pitfalls:
      - Aperture too large causes everything to be blurry
      - Focus distance wrong makes subject blurry
      - Random disk sampling bias causes visible artifacts
      - Thin lens approximation breaks at extreme apertures
      deliverables:
      - Thin lens model with configurable aperture size parameter
      - Focus distance parameter controlling plane of sharp focus
      - Random ray origin offset within lens disk for defocus blur
      - Final scene rendering combining all materials and camera effects
  build-react:
    id: build-react
    name: Build Your Own React
    description: Build a React-like library with Virtual DOM, reconciliation, hooks, and fiber architecture.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - DOM manipulation
    - JavaScript advanced concepts
    - Tree data structures
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible: []
    resources:
    - type: article
      name: Build your own React
      url: https://pomb.us/build-your-own-react/
    - type: talk
      name: React Fiber Architecture
      url: https://github.com/acdlite/react-fiber-architecture
    milestones:
    - id: 1
      name: Virtual DOM
      description: Create virtual DOM representation and rendering.
      acceptance_criteria:
      - createElement produces virtual nodes with type, props, and children properties
      - Virtual nodes represent both HTML elements and text content nodes
      - Render function creates matching real DOM elements from virtual node tree
      - Text nodes are correctly created for string and number child values
      hints:
        level1: 'VNode is just an object: { type, props, children }. Recursively create DOM elements.'
        level2: Handle primitives (string/number) as text nodes. props.children can be array or single element.
        level3: "// Virtual DOM element\nfunction createElement(type, props, ...children) {\n    return {\n        type,\n\
          \        props: {\n            ...props,\n            children: children.flat().map(child =>\n                typeof\
          \ child === 'object' ? child : createTextElement(child)\n            )\n        }\n    };\n}\n\nfunction createTextElement(text)\
          \ {\n    return {\n        type: 'TEXT_ELEMENT',\n        props: {\n            nodeValue: text,\n            children:\
          \ []\n        }\n    };\n}\n\n// Render vnode to DOM\nfunction render(element, container) {\n    const dom = element.type\
          \ === 'TEXT_ELEMENT'\n        ? document.createTextNode('')\n        : document.createElement(element.type);\n \
          \   \n    // Set properties\n    Object.keys(element.props)\n        .filter(key => key !== 'children')\n      \
          \  .forEach(name => {\n            if (name.startsWith('on')) {\n                const eventType = name.toLowerCase().substring(2);\n\
          \                dom.addEventListener(eventType, element.props[name]);\n            } else {\n                dom[name]\
          \ = element.props[name];\n            }\n        });\n    \n    // Render children\n    element.props.children.forEach(child\
          \ => render(child, dom));\n    \n    container.appendChild(dom);\n}"
      pitfalls:
      - Handling null/undefined children
      - Event listener naming
      - SVG namespace
      concepts:
      - Virtual DOM
      - Declarative UI
      - Tree representation
      estimated_hours: 8-12
      deliverables:
      - createElement function constructing virtual node tree structures
      - Virtual node structure storing type, props, and children
      - Render function converting virtual DOM tree to real DOM elements
      - Text node handling for plain string and number children
    - id: 2
      name: Reconciliation (Diffing)
      description: Implement efficient DOM updates through reconciliation.
      acceptance_criteria:
      - Diff algorithm identifies minimal set of changes between two virtual trees
      - Patch operations update real DOM without recreating unchanged subtrees
      - Keyed children are reordered efficiently without unnecessary recreation
      - Event handler props are correctly attached and detached during updates
      hints:
        level1: Compare type first. Same type = update props. Different = replace entire subtree.
        level2: For lists, use keys to match elements. Reorder instead of recreate.
        level3: "function reconcile(dom, oldVNode, newVNode) {\n    // Addition\n    if (!oldVNode) {\n        return render(newVNode,\
          \ dom);\n    }\n    \n    // Deletion\n    if (!newVNode) {\n        dom.remove();\n        return null;\n    }\n\
          \    \n    // Replace entire subtree if type changed\n    if (oldVNode.type !== newVNode.type) {\n        const\
          \ newDom = createDom(newVNode);\n        dom.replaceWith(newDom);\n        return newDom;\n    }\n    \n    // Same\
          \ type - update props\n    updateProps(dom, oldVNode.props, newVNode.props);\n    \n    // Reconcile children\n\
          \    reconcileChildren(dom, oldVNode.props.children, newVNode.props.children);\n    \n    return dom;\n}\n\nfunction\
          \ reconcileChildren(dom, oldChildren, newChildren) {\n    const oldKeyed = new Map();\n    const newKeyed = new\
          \ Map();\n    \n    // Index by key\n    oldChildren.forEach((child, i) => {\n        const key = child.props?.key\
          \ ?? i;\n        oldKeyed.set(key, { vnode: child, dom: dom.childNodes[i] });\n    });\n    \n    newChildren.forEach((child,\
          \ i) => {\n        const key = child.props?.key ?? i;\n        newKeyed.set(key, child);\n    });\n    \n    //\
          \ Remove deleted\n    for (const [key, { dom: childDom }] of oldKeyed) {\n        if (!newKeyed.has(key)) {\n  \
          \          childDom.remove();\n        }\n    }\n    \n    // Update or add\n    newChildren.forEach((newChild,\
          \ i) => {\n        const key = newChild.props?.key ?? i;\n        const old = oldKeyed.get(key);\n        \n   \
          \     if (old) {\n            reconcile(old.dom, old.vnode, newChild);\n        } else {\n            render(newChild,\
          \ dom);\n        }\n    });\n}"
      pitfalls:
      - Key stability
      - Index as key problems
      - DOM node reference tracking
      concepts:
      - Tree diffing
      - Minimal updates
      - Keyed reconciliation
      estimated_hours: 12-18
      deliverables:
      - Diff algorithm comparing old and new virtual DOM trees
      - Patch application updating only changed portions of real DOM
      - Key-based reconciliation for efficient list reordering
      - Property diffing detecting added, removed, and changed attributes
    - id: 3
      name: Fiber Architecture
      description: Implement interruptible rendering with fiber nodes.
      acceptance_criteria:
      - Fiber tree mirrors component hierarchy with parent, child, and sibling links
      - Work loop uses requestIdleCallback to process fibers without blocking main thread
      - Commit phase applies all DOM mutations atomically after render completes
      - In-progress render work can be interrupted and resumed without visual artifacts
      hints:
        level1: Fiber = work unit with parent/child/sibling pointers. Process one fiber, then yield.
        level2: Separate render (can interrupt) from commit (must complete). Use double buffering.
        level3: "// Fiber node\nlet nextUnitOfWork = null;\nlet wipRoot = null;  // Work in progress root\nlet currentRoot\
          \ = null;  // Committed root\nlet deletions = [];\n\nfunction createFiber(element, parent) {\n    return {\n   \
          \     type: element.type,\n        props: element.props,\n        parent,\n        dom: null,\n        child: null,\n\
          \        sibling: null,\n        alternate: null,  // Previous fiber\n        effectTag: null   // PLACEMENT, UPDATE,\
          \ DELETION\n    };\n}\n\nfunction workLoop(deadline) {\n    let shouldYield = false;\n    \n    while (nextUnitOfWork\
          \ && !shouldYield) {\n        nextUnitOfWork = performUnitOfWork(nextUnitOfWork);\n        shouldYield = deadline.timeRemaining()\
          \ < 1;\n    }\n    \n    // Commit when all work is done\n    if (!nextUnitOfWork && wipRoot) {\n        commitRoot();\n\
          \    }\n    \n    requestIdleCallback(workLoop);\n}\n\nrequestIdleCallback(workLoop);\n\nfunction performUnitOfWork(fiber)\
          \ {\n    // Create DOM node\n    if (!fiber.dom) {\n        fiber.dom = createDom(fiber);\n    }\n    \n    // Create\
          \ fibers for children\n    reconcileChildren(fiber, fiber.props.children);\n    \n    // Return next unit of work\
          \ (depth-first)\n    if (fiber.child) return fiber.child;\n    \n    let nextFiber = fiber;\n    while (nextFiber)\
          \ {\n        if (nextFiber.sibling) return nextFiber.sibling;\n        nextFiber = nextFiber.parent;\n    }\n  \
          \  return null;\n}\n\nfunction commitRoot() {\n    deletions.forEach(commitWork);\n    commitWork(wipRoot.child);\n\
          \    currentRoot = wipRoot;\n    wipRoot = null;\n}\n\nfunction commitWork(fiber) {\n    if (!fiber) return;\n \
          \   \n    const parentDom = fiber.parent.dom;\n    \n    if (fiber.effectTag === 'PLACEMENT' && fiber.dom) {\n \
          \       parentDom.appendChild(fiber.dom);\n    } else if (fiber.effectTag === 'DELETION') {\n        parentDom.removeChild(fiber.dom);\n\
          \    } else if (fiber.effectTag === 'UPDATE' && fiber.dom) {\n        updateDom(fiber.dom, fiber.alternate.props,\
          \ fiber.props);\n    }\n    \n    commitWork(fiber.child);\n    commitWork(fiber.sibling);\n}"
      pitfalls:
      - Effect ordering
      - Interrupted state
      - Memory leaks in alternate
      concepts:
      - Cooperative scheduling
      - Incremental rendering
      - Work units
      estimated_hours: 15-25
      deliverables:
      - Fiber node structure linking parent, child, and sibling fibers
      - Work loop yielding control to browser between unit-of-work increments
      - Commit phase applying all accumulated DOM changes in single batch
      - Concurrent rendering support allowing work interruption and resumption
    - id: 4
      name: Hooks
      description: Implement useState and useEffect hooks.
      acceptance_criteria:
      - useState returns current value and setter function triggering component re-render
      - useEffect runs cleanup function before re-running effect on dependency changes
      - Hooks called in different order across renders produce a descriptive error
      - Custom hooks can compose useState and useEffect for reusable stateful logic
      hints:
        level1: Track hooks in array per fiber. Index determines which hook. Reset index each render.
        level2: useEffect runs after commit. Compare deps to decide if effect runs.
        level3: "let wipFiber = null;\nlet hookIndex = null;\n\nfunction useState(initial) {\n    const oldHook = wipFiber.alternate?.hooks?.[hookIndex];\n\
          \    \n    const hook = {\n        state: oldHook ? oldHook.state : initial,\n        queue: []\n    };\n    \n\
          \    // Process queued setState calls\n    const actions = oldHook ? oldHook.queue : [];\n    actions.forEach(action\
          \ => {\n        hook.state = typeof action === 'function' ? action(hook.state) : action;\n    });\n    \n    const\
          \ setState = action => {\n        hook.queue.push(action);\n        // Trigger re-render\n        wipRoot = {\n\
          \            dom: currentRoot.dom,\n            props: currentRoot.props,\n            alternate: currentRoot\n\
          \        };\n        nextUnitOfWork = wipRoot;\n        deletions = [];\n    };\n    \n    wipFiber.hooks.push(hook);\n\
          \    hookIndex++;\n    return [hook.state, setState];\n}\n\nfunction useEffect(callback, deps) {\n    const oldHook\
          \ = wipFiber.alternate?.hooks?.[hookIndex];\n    \n    const hasChanged = !oldHook || \n        !deps ||\n     \
          \   deps.some((dep, i) => dep !== oldHook.deps[i]);\n    \n    const hook = {\n        deps,\n        cleanup: oldHook?.cleanup,\n\
          \        effect: hasChanged ? callback : null\n    };\n    \n    wipFiber.hooks.push(hook);\n    hookIndex++;\n\
          }\n\n// In commit phase\nfunction commitEffects(fiber) {\n    if (!fiber) return;\n    \n    fiber.hooks?.forEach(hook\
          \ => {\n        if (hook.cleanup) hook.cleanup();\n        if (hook.effect) {\n            hook.cleanup = hook.effect();\n\
          \        }\n    });\n    \n    commitEffects(fiber.child);\n    commitEffects(fiber.sibling);\n}\n\n// Function\
          \ component handling\nfunction updateFunctionComponent(fiber) {\n    wipFiber = fiber;\n    hookIndex = 0;\n   \
          \ wipFiber.hooks = [];\n    \n    const children = [fiber.type(fiber.props)];\n    reconcileChildren(fiber, children);\n\
          }"
      pitfalls:
      - Conditional hooks
      - Stale closures
      - Effect cleanup timing
      concepts:
      - Hooks pattern
      - State management
      - Side effects
      estimated_hours: 25-45
      deliverables:
      - useState hook managing component-local state with update triggers
      - useEffect hook running side effects after component render commits
      - Hook ordering rules enforcing consistent call order across renders
      - Custom hooks composing built-in hooks into reusable logic units
  build-redis:
    id: build-redis
    name: Build Your Own Redis
    description: Build an in-memory data structure store that implements the Redis protocol. You'll learn about TCP servers,
      protocol parsing, data structures, and persistence.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - TCP/IP networking basics
    - Hash tables and data structures
    - Concurrency fundamentals
    - File I/O
    languages:
      recommended:
      - Go
      - Rust
      - C
      also_possible:
      - Python
      - Java
      - TypeScript
    resources:
    - name: CodeCrafters Redis Challenge
      url: https://app.codecrafters.io/courses/redis/overview
      type: interactive
    - name: Build Your Own Redis (Book)
      url: https://build-your-own.org/redis/
      type: book
    - name: Redis Protocol Specification (RESP)
      url: https://redis.io/docs/reference/protocol-spec/
      type: documentation
    milestones:
    - id: 1
      name: TCP Server + RESP Protocol
      description: Create a TCP server that listens on port 6379 and responds to PING with +PONG. Implement basic RESP parsing.
      acceptance_criteria:
      - TCP server listens on configurable port and accepts concurrent client connections
      - RESP parser correctly decodes all five RESP data types from byte stream
      - RESP serializer produces wire-format output compatible with redis-cli client
      - PING command returns PONG response to verify end-to-end protocol implementation
      - Server handles client disconnection without crashing or leaking resources
      hints:
        level1: Start with a simple TCP server using your language's net library. Accept connections in a loop.
        level2: RESP Simple Strings start with '+', errors with '-'. PING expects '+PONG\r\n' response.
        level3: |-
          TCP server with RESP protocol in Python:

          import socket
          import threading

          class RESPParser:
              def parse(self, data):
                  """Parse RESP protocol message."""
                  if not data:
                      return None, data

                  msg_type = chr(data[0])

                  if msg_type == '+':  # Simple String
                      end = data.index(b'\r\n')
                      return data[1:end].decode(), data[end+2:]

                  elif msg_type == '-':  # Error
                      end = data.index(b'\r\n')
                      return Exception(data[1:end].decode()), data[end+2:]

                  elif msg_type == ':':  # Integer
                      end = data.index(b'\r\n')
                      return int(data[1:end]), data[end+2:]

                  elif msg_type == '$':  # Bulk String
                      end = data.index(b'\r\n')
                      length = int(data[1:end])
                      if length == -1:
                          return None, data[end+2:]
                      start = end + 2
                      return data[start:start+length], data[start+length+2:]

                  elif msg_type == '*':  # Array
                      end = data.index(b'\r\n')
                      count = int(data[1:end])
                      remaining = data[end+2:]
                      result = []
                      for _ in range(count):
                          item, remaining = self.parse(remaining)
                          result.append(item)
                      return result, remaining

          class RedisServer:
              def __init__(self, host='127.0.0.1', port=6379):
                  self.host = host
                  self.port = port
                  self.store = {}
                  self.parser = RESPParser()

              def start(self):
                  sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                  sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                  sock.bind((self.host, self.port))
                  sock.listen(100)

                  while True:
                      client, addr = sock.accept()
                      threading.Thread(target=self.handle_client, args=(client,)).start()

              def handle_client(self, client):
                  buffer = b''
                  while True:
                      data = client.recv(4096)
                      if not data:
                          break
                      buffer += data
                      command, buffer = self.parser.parse(buffer)
                      if command:
                          response = self.execute(command)
                          client.send(response)

              def encode_response(self, value):
                  if value is None:
                      return b'$-1\r\n'
                  if isinstance(value, str):
                      return f'+{value}\r\n'.encode()
                  if isinstance(value, int):
                      return f':{value}\r\n'.encode()
                  return b'-ERR unknown type\r\n'
      pitfalls:
      - Forgetting \r\n line endings (CRLF, not just LF)
      - Not handling partial reads (TCP is a stream)
      - Blocking the main thread on single client
      concepts:
      - TCP sockets and server lifecycle
      - RESP protocol encoding/decoding
      - Basic network I/O
      estimated_hours: 3-4
      deliverables:
      - TCP server accepting multiple concurrent client connections
      - RESP protocol parser decoding simple strings, errors, integers, bulk strings, and arrays
      - RESP serializer encoding server responses into wire format
      - Client connection lifecycle management with graceful disconnection
    - id: 2
      name: GET/SET/DEL Commands
      description: Implement basic key-value operations. Store data in an in-memory hash map.
      acceptance_criteria:
      - SET stores key-value pair accessible by subsequent GET commands
      - GET returns bulk string for existing keys and nil for missing keys
      - DEL removes specified keys and returns integer count of keys actually deleted
      - SET supports NX (only if not exists) and XX (only if exists) flags
      - Key-value store handles concurrent access from multiple clients safely
      hints:
        level1: Use a hash map (dict/map) to store key-value pairs.
        level2: Parse RESP arrays for commands. SET comes as *3\r\n$3\r\nSET\r\n$3\r\nkey\r\n$5\r\nvalue\r\n
        level3: |-
          # Redis-like command implementation
          class RedisStore:
              def __init__(self):
                  self.data = {}
                  self.expiry = {}  # key -> expiry timestamp

              def set(self, key, value, ex=None, px=None, nx=False, xx=False):
                  # NX: only set if key doesn't exist
                  if nx and key in self.data:
                      return None
                  # XX: only set if key exists
                  if xx and key not in self.data:
                      return None

                  self.data[key] = value

                  if ex:  # Expire in seconds
                      self.expiry[key] = time.time() + ex
                  elif px:  # Expire in milliseconds
                      self.expiry[key] = time.time() + px / 1000.0
                  elif key in self.expiry:
                      del self.expiry[key]

                  return "OK"

              def get(self, key):
                  if not self._check_expiry(key):
                      return None
                  return self.data.get(key)

              def delete(self, *keys):
                  count = 0
                  for key in keys:
                      if key in self.data:
                          del self.data[key]
                          self.expiry.pop(key, None)
                          count += 1
                  return count

              def _check_expiry(self, key):
                  if key in self.expiry:
                      if time.time() >= self.expiry[key]:
                          del self.data[key]
                          del self.expiry[key]
                          return False
                  return key in self.data

          # RESP protocol parser
          def parse_resp(data):
              if data[0:1] == b'+':  # Simple string
                  return data[1:data.index(b'\r\n')].decode()
              elif data[0:1] == b'-':  # Error
                  return Exception(data[1:data.index(b'\r\n')].decode())
              elif data[0:1] == b':':  # Integer
                  return int(data[1:data.index(b'\r\n')])
              elif data[0:1] == b'$':  # Bulk string
                  length = int(data[1:data.index(b'\r\n')])
                  if length == -1:
                      return None
                  start = data.index(b'\r\n') + 2
                  return data[start:start+length]
              elif data[0:1] == b'*':  # Array
                  count = int(data[1:data.index(b'\r\n')])
                  # Recursively parse elements...
                  pass
      pitfalls:
      - Not handling binary-safe strings
      - Forgetting null response for non-existent keys
      - Case sensitivity issues
      concepts:
      - Hash table implementation
      - RESP array parsing
      - Command dispatch pattern
      estimated_hours: 2-3
      deliverables:
      - In-memory key-value store supporting string values
      - SET command storing key-value pairs with optional flags
      - GET command retrieving values by key with nil for missing keys
      - DEL command removing one or more keys and returning count deleted
    - id: 3
      name: Expiration (TTL)
      description: Add key expiration support. Keys can be set with PX (milliseconds) or EX (seconds) options.
      acceptance_criteria:
      - EXPIRE sets TTL on existing key and returns 1 on success or 0 if key missing
      - Keys become inaccessible via GET after their TTL has elapsed
      - TTL command returns remaining seconds until key expiration
      - Lazy expiration removes expired key when client attempts to access it
      - Active expiration periodically scans and removes a sample of expired keys
      hints:
        level1: Store expiration timestamp alongside each value.
        level2: Check expiration on GET (lazy deletion). Optionally run background cleanup.
        level3: |-
          Key expiration with lazy and active deletion in Python:

          import time
          import threading
          import random

          class RedisStore:
              def __init__(self):
                  self.data = {}
                  self.expires = {}  # key -> absolute expiry timestamp
                  self._start_expiry_thread()

              def set(self, key, value, ex=None, px=None):
                  self.data[key] = value
                  if ex:  # Seconds
                      self.expires[key] = time.time() + ex
                  elif px:  # Milliseconds
                      self.expires[key] = time.time() + px / 1000.0
                  elif key in self.expires:
                      del self.expires[key]
                  return "OK"

              def get(self, key):
                  # Lazy expiration: check on access
                  if key in self.expires:
                      if time.time() >= self.expires[key]:
                          del self.data[key]
                          del self.expires[key]
                          return None
                  return self.data.get(key)

              def ttl(self, key):
                  if key not in self.data:
                      return -2  # Key doesn't exist
                  if key not in self.expires:
                      return -1  # No expiry set
                  remaining = self.expires[key] - time.time()
                  return max(0, int(remaining))

              def expire(self, key, seconds):
                  if key not in self.data:
                      return 0
                  self.expires[key] = time.time() + seconds
                  return 1

              def _start_expiry_thread(self):
                  def active_expiry():
                      while True:
                          time.sleep(0.1)  # Run 10 times per second
                          # Sample 20 random keys with expiry
                          keys = list(self.expires.keys())
                          sample = random.sample(keys, min(20, len(keys)))
                          now = time.time()
                          for key in sample:
                              if key in self.expires and now >= self.expires[key]:
                                  self.data.pop(key, None)
                                  self.expires.pop(key, None)

                  thread = threading.Thread(target=active_expiry, daemon=True)
                  thread.start()
      pitfalls:
      - Using relative time instead of absolute timestamp
      - Not handling clock drift
      - Memory leaks from never-accessed expired keys
      concepts:
      - TTL and expiration strategies
      - Lazy vs active deletion
      - Time handling and precision
      estimated_hours: 2-3
      deliverables:
      - EXPIRE command setting time-to-live in seconds on existing keys
      - TTL storage per key tracking absolute expiration timestamps
      - Lazy expiration checking and removing expired keys on access
      - Active expiration sampling and removing expired keys periodically
    - id: 4
      name: Data Structures (List, Set, Hash)
      description: Implement Redis data structure commands beyond simple strings.
      acceptance_criteria:
      - LPUSH and RPUSH prepend and append elements to list respectively
      - SADD adds unique members to set and returns count of newly added elements
      - HSET stores field-value pairs in hash and HGET retrieves by field name
      - Operations on wrong type return WRONGTYPE error without modifying data
      - LRANGE returns list elements within specified start and stop index range
      hints:
        level1: 'Use native data structures: arrays for lists, sets for sets, nested maps for hashes.'
        level2: Lists in Redis are doubly-linked for O(1) push/pop at both ends.
        level3: |-
          # Redis data structure implementations
          class RedisList:
              def __init__(self):
                  self.items = []

              def lpush(self, *values):
                  for v in reversed(values):
                      self.items.insert(0, v)
                  return len(self.items)

              def rpush(self, *values):
                  self.items.extend(values)
                  return len(self.items)

              def lpop(self, count=1):
                  result = self.items[:count]
                  self.items = self.items[count:]
                  return result[0] if count == 1 else result

              def lrange(self, start, stop):
                  # Redis uses inclusive stop, Python uses exclusive
                  if stop < 0:
                      stop = len(self.items) + stop + 1
                  else:
                      stop += 1
                  return self.items[start:stop]

          class RedisSet:
              def __init__(self):
                  self.members = set()

              def sadd(self, *members):
                  added = sum(1 for m in members if m not in self.members)
                  self.members.update(members)
                  return added

              def srem(self, *members):
                  removed = sum(1 for m in members if m in self.members)
                  self.members -= set(members)
                  return removed

              def sismember(self, member):
                  return 1 if member in self.members else 0

              def smembers(self):
                  return list(self.members)

          class RedisHash:
              def __init__(self):
                  self.fields = {}

              def hset(self, field, value):
                  is_new = field not in self.fields
                  self.fields[field] = value
                  return 1 if is_new else 0

              def hget(self, field):
                  return self.fields.get(field)

              def hgetall(self):
                  result = []
                  for k, v in self.fields.items():
                      result.extend([k, v])
                  return result

              def hincrby(self, field, increment):
                  self.fields[field] = int(self.fields.get(field, 0)) + increment
                  return self.fields[field]
      pitfalls:
      - Using array for list (O(n) insert at head)
      - Not handling type errors
      - LRANGE negative indices
      concepts:
      - Linked lists vs arrays
      - Set data structure
      - Skip lists (for sorted sets)
      estimated_hours: 4-6
      deliverables:
      - List data type with LPUSH, RPUSH, LPOP, RPOP, and LRANGE operations
      - Set data type with SADD, SREM, SMEMBERS, and SISMEMBER operations
      - Hash data type with HSET, HGET, HDEL, and HGETALL operations
      - Type checking preventing wrong-type operations on existing keys
    - id: 5
      name: Persistence (RDB Snapshots)
      description: Implement point-in-time snapshots using RDB format. SAVE blocks, BGSAVE forks.
      acceptance_criteria:
      - RDB writer serializes all keys, values, and TTLs to binary file format
      - RDB reader restores complete database state from binary snapshot file on startup
      - BGSAVE command forks background process to create snapshot without blocking clients
      - Automatic saves trigger when configured number of changes occur within time window
      hints:
        level1: Start with custom binary format. Later match Redis RDB format.
        level2: BGSAVE uses fork() - child inherits memory snapshot via copy-on-write.
        level3: |-
          RDB snapshot persistence in Python:

          import struct
          import os
          import pickle

          class RDBPersistence:
              MAGIC = b'REDIS0011'  # RDB version 11

              def save(self, store, filename='dump.rdb'):
                  """Save database to RDB file."""
                  with open(filename + '.tmp', 'wb') as f:
                      # Magic string
                      f.write(self.MAGIC)

                      # Database selector (DB 0)
                      f.write(bytes([0xFE, 0x00]))  # SELECTDB opcode + db number

                      # Resize DB info
                      f.write(bytes([0xFB]))  # RESIZEDB opcode
                      self._write_length(f, len(store.data))
                      self._write_length(f, len(store.expires))

                      # Key-value pairs
                      for key, value in store.data.items():
                          # Expiry (if exists)
                          if key in store.expires:
                              f.write(bytes([0xFC]))  # EXPIRETIME_MS opcode
                              f.write(struct.pack('<Q', int(store.expires[key] * 1000)))

                          # Value type (0 = string)
                          f.write(bytes([0x00]))

                          # Key and value
                          self._write_string(f, key.encode())
                          self._write_string(f, value.encode() if isinstance(value, str) else pickle.dumps(value))

                      # EOF
                      f.write(bytes([0xFF]))

                      # CRC64 checksum (simplified - use actual CRC64 in production)
                      f.write(b'\x00' * 8)

                  # Atomic rename
                  os.rename(filename + '.tmp', filename)

              def _write_length(self, f, length):
                  if length < 64:
                      f.write(bytes([length]))
                  elif length < 16384:
                      f.write(bytes([0x40 | (length >> 8), length & 0xFF]))

              def _write_string(self, f, data):
                  self._write_length(f, len(data))
                  f.write(data)

              def load(self, filename='dump.rdb'):
                  """Load database from RDB file."""
                  # ... parse RDB format and restore data
      pitfalls:
      - Blocking main thread during save
      - Not using fork() for BGSAVE
      - Corrupted RDB from incomplete writes
      concepts:
      - Binary serialization
      - Process forking and copy-on-write
      - Atomic file operations
      estimated_hours: 5-8
      deliverables:
      - RDB file format writer serializing entire database to binary snapshot
      - RDB file format reader deserializing snapshot to restore database state
      - Background save process creating snapshot without blocking main thread
      - Automatic save triggers based on configurable change count thresholds
    - id: 6
      name: Persistence (AOF)
      description: Implement Append-Only File logging for durability.
      acceptance_criteria:
      - Every write command is appended to AOF file in RESP wire format
      - AOF replay correctly reconstructs database state from command log on startup
      - AOF rewrite produces compact file equivalent to current database state
      - Fsync policy supports always, every-second, and OS-controlled modes
      - AOF and RDB can coexist with AOF taking priority for recovery
      hints:
        level1: Simply append each write command to a file in RESP format.
        level2: 'BGREWRITEAOF: fork, iterate current state, write minimal commands.'
        level3: |-
          fsync strategies:
          - always: fsync after every write (safest, slowest)
          - everysec: fsync once per second (good balance)
          - no: let OS decide (fastest, riskier)
      pitfalls:
      - AOF growing unbounded without rewrite
      - Concurrent writes during BGREWRITEAOF
      - Data loss from buffered writes
      concepts:
      - Write-ahead logging
      - fsync and durability guarantees
      - Log compaction
      estimated_hours: 4-6
      deliverables:
      - Append-only file logging every write command in RESP format
      - AOF replay mechanism reconstructing database state from command log
      - AOF rewrite compacting log by generating minimal command sequence
      - Configurable fsync policy balancing durability against write performance
    - id: 7
      name: Pub/Sub
      description: Implement publish/subscribe messaging pattern.
      acceptance_criteria:
      - SUBSCRIBE registers client to channel and confirms subscription with message count
      - PUBLISH delivers message to all clients subscribed to the target channel
      - UNSUBSCRIBE removes subscription and notifies client of remaining subscription count
      - PSUBSCRIBE matches channels using glob patterns like 'news.*' for wildcard subscription
      hints:
        level1: Maintain map of channel -> list of subscriber connections.
        level2: Subscribed clients enter special mode - only pub/sub commands allowed.
        level3: |-
          # Redis Pub/Sub implementation
          import asyncio
          from collections import defaultdict

          class PubSubManager:
              def __init__(self):
                  self.subscribers = defaultdict(set)  # channel -> set of clients
                  self.patterns = defaultdict(set)      # pattern -> set of clients

              def subscribe(self, client, *channels):
                  for channel in channels:
                      self.subscribers[channel].add(client)
                      client.send_message(['subscribe', channel, len(client.subscriptions)])
                      client.subscriptions.add(channel)

              def unsubscribe(self, client, *channels):
                  if not channels:
                      channels = list(client.subscriptions)
                  for channel in channels:
                      self.subscribers[channel].discard(client)
                      client.subscriptions.discard(channel)
                      client.send_message(['unsubscribe', channel, len(client.subscriptions)])

              def publish(self, channel, message):
                  count = 0
                  # Direct subscribers
                  for client in self.subscribers[channel]:
                      client.send_message(['message', channel, message])
                      count += 1
                  # Pattern subscribers
                  for pattern, clients in self.patterns.items():
                      if self._match_pattern(pattern, channel):
                          for client in clients:
                              client.send_message(['pmessage', pattern, channel, message])
                              count += 1
                  return count

              def _match_pattern(self, pattern, channel):
                  # Simple glob matching: * matches any sequence
                  import fnmatch
                  return fnmatch.fnmatch(channel, pattern)

          # Async client handling for pub/sub
          class RedisClient:
              def __init__(self, reader, writer):
                  self.reader = reader
                  self.writer = writer
                  self.subscriptions = set()
                  self.message_queue = asyncio.Queue()

              async def handle_pubsub_mode(self):
                  # In pub/sub mode, client can only send SUBSCRIBE/UNSUBSCRIBE
                  while self.subscriptions:
                      msg = await self.message_queue.get()
                      self.writer.write(encode_resp_array(msg))
                      await self.writer.drain()
      pitfalls:
      - Not blocking other commands in subscribed state
      - Memory leaks from disconnected subscribers
      - Race conditions in publish
      concepts:
      - Pub/Sub pattern
      - Observer pattern
      - Connection state management
      estimated_hours: 3-4
      deliverables:
      - SUBSCRIBE command registering client to receive messages on named channels
      - PUBLISH command broadcasting message to all subscribers of a channel
      - UNSUBSCRIBE command removing client subscription from specified channels
      - Pattern-based subscription matching channels with glob-style patterns
    - id: 8
      name: Cluster Mode (Sharding)
      description: Implement horizontal scaling with hash slot based sharding.
      acceptance_criteria:
      - 16384 hash slots are distributed evenly across all cluster nodes
      - Key routing consistently maps same key to same hash slot using CRC16
      - MOVED response includes correct target node address for redirected key
      - Cluster nodes exchange topology information and detect failed peers via gossip
      hints:
        level1: CRC16(key) % 16384 gives the slot. Each node owns a range.
        level2: On wrong node, return -MOVED slot ip:port. Client should retry.
        level3: |-
          # Redis Cluster sharding implementation
          import hashlib

          def crc16(data):
              '''CRC16 XMODEM - used by Redis Cluster'''
              crc = 0
              for byte in data:
                  crc ^= byte << 8
                  for _ in range(8):
                      if crc & 0x8000:
                          crc = (crc << 1) ^ 0x1021
                      else:
                          crc <<= 1
                      crc &= 0xFFFF
              return crc

          def key_slot(key):
              '''Calculate hash slot for a key (0-16383)'''
              # Handle hash tags: {tag}key uses only "tag" for hashing
              start = key.find(b'{')
              if start >= 0:
                  end = key.find(b'}', start + 1)
                  if end > start + 1:
                      key = key[start + 1:end]
              return crc16(key) % 16384

          class ClusterNode:
              def __init__(self, node_id, host, port):
                  self.id = node_id
                  self.host = host
                  self.port = port
                  self.slots = set()  # Set of slot numbers this node owns
                  self.replicas = []  # Replica nodes

          class Cluster:
              def __init__(self):
                  self.nodes = {}  # node_id -> ClusterNode
                  self.slot_map = [None] * 16384  # slot -> node_id

              def assign_slots(self, node_id, start_slot, end_slot):
                  node = self.nodes[node_id]
                  for slot in range(start_slot, end_slot + 1):
                      self.slot_map[slot] = node_id
                      node.slots.add(slot)

              def get_node_for_key(self, key):
                  slot = key_slot(key if isinstance(key, bytes) else key.encode())
                  node_id = self.slot_map[slot]
                  return self.nodes[node_id]

              def handle_moved(self, slot, new_node):
                  '''Handle MOVED response during resharding'''
                  return f"MOVED {slot} {new_node.host}:{new_node.port}"

          # Client-side routing
          class ClusterClient:
              def __init__(self, startup_nodes):
                  self.cluster = Cluster()
                  self.refresh_slots(startup_nodes)

              def execute(self, *args):
                  key = args[1]  # Most commands have key as second arg
                  node = self.cluster.get_node_for_key(key)
                  try:
                      return node.execute(*args)
                  except MovedError as e:
                      self.refresh_slots()
                      return self.execute(*args)
      pitfalls:
      - Not handling key migration
      - Cross-slot operations
      - Network partitions and split brain
      concepts:
      - Consistent hashing
      - Hash slots and key routing
      - Distributed systems fundamentals
      estimated_hours: 8-12
      deliverables:
      - Hash slot assignment mapping 16384 slots across cluster nodes
      - Key-to-slot mapping using CRC16 hash of key modulo 16384
      - MOVED redirect response directing clients to correct shard node
      - Cluster topology gossip protocol sharing node state information
  build-regex:
    id: build-regex
    name: Build Your Own Regex Engine
    description: Build a regex engine using NFA/DFA and Thompson's construction.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - Automata theory
    - Graph algorithms
    - Recursion
    - C/Rust/Go
    languages:
      recommended:
      - C
      - Rust
      - Go
      - Python
      also_possible:
      - JavaScript
    resources:
    - type: article
      name: Regular Expression Matching
      url: https://swtch.com/~rsc/regexp/regexp1.html
    - type: book
      name: Introduction to Automata Theory
      url: https://www.amazon.com/Introduction-Automata-Theory-Languages-Computation/dp/0321455363
    milestones:
    - id: 1
      name: Regex Parser
      description: Parse regex pattern into AST.
      acceptance_criteria:
      - Parser correctly handles literal character matching in patterns
      - Alternation operator splits pattern into two alternative branches
      - Concatenation implicitly chains adjacent pattern elements in sequence
      - Quantifiers *, +, and ? are parsed with correct binding to preceding element
      - Parenthesized groups override default operator precedence in pattern parsing
      - Character classes like [a-z] are parsed into sets of matching characters
      hints:
        level1: 'Use recursive descent. Handle operator precedence: () > * > concat > |'
        level2: Build AST nodes for each regex construct.
        level3: "## Regex Parser\n\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\n\
          class Literal:\n    char: str\n\n@dataclass\nclass Concat:\n    left: 'Node'\n    right: 'Node'\n\n@dataclass\n\
          class Alternation:\n    left: 'Node'\n    right: 'Node'\n\n@dataclass\nclass Star:\n    child: 'Node'\n\n@dataclass\n\
          class Plus:\n    child: 'Node'\n\n@dataclass\nclass Optional:\n    child: 'Node'\n\n@dataclass\nclass CharClass:\n\
          \    chars: set\n    negated: bool = False\n\nNode = Literal | Concat | Alternation | Star | Plus | Optional | CharClass\n\
          \nclass RegexParser:\n    def __init__(self, pattern: str):\n        self.pattern = pattern\n        self.pos =\
          \ 0\n    \n    def parse(self) -> Node:\n        return self.parseAlternation()\n    \n    def parseAlternation(self)\
          \ -> Node:\n        left = self.parseConcat()\n        while self.match('|'):\n            right = self.parseConcat()\n\
          \            left = Alternation(left, right)\n        return left\n    \n    def parseConcat(self) -> Node:\n  \
          \      nodes = []\n        while self.pos < len(self.pattern) and self.peek() not in '|)':\n            nodes.append(self.parseQuantified())\n\
          \        \n        if not nodes:\n            return Literal('')\n        \n        result = nodes[0]\n        for\
          \ node in nodes[1:]:\n            result = Concat(result, node)\n        return result\n    \n    def parseQuantified(self)\
          \ -> Node:\n        base = self.parseAtom()\n        \n        if self.match('*'):\n            return Star(base)\n\
          \        elif self.match('+'):\n            return Plus(base)\n        elif self.match('?'):\n            return\
          \ Optional(base)\n        \n        return base\n    \n    def parseAtom(self) -> Node:\n        if self.match('('):\n\
          \            node = self.parseAlternation()\n            self.expect(')')\n            return node\n        \n \
          \       if self.match('['):\n            return self.parseCharClass()\n        \n        if self.match('\\\\'):\n\
          \            return self.parseEscape()\n        \n        if self.match('.'):\n            return CharClass(set(),\
          \ negated=True)  # Match any\n        \n        char = self.advance()\n        return Literal(char)\n    \n    def\
          \ parseCharClass(self) -> Node:\n        negated = self.match('^')\n        chars = set()\n        \n        while\
          \ not self.match(']'):\n            c = self.advance()\n            if self.match('-') and self.peek() != ']':\n\
          \                end = self.advance()\n                for i in range(ord(c), ord(end) + 1):\n                 \
          \   chars.add(chr(i))\n            else:\n                chars.add(c)\n        \n        return CharClass(chars,\
          \ negated)\n    \n    # Helper methods\n    def peek(self) -> str:\n        return self.pattern[self.pos] if self.pos\
          \ < len(self.pattern) else ''\n    \n    def advance(self) -> str:\n        c = self.peek()\n        self.pos +=\
          \ 1\n        return c\n    \n    def match(self, c: str) -> bool:\n        if self.peek() == c:\n            self.pos\
          \ += 1\n            return True\n        return False\n```"
      pitfalls:
      - Operator precedence wrong
      - Escape handling
      - Empty alternation branches
      concepts:
      - Parsing
      - AST construction
      - Regex syntax
      estimated_hours: 8-12
      deliverables:
      - Tokenizer splitting regex pattern into operator and literal tokens
      - Recursive descent parser building abstract syntax tree from tokens
      - AST node types for literal, concatenation, alternation, and quantifiers
      - Escape sequence handling for special characters and character classes
    - id: 2
      name: Thompson's Construction (NFA)
      description: Convert regex AST to NFA using Thompson's construction.
      acceptance_criteria:
      - NFA states store transitions as maps from characters to destination state sets
      - Epsilon transitions connect states without requiring input character consumption
      - Thompson's construction produces correct NFA for each AST node type
      - Composed NFA correctly matches strings accepted by the original regex pattern
      hints:
        level1: Each regex construct becomes a small NFA fragment with start/accept states.
        level2: 'Concatenation: connect first''s accept to second''s start. Alternation: split with epsilon.'
        level3: "## Thompson's Construction\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Set,\
          \ Dict, List\n\n@dataclass\nclass NFAState:\n    id: int\n    epsilon: List['NFAState'] = field(default_factory=list)\n\
          \    transitions: Dict[str, List['NFAState']] = field(default_factory=dict)\n    is_accept: bool = False\n\nclass\
          \ NFAFragment:\n    def __init__(self, start: NFAState, accept: NFAState):\n        self.start = start\n       \
          \ self.accept = accept\n\nclass NFABuilder:\n    def __init__(self):\n        self.state_count = 0\n    \n    def\
          \ new_state(self) -> NFAState:\n        state = NFAState(id=self.state_count)\n        self.state_count += 1\n \
          \       return state\n    \n    def build(self, node: Node) -> NFAFragment:\n        if isinstance(node, Literal):\n\
          \            start = self.new_state()\n            accept = self.new_state()\n            if node.char:\n      \
          \          start.transitions[node.char] = [accept]\n            else:\n                start.epsilon.append(accept)\n\
          \            return NFAFragment(start, accept)\n        \n        elif isinstance(node, Concat):\n            left\
          \ = self.build(node.left)\n            right = self.build(node.right)\n            left.accept.epsilon.append(right.start)\n\
          \            return NFAFragment(left.start, right.accept)\n        \n        elif isinstance(node, Alternation):\n\
          \            left = self.build(node.left)\n            right = self.build(node.right)\n            start = self.new_state()\n\
          \            accept = self.new_state()\n            start.epsilon.extend([left.start, right.start])\n          \
          \  left.accept.epsilon.append(accept)\n            right.accept.epsilon.append(accept)\n            return NFAFragment(start,\
          \ accept)\n        \n        elif isinstance(node, Star):\n            child = self.build(node.child)\n        \
          \    start = self.new_state()\n            accept = self.new_state()\n            start.epsilon.extend([child.start,\
          \ accept])\n            child.accept.epsilon.extend([child.start, accept])\n            return NFAFragment(start,\
          \ accept)\n        \n        elif isinstance(node, Plus):\n            child = self.build(node.child)\n        \
          \    start = self.new_state()\n            accept = self.new_state()\n            start.epsilon.append(child.start)\n\
          \            child.accept.epsilon.extend([child.start, accept])\n            return NFAFragment(start, accept)\n\
          \        \n        elif isinstance(node, Optional):\n            child = self.build(node.child)\n            start\
          \ = self.new_state()\n            accept = self.new_state()\n            start.epsilon.extend([child.start, accept])\n\
          \            child.accept.epsilon.append(accept)\n            return NFAFragment(start, accept)\n        \n    \
          \    elif isinstance(node, CharClass):\n            start = self.new_state()\n            accept = self.new_state()\n\
          \            # Add transitions for each char in class\n            for c in node.chars:\n                start.transitions.setdefault(c,\
          \ []).append(accept)\n            return NFAFragment(start, accept)\n\ndef compile_regex(pattern: str) -> NFAFragment:\n\
          \    parser = RegexParser(pattern)\n    ast = parser.parse()\n    builder = NFABuilder()\n    nfa = builder.build(ast)\n\
          \    nfa.accept.is_accept = True\n    return nfa\n```"
      pitfalls:
      - Wrong epsilon connections
      - Not marking accept state
      - Memory leaks from circular refs
      concepts:
      - NFAs
      - Thompson's construction
      - Epsilon transitions
      estimated_hours: 10-15
      deliverables:
      - NFA state representation with labeled transitions and epsilon moves
      - Epsilon transition edges connecting states without consuming input
      - AST-to-NFA construction building automaton from parsed regex tree
      - NFA fragment composition for concatenation, alternation, and quantifiers
    - id: 3
      name: NFA Simulation
      description: Match strings using NFA simulation.
      acceptance_criteria:
      - Epsilon closure correctly computes full set of reachable states from starting set
      - Simulation tracks all possible NFA states simultaneously during input processing
      - Match returns true only when final state is reachable after consuming entire input
      - Simulation handles patterns with nested quantifiers without exponential blowup
      hints:
        level1: Track set of current states. For each input char, compute next states.
        level2: 'Epsilon closure: follow all epsilon transitions from a state set.'
        level3: "## NFA Simulation\n\n```python\ndef epsilon_closure(states: Set[NFAState]) -> Set[NFAState]:\n    \"\"\"\
          Find all states reachable via epsilon transitions.\"\"\"\n    closure = set(states)\n    stack = list(states)\n\
          \    \n    while stack:\n        state = stack.pop()\n        for next_state in state.epsilon:\n            if next_state\
          \ not in closure:\n                closure.add(next_state)\n                stack.append(next_state)\n    \n   \
          \ return closure\n\ndef move(states: Set[NFAState], char: str) -> Set[NFAState]:\n    \"\"\"Find states reachable\
          \ via the given character.\"\"\"\n    next_states = set()\n    for state in states:\n        if char in state.transitions:\n\
          \            next_states.update(state.transitions[char])\n    return next_states\n\ndef nfa_match(nfa: NFAFragment,\
          \ text: str) -> bool:\n    \"\"\"Check if NFA matches the entire text.\"\"\"\n    current = epsilon_closure({nfa.start})\n\
          \    \n    for char in text:\n        current = epsilon_closure(move(current, char))\n        if not current:\n\
          \            return False\n    \n    return any(state.is_accept for state in current)\n\ndef nfa_search(nfa: NFAFragment,\
          \ text: str) -> Optional[tuple]:\n    \"\"\"Find first match in text, return (start, end) indices.\"\"\"\n    for\
          \ start in range(len(text)):\n        current = epsilon_closure({nfa.start})\n        \n        for end in range(start,\
          \ len(text) + 1):\n            if any(state.is_accept for state in current):\n                return (start, end)\n\
          \            \n            if end < len(text):\n                current = epsilon_closure(move(current, text[end]))\n\
          \                if not current:\n                    break\n    \n    return None\n\ndef nfa_findall(nfa: NFAFragment,\
          \ text: str) -> List[str]:\n    \"\"\"Find all non-overlapping matches.\"\"\"\n    matches = []\n    pos = 0\n \
          \   \n    while pos < len(text):\n        match = nfa_search_from(nfa, text, pos)\n        if match:\n         \
          \   start, end = match\n            matches.append(text[start:end])\n            pos = end if end > pos else pos\
          \ + 1\n        else:\n            pos += 1\n    \n    return matches\n```"
      pitfalls:
      - Forgetting epsilon closure
      - Infinite loop on empty match
      - Wrong match boundaries
      concepts:
      - NFA simulation
      - State sets
      - Pattern matching
      estimated_hours: 8-12
      deliverables:
      - Epsilon closure computation finding all states reachable via epsilon transitions
      - Simultaneous state tracking following all active NFA states in parallel
      - Match function accepting or rejecting input strings against NFA
      - Step-by-step simulation advancing state set on each input character
    - id: 4
      name: DFA Conversion & Optimization
      description: Convert NFA to DFA for faster matching.
      acceptance_criteria:
      - Subset construction produces DFA accepting exactly the same language as input NFA
      - DFA minimization reduces state count without changing accepted language
      - DFA execution processes each input character in constant time with single transition
      - Benchmarks demonstrate DFA is faster than NFA simulation for repeated matching
      hints:
        level1: Each DFA state = set of NFA states. Build on demand (lazy).
        level2: Minimize DFA by merging equivalent states (Hopcroft's algorithm).
        level3: "## NFA to DFA Conversion\n\n```python\nclass DFAState:\n    def __init__(self, nfa_states: frozenset):\n\
          \        self.nfa_states = nfa_states\n        self.transitions: Dict[str, 'DFAState'] = {}\n        self.is_accept\
          \ = any(s.is_accept for s in nfa_states)\n\nclass DFA:\n    def __init__(self, nfa: NFAFragment):\n        self.start_nfa_states\
          \ = frozenset(epsilon_closure({nfa.start}))\n        self.states: Dict[frozenset, DFAState] = {}\n        self.start\
          \ = self.get_or_create_state(self.start_nfa_states)\n    \n    def get_or_create_state(self, nfa_states: frozenset)\
          \ -> DFAState:\n        if nfa_states not in self.states:\n            self.states[nfa_states] = DFAState(nfa_states)\n\
          \        return self.states[nfa_states]\n    \n    def get_transition(self, state: DFAState, char: str) -> Optional[DFAState]:\n\
          \        if char in state.transitions:\n            return state.transitions[char]\n        \n        # Lazy construction\n\
          \        next_nfa_states = frozenset(epsilon_closure(move(state.nfa_states, char)))\n        if not next_nfa_states:\n\
          \            return None\n        \n        next_state = self.get_or_create_state(next_nfa_states)\n        state.transitions[char]\
          \ = next_state\n        return next_state\n    \n    def match(self, text: str) -> bool:\n        current = self.start\n\
          \        for char in text:\n            current = self.get_transition(current, char)\n            if current is\
          \ None:\n                return False\n        return current.is_accept\n\n# DFA Minimization (Hopcroft's algorithm)\n\
          def minimize_dfa(dfa: DFA) -> DFA:\n    # Partition states into accept and non-accept\n    accept = {s for s in\
          \ dfa.states.values() if s.is_accept}\n    non_accept = {s for s in dfa.states.values() if not s.is_accept}\n  \
          \  \n    partitions = [accept, non_accept] if non_accept else [accept]\n    worklist = list(partitions)\n    \n\
          \    alphabet = set()\n    for state in dfa.states.values():\n        alphabet.update(state.transitions.keys())\n\
          \    \n    while worklist:\n        A = worklist.pop()\n        for c in alphabet:\n            # States that transition\
          \ to A on c\n            X = {s for s in dfa.states.values() \n                 if s.transitions.get(c) in A}\n\
          \            \n            new_partitions = []\n            for Y in partitions:\n                intersection =\
          \ Y & X\n                difference = Y - X\n                \n                if intersection and difference:\n\
          \                    new_partitions.extend([intersection, difference])\n                    if Y in worklist:\n\
          \                        worklist.remove(Y)\n                        worklist.extend([intersection, difference])\n\
          \                    else:\n                        worklist.append(min(intersection, difference, key=len))\n  \
          \              else:\n                    new_partitions.append(Y)\n            \n            partitions = new_partitions\n\
          \    \n    # Build minimized DFA from partitions\n    # ...\n```"
      pitfalls:
      - Exponential state blowup
      - Not handling dead states
      - Alphabet enumeration
      concepts:
      - Subset construction
      - DFA minimization
      - Lazy evaluation
      estimated_hours: 12-18
      deliverables:
      - Subset construction algorithm converting NFA to equivalent DFA
      - DFA state minimization merging equivalent states to reduce size
      - Optimized DFA execution matching input in linear time per character
      - Performance comparison benchmarking NFA simulation versus DFA execution
  build-shell:
    id: build-shell
    name: Build Your Own Shell
    description: Build a Unix shell that can execute commands, handle pipes, redirections, and job control. A fundamental
      systems programming project.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - C programming
    - Unix process model (fork, exec)
    - File descriptors
    - Signal handling basics
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible: []
    resources:
    - name: Write a Shell in C
      url: https://brennan.io/2015/01/16/write-a-shell-in-c/
      type: blog
    - name: CodeCrafters Shell Challenge
      url: https://app.codecrafters.io/courses/shell/overview
      type: interactive
    - name: GNU Implementing a Shell
      url: https://www.gnu.org/software/libc/manual/html_node/Implementing-a-Shell.html
      type: documentation
    milestones:
    - id: 1
      name: Basic REPL and Command Execution
      description: Read input, parse into command and arguments, execute with fork/exec.
      acceptance_criteria:
      - Shell displays prompt and waits for user input on each iteration
      - Tokenizer correctly splits command line respecting quoted strings
      - External commands are found in PATH and executed in child process
      - Shell reports non-zero exit codes when child process fails
      - Empty input lines are handled gracefully without errors
      hints:
        level1: Use fgets() to read input, strtok() to split by spaces.
        level2: fork() returns 0 in child. In child, call execvp(cmd, args). Parent calls waitpid().
        level3: |-
          Basic loop:
          while (1) {
            printf("> ");
            fgets(line, sizeof(line), stdin);
            char *args[] = parse(line);
            pid_t pid = fork();
            if (pid == 0) execvp(args[0], args);
            else waitpid(pid, &status, 0);
          }
      pitfalls:
      - Forgetting to null-terminate args array
      - Not handling newline from fgets()
      - Zombie processes
      concepts:
      - REPL pattern
      - fork() and exec() family
      - Process creation and waiting
      estimated_hours: 3-4
      deliverables:
      - Read-eval-print loop displaying prompt and reading user input
      - Command line tokenizer splitting input into command and arguments
      - Process spawning executing external commands with fork and exec
      - Exit status collection using waitpid and reporting to user
    - id: 2
      name: Built-in Commands
      description: Implement cd, exit, pwd, export, and other built-in commands.
      acceptance_criteria:
      - cd changes directory and updates PWD environment variable accordingly
      - cd with no arguments changes to HOME directory by default
      - exit terminates shell returning specified or zero exit code to parent
      - export sets environment variables visible to subsequently spawned child processes
      - echo prints its arguments separated by spaces followed by newline
      hints:
        level1: Built-ins don't fork - they modify shell state directly.
        level2: Use chdir() for cd, getcwd() for pwd, setenv() for export.
        level3: |-
          # Shell built-in commands implementation
          import os
          import sys

          class Shell:
              def __init__(self):
                  self.builtins = {
                      'cd': self.builtin_cd,
                      'pwd': self.builtin_pwd,
                      'export': self.builtin_export,
                      'exit': self.builtin_exit,
                      'echo': self.builtin_echo,
                      'type': self.builtin_type,
                      'history': self.builtin_history,
                      'alias': self.builtin_alias,
                  }
                  self.aliases = {}
                  self.history = []
                  self.env = dict(os.environ)

              def builtin_cd(self, args):
                  if not args:
                      path = self.env.get('HOME', '/')
                  elif args[0] == '-':
                      path = self.env.get('OLDPWD', '.')
                  elif args[0].startswith('~'):
                      path = os.path.expanduser(args[0])
                  else:
                      path = args[0]

                  try:
                      old_pwd = os.getcwd()
                      os.chdir(path)
                      self.env['OLDPWD'] = old_pwd
                      self.env['PWD'] = os.getcwd()
                      return 0
                  except FileNotFoundError:
                      print(f"cd: {path}: No such file or directory", file=sys.stderr)
                      return 1

              def builtin_export(self, args):
                  for arg in args:
                      if '=' in arg:
                          name, value = arg.split('=', 1)
                          self.env[name] = value
                          os.environ[name] = value
                      else:
                          # Export existing variable
                          if arg in self.env:
                              os.environ[arg] = self.env[arg]
                  return 0

              def builtin_type(self, args):
                  for name in args:
                      if name in self.builtins:
                          print(f"{name} is a shell builtin")
                      elif name in self.aliases:
                          print(f"{name} is aliased to `{self.aliases[name]}'")
                      else:
                          path = self.find_executable(name)
                          if path:
                              print(f"{name} is {path}")
                          else:
                              print(f"{name}: not found")
                              return 1
                  return 0

              def find_executable(self, name):
                  for dir in self.env.get('PATH', '').split(':'):
                      path = os.path.join(dir, name)
                      if os.path.isfile(path) and os.access(path, os.X_OK):
                          return path
                  return None
      pitfalls:
      - Trying to cd in child process (only affects child)
      - Not expanding ~ to HOME
      - Environment variables not inherited
      concepts:
      - Built-in vs external commands
      - Working directory
      - Environment variables
      estimated_hours: 2-3
      deliverables:
      - cd command changing current working directory with error handling
      - exit command terminating shell with optional exit code parameter
      - export command setting environment variables for child processes
      - echo command printing arguments with variable expansion support
    - id: 3
      name: I/O Redirection
      description: Implement input (<), output (>), and append (>>) redirection.
      acceptance_criteria:
      - Output redirect > creates or truncates file and writes command stdout to it
      - Input redirect < opens file and connects it to command stdin
      - Append redirect >> creates or appends to file without truncating existing content
      - Error redirect 2> writes command stderr to specified file separately from stdout
      - Multiple redirections can be combined in a single command line
      hints:
        level1: Use open() to get file descriptor, dup2() to redirect.
        level2: dup2(fd, STDOUT_FILENO) makes stdout point to fd. Do this in child before exec.
        level3: |-
          I/O redirection implementation in C:

          #include <fcntl.h>
          #include <unistd.h>
          #include <string.h>

          typedef struct {
              char *cmd;
              char **args;
              char *stdin_file;   // < redirect
              char *stdout_file;  // > redirect
              char *stderr_file;  // 2> redirect
              int append_stdout;  // >> flag
          } Command;

          void setup_redirections(Command *cmd) {
              // Input redirection: < file
              if (cmd->stdin_file) {
                  int fd = open(cmd->stdin_file, O_RDONLY);
                  if (fd < 0) {
                      perror(cmd->stdin_file);
                      exit(1);
                  }
                  dup2(fd, STDIN_FILENO);
                  close(fd);
              }

              // Output redirection: > file or >> file
              if (cmd->stdout_file) {
                  int flags = O_WRONLY | O_CREAT;
                  flags |= cmd->append_stdout ? O_APPEND : O_TRUNC;
                  int fd = open(cmd->stdout_file, flags, 0644);
                  if (fd < 0) {
                      perror(cmd->stdout_file);
                      exit(1);
                  }
                  dup2(fd, STDOUT_FILENO);
                  close(fd);
              }

              // Error redirection: 2> file
              if (cmd->stderr_file) {
                  int fd = open(cmd->stderr_file, O_WRONLY | O_CREAT | O_TRUNC, 0644);
                  if (fd < 0) {
                      perror(cmd->stderr_file);
                      exit(1);
                  }
                  dup2(fd, STDERR_FILENO);
                  close(fd);
              }
          }

          void execute_with_redirection(Command *cmd) {
              pid_t pid = fork();
              if (pid == 0) {
                  setup_redirections(cmd);
                  execvp(cmd->cmd, cmd->args);
                  perror(cmd->cmd);
                  exit(127);
              }
              int status;
              waitpid(pid, &status, 0);
          }
      pitfalls:
      - Forgetting to close original fd after dup2
      - Wrong open() flags
      - File permissions on created files
      concepts:
      - File descriptors
      - dup2() system call
      - Unix I/O model
      estimated_hours: 2-3
      deliverables:
      - Output redirection writing command stdout to specified file
      - Input redirection reading command stdin from specified file
      - Append redirection adding command stdout to end of existing file
      - Error redirection writing command stderr to specified file
    - id: 4
      name: Pipes
      description: Implement command pipelines (cmd1 | cmd2 | cmd3).
      acceptance_criteria:
      - Pipe operator connects stdout of first command to stdin of second command
      - Multi-stage pipelines chain three or more commands passing data through each
      - All processes in pipeline run concurrently without deadlocking on full pipe buffers
      - Pipeline exit status reflects the exit code of the last command in the chain
      hints:
        level1: pipe() creates fd pair. pipe[0] for reading, pipe[1] for writing.
        level2: Fork for each command. Connect stdout of cmd1 to stdin of cmd2 via pipe.
        level3: |-
          For cmd1 | cmd2:
          int pipefd[2];
          pipe(pipefd);
          if (fork() == 0) {  // cmd1
            dup2(pipefd[1], STDOUT_FILENO);
            close(pipefd[0]); close(pipefd[1]);
            exec(cmd1);
          }
          if (fork() == 0) {  // cmd2
            dup2(pipefd[0], STDIN_FILENO);
            close(pipefd[0]); close(pipefd[1]);
            exec(cmd2);
          }
          close(pipefd[0]); close(pipefd[1]);
          wait(); wait();
      pitfalls:
      - Not closing unused pipe ends (causes hang)
      - Parent not closing pipe fds
      - Order of fork/close operations
      concepts:
      - Unix pipes
      - Process communication
      - File descriptor inheritance
      estimated_hours: 4-6
      deliverables:
      - Pipe creation connecting stdout of left command to stdin of right command
      - Pipeline execution chaining multiple commands with pipe operators
      - Process group management for pipeline child processes
      - Pipeline exit status returning status of last command in chain
    - id: 5
      name: Background Jobs
      description: Run commands in background with &, implement job listing.
      acceptance_criteria:
      - Trailing & launches command in background and immediately shows next prompt
      - Job table stores PID, command string, and running status for each background job
      - jobs command displays job number, status, and command for all background processes
      - Completed background jobs are reported to user at next prompt display
      hints:
        level1: Don't waitpid() immediately for background jobs. Store PID in job list.
        level2: Use SIGCHLD handler to detect when background jobs finish.
        level3: |-
          # Job control for background processes
          import os
          import signal
          from enum import Enum

          class JobState(Enum):
              RUNNING = "Running"
              STOPPED = "Stopped"
              DONE = "Done"

          class Job:
              def __init__(self, job_id, pgid, command, processes):
                  self.id = job_id
                  self.pgid = pgid  # Process group ID
                  self.command = command
                  self.processes = processes  # List of PIDs
                  self.state = JobState.RUNNING

              def __str__(self):
                  return f"[{self.id}]  {self.state.value}  {self.command}"

          class JobController:
              def __init__(self):
                  self.jobs = {}
                  self.next_id = 1
                  self.foreground_pgid = None
                  signal.signal(signal.SIGCHLD, self.sigchld_handler)

              def launch_job(self, command, background=False):
                  pid = os.fork()
                  if pid == 0:
                      # Child: create new process group
                      os.setpgid(0, 0)
                      if not background:
                          # Give terminal to foreground job
                          os.tcsetpgrp(0, os.getpgrp())
                      signal.signal(signal.SIGINT, signal.SIG_DFL)
                      signal.signal(signal.SIGTSTP, signal.SIG_DFL)
                      os.execvp(command[0], command)
                  else:
                      # Parent
                      pgid = pid
                      os.setpgid(pid, pgid)
                      job = Job(self.next_id, pgid, ' '.join(command), [pid])
                      self.jobs[self.next_id] = job
                      self.next_id += 1

                      if background:
                          print(f"[{job.id}] {pid}")
                      else:
                          self.foreground_pgid = pgid
                          os.tcsetpgrp(0, pgid)
                          self.wait_for_job(job)
                          os.tcsetpgrp(0, os.getpgrp())

              def wait_for_job(self, job):
                  while job.state == JobState.RUNNING:
                      pid, status = os.waitpid(-job.pgid, os.WUNTRACED)
                      if os.WIFSTOPPED(status):
                          job.state = JobState.STOPPED
                          print(f"\n{job}")
                      elif os.WIFEXITED(status) or os.WIFSIGNALED(status):
                          job.state = JobState.DONE

              def builtin_fg(self, args):
                  job_id = int(args[0]) if args else max(self.jobs.keys())
                  job = self.jobs.get(job_id)
                  if job:
                      print(job.command)
                      os.killpg(job.pgid, signal.SIGCONT)
                      job.state = JobState.RUNNING
                      os.tcsetpgrp(0, job.pgid)
                      self.wait_for_job(job)
                      os.tcsetpgrp(0, os.getpgrp())

              def builtin_bg(self, args):
                  job_id = int(args[0]) if args else max(self.jobs.keys())
                  job = self.jobs.get(job_id)
                  if job and job.state == JobState.STOPPED:
                      os.killpg(job.pgid, signal.SIGCONT)
                      job.state = JobState.RUNNING
                      print(f"[{job.id}]+ {job.command} &")
      pitfalls:
      - Zombie processes from background jobs
      - Race condition in SIGCHLD handler
      - Reaping wrong child
      concepts:
      - Background execution
      - Asynchronous process management
      - Zombie processes
      estimated_hours: 3-4
      deliverables:
      - Background execution launching commands with trailing ampersand operator
      - Job table tracking background process IDs and status
      - jobs command listing all active background processes with status
      - wait command blocking until specified background job completes
    - id: 6
      name: Job Control (fg, bg, Ctrl+Z)
      description: Full job control with suspend, resume, foreground/background.
      acceptance_criteria:
      - Ctrl+Z sends SIGTSTP to foreground process and returns shell prompt
      - fg resumes specified job in foreground and waits for its completion
      - bg sends SIGCONT to suspended job allowing it to continue in background
      - Signal forwarding delivers SIGINT and SIGTSTP to correct foreground process group
      hints:
        level1: Handle SIGTSTP (Ctrl+Z). Suspended jobs need to be resumed with SIGCONT.
        level2: Use setpgid() to put job in its own process group. Use tcsetpgrp() for foreground.
        level3: |-
          Job control with fg, bg, and signal handling in C:

          #include <signal.h>
          #include <sys/wait.h>
          #include <termios.h>

          typedef struct Job {
              int id;
              pid_t pgid;
              char *command;
              enum { RUNNING, STOPPED, DONE } state;
              struct Job *next;
          } Job;

          Job *jobs = NULL;
          pid_t shell_pgid;
          int shell_terminal;

          void init_shell() {
              shell_terminal = STDIN_FILENO;
              shell_pgid = getpid();

              // Put shell in its own process group
              setpgid(shell_pgid, shell_pgid);
              tcsetpgrp(shell_terminal, shell_pgid);

              // Ignore interactive signals in shell
              signal(SIGTSTP, SIG_IGN);
              signal(SIGINT, SIG_IGN);
              signal(SIGTTOU, SIG_IGN);
          }

          void launch_job(char **args, int foreground) {
              pid_t pid = fork();

              if (pid == 0) {
                  // Child: restore default signal handlers
                  signal(SIGTSTP, SIG_DFL);
                  signal(SIGINT, SIG_DFL);

                  // Create new process group
                  setpgid(0, 0);

                  if (foreground)
                      tcsetpgrp(shell_terminal, getpid());

                  execvp(args[0], args);
                  exit(127);
              }

              // Parent: add to job list
              setpgid(pid, pid);
              Job *job = add_job(pid, args[0]);

              if (foreground) {
                  put_job_foreground(job, 0);
              } else {
                  printf("[%d] %d\n", job->id, pid);
              }
          }

          void put_job_foreground(Job *job, int cont) {
              tcsetpgrp(shell_terminal, job->pgid);

              if (cont) {
                  kill(-job->pgid, SIGCONT);
                  job->state = RUNNING;
              }

              int status;
              waitpid(-job->pgid, &status, WUNTRACED);

              tcsetpgrp(shell_terminal, shell_pgid);

              if (WIFSTOPPED(status)) {
                  job->state = STOPPED;
                  printf("\n[%d]+ Stopped\t%s\n", job->id, job->command);
              }
          }

          void builtin_bg(int job_id) {
              Job *job = find_job(job_id);
              if (job && job->state == STOPPED) {
                  kill(-job->pgid, SIGCONT);
                  job->state = RUNNING;
                  printf("[%d]+ %s &\n", job->id, job->command);
              }
          }
      pitfalls:
      - Terminal control group confusion
      - Orphaned process groups
      - Signal handling race conditions
      concepts:
      - Process groups
      - Terminal control
      - Signal handling (SIGTSTP, SIGCONT)
      estimated_hours: 5-8
      deliverables:
      - SIGTSTP handling suspending foreground process on Ctrl+Z input
      - fg command resuming suspended or background job in foreground
      - bg command resuming suspended job to continue running in background
      - Signal forwarding delivering terminal signals to foreground process group
  build-spreadsheet:
    id: build-spreadsheet
    name: Build Your Own Spreadsheet
    description: Build an Excel-like spreadsheet application with formulas, cell references, and recalculation.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - DOM manipulation
    - Graph algorithms
    - Expression parsing
    - Event handling
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible:
      - Python
      - Rust
    resources:
    - type: article
      name: Building a Spreadsheet Engine
      url: https://leanrada.com/notes/spreadsheet-engine/
    - type: video
      name: How Excel Recalculates
      url: https://www.youtube.com/watch?v=R0hhDgvWbUU
    milestones:
    - id: 1
      name: Grid & Cell Rendering
      description: Build the spreadsheet grid with editable cells.
      acceptance_criteria:
      - Grid renders at least 26 columns and 100 rows with scrollable viewport
      - Double-clicking a cell enters edit mode with cursor in inline text input
      - Selected cell is visually distinct with highlighted border or background color
      - Scrolling pans the grid while keeping row and column headers visible
      hints:
        level1: Use a virtualized grid - only render visible cells plus buffer.
        level2: Track selection state, handle keyboard navigation (arrow keys, Tab, Enter).
        level3: "## Grid Implementation\n\n```javascript\nclass SpreadsheetGrid {\n  constructor(container, rows = 1000, cols\
          \ = 26) {\n    this.rows = rows;\n    this.cols = cols;\n    this.cellData = new Map(); // 'A1' -> { value, formula,\
          \ formatted }\n    this.selection = { row: 0, col: 0 };\n    this.viewportStart = { row: 0, col: 0 };\n    \n  \
          \  this.rowHeight = 24;\n    this.defaultColWidth = 100;\n    this.colWidths = new Map();\n    \n    this.setupDOM(container);\n\
          \    this.render();\n  }\n  \n  getCellId(row, col) {\n    return String.fromCharCode(65 + col) + (row + 1);\n \
          \ }\n  \n  parseCellId(id) {\n    const match = id.match(/^([A-Z]+)(\\d+)$/);\n    if (!match) return null;\n  \
          \  const col = match[1].split('').reduce((acc, c) => acc * 26 + c.charCodeAt(0) - 64, 0) - 1;\n    const row = parseInt(match[2])\
          \ - 1;\n    return { row, col };\n  }\n  \n  render() {\n    const visibleRows = Math.ceil(this.container.clientHeight\
          \ / this.rowHeight) + 2;\n    const visibleCols = this.getVisibleCols();\n    \n    // Only render visible cells\n\
          \    this.gridBody.innerHTML = '';\n    for (let r = this.viewportStart.row; r < this.viewportStart.row + visibleRows;\
          \ r++) {\n      const rowEl = document.createElement('div');\n      rowEl.className = 'row';\n      rowEl.style.top\
          \ = r * this.rowHeight + 'px';\n      \n      for (const col of visibleCols) {\n        const cell = this.createCell(r,\
          \ col);\n        rowEl.appendChild(cell);\n      }\n      this.gridBody.appendChild(rowEl);\n    }\n  }\n  \n  handleKeydown(e)\
          \ {\n    switch(e.key) {\n      case 'ArrowUp': this.moveSelection(-1, 0); break;\n      case 'ArrowDown': this.moveSelection(1,\
          \ 0); break;\n      case 'ArrowLeft': this.moveSelection(0, -1); break;\n      case 'ArrowRight': this.moveSelection(0,\
          \ 1); break;\n      case 'Tab': this.moveSelection(0, e.shiftKey ? -1 : 1); e.preventDefault(); break;\n      case\
          \ 'Enter': this.startEditing(); break;\n    }\n  }\n}\n```"
      pitfalls:
      - Rendering too many cells
      - Slow scrolling
      - Focus management issues
      concepts:
      - Virtual scrolling
      - DOM optimization
      - Keyboard navigation
      estimated_hours: 10-15
      deliverables:
      - Scrollable grid rendering cells in rows and columns layout
      - Cell editing UI with inline text input on double-click activation
      - Selection highlighting showing currently active cell with border
      - Column and row headers displaying A-Z labels and row numbers
    - id: 2
      name: Formula Parser
      description: Parse and evaluate spreadsheet formulas.
      acceptance_criteria:
      - Formulas starting with = are parsed into evaluable expression trees
      - Cell references like A1 and B2 resolve to their current numeric values
      - Arithmetic operators +, -, *, / evaluate with correct precedence and associativity
      - SUM(A1:A10) correctly sums all values in the specified cell range
      hints:
        level1: Use recursive descent parsing. Formulas start with '='.
        level2: Build an AST, then evaluate. Handle operator precedence.
        level3: "## Formula Parser\n\n```javascript\nclass FormulaParser {\n  constructor(getCellValue) {\n    this.getCellValue\
          \ = getCellValue;\n  }\n  \n  parse(formula) {\n    this.tokens = this.tokenize(formula.substring(1)); // Remove\
          \ '='\n    this.pos = 0;\n    return this.parseExpression();\n  }\n  \n  tokenize(str) {\n    const tokens = [];\n\
          \    const regex = /([A-Z]+\\d+:[A-Z]+\\d+|[A-Z]+\\d+|\\d+\\.?\\d*|[+\\-*/^(),]|[A-Z]+(?=\\()|\"[^\"]*\")/gi;\n\
          \    let match;\n    while ((match = regex.exec(str)) !== null) {\n      tokens.push(match[0]);\n    }\n    return\
          \ tokens;\n  }\n  \n  parseExpression() {\n    let left = this.parseTerm();\n    while (this.current() === '+' ||\
          \ this.current() === '-') {\n      const op = this.consume();\n      const right = this.parseTerm();\n      left\
          \ = { type: 'binary', op, left, right };\n    }\n    return left;\n  }\n  \n  parseTerm() {\n    let left = this.parseFactor();\n\
          \    while (this.current() === '*' || this.current() === '/') {\n      const op = this.consume();\n      const right\
          \ = this.parseFactor();\n      left = { type: 'binary', op, left, right };\n    }\n    return left;\n  }\n  \n \
          \ parseFactor() {\n    const token = this.current();\n    \n    if (token === '(') {\n      this.consume();\n  \
          \    const expr = this.parseExpression();\n      this.consume(); // ')'\n      return expr;\n    }\n    \n    if\
          \ (/^[A-Z]+$/i.test(token) && this.peek() === '(') {\n      return this.parseFunction();\n    }\n    \n    if (/^[A-Z]+\\\
          d+:[A-Z]+\\d+$/i.test(token)) {\n      this.consume();\n      return { type: 'range', range: token };\n    }\n \
          \   \n    if (/^[A-Z]+\\d+$/i.test(token)) {\n      this.consume();\n      return { type: 'cell', ref: token };\n\
          \    }\n    \n    if (/^\\d/.test(token)) {\n      this.consume();\n      return { type: 'number', value: parseFloat(token)\
          \ };\n    }\n    \n    throw new Error(`Unexpected token: ${token}`);\n  }\n  \n  parseFunction() {\n    const name\
          \ = this.consume();\n    this.consume(); // '('\n    const args = [];\n    while (this.current() !== ')') {\n  \
          \    args.push(this.parseExpression());\n      if (this.current() === ',') this.consume();\n    }\n    this.consume();\
          \ // ')'\n    return { type: 'function', name: name.toUpperCase(), args };\n  }\n}\n```"
      pitfalls:
      - Operator precedence errors
      - Not handling negative numbers
      - String vs number coercion
      concepts:
      - Lexical analysis
      - Recursive descent parsing
      - AST construction
      estimated_hours: 12-18
      deliverables:
      - Expression tokenizer splitting formulas into operators, numbers, and cell references
      - Recursive descent parser building expression tree from tokenized formula
      - Cell reference resolver fetching values from referenced cell addresses
      - Built-in function library supporting SUM, AVG, MIN, MAX, and COUNT
    - id: 3
      name: Dependency Graph & Recalculation
      description: Track cell dependencies and recalculate in correct order.
      acceptance_criteria:
      - Changing a cell value triggers recalculation of all dependent cells automatically
      - Topological sort ensures cells are recalculated after their dependencies update
      - Circular reference is detected and displays error instead of hanging indefinitely
      - Only cells directly or transitively depending on changed cell are recalculated
      hints:
        level1: When a cell is edited, find all cells that depend on it and recalculate.
        level2: Build a dependency graph. Use topological sort to determine recalc order.
        level3: "## Dependency Graph\n\n```javascript\nclass DependencyGraph {\n  constructor() {\n    this.dependencies =\
          \ new Map();  // cell -> Set of cells it depends on\n    this.dependents = new Map();    // cell -> Set of cells\
          \ that depend on it\n  }\n  \n  setDependencies(cell, deps) {\n    // Remove old dependencies\n    const oldDeps\
          \ = this.dependencies.get(cell) || new Set();\n    for (const dep of oldDeps) {\n      this.dependents.get(dep)?.delete(cell);\n\
          \    }\n    \n    // Set new dependencies\n    this.dependencies.set(cell, new Set(deps));\n    for (const dep of\
          \ deps) {\n      if (!this.dependents.has(dep)) {\n        this.dependents.set(dep, new Set());\n      }\n     \
          \ this.dependents.get(dep).add(cell);\n    }\n  }\n  \n  getCellsToRecalculate(changedCell) {\n    // Get all cells\
          \ affected by this change\n    const affected = new Set();\n    const queue = [changedCell];\n    \n    while (queue.length\
          \ > 0) {\n      const cell = queue.shift();\n      const deps = this.dependents.get(cell) || new Set();\n      for\
          \ (const dep of deps) {\n        if (!affected.has(dep)) {\n          affected.add(dep);\n          queue.push(dep);\n\
          \        }\n      }\n    }\n    \n    // Topological sort\n    return this.topologicalSort(affected);\n  }\n  \n\
          \  topologicalSort(cells) {\n    const visited = new Set();\n    const result = [];\n    \n    const visit = (cell)\
          \ => {\n      if (visited.has(cell)) return;\n      visited.add(cell);\n      \n      const deps = this.dependencies.get(cell)\
          \ || new Set();\n      for (const dep of deps) {\n        if (cells.has(dep)) visit(dep);\n      }\n      result.push(cell);\n\
          \    };\n    \n    for (const cell of cells) {\n      visit(cell);\n    }\n    \n    return result;\n  }\n  \n \
          \ detectCircular(cell, newDeps) {\n    // DFS to check if adding these deps would create a cycle\n    const visited\
          \ = new Set();\n    const stack = [...newDeps];\n    \n    while (stack.length > 0) {\n      const current = stack.pop();\n\
          \      if (current === cell) return true;  // Circular!\n      if (visited.has(current)) continue;\n      visited.add(current);\n\
          \      \n      const deps = this.dependencies.get(current) || new Set();\n      for (const dep of deps) {\n    \
          \    stack.push(dep);\n      }\n    }\n    return false;\n  }\n}\n```"
      pitfalls:
      - Infinite loops from circular refs
      - Recalculating too much
      - Wrong recalc order
      concepts:
      - Directed graphs
      - Topological sorting
      - Cycle detection
      estimated_hours: 12-18
      deliverables:
      - Cell dependency tracking recording which cells reference which others
      - Topological sort determining correct recalculation order for dependent cells
      - Circular reference detection preventing infinite recalculation loops
      - Incremental recalculation updating only affected cells when source changes
    - id: 4
      name: Advanced Features
      description: Add formatting, copy/paste, and undo/redo.
      acceptance_criteria:
      - Pasting a formula adjusts relative cell references to match new position
      - Undo reverts the most recent cell change and redo reapplies it correctly
      - CSV export produces valid comma-separated file openable by other spreadsheet programs
      - Number formatting displays cells as currency, percentage, or fixed decimal places
      hints:
        level1: For copy/paste, adjust relative cell references based on destination.
        level2: Use command pattern for undo/redo. Each action is a command that can be reversed.
        level3: "## Copy/Paste & Undo\n\n```javascript\n// Adjust cell references when copying\nfunction adjustFormula(formula,\
          \ rowDelta, colDelta) {\n  return formula.replace(/([A-Z]+)(\\d+)/g, (match, col, row) => {\n    // Check if reference\
          \ is absolute ($A$1)\n    const newCol = String.fromCharCode(col.charCodeAt(0) + colDelta);\n    const newRow =\
          \ parseInt(row) + rowDelta;\n    return newCol + newRow;\n  });\n}\n\n// Command pattern for undo/redo\nclass Command\
          \ {\n  execute() { throw new Error('Not implemented'); }\n  undo() { throw new Error('Not implemented'); }\n}\n\n\
          class SetCellCommand extends Command {\n  constructor(spreadsheet, cellId, newValue) {\n    super();\n    this.spreadsheet\
          \ = spreadsheet;\n    this.cellId = cellId;\n    this.newValue = newValue;\n    this.oldValue = spreadsheet.getRawValue(cellId);\n\
          \  }\n  \n  execute() {\n    this.spreadsheet.setCellValue(this.cellId, this.newValue, false);\n  }\n  \n  undo()\
          \ {\n    this.spreadsheet.setCellValue(this.cellId, this.oldValue, false);\n  }\n}\n\nclass UndoManager {\n  constructor()\
          \ {\n    this.undoStack = [];\n    this.redoStack = [];\n  }\n  \n  execute(command) {\n    command.execute();\n\
          \    this.undoStack.push(command);\n    this.redoStack = [];  // Clear redo stack\n  }\n  \n  undo() {\n    if (this.undoStack.length\
          \ === 0) return;\n    const command = this.undoStack.pop();\n    command.undo();\n    this.redoStack.push(command);\n\
          \  }\n  \n  redo() {\n    if (this.redoStack.length === 0) return;\n    const command = this.redoStack.pop();\n\
          \    command.execute();\n    this.undoStack.push(command);\n  }\n}\n```"
      pitfalls:
      - Memory leaks in undo stack
      - Not handling absolute references ($A$1)
      - Paste overwriting formulas
      concepts:
      - Command pattern
      - Reference adjustment
      - State management
      estimated_hours: 15-20
      deliverables:
      - Copy and paste support duplicating cell values and formulas with reference adjustment
      - Undo and redo stack tracking cell modifications for reversal
      - CSV import and export converting between spreadsheet data and file format
      - Cell formatting options for number display, alignment, and font style
  build-sqlite:
    id: build-sqlite
    name: Build Your Own SQLite
    description: Build an embedded SQL database that stores data in a single file. You'll learn about SQL parsing, B-trees,
      query execution, and transactions.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - B-tree data structure
    - SQL basics
    - File I/O and binary formats
    - Basic compiler concepts
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
      - Java
    resources:
    - name: Let's Build a Simple Database
      url: https://cstack.github.io/db_tutorial/
      type: tutorial
    - name: CodeCrafters SQLite Challenge
      url: https://app.codecrafters.io/courses/sqlite/overview
      type: interactive
    - name: SQLite File Format
      url: https://www.sqlite.org/fileformat2.html
      type: documentation
    - name: CMU 15-445 Database Systems
      url: https://15445.courses.cs.cmu.edu
      type: course
    milestones:
    - id: 1
      name: SQL Tokenizer
      description: Build a lexer that converts SQL text into tokens.
      acceptance_criteria:
      - Tokenizer correctly identifies SQL keywords regardless of letter casing
      - String literals enclosed in single quotes are parsed including escape sequences
      - Numeric literals including integers and floating point values are recognized
      - Operators like =, <, >, !=, AND, OR are tokenized as distinct operator tokens
      hints:
        level1: Use a state machine or regular expressions for each token type.
        level2: SQL keywords are case-insensitive. Normalize to uppercase.
        level3: |-
          SQL tokenizer in Python:

          from enum import Enum, auto
          from dataclasses import dataclass
          from typing import List

          class TokenType(Enum):
              SELECT = auto()
              INSERT = auto()
              UPDATE = auto()
              DELETE = auto()
              FROM = auto()
              WHERE = auto()
              AND = auto()
              OR = auto()
              CREATE = auto()
              TABLE = auto()
              IDENTIFIER = auto()
              STRING = auto()
              NUMBER = auto()
              OPERATOR = auto()
              LPAREN = auto()
              RPAREN = auto()
              COMMA = auto()
              SEMICOLON = auto()
              STAR = auto()
              EOF = auto()

          @dataclass
          class Token:
              type: TokenType
              value: str
              line: int
              column: int

          KEYWORDS = {
              'SELECT', 'INSERT', 'UPDATE', 'DELETE', 'FROM', 'WHERE',
              'AND', 'OR', 'CREATE', 'TABLE', 'INTO', 'VALUES', 'SET'
          }

          class SQLTokenizer:
              def __init__(self, sql: str):
                  self.sql = sql
                  self.pos = 0
                  self.line = 1
                  self.column = 1

              def tokenize(self) -> List[Token]:
                  tokens = []
                  while self.pos < len(self.sql):
                      self.skip_whitespace()
                      if self.pos >= len(self.sql):
                          break

                      ch = self.sql[self.pos]

                      if ch.isalpha() or ch == '_':
                          tokens.append(self.read_identifier())
                      elif ch.isdigit():
                          tokens.append(self.read_number())
                      elif ch == "'":
                          tokens.append(self.read_string())
                      elif ch in '=<>!':
                          tokens.append(self.read_operator())
                      elif ch in '(),;*':
                          tokens.append(self.read_punctuation())
                      else:
                          self.pos += 1

                  tokens.append(Token(TokenType.EOF, '', self.line, self.column))
                  return tokens

              def read_identifier(self):
                  start = self.pos
                  while self.pos < len(self.sql) and (self.sql[self.pos].isalnum() or self.sql[self.pos] == '_'):
                      self.pos += 1
                  value = self.sql[start:self.pos].upper()
                  token_type = TokenType[value] if value in KEYWORDS else TokenType.IDENTIFIER
                  return Token(token_type, self.sql[start:self.pos], self.line, self.column)

              def read_string(self):
                  self.pos += 1  # Skip opening quote
                  start = self.pos
                  while self.pos < len(self.sql) and self.sql[self.pos] != "'":
                      if self.sql[self.pos:self.pos+2] == "''":  # Escaped quote
                          self.pos += 2
                      else:
                          self.pos += 1
                  value = self.sql[start:self.pos].replace("''", "'")
                  self.pos += 1  # Skip closing quote
                  return Token(TokenType.STRING, value, self.line, self.column)
      pitfalls:
      - Not handling escaped quotes in strings ('it''s')
      - Case sensitivity
      - Unicode identifiers
      concepts:
      - Lexical analysis
      - Finite state machines
      - Token representation
      estimated_hours: 3-4
      deliverables:
      - Lexer splitting SQL input into keyword, identifier, and literal tokens
      - Keyword recognition for SELECT, INSERT, CREATE, WHERE, and other SQL words
      - String and number literal parsing with proper escape handling
      - Operator and punctuation tokenization for comparison and grouping symbols
    - id: 2
      name: SQL Parser (AST)
      description: Build a parser that converts tokens into an Abstract Syntax Tree.
      acceptance_criteria:
      - SELECT parser produces AST node with column list, FROM table, and optional WHERE
      - INSERT parser produces AST with target table, column names, and value expressions
      - CREATE TABLE parser extracts column definitions with types and constraint annotations
      - Expression parser correctly handles operator precedence and parenthesized groups
      hints:
        level1: Use recursive descent parsing - one function per grammar rule.
        level2: For expressions, use Pratt parsing or precedence climbing for operators.
        level3: |-
          SQL parser producing AST in Python:

          from dataclasses import dataclass
          from typing import List, Optional, Any

          @dataclass
          class SelectStmt:
              columns: List[str]
              table: str
              where: Optional['Expr'] = None
              order_by: Optional[List[tuple]] = None
              limit: Optional[int] = None

          @dataclass
          class InsertStmt:
              table: str
              columns: List[str]
              values: List[Any]

          @dataclass
          class BinaryExpr:
              left: 'Expr'
              op: str
              right: 'Expr'

          class SQLParser:
              def __init__(self, tokens):
                  self.tokens = tokens
                  self.pos = 0

              def parse_select(self):
                  self.expect('SELECT')

                  # Parse columns
                  columns = []
                  if self.match('*'):
                      columns = ['*']
                  else:
                      columns.append(self.expect_identifier())
                      while self.match(','):
                          columns.append(self.expect_identifier())

                  self.expect('FROM')
                  table = self.expect_identifier()

                  # Optional WHERE clause
                  where = None
                  if self.match('WHERE'):
                      where = self.parse_expression()

                  return SelectStmt(columns, table, where)

              def parse_expression(self):
                  return self.parse_or()

              def parse_or(self):
                  left = self.parse_and()
                  while self.match('OR'):
                      right = self.parse_and()
                      left = BinaryExpr(left, 'OR', right)
                  return left

              def parse_and(self):
                  left = self.parse_comparison()
                  while self.match('AND'):
                      right = self.parse_comparison()
                      left = BinaryExpr(left, 'AND', right)
                  return left

              def parse_comparison(self):
                  left = self.parse_primary()
                  if self.current() in ('=', '<', '>', '<=', '>=', '!=', '<>'):
                      op = self.advance()
                      right = self.parse_primary()
                      return BinaryExpr(left, op, right)
                  return left

          # Usage:
          # parser = SQLParser(tokenize("SELECT name, age FROM users WHERE age > 18"))
          # stmt = parser.parse_select()
      pitfalls:
      - Left recursion in grammar
      - Operator precedence (AND vs OR)
      - Not handling parentheses
      concepts:
      - Recursive descent parsing
      - AST design
      - Operator precedence
      estimated_hours: 5-8
      deliverables:
      - SELECT statement parser producing AST with columns, table, and where clause
      - INSERT statement parser producing AST with table, columns, and values
      - CREATE TABLE parser producing AST with column names, types, and constraints
      - Expression parser handling arithmetic, comparison, and boolean expressions
    - id: 3
      name: B-tree Page Format
      description: Implement the on-disk B-tree page structure.
      acceptance_criteria:
      - Page header contains page type, cell count, and free space offset fields
      - Leaf pages store sorted key-value cells accessible by binary search
      - Internal pages store separator keys and child page numbers for tree navigation
      - Pages serialize to and deserialize from fixed-size byte buffers correctly
      hints:
        level1: Each page is a node in the B-tree. Start with leaf nodes only.
        level2: |-
          Page layout:
          [Header (8-12 bytes)]
          [Cell Pointer Array (2 bytes each)]
          [Free space]
          [Cells (grow from bottom)]
        level3: |-
          # SQLite-like B-tree page format
          import struct
          from enum import IntEnum

          class PageType(IntEnum):
              INTERIOR_INDEX = 2
              INTERIOR_TABLE = 5
              LEAF_INDEX = 10
              LEAF_TABLE = 13

          class BTreePage:
              '''
              Page format (4096 bytes default):
              - Header (8-12 bytes)
              - Cell pointer array
              - Unallocated space
              - Cell content area (grows from end)

              Header format:
              - 1 byte: page type
              - 2 bytes: first freeblock offset (0 if none)
              - 2 bytes: number of cells
              - 2 bytes: start of cell content area
              - 1 byte: fragmented free bytes
              - 4 bytes: right-most pointer (interior pages only)
              '''

              def __init__(self, page_num, page_type=PageType.LEAF_TABLE):
                  self.page_num = page_num
                  self.page_type = page_type
                  self.cells = []
                  self.right_ptr = None  # For interior pages
                  self.page_size = 4096

              def serialize(self):
                  # Build cell content from the end
                  cell_data = b''
                  cell_pointers = []
                  content_start = self.page_size

                  for cell in self.cells:
                      cell_bytes = cell.serialize()
                      content_start -= len(cell_bytes)
                      cell_data = cell_bytes + cell_data
                      cell_pointers.append(content_start)

                  # Header
                  header_size = 12 if self.page_type in (PageType.INTERIOR_INDEX, PageType.INTERIOR_TABLE) else 8
                  header = struct.pack('>BHHHB',
                      self.page_type,
                      0,  # first freeblock
                      len(self.cells),
                      content_start,
                      0   # fragmented bytes
                  )
                  if header_size == 12:
                      header += struct.pack('>I', self.right_ptr or 0)

                  # Cell pointers
                  pointers = b''.join(struct.pack('>H', p) for p in cell_pointers)

                  # Assemble page
                  used = header_size + len(pointers)
                  unallocated = self.page_size - used - len(cell_data)
                  return header + pointers + (b'\x00' * unallocated) + cell_data

              @classmethod
              def deserialize(cls, page_num, data):
                  page_type = data[0]
                  num_cells = struct.unpack('>H', data[3:5])[0]

                  page = cls(page_num, PageType(page_type))
                  header_size = 12 if page_type in (2, 5) else 8

                  # Read cell pointers
                  for i in range(num_cells):
                      offset = struct.unpack('>H', data[header_size + i*2:header_size + i*2 + 2])[0]
                      # Parse cell at offset...
                  return page
      pitfalls:
      - Cell overflow (value too large for page)
      - Page fragmentation after deletions
      - Endianness
      concepts:
      - B-tree structure
      - Page-based storage
      - Variable-length records
      estimated_hours: 6-10
      deliverables:
      - Page structure with header, cell pointers, and cell content areas
      - Leaf page storing key-value pairs sorted by key within page
      - Internal page storing keys and child page pointers for navigation
      - Page serialization writing structured data to fixed-size disk pages
    - id: 4
      name: Table Storage
      description: Store table rows in B-tree leaves with rowid as key.
      acceptance_criteria:
      - CREATE TABLE creates B-tree root page and records schema in system catalog
      - INSERT serializes row data and inserts into correct B-tree leaf page by key
      - Node splitting creates new leaf page and promotes separator key to parent
      - Full scan returns all rows in primary key order by traversing leaf pages
      hints:
        level1: 'Simplest: rowid -> serialized row bytes. Key is rowid, value is row.'
        level2: 'sqlite_master stores schema: CREATE TABLE statements as text.'
        level3: |-
          B-tree table storage in Python:

          import struct
          from dataclasses import dataclass
          from typing import List, Any, Optional

          @dataclass
          class Row:
              rowid: int
              values: List[Any]

          class TableBTree:
              PAGE_SIZE = 4096

              def __init__(self, pager, root_page_num: int):
                  self.pager = pager
                  self.root_page = root_page_num

              def insert(self, rowid: int, values: List[Any]):
                  """Insert a row into the B-tree."""
                  record = self.serialize_row(rowid, values)

                  # Find leaf page for this rowid
                  leaf_page = self.find_leaf(self.root_page, rowid)
                  page = self.pager.get_page(leaf_page)

                  # Check if page has space
                  if page.num_cells >= page.max_cells:
                      # Split the page
                      self.split_leaf(leaf_page)
                      # Re-find after split
                      leaf_page = self.find_leaf(self.root_page, rowid)
                      page = self.pager.get_page(leaf_page)

                  # Insert cell in sorted order
                  insert_idx = self.find_cell_index(page, rowid)
                  page.insert_cell(insert_idx, rowid, record)
                  self.pager.mark_dirty(leaf_page)

              def serialize_row(self, rowid: int, values: List[Any]) -> bytes:
                  """Serialize row to SQLite record format."""
                  header = []
                  body = b''

                  for val in values:
                      if val is None:
                          header.append(0)
                      elif isinstance(val, int):
                          header.append(1)  # INT8
                          body += struct.pack('>b', val)
                      elif isinstance(val, str):
                          encoded = val.encode('utf-8')
                          header.append(13 + len(encoded) * 2)
                          body += encoded

                  header_bytes = bytes(header)
                  return bytes([len(header_bytes) + 1]) + header_bytes + body

              def scan(self):
                  """Yield all rows in rowid order."""
                  page_num = self.find_leftmost_leaf(self.root_page)

                  while page_num is not None:
                      page = self.pager.get_page(page_num)
                      for cell in page.cells:
                          yield self.deserialize_row(cell.key, cell.payload)
                      page_num = page.right_sibling
      pitfalls:
      - Not handling NULL values
      - Rowid gaps after deletes
      - Variable-length integer encoding
      concepts:
      - Row storage formats
      - Schema management
      - Type serialization
      estimated_hours: 4-6
      deliverables:
      - Table creation allocating root B-tree page and storing schema metadata
      - Row insertion serializing column values and inserting into B-tree leaf page
      - B-tree node splitting when leaf page exceeds capacity after insertion
      - Full table scan iterating all rows by traversing B-tree leaf pages in order
    - id: 5
      name: SELECT Execution (Table Scan)
      description: Execute SELECT queries by scanning all rows.
      acceptance_criteria:
      - SELECT * FROM table returns all rows
      - SELECT col1, col2 returns specific columns
      - Rows returned in rowid order
      - Handle non-existent tables with error
      hints:
        level1: Iterate B-tree leaves from leftmost to rightmost.
        level2: Deserialize each row, project requested columns.
        level3: |-
          # SELECT query execution with table scan
          from dataclasses import dataclass
          from typing import List, Any, Optional

          @dataclass
          class Column:
              name: str
              type: str
              primary_key: bool = False

          @dataclass
          class Table:
              name: str
              columns: List[Column]
              root_page: int

          class QueryExecutor:
              def __init__(self, pager, schema):
                  self.pager = pager
                  self.schema = schema

              def execute_select(self, stmt):
                  table = self.schema.get_table(stmt.table_name)
                  if not table:
                      raise RuntimeError(f"no such table: {stmt.table_name}")

                  # Full table scan
                  results = []
                  for row in self.scan_table(table):
                      # Apply WHERE filter
                      if stmt.where_clause:
                          if not self.evaluate_where(row, stmt.where_clause, table):
                              continue

                      # Project columns
                      if stmt.columns == ['*']:
                          results.append(row)
                      else:
                          projected = []
                          for col_name in stmt.columns:
                              idx = self.get_column_index(table, col_name)
                              projected.append(row[idx])
                          results.append(tuple(projected))

                  return results

              def scan_table(self, table):
                  '''Iterate all rows via B-tree traversal'''
                  def scan_page(page_num):
                      page = self.pager.get_page(page_num)

                      if page.is_leaf():
                          for cell in page.cells:
                              yield cell.payload  # (rowid, col1, col2, ...)
                      else:
                          # Interior page: traverse children
                          for cell in page.cells:
                              yield from scan_page(cell.left_child)
                          if page.right_ptr:
                              yield from scan_page(page.right_ptr)

                  yield from scan_page(table.root_page)

              def evaluate_where(self, row, where, table):
                  '''Evaluate WHERE clause against a row'''
                  if where.op == 'AND':
                      return (self.evaluate_where(row, where.left, table) and
                              self.evaluate_where(row, where.right, table))
                  elif where.op == 'OR':
                      return (self.evaluate_where(row, where.left, table) or
                              self.evaluate_where(row, where.right, table))
                  else:
                      # Comparison: column op value
                      col_idx = self.get_column_index(table, where.column)
                      col_value = row[col_idx]

                      if where.op == '=':
                          return col_value == where.value
                      elif where.op == '<':
                          return col_value < where.value
                      elif where.op == '>':
                          return col_value > where.value
                      # ... other operators
      pitfalls:
      - Column name case sensitivity
      - NULL handling in output
      - Memory management for large result sets
      concepts:
      - B-tree traversal
      - Projection operator
      - Cursor pattern
      estimated_hours: 3-4
      deliverables:
      - Table scan operator
      - Row deserialization from binary page format to in-memory record structure with typed column values
      - Column projection that selects specified fields from result rows and constructs the output tuple
      - Result set building
    - id: 6
      name: INSERT/UPDATE/DELETE
      description: Implement data modification operations.
      acceptance_criteria:
      - INSERT adds row to B-tree and subsequent SELECT returns the inserted data
      - UPDATE modifies specified columns in rows matching WHERE condition
      - DELETE removes matched rows and subsequent SELECT no longer returns them
      - NOT NULL constraint rejects INSERT or UPDATE setting column to null value
      hints:
        level1: 'INSERT: serialize row, insert into B-tree with next rowid.'
        level2: 'DELETE: find matching rows, remove from B-tree, handle rebalancing.'
        level3: |-
          # Data modification operations
          class QueryExecutor:
              def execute_insert(self, stmt):
                  table = self.schema.get_table(stmt.table_name)

                  # Get next rowid
                  rowid = self.get_next_rowid(table)

                  # Build record
                  values = [rowid]
                  for i, col in enumerate(table.columns):
                      if col.name in stmt.columns:
                          idx = stmt.columns.index(col.name)
                          values.append(stmt.values[idx])
                      else:
                          values.append(None)  # Default value

                  # Insert into B-tree
                  record = self.encode_record(values)
                  self.btree_insert(table.root_page, rowid, record)

                  return rowid

              def execute_update(self, stmt):
                  table = self.schema.get_table(stmt.table_name)
                  updated_count = 0

                  # Scan and update matching rows
                  for row in self.scan_table(table):
                      if stmt.where_clause and not self.evaluate_where(row, stmt.where_clause, table):
                          continue

                      rowid = row[0]
                      new_values = list(row)

                      # Apply SET clauses
                      for col_name, new_value in stmt.set_clauses:
                          idx = self.get_column_index(table, col_name)
                          new_values[idx] = new_value

                      # Update in B-tree (delete + insert)
                      self.btree_delete(table.root_page, rowid)
                      record = self.encode_record(new_values)
                      self.btree_insert(table.root_page, rowid, record)
                      updated_count += 1

                  return updated_count

              def execute_delete(self, stmt):
                  table = self.schema.get_table(stmt.table_name)
                  deleted_count = 0

                  # Collect rowids to delete (can't modify during scan)
                  to_delete = []
                  for row in self.scan_table(table):
                      if stmt.where_clause and not self.evaluate_where(row, stmt.where_clause, table):
                          continue
                      to_delete.append(row[0])  # rowid

                  # Delete from B-tree
                  for rowid in to_delete:
                      self.btree_delete(table.root_page, rowid)
                      deleted_count += 1

                  return deleted_count

              def encode_record(self, values):
                  '''SQLite record format: header + body'''
                  header = []
                  body = b''
                  for val in values:
                      if val is None:
                          header.append(0)
                      elif isinstance(val, int):
                          if val == 0: header.append(8)
                          elif val == 1: header.append(9)
                          else: header.append(1); body += struct.pack('>b', val)
                      elif isinstance(val, str):
                          encoded = val.encode('utf-8')
                          header.append(13 + len(encoded) * 2)
                          body += encoded
                  return self.encode_varint_header(header) + body
      pitfalls:
      - B-tree rebalancing after delete
      - Updating primary key
      - Constraint violations
      concepts:
      - B-tree insertion and deletion
      - Record modification
      - Constraint checking
      estimated_hours: 5-8
      deliverables:
      - INSERT execution adding new rows to table B-tree storage
      - UPDATE execution modifying column values in existing rows matching condition
      - DELETE execution removing rows matching condition from B-tree storage
      - Constraint enforcement validating NOT NULL and UNIQUE during write operations
    - id: 7
      name: WHERE Clause and Indexes
      description: Implement filtering and secondary indexes.
      acceptance_criteria:
      - WHERE clause filters rows using comparison operators and boolean logic correctly
      - Secondary index stores column values sorted for efficient range and equality lookups
      - Index lookup retrieves matching rows without scanning entire table
      - CREATE INDEX builds B-tree index from existing table data on specified column
      hints:
        level1: 'Full table scan with filter: scan all, test predicate, output matches.'
        level2: 'Index B-tree: key=indexed column, value=rowid. Lookup, then fetch row.'
        level3: |-
          # Index-based query optimization
          from bisect import bisect_left

          class Index:
              def __init__(self, name, table_name, columns, root_page):
                  self.name = name
                  self.table_name = table_name
                  self.columns = columns  # List of column names
                  self.root_page = root_page

          class QueryOptimizer:
              def __init__(self, schema, executor):
                  self.schema = schema
                  self.executor = executor

              def optimize_select(self, stmt):
                  '''Choose best execution strategy'''
                  table = self.schema.get_table(stmt.table_name)

                  if not stmt.where_clause:
                      return TableScan(table)

                  # Look for usable indexes
                  index = self.find_usable_index(stmt.where_clause, table)
                  if index:
                      return IndexScan(table, index, stmt.where_clause)

                  return TableScan(table)

              def find_usable_index(self, where, table):
                  '''Find index that covers WHERE clause'''
                  if where.op in ('=', '<', '>', '<=', '>='):
                      for idx in self.schema.get_indexes(table.name):
                          if idx.columns[0] == where.column:
                              return idx
                  return None

          class IndexScan:
              def __init__(self, table, index, where):
                  self.table = table
                  self.index = index
                  self.where = where

              def execute(self, executor):
                  '''Use index to find matching rowids, then fetch rows'''
                  if self.where.op == '=':
                      # Point lookup
                      rowids = executor.index_lookup(self.index, self.where.value)
                  elif self.where.op in ('<', '<='):
                      # Range scan from start
                      rowids = executor.index_range(self.index, None, self.where.value,
                                                   include_end=(self.where.op == '<='))
                  elif self.where.op in ('>', '>='):
                      # Range scan to end
                      rowids = executor.index_range(self.index, self.where.value, None,
                                                   include_start=(self.where.op == '>='))

                  # Fetch actual rows
                  for rowid in rowids:
                      yield executor.fetch_row(self.table, rowid)

          class QueryExecutor:
              def index_lookup(self, index, key):
                  '''B-tree lookup for exact match'''
                  page = self.pager.get_page(index.root_page)
                  while True:
                      if page.is_leaf():
                          for cell in page.cells:
                              if cell.key == key:
                                  yield cell.rowid
                          return
                      else:
                          # Find child to descend
                          child_page = self.find_child(page, key)
                          page = self.pager.get_page(child_page)
      pitfalls:
      - Index not used for non-equality predicates
      - Maintaining index on INSERT/UPDATE/DELETE
      - Choosing between index scan vs table scan
      concepts:
      - Secondary indexes
      - Query planning basics
      - Filter predicates
      estimated_hours: 6-10
      deliverables:
      - WHERE clause evaluator filtering rows by boolean expression during scan
      - Secondary index B-tree mapping indexed column values to primary keys
      - Index-based lookup using B-tree search instead of full table scan
      - CREATE INDEX statement building secondary index on specified column
    - id: 8
      name: Query Planner
      description: Implement cost-based query optimization.
      acceptance_criteria:
      - Planner considers both full table scan and available index scan for each query
      - Cost model estimates I/O pages read for each candidate execution plan
      - Planner selects index scan when estimated cost is lower than full table scan
      - EXPLAIN shows chosen plan including scan type, index used, and estimated rows
      hints:
        level1: Store row count per table. Estimate selectivity of predicates.
        level2: 'Cost model: table scan = N rows, index lookup = log(N) + matching rows.'
        level3: |-
          # Query planner with cost estimation
          from dataclasses import dataclass
          from typing import List
          import math

          @dataclass
          class Plan:
              cost: float
              rows: int
              description: str

          class QueryPlanner:
              def __init__(self, schema, stats):
                  self.schema = schema
                  self.stats = stats  # Table statistics

              def plan_select(self, stmt):
                  table = self.schema.get_table(stmt.table_name)
                  table_stats = self.stats.get(table.name)

                  plans = []

                  # Option 1: Full table scan
                  scan_cost = table_stats.row_count * table_stats.avg_row_size
                  plans.append(Plan(
                      cost=scan_cost,
                      rows=self.estimate_rows(stmt.where_clause, table_stats),
                      description=f"SCAN TABLE {table.name}"
                  ))

                  # Option 2: Index scans
                  for index in self.schema.get_indexes(table.name):
                      usability = self.analyze_index_usability(index, stmt.where_clause)
                      if usability:
                          idx_stats = self.stats.get_index(index.name)

                          # Cost = index lookup + row fetches
                          selectivity = self.estimate_selectivity(usability, idx_stats)
                          index_cost = math.log2(idx_stats.entries) + (selectivity * table_stats.row_count)

                          plans.append(Plan(
                              cost=index_cost,
                              rows=int(selectivity * table_stats.row_count),
                              description=f"SEARCH {table.name} USING INDEX {index.name}"
                          ))

                  # Choose lowest cost plan
                  return min(plans, key=lambda p: p.cost)

              def estimate_selectivity(self, condition, stats):
                  '''Estimate fraction of rows matching condition'''
                  if condition.op == '=':
                      # Assume uniform distribution
                      return 1.0 / stats.distinct_values
                  elif condition.op in ('<', '>', '<=', '>='):
                      # Assume 1/3 of range
                      return 0.33
                  elif condition.op == 'BETWEEN':
                      return 0.25
                  return 0.5  # Default

              def explain(self, stmt):
                  '''Generate EXPLAIN output'''
                  plan = self.plan_select(stmt)
                  return [
                      f"--  --  --  --  Detail",
                      f"0   0   0   EXECUTE",
                      f"1   0   0   {plan.description}",
                      f"",
                      f"Estimated rows: {plan.rows}",
                      f"Estimated cost: {plan.cost:.2f}"
                  ]

          @dataclass
          class TableStats:
              row_count: int
              avg_row_size: int
              page_count: int

          @dataclass
          class IndexStats:
              entries: int
              distinct_values: int
              depth: int
      pitfalls:
      - Stale statistics
      - Wrong cardinality estimates
      - Exponential plan space for many JOINs
      concepts:
      - Cost-based optimization
      - Cardinality estimation
      - Join algorithms
      estimated_hours: 8-12
      deliverables:
      - Plan enumeration generating candidate execution strategies for each query
      - Cost estimation model comparing full scan versus index scan costs
      - Index selection choosing cheapest available index for WHERE clause predicates
      - EXPLAIN command displaying chosen execution plan for a given query
    - id: 9
      name: Transactions (BEGIN/COMMIT/ROLLBACK)
      description: Implement ACID transactions.
      acceptance_criteria:
      - BEGIN starts transaction
      - COMMIT makes changes permanent
      - ROLLBACK undoes all changes since BEGIN
      - Changes not visible to other connections until commit
      - Crash recovery maintains consistency
      hints:
        level1: 'Shadow paging: copy pages before modify, commit = swap pointers.'
        level2: 'Or WAL: write changes to separate log, replay on recovery.'
        level3: |-
          Transaction management with rollback journal in Python:

          import os
          import shutil
          from enum import Enum

          class TransactionState(Enum):
              NONE = 0
              ACTIVE = 1
              COMMIT = 2

          class TransactionManager:
              def __init__(self, db_path):
                  self.db_path = db_path
                  self.journal_path = db_path + "-journal"
                  self.state = TransactionState.NONE
                  self.dirty_pages = {}
                  self.original_pages = {}

              def begin(self):
                  if self.state != TransactionState.NONE:
                      raise Exception("Transaction already active")
                  self.state = TransactionState.ACTIVE
                  self.dirty_pages = {}
                  self.original_pages = {}

              def write_page(self, page_num, data):
                  """Write page through transaction."""
                  if self.state != TransactionState.ACTIVE:
                      raise Exception("No active transaction")

                  # Save original page to journal (first write only)
                  if page_num not in self.original_pages:
                      original = self.read_page_direct(page_num)
                      self.original_pages[page_num] = original
                      self._append_to_journal(page_num, original)

                  self.dirty_pages[page_num] = data

              def _append_to_journal(self, page_num, data):
                  """Append page to rollback journal."""
                  with open(self.journal_path, 'ab') as f:
                      if f.tell() == 0:
                          # Write journal header
                          f.write(b'SQLite format 3')
                          f.write(struct.pack('>I', os.path.getsize(self.db_path)))
                      f.write(struct.pack('>I', page_num))
                      f.write(data)
                      f.write(struct.pack('>I', self._checksum(data)))

              def commit(self):
                  if self.state != TransactionState.ACTIVE:
                      return

                  # Write all dirty pages to database
                  with open(self.db_path, 'r+b') as f:
                      for page_num, data in self.dirty_pages.items():
                          f.seek(page_num * 4096)
                          f.write(data)
                      f.flush()
                      os.fsync(f.fileno())

                  # Delete journal (commit point)
                  if os.path.exists(self.journal_path):
                      os.remove(self.journal_path)

                  self.state = TransactionState.NONE
                  self.dirty_pages = {}
                  self.original_pages = {}

              def rollback(self):
                  if self.state != TransactionState.ACTIVE:
                      return

                  # Restore pages from journal
                  if os.path.exists(self.journal_path):
                      self._restore_from_journal()
                      os.remove(self.journal_path)

                  self.state = TransactionState.NONE
                  self.dirty_pages = {}
                  self.original_pages = {}
      pitfalls:
      - Partial writes (torn pages)
      - Lock ordering deadlocks
      - Long-running transactions blocking others
      concepts:
      - ACID properties
      - Shadow paging vs WAL
      - Crash recovery
      estimated_hours: 8-12
      deliverables:
      - Transaction state tracking with active, committed, and aborted states plus isolation level configuration
      - Rollback journal recording original page contents before modification for transaction undo support
      - Commit processing that flushes dirty pages to disk and removes the rollback journal atomically
      - ACID guarantees ensuring atomicity via rollback journal, consistency via constraints, isolation via locking, and durability
        via fsync
    - id: 10
      name: WAL Mode
      description: Implement Write-Ahead Logging for better concurrency.
      acceptance_criteria:
      - WAL mode writes committed pages to separate WAL file instead of main database
      - Readers check WAL first for most recent page version before reading main database
      - Checkpoint merges WAL pages into main database file and truncates WAL
      - Multiple readers can query database concurrently while a writer holds WAL lock
      hints:
        level1: 'WAL append-only: each commit appends frames with changed pages.'
        level2: 'Read: check WAL first (most recent), fallback to main DB.'
        level3: |-
          Write-Ahead Logging implementation in Python:

          import struct
          import mmap
          from dataclasses import dataclass
          from typing import Dict, Optional

          @dataclass
          class WALFrame:
              page_num: int
              commit: bool  # Is this the last frame in a transaction?
              data: bytes
              checksum: int

          class WALWriter:
              FRAME_HEADER_SIZE = 24

              def __init__(self, db_path):
                  self.wal_path = db_path + "-wal"
                  self.shm_path = db_path + "-shm"
                  self.page_cache: Dict[int, bytes] = {}  # page_num -> latest data
                  self.wal_index: Dict[int, int] = {}  # page_num -> frame offset
                  self.frame_count = 0

              def begin_write_transaction(self):
                  # Acquire write lock via shared memory
                  pass

              def write_page(self, page_num: int, data: bytes, commit: bool = False):
                  """Append page to WAL file."""
                  checksum = self._checksum(data)

                  with open(self.wal_path, 'ab') as f:
                      if f.tell() == 0:
                          self._write_wal_header(f)

                      frame_offset = f.tell()

                      # Frame header: page_num (4), commit (4), checksum (16)
                      header = struct.pack('>II', page_num, 1 if commit else 0)
                      header += struct.pack('>QQ', checksum, checksum)
                      f.write(header)
                      f.write(data)

                      self.wal_index[page_num] = frame_offset
                      self.frame_count += 1

                      if commit:
                          f.flush()
                          os.fsync(f.fileno())

              def read_page(self, page_num: int, db_file) -> bytes:
                  """Read page, checking WAL first then main DB."""
                  # Check WAL index for most recent version
                  if page_num in self.wal_index:
                      frame_offset = self.wal_index[page_num]
                      with open(self.wal_path, 'rb') as f:
                          f.seek(frame_offset + self.FRAME_HEADER_SIZE)
                          return f.read(4096)

                  # Read from main database
                  db_file.seek(page_num * 4096)
                  return db_file.read(4096)

              def checkpoint(self, db_path):
                  """Copy WAL pages back to main database."""
                  if not os.path.exists(self.wal_path):
                      return

                  with open(db_path, 'r+b') as db, open(self.wal_path, 'rb') as wal:
                      wal.seek(32)  # Skip WAL header

                      while True:
                          header = wal.read(self.FRAME_HEADER_SIZE)
                          if len(header) < self.FRAME_HEADER_SIZE:
                              break

                          page_num = struct.unpack('>I', header[:4])[0]
                          data = wal.read(4096)

                          db.seek(page_num * 4096)
                          db.write(data)

                      db.flush()
                      os.fsync(db.fileno())

                  # Truncate WAL
                  open(self.wal_path, 'w').close()
                  self.wal_index.clear()
                  self.frame_count = 0
      pitfalls:
      - WAL growing unbounded without checkpoint
      - Readers pinning old WAL frames
      - WAL file corruption detection
      concepts:
      - Write-ahead logging
      - MVCC basics
      - Checkpointing
      estimated_hours: 8-12
      deliverables:
      - WAL file format appending committed page images to separate log file
      - WAL reader serving reads from WAL for recently committed pages
      - Checkpoint process copying WAL pages back into main database file
      - Concurrent reader support allowing reads during active write transactions
  build-tcp-stack:
    id: build-tcp-stack
    name: Build Your Own TCP/IP Stack
    description: Implement a TCP/IP network stack from scratch.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - Networking fundamentals
    - C programming
    - Packet analysis
    - State machines
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible: []
    resources:
    - type: book
      name: TCP/IP Illustrated Vol 1
      url: https://www.amazon.com/TCP-Illustrated-Vol-Addison-Wesley-Professional/dp/0201633469
    - type: rfc
      name: RFC 793 - TCP
      url: https://tools.ietf.org/html/rfc793
    - type: tutorial
      name: Let's code a TCP/IP stack
      url: https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/
    milestones:
    - id: 1
      name: Ethernet & ARP
      description: Handle Ethernet frames and ARP protocol.
      acceptance_criteria:
      - Raw socket or TAP device receives and sends Ethernet frames on network interface
      - Ethernet parser correctly extracts 6-byte source and destination MAC addresses
      - ARP reply is sent with correct MAC address when ARP request matches our IP
      - MAC address table caches ARP entries and evicts them after configurable timeout
      hints:
        level1: Use TAP device or raw sockets to send/receive Ethernet frames.
        level2: ARP resolves IP to MAC. Cache results in ARP table.
        level3: "## Ethernet & ARP\n\n```c\n#include <linux/if_tun.h>\n#include <net/if.h>\n\n// Ethernet header\nstruct eth_hdr\
          \ {\n    uint8_t  dst[6];\n    uint8_t  src[6];\n    uint16_t ethertype;\n} __attribute__((packed));\n\n#define\
          \ ETH_P_ARP  0x0806\n#define ETH_P_IP   0x0800\n\n// ARP header\nstruct arp_hdr {\n    uint16_t hwtype;\n    uint16_t\
          \ protype;\n    uint8_t  hwsize;\n    uint8_t  prosize;\n    uint16_t opcode;\n    uint8_t  sender_mac[6];\n   \
          \ uint32_t sender_ip;\n    uint8_t  target_mac[6];\n    uint32_t target_ip;\n} __attribute__((packed));\n\n#define\
          \ ARP_REQUEST 1\n#define ARP_REPLY   2\n\n// ARP cache\nstruct arp_entry {\n    uint32_t ip;\n    uint8_t mac[6];\n\
          \    time_t expires;\n};\n\nstruct arp_entry arp_cache[256];\n\nvoid handle_arp(struct eth_hdr *eth, struct arp_hdr\
          \ *arp) {\n    if (ntohs(arp->opcode) == ARP_REQUEST) {\n        if (arp->target_ip == our_ip) {\n            //\
          \ Send ARP reply\n            struct arp_hdr reply = {\n                .hwtype = htons(1),\n                .protype\
          \ = htons(ETH_P_IP),\n                .hwsize = 6,\n                .prosize = 4,\n                .opcode = htons(ARP_REPLY),\n\
          \                .sender_ip = our_ip,\n                .target_ip = arp->sender_ip,\n            };\n          \
          \  memcpy(reply.sender_mac, our_mac, 6);\n            memcpy(reply.target_mac, arp->sender_mac, 6);\n          \
          \  \n            send_ethernet(arp->sender_mac, ETH_P_ARP, &reply, sizeof(reply));\n        }\n    } else if (ntohs(arp->opcode)\
          \ == ARP_REPLY) {\n        // Update ARP cache\n        arp_cache_add(arp->sender_ip, arp->sender_mac);\n    }\n\
          }\n\nvoid arp_resolve(uint32_t ip, void (*callback)(uint8_t *mac)) {\n    // Check cache first\n    struct arp_entry\
          \ *entry = arp_cache_lookup(ip);\n    if (entry) {\n        callback(entry->mac);\n        return;\n    }\n    \n\
          \    // Send ARP request\n    struct arp_hdr req = {\n        .hwtype = htons(1),\n        .protype = htons(ETH_P_IP),\n\
          \        .hwsize = 6,\n        .prosize = 4,\n        .opcode = htons(ARP_REQUEST),\n        .sender_ip = our_ip,\n\
          \        .target_ip = ip,\n    };\n    memcpy(req.sender_mac, our_mac, 6);\n    memset(req.target_mac, 0, 6);\n\
          \    \n    uint8_t broadcast[6] = {0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF};\n    send_ethernet(broadcast, ETH_P_ARP,\
          \ &req, sizeof(req));\n}\n```"
      pitfalls:
      - Byte order (network vs host)
      - ARP cache poisoning
      - Broadcast handling
      concepts:
      - Layer 2 networking
      - Address resolution
      - Frame parsing
      estimated_hours: 10-15
      deliverables:
      - Raw socket or TAP device setup for capturing network frames
      - Ethernet frame parser extracting source MAC, destination MAC, and EtherType
      - ARP request and reply handler resolving IP addresses to MAC addresses
      - MAC address table caching resolved ARP entries with timeout expiration
    - id: 2
      name: IP & ICMP
      description: Implement IP routing and ICMP (ping).
      acceptance_criteria:
      - IP parser extracts source address, destination address, protocol, and payload correctly
      - IP checksum is computed correctly and verified on received packets
      - ICMP echo reply is sent in response to incoming echo request (ping responds)
      - Routing table selects correct next-hop gateway for destination IP address
      hints:
        level1: IP header has checksum over header only. ICMP rides on IP.
        level2: Route packets based on destination IP. Handle TTL expiry.
        level3: "## IP & ICMP\n\n```c\nstruct ip_hdr {\n    uint8_t  ihl:4, version:4;\n    uint8_t  tos;\n    uint16_t len;\n\
          \    uint16_t id;\n    uint16_t frag_offset;\n    uint8_t  ttl;\n    uint8_t  protocol;\n    uint16_t checksum;\n\
          \    uint32_t src;\n    uint32_t dst;\n} __attribute__((packed));\n\n#define IP_PROTO_ICMP 1\n#define IP_PROTO_TCP\
          \  6\n#define IP_PROTO_UDP  17\n\nstruct icmp_hdr {\n    uint8_t  type;\n    uint8_t  code;\n    uint16_t checksum;\n\
          \    uint16_t id;\n    uint16_t seq;\n} __attribute__((packed));\n\n#define ICMP_ECHO_REQUEST 8\n#define ICMP_ECHO_REPLY\
          \   0\n\nuint16_t checksum(void *data, int len) {\n    uint32_t sum = 0;\n    uint16_t *ptr = data;\n    \n    while\
          \ (len > 1) {\n        sum += *ptr++;\n        len -= 2;\n    }\n    if (len) {\n        sum += *(uint8_t*)ptr;\n\
          \    }\n    \n    while (sum >> 16) {\n        sum = (sum & 0xFFFF) + (sum >> 16);\n    }\n    \n    return ~sum;\n\
          }\n\nvoid handle_ip(struct eth_hdr *eth, struct ip_hdr *ip) {\n    // Verify checksum\n    if (checksum(ip, ip->ihl\
          \ * 4) != 0) {\n        return;  // Bad checksum\n    }\n    \n    // Check if for us\n    if (ip->dst != our_ip)\
          \ {\n        // Forward or drop\n        return;\n    }\n    \n    switch (ip->protocol) {\n        case IP_PROTO_ICMP:\n\
          \            handle_icmp(ip, (struct icmp_hdr*)((uint8_t*)ip + ip->ihl * 4));\n            break;\n        case\
          \ IP_PROTO_TCP:\n            handle_tcp(ip, (struct tcp_hdr*)((uint8_t*)ip + ip->ihl * 4));\n            break;\n\
          \    }\n}\n\nvoid handle_icmp(struct ip_hdr *ip, struct icmp_hdr *icmp) {\n    if (icmp->type == ICMP_ECHO_REQUEST)\
          \ {\n        // Send echo reply\n        send_icmp_reply(ip->src, icmp->id, icmp->seq,\n                       (uint8_t*)(icmp\
          \ + 1),\n                       ntohs(ip->len) - ip->ihl * 4 - sizeof(struct icmp_hdr));\n    }\n}\n```"
      pitfalls:
      - Checksum calculation errors
      - Fragmentation handling
      - TTL not decremented
      concepts:
      - Layer 3 networking
      - Routing
      - Error handling
      estimated_hours: 12-18
      deliverables:
      - IPv4 packet parser extracting header fields and payload data
      - IPv4 packet builder constructing valid packets with checksum calculation
      - ICMP echo request and reply implementing ping functionality
      - IP routing table lookup selecting next hop for destination addresses
    - id: 3
      name: TCP Connection Management
      description: Implement TCP 3-way handshake and state machine.
      acceptance_criteria:
      - TCP parser extracts source port, destination port, sequence number, and flags
      - Three-way handshake completes successfully establishing bidirectional connection
      - State machine correctly transitions through LISTEN, SYN_SENT, ESTABLISHED, and other states
      - Four-way FIN handshake gracefully closes connection from either endpoint
      hints:
        level1: 'TCP has many states: LISTEN, SYN_SENT, ESTABLISHED, etc.'
        level2: 'Handshake: SYN -> SYN-ACK -> ACK. Track sequence numbers.'
        level3: "## TCP State Machine\n\n```c\nstruct tcp_hdr {\n    uint16_t src_port;\n    uint16_t dst_port;\n    uint32_t\
          \ seq;\n    uint32_t ack;\n    uint8_t  reserved:4, data_offset:4;\n    uint8_t  flags;\n    uint16_t window;\n\
          \    uint16_t checksum;\n    uint16_t urgent;\n} __attribute__((packed));\n\n#define TCP_FIN 0x01\n#define TCP_SYN\
          \ 0x02\n#define TCP_RST 0x04\n#define TCP_PSH 0x08\n#define TCP_ACK 0x10\n\nenum tcp_state {\n    CLOSED, LISTEN,\
          \ SYN_SENT, SYN_RECEIVED,\n    ESTABLISHED, FIN_WAIT_1, FIN_WAIT_2,\n    CLOSE_WAIT, CLOSING, LAST_ACK, TIME_WAIT\n\
          };\n\nstruct tcp_conn {\n    uint32_t local_ip, remote_ip;\n    uint16_t local_port, remote_port;\n    enum tcp_state\
          \ state;\n    uint32_t snd_una;  // Send unacknowledged\n    uint32_t snd_nxt;  // Send next\n    uint32_t rcv_nxt;\
          \  // Receive next\n    uint16_t rcv_wnd;  // Receive window\n    // ... buffers, timers ...\n};\n\nvoid tcp_input(struct\
          \ ip_hdr *ip, struct tcp_hdr *tcp) {\n    struct tcp_conn *conn = find_connection(ip, tcp);\n    \n    if (!conn)\
          \ {\n        if (tcp->flags & TCP_SYN) {\n            // New connection attempt\n            conn = create_connection(ip,\
          \ tcp);\n            conn->state = SYN_RECEIVED;\n            conn->rcv_nxt = ntohl(tcp->seq) + 1;\n           \
          \ conn->snd_nxt = generate_isn();\n            \n            // Send SYN-ACK\n            send_tcp(conn, TCP_SYN\
          \ | TCP_ACK, NULL, 0);\n            conn->snd_nxt++;\n        }\n        return;\n    }\n    \n    switch (conn->state)\
          \ {\n        case SYN_SENT:\n            if ((tcp->flags & (TCP_SYN | TCP_ACK)) == (TCP_SYN | TCP_ACK)) {\n    \
          \            conn->rcv_nxt = ntohl(tcp->seq) + 1;\n                conn->snd_una = ntohl(tcp->ack);\n          \
          \      conn->state = ESTABLISHED;\n                send_tcp(conn, TCP_ACK, NULL, 0);\n            }\n          \
          \  break;\n            \n        case SYN_RECEIVED:\n            if (tcp->flags & TCP_ACK) {\n                conn->snd_una\
          \ = ntohl(tcp->ack);\n                conn->state = ESTABLISHED;\n            }\n            break;\n          \
          \  \n        case ESTABLISHED:\n            if (tcp->flags & TCP_FIN) {\n                conn->rcv_nxt++;\n    \
          \            conn->state = CLOSE_WAIT;\n                send_tcp(conn, TCP_ACK, NULL, 0);\n            } else if\
          \ (tcp->flags & TCP_ACK) {\n                // Handle data\n                process_tcp_data(conn, tcp);\n     \
          \       }\n            break;\n        // ... other states ...\n    }\n}\n```"
      pitfalls:
      - Wrong state transitions
      - Sequence number wraparound
      - Not handling RST
      concepts:
      - State machines
      - Connection management
      - Reliability
      estimated_hours: 18-25
      deliverables:
      - TCP segment parser extracting header fields, flags, and payload
      - Three-way handshake implementing SYN, SYN-ACK, ACK connection setup
      - Connection state machine tracking TCP states from LISTEN through CLOSED
      - Connection teardown implementing FIN handshake for graceful close
    - id: 4
      name: TCP Data Transfer & Flow Control
      description: Implement reliable data transfer with flow control.
      acceptance_criteria:
      - Sliding window allows sending multiple segments before requiring acknowledgment
      - Unacknowledged segments are retransmitted after retransmission timeout expires
      - Sender limits transmission to receiver advertised window size
      - Slow start exponentially increases congestion window until threshold is reached
      hints:
        level1: Window size controls how much unacknowledged data can be in flight.
        level2: Retransmit on timeout. Implement fast retransmit on 3 duplicate ACKs.
        level3: "## Sliding Window & Retransmission\n\n```c\nstruct tcp_conn {\n    // ... previous fields ...\n    \n   \
          \ // Send buffer\n    uint8_t *snd_buf;\n    size_t snd_buf_size;\n    \n    // Receive buffer (for out-of-order)\n\
          \    uint8_t *rcv_buf;\n    uint32_t rcv_buf_start;  // seq of first byte\n    \n    // Congestion control\n   \
          \ uint32_t cwnd;           // Congestion window\n    uint32_t ssthresh;       // Slow start threshold\n    \n  \
          \  // RTT estimation\n    uint32_t srtt;           // Smoothed RTT\n    uint32_t rttvar;         // RTT variance\n\
          \    uint32_t rto;            // Retransmission timeout\n    \n    // Retransmission\n    struct timer *retx_timer;\n\
          \    int dup_ack_count;\n};\n\nvoid tcp_send(struct tcp_conn *conn, const void *data, size_t len) {\n    // Copy\
          \ to send buffer\n    memcpy(conn->snd_buf + conn->snd_nxt - conn->snd_una, data, len);\n    \n    // Send what\
          \ window allows\n    size_t can_send = MIN(conn->cwnd, conn->rcv_wnd) - (conn->snd_nxt - conn->snd_una);\n    can_send\
          \ = MIN(can_send, len);\n    \n    if (can_send > 0) {\n        send_tcp(conn, TCP_ACK | TCP_PSH, data, can_send);\n\
          \        conn->snd_nxt += can_send;\n        \n        // Start retransmission timer\n        if (!timer_active(conn->retx_timer))\
          \ {\n            timer_start(conn->retx_timer, conn->rto);\n        }\n    }\n}\n\nvoid tcp_ack_received(struct\
          \ tcp_conn *conn, uint32_t ack_num) {\n    if (ack_num > conn->snd_una) {\n        // New data acknowledged\n  \
          \      size_t acked = ack_num - conn->snd_una;\n        conn->snd_una = ack_num;\n        \n        // Update congestion\
          \ window\n        if (conn->cwnd < conn->ssthresh) {\n            // Slow start: exponential growth\n          \
          \  conn->cwnd += MSS;\n        } else {\n            // Congestion avoidance: linear growth\n            conn->cwnd\
          \ += MSS * MSS / conn->cwnd;\n        }\n        \n        // Reset retransmission timer\n        timer_stop(conn->retx_timer);\n\
          \        if (conn->snd_nxt > conn->snd_una) {\n            timer_start(conn->retx_timer, conn->rto);\n        }\n\
          \        \n        conn->dup_ack_count = 0;\n    } else {\n        // Duplicate ACK\n        conn->dup_ack_count++;\n\
          \        if (conn->dup_ack_count == 3) {\n            // Fast retransmit\n            conn->ssthresh = conn->cwnd\
          \ / 2;\n            conn->cwnd = conn->ssthresh + 3 * MSS;\n            retransmit(conn);\n        }\n    }\n}\n\
          \nvoid retransmit_timeout(struct tcp_conn *conn) {\n    // Timeout: retransmit and back off\n    conn->ssthresh\
          \ = conn->cwnd / 2;\n    conn->cwnd = MSS;  // Reset to slow start\n    conn->rto *= 2;    // Exponential backoff\n\
          \    retransmit(conn);\n}\n```"
      pitfalls:
      - Timer management bugs
      - Window calculation errors
      - Sequence wraparound
      concepts:
      - Flow control
      - Congestion control
      - Reliable delivery
      estimated_hours: 20-30
      deliverables:
      - Sliding window mechanism controlling outstanding unacknowledged data bytes
      - Retransmission timer resending segments after acknowledgment timeout expires
      - Flow control using receiver window size to prevent buffer overflow
      - Congestion control implementing slow start and congestion avoidance phases
  build-test-framework:
    id: build-test-framework
    name: Build Your Own Test Framework
    description: Build a test framework like pytest or jest.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - Reflection/metaprogramming
    - Assertion libraries
    - CLI development
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - Rust
      - Java
    resources:
    - type: code
      name: pytest source
      url: https://github.com/pytest-dev/pytest
    - type: article
      name: Building a Test Framework
      url: https://www.destroyallsoftware.com/screencasts/catalog/building-a-test-framework
    milestones:
    - id: 1
      name: Test Discovery & Execution
      description: Discover and run test functions.
      acceptance_criteria:
      - Discovery finds all functions matching test naming convention in specified modules
      - Runner executes each test and records whether it passed, failed, or errored
      - Each test runs in isolation so one test failure does not affect subsequent tests
      - Parallel execution runs independent tests concurrently reducing total suite time
      hints:
        level1: Walk directory tree, import test modules, find functions starting with 'test_'.
        level2: Run each test in isolation (fresh module state). Catch exceptions.
        level3: "## Test Discovery\n\n```python\nimport importlib.util\nimport sys\nimport os\nimport traceback\nfrom dataclasses\
          \ import dataclass\nfrom typing import Callable, List\nfrom enum import Enum\n\nclass TestResult(Enum):\n    PASSED\
          \ = 'passed'\n    FAILED = 'failed'\n    ERROR = 'error'\n    SKIPPED = 'skipped'\n\n@dataclass\nclass TestCase:\n\
          \    name: str\n    module: str\n    func: Callable\n\n@dataclass\nclass TestOutcome:\n    test: TestCase\n    result:\
          \ TestResult\n    duration: float\n    error: Exception = None\n    traceback: str = None\n\nclass TestCollector:\n\
          \    def __init__(self, path='.'):\n        self.path = path\n        self.tests: List[TestCase] = []\n    \n  \
          \  def collect(self):\n        for root, dirs, files in os.walk(self.path):\n            # Skip hidden and __pycache__\
          \ directories\n            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n       \
          \     \n            for file in files:\n                if file.startswith('test_') and file.endswith('.py'):\n\
          \                    filepath = os.path.join(root, file)\n                    self.collect_from_file(filepath)\n\
          \        \n        return self.tests\n    \n    def collect_from_file(self, filepath):\n        module_name = os.path.splitext(os.path.basename(filepath))[0]\n\
          \        \n        spec = importlib.util.spec_from_file_location(module_name, filepath)\n        module = importlib.util.module_from_spec(spec)\n\
          \        sys.modules[module_name] = module\n        \n        try:\n            spec.loader.exec_module(module)\n\
          \        except Exception as e:\n            print(f\"Error loading {filepath}: {e}\")\n            return\n   \
          \     \n        for name in dir(module):\n            if name.startswith('test_'):\n                obj = getattr(module,\
          \ name)\n                if callable(obj):\n                    self.tests.append(TestCase(\n                  \
          \      name=name,\n                        module=module_name,\n                        func=obj\n             \
          \       ))\n\nclass TestRunner:\n    def __init__(self):\n        self.outcomes: List[TestOutcome] = []\n    \n\
          \    def run(self, tests: List[TestCase]):\n        for test in tests:\n            outcome = self.run_one(test)\n\
          \            self.outcomes.append(outcome)\n            self.report_one(outcome)\n        \n        return self.outcomes\n\
          \    \n    def run_one(self, test: TestCase) -> TestOutcome:\n        import time\n        start = time.time()\n\
          \        \n        try:\n            test.func()\n            result = TestResult.PASSED\n            error = None\n\
          \            tb = None\n        except AssertionError as e:\n            result = TestResult.FAILED\n          \
          \  error = e\n            tb = traceback.format_exc()\n        except Exception as e:\n            result = TestResult.ERROR\n\
          \            error = e\n            tb = traceback.format_exc()\n        \n        duration = time.time() - start\n\
          \        return TestOutcome(test, result, duration, error, tb)\n    \n    def report_one(self, outcome: TestOutcome):\n\
          \        symbol = {\n            TestResult.PASSED: '.',\n            TestResult.FAILED: 'F',\n            TestResult.ERROR:\
          \ 'E',\n            TestResult.SKIPPED: 's'\n        }[outcome.result]\n        print(symbol, end='', flush=True)\n\
          ```"
      pitfalls:
      - Module import side effects
      - Test isolation
      - Path handling
      concepts:
      - Reflection
      - Module loading
      - Test isolation
      estimated_hours: 10-15
      deliverables:
      - Test function discovery scanning modules for test-prefixed functions
      - Test runner executing discovered tests and collecting pass/fail results
      - Test isolation ensuring each test runs independently without shared state
      - Parallel test execution running independent tests concurrently for speed
    - id: 2
      name: Assertions & Matchers
      description: Implement rich assertion library.
      acceptance_criteria:
      - assertEqual fails with descriptive message showing expected versus actual values
      - Collection assertions verify contains, length, and subset relationships correctly
      - assertRaises fails if expected exception type is not raised by callable
      - Custom matchers provide domain-specific failure messages for better diagnostics
      hints:
        level1: Assertions raise AssertionError with helpful messages.
        level2: Show diff for string/collection comparisons. Pretty-print values.
        level3: "## Assertions\n\n```python\nimport difflib\nfrom typing import Any, Type, Callable\n\nclass AssertionHelper:\n\
          \    @staticmethod\n    def assertEqual(actual: Any, expected: Any, msg: str = None):\n        if actual != expected:\n\
          \            diff = AssertionHelper._format_diff(actual, expected)\n            message = msg or f\"\\nExpected:\
          \ {expected!r}\\nActual:   {actual!r}\\n{diff}\"\n            raise AssertionError(message)\n    \n    @staticmethod\n\
          \    def assertTrue(value: Any, msg: str = None):\n        if not value:\n            raise AssertionError(msg or\
          \ f\"Expected truthy value, got {value!r}\")\n    \n    @staticmethod\n    def assertFalse(value: Any, msg: str\
          \ = None):\n        if value:\n            raise AssertionError(msg or f\"Expected falsy value, got {value!r}\"\
          )\n    \n    @staticmethod\n    def assertIn(item: Any, container: Any, msg: str = None):\n        if item not in\
          \ container:\n            raise AssertionError(msg or f\"{item!r} not found in {container!r}\")\n    \n    @staticmethod\n\
          \    def assertRaises(exc_type: Type[Exception]):\n        return _ExceptionContext(exc_type)\n    \n    @staticmethod\n\
          \    def assertAlmostEqual(actual: float, expected: float, places: int = 7):\n        if round(abs(actual - expected),\
          \ places) != 0:\n            raise AssertionError(f\"{actual} != {expected} within {places} places\")\n    \n  \
          \  @staticmethod\n    def _format_diff(actual: Any, expected: Any) -> str:\n        if isinstance(actual, str) and\
          \ isinstance(expected, str):\n            diff = difflib.unified_diff(\n                expected.splitlines(keepends=True),\n\
          \                actual.splitlines(keepends=True),\n                fromfile='expected',\n                tofile='actual'\n\
          \            )\n            return ''.join(diff)\n        \n        if isinstance(actual, (list, tuple)) and isinstance(expected,\
          \ (list, tuple)):\n            diff_lines = []\n            for i, (a, e) in enumerate(zip(actual, expected)):\n\
          \                if a != e:\n                    diff_lines.append(f\"  Index {i}: expected {e!r}, got {a!r}\")\n\
          \            if len(actual) != len(expected):\n                diff_lines.append(f\"  Length: expected {len(expected)},\
          \ got {len(actual)}\")\n            return '\\n'.join(diff_lines)\n        \n        return ''\n\nclass _ExceptionContext:\n\
          \    def __init__(self, exc_type: Type[Exception]):\n        self.exc_type = exc_type\n        self.exception =\
          \ None\n    \n    def __enter__(self):\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n\
          \        if exc_type is None:\n            raise AssertionError(f\"Expected {self.exc_type.__name__} to be raised\"\
          )\n        \n        if not issubclass(exc_type, self.exc_type):\n            raise AssertionError(\n          \
          \      f\"Expected {self.exc_type.__name__}, got {exc_type.__name__}\"\n            )\n        \n        self.exception\
          \ = exc_val\n        return True  # Suppress the exception\n\n# Usage convenience\nassert_equal = AssertionHelper.assertEqual\n\
          assert_true = AssertionHelper.assertTrue\nassert_raises = AssertionHelper.assertRaises\n```"
      pitfalls:
      - Unhelpful error messages
      - Float comparison
      - Exception context
      concepts:
      - Assertions
      - Diff algorithms
      - Context managers
      estimated_hours: 8-12
      deliverables:
      - assertEqual, assertTrue, assertFalse verifying expected values and conditions
      - Collection assertions checking list contents, length, and membership
      - Exception assertions verifying that expected exceptions are raised
      - Custom matcher API allowing user-defined assertion predicates with messages
    - id: 3
      name: Fixtures & Setup/Teardown
      description: Implement test fixtures for setup and cleanup.
      acceptance_criteria:
      - setUp runs before each test and tearDown runs after each test regardless of outcome
      - Shared fixtures create expensive resources once and share across multiple tests
      - Module-scoped fixtures are created once per module and torn down after all tests complete
      - Fixture injection provides named resources as test function parameters automatically
      hints:
        level1: Fixtures are functions that provide test dependencies.
        level2: Use generators for setup/teardown. Scope controls lifetime.
        level3: "## Fixtures\n\n```python\nfrom typing import Dict, Any, Generator\nfrom functools import wraps\nimport inspect\n\
          \n_fixtures: Dict[str, 'Fixture'] = {}\n\nclass Fixture:\n    def __init__(self, func: Callable, scope: str = 'function'):\n\
          \        self.func = func\n        self.scope = scope  # 'function', 'module', 'session'\n        self.name = func.__name__\n\
          \        self._cached_value = None\n        self._finalized = True\n    \n    def get_value(self, context: 'FixtureContext'):\n\
          \        if self.scope != 'function' and not self._finalized:\n            return self._cached_value\n        \n\
          \        # Resolve fixture dependencies\n        sig = inspect.signature(self.func)\n        kwargs = {}\n     \
          \   for param in sig.parameters:\n            if param in _fixtures:\n                kwargs[param] = _fixtures[param].get_value(context)\n\
          \        \n        result = self.func(**kwargs)\n        \n        if inspect.isgenerator(result):\n           \
          \ # Setup/teardown fixture\n            value = next(result)\n            context.add_finalizer(lambda: self._finalize(result))\n\
          \        else:\n            value = result\n        \n        self._cached_value = value\n        self._finalized\
          \ = False\n        return value\n    \n    def _finalize(self, gen: Generator):\n        try:\n            next(gen)\n\
          \        except StopIteration:\n            pass\n        self._finalized = True\n\ndef fixture(scope: str = 'function'):\n\
          \    def decorator(func: Callable):\n        fix = Fixture(func, scope)\n        _fixtures[func.__name__] = fix\n\
          \        return func\n    return decorator\n\nclass FixtureContext:\n    def __init__(self):\n        self.finalizers\
          \ = []\n    \n    def add_finalizer(self, func: Callable):\n        self.finalizers.append(func)\n    \n    def\
          \ teardown(self):\n        for finalizer in reversed(self.finalizers):\n            try:\n                finalizer()\n\
          \            except Exception as e:\n                print(f\"Finalizer error: {e}\")\n        self.finalizers.clear()\n\
          \n# Updated TestRunner\nclass TestRunner:\n    def run_one(self, test: TestCase) -> TestOutcome:\n        context\
          \ = FixtureContext()\n        \n        try:\n            # Inject fixtures into test function\n            sig\
          \ = inspect.signature(test.func)\n            kwargs = {}\n            for param in sig.parameters:\n          \
          \      if param in _fixtures:\n                    kwargs[param] = _fixtures[param].get_value(context)\n       \
          \     \n            test.func(**kwargs)\n            result = TestResult.PASSED\n            error = None\n    \
          \    except AssertionError as e:\n            result = TestResult.FAILED\n            error = e\n        finally:\n\
          \            context.teardown()\n        \n        return TestOutcome(test, result, 0, error)\n\n# Example fixtures\n\
          @fixture(scope='function')\ndef temp_dir():\n    import tempfile\n    import shutil\n    d = tempfile.mkdtemp()\n\
          \    yield d\n    shutil.rmtree(d)\n\n@fixture(scope='session')\ndef database():\n    db = connect_to_test_db()\n\
          \    yield db\n    db.close()\n```"
      pitfalls:
      - Fixture cleanup on error
      - Circular dependencies
      - Scope leaks
      concepts:
      - Dependency injection
      - Resource management
      - Generators
      estimated_hours: 12-18
      deliverables:
      - setUp and tearDown hooks running before and after each test function
      - Shared fixture factory creating reusable test resources across multiple tests
      - Fixture scoping controlling lifetime at function, class, or module level
      - Fixture dependency injection providing test resources via parameter names
    - id: 4
      name: Reporting & CLI
      description: Generate test reports and provide CLI interface.
      acceptance_criteria:
      - Report displays each test name with pass/fail status and execution duration
      - Summary shows total count of passed, failed, and skipped tests with overall time
      - CLI accepts glob patterns to select test files and name filters to select tests
      - JUnit XML output is compatible with CI tools like Jenkins and GitHub Actions
      hints:
        level1: Color-code pass/fail. Show failure details at end.
        level2: JUnit XML is standard format for CI integration.
        level3: "## Reporting & CLI\n\n```python\nimport argparse\nimport xml.etree.ElementTree as ET\nfrom typing import\
          \ List\nimport sys\n\nclass ConsoleReporter:\n    def __init__(self, verbose: bool = False):\n        self.verbose\
          \ = verbose\n        self.colors = sys.stdout.isatty()\n    \n    def _color(self, text: str, color: str) -> str:\n\
          \        if not self.colors:\n            return text\n        colors = {'green': '\\033[92m', 'red': '\\033[91m',\
          \ 'yellow': '\\033[93m', 'reset': '\\033[0m'}\n        return f\"{colors.get(color, '')}{text}{colors['reset']}\"\
          \n    \n    def report_start(self, tests: List[TestCase]):\n        print(f\"\\nCollected {len(tests)} tests\\n\"\
          )\n    \n    def report_test(self, outcome: TestOutcome):\n        if self.verbose:\n            status = {\n  \
          \              TestResult.PASSED: self._color('PASSED', 'green'),\n                TestResult.FAILED: self._color('FAILED',\
          \ 'red'),\n                TestResult.ERROR: self._color('ERROR', 'red'),\n                TestResult.SKIPPED: self._color('SKIPPED',\
          \ 'yellow')\n            }[outcome.result]\n            print(f\"{outcome.test.module}::{outcome.test.name} {status}\"\
          )\n        else:\n            symbol = {\n                TestResult.PASSED: self._color('.', 'green'),\n      \
          \          TestResult.FAILED: self._color('F', 'red'),\n                TestResult.ERROR: self._color('E', 'red'),\n\
          \                TestResult.SKIPPED: self._color('s', 'yellow')\n            }[outcome.result]\n            print(symbol,\
          \ end='', flush=True)\n    \n    def report_summary(self, outcomes: List[TestOutcome]):\n        passed = sum(1\
          \ for o in outcomes if o.result == TestResult.PASSED)\n        failed = sum(1 for o in outcomes if o.result == TestResult.FAILED)\n\
          \        errors = sum(1 for o in outcomes if o.result == TestResult.ERROR)\n        total_time = sum(o.duration\
          \ for o in outcomes)\n        \n        print(f\"\\n\\n{'='*60}\")\n        \n        # Show failures\n        for\
          \ outcome in outcomes:\n            if outcome.result in (TestResult.FAILED, TestResult.ERROR):\n              \
          \  print(f\"\\n{self._color('FAILED', 'red')} {outcome.test.module}::{outcome.test.name}\")\n                print(outcome.traceback)\n\
          \        \n        print(f\"\\n{passed} passed, {failed} failed, {errors} errors in {total_time:.2f}s\")\n\nclass\
          \ JUnitReporter:\n    def generate(self, outcomes: List[TestOutcome], output_file: str):\n        testsuite = ET.Element('testsuite')\n\
          \        testsuite.set('tests', str(len(outcomes)))\n        testsuite.set('failures', str(sum(1 for o in outcomes\
          \ if o.result == TestResult.FAILED)))\n        testsuite.set('errors', str(sum(1 for o in outcomes if o.result ==\
          \ TestResult.ERROR)))\n        \n        for outcome in outcomes:\n            testcase = ET.SubElement(testsuite,\
          \ 'testcase')\n            testcase.set('classname', outcome.test.module)\n            testcase.set('name', outcome.test.name)\n\
          \            testcase.set('time', str(outcome.duration))\n            \n            if outcome.result == TestResult.FAILED:\n\
          \                failure = ET.SubElement(testcase, 'failure')\n                failure.set('message', str(outcome.error))\n\
          \                failure.text = outcome.traceback\n            elif outcome.result == TestResult.ERROR:\n      \
          \          error = ET.SubElement(testcase, 'error')\n                error.set('message', str(outcome.error))\n\
          \                error.text = outcome.traceback\n        \n        tree = ET.ElementTree(testsuite)\n        tree.write(output_file,\
          \ encoding='unicode', xml_declaration=True)\n\n# CLI\ndef main():\n    parser = argparse.ArgumentParser(description='Test\
          \ Framework')\n    parser.add_argument('path', nargs='?', default='.', help='Test directory')\n    parser.add_argument('-v',\
          \ '--verbose', action='store_true')\n    parser.add_argument('--junit-xml', help='Output JUnit XML file')\n    parser.add_argument('-k',\
          \ '--filter', help='Filter tests by name')\n    args = parser.parse_args()\n    \n    collector = TestCollector(args.path)\n\
          \    tests = collector.collect()\n    \n    if args.filter:\n        tests = [t for t in tests if args.filter in\
          \ t.name]\n    \n    reporter = ConsoleReporter(verbose=args.verbose)\n    reporter.report_start(tests)\n    \n\
          \    runner = TestRunner()\n    outcomes = runner.run(tests)\n    \n    reporter.report_summary(outcomes)\n    \n\
          \    if args.junit_xml:\n        JUnitReporter().generate(outcomes, args.junit_xml)\n    \n    sys.exit(0 if all(o.result\
          \ == TestResult.PASSED for o in outcomes) else 1)\n\nif __name__ == '__main__':\n    main()\n```"
      pitfalls:
      - Exit codes
      - Terminal detection
      - XML escaping
      concepts:
      - CLI design
      - Reporting formats
      - CI integration
      estimated_hours: 10-15
      deliverables:
      - Pass/fail reporting displaying test results with execution time per test
      - Summary statistics showing total passed, failed, skipped, and execution duration
      - CLI interface accepting file patterns, verbosity, and filter arguments
      - JUnit XML output format for integration with CI/CD pipeline tools
  build-text-editor:
    id: build-text-editor
    name: Build Your Own Text Editor
    description: Build a terminal-based text editor from scratch. Based on the kilo editor (~1000 lines of C).
    difficulty: advanced
    estimated_hours: 20-30
    prerequisites:
    - Terminal I/O
    - C or systems language
    - Basic data structures
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible: []
    resources:
    - name: Build Your Own Text Editor
      url: https://viewsourcecode.org/snaptoken/kilo/
      type: tutorial
    - name: antirez/kilo source
      url: https://github.com/antirez/kilo
      type: reference
    - name: Hecto (Rust version)
      url: https://philippflenker.com/hecto/
      type: tutorial
    milestones:
    - id: 1
      name: Raw Mode and Input
      description: Put terminal in raw mode and read keypresses.
      acceptance_criteria:
      - Raw mode disables echo so typed characters are not automatically displayed
      - Individual keypresses are read without waiting for Enter key confirmation
      - Arrow keys are recognized from multi-byte escape sequences and mapped to actions
      - Terminal is restored to original cooked mode when editor exits normally or on error
      hints:
        level1: Use tcgetattr/tcsetattr to modify terminal settings.
        level2: Disable ICANON and ECHO flags. Save original termios to restore later.
        level3: |-
          struct termios raw;
          tcgetattr(STDIN_FILENO, &raw);
          raw.c_lflag &= ~(ECHO | ICANON | ISIG | IEXTEN);
          raw.c_iflag &= ~(IXON | ICRNL | BRKINT | INPCK | ISTRIP);
          raw.c_oflag &= ~(OPOST);
          tcsetattr(STDIN_FILENO, TCSAFLUSH, &raw);
      pitfalls:
      - Not restoring terminal on crash
      - Ctrl+C killing before cleanup
      - Different terminal emulators
      concepts:
      - Terminal modes (canonical vs raw)
      - termios structure
      - Signal handling for cleanup
      estimated_hours: 2-3
      deliverables:
      - Terminal raw mode setup disabling echo and canonical line buffering
      - Keypress reader processing individual characters and escape sequences
      - Special key handling for arrow keys, home, end, and delete
      - Graceful cleanup restoring terminal settings on editor exit
    - id: 2
      name: Screen Refresh
      description: Clear screen and position cursor using escape sequences.
      acceptance_criteria:
      - Screen refresh redraws all visible lines using ANSI cursor positioning sequences
      - Cursor movement commands update both logical position and visible cursor location
      - Status bar at bottom shows current filename, total lines, and cursor row/column
      - Screen refresh avoids flicker by writing complete frame in single terminal write
      hints:
        level1: \x1b[2J clears screen. \x1b[H positions cursor at top-left.
        level2: \x1b[K clears line from cursor. Build screen in buffer, write once.
        level3: |-
          Common escape sequences:
          \x1b[2J    - Clear entire screen
          \x1b[H     - Move cursor to 1,1
          \x1b[{r};{c}H - Move cursor to row,col
          \x1b[K     - Clear line from cursor
          \x1b[?25l  - Hide cursor
          \x1b[?25h  - Show cursor
      pitfalls:
      - Flickering (write small chunks vs one big write)
      - Off-by-one in cursor positioning (1-indexed)
      - Screen size detection
      concepts:
      - VT100 escape sequences
      - Screen buffers
      - Terminal graphics
      estimated_hours: 2-3
      deliverables:
      - ANSI escape sequence writer controlling cursor position and screen clearing
      - Screen clearing and full redraw rendering buffer contents to terminal
      - Status bar displaying filename, line count, and cursor position information
      - Cursor positioning keeping visible cursor synchronized with logical position
    - id: 3
      name: File Viewing
      description: Load and display file contents with scrolling.
      acceptance_criteria:
      - File loader reads specified file and stores each line in editable buffer structure
      - Scrolling adjusts viewport so cursor line is always visible on screen
      - Horizontal scroll allows viewing and navigating lines longer than terminal width
      - Line numbers display in gutter with consistent width padding for alignment
      hints:
        level1: Store lines in dynamic array. Track row offset for scrolling.
        level2: Only render visible rows (offset to offset+screen_rows).
        level3: |-
          typedef struct {
            char *chars;
            int size;
          } Row;
          Row *rows;
          int numrows;
          int rowoff;  // scroll offset
      pitfalls:
      - Memory allocation for lines
      - Horizontal scrolling (long lines)
      - Tabs rendering
      concepts:
      - File I/O
      - Dynamic arrays
      - Viewport scrolling
      estimated_hours: 3-4
      deliverables:
      - File loading reading text file contents into line-based buffer structure
      - Vertical scrolling adjusting viewport offset when cursor moves beyond visible area
      - Horizontal scrolling handling lines wider than terminal column width
      - Line number display showing row numbers in left margin gutter
    - id: 4
      name: Text Editing
      description: Insert and delete characters, handle Enter and Backspace.
      acceptance_criteria:
      - Inserting a character shifts remaining text right and advances cursor by one
      - Backspace removes character before cursor and delete removes character after cursor
      - Enter key splits current line into two lines at cursor position
      - Backspace at beginning of line joins it with previous line moving cursor accordingly
      hints:
        level1: 'Insert: shift chars right, insert at cursor. Delete: shift chars left.'
        level2: 'Enter: split current line at cursor, insert new row.'
        level3: |-
          void insertChar(Row *row, int at, int c) {
            row->chars = realloc(row->chars, row->size + 2);
            memmove(&row->chars[at + 1], &row->chars[at], row->size - at + 1);
            row->chars[at] = c;
            row->size++;
          }
      pitfalls:
      - Cursor at end of line edge cases
      - Deleting at beginning of line (join with previous)
      - Memory reallocation
      concepts:
      - Text buffer operations
      - Gap buffer or simple array
      - Change tracking
      estimated_hours: 4-6
      deliverables:
      - Character insertion adding typed characters at current cursor position
      - Character deletion removing characters with backspace and delete keys
      - Line breaking splitting line at cursor position when Enter is pressed
      - Line joining merging current line with previous when backspacing at line start
    - id: 5
      name: Save and Undo
      description: Save file to disk and implement undo functionality.
      acceptance_criteria:
      - Save writes all buffer lines to file and clears the dirty/modified flag
      - Dirty flag is set on any edit and cleared on save, shown in status bar
      - Undo reverses the most recent edit operation restoring previous buffer state
      - Redo re-applies the last undone operation returning buffer to later state
      hints:
        level1: Convert rows back to string with newlines, write to file.
        level2: 'Undo: keep stack of operations. Each edit pushes inverse operation.'
        level3: |-
          Save and undo functionality in C:

          typedef enum {
              OP_INSERT_CHAR,
              OP_DELETE_CHAR,
              OP_INSERT_LINE,
              OP_DELETE_LINE,
              OP_JOIN_LINES
          } OperationType;

          typedef struct {
              OperationType type;
              int row, col;
              char ch;
              char *line;  // For line operations
          } Operation;

          #define UNDO_STACK_SIZE 1000
          Operation undo_stack[UNDO_STACK_SIZE];
          int undo_top = 0;
          int dirty = 0;

          void push_undo(Operation op) {
              if (undo_top < UNDO_STACK_SIZE) {
                  undo_stack[undo_top++] = op;
              }
              dirty = 1;
          }

          void editor_insert_char(int c) {
              // Push inverse operation for undo
              push_undo((Operation){
                  .type = OP_DELETE_CHAR,
                  .row = E.cy, .col = E.cx,
                  .ch = c
              });

              row_insert_char(&E.row[E.cy], E.cx, c);
              E.cx++;
          }

          void editor_undo() {
              if (undo_top == 0) return;

              Operation op = undo_stack[--undo_top];

              switch (op.type) {
                  case OP_INSERT_CHAR:
                      row_insert_char(&E.row[op.row], op.col, op.ch);
                      break;
                  case OP_DELETE_CHAR:
                      row_delete_char(&E.row[op.row], op.col);
                      break;
                  case OP_INSERT_LINE:
                      editor_insert_row(op.row, op.line, strlen(op.line));
                      free(op.line);
                      break;
                  case OP_DELETE_LINE:
                      editor_delete_row(op.row);
                      break;
              }
          }

          void editor_save() {
              if (E.filename == NULL) {
                  E.filename = editor_prompt("Save as: %s");
                  if (E.filename == NULL) return;
              }

              int len;
              char *buf = rows_to_string(&len);

              int fd = open(E.filename, O_RDWR | O_CREAT | O_TRUNC, 0644);
              if (fd != -1) {
                  if (write(fd, buf, len) == len) {
                      close(fd);
                      dirty = 0;
                      editor_set_status_message("%d bytes written", len);
                  }
              }
              free(buf);
          }
      pitfalls:
      - Handling write errors
      - Undo across multiple edits
      - Memory for undo history
      concepts:
      - File writing
      - Undo architectures
      - Command pattern
      estimated_hours: 3-4
      deliverables:
      - File saving writing buffer contents back to disk with confirmation message
      - Dirty flag tracking whether buffer has unsaved modifications
      - Undo stack recording edit operations for reversal on undo command
      - Redo stack allowing re-application of previously undone operations
    - id: 6
      name: Search
      description: Implement incremental search functionality.
      acceptance_criteria:
      - Incremental search updates highlighted match after each character typed in query
      - Forward search finds next occurrence after current cursor position wrapping to start
      - Backward search finds previous occurrence before cursor wrapping to document end
      - Escape key cancels search and restores cursor to original position before search began
      hints:
        level1: Simple strstr() for each line.
        level2: 'Incremental: search on each keypress, restore cursor on cancel.'
        level3: |-
          Incremental search in C:

          void editor_find() {
              char *query = editor_prompt("Search: %s (ESC to cancel)");
              if (query == NULL) return;

              int saved_cx = E.cx;
              int saved_cy = E.cy;
              int saved_coloff = E.coloff;
              int saved_rowoff = E.rowoff;

              int direction = 1;  // 1 = forward, -1 = backward
              int last_match = -1;

              for (;;) {
                  char *match = editor_find_next(query, &last_match, direction);

                  editor_refresh_screen();

                  int c = editor_read_key();

                  if (c == '\x1b' || c == '\r') {
                      // ESC or Enter - restore if ESC
                      if (c == '\x1b') {
                          E.cx = saved_cx;
                          E.cy = saved_cy;
                          E.coloff = saved_coloff;
                          E.rowoff = saved_rowoff;
                      }
                      break;
                  } else if (c == ARROW_DOWN || c == CTRL_KEY('n')) {
                      direction = 1;
                  } else if (c == ARROW_UP || c == CTRL_KEY('p')) {
                      direction = -1;
                  }
              }
              free(query);
          }

          char *editor_find_next(char *query, int *last_match, int direction) {
              int current = *last_match;

              for (int i = 0; i < E.numrows; i++) {
                  current += direction;
                  if (current == -1) current = E.numrows - 1;
                  else if (current == E.numrows) current = 0;

                  erow *row = &E.row[current];
                  char *match = strstr(row->render, query);

                  if (match) {
                      *last_match = current;
                      E.cy = current;
                      E.cx = match - row->render;
                      E.rowoff = E.numrows;  // Force scroll

                      // Highlight match
                      memset(&row->hl[E.cx], HL_MATCH, strlen(query));
                      return match;
                  }
              }
              return NULL;
          }
      pitfalls:
      - Search wrapping at end of file
      - Case sensitivity
      - Restoring state on cancel
      concepts:
      - Text searching
      - Incremental search UX
      - State management
      estimated_hours: 2-3
      deliverables:
      - Incremental search highlighting matches as user types search query
      - Forward and backward search navigation jumping between match occurrences
      - Search prompt in status bar area accepting query input and showing results
      - Search highlighting visually marking all matching occurrences in visible text
    - id: 7
      name: Syntax Highlighting
      description: Add syntax highlighting for common languages.
      acceptance_criteria:
      - File extension maps to correct syntax highlighting rules for that language
      - Keywords like if, else, for, while are displayed in distinct highlight color
      - String literals enclosed in quotes are highlighted including multi-line strings
      - Comments are highlighted differently from code including block comment spans
      hints:
        level1: For each row, track highlighting state per character.
        level2: 'State machine: normal, string, comment. Different color per state.'
        level3: |-
          Syntax highlighting state machine in C:

          enum Highlight {
              HL_NORMAL = 0,
              HL_COMMENT,
              HL_MLCOMMENT,
              HL_KEYWORD1,
              HL_KEYWORD2,
              HL_STRING,
              HL_NUMBER,
              HL_MATCH
          };

          struct EditorSyntax {
              char *filetype;
              char **filematch;
              char **keywords;
              char *singleline_comment_start;
              char *multiline_comment_start;
              char *multiline_comment_end;
              int flags;
          };

          char *C_extensions[] = {".c", ".h", ".cpp", NULL};
          char *C_keywords[] = {
              "switch", "if", "while", "for", "break", "continue", "return",
              "else", "struct", "union", "typedef", "enum", "class", "case",
              // Type keywords end with |
              "int|", "long|", "double|", "float|", "char|", "void|", "unsigned|",
              NULL
          };

          struct EditorSyntax HLDB[] = {
              {"c", C_extensions, C_keywords, "//", "/*", "*/", HL_HIGHLIGHT_NUMBERS | HL_HIGHLIGHT_STRINGS}
          };

          void editor_update_syntax(erow *row) {
              row->hl = realloc(row->hl, row->rsize);
              memset(row->hl, HL_NORMAL, row->rsize);

              if (E.syntax == NULL) return;

              int in_string = 0;
              int in_comment = row->idx > 0 ? E.row[row->idx - 1].hl_open_comment : 0;

              int i = 0;
              while (i < row->rsize) {
                  char c = row->render[i];
                  unsigned char prev_hl = (i > 0) ? row->hl[i - 1] : HL_NORMAL;

                  // Multi-line comments
                  if (in_comment) {
                      row->hl[i] = HL_MLCOMMENT;
                      if (!strncmp(&row->render[i], E.syntax->multiline_comment_end, 2)) {
                          row->hl[i + 1] = HL_MLCOMMENT;
                          i += 2;
                          in_comment = 0;
                      } else {
                          i++;
                      }
                      continue;
                  }

                  // Single-line comments
                  if (!in_string && !strncmp(&row->render[i], E.syntax->singleline_comment_start, 2)) {
                      memset(&row->hl[i], HL_COMMENT, row->rsize - i);
                      break;
                  }

                  // Strings
                  if (in_string) {
                      row->hl[i] = HL_STRING;
                      if (c == '\\' && i + 1 < row->rsize) {
                          row->hl[i + 1] = HL_STRING;
                          i += 2;
                          continue;
                      }
                      if (c == in_string) in_string = 0;
                      i++;
                      continue;
                  } else if (c == '"' || c == '\'') {
                      in_string = c;
                      row->hl[i] = HL_STRING;
                      i++;
                      continue;
                  }

                  // Keywords
                  if (is_separator(prev_hl)) {
                      for (int j = 0; E.syntax->keywords[j]; j++) {
                          // ... match keywords
                      }
                  }

                  i++;
              }
              row->hl_open_comment = in_comment;
          }
      pitfalls:
      - Multi-line strings and comments
      - Escape sequences in strings
      - Performance on large files
      concepts:
      - Syntax highlighting algorithms
      - State machines
      - ANSI color codes
      estimated_hours: 4-6
      deliverables:
      - Language detection selecting syntax rules based on file extension
      - Keyword highlighting coloring reserved words in distinct color
      - String and comment highlighting with multi-line span support
      - Number literal highlighting distinguishing numeric values from identifiers
  build-tls:
    id: build-tls
    name: Build Your Own TLS
    description: Implement TLS 1.3 handshake and record layer. Learn modern cryptographic protocols.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - HTTPS client
    - AES implementation
    - Elliptic curve basics
    languages:
      recommended:
      - Rust
      - Go
      - C
      also_possible:
      - Python
      - Java
    resources:
    - type: spec
      name: RFC 8446 - TLS 1.3
      url: https://datatracker.ietf.org/doc/html/rfc8446
    - type: book
      name: Illustrated TLS 1.3
      url: https://tls13.xargs.org/
    milestones:
    - id: 1
      name: Record Layer
      description: Implement TLS record protocol for fragmenting and encrypting data.
      acceptance_criteria:
      - Record parser extracts 5-byte header with content type, protocol version, and length
      - Messages larger than 16KB are split into multiple TLS records correctly
      - Content type field routes records to correct handler for handshake, alert, or data
      - Fragmented handshake messages are reassembled into complete messages before processing
      hints:
        level1: 'Records have 5-byte header: type(1), version(2), length(2). Max payload 16KB.'
        level2: In TLS 1.3, actual content type is encrypted. Outer shows 'application_data'.
        level3: "class TLSRecord:\n    MAX_PAYLOAD = 16384  # 2^14\n    \n    # Content types\n    CHANGE_CIPHER_SPEC = 20\n\
          \    ALERT = 21\n    HANDSHAKE = 22\n    APPLICATION_DATA = 23\n    \n    def __init__(self, content_type, payload):\n\
          \        self.content_type = content_type\n        self.payload = payload\n    \n    def encode(self):\n       \
          \ # TLS 1.3 uses legacy version 0x0303 (TLS 1.2)\n        return bytes([\n            self.content_type,\n     \
          \       0x03, 0x03,  # Legacy version\n            len(self.payload) >> 8,\n            len(self.payload) & 0xFF\n\
          \        ]) + self.payload\n    \n    @classmethod\n    def parse(cls, data):\n        if len(data) < 5:\n     \
          \       return None, data\n        \n        content_type = data[0]\n        version = (data[1] << 8) | data[2]\n\
          \        length = (data[3] << 8) | data[4]\n        \n        if len(data) < 5 + length:\n            return None,\
          \ data\n        \n        payload = data[5:5+length]\n        return cls(content_type, payload), data[5+length:]\n\
          \nclass RecordLayer:\n    def __init__(self):\n        self.read_cipher = None\n        self.write_cipher = None\n\
          \        self.read_seq = 0\n        self.write_seq = 0\n    \n    def encrypt_record(self, content_type, plaintext):\n\
          \        if self.write_cipher is None:\n            return TLSRecord(content_type, plaintext)\n        \n      \
          \  # TLS 1.3: append real content type to plaintext\n        inner_plaintext = plaintext + bytes([content_type])\n\
          \        \n        # Encrypt with AEAD\n        nonce = self._build_nonce(self.write_seq, self.write_iv)\n     \
          \   ciphertext = self.write_cipher.encrypt(nonce, inner_plaintext, b'')\n        \n        self.write_seq += 1\n\
          \        \n        # Outer record always shows APPLICATION_DATA\n        return TLSRecord(TLSRecord.APPLICATION_DATA,\
          \ ciphertext)"
      pitfalls:
      - Version field confusion
      - Padding in TLS 1.3
      - Sequence number overflow
      concepts:
      - Record protocol
      - AEAD encryption
      - Protocol layering
      estimated_hours: 8-12
      deliverables:
      - Record header parsing extracting content type, version, and length fields
      - Fragmentation handling splitting large messages across multiple records
      - Content type handling routing handshake, alert, and application data records
      - Record reassembly combining fragmented messages into complete protocol messages
    - id: 2
      name: Key Exchange
      description: Implement ECDHE key exchange with X25519.
      acceptance_criteria:
      - Diffie-Hellman exchange produces identical shared secret on both client and server
      - ECDHE key exchange completes successfully using X25519 or P-256 curve parameters
      - Key derivation produces separate encryption keys for client-to-server and server-to-client
      - Pre-master secret is securely derived and never transmitted in plaintext
      hints:
        level1: 'X25519: generate 32-byte private key, compute public key. Shared secret from ECDH.'
        level2: HKDF-Extract then HKDF-Expand for key derivation. Different keys for client/server.
        level3: "from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PrivateKey\nfrom cryptography.hazmat.primitives.kdf.hkdf\
          \ import HKDFExpand, HKDF\nfrom cryptography.hazmat.primitives import hashes\n\nclass KeyExchange:\n    def __init__(self):\n\
          \        self.private_key = X25519PrivateKey.generate()\n        self.public_key = self.private_key.public_key()\n\
          \    \n    def get_public_bytes(self):\n        return self.public_key.public_bytes_raw()\n    \n    def compute_shared_secret(self,\
          \ peer_public_bytes):\n        from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PublicKey\n  \
          \      peer_key = X25519PublicKey.from_public_bytes(peer_public_bytes)\n        return self.private_key.exchange(peer_key)\n\
          \nclass KeySchedule:\n    def __init__(self, shared_secret, transcript_hash):\n        # Early secret (no PSK)\n\
          \        self.early_secret = self._hkdf_extract(b'\\x00' * 32, b'')\n        \n        # Handshake secret\n    \
          \    derived = self._derive_secret(self.early_secret, b'derived', b'')\n        self.handshake_secret = self._hkdf_extract(derived,\
          \ shared_secret)\n        \n        # Traffic secrets\n        self.client_handshake_secret = self._derive_secret(\n\
          \            self.handshake_secret, b'c hs traffic', transcript_hash)\n        self.server_handshake_secret = self._derive_secret(\n\
          \            self.handshake_secret, b's hs traffic', transcript_hash)\n    \n    def _hkdf_extract(self, salt, ikm):\n\
          \        import hmac\n        return hmac.new(salt, ikm, 'sha256').digest()\n    \n    def _derive_secret(self,\
          \ secret, label, context):\n        return self._hkdf_expand_label(secret, label, context, 32)\n    \n    def _hkdf_expand_label(self,\
          \ secret, label, context, length):\n        # TLS 1.3 specific label format\n        full_label = b'tls13 ' + label\n\
          \        hkdf_label = (\n            length.to_bytes(2, 'big') +\n            bytes([len(full_label)]) + full_label\
          \ +\n            bytes([len(context)]) + context\n        )\n        hkdf = HKDFExpand(hashes.SHA256(), length,\
          \ hkdf_label)\n        return hkdf.derive(secret)"
      pitfalls:
      - Endianness in key encoding
      - Transcript hash timing
      - Label format details
      concepts:
      - ECDH
      - Key derivation
      - Forward secrecy
      estimated_hours: 12-18
      deliverables:
      - Diffie-Hellman key exchange generating shared secret from public parameters
      - Elliptic curve key exchange using ECDHE with supported curve negotiation
      - Key derivation function expanding shared secret into encryption key material
      - Pre-master secret computation from selected key exchange algorithm output
    - id: 3
      name: Handshake Protocol
      description: Implement TLS 1.3 full handshake flow.
      acceptance_criteria:
      - ClientHello includes supported TLS versions, cipher suites, and SNI extension
      - ServerHello response selects mutually supported cipher suite and key exchange method
      - Handshake state machine rejects out-of-order messages with appropriate alert
      - Finished message verify data matches expected hash of all preceding handshake messages
      hints:
        level1: 'ClientHello: version, random, cipher suites, extensions (key_share, supported_versions).'
        level2: After ServerHello, switch to encrypted handshake. Verify Finished with HMAC.
        level3: "class Handshake:\n    # Handshake types\n    CLIENT_HELLO = 1\n    SERVER_HELLO = 2\n    ENCRYPTED_EXTENSIONS\
          \ = 8\n    CERTIFICATE = 11\n    CERTIFICATE_VERIFY = 15\n    FINISHED = 20\n    \n    def __init__(self):\n   \
          \     self.transcript = b''  # All handshake messages\n        self.key_exchange = KeyExchange()\n    \n    def\
          \ create_client_hello(self):\n        # Random\n        client_random = os.urandom(32)\n        \n        # Cipher\
          \ suites (TLS_AES_128_GCM_SHA256)\n        cipher_suites = bytes([0x00, 0x02, 0x13, 0x01])\n        \n        #\
          \ Extensions\n        extensions = b''\n        \n        # supported_versions\n        extensions += self._extension(43,\
          \ bytes([0x02, 0x03, 0x04]))  # TLS 1.3\n        \n        # key_share (X25519)\n        key_share_data = (\n  \
          \          bytes([0x00, 0x1d]) +  # X25519\n            bytes([0x00, 0x20]) +  # Key length\n            self.key_exchange.get_public_bytes()\n\
          \        )\n        extensions += self._extension(51, bytes([0x00, len(key_share_data)]) + key_share_data)\n   \
          \     \n        # signature_algorithms\n        extensions += self._extension(13, bytes([0x00, 0x04, 0x04, 0x03,\
          \ 0x08, 0x04]))  # ECDSA, RSA-PSS\n        \n        # Build ClientHello\n        body = (\n            bytes([0x03,\
          \ 0x03]) +  # Legacy version (TLS 1.2)\n            client_random +\n            bytes([0x00]) +  # Session ID length\n\
          \            cipher_suites +\n            bytes([0x01, 0x00]) +  # Compression (null)\n            len(extensions).to_bytes(2,\
          \ 'big') + extensions\n        )\n        \n        msg = bytes([self.CLIENT_HELLO]) + len(body).to_bytes(3, 'big')\
          \ + body\n        self.transcript += msg\n        return msg\n    \n    def process_server_hello(self, data):\n\
          \        # Parse and extract key_share\n        # ... update transcript\n        # Derive handshake keys\n     \
          \   pass\n    \n    def verify_finished(self, received_verify_data):\n        # Compute expected verify_data\n \
          \       transcript_hash = hashlib.sha256(self.transcript).digest()\n        finished_key = self.key_schedule._hkdf_expand_label(\n\
          \            self.key_schedule.server_handshake_secret,\n            b'finished', b'', 32\n        )\n        expected\
          \ = hmac.new(finished_key, transcript_hash, 'sha256').digest()\n        return hmac.compare_digest(received_verify_data,\
          \ expected)"
      pitfalls:
      - Extension ordering
      - Transcript hash calculation
      - Cipher suite negotiation
      concepts:
      - Protocol negotiation
      - Message authentication
      - State machine
      estimated_hours: 15-25
      deliverables:
      - ClientHello message builder with supported cipher suites and extensions
      - ServerHello message parser extracting selected cipher suite and parameters
      - Handshake state machine tracking progression through handshake message sequence
      - Finished message verification confirming both parties derived matching keys
    - id: 4
      name: Certificate Verification
      description: Verify server certificate chain and signature.
      acceptance_criteria:
      - X.509 parser extracts subject name, issuer name, validity dates, and public key
      - Chain builder constructs path from server certificate to trusted root certificate
      - Signature verification confirms each certificate was signed by its issuer's key
      - Expired or not-yet-valid certificates are rejected with appropriate error message
      hints:
        level1: Parse certificates from DER. Verify chain up to trusted root.
        level2: CertificateVerify signs transcript hash. Check signature algorithm.
        level3: "from cryptography import x509\nfrom cryptography.hazmat.primitives.asymmetric import padding, ec\n\nclass\
          \ CertificateVerifier:\n    def __init__(self, trusted_roots):\n        self.trusted_roots = trusted_roots  # List\
          \ of CA certs\n    \n    def verify_chain(self, cert_chain, hostname):\n        if not cert_chain:\n           \
          \ raise TLSError('Empty certificate chain')\n        \n        # Parse certificates\n        certs = [x509.load_der_x509_certificate(c)\
          \ for c in cert_chain]\n        \n        # Verify hostname\n        leaf = certs[0]\n        self._verify_hostname(leaf,\
          \ hostname)\n        \n        # Build and verify chain\n        for i in range(len(certs) - 1):\n            issuer\
          \ = certs[i + 1]\n            self._verify_signature(certs[i], issuer)\n        \n        # Check root is trusted\n\
          \        root = certs[-1]\n        if not self._is_trusted(root):\n            raise TLSError('Untrusted root certificate')\n\
          \        \n        return leaf.public_key()\n    \n    def _verify_hostname(self, cert, hostname):\n        # Check\
          \ SAN extension\n        try:\n            san = cert.extensions.get_extension_for_class(x509.SubjectAlternativeName)\n\
          \            names = san.value.get_values_for_type(x509.DNSName)\n            if hostname not in names and not self._matches_wildcard(hostname,\
          \ names):\n                raise TLSError(f'Hostname {hostname} not in certificate')\n        except x509.ExtensionNotFound:\n\
          \            # Fall back to CN\n            cn = cert.subject.get_attributes_for_oid(x509.oid.NameOID.COMMON_NAME)\n\
          \            if not cn or cn[0].value != hostname:\n                raise TLSError('Hostname mismatch')\n    \n\
          \    def verify_certificate_verify(self, signature, algorithm, transcript_hash, public_key):\n        # TLS 1.3\
          \ signature context\n        context = b' ' * 64 + b'TLS 1.3, server CertificateVerify' + b'\\x00' + transcript_hash\n\
          \        \n        if isinstance(public_key, ec.EllipticCurvePublicKey):\n            public_key.verify(signature,\
          \ context, ec.ECDSA(hashes.SHA256()))\n        else:\n            public_key.verify(signature, context, padding.PSS(...))"
      pitfalls:
      - Chain validation order
      - Wildcard matching
      - Signature algorithm selection
      concepts:
      - PKI
      - X.509
      - Certificate validation
      estimated_hours: 15-25
      deliverables:
      - X.509 certificate parsing extracting subject, issuer, and public key fields
      - Certificate chain building linking server certificate to trusted root CA
      - Signature verification validating each certificate signature in the chain
      - Certificate validity checking including expiration dates and revocation status
  build-transformer:
    id: build-transformer
    name: Build Your Own Transformer
    description: Implement a GPT-style transformer from scratch.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Neural networks
    - Attention mechanism basics
    - NLP fundamentals
    - PyTorch/TensorFlow
    languages:
      recommended:
      - Python
      also_possible:
      - Julia
      - Rust
    resources:
    - type: paper
      name: Attention Is All You Need
      url: https://arxiv.org/abs/1706.03762
    - type: video
      name: Let's build GPT by Karpathy
      url: https://www.youtube.com/watch?v=kCc8FmEb1nY
    - type: code
      name: minGPT
      url: https://github.com/karpathy/minGPT
    milestones:
    - id: 1
      name: Self-Attention
      description: Implement scaled dot-product attention.
      acceptance_criteria:
      - Q, K, V projections produce matrices of correct dimensions for each attention head
      - Attention scores are scaled by square root of key dimension before softmax
      - Multi-head attention concatenates outputs from all heads and projects to output dimension
      - Causal attention mask sets future position scores to negative infinity before softmax
      hints:
        level1: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V
        level2: 'For autoregressive: mask future positions with -inf before softmax.'
        level3: "## Self-Attention\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport\
          \ math\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n\
          \        assert embed_dim % num_heads == 0\n        \n        self.embed_dim = embed_dim\n        self.num_heads\
          \ = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.q_proj = nn.Linear(embed_dim,\
          \ embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim,\
          \ embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n    \n    def forward(self, x, mask=None):\n\
          \        batch_size, seq_len, _ = x.shape\n        \n        # Project to Q, K, V\n        Q = self.q_proj(x)  #\
          \ (batch, seq, embed)\n        K = self.k_proj(x)\n        V = self.v_proj(x)\n        \n        # Reshape for multi-head:\
          \ (batch, heads, seq, head_dim)\n        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,\
          \ 2)\n        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size,\
          \ seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Scaled dot-product attention\n  \
          \      scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        # Apply causal\
          \ mask\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n      \
          \  \n        attn_weights = F.softmax(scores, dim=-1)\n        \n        # Apply attention to values\n        context\
          \ = torch.matmul(attn_weights, V)  # (batch, heads, seq, head_dim)\n        \n        # Concatenate heads\n    \
          \    context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n        \n      \
          \  return self.out_proj(context)\n\ndef create_causal_mask(seq_len):\n    # Lower triangular matrix (1s where we\
          \ can attend)\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask.unsqueeze(0).unsqueeze(0)  #\
          \ (1, 1, seq, seq)\n```"
      pitfalls:
      - Wrong attention dimension
      - Forgetting to scale
      - Mask applied after softmax
      concepts:
      - Attention mechanism
      - Multi-head attention
      - Masking
      estimated_hours: 10-15
      deliverables:
      - Query, key, value projection matrices transforming input embeddings
      - Scaled dot-product attention computing weighted sum of value vectors
      - Multi-head attention splitting computation across parallel attention heads
      - Attention mask preventing positions from attending to future tokens
    - id: 2
      name: Transformer Block
      description: Build a complete transformer block with FFN and normalization.
      acceptance_criteria:
      - Feed-forward network expands to 4x hidden dimension then projects back to original
      - Layer normalization normalizes activations to zero mean and unit variance per token
      - Residual connections preserve gradient flow by adding sub-layer input to its output
      - Dropout randomly zeros elements with configurable probability during training only
      hints:
        level1: 'Block: x -> LayerNorm -> Attention -> Residual -> LayerNorm -> FFN -> Residual'
        level2: FFN typically expands 4x then projects back. Use GELU activation.
        level3: "## Transformer Block\n\n```python\nclass FeedForward(nn.Module):\n    def __init__(self, embed_dim, hidden_dim,\
          \ dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n        self.fc2\
          \ = nn.Linear(hidden_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n\
          \        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return\
          \ x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n\
          \        super().__init__()\n        self.attention = SelfAttention(embed_dim, num_heads)\n        self.ffn = FeedForward(embed_dim,\
          \ ff_hidden_dim, dropout)\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n\
          \        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n    \n    def forward(self,\
          \ x, mask=None):\n        # Pre-norm architecture (GPT-2 style)\n        attn_out = self.attention(self.ln1(x),\
          \ mask)\n        x = x + self.dropout1(attn_out)\n        \n        ffn_out = self.ffn(self.ln2(x))\n        x =\
          \ x + self.dropout2(ffn_out)\n        \n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, vocab_size,\
          \ embed_dim, num_heads, num_layers, max_seq_len, dropout=0.1):\n        super().__init__()\n        self.embed_dim\
          \ = embed_dim\n        self.max_seq_len = max_seq_len\n        \n        self.token_embedding = nn.Embedding(vocab_size,\
          \ embed_dim)\n        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\
          \        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, embed_dim *\
          \ 4, dropout)\n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(embed_dim)\n\
          \        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n        \n        # Weight tying\n       \
          \ self.lm_head.weight = self.token_embedding.weight\n    \n    def forward(self, idx):\n        batch_size, seq_len\
          \ = idx.shape\n        \n        # Embeddings\n        tok_emb = self.token_embedding(idx)\n        pos = torch.arange(0,\
          \ seq_len, device=idx.device)\n        pos_emb = self.position_embedding(pos)\n        x = self.dropout(tok_emb\
          \ + pos_emb)\n        \n        # Causal mask\n        mask = create_causal_mask(seq_len).to(idx.device)\n     \
          \   \n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x, mask)\n        \n\
          \        x = self.ln_final(x)\n        logits = self.lm_head(x)\n        \n        return logits\n```"
      pitfalls:
      - Pre-norm vs post-norm confusion
      - Missing residual connections
      - Wrong FFN expansion ratio
      concepts:
      - Transformer architecture
      - Layer normalization
      - Residual learning
      estimated_hours: 12-18
      deliverables:
      - Feed-forward network with two linear layers and activation between them
      - Layer normalization stabilizing activations with learned scale and shift
      - Residual connections adding input to output of each sub-layer
      - Dropout layer randomly zeroing activations during training for regularization
    - id: 3
      name: Training Pipeline
      description: Implement training with language modeling objective.
      acceptance_criteria:
      - Tokenizer splits text into subword or character tokens and maps to integer IDs
      - Data loader yields batches of fixed-length token sequences with next-token targets
      - Cross-entropy loss decreases consistently over training epochs on training data
      - Training loop logs loss at regular intervals showing convergence progress
      hints:
        level1: 'Shift labels by 1: predict token[i+1] from token[0:i].'
        level2: Use cosine learning rate schedule with warmup.
        level3: "## Training\n\n```python\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\n\n\
          class GPTTrainer:\n    def __init__(self, model, train_data, config):\n        self.model = model\n        self.train_data\
          \ = train_data\n        self.config = config\n        \n        self.optimizer = optim.AdamW(\n            model.parameters(),\n\
          \            lr=config.learning_rate,\n            betas=(0.9, 0.95),\n            weight_decay=config.weight_decay\n\
          \        )\n        \n        self.scaler = GradScaler()  # For mixed precision\n    \n    def get_lr(self, step):\n\
          \        # Linear warmup then cosine decay\n        warmup_steps = self.config.warmup_steps\n        max_steps =\
          \ self.config.max_steps\n        min_lr = self.config.learning_rate / 10\n        \n        if step < warmup_steps:\n\
          \            return self.config.learning_rate * step / warmup_steps\n        elif step > max_steps:\n          \
          \  return min_lr\n        else:\n            decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n\
          \            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n            return min_lr + coeff * (self.config.learning_rate\
          \ - min_lr)\n    \n    def train_step(self, batch, step):\n        # Update learning rate\n        lr = self.get_lr(step)\n\
          \        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        \n       \
          \ self.model.train()\n        x, y = batch  # x: input tokens, y: target tokens (shifted by 1)\n        \n     \
          \   with autocast():  # Mixed precision\n            logits = self.model(x)\n            loss = F.cross_entropy(\n\
          \                logits.view(-1, logits.size(-1)),\n                y.view(-1),\n                ignore_index=-100\
          \  # Padding token\n            )\n        \n        self.optimizer.zero_grad()\n        self.scaler.scale(loss).backward()\n\
          \        \n        # Gradient clipping\n        self.scaler.unscale_(self.optimizer)\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(),\
          \ self.config.grad_clip)\n        \n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n   \
          \     \n        return loss.item()\n    \n    def train(self):\n        step = 0\n        for epoch in range(self.config.epochs):\n\
          \            for batch in self.train_data:\n                loss = self.train_step(batch, step)\n              \
          \  step += 1\n                \n                if step % 100 == 0:\n                    print(f\"Step {step}, Loss:\
          \ {loss:.4f}, LR: {self.get_lr(step):.6f}\")\n                \n                if step >= self.config.max_steps:\n\
          \                    return\n```"
      pitfalls:
      - Label shifting wrong
      - No gradient clipping causing instability
      - Wrong loss computation
      concepts:
      - Language modeling
      - Learning rate scheduling
      - Mixed precision
      estimated_hours: 12-18
      deliverables:
      - Tokenizer converting text strings into integer token sequences
      - Data loader producing batched token sequences for training iteration
      - Cross-entropy loss computing prediction error against target tokens
      - Training loop with gradient computation, optimizer step, and loss logging
    - id: 4
      name: Text Generation
      description: Implement autoregressive text generation.
      acceptance_criteria:
      - Greedy decoding produces deterministic output by always selecting most likely token
      - Temperature parameter scales logits so lower values produce more focused text
      - Top-k sampling restricts candidates to k highest-probability tokens before sampling
      - Generated text is coherent at short lengths after training on sufficient data
      hints:
        level1: Generate one token at a time, append to input, repeat.
        level2: 'KV cache: store key/value from previous tokens to avoid recomputation.'
        level3: "## Text Generation\n\n```python\nclass GPT(nn.Module):\n    # ... previous code ...\n    \n    @torch.no_grad()\n\
          \    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None):\n        for _ in range(max_new_tokens):\n\
          \            # Crop to max sequence length\n            idx_cond = idx if idx.size(1) <= self.max_seq_len else idx[:,\
          \ -self.max_seq_len:]\n            \n            # Forward pass\n            logits = self(idx_cond)\n         \
          \   logits = logits[:, -1, :] / temperature  # Last position only\n            \n            # Apply top-k\n   \
          \         if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n      \
          \          logits[logits < v[:, [-1]]] = float('-inf')\n            \n            # Apply top-p (nucleus sampling)\n\
          \            if top_p is not None:\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n\
          \                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n                \n \
          \               # Remove tokens with cumulative probability above threshold\n                sorted_indices_to_remove\
          \ = cumulative_probs > top_p\n                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n\
          \                sorted_indices_to_remove[:, 0] = 0\n                \n                indices_to_remove = sorted_indices_to_remove.scatter(1,\
          \ sorted_indices, sorted_indices_to_remove)\n                logits[indices_to_remove] = float('-inf')\n       \
          \     \n            # Sample\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs,\
          \ num_samples=1)\n            \n            idx = torch.cat((idx, idx_next), dim=1)\n        \n        return idx\n\
          \n# KV Cache for efficient generation\nclass CachedSelfAttention(nn.Module):\n    def __init__(self, embed_dim,\
          \ num_heads):\n        super().__init__()\n        # ... same as SelfAttention ...\n    \n    def forward(self,\
          \ x, kv_cache=None):\n        batch_size, seq_len, _ = x.shape\n        \n        Q = self.q_proj(x)\n        K\
          \ = self.k_proj(x)\n        V = self.v_proj(x)\n        \n        if kv_cache is not None:\n            # Append\
          \ new K, V to cache\n            cached_k, cached_v = kv_cache\n            K = torch.cat([cached_k, K], dim=1)\n\
          \            V = torch.cat([cached_v, V], dim=1)\n        \n        new_cache = (K, V)\n        \n        # Reshape\
          \ for multi-head\n        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K =\
          \ K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads,\
          \ self.head_dim).transpose(1, 2)\n        \n        # Attention (Q only attends to available K, V)\n        scores\
          \ = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attn_weights = F.softmax(scores, dim=-1)\n\
          \        context = torch.matmul(attn_weights, V)\n        \n        context = context.transpose(1, 2).contiguous().view(batch_size,\
          \ -1, self.embed_dim)\n        return self.out_proj(context), new_cache\n```"
      pitfalls:
      - Repetitive text without sampling
      - KV cache dimension mismatch
      - Temperature of 0 causing division
      concepts:
      - Autoregressive generation
      - Sampling strategies
      - KV caching
      estimated_hours: 12-18
      deliverables:
      - Greedy decoding selecting highest-probability token at each generation step
      - Temperature-based sampling controlling randomness of generated text
      - Top-k and top-p sampling filtering token candidates before sampling
      - Text generation loop producing sequences from given prompt string
  build-web-framework:
    id: build-web-framework
    name: Build Your Own Web Framework
    description: Build a web framework like Express or Django with routing, middleware, and templating.
    difficulty: expert
    estimated_hours: 40-70
    prerequisites:
    - HTTP server
    - Request/response handling
    - Basic routing
    languages:
      recommended:
      - JavaScript/Node.js
      - Python
      - Go
      also_possible:
      - Ruby
      - Rust
    resources:
    - type: code
      name: Express.js source
      url: https://github.com/expressjs/express
    - type: article
      name: Build Express from Scratch
      url: https://www.freecodecamp.org/news/express-explained-with-examples-installation-routing-middleware-and-more/
    milestones:
    - id: 1
      name: Routing
      description: Implement URL routing with parameters and methods.
      acceptance_criteria:
      - Route registration associates GET/POST/PUT/DELETE method with URL pattern and handler
      - URL parameters like /users/:id extract named values from matching path segments
      - Route matching returns correct handler or 404 response for unmatched paths
      - Route groups share common prefix so /api/users and /api/posts share /api prefix
      hints:
        level1: Store routes as array of { method, pattern, handler }. Match in order.
        level2: Convert path patterns to regex. Capture groups for parameters.
        level3: "class Router {\n    constructor() {\n        this.routes = [];\n    }\n    \n    addRoute(method, path, handler)\
          \ {\n        const { pattern, paramNames } = this.compilePath(path);\n        this.routes.push({ method, pattern,\
          \ paramNames, handler });\n    }\n    \n    compilePath(path) {\n        const paramNames = [];\n        \n    \
          \    // Convert /users/:id/posts/:postId to regex\n        const pattern = path.replace(/:([^/]+)/g, (_, name) =>\
          \ {\n            paramNames.push(name);\n            return '([^/]+)';\n        });\n        \n        return {\n\
          \            pattern: new RegExp(`^${pattern}$`),\n            paramNames\n        };\n    }\n    \n    match(method,\
          \ url) {\n        const [path, queryString] = url.split('?');\n        \n        for (const route of this.routes)\
          \ {\n            if (route.method !== method && route.method !== 'ALL') continue;\n            \n            const\
          \ match = path.match(route.pattern);\n            if (match) {\n                // Extract params\n            \
          \    const params = {};\n                route.paramNames.forEach((name, i) => {\n                    params[name]\
          \ = decodeURIComponent(match[i + 1]);\n                });\n                \n                // Parse query string\n\
          \                const query = this.parseQuery(queryString);\n                \n                return { handler:\
          \ route.handler, params, query };\n            }\n        }\n        \n        return null;\n    }\n    \n    parseQuery(queryString)\
          \ {\n        if (!queryString) return {};\n        \n        return queryString.split('&').reduce((acc, pair) =>\
          \ {\n            const [key, value] = pair.split('=').map(decodeURIComponent);\n            acc[key] = value;\n\
          \            return acc;\n        }, {});\n    }\n    \n    // Convenience methods\n    get(path, handler) { this.addRoute('GET',\
          \ path, handler); }\n    post(path, handler) { this.addRoute('POST', path, handler); }\n    put(path, handler) {\
          \ this.addRoute('PUT', path, handler); }\n    delete(path, handler) { this.addRoute('DELETE', path, handler); }\n\
          }"
      pitfalls:
      - Route ordering matters
      - URL encoding
      - Trailing slashes
      concepts:
      - URL routing
      - Pattern matching
      - HTTP methods
      estimated_hours: 6-10
      deliverables:
      - Route registration mapping HTTP method and URL pattern to handler functions
      - URL parameter extraction parsing named segments from request path
      - Route matching selecting correct handler for incoming request method and path
      - Route group prefixing organizing related routes under common URL prefix
    - id: 2
      name: Middleware
      description: Implement middleware pipeline for request processing.
      acceptance_criteria:
      - Middleware executes in registration order before request reaches route handler
      - Calling next() passes control to the next middleware or final route handler
      - Logging middleware logs HTTP method, URL path, status code, and response duration
      - Error middleware catches unhandled exceptions and returns 500 response with message
      hints:
        level1: Middleware is function(req, res, next). Call next() to continue chain.
        level2: 'Error middleware has 4 params: (err, req, res, next). Skip to error handlers on throw.'
        level3: "class Application {\n    constructor() {\n        this.middleware = [];\n        this.router = new Router();\n\
          \    }\n    \n    use(pathOrFn, fn) {\n        if (typeof pathOrFn === 'function') {\n            this.middleware.push({\
          \ path: '/', handler: pathOrFn });\n        } else {\n            this.middleware.push({ path: pathOrFn, handler:\
          \ fn });\n        }\n    }\n    \n    async handleRequest(req, res) {\n        // Enhance request/response\n   \
          \     req.params = {};\n        req.query = {};\n        res.json = (data) => {\n            res.setHeader('Content-Type',\
          \ 'application/json');\n            res.end(JSON.stringify(data));\n        };\n        res.status = (code) => {\
          \ res.statusCode = code; return res; };\n        \n        // Collect applicable middleware\n        const stack\
          \ = [\n            ...this.middleware.filter(m => req.url.startsWith(m.path)),\n            { handler: this.routeHandler.bind(this)\
          \ }\n        ];\n        \n        let index = 0;\n        let error = null;\n        \n        const next = async\
          \ (err) => {\n            if (err) error = err;\n            \n            while (index < stack.length) {\n    \
          \            const layer = stack[index++];\n                const handler = layer.handler;\n                \n \
          \               try {\n                    if (error) {\n                        // Error handler has 4 params\n\
          \                        if (handler.length === 4) {\n                            await handler(error, req, res,\
          \ next);\n                            error = null;\n                            return;\n                     \
          \   }\n                    } else {\n                        if (handler.length < 4) {\n                       \
          \     await handler(req, res, next);\n                            return;\n                        }\n         \
          \           }\n                } catch (e) {\n                    error = e;\n                }\n            }\n\
          \            \n            // Unhandled error\n            if (error) {\n                res.statusCode = 500;\n\
          \                res.end('Internal Server Error');\n            }\n        };\n        \n        await next();\n\
          \    }\n    \n    routeHandler(req, res, next) {\n        const match = this.router.match(req.method, req.url);\n\
          \        if (match) {\n            req.params = match.params;\n            req.query = match.query;\n          \
          \  match.handler(req, res, next);\n        } else {\n            res.statusCode = 404;\n            res.end('Not\
          \ Found');\n        }\n    }\n}"
      pitfalls:
      - Calling next multiple times
      - Async error handling
      - Middleware ordering
      concepts:
      - Middleware pattern
      - Pipeline
      - Error propagation
      estimated_hours: 10-15
      deliverables:
      - Middleware chain processing request through ordered sequence of middleware functions
      - next() function passing control from current middleware to next in chain
      - Built-in logging middleware recording request method, path, and response time
      - Error handling middleware catching exceptions and returning error responses
    - id: 3
      name: Request/Response Enhancement
      description: Add convenience methods and body parsing.
      acceptance_criteria:
      - JSON body parser decodes request body and makes parsed object available on request
      - Response JSON helper sets Content-Type header and serializes object to JSON body
      - Cookie methods read incoming cookies by name and set outgoing cookies with options
      - Query parameters are parsed from URL and accessible as key-value dictionary
      hints:
        level1: Body comes as stream. Buffer chunks, parse based on Content-Type.
        level2: Add methods to res object for common patterns. Handle content negotiation.
        level3: "// Body parser middleware\nfunction bodyParser() {\n    return async (req, res, next) => {\n        const\
          \ contentType = req.headers['content-type'] || '';\n        \n        // Collect body\n        const chunks = [];\n\
          \        for await (const chunk of req) {\n            chunks.push(chunk);\n        }\n        const body = Buffer.concat(chunks).toString();\n\
          \        \n        if (contentType.includes('application/json')) {\n            try {\n                req.body\
          \ = JSON.parse(body);\n            } catch (e) {\n                req.body = {};\n            }\n        } else\
          \ if (contentType.includes('application/x-www-form-urlencoded')) {\n            req.body = Object.fromEntries(new\
          \ URLSearchParams(body));\n        } else {\n            req.body = body;\n        }\n        \n        next();\n\
          \    };\n}\n\n// Response enhancements\nfunction enhanceResponse(res) {\n    res.json = function(data) {\n     \
          \   this.setHeader('Content-Type', 'application/json');\n        this.end(JSON.stringify(data));\n        return\
          \ this;\n    };\n    \n    res.send = function(data) {\n        if (typeof data === 'object') {\n            return\
          \ this.json(data);\n        }\n        this.setHeader('Content-Type', 'text/html');\n        this.end(String(data));\n\
          \        return this;\n    };\n    \n    res.redirect = function(url, status = 302) {\n        this.statusCode =\
          \ status;\n        this.setHeader('Location', url);\n        this.end();\n        return this;\n    };\n    \n \
          \   res.cookie = function(name, value, options = {}) {\n        let cookie = `${name}=${encodeURIComponent(value)}`;\n\
          \        if (options.maxAge) cookie += `; Max-Age=${options.maxAge}`;\n        if (options.httpOnly) cookie += ';\
          \ HttpOnly';\n        if (options.secure) cookie += '; Secure';\n        if (options.path) cookie += `; Path=${options.path}`;\n\
          \        this.setHeader('Set-Cookie', cookie);\n        return this;\n    };\n}\n\n// Cookie parser\nfunction cookieParser()\
          \ {\n    return (req, res, next) => {\n        req.cookies = {};\n        const cookieHeader = req.headers.cookie;\n\
          \        if (cookieHeader) {\n            cookieHeader.split(';').forEach(cookie => {\n                const [name,\
          \ value] = cookie.trim().split('=');\n                req.cookies[name] = decodeURIComponent(value);\n         \
          \   });\n        }\n        next();\n    };\n}"
      pitfalls:
      - Large body handling
      - Content-Type edge cases
      - Cookie security
      concepts:
      - Request parsing
      - Response helpers
      - HTTP cookies
      estimated_hours: 8-12
      deliverables:
      - Request body parsing supporting JSON, form-urlencoded, and multipart formats
      - Response helpers for sending JSON, HTML, redirects, and file downloads
      - Cookie handling for reading request cookies and setting response cookies
      - Query parameter parsing extracting key-value pairs from URL query string
    - id: 4
      name: Template Engine
      description: Implement a simple template engine for HTML rendering.
      acceptance_criteria:
      - Template variables like {{ name }} are replaced with corresponding context values
      - HTML special characters in variable output are escaped to prevent XSS attacks
      - If/else blocks conditionally include template sections based on context values
      - Child templates extend base template overriding only defined block sections
      hints:
        level1: Replace {{ variable }} with values. Compile template to function for performance.
        level2: Parse control structures into AST. Generate JavaScript code to build string.
        level3: "class TemplateEngine {\n    constructor(options = {}) {\n        this.cache = new Map();\n        this.viewsDir\
          \ = options.views || './views';\n    }\n    \n    compile(template) {\n        // Convert template to function\n\
          \        let code = 'let __output = [];\\n';\n        let cursor = 0;\n        \n        // Match {{ }}, {% %},\
          \ {# #}\n        const regex = /\\{\\{([\\s\\S]+?)\\}\\}|\\{%([\\s\\S]+?)%\\}|\\{#[\\s\\S]+?#\\}/g;\n        let\
          \ match;\n        \n        while ((match = regex.exec(template)) !== null) {\n            // Add literal text before\
          \ match\n            if (match.index > cursor) {\n                const text = template.slice(cursor, match.index);\n\
          \                code += `__output.push(${JSON.stringify(text)});\\n`;\n            }\n            \n          \
          \  if (match[1]) {\n                // {{ expression }} - output with escaping\n                const expr = match[1].trim();\n\
          \                code += `__output.push(__escape(${expr}));\\n`;\n            } else if (match[2]) {\n         \
          \       // {% statement %}\n                const stmt = match[2].trim();\n                \n                if\
          \ (stmt.startsWith('if ')) {\n                    code += `if (${stmt.slice(3)}) {\\n`;\n                } else\
          \ if (stmt === 'endif') {\n                    code += '}\\n';\n                } else if (stmt.startsWith('for\
          \ ')) {\n                    // {% for item in items %}\n                    const forMatch = stmt.match(/for (\\\
          w+) in (\\w+)/);\n                    if (forMatch) {\n                        code += `for (const ${forMatch[1]}\
          \ of ${forMatch[2]}) {\\n`;\n                    }\n                } else if (stmt === 'endfor') {\n          \
          \          code += '}\\n';\n                } else if (stmt.startsWith('include ')) {\n                    const\
          \ includePath = stmt.slice(8).trim().replace(/['\"`]/g, '');\n                    code += `__output.push(__include('${includePath}',\
          \ __context));\\n`;\n                }\n            }\n            // {# comments #} are ignored\n            \n\
          \            cursor = match.index + match[0].length;\n        }\n        \n        // Add remaining text\n     \
          \   if (cursor < template.length) {\n            code += `__output.push(${JSON.stringify(template.slice(cursor))});\\\
          n`;\n        }\n        \n        code += 'return __output.join(\"\");';\n        \n        // Create function\n\
          \        return new Function('__context', '__escape', '__include', `\n            with (__context) {\n         \
          \       ${code}\n            }\n        `);\n    }\n    \n    render(templatePath, context = {}) {\n        if (!this.cache.has(templatePath))\
          \ {\n            const fullPath = path.join(this.viewsDir, templatePath);\n            const template = fs.readFileSync(fullPath,\
          \ 'utf-8');\n            this.cache.set(templatePath, this.compile(template));\n        }\n        \n        const\
          \ fn = this.cache.get(templatePath);\n        \n        const escape = (str) => String(str)\n            .replace(/&/g,\
          \ '&amp;')\n            .replace(/</g, '&lt;')\n            .replace(/>/g, '&gt;')\n            .replace(/\"/g,\
          \ '&quot;');\n        \n        const include = (path, ctx) => this.render(path, ctx);\n        \n        return\
          \ fn(context, escape, include);\n    }\n}"
      pitfalls:
      - XSS prevention
      - Template injection
      - Performance with large templates
      concepts:
      - Template compilation
      - Code generation
      - HTML escaping
      estimated_hours: 16-33
      deliverables:
      - Variable interpolation substituting template placeholders with provided values
      - HTML escaping preventing cross-site scripting by encoding special characters
      - Control flow directives supporting if/else conditionals and for-each loops
      - Template inheritance allowing child templates to extend base layout templates
  bytecode-compiler:
    id: bytecode-compiler
    name: Bytecode Compiler
    description: Compile AST to bytecode for a stack-based VM. Learn code generation, instruction encoding, and optimization.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - AST builder
    - Bytecode VM
    - Stack-based execution
    languages:
      recommended:
      - Python
      - Java
      - Rust
      also_possible:
      - C
      - Go
      - TypeScript
    resources:
    - type: book
      name: Crafting Interpreters - Compiling Expressions
      url: https://craftinginterpreters.com/compiling-expressions.html
    - type: article
      name: A Python Interpreter Written in Python
      url: https://aosabook.org/en/500L/a-python-interpreter-written-in-python.html
    milestones:
    - id: 1
      name: Expression Compilation
      description: Compile arithmetic and boolean expressions to bytecode.
      acceptance_criteria:
      - Literal values are stored in constant pool and loaded via index-based instruction
      - Binary operations emit left operand, right operand, then operator instruction in order
      - Unary negation compiles to operand push followed by negate instruction
      - Compiled bytecode for expressions evaluates to same result as source expression
      hints:
        level1: 'Post-order traversal: compile children first, then emit operator.'
        level2: Literals push to stack. Binary ops pop 2, push 1 result.
        level3: "class Compiler:\n    def __init__(self):\n        self.code = []  # bytecode\n        self.constants = []\
          \  # constant pool\n    \n    def compile(self, node):\n        method = f'compile_{type(node).__name__}'\n    \
          \    return getattr(self, method)(node)\n    \n    def compile_Number(self, node):\n        idx = self.add_constant(node.value)\n\
          \        self.emit(OpCode.CONST, idx)\n    \n    def compile_BinaryOp(self, node):\n        self.compile(node.left)\n\
          \        self.compile(node.right)\n        \n        ops = {\n            '+': OpCode.ADD,\n            '-': OpCode.SUB,\n\
          \            '*': OpCode.MUL,\n            '/': OpCode.DIV,\n            '<': OpCode.LT,\n            '>': OpCode.GT,\n\
          \            '==': OpCode.EQ,\n        }\n        self.emit(ops[node.op])\n    \n    def compile_UnaryOp(self, node):\n\
          \        self.compile(node.operand)\n        if node.op == '-':\n            self.emit(OpCode.NEG)\n        elif\
          \ node.op == '!':\n            self.emit(OpCode.NOT)\n    \n    def emit(self, opcode, operand=None):\n        self.code.append(opcode)\n\
          \        if operand is not None:\n            self.code.append(operand)\n    \n    def add_constant(self, value):\n\
          \        self.constants.append(value)\n        return len(self.constants) - 1"
      pitfalls:
      - Wrong operand order for non-commutative ops
      - Constant pool indexing
      - Forgetting to handle all operators
      concepts:
      - Tree traversal
      - Stack-based code generation
      - Constant pools
      estimated_hours: 4-6
      deliverables:
      - Literal emission generating bytecode instructions for numeric and string constants
      - Binary operation compilation emitting opcodes for arithmetic and comparison operators
      - Unary operation compilation emitting opcodes for negation and logical not
      - Constant pool storing literal values referenced by bytecode load instructions
    - id: 2
      name: Variables and Assignment
      description: Compile variable declarations, assignments, and references.
      acceptance_criteria:
      - Variable declaration allocates named slot and optionally stores initial value
      - Assignment expression compiles to evaluate right side then store to variable slot
      - Variable read compiles to load instruction that pushes variable value onto stack
      - Inner scope variables shadow outer scope variables and are freed on scope exit
      hints:
        level1: Track variables in a symbol table mapping name to slot index.
        level2: Use LOAD_LOCAL/STORE_LOCAL with slot index. Handle nested scopes.
        level3: "class Compiler:\n    def __init__(self):\n        self.code = []\n        self.locals = []  # Stack of scope\
          \ dicts\n        self.push_scope()\n    \n    def push_scope(self):\n        self.locals.append({})\n    \n    def\
          \ pop_scope(self):\n        scope = self.locals.pop()\n        # Emit pops for local variables\n        for _ in\
          \ scope:\n            self.emit(OpCode.POP)\n    \n    def define_local(self, name):\n        scope = self.locals[-1]\n\
          \        slot = sum(len(s) for s in self.locals) - 1 + len(scope)\n        scope[name] = slot\n        return slot\n\
          \    \n    def resolve_local(self, name):\n        for scope in reversed(self.locals):\n            if name in scope:\n\
          \                return scope[name]\n        return None  # Global or undefined\n    \n    def compile_VarDecl(self,\
          \ node):\n        if node.initializer:\n            self.compile(node.initializer)\n        else:\n            self.emit(OpCode.NIL)\n\
          \        self.define_local(node.name)\n    \n    def compile_Variable(self, node):\n        slot = self.resolve_local(node.name)\n\
          \        if slot is not None:\n            self.emit(OpCode.LOAD_LOCAL, slot)\n        else:\n            idx =\
          \ self.add_constant(node.name)\n            self.emit(OpCode.LOAD_GLOBAL, idx)\n    \n    def compile_Assignment(self,\
          \ node):\n        self.compile(node.value)\n        slot = self.resolve_local(node.name)\n        if slot is not\
          \ None:\n            self.emit(OpCode.STORE_LOCAL, slot)\n        else:\n            idx = self.add_constant(node.name)\n\
          \            self.emit(OpCode.STORE_GLOBAL, idx)"
      pitfalls:
      - Scope lifetime management
      - Variable shadowing bugs
      - Uninitialized variables
      concepts:
      - Symbol tables
      - Variable resolution
      - Scope management
      estimated_hours: 5-8
      deliverables:
      - Variable declaration compiling to stack slot allocation instruction
      - Variable assignment compiling to store instruction targeting named slot
      - Variable read compiling to load instruction from named stack slot
      - Scope management tracking variable visibility and stack depth per block
    - id: 3
      name: Control Flow
      description: Compile if statements, while loops, and logical operators.
      acceptance_criteria:
      - Conditional jump pops stack top and branches if value is falsy to target offset
      - If-else compiles to condition, conditional jump over then-block, then unconditional jump over else-block
      - While loop compiles to condition check, conditional exit jump, body, and backward loop jump
      - Forward jump targets are patched with correct offset after compilation of skipped block
      hints:
        level1: Emit jump with placeholder, then patch address after compiling body.
        level2: 'Short-circuit: ''and'' jumps to false if left is false, ''or'' jumps to true if left is true.'
        level3: "def compile_If(self, node):\n    self.compile(node.condition)\n    \n    # Jump to else if false\n    jump_to_else\
          \ = self.emit_jump(OpCode.JUMP_IF_FALSE)\n    self.emit(OpCode.POP)  # Pop condition\n    \n    self.compile(node.then_branch)\n\
          \    \n    # Jump over else\n    jump_to_end = self.emit_jump(OpCode.JUMP)\n    \n    self.patch_jump(jump_to_else)\n\
          \    self.emit(OpCode.POP)  # Pop condition\n    \n    if node.else_branch:\n        self.compile(node.else_branch)\n\
          \    \n    self.patch_jump(jump_to_end)\n\ndef compile_While(self, node):\n    loop_start = len(self.code)\n   \
          \ \n    self.compile(node.condition)\n    exit_jump = self.emit_jump(OpCode.JUMP_IF_FALSE)\n    self.emit(OpCode.POP)\n\
          \    \n    self.compile(node.body)\n    self.emit_loop(loop_start)\n    \n    self.patch_jump(exit_jump)\n    self.emit(OpCode.POP)\n\
          \ndef emit_jump(self, opcode):\n    self.emit(opcode)\n    self.emit(0xFF)  # Placeholder\n    self.emit(0xFF)\n\
          \    return len(self.code) - 2\n\ndef patch_jump(self, offset):\n    jump = len(self.code) - offset - 2\n    self.code[offset]\
          \ = (jump >> 8) & 0xFF\n    self.code[offset + 1] = jump & 0xFF\n\ndef emit_loop(self, loop_start):\n    self.emit(OpCode.LOOP)\n\
          \    offset = len(self.code) - loop_start + 2\n    self.emit((offset >> 8) & 0xFF)\n    self.emit(offset & 0xFF)"
      pitfalls:
      - Jump offset calculation
      - Forgetting to pop condition
      - Break/continue in nested loops
      concepts:
      - Jump patching
      - Backpatching
      - Short-circuit evaluation
      estimated_hours: 6-10
      deliverables:
      - Conditional jump instructions branching based on stack top boolean value
      - If-else compilation emitting conditional jump over then-branch to else-branch
      - While loop compilation emitting backward jump to loop condition check
      - Jump target patching resolving forward jump offsets after target is known
    - id: 4
      name: Functions
      description: Compile function definitions and calls.
      acceptance_criteria:
      - Function body compiles to separate bytecode chunk with its own constant pool
      - Call instruction creates new call frame with return address and local variable slots
      - Return instruction pops call frame and pushes return value onto caller stack
      - Arguments are accessible as local variables in called function frame
      hints:
        level1: Each function has its own bytecode chunk. Compile separately.
        level2: Parameters are just the first N local slots. CALL pushes frame.
        level3: "class FunctionObject:\n    def __init__(self, name, arity):\n        self.name = name\n        self.arity\
          \ = arity\n        self.code = []\n        self.constants = []\n\ndef compile_FunctionDecl(self, node):\n    func\
          \ = FunctionObject(node.name, len(node.params))\n    \n    # Save current compiler state\n    enclosing_code = self.code\n\
          \    enclosing_constants = self.constants\n    enclosing_locals = self.locals\n    \n    # Setup for function\n\
          \    self.code = func.code\n    self.constants = func.constants\n    self.locals = [{}]\n    \n    # Parameters\
          \ become first locals\n    for param in node.params:\n        self.define_local(param)\n    \n    # Compile body\n\
          \    self.compile(node.body)\n    \n    # Implicit return nil\n    self.emit(OpCode.NIL)\n    self.emit(OpCode.RETURN)\n\
          \    \n    # Restore state\n    self.code = enclosing_code\n    self.constants = enclosing_constants\n    self.locals\
          \ = enclosing_locals\n    \n    # Emit function as constant\n    idx = self.add_constant(func)\n    self.emit(OpCode.CONST,\
          \ idx)\n    \n    # Define in current scope\n    self.define_local(node.name)\n\ndef compile_Call(self, node):\n\
          \    self.compile(node.callee)  # Push function\n    \n    for arg in node.arguments:\n        self.compile(arg)\
          \  # Push args\n    \n    self.emit(OpCode.CALL, len(node.arguments))\n\ndef compile_Return(self, node):\n    if\
          \ node.value:\n        self.compile(node.value)\n    else:\n        self.emit(OpCode.NIL)\n    self.emit(OpCode.RETURN)"
      pitfalls:
      - Frame pointer management
      - Argument count validation
      - Stack cleanup on return
      concepts:
      - Call frames
      - Parameter binding
      - Return addresses
      estimated_hours: 10-16
      deliverables:
      - Function compilation generating separate bytecode chunk for function body
      - Call instruction pushing frame and transferring control to function bytecode
      - Return instruction popping frame and resuming caller execution context
      - Argument passing pushing arguments onto stack before call instruction
  bytecode-vm:
    id: bytecode-vm
    name: Bytecode Virtual Machine
    description: Build a stack-based virtual machine that executes bytecode. Learn about instruction sets and execution models.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Basic assembly concepts
    - Stack data structure
    - Binary representation
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
      - Java
    resources:
    - name: Crafting Interpreters - Bytecode VM
      url: https://craftinginterpreters.com/a-bytecode-virtual-machine.html
      type: book
    - name: Writing a Simple VM
      url: https://felix.engineer/blogs/virtual-machine-in-c
      type: article
    milestones:
    - id: 1
      name: Instruction Set Design
      description: Define opcodes and bytecode format.
      acceptance_criteria:
      - Opcode enum covers arithmetic, comparison, load, store, jump, and call operations
      - Instruction format encodes opcode in first byte with operand bytes following
      - Bytecode chunk stores instruction stream alongside indexed constant pool
      - Disassembler outputs instruction name, operands, and offset for each bytecode instruction
      hints:
        level1: 'Keep it simple: 1 byte opcode, optional operands follow.'
        level2: 'Common opcodes: PUSH, POP, ADD, SUB, JUMP, CALL, RETURN.'
        level3: "from enum import IntEnum\n\nclass OpCode(IntEnum):\n    # Stack operations\n    CONST = 0x01      # Push\
          \ constant: CONST <index>\n    POP = 0x02        # Pop top of stack\n    DUP = 0x03        # Duplicate top\n   \
          \ \n    # Arithmetic\n    ADD = 0x10\n    SUB = 0x11\n    MUL = 0x12\n    DIV = 0x13\n    NEG = 0x14\n    \n   \
          \ # Comparison\n    EQ = 0x20\n    LT = 0x21\n    GT = 0x22\n    \n    # Control flow\n    JUMP = 0x30       # JUMP\
          \ <offset>\n    JUMP_IF_FALSE = 0x31\n    \n    # Variables\n    LOAD_LOCAL = 0x40  # LOAD_LOCAL <slot>\n    STORE_LOCAL\
          \ = 0x41\n    LOAD_GLOBAL = 0x42\n    STORE_GLOBAL = 0x43\n    \n    # Functions\n    CALL = 0x50       # CALL <arg_count>\n\
          \    RETURN = 0x51\n    \n    # Special\n    HALT = 0xFF\n\nclass Chunk:\n    '''Bytecode container'''\n    def\
          \ __init__(self):\n        self.code = bytearray()\n        self.constants = []\n        self.lines = []  # For\
          \ error reporting\n    \n    def write(self, byte, line):\n        self.code.append(byte)\n        self.lines.append(line)\n\
          \    \n    def add_constant(self, value):\n        self.constants.append(value)\n        return len(self.constants)\
          \ - 1"
      pitfalls:
      - Too complex instruction set
      - Forgetting HALT
      - Operand encoding issues
      concepts:
      - Opcodes
      - Instruction encoding
      - Bytecode format
      estimated_hours: 2-3
      deliverables:
      - Opcode enumeration defining all supported bytecode instruction types
      - Instruction encoding format specifying opcode byte and operand bytes layout
      - Bytecode chunk structure holding instruction bytes and constant pool together
      - Disassembler printing human-readable representation of bytecode instructions
    - id: 2
      name: Stack-Based Execution
      description: Implement the execution loop with value stack.
      acceptance_criteria:
      - Operand stack correctly pushes and pops values without overflow or underflow errors
      - ADD instruction pops two values, computes sum, and pushes result onto stack
      - Comparison instructions pop two values and push boolean result for conditional use
      - Instruction pointer advances sequentially and dispatches correct handler per opcode
      hints:
        level1: 'Stack machine: operands pushed, operators pop and push result.'
        level2: IP points to next instruction. advance() returns byte and increments IP.
        level3: "class VM:\n    STACK_MAX = 256\n    \n    def __init__(self):\n        self.stack = []\n        self.ip =\
          \ 0\n        self.chunk = None\n    \n    def push(self, value):\n        if len(self.stack) >= self.STACK_MAX:\n\
          \            raise RuntimeError('Stack overflow')\n        self.stack.append(value)\n    \n    def pop(self):\n\
          \        if not self.stack:\n            raise RuntimeError('Stack underflow')\n        return self.stack.pop()\n\
          \    \n    def read_byte(self):\n        byte = self.chunk.code[self.ip]\n        self.ip += 1\n        return byte\n\
          \    \n    def run(self, chunk):\n        self.chunk = chunk\n        self.ip = 0\n        self.stack = []\n   \
          \     \n        while True:\n            op = self.read_byte()\n            \n            if op == OpCode.CONST:\n\
          \                index = self.read_byte()\n                self.push(self.chunk.constants[index])\n            \n\
          \            elif op == OpCode.ADD:\n                b = self.pop()\n                a = self.pop()\n          \
          \      self.push(a + b)\n            \n            elif op == OpCode.SUB:\n                b = self.pop()\n    \
          \            a = self.pop()\n                self.push(a - b)\n            \n            elif op == OpCode.MUL:\n\
          \                b = self.pop()\n                a = self.pop()\n                self.push(a * b)\n            \n\
          \            elif op == OpCode.DIV:\n                b = self.pop()\n                a = self.pop()\n          \
          \      self.push(a / b)\n            \n            elif op == OpCode.NEG:\n                self.push(-self.pop())\n\
          \            \n            elif op == OpCode.HALT:\n                return self.pop() if self.stack else None"
      pitfalls:
      - Stack over/underflow
      - Off-by-one in IP
      - Order of operands for SUB/DIV
      concepts:
      - Stack machines
      - Instruction pointer
      - Fetch-decode-execute
      estimated_hours: 4-5
      deliverables:
      - Operand stack pushing and popping values during instruction execution
      - Arithmetic instruction execution performing operations on stack top values
      - Comparison instruction execution pushing boolean result onto stack
      - Instruction pointer advancing through bytecode and dispatching each opcode
    - id: 3
      name: Control Flow
      description: Add jumps and conditionals.
      acceptance_criteria:
      - Unconditional jump sets instruction pointer to specified absolute or relative offset
      - Conditional jump pops stack top and jumps only if value evaluates to false
      - Loop back-edge correctly jumps backward creating repeated execution of loop body
      - Invalid jump targets outside bytecode bounds are detected and reported as errors
      hints:
        level1: JUMP modifies IP directly. Jump offset can be relative or absolute.
        level2: 'JUMP_IF_FALSE: pop condition, jump if falsy, else continue.'
        level3: |-
          # In VM.run():

          elif op == OpCode.EQ:
              b = self.pop()
              a = self.pop()
              self.push(a == b)

          elif op == OpCode.LT:
              b = self.pop()
              a = self.pop()
              self.push(a < b)

          elif op == OpCode.GT:
              b = self.pop()
              a = self.pop()
              self.push(a > b)

          elif op == OpCode.JUMP:
              offset = self.read_short()  # 2-byte offset
              self.ip = offset

          elif op == OpCode.JUMP_IF_FALSE:
              offset = self.read_short()
              if not self.peek_stack():  # Check without popping
                  self.ip = offset
              self.pop()  # Pop condition after check

          def read_short(self):
              '''Read 2-byte offset'''
              high = self.read_byte()
              low = self.read_byte()
              return (high << 8) | low

          def peek_stack(self):
              return self.stack[-1] if self.stack else None

          # Example: if-else compilation
          # if (x < 10) { a } else { b }
          # Compiles to:
          #   LOAD x
          #   CONST 10
          #   LT
          #   JUMP_IF_FALSE else_label
          #   <a code>
          #   JUMP end_label
          # else_label:
          #   <b code>
          # end_label:
      pitfalls:
      - Jump offset calculation
      - Forgetting to pop condition
      - Infinite loops
      concepts:
      - Control flow
      - Jumps
      - Conditional execution
      estimated_hours: 3-4
      deliverables:
      - Unconditional jump instruction setting instruction pointer to target offset
      - Conditional jump instruction branching based on stack top truth value
      - Loop back-edge jumping instruction pointer backward to loop condition
      - Branch target validation ensuring jump offsets stay within bytecode bounds
    - id: 4
      name: Variables and Functions
      description: Add local variables and function calls.
      acceptance_criteria:
      - Local variable store and load instructions access slots by index within current frame
      - Call frame stores return address so execution resumes at correct point after return
      - Function call pushes new frame with argument values accessible as local variables
      - Return instruction pops frame, restores caller state, and pushes return value onto stack
      hints:
        level1: 'Locals: use stack slots relative to frame pointer.'
        level2: 'Call frame: save IP, base pointer. Return: restore them.'
        level3: "class CallFrame:\n    def __init__(self, function, ip, base):\n        self.function = function  # Function\
          \ being called\n        self.ip = ip              # Return address\n        self.base = base          # Stack base\
          \ for locals\n\nclass VM:\n    def __init__(self):\n        self.stack = []\n        self.frames = []  # Call stack\n\
          \    \n    @property\n    def frame(self):\n        return self.frames[-1]\n    \n    def run(self, main_function):\n\
          \        self.frames = [CallFrame(main_function, 0, 0)]\n        \n        while self.frames:\n            op =\
          \ self.read_byte()\n            \n            if op == OpCode.LOAD_LOCAL:\n                slot = self.read_byte()\n\
          \                self.push(self.stack[self.frame.base + slot])\n            \n            elif op == OpCode.STORE_LOCAL:\n\
          \                slot = self.read_byte()\n                self.stack[self.frame.base + slot] = self.peek_stack()\n\
          \            \n            elif op == OpCode.CALL:\n                arg_count = self.read_byte()\n             \
          \   function = self.stack[-arg_count - 1]  # Function is below args\n                frame = CallFrame(\n      \
          \              function,\n                    self.frame.ip,\n                    len(self.stack) - arg_count -\
          \ 1\n                )\n                self.frames.append(frame)\n                self.frame.ip = 0\n         \
          \   \n            elif op == OpCode.RETURN:\n                result = self.pop()\n                # Discard frame\n\
          \                old_frame = self.frames.pop()\n                # Pop locals and function\n                while\
          \ len(self.stack) > old_frame.base:\n                    self.pop()\n                self.push(result)\n       \
          \         \n                if not self.frames:\n                    return result"
      pitfalls:
      - Frame pointer calculation
      - Argument passing order
      - Return value handling
      concepts:
      - Call frames
      - Local variables
      - Function calls
      estimated_hours: 5-7
      deliverables:
      - Local variable slots indexed storage for function-scoped variable values
      - Call frame stack tracking return addresses and local variable base pointers
      - Function call instruction creating new frame and transferring execution control
      - Return instruction restoring caller frame and passing return value back
  calculator-parser:
    id: calculator-parser
    name: Calculator Parser
    description: Build a calculator that parses and evaluates arithmetic expressions. Learn expression parsing and operator
      precedence.
    difficulty: beginner
    estimated_hours: 6-10
    prerequisites:
    - Basic programming
    - Understanding of precedence
    languages:
      recommended:
      - Python
      - JavaScript
      - C
      also_possible:
      - Go
      - Rust
    resources:
    - name: Recursive Descent Parsing
      url: https://craftinginterpreters.com/parsing-expressions.html
      type: book
    - name: Pratt Parsing
      url: https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html
      type: article
    milestones:
    - id: 1
      name: Basic Arithmetic
      description: Evaluate simple arithmetic expressions.
      acceptance_criteria:
      - Parser reads integer and floating-point numbers including negative values
      - Addition and subtraction evaluate left-to-right at lowest precedence level
      - Multiplication and division bind tighter than addition and subtraction
      - Parenthesized expressions are evaluated first regardless of surrounding operators
      hints:
        level1: 'One function per precedence level: expr() calls term() calls factor().'
        level2: factor handles numbers and parentheses, term handles * /, expr handles + -.
        level3: "class Calculator:\n    def __init__(self, text):\n        self.text = text\n        self.pos = 0\n    \n\
          \    def parse(self):\n        return self.expr()\n    \n    def expr(self):\n        result = self.term()\n   \
          \     while self.pos < len(self.text) and self.text[self.pos] in '+-':\n            op = self.text[self.pos]\n \
          \           self.pos += 1\n            if op == '+':\n                result += self.term()\n            else:\n\
          \                result -= self.term()\n        return result\n    \n    def term(self):\n        result = self.factor()\n\
          \        while self.pos < len(self.text) and self.text[self.pos] in '*/':\n            op = self.text[self.pos]\n\
          \            self.pos += 1\n            if op == '*':\n                result *= self.factor()\n            else:\n\
          \                result /= self.factor()\n        return result\n    \n    def factor(self):\n        self.skip_whitespace()\n\
          \        if self.text[self.pos] == '(':\n            self.pos += 1\n            result = self.expr()\n         \
          \   self.pos += 1  # skip ')'\n            return result\n        return self.number()"
      pitfalls:
      - Left vs right associativity
      - Division by zero
      - Whitespace handling
      concepts:
      - Recursive descent
      - Operator precedence
      - Expression trees
      estimated_hours: 2-3
      deliverables:
      - Number parser reading integer and decimal numeric literals from input
      - Addition and subtraction operators handling left-associative expressions
      - Multiplication and division operators with higher precedence than addition
      - Parenthesized expression grouping overriding default operator precedence
    - id: 2
      name: Unary and Power
      description: Add unary operators and exponentiation.
      acceptance_criteria:
      - Unary minus correctly negates values including nested unary like --5 evaluating to 5
      - Exponentiation computes base raised to power for integer and floating-point operands
      - Right associativity evaluates 2^3^2 as 2^(3^2) equaling 512 not 64
      - Power binds tighter than unary so -2^2 evaluates to -(2^2) equaling -4
      hints:
        level1: Unary goes in factor(). Power needs its own level.
        level2: 'Power is right-associative: 2^3^2 = 2^(3^2) = 512.'
        level3: |-
          def factor(self):
              self.skip_whitespace()
              if self.text[self.pos] in '+-':
                  op = self.text[self.pos]
                  self.pos += 1
                  value = self.factor()  # Recursive for --5
                  return -value if op == '-' else value
              return self.power()

          def power(self):
              base = self.primary()
              if self.pos < len(self.text) and self.text[self.pos] == '^':
                  self.pos += 1
                  exp = self.factor()  # Right-associative: recurse up
                  return base ** exp
              return base
      pitfalls:
      - --5 (double negative)
      - Power precedence vs unary
      - Right associativity
      concepts:
      - Unary operators
      - Associativity
      - Precedence climbing
      estimated_hours: 2-3
      deliverables:
      - Unary minus operator negating the following numeric expression
      - Exponentiation operator raising base to power with right associativity
      - Right-associative evaluation for power operator so 2^3^2 equals 2^9
      - Operator precedence hierarchy placing power above unary above multiplication
    - id: 3
      name: Variables and Functions
      description: Add variables and built-in functions.
      acceptance_criteria:
      - Variable assignment with x = 5 stores value and subsequent x + 2 evaluates to 7
      - Undefined variable reference produces descriptive error message with variable name
      - Built-in sin, cos, sqrt functions compute correct mathematical results
      - Function calls like sqrt(16) parse identifier, open paren, argument, and close paren
      hints:
        level1: Store variables in a dict. Look up on identifier.
        level2: Check if identifier followed by '(' for function call.
        level3: "class Calculator:\n    def __init__(self):\n        self.variables = {}\n        self.functions = {\n   \
          \         'sin': math.sin,\n            'cos': math.cos,\n            'sqrt': math.sqrt,\n            'abs': abs\n\
          \        }\n    \n    def statement(self):\n        self.skip_whitespace()\n        # Check for assignment\n   \
          \     if self.peek_identifier():\n            name = self.identifier()\n            self.skip_whitespace()\n   \
          \         if self.pos < len(self.text) and self.text[self.pos] == '=':\n                self.pos += 1\n        \
          \        value = self.expr()\n                self.variables[name] = value\n                return value\n     \
          \       # Put back and parse as expression\n            self.pos -= len(name)\n        return self.expr()\n    \n\
          \    def primary(self):\n        self.skip_whitespace()\n        if self.text[self.pos].isalpha():\n           \
          \ name = self.identifier()\n            if self.text[self.pos] == '(':  # Function call\n                self.pos\
          \ += 1\n                arg = self.expr()\n                self.pos += 1  # skip ')'\n                if name not\
          \ in self.functions:\n                    raise ValueError(f'Unknown function: {name}')\n                return\
          \ self.functions[name](arg)\n            if name not in self.variables:\n                raise ValueError(f'Undefined\
          \ variable: {name}')\n            return self.variables[name]\n        return self.number()"
      pitfalls:
      - Function vs variable ambiguity
      - Assignment in expression
      - Scope issues
      concepts:
      - Symbol tables
      - Function calls
      - Variable binding
      estimated_hours: 2-3
      deliverables:
      - Variable assignment storing named values in environment dictionary
      - Variable lookup resolving identifier names to stored numeric values
      - Built-in function library providing sin, cos, sqrt, and abs functions
      - Function call parsing detecting identifier followed by parenthesized arguments
  cd-deployment:
    id: cd-deployment
    name: CD with Blue-Green Deployment
    description: Implement continuous deployment with zero-downtime blue-green deployment strategy.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - CI pipeline basics
    - Docker/containers
    - Load balancer concepts
    - Shell scripting
    languages:
      recommended:
      - Bash
      - Python
      - Go
      also_possible:
      - JavaScript
      - Ruby
    resources:
    - type: article
      name: Blue-Green Deployments on AWS
      url: https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/welcome.html
    - type: documentation
      name: Kubernetes Rolling Updates
      url: https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/
    - type: article
      name: Martin Fowler on Blue-Green
      url: https://martinfowler.com/bliki/BlueGreenDeployment.html
    milestones:
    - id: 1
      name: Dual Environment Setup
      description: Set up blue and green environments that can run simultaneously.
      acceptance_criteria:
      - Two identical environments run simultaneously on separate ports or addresses
      - Health check endpoint returns environment color, version, and uptime information
      - Deploying to one environment does not affect the other currently serving traffic
      - Environment variables configure database, port, and feature flags per environment
      hints:
        level1: Use Docker Compose or similar to define both environments.
        level2: Each environment needs its own port/address. Use environment variables for configuration.
        level3: |-
          ## Dual Environment Architecture

          ```yaml
          # docker-compose.yml
          version: '3.8'

          services:
            blue:
              build: .
              environment:
                - ENV_COLOR=blue
                - PORT=3001
              ports:
                - "3001:3000"
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
                interval: 10s
                timeout: 5s
                retries: 3

            green:
              build: .
              environment:
                - ENV_COLOR=green
                - PORT=3002
              ports:
                - "3002:3000"
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
                interval: 10s
                timeout: 5s
                retries: 3

            nginx:
              image: nginx:alpine
              ports:
                - "80:80"
              volumes:
                - ./nginx.conf:/etc/nginx/nginx.conf:ro
              depends_on:
                - blue
                - green
          ```

          ```javascript
          // Health check endpoint
          app.get('/health', (req, res) => {
            const health = {
              status: 'healthy',
              version: process.env.APP_VERSION,
              color: process.env.ENV_COLOR,
              uptime: process.uptime(),
              timestamp: Date.now()
            };
            res.json(health);
          });
          ```
      pitfalls:
      - Forgetting database migrations
      - Environment config drift
      - Not testing both environments equally
      concepts:
      - Environment isolation
      - Infrastructure as code
      - Health checks
      estimated_hours: 4-6
      deliverables:
      - Blue and green environment provisioning with identical configurations
      - Environment health check endpoints reporting readiness and version info
      - Environment isolation ensuring independent deployment of each environment
      - Configuration management injecting environment-specific settings via variables
    - id: 2
      name: Load Balancer Switching
      description: Implement traffic switching between blue and green environments.
      acceptance_criteria:
      - Nginx or HAProxy routes all traffic to the currently active environment
      - Traffic switch uses graceful reload so no in-flight connections are dropped
      - Health check confirms target environment is healthy before switching traffic to it
      - Rollback switches traffic back to previous environment within seconds
      hints:
        level1: Use Nginx upstream directive to define backends.
        level2: Update nginx config and reload (not restart) for zero-downtime.
        level3: "## Load Balancer Configuration\n\n```nginx\n# nginx.conf\nupstream backend {\n    # Active environment (switched\
          \ during deployment)\n    server blue:3000;\n    # Standby (commented out)\n    # server green:3000;\n}\n\nserver\
          \ {\n    listen 80;\n    \n    location / {\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n\
          \        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header\
          \ Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_cache_bypass $http_upgrade;\n   \
          \ }\n    \n    location /health {\n        proxy_pass http://backend/health;\n    }\n}\n```\n\n```bash\n#!/bin/bash\n\
          # switch-traffic.sh\n\nTARGET=$1  # blue or green\n\nif [ \"$TARGET\" != \"blue\" ] && [ \"$TARGET\" != \"green\"\
          \ ]; then\n    echo \"Usage: $0 <blue|green>\"\n    exit 1\nfi\n\n# Generate new nginx config\ncat > /etc/nginx/conf.d/upstream.conf\
          \ << EOF\nupstream backend {\n    server ${TARGET}:3000;\n}\nEOF\n\n# Test config\nnginx -t\nif [ $? -ne 0 ]; then\n\
          \    echo \"Nginx config test failed!\"\n    exit 1\nfi\n\n# Reload nginx (graceful, no dropped connections)\nnginx\
          \ -s reload\n\necho \"Traffic switched to $TARGET\"\n```"
      pitfalls:
      - Using restart instead of reload
      - Not testing config before applying
      - Forgetting to drain connections
      concepts:
      - Reverse proxy
      - Graceful reload
      - Connection draining
      estimated_hours: 4-6
      deliverables:
      - Load balancer configuration routing traffic to active environment upstream
      - Traffic switching mechanism redirecting requests to target environment atomically
      - Health check validation verifying target environment before switching traffic
      - Instant rollback capability switching traffic back to previous environment
    - id: 3
      name: Deployment Automation
      description: Automate the deployment process with pre-deployment checks.
      acceptance_criteria:
      - Deployment script automates full sequence from build through traffic switch
      - New version is deployed to inactive environment while active serves live traffic
      - Smoke tests validate critical API endpoints return expected responses after deploy
      - Switch trigger only executes after all pre-deployment checks pass successfully
      hints:
        level1: 'Write a deployment script that: builds, deploys to standby, tests, switches traffic.'
        level2: Add timeouts and failure handling. Log everything for debugging.
        level3: |-
          ## Deployment Script

          ```bash
          #!/bin/bash
          # deploy.sh - Blue-Green Deployment Script

          set -e  # Exit on error

          # Configuration
          DEPLOY_TIMEOUT=300
          HEALTH_CHECK_RETRIES=30
          HEALTH_CHECK_INTERVAL=5

          # Determine current and target environments
          CURRENT=$(curl -s http://localhost/health | jq -r '.color')
          if [ "$CURRENT" == "blue" ]; then
              TARGET="green"
          else
              TARGET="blue"
          fi

          echo "Current: $CURRENT, Deploying to: $TARGET"

          # Step 1: Build and deploy to target
          echo "Building new version..."
          docker-compose build $TARGET

          echo "Starting $TARGET environment..."
          docker-compose up -d $TARGET

          # Step 2: Wait for health check
          echo "Waiting for $TARGET to be healthy..."
          for i in $(seq 1 $HEALTH_CHECK_RETRIES); do
              if curl -sf "http://${TARGET}:3000/health" > /dev/null; then
                  echo "$TARGET is healthy!"
                  break
              fi
              if [ $i -eq $HEALTH_CHECK_RETRIES ]; then
                  echo "Health check failed after $HEALTH_CHECK_RETRIES attempts"
                  exit 1
              fi
              echo "Attempt $i/$HEALTH_CHECK_RETRIES - waiting..."
              sleep $HEALTH_CHECK_INTERVAL
          done

          # Step 3: Run smoke tests
          echo "Running smoke tests on $TARGET..."
          ./smoke-tests.sh "http://${TARGET}:3000"
          if [ $? -ne 0 ]; then
              echo "Smoke tests failed!"
              exit 1
          fi

          # Step 4: Switch traffic
          echo "Switching traffic to $TARGET..."
          ./switch-traffic.sh $TARGET

          # Step 5: Verify production
          echo "Verifying production..."
          sleep 5
          PROD_COLOR=$(curl -s http://localhost/health | jq -r '.color')
          if [ "$PROD_COLOR" != "$TARGET" ]; then
              echo "Traffic switch verification failed!"
              exit 1
          fi

          echo "Deployment successful! Now running: $TARGET"
          echo "Previous environment ($CURRENT) is still available for rollback"
          ```
      pitfalls:
      - No timeout handling
      - Missing error handling
      - Not verifying after switch
      concepts:
      - Deployment automation
      - Smoke testing
      - Idempotent deployments
      estimated_hours: 5-8
      deliverables:
      - Deployment script orchestrating build, deploy, test, and switch steps
      - Artifact deployment to inactive environment without affecting live traffic
      - Smoke test automation verifying basic functionality after deployment completes
      - Switch trigger initiating traffic cutover after smoke tests pass successfully
    - id: 4
      name: Rollback & Database Migrations
      description: Handle rollbacks and database migrations safely.
      acceptance_criteria:
      - Rollback completes within seconds by switching traffic to still-running previous environment
      - Expand-contract migrations add nullable columns before enforcing constraints later
      - Both blue and green application versions work correctly with current database schema
      - Database version is tracked and migrations are applied idempotently without duplication
      hints:
        level1: Rollback is just switching back to the previous environment.
        level2: Database migrations must be backward-compatible (expand-contract pattern).
        level3: "## Rollback & Migrations\n\n```bash\n#!/bin/bash\n# rollback.sh - Instant rollback to previous environment\n\
          \nCURRENT=$(curl -s http://localhost/health | jq -r '.color')\nif [ \"$CURRENT\" == \"blue\" ]; then\n    PREVIOUS=\"\
          green\"\nelse\n    PREVIOUS=\"blue\"\nfi\n\necho \"Rolling back from $CURRENT to $PREVIOUS...\"\n\n# Verify previous\
          \ environment is still running\nif ! curl -sf \"http://${PREVIOUS}:3000/health\" > /dev/null; then\n    echo \"\
          Previous environment is not available!\"\n    exit 1\nfi\n\n# Switch traffic back\n./switch-traffic.sh $PREVIOUS\n\
          \necho \"Rollback complete. Now running: $PREVIOUS\"\n```\n\n### Database Migration Strategy\n\n```\nExpand-Contract\
          \ Pattern:\n\n1. EXPAND: Add new column (nullable or with default)\n   - Both old and new code can work\n   \n2.\
          \ MIGRATE: Deploy new code that writes to both\n   - Backfill existing data\n   \n3. CONTRACT: Remove old column/code\
          \ (after verification)\n   - Only when sure rollback isn't needed\n```\n\n```javascript\n// migrations/20240115_add_user_email.js\n\
          \n// EXPAND phase - backward compatible\nexports.up = async (db) => {\n  // Add column as nullable (old code ignores\
          \ it)\n  await db.query(`\n    ALTER TABLE users \n    ADD COLUMN email VARCHAR(255) NULL\n  `);\n};\n\nexports.down\
          \ = async (db) => {\n  await db.query(`\n    ALTER TABLE users \n    DROP COLUMN email\n  `);\n};\n\n// CONTRACT\
          \ phase (separate migration, run later)\nexports.up = async (db) => {\n  // Only run after all instances use new\
          \ code\n  await db.query(`\n    ALTER TABLE users \n    ALTER COLUMN email SET NOT NULL\n  `);\n};\n```\n\n```bash\n\
          # deployment with migrations\n./run-migrations.sh  # Run expand migrations first\n./deploy.sh          # Deploy\
          \ new code\n# ... wait for stability ...\n./run-migrations.sh --contract  # Run contract migrations\n```"
      pitfalls:
      - Breaking migrations that prevent rollback
      - Running contract too early
      - Not testing rollback regularly
      concepts:
      - Expand-contract pattern
      - Backward compatibility
      - Database versioning
      estimated_hours: 5-8
      deliverables:
      - Rollback automation reverting traffic to previous environment on failure detection
      - Database migration strategy using expand-contract pattern for compatibility
      - Backward-compatible migrations ensuring both old and new code work with schema
      - Migration rollback scripts undoing schema changes if deployment fails
  chat-app:
    id: chat-app
    name: Real-time Chat
    description: Build a real-time chat application using WebSockets. Learn about bi-directional communication, presence,
      and message persistence.
    difficulty: intermediate
    estimated_hours: 25-35
    prerequisites:
    - JavaScript/Node.js
    - Basic HTML/CSS
    - Understanding of HTTP
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible:
      - Go
      - Python
      - Rust
    resources:
    - name: Socket.io Chat Tutorial
      url: https://socket.io/get-started/chat
      type: tutorial
    - name: WebSocket API - MDN
      url: https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API
      type: documentation
    milestones:
    - id: 1
      name: WebSocket Server Setup
      description: Set up WebSocket server and basic connection handling.
      acceptance_criteria:
      - Server accepts WebSocket upgrade requests and establishes persistent connections
      - Connection events are logged when clients connect and disconnect from server
      - Server correctly handles malformed messages without crashing or dropping other clients
      - Connection state tracks all active clients and removes disconnected ones promptly
      hints:
        level1: Use ws library (Node.js) or gorilla/websocket (Go).
        level2: Track connected clients in a Map or Set.
        level3: |-
          const WebSocket = require('ws');
          const wss = new WebSocket.Server({ port: 8080 });
          const clients = new Set();

          wss.on('connection', (ws) => {
            clients.add(ws);
            console.log('Client connected', clients.size);
            ws.on('close', () => {
              clients.delete(ws);
              console.log('Client disconnected', clients.size);
            });
          });
      pitfalls:
      - Not handling connection errors
      - Memory leaks from not cleaning up closed connections
      - Not implementing heartbeat/ping-pong
      concepts:
      - WebSocket protocol
      - Connection lifecycle
      - Event-driven architecture
      estimated_hours: 2-3
      deliverables:
      - WebSocket server implementation accepting upgrade requests from HTTP clients
      - Connection handling managing client connect and disconnect lifecycle events
      - Message routing dispatching incoming messages to appropriate handler functions
      - Connection state tracking maintaining set of currently connected client sockets
    - id: 2
      name: Message Broadcasting
      description: Implement sending and receiving messages.
      acceptance_criteria:
      - Broadcast delivers message to all connected clients except the original sender
      - Messages include sender username, text content, and server-assigned timestamp
      - Join notification is broadcast to room members when a new user enters the room
      - Typing indicator is broadcast when user starts typing and cleared after timeout
      hints:
        level1: Parse incoming messages as JSON, broadcast to all clients.
        level2: Don't send message back to sender (unless you want echo).
        level3: |-
          ws.on('message', (data) => {
            const message = JSON.parse(data);
            const broadcast = JSON.stringify({
              type: 'message',
              user: message.user,
              text: message.text,
              timestamp: Date.now()
            });
            clients.forEach(client => {
              if (client !== ws && client.readyState === WebSocket.OPEN) {
                client.send(broadcast);
              }
            });
          });
      pitfalls:
      - Not checking readyState before sending
      - Invalid JSON crashing server
      - Missing message validation
      concepts:
      - Broadcasting pattern
      - JSON message protocol
      - Error handling
      estimated_hours: 3-4
      deliverables:
      - Room-based broadcasting delivering messages to all clients in target room
      - Message format including sender name, content text, and UTC timestamp
      - User join and leave event notifications broadcast to other room members
      - Typing indicator messages notifying room members when user is composing
    - id: 3
      name: Chat Rooms
      description: Implement multiple chat rooms/channels.
      acceptance_criteria:
      - Users can create new rooms and join existing rooms by name
      - Room member list is updated when users join or leave the room
      - Room listing endpoint returns all available rooms with member counts
      - Joining a room loads configurable number of recent messages from history
      - Messages sent to a room are delivered only to members currently in that room
      hints:
        level1: 'Use a Map: roomName -> Set of client connections.'
        level2: Each message includes room name, only broadcast to room members.
        level3: |-
          const rooms = new Map();

          function joinRoom(ws, roomName) {
            if (!rooms.has(roomName)) {
              rooms.set(roomName, new Set());
            }
            rooms.get(roomName).add(ws);
            ws.currentRoom = roomName;
          }

          function broadcastToRoom(roomName, message, sender) {
            const room = rooms.get(roomName);
            if (!room) return;
            room.forEach(client => {
              if (client !== sender && client.readyState === WebSocket.OPEN) {
                client.send(JSON.stringify(message));
              }
            });
          }
      pitfalls:
      - Not removing user from room on disconnect
      - Room names not sanitized
      - Empty rooms accumulating
      concepts:
      - Room-based messaging
      - Namespaces
      - User presence
      estimated_hours: 4-5
      deliverables:
      - Room creation and joining allowing users to enter named chat channels
      - Room member list management tracking which users are in each room
      - Message history loading providing recent messages when user joins room
      - Room-based message routing delivering messages only to members of target room
    - id: 4
      name: User Authentication & Persistence
      description: Add user authentication and message history.
      acceptance_criteria:
      - Users must authenticate before WebSocket connection is accepted by server
      - Messages are stored in database with sender, room, content, and timestamp fields
      - Message history loads previous messages when user joins room with pagination
      - Typing indicator events broadcast to room members showing who is currently typing
      - Online and offline status updates broadcast when authenticated users connect or disconnect
      hints:
        level1: Send JWT token on WebSocket connect, verify before accepting.
        level2: Store last N messages per room, load on join.
        level3: |-
          // Typing indicator with debounce
          ws.on('message', (data) => {
            const msg = JSON.parse(data);
            if (msg.type === 'typing') {
              broadcastToRoom(ws.currentRoom, { type: 'typing', user: ws.user.name }, ws);
              clearTimeout(ws.typingTimeout);
              ws.typingTimeout = setTimeout(() => {
                broadcastToRoom(ws.currentRoom, { type: 'stopped_typing', user: ws.user.name }, ws);
              }, 3000);
            }
          });
      pitfalls:
      - Loading too much history (pagination needed)
      - Typing indicator spam
      - Race conditions with presence
      concepts:
      - WebSocket authentication
      - Message persistence
      - Presence system
      estimated_hours: 6-8
      deliverables:
      - User login and registration with username and password credentials
      - Session management maintaining authenticated state across WebSocket connections
      - Message persistence storing chat messages in database for history retrieval
      - Chat history loading retrieving past messages with pagination support
  config-parser:
    id: config-parser
    name: Config File Parser
    description: Build a multi-format configuration file parser supporting INI, TOML, and YAML-subset. Learn recursive descent
      parsing and data structure mapping.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - String manipulation
    - Data structures
    - Recursive thinking
    languages:
      recommended:
      - Python
      - Go
      - Rust
      also_possible:
      - JavaScript
      - C
    resources:
    - name: TOML Specification
      url: https://toml.io/en/v1.0.0
      type: specification
    - name: INI File Format
      url: https://en.wikipedia.org/wiki/INI_file
      type: reference
    - name: Writing a Parser in Python
      url: https://www.freecodecamp.org/news/how-to-write-a-parser-in-python/
      type: tutorial
    milestones:
    - id: 1
      name: INI Parser
      description: Parse INI files with sections, key-value pairs, and comments.
      acceptance_criteria:
      - Parse [section] headers and create nested structure correctly
      - 'Parse key=value and key: value pairs with whitespace trimming'
      - 'Handle ; and # comment lines by ignoring them during parsing'
      - Support quoted strings with proper escape sequence processing
      - Return nested dictionary structure with sections as top-level keys
      hints:
        level1: 'INI is line-based. Process line by line: check if it''s a section, comment, or key-value.'
        level2: 'Use regex or simple string operations. Section: ^\[(.+)\]$, Comment: ^[;#], Key-value: split on first = or
          :'
        level3: |-
          import re

          def parse_ini(text: str) -> dict:
              result = {}
              current_section = None

              for line_num, line in enumerate(text.splitlines(), 1):
                  line = line.strip()

                  # Skip empty lines and comments
                  if not line or line.startswith(';') or line.startswith('#'):
                      continue

                  # Section header
                  if line.startswith('[') and line.endswith(']'):
                      current_section = line[1:-1].strip()
                      if current_section not in result:
                          result[current_section] = {}
                      continue

                  # Key-value pair
                  match = re.match(r'^([^=:]+)[=:](.*)$', line)
                  if match:
                      key = match.group(1).strip()
                      value = match.group(2).strip()

                      # Remove quotes
                      if (value.startswith('"') and value.endswith('"')) or \
                         (value.startswith("'") and value.endswith("'")):
                          value = value[1:-1]

                      # Type coercion
                      if value.lower() in ('true', 'yes', 'on'):
                          value = True
                      elif value.lower() in ('false', 'no', 'off'):
                          value = False
                      elif value.isdigit():
                          value = int(value)

                      target = result[current_section] if current_section else result
                      target[key] = value
                  else:
                      raise ValueError(f"Invalid line {line_num}: {line}")

              return result
      pitfalls:
      - Not handling keys outside of sections (global keys)
      - Forgetting to handle inline comments after values
      - Breaking on = inside quoted strings
      concepts:
      - Line-based parsing
      - Regular expressions
      - Type coercion
      estimated_hours: 3-4
      deliverables:
      - Section header parsing with bracket notation support
      - Key-value pair parsing for both equals and colon delimiters
      - Comment handling for semicolon and hash prefixes
      - Multi-line value continuation with proper joining
    - id: 2
      name: TOML Tokenizer
      description: Build a lexer/tokenizer for TOML format.
      acceptance_criteria:
      - Tokenize brackets, dots, equals, strings, and numbers into typed tokens
      - Handle basic strings, literal strings, and multiline string variants correctly
      - Recognize integers, floats, booleans, and date-time values as distinct tokens
      - Track line and column numbers for meaningful error message reporting
      hints:
        level1: TOML has more complex string types than INI. Basic strings use ", literal strings use '.
        level2: Create Token class with type, value, line, column. Use a Lexer class that tracks position.
        level3: |-
          from enum import Enum
          from dataclasses import dataclass

          class TokenType(Enum):
              LBRACKET = '['
              RBRACKET = ']'
              LBRACE = '{'
              RBRACE = '}'
              DOT = '.'
              EQUALS = '='
              COMMA = ','
              STRING = 'STRING'
              INTEGER = 'INTEGER'
              FLOAT = 'FLOAT'
              BOOLEAN = 'BOOLEAN'
              DATETIME = 'DATETIME'
              BARE_KEY = 'BARE_KEY'
              NEWLINE = 'NEWLINE'
              EOF = 'EOF'

          @dataclass
          class Token:
              type: TokenType
              value: any
              line: int
              column: int

          class Lexer:
              def __init__(self, text: str):
                  self.text = text
                  self.pos = 0
                  self.line = 1
                  self.column = 1

              def peek(self) -> str:
                  return self.text[self.pos] if self.pos < len(self.text) else ''

              def advance(self) -> str:
                  ch = self.peek()
                  self.pos += 1
                  if ch == '\n':
                      self.line += 1
                      self.column = 1
                  else:
                      self.column += 1
                  return ch

              def read_string(self, quote: str) -> str:
                  # Handle basic string (") with escapes
                  # Handle literal string (') without escapes
                  # Handle multiline (''' or """)
                  pass

              def tokenize(self) -> list[Token]:
                  tokens = []
                  while self.pos < len(self.text):
                      ch = self.peek()
                      if ch in ' \t':
                          self.advance()
                      elif ch == '\n':
                          tokens.append(Token(TokenType.NEWLINE, None, self.line, self.column))
                          self.advance()
                      elif ch == '#':
                          while self.peek() and self.peek() != '\n':
                              self.advance()
                      # ... handle other token types
                  return tokens
      pitfalls:
      - TOML multiline strings have complex rules for leading newlines
      - Literal strings don't process escapes (backslash is literal)
      - 'Integer underscores are allowed: 1_000_000'
      concepts:
      - Lexical analysis
      - State machines
      - Unicode handling
      estimated_hours: 4-5
      deliverables:
      - Token type definitions for all TOML grammar elements
      - String literal handling with basic, literal, and multiline variants
      - Date and time value parsing into appropriate token types
      - Array and table tokens with proper bracket recognition
    - id: 3
      name: TOML Parser
      description: Build recursive descent parser for TOML tables and arrays.
      acceptance_criteria:
      - Parse [table] and [table.subtable] headers into nested dictionary structure
      - Parse [[array.of.tables]] into list of dictionary entries correctly
      - Handle inline tables using { key = value } syntax within expressions
      - Handle inline arrays using [ 1, 2, 3 ] syntax with mixed types
      - Dotted keys like physical.color = 'orange' create nested structure correctly
      hints:
        level1: 'TOML keys can define nested structure: a.b.c = 1 creates {a: {b: {c: 1}}}'
        level2: '[[array]] appends to an array of tables. Each [[array]] block is a new element.'
        level3: |-
          class Parser:
              def __init__(self, tokens: list[Token]):
                  self.tokens = tokens
                  self.pos = 0
                  self.result = {}
                  self.current_table = self.result

              def parse_key(self) -> list[str]:
                  '''Parse dotted key like a.b.c into ["a", "b", "c"]'''
                  keys = []
                  while True:
                      tok = self.expect(TokenType.BARE_KEY, TokenType.STRING)
                      keys.append(tok.value)
                      if not self.match(TokenType.DOT):
                          break
                  return keys

              def set_nested(self, keys: list[str], value: any):
                  '''Set value at nested key path'''
                  target = self.current_table
                  for key in keys[:-1]:
                      if key not in target:
                          target[key] = {}
                      target = target[key]
                  target[keys[-1]] = value

              def parse_table_header(self):
                  '''Parse [table] or [[array.of.tables]]'''
                  is_array = self.match(TokenType.LBRACKET)  # Second [
                  keys = self.parse_key()
                  self.expect(TokenType.RBRACKET)
                  if is_array:
                      self.expect(TokenType.RBRACKET)

                  # Navigate to (or create) the table
                  target = self.result
                  for i, key in enumerate(keys):
                      if key not in target:
                          target[key] = [] if (is_array and i == len(keys)-1) else {}
                      target = target[key]
                      if isinstance(target, list):
                          if i == len(keys) - 1:
                              target.append({})
                          target = target[-1]

                  self.current_table = target
      pitfalls:
      - Can't redefine a key that already exists
      - Tables can be defined implicitly by dotted keys
      - Array of tables vs array value have different syntax
      concepts:
      - Recursive descent parsing
      - Symbol tables
      - Nested data structures
      estimated_hours: 5-6
      deliverables:
      - Table parsing with nested dotted key path support
      - Array of tables parsing with double-bracket syntax
      - Inline tables and inline arrays with nested value support
      - Value type inference for automatic type conversion
    - id: 4
      name: YAML Subset Parser
      description: 'Parse a subset of YAML: indentation-based nesting, mappings, sequences.'
      acceptance_criteria:
      - Indentation-based block structure determines nesting depth correctly
      - 'Key: value mappings are parsed and stored as dictionary entries'
      - Sequences with - prefix are parsed into ordered list structures
      - Quoted and unquoted strings are handled with proper type inference
      - Flow style [list] and {map} inline syntax is parsed correctly
      hints:
        level1: Track indentation level. Deeper indent = nested structure. Same indent = sibling.
        level2: Build a stack of (indent_level, container). When indent decreases, pop back to matching level.
        level3: |-
          def parse_yaml(text: str) -> dict:
              lines = text.splitlines()
              result = {}
              stack = [(0, result)]  # (indent_level, container)

              for line in lines:
                  stripped = line.lstrip()
                  if not stripped or stripped.startswith('#'):
                      continue

                  indent = len(line) - len(stripped)

                  # Pop stack until we find appropriate parent
                  while stack and stack[-1][0] >= indent:
                      stack.pop()

                  parent = stack[-1][1] if stack else result

                  if stripped.startswith('- '):
                      # Sequence item
                      value = stripped[2:].strip()
                      if isinstance(parent, dict):
                          # First item, convert to list (need parent key)
                          pass
                      parent.append(parse_value(value) if value else {})
                      if not value:
                          stack.append((indent + 2, parent[-1]))
                  elif ':' in stripped:
                      # Mapping
                      key, _, value = stripped.partition(':')
                      key = key.strip()
                      value = value.strip()

                      if value:
                          parent[key] = parse_value(value)
                      else:
                          parent[key] = {}
                          stack.append((indent + 2, parent[key]))

              return result

          def parse_value(s: str):
              if s.startswith('['):
                  return parse_flow_sequence(s)
              if s.startswith('{'):
                  return parse_flow_mapping(s)
              if s.lower() in ('true', 'yes'):
                  return True
              if s.lower() in ('false', 'no'):
                  return False
              if s.lower() == 'null':
                  return None
              try:
                  return int(s)
              except:
                  try:
                      return float(s)
                  except:
                      return s.strip('"\''')
      pitfalls:
      - Tabs vs spaces (YAML forbids tabs for indentation)
      - Implicit type detection can be surprising (yes = true, 1.0 = float)
      - Multiline strings have multiple syntaxes (|, >, etc.)
      concepts:
      - Indentation-sensitive parsing
      - Implicit typing
      - Stack-based parsing
      estimated_hours: 5-6
      deliverables:
      - Indentation-based structure detection and nesting resolution
      - Mapping parsing for key-value pairs with colon separator
      - Sequence parsing for list items with dash prefix
      - Scalar type inference for strings, numbers, and booleans
  container-basic:
    id: container-basic
    name: Container (Basic)
    description: Build a simple container using Linux namespaces. Learn process isolation and cgroups basics.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Linux system calls
    - Process management
    - Filesystem basics
    languages:
      recommended:
      - C
      - Go
      - Rust
      also_possible:
      - Python
    resources:
    - name: Linux Namespaces
      url: https://man7.org/linux/man-pages/man7/namespaces.7.html
      type: documentation
    - name: Containers from Scratch
      url: https://ericchiang.github.io/post/containers-from-scratch/
      type: article
    milestones:
    - id: 1
      name: Process Namespace
      description: Isolate process tree with PID namespace.
      acceptance_criteria:
      - Create new PID namespace isolating child process IDs from host
      - Child process sees itself as PID 1 within the new namespace
      - Parent process sees the real host PID of the child process
      - Handle zombie process reaping within the namespace correctly
      hints:
        level1: clone() with CLONE_NEWPID creates new PID namespace.
        level2: First process in namespace is PID 1, must reap children.
        level3: "#define _GNU_SOURCE\n#include <sched.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include\
          \ <sys/wait.h>\n\nstatic int child_fn(void *arg) {\n    printf(\"Child PID: %d\\n\", getpid());  // Will print 1\n\
          \    \n    // Mount proc for accurate /proc\n    mount(\"proc\", \"/proc\", \"proc\", 0, NULL);\n    \n    // Execute\
          \ the command\n    char *argv[] = {\"/bin/sh\", NULL};\n    execv(argv[0], argv);\n    return 1;\n}\n\nint main()\
          \ {\n    #define STACK_SIZE (1024 * 1024)\n    char *stack = malloc(STACK_SIZE);\n    \n    pid_t pid = clone(\n\
          \        child_fn,\n        stack + STACK_SIZE,  // Stack grows down\n        CLONE_NEWPID | SIGCHLD,\n        NULL\n\
          \    );\n    \n    printf(\"Parent sees child PID: %d\\n\", pid);  // Real PID\n    waitpid(pid, NULL, 0);\n   \
          \ \n    return 0;\n}"
      pitfalls:
      - Stack direction
      - PID 1 responsibilities
      - Zombie processes
      concepts:
      - PID namespaces
      - clone()
      - Process isolation
      estimated_hours: 3-4
      deliverables:
      - PID namespace creation using clone or unshare syscall
      - Process isolation verification comparing inner and outer PIDs
      - Init process setup running as PID 1 in namespace
      - Namespace unsharing from parent process context
    - id: 2
      name: Mount Namespace
      description: Isolate filesystem mounts.
      acceptance_criteria:
      - New mount namespace isolates filesystem mounts from the host
      - Private mount propagation prevents mount events leaking to host
      - Pivot root switches container to new filesystem root directory
      - Mount essential filesystems like /proc and /sys inside container
      hints:
        level1: CLONE_NEWNS isolates mounts. pivot_root changes root.
        level2: Need to mount /proc, /sys, /dev inside container.
        level3: "void setup_mount_namespace(const char *rootfs) {\n    // Make all mounts private\n    mount(NULL, \"/\",\
          \ NULL, MS_REC | MS_PRIVATE, NULL);\n    \n    // Mount the new root filesystem\n    mount(rootfs, rootfs, \"bind\"\
          , MS_BIND | MS_REC, NULL);\n    \n    // Create mount points\n    char path[256];\n    snprintf(path, sizeof(path),\
          \ \"%s/proc\", rootfs);\n    mkdir(path, 0755);\n    snprintf(path, sizeof(path), \"%s/sys\", rootfs);\n    mkdir(path,\
          \ 0755);\n    snprintf(path, sizeof(path), \"%s/dev\", rootfs);\n    mkdir(path, 0755);\n    \n    // Mount essential\
          \ filesystems\n    snprintf(path, sizeof(path), \"%s/proc\", rootfs);\n    mount(\"proc\", path, \"proc\", 0, NULL);\n\
          \    \n    snprintf(path, sizeof(path), \"%s/sys\", rootfs);\n    mount(\"sysfs\", path, \"sysfs\", 0, NULL);\n\
          \    \n    // Pivot root\n    char old_root[256];\n    snprintf(old_root, sizeof(old_root), \"%s/.old_root\", rootfs);\n\
          \    mkdir(old_root, 0755);\n    \n    if (pivot_root(rootfs, old_root) < 0) {\n        perror(\"pivot_root\");\n\
          \        exit(1);\n    }\n    \n    chdir(\"/\");\n    \n    // Unmount old root\n    umount2(\"/.old_root\", MNT_DETACH);\n\
          \    rmdir(\"/.old_root\");\n}"
      pitfalls:
      - Mount propagation
      - pivot_root requirements
      - Device nodes
      concepts:
      - Mount namespaces
      - pivot_root
      - Filesystem isolation
      estimated_hours: 4-5
      deliverables:
      - Mount namespace creation for filesystem isolation
      - Root filesystem setup with minimal directory structure
      - Bind mount configuration for shared host directories
      - Proc and sys filesystem mounting inside the namespace
    - id: 3
      name: Network Namespace
      description: Isolate network stack.
      acceptance_criteria:
      - Create network namespace with isolated network interfaces and routing
      - Set up veth pair connecting container namespace to host bridge
      - Configure IP addresses on both ends of the veth pair correctly
      - Enable container networking with outbound connectivity through NAT
      hints:
        level1: CLONE_NEWNET gives isolated network stack. veth connects to host.
        level2: 'veth pair: one end in host, one in container namespace.'
        level3: "// Note: This typically requires root and netlink or ip commands\n\nvoid setup_network(pid_t child_pid) {\n\
          \    char cmd[256];\n    \n    // Create veth pair\n    snprintf(cmd, sizeof(cmd), \n             \"ip link add\
          \ veth0 type veth peer name veth1\");\n    system(cmd);\n    \n    // Move veth1 to container namespace\n    snprintf(cmd,\
          \ sizeof(cmd),\n             \"ip link set veth1 netns %d\", child_pid);\n    system(cmd);\n    \n    // Configure\
          \ host side\n    system(\"ip addr add 10.0.0.1/24 dev veth0\");\n    system(\"ip link set veth0 up\");\n    \n \
          \   // Enable IP forwarding and NAT\n    system(\"echo 1 > /proc/sys/net/ipv4/ip_forward\");\n    system(\"iptables\
          \ -t nat -A POSTROUTING -s 10.0.0.0/24 -j MASQUERADE\");\n}\n\nvoid setup_network_inside_container() {\n    // Run\
          \ inside container namespace\n    system(\"ip addr add 10.0.0.2/24 dev veth1\");\n    system(\"ip link set veth1\
          \ up\");\n    system(\"ip link set lo up\");\n    system(\"ip route add default via 10.0.0.1\");\n}"
      pitfalls:
      - Namespace timing
      - veth cleanup
      - NAT configuration
      concepts:
      - Network namespaces
      - veth pairs
      - Container networking
      estimated_hours: 4-5
      deliverables:
      - Network namespace creation for isolated network stack
      - Veth pair setup connecting container to host network
      - Bridge networking configuration for multi-container communication
      - NAT configuration for outbound internet access from container
    - id: 4
      name: Cgroups (Resource Limits)
      description: Limit CPU, memory, and other resources.
      acceptance_criteria:
      - Create cgroup for container process and assign PID correctly
      - Set memory limit and verify OOM behavior when exceeded
      - Set CPU quota and period to restrict CPU usage percentage
      - Clean up cgroup hierarchy on container exit to prevent leaks
      hints:
        level1: 'cgroups v2: create directory in /sys/fs/cgroup, write limits.'
        level2: 'Memory limit: memory.max. CPU: cpu.max (quota period).'
        level3: "void setup_cgroups(pid_t pid, int memory_limit_mb, int cpu_percent) {\n    char path[256];\n    char value[64];\n\
          \    \n    // Create cgroup directory (cgroups v2)\n    snprintf(path, sizeof(path), \"/sys/fs/cgroup/container_%d\"\
          , pid);\n    mkdir(path, 0755);\n    \n    // Set memory limit\n    snprintf(path, sizeof(path), \n            \
          \ \"/sys/fs/cgroup/container_%d/memory.max\", pid);\n    snprintf(value, sizeof(value), \"%d\", memory_limit_mb\
          \ * 1024 * 1024);\n    write_file(path, value);\n    \n    // Set CPU quota (e.g., 50% = 50000 100000)\n    snprintf(path,\
          \ sizeof(path),\n             \"/sys/fs/cgroup/container_%d/cpu.max\", pid);\n    snprintf(value, sizeof(value),\
          \ \"%d 100000\", cpu_percent * 1000);\n    write_file(path, value);\n    \n    // Add process to cgroup\n    snprintf(path,\
          \ sizeof(path),\n             \"/sys/fs/cgroup/container_%d/cgroup.procs\", pid);\n    snprintf(value, sizeof(value),\
          \ \"%d\", pid);\n    write_file(path, value);\n}\n\nvoid cleanup_cgroups(pid_t pid) {\n    char path[256];\n   \
          \ snprintf(path, sizeof(path), \"/sys/fs/cgroup/container_%d\", pid);\n    rmdir(path);  // Only works if empty\
          \ (process exited)\n}"
      pitfalls:
      - cgroups v1 vs v2
      - Controller availability
      - Cleanup order
      concepts:
      - cgroups
      - Resource limits
      - Container resources
      estimated_hours: 4-5
      deliverables:
      - Cgroup creation and process assignment for resource control
      - Memory limit configuration with enforcement verification
      - CPU limit configuration using quota and period settings
      - Process limit to cap number of tasks in container
  diff-tool:
    id: diff-tool
    name: Diff Tool
    description: Build a text diff tool using the Longest Common Subsequence algorithm and Myers' diff algorithm. Learn dynamic
      programming and edit distance concepts.
    difficulty: beginner
    estimated_hours: 12-18
    prerequisites:
    - Dynamic programming basics
    - File I/O
    - String manipulation
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - C
      - Rust
      - Java
    resources:
    - name: Myers' Diff Algorithm Tutorial
      url: http://simplygenius.net/Article/DiffTutorial1
      type: tutorial
    - name: The Myers Difference Algorithm
      url: https://nathaniel.ai/myers-diff/
      type: article
    - name: Wikipedia - LCS
      url: https://en.wikipedia.org/wiki/Longest_common_subsequence
      type: reference
    milestones:
    - id: 1
      name: Line Tokenization
      description: Read two files and split into line arrays for comparison.
      acceptance_criteria:
      - Read files handling different encodings such as UTF-8 and Latin-1
      - Split content by newlines while preserving empty lines in the sequence
      - Handle different line endings including LF, CRLF, and CR formats
      - Report line counts for each input file before comparison begins
      hints:
        level1: Use splitlines() in Python or split by regex /\r?\n/ in JavaScript.
        level2: Consider normalizing line endings first, or preserve them for accurate diff.
        level3: |-
          def read_lines(filepath: str) -> list[str]:
              '''Read file and return list of lines'''
              with open(filepath, 'r', encoding='utf-8') as f:
                  content = f.read()

              # Normalize line endings
              content = content.replace('\r\n', '\n').replace('\r', '\n')

              # Split into lines (keeping empty lines)
              lines = content.split('\n')

              # Remove trailing empty line if file ends with newline
              if lines and lines[-1] == '':
                  lines = lines[:-1]

              return lines

          def main():
              file1_lines = read_lines(sys.argv[1])
              file2_lines = read_lines(sys.argv[2])

              print(f"File 1: {len(file1_lines)} lines")
              print(f"File 2: {len(file2_lines)} lines")
      pitfalls:
      - Binary files will cause encoding errors
      - Large files can exhaust memory
      - Trailing newline handling varies between tools
      concepts:
      - File encoding
      - Line endings
      - Text normalization
      estimated_hours: 2-3
      deliverables:
      - File reading with line splitting preserving structure
      - Line normalization handling trailing whitespace differences
      - Empty line handling preserving blank lines in output
      - Encoding detection supporting UTF-8 and Latin-1 inputs
    - id: 2
      name: LCS Algorithm
      description: Implement Longest Common Subsequence using dynamic programming.
      acceptance_criteria:
      - Build LCS length matrix from both input line sequences correctly
      - Backtrack through matrix to find the actual longest common subsequence
      - Handle input sequences of different lengths without index errors
      - Achieve O(mn) time and space complexity for the dynamic programming solution
      hints:
        level1: 'LCS matrix: if items match, dp[i][j] = dp[i-1][j-1] + 1. Otherwise, max of left or up.'
        level2: 'Backtrack from dp[m][n]: if match, include item and go diagonal. Otherwise, go to larger neighbor.'
        level3: |-
          def lcs_matrix(seq1: list, seq2: list) -> list[list[int]]:
              '''Build LCS length matrix'''
              m, n = len(seq1), len(seq2)

              # dp[i][j] = length of LCS of seq1[:i] and seq2[:j]
              dp = [[0] * (n + 1) for _ in range(m + 1)]

              for i in range(1, m + 1):
                  for j in range(1, n + 1):
                      if seq1[i-1] == seq2[j-1]:
                          dp[i][j] = dp[i-1][j-1] + 1
                      else:
                          dp[i][j] = max(dp[i-1][j], dp[i][j-1])

              return dp

          def backtrack_lcs(dp: list[list[int]], seq1: list, seq2: list) -> list:
              '''Backtrack to find LCS'''
              lcs = []
              i, j = len(seq1), len(seq2)

              while i > 0 and j > 0:
                  if seq1[i-1] == seq2[j-1]:
                      lcs.append((i-1, j-1, seq1[i-1]))  # (idx1, idx2, item)
                      i -= 1
                      j -= 1
                  elif dp[i-1][j] > dp[i][j-1]:
                      i -= 1
                  else:
                      j -= 1

              lcs.reverse()
              return lcs
      pitfalls:
      - Off-by-one errors in matrix indexing
      - Not handling empty sequences
      - Memory explosion for large files (optimize with Hirschberg)
      concepts:
      - Dynamic programming
      - 2D matrices
      - Backtracking
      estimated_hours: 4-5
      deliverables:
      - Longest common subsequence computation for line sequences
      - Dynamic programming table construction for optimal alignment
      - Backtracking implementation to recover actual LCS from table
      - Memory optimization to reduce space usage for large files
    - id: 3
      name: Diff Generation
      description: Convert LCS result into diff hunks with context.
      acceptance_criteria:
      - Mark lines as unchanged, added, or deleted using diff algorithm output
      - Generate unified diff format with -/+ prefixes for changed lines
      - Group changes into hunks with configurable context lines between them
      - Include @@ line range markers showing hunk positions in both files
      hints:
        level1: Walk both sequences. If in LCS, it's unchanged. Otherwise, if only in seq1, deleted. Only in seq2, added.
        level2: 'Unified format: context lines have '' '' prefix, deletions ''-'', additions ''+''.'
        level3: |-
          def generate_diff(lines1: list[str], lines2: list[str], context: int = 3):
              '''Generate unified diff output'''
              lcs_indices = set()
              for idx1, idx2, _ in backtrack_lcs(lcs_matrix(lines1, lines2), lines1, lines2):
                  lcs_indices.add(('1', idx1))
                  lcs_indices.add(('2', idx2))

              # Build diff operations
              ops = []
              i, j = 0, 0

              while i < len(lines1) or j < len(lines2):
                  if ('1', i) in lcs_indices and ('2', j) in lcs_indices:
                      ops.append((' ', lines1[i]))
                      i += 1
                      j += 1
                  elif ('1', i) not in lcs_indices and i < len(lines1):
                      ops.append(('-', lines1[i]))
                      i += 1
                  elif ('2', j) not in lcs_indices and j < len(lines2):
                      ops.append(('+', lines2[j]))
                      j += 1

              # Group into hunks with context
              hunks = []
              current_hunk = []
              # ... group changes separated by more than context*2 unchanged lines

              # Output
              for hunk in hunks:
                  print(f"@@ -{hunk.start1},{hunk.len1} +{hunk.start2},{hunk.len2} @@")
                  for op, line in hunk.lines:
                      print(f"{op}{line}")
      pitfalls:
      - Off-by-one in line numbers (diff format is 1-indexed)
      - Forgetting to handle files with no common lines
      - Context overlapping between hunks
      concepts:
      - Diff algorithms
      - Unified diff format
      - Hunk generation
      estimated_hours: 4-5
      deliverables:
      - Edit script generation marking lines as added, deleted, or unchanged
      - Chunk (hunk) formation grouping nearby changes together
      - Context lines inclusion surrounding each hunk for readability
      - Unified diff format output compatible with standard tools
    - id: 4
      name: CLI and Color Output
      description: Build command-line interface with colored output and options.
      acceptance_criteria:
      - Accept two file paths as command-line arguments for comparison
      - Color output shows red for deletions and green for additions in terminal
      - --no-color flag disables ANSI codes for plain text output
      - --context N flag sets the number of context lines around each hunk
      - Exit code 0 if files are same and 1 if files are different
      hints:
        level1: 'Use ANSI escape codes: \033[31m for red, \033[32m for green, \033[0m to reset.'
        level2: Check if stdout is a TTY before using colors. Use --no-color to force plain.
        level3: |-
          import argparse
          import sys

          class Colors:
              RED = '\033[31m'
              GREEN = '\033[32m'
              CYAN = '\033[36m'
              RESET = '\033[0m'

              @classmethod
              def disable(cls):
                  cls.RED = cls.GREEN = cls.CYAN = cls.RESET = ''

          def main():
              parser = argparse.ArgumentParser(description='Compare two files')
              parser.add_argument('file1', help='First file')
              parser.add_argument('file2', help='Second file')
              parser.add_argument('-u', '--unified', type=int, default=3,
                                  help='Number of context lines (default: 3)')
              parser.add_argument('--no-color', action='store_true',
                                  help='Disable colored output')
              args = parser.parse_args()

              if args.no_color or not sys.stdout.isatty():
                  Colors.disable()

              lines1 = read_lines(args.file1)
              lines2 = read_lines(args.file2)

              if lines1 == lines2:
                  sys.exit(0)

              # Print header
              print(f"{Colors.CYAN}--- {args.file1}{Colors.RESET}")
              print(f"{Colors.CYAN}+++ {args.file2}{Colors.RESET}")

              for hunk in generate_hunks(lines1, lines2, args.unified):
                  print(f"{Colors.CYAN}@@ ... @@{Colors.RESET}")
                  for op, line in hunk:
                      if op == '-':
                          print(f"{Colors.RED}-{line}{Colors.RESET}")
                      elif op == '+':
                          print(f"{Colors.GREEN}+{line}{Colors.RESET}")
                      else:
                          print(f" {line}")

              sys.exit(1)
      pitfalls:
      - ANSI codes break when piped to file
      - Windows needs special handling for colors
      - Exit codes are important for scripting
      concepts:
      - CLI argument parsing
      - ANSI colors
      - Exit codes
      - TTY detection
      estimated_hours: 3-4
      deliverables:
      - Argument parsing for file paths and formatting options
      - ANSI color output for visual diff highlighting in terminal
      - Context line count configuration via command-line flag
      - Side-by-side display option for parallel comparison view
  disassembler:
    id: disassembler
    name: x86 Disassembler
    description: Build an x86/x64 instruction disassembler. Learn machine code encoding, instruction formats, and binary parsing.
    difficulty: advanced
    estimated_hours: 35-50
    prerequisites:
    - x86 assembly basics
    - Binary file handling
    - Bitwise operations
    languages:
      recommended:
      - C
      - Rust
      - Python
      also_possible:
      - Go
      - C++
    resources:
    - name: Intel x86 Manual Vol. 2
      url: https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html
      type: reference
    - name: Medium - Building x86-64 Disassembler
      url: https://medium.com/@Koukyosyumei/learning-x86-64-machine-language-and-assembly-by-implementing-a-disassembler-dccc736ae85f
      type: tutorial
    - name: x86 Instruction Encoding
      url: http://wiki.osdev.org/X86-64_Instruction_Encoding
      type: reference
    milestones:
    - id: 1
      name: Binary File Loading
      description: Load and parse ELF or PE executable headers to find code sections.
      acceptance_criteria:
      - Parse ELF header fields (or PE header for Windows binaries) correctly
      - Locate .text section containing executable code for disassembly
      - Extract code bytes and base address for instruction decoding
      - Handle both 32-bit and 64-bit binary formats without errors
      hints:
        level1: 'ELF magic: 0x7F ''E'' ''L'' ''F''. Check byte 5 for 32/64-bit. Section headers list all sections.'
        level2: e_shoff points to section header table. e_shstrndx is section name string table index.
        level3: |-
          from dataclasses import dataclass
          from struct import unpack

          @dataclass
          class Section:
              name: str
              addr: int
              offset: int
              size: int
              data: bytes

          def parse_elf(data: bytes) -> tuple[int, list[Section]]:
              '''Parse ELF and return (bitness, sections)'''
              if data[:4] != b'\x7fELF':
                  raise ValueError("Not an ELF file")

              is_64bit = data[4] == 2
              is_little = data[5] == 1
              endian = '<' if is_little else '>'

              if is_64bit:
                  # 64-bit header
                  e_shoff = unpack(f'{endian}Q', data[40:48])[0]
                  e_shentsize = unpack(f'{endian}H', data[58:60])[0]
                  e_shnum = unpack(f'{endian}H', data[60:62])[0]
                  e_shstrndx = unpack(f'{endian}H', data[62:64])[0]
              else:
                  # 32-bit header
                  e_shoff = unpack(f'{endian}I', data[32:36])[0]
                  e_shentsize = unpack(f'{endian}H', data[46:48])[0]
                  e_shnum = unpack(f'{endian}H', data[48:50])[0]
                  e_shstrndx = unpack(f'{endian}H', data[50:52])[0]

              # Parse section headers
              sections = []
              # ... read section names from shstrtab, extract .text

              return (64 if is_64bit else 32, sections)
      pitfalls:
      - Endianness varies (most x86 is little-endian)
      - Virtual address != file offset
      - Some binaries are stripped (no symbol table)
      concepts:
      - Executable formats
      - Binary parsing
      - Memory mapping
      estimated_hours: 6-8
      deliverables:
      - ELF or PE header parsing for binary format detection
      - Section identification locating code and data segments
      - Entry point detection from executable header fields
      - Symbol table loading for function name resolution
    - id: 2
      name: Instruction Prefixes
      description: 'Decode x86 instruction prefixes: legacy, REX, VEX.'
      acceptance_criteria:
      - Detect legacy prefixes (66h, 67h, F0h, F2h, F3h, segment overrides) correctly
      - Decode REX prefix byte in 64-bit mode for register extension
      - Extract REX.W, REX.R, REX.X, and REX.B bit fields from prefix
      - Handle prefix ordering and multiple prefixes on single instruction
      hints:
        level1: 'REX prefix: 0x40-0x4F in 64-bit mode. Format: 0100WRXB.'
        level2: 66h = operand size override, 67h = address size override. They change default sizes.
        level3: |-
          @dataclass
          class Prefixes:
              rex: int = 0        # Full REX byte (0 if none)
              rex_w: bool = False # 64-bit operand
              rex_r: bool = False # Extends ModRM.reg
              rex_x: bool = False # Extends SIB.index
              rex_b: bool = False # Extends ModRM.rm or SIB.base
              operand_size: bool = False  # 66h prefix
              address_size: bool = False  # 67h prefix
              rep: int = 0        # F2/F3 prefix
              lock: bool = False  # F0 prefix
              segment: int = 0    # Segment override

          def decode_prefixes(code: bytes, offset: int, is_64bit: bool) -> tuple[Prefixes, int]:
              '''Decode prefixes, return (Prefixes, bytes_consumed)'''
              prefixes = Prefixes()
              pos = offset

              while pos < len(code):
                  byte = code[pos]

                  if byte == 0x66:
                      prefixes.operand_size = True
                  elif byte == 0x67:
                      prefixes.address_size = True
                  elif byte == 0xF0:
                      prefixes.lock = True
                  elif byte in (0xF2, 0xF3):
                      prefixes.rep = byte
                  elif byte in (0x2E, 0x3E, 0x26, 0x64, 0x65, 0x36):
                      prefixes.segment = byte
                  elif is_64bit and 0x40 <= byte <= 0x4F:
                      prefixes.rex = byte
                      prefixes.rex_w = bool(byte & 0x08)
                      prefixes.rex_r = bool(byte & 0x04)
                      prefixes.rex_x = bool(byte & 0x02)
                      prefixes.rex_b = bool(byte & 0x01)
                  else:
                      break  # Not a prefix

                  pos += 1

              return prefixes, pos - offset
      pitfalls:
      - REX must be immediately before opcode
      - Some prefix combinations are invalid or have special meaning
      - VEX/EVEX prefixes in modern code need separate handling
      concepts:
      - Prefix encoding
      - Mode-dependent behavior
      - Bit field extraction
      estimated_hours: 6-8
      deliverables:
      - REX prefix handling for 64-bit register extensions
      - Operand size prefix detection for 16/32-bit overrides
      - Address size prefix detection for addressing mode overrides
      - Segment override prefix handling for non-default segments
    - id: 3
      name: Opcode Tables
      description: Build opcode lookup tables for instruction identification.
      acceptance_criteria:
      - One-byte opcode table maps all primary opcodes to instruction mnemonics
      - Two-byte opcode table handles 0F xx extended instruction encodings
      - Handle opcode extensions via ModRM.reg field for multiplexed opcodes
      - Map opcodes to mnemonics with correct operand size and type information
      hints:
        level1: Many opcodes share byte but differ by ModRM.reg field. E.g., 0x80 is ADD/OR/ADC/SBB/AND/SUB/XOR/CMP.
        level2: 'Create separate tables: one_byte_opcodes, two_byte_opcodes, extension_groups.'
        level3: |-
          # Opcode table entry
          @dataclass
          class OpcodeEntry:
              mnemonic: str
              operands: str  # e.g., "Ev,Gv" or "rAX,Iz"
              flags: int = 0
              extension_group: int = 0  # 0 = no extension, 1-17 = group number

          # One-byte opcode table (partial)
          ONE_BYTE = {
              0x00: OpcodeEntry("ADD", "Eb,Gb"),
              0x01: OpcodeEntry("ADD", "Ev,Gv"),
              0x02: OpcodeEntry("ADD", "Gb,Eb"),
              0x03: OpcodeEntry("ADD", "Gv,Ev"),
              0x04: OpcodeEntry("ADD", "AL,Ib"),
              0x05: OpcodeEntry("ADD", "rAX,Iz"),
              # ...
              0x50: OpcodeEntry("PUSH", "rAX"),  # 50-57 are PUSH r16/r64
              # ...
              0x80: OpcodeEntry("", "Eb,Ib", extension_group=1),  # Group 1
              0x81: OpcodeEntry("", "Ev,Iz", extension_group=1),
              # ...
              0x0F: OpcodeEntry("", "", flags=TWO_BYTE),  # Escape to two-byte table
          }

          # Extension group 1: ADD/OR/ADC/SBB/AND/SUB/XOR/CMP
          GROUP_1 = ["ADD", "OR", "ADC", "SBB", "AND", "SUB", "XOR", "CMP"]

          def lookup_opcode(byte: int, modrm_reg: int = 0) -> OpcodeEntry:
              entry = ONE_BYTE.get(byte)
              if entry and entry.extension_group:
                  mnemonic = GROUPS[entry.extension_group][modrm_reg]
                  return OpcodeEntry(mnemonic, entry.operands)
              return entry
      pitfalls:
      - Opcode tables are large and error-prone to type manually
      - Some opcodes are invalid in certain modes
      - Three-byte opcodes exist (0F 38 xx, 0F 3A xx)
      concepts:
      - Lookup tables
      - Instruction encoding
      - Opcode extensions
      estimated_hours: 8-10
      deliverables:
      - Primary opcode lookup table for single-byte opcodes
      - Extended opcode table for two-byte 0F-prefixed instructions
      - VEX and EVEX prefix handling for SIMD instructions
      - Instruction metadata including operand types and sizes
    - id: 4
      name: ModRM and SIB Decoding
      description: Decode ModRM and SIB bytes for operand addressing.
      acceptance_criteria:
      - Parse ModRM byte into mod, reg, and rm fields with correct bit masking
      - Handle SIB byte decoding when ModRM rm field equals 100 binary
      - Decode all 16 addressing modes including register and memory variants
      - Handle RIP-relative addressing mode used in 64-bit position-independent code
      hints:
        level1: ModRM = mod(2) | reg(3) | rm(3). mod=11 means register, otherwise memory.
        level2: SIB = scale(2) | index(3) | base(3). Address = base + index * scale + disp.
        level3: |-
          def decode_modrm(code: bytes, offset: int, prefixes: Prefixes, is_64bit: bool) -> tuple[str, int]:
              '''Decode ModRM, return (operand_string, bytes_consumed)'''
              modrm = code[offset]
              mod = (modrm >> 6) & 0x3
              reg = (modrm >> 3) & 0x7
              rm = modrm & 0x7

              # Apply REX extensions
              if prefixes.rex_r:
                  reg |= 0x8
              if prefixes.rex_b:
                  rm |= 0x8

              consumed = 1

              if mod == 0b11:
                  # Register direct
                  return get_register(rm, prefixes), consumed

              # Memory addressing
              if rm == 0b100 and not is_64bit:
                  # SIB follows (32-bit) or rm=100 with REX.B (64-bit)
                  sib = code[offset + 1]
                  consumed += 1
                  scale = 1 << ((sib >> 6) & 0x3)
                  index = (sib >> 3) & 0x7
                  base = sib & 0x7

                  if prefixes.rex_x:
                      index |= 0x8
                  if prefixes.rex_b:
                      base |= 0x8

                  # Build address string
                  # ...

              if mod == 0b00 and rm == 0b101:
                  if is_64bit:
                      # RIP-relative
                      disp = unpack('<i', code[offset+consumed:offset+consumed+4])[0]
                      consumed += 4
                      return f"[rip + {disp:#x}]", consumed
                  else:
                      # 32-bit displacement only
                      disp = unpack('<i', code[offset+consumed:offset+consumed+4])[0]
                      consumed += 4
                      return f"[{disp:#x}]", consumed

              # ... handle other mod values
      pitfalls:
      - RIP-relative is 64-bit only, in 32-bit mod=00 rm=101 is disp32
      - SIB with index=100 means no index (unless REX.X set)
      - REX extends registers to r8-r15
      concepts:
      - Addressing modes
      - Register encoding
      - Memory operands
      estimated_hours: 8-10
      deliverables:
      - ModRM byte parsing extracting mod, reg, and rm fields
      - Register operand decoding from reg and rm field values
      - Memory operand decoding with displacement and base register
      - SIB byte handling for scaled-index-base addressing modes
    - id: 5
      name: Output Formatting
      description: Format disassembly output with addresses, bytes, and mnemonics.
      acceptance_criteria:
      - Show address, hex bytes, mnemonic, and operands for each instruction
      - Support both Intel and AT&T syntax output via command-line option
      - Label jump and call targets with known symbol names where available
      - Handle undefined or invalid opcodes gracefully without crashing disassembler
      hints:
        level1: 'Intel: op dst, src. AT&T: op src, dst (with size suffixes and % for registers).'
        level2: Track branch targets to insert labels. Relative jumps need base address calculation.
        level3: |-
          class Disassembler:
              def __init__(self, code: bytes, base_addr: int, is_64bit: bool):
                  self.code = code
                  self.base = base_addr
                  self.is_64bit = is_64bit
                  self.labels = {}  # addr -> label name

              def disassemble(self) -> list[str]:
                  # First pass: find branch targets
                  offset = 0
                  while offset < len(self.code):
                      instr = self.decode_instruction(offset)
                      if instr.is_branch:
                          target = instr.branch_target
                          if target not in self.labels:
                              self.labels[target] = f"loc_{target:x}"
                      offset += instr.length

                  # Second pass: generate output
                  output = []
                  offset = 0
                  while offset < len(self.code):
                      addr = self.base + offset

                      # Insert label if this is a branch target
                      if addr in self.labels:
                          output.append(f"\n{self.labels[addr]}:")

                      instr = self.decode_instruction(offset)
                      hex_bytes = self.code[offset:offset+instr.length].hex()

                      output.append(f"{addr:08x}:  {hex_bytes:<20}  {instr}")
                      offset += instr.length

                  return output

              def format_intel(self, mnemonic: str, operands: list[str]) -> str:
                  return f"{mnemonic} {', '.join(operands)}"

              def format_att(self, mnemonic: str, operands: list[str]) -> str:
                  # Reverse operands, add suffixes
                  return f"{mnemonic} {', '.join(reversed(operands))}"

          # Output example:
          # 00401000:  55                    push rbp
          # 00401001:  48 89 e5              mov rbp, rsp
          # 00401004:  e8 17 00 00 00        call loc_401020
      pitfalls:
      - Hex bytes should be padded/aligned for readability
      - Relative addresses need base address to calculate target
      - Invalid opcodes should print as .byte or db
      concepts:
      - Assembly syntax
      - Address calculation
      - Output formatting
      estimated_hours: 6-8
      deliverables:
      - Intel syntax output with destination-first operand ordering
      - AT&T syntax option with source-first operand ordering
      - Address and raw bytes display alongside decoded mnemonics
      - Symbol resolution replacing addresses with known function names
  distributed-cache:
    id: distributed-cache
    name: Distributed Cache
    description: Build a distributed cache with consistent hashing.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - Hash tables
    - Networking basics
    - Consistent hashing concept
    languages:
      recommended:
      - Go
      - Java
      - Python
      also_possible:
      - Rust
      - JavaScript
    resources:
    - type: article
      name: Consistent Hashing
      url: https://www.toptal.com/big-data/consistent-hashing
    - type: paper
      name: Dynamo Paper
      url: https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf
    milestones:
    - id: 1
      name: Consistent Hash Ring
      description: Implement consistent hashing for key distribution.
      acceptance_criteria:
      - Hash ring distributes keys evenly across available cache nodes
      - Virtual nodes reduce hotspot risk and improve distribution balance
      - Node addition or removal only remaps minimal subset of existing keys
      - Key lookup returns correct target node in constant time complexity
      hints:
        level1: Hash both nodes and keys to positions on a ring (0 to 2^32-1).
        level2: 'Virtual nodes: each physical node gets multiple positions for better distribution.'
        level3: "## Consistent Hash Ring\n\n```go\npackage cache\n\nimport (\n    \"hash/crc32\"\n    \"sort\"\n    \"sync\"\
          \n)\n\ntype ConsistentHash struct {\n    mu           sync.RWMutex\n    ring         []uint32          // Sorted\
          \ hash values\n    nodes        map[uint32]string // Hash -> node ID\n    virtualNodes int               // Virtual\
          \ nodes per physical node\n}\n\nfunc NewConsistentHash(virtualNodes int) *ConsistentHash {\n    return &ConsistentHash{\n\
          \        nodes:        make(map[uint32]string),\n        virtualNodes: virtualNodes,\n    }\n}\n\nfunc (c *ConsistentHash)\
          \ hash(key string) uint32 {\n    return crc32.ChecksumIEEE([]byte(key))\n}\n\nfunc (c *ConsistentHash) AddNode(nodeID\
          \ string) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    for i := 0; i < c.virtualNodes; i++ {\n       \
          \ virtualKey := fmt.Sprintf(\"%s-%d\", nodeID, i)\n        h := c.hash(virtualKey)\n        c.ring = append(c.ring,\
          \ h)\n        c.nodes[h] = nodeID\n    }\n    \n    sort.Slice(c.ring, func(i, j int) bool {\n        return c.ring[i]\
          \ < c.ring[j]\n    })\n}\n\nfunc (c *ConsistentHash) RemoveNode(nodeID string) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n\
          \    \n    newRing := make([]uint32, 0)\n    for _, h := range c.ring {\n        if c.nodes[h] != nodeID {\n   \
          \         newRing = append(newRing, h)\n        } else {\n            delete(c.nodes, h)\n        }\n    }\n   \
          \ c.ring = newRing\n}\n\nfunc (c *ConsistentHash) GetNode(key string) string {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n\
          \    \n    if len(c.ring) == 0 {\n        return \"\"\n    }\n    \n    h := c.hash(key)\n    \n    // Binary search\
          \ for first node >= hash\n    idx := sort.Search(len(c.ring), func(i int) bool {\n        return c.ring[i] >= h\n\
          \    })\n    \n    if idx == len(c.ring) {\n        idx = 0  // Wrap around\n    }\n    \n    return c.nodes[c.ring[idx]]\n\
          }\n\n// Get N nodes for replication\nfunc (c *ConsistentHash) GetNodes(key string, n int) []string {\n    c.mu.RLock()\n\
          \    defer c.mu.RUnlock()\n    \n    if len(c.ring) == 0 {\n        return nil\n    }\n    \n    h := c.hash(key)\n\
          \    idx := sort.Search(len(c.ring), func(i int) bool {\n        return c.ring[i] >= h\n    })\n    \n    seen :=\
          \ make(map[string]bool)\n    result := make([]string, 0, n)\n    \n    for i := 0; len(result) < n && i < len(c.ring);\
          \ i++ {\n        nodeID := c.nodes[c.ring[(idx+i)%len(c.ring)]]\n        if !seen[nodeID] {\n            seen[nodeID]\
          \ = true\n            result = append(result, nodeID)\n        }\n    }\n    \n    return result\n}\n```"
      pitfalls:
      - Poor hash distribution
      - Not enough virtual nodes
      - Race conditions
      concepts:
      - Consistent hashing
      - Load balancing
      - Ring topology
      estimated_hours: 6-10
      deliverables:
      - Hash ring implementation mapping keys to node positions
      - Virtual nodes for balanced key distribution across nodes
      - Key to node mapping using consistent hashing algorithm
      - Ring rebalancing on node addition or removal events
    - id: 2
      name: Cache Node Implementation
      description: Implement a cache node with LRU eviction.
      acceptance_criteria:
      - Get, set, and delete operations function correctly under concurrent access
      - LRU eviction removes least recently used entries when memory limit is reached
      - TTL support expires keys automatically after their configured time-to-live
      - Memory limit enforced by evicting entries when total size exceeds threshold
      hints:
        level1: Use hashmap + doubly linked list for O(1) LRU operations.
        level2: Track memory usage, evict LRU items when limit exceeded.
        level3: "## LRU Cache Node\n\n```go\ntype CacheEntry struct {\n    key        string\n    value      []byte\n    expireAt\
          \   time.Time\n    prev, next *CacheEntry\n}\n\ntype CacheNode struct {\n    mu          sync.RWMutex\n    data\
          \        map[string]*CacheEntry\n    head, tail  *CacheEntry  // LRU list\n    maxMemory   int64\n    usedMemory\
          \  int64\n}\n\nfunc NewCacheNode(maxMemory int64) *CacheNode {\n    c := &CacheNode{\n        data:      make(map[string]*CacheEntry),\n\
          \        maxMemory: maxMemory,\n    }\n    // Initialize sentinel nodes\n    c.head = &CacheEntry{}\n    c.tail\
          \ = &CacheEntry{}\n    c.head.next = c.tail\n    c.tail.prev = c.head\n    return c\n}\n\nfunc (c *CacheNode) moveToFront(entry\
          \ *CacheEntry) {\n    // Remove from current position\n    entry.prev.next = entry.next\n    entry.next.prev = entry.prev\n\
          \    \n    // Add to front\n    entry.next = c.head.next\n    entry.prev = c.head\n    c.head.next.prev = entry\n\
          \    c.head.next = entry\n}\n\nfunc (c *CacheNode) Get(key string) ([]byte, bool) {\n    c.mu.Lock()\n    defer\
          \ c.mu.Unlock()\n    \n    entry, ok := c.data[key]\n    if !ok {\n        return nil, false\n    }\n    \n    //\
          \ Check expiration\n    if !entry.expireAt.IsZero() && time.Now().After(entry.expireAt) {\n        c.removeEntry(entry)\n\
          \        return nil, false\n    }\n    \n    c.moveToFront(entry)\n    return entry.value, true\n}\n\nfunc (c *CacheNode)\
          \ Set(key string, value []byte, ttl time.Duration) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    entrySize\
          \ := int64(len(key) + len(value))\n    \n    // Evict if necessary\n    for c.usedMemory + entrySize > c.maxMemory\
          \ && c.tail.prev != c.head {\n        c.removeEntry(c.tail.prev)\n    }\n    \n    entry := &CacheEntry{\n     \
          \   key:   key,\n        value: value,\n    }\n    if ttl > 0 {\n        entry.expireAt = time.Now().Add(ttl)\n\
          \    }\n    \n    // Remove old entry if exists\n    if old, ok := c.data[key]; ok {\n        c.removeEntry(old)\n\
          \    }\n    \n    // Add new entry\n    c.data[key] = entry\n    c.usedMemory += entrySize\n    \n    // Add to\
          \ front of LRU list\n    entry.next = c.head.next\n    entry.prev = c.head\n    c.head.next.prev = entry\n    c.head.next\
          \ = entry\n}\n\nfunc (c *CacheNode) removeEntry(entry *CacheEntry) {\n    entry.prev.next = entry.next\n    entry.next.prev\
          \ = entry.prev\n    \n    c.usedMemory -= int64(len(entry.key) + len(entry.value))\n    delete(c.data, entry.key)\n\
          }\n```"
      pitfalls:
      - Memory accounting errors
      - TTL cleanup overhead
      - Lock contention
      concepts:
      - LRU cache
      - Memory management
      - TTL expiration
      estimated_hours: 6-10
      deliverables:
      - LRU cache implementation per node with configurable capacity
      - Get, set, and delete operations with proper concurrency handling
      - TTL support for automatic key expiration after timeout
      - Memory limit enforcement with eviction when capacity is reached
    - id: 3
      name: Cluster Communication
      description: Implement inter-node communication and routing.
      acceptance_criteria:
      - Node discovery detects new nodes joining the cluster automatically
      - Request routing forwards cache operations to the correct hash ring node
      - Health checks detect and mark unresponsive nodes within timeout period
      - Cluster membership updates propagate to all nodes via gossip protocol
      hints:
        level1: Each node needs to know about others. Route requests to correct node.
        level2: Use gossip or central coordinator for membership. Health check regularly.
        level3: "## Cluster Communication\n\n```go\ntype ClusterNode struct {\n    ID       string\n    Address  string\n\
          \    Healthy  bool\n    LastSeen time.Time\n}\n\ntype CacheCluster struct {\n    localNode   *CacheNode\n    nodeID\
          \      string\n    hashRing    *ConsistentHash\n    nodes       map[string]*ClusterNode\n    mu          sync.RWMutex\n\
          }\n\nfunc (c *CacheCluster) Get(key string) ([]byte, error) {\n    targetNode := c.hashRing.GetNode(key)\n    \n\
          \    if targetNode == c.nodeID {\n        // Local\n        val, ok := c.localNode.Get(key)\n        if !ok {\n\
          \            return nil, ErrNotFound\n        }\n        return val, nil\n    }\n    \n    // Remote\n    return\
          \ c.forwardGet(targetNode, key)\n}\n\nfunc (c *CacheCluster) forwardGet(nodeID, key string) ([]byte, error) {\n\
          \    c.mu.RLock()\n    node, ok := c.nodes[nodeID]\n    c.mu.RUnlock()\n    \n    if !ok || !node.Healthy {\n  \
          \      return nil, ErrNodeUnavailable\n    }\n    \n    // HTTP request to remote node\n    resp, err := http.Get(fmt.Sprintf(\"\
          http://%s/cache/%s\", node.Address, key))\n    if err != nil {\n        return nil, err\n    }\n    defer resp.Body.Close()\n\
          \    \n    if resp.StatusCode == http.StatusNotFound {\n        return nil, ErrNotFound\n    }\n    \n    return\
          \ io.ReadAll(resp.Body)\n}\n\n// Health checker\nfunc (c *CacheCluster) healthCheck() {\n    ticker := time.NewTicker(5\
          \ * time.Second)\n    for range ticker.C {\n        c.mu.Lock()\n        for id, node := range c.nodes {\n     \
          \       if id == c.nodeID {\n                continue\n            }\n            \n            resp, err := http.Get(fmt.Sprintf(\"\
          http://%s/health\", node.Address))\n            if err != nil || resp.StatusCode != http.StatusOK {\n          \
          \      node.Healthy = false\n                if time.Since(node.LastSeen) > 30*time.Second {\n                 \
          \   // Remove dead node\n                    c.hashRing.RemoveNode(id)\n                    delete(c.nodes, id)\n\
          \                }\n            } else {\n                node.Healthy = true\n                node.LastSeen = time.Now()\n\
          \            }\n        }\n        c.mu.Unlock()\n    }\n}\n```"
      pitfalls:
      - Split brain scenarios
      - Network partition handling
      - Stale membership info
      concepts:
      - Distributed systems
      - Health checking
      - Service discovery
      estimated_hours: 8-12
      deliverables:
      - Node discovery mechanism for automatic cluster membership
      - Gossip protocol for propagating cluster state between nodes
      - Request routing forwarding operations to the correct node
      - Node health checking with configurable probe intervals
    - id: 4
      name: Replication & Consistency
      description: Add data replication for fault tolerance.
      acceptance_criteria:
      - Configurable replication factor stores copies on N successor nodes
      - Read and write quorums ensure desired consistency level per operation
      - Conflict resolution handles divergent values using configurable strategy
      - Anti-entropy process repairs inconsistent replicas in the background
      hints:
        level1: 'Replicate to N nodes. Use quorums: W + R > N for consistency.'
        level2: Last-write-wins or vector clocks for conflict resolution.
        level3: "## Replication\n\n```go\ntype ReplicatedCache struct {\n    cluster         *CacheCluster\n    replicationFactor\
          \ int  // N\n    writeQuorum     int  // W\n    readQuorum      int  // R\n}\n\nfunc (r *ReplicatedCache) Set(key\
          \ string, value []byte, ttl time.Duration) error {\n    nodes := r.cluster.hashRing.GetNodes(key, r.replicationFactor)\n\
          \    \n    // Write to all replicas, wait for quorum\n    results := make(chan error, len(nodes))\n    for _, nodeID\
          \ := range nodes {\n        go func(id string) {\n            results <- r.cluster.writeToNode(id, key, value, ttl)\n\
          \        }(nodeID)\n    }\n    \n    successCount := 0\n    var lastErr error\n    for i := 0; i < len(nodes); i++\
          \ {\n        if err := <-results; err == nil {\n            successCount++\n            if successCount >= r.writeQuorum\
          \ {\n                return nil  // Quorum achieved\n            }\n        } else {\n            lastErr = err\n\
          \        }\n    }\n    \n    return fmt.Errorf(\"write quorum not achieved: %v\", lastErr)\n}\n\nfunc (r *ReplicatedCache)\
          \ Get(key string) ([]byte, error) {\n    nodes := r.cluster.hashRing.GetNodes(key, r.replicationFactor)\n    \n\
          \    type readResult struct {\n        value     []byte\n        timestamp int64\n        err       error\n    }\n\
          \    results := make(chan readResult, len(nodes))\n    \n    for _, nodeID := range nodes {\n        go func(id\
          \ string) {\n            val, ts, err := r.cluster.readFromNode(id, key)\n            results <- readResult{val,\
          \ ts, err}\n        }(nodeID)\n    }\n    \n    var values []readResult\n    for i := 0; i < len(nodes); i++ {\n\
          \        res := <-results\n        if res.err == nil {\n            values = append(values, res)\n            if\
          \ len(values) >= r.readQuorum {\n                break\n            }\n        }\n    }\n    \n    if len(values)\
          \ < r.readQuorum {\n        return nil, ErrReadQuorumNotMet\n    }\n    \n    // Return most recent value (last-write-wins)\n\
          \    latest := values[0]\n    for _, v := range values[1:] {\n        if v.timestamp > latest.timestamp {\n    \
          \        latest = v\n        }\n    }\n    \n    return latest.value, nil\n}\n```"
      pitfalls:
      - Stale reads
      - Write conflicts
      - Replica divergence
      concepts:
      - Quorum systems
      - Eventual consistency
      - Conflict resolution
      estimated_hours: 8-12
      deliverables:
      - Replication factor configuration for data redundancy level
      - Read and write quorum settings for consistency control
      - Consistency level options from eventual to strong consistency
      - Conflict resolution using vector clocks or last-write-wins strategy
  distributed-tracing:
    id: distributed-tracing
    name: Distributed Tracing System
    description: Build a distributed tracing system to track requests across microservices.
    difficulty: advanced
    estimated_hours: 40-60
    prerequisites:
    - Microservices
    - Networking
    - Data storage
    languages:
      recommended:
      - Go
      - Java
      also_possible:
      - Rust
      - Python
    resources:
    - name: OpenTelemetry
      url: https://opentelemetry.io/docs/
      type: documentation
    - name: Jaeger Architecture
      url: https://www.jaegertracing.io/docs/architecture/
      type: documentation
    milestones:
    - id: 1
      name: Trace Context & Propagation
      description: Implement trace/span IDs and context propagation.
      acceptance_criteria:
      - Generate globally unique trace and span IDs for each operation
      - Propagate trace context via HTTP headers using W3C Trace Context format
      - Propagate trace context via gRPC metadata for inter-service tracing
      - Parent-child span relationships correctly model the call tree hierarchy
      - Context injection and extraction work across HTTP and gRPC boundaries
      hints:
        level1: Trace ID = request ID across services. Span ID = operation within service.
        level2: 'W3C traceparent: version-traceid-spanid-flags (00-{32hex}-{16hex}-01).'
        level3: |-
          import (
              "crypto/rand"
              "encoding/hex"
              "context"
              "net/http"
          )

          type TraceID [16]byte
          type SpanID [8]byte

          func (t TraceID) String() string { return hex.EncodeToString(t[:]) }
          func (s SpanID) String() string  { return hex.EncodeToString(s[:]) }

          func NewTraceID() TraceID {
              var id TraceID
              rand.Read(id[:])
              return id
          }

          func NewSpanID() SpanID {
              var id SpanID
              rand.Read(id[:])
              return id
          }

          type SpanContext struct {
              TraceID    TraceID
              SpanID     SpanID
              ParentID   SpanID
              TraceFlags byte
          }

          // W3C Trace Context format
          const (
              traceparentHeader = "traceparent"
              tracestateHeader  = "tracestate"
          )

          type Propagator struct{}

          func (p *Propagator) Inject(ctx context.Context, carrier http.Header) {
              sc := SpanContextFromContext(ctx)
              if sc == nil {
                  return
              }

              // Format: 00-{trace-id}-{span-id}-{trace-flags}
              traceparent := fmt.Sprintf("00-%s-%s-%02x",
                  sc.TraceID.String(),
                  sc.SpanID.String(),
                  sc.TraceFlags,
              )

              carrier.Set(traceparentHeader, traceparent)
          }

          func (p *Propagator) Extract(ctx context.Context, carrier http.Header) context.Context {
              traceparent := carrier.Get(traceparentHeader)
              if traceparent == "" {
                  return ctx
              }

              // Parse: 00-{trace-id}-{span-id}-{trace-flags}
              parts := strings.Split(traceparent, "-")
              if len(parts) != 4 || parts[0] != "00" {
                  return ctx
              }

              traceID, _ := hex.DecodeString(parts[1])
              spanID, _ := hex.DecodeString(parts[2])
              flags, _ := strconv.ParseUint(parts[3], 16, 8)

              var tid TraceID
              var sid SpanID
              copy(tid[:], traceID)
              copy(sid[:], spanID)

              sc := &SpanContext{
                  TraceID:    tid,
                  ParentID:   sid,  // Incoming span becomes parent
                  SpanID:     NewSpanID(),  // Create new span for this service
                  TraceFlags: byte(flags),
              }

              return ContextWithSpanContext(ctx, sc)
          }

          // Context helpers
          type spanContextKey struct{}

          func ContextWithSpanContext(ctx context.Context, sc *SpanContext) context.Context {
              return context.WithValue(ctx, spanContextKey{}, sc)
          }

          func SpanContextFromContext(ctx context.Context) *SpanContext {
              sc, _ := ctx.Value(spanContextKey{}).(*SpanContext)
              return sc
          }

          // HTTP middleware
          func TracingMiddleware(next http.Handler) http.Handler {
              propagator := &Propagator{}

              return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
                  // Extract or create trace context
                  ctx := propagator.Extract(r.Context(), r.Header)

                  if SpanContextFromContext(ctx) == nil {
                      // No incoming trace, start new one
                      sc := &SpanContext{
                          TraceID:    NewTraceID(),
                          SpanID:     NewSpanID(),
                          TraceFlags: 0x01,  // Sampled
                      }
                      ctx = ContextWithSpanContext(ctx, sc)
                  }

                  r = r.WithContext(ctx)
                  next.ServeHTTP(w, r)
              })
          }
      pitfalls:
      - Trace ID not propagated to async operations
      - Invalid trace context parsing crashes
      - Generating new trace ID when should continue existing
      - Not handling malformed traceparent
      concepts:
      - Trace context
      - W3C Trace Context spec
      - Context propagation
      - Span hierarchy
      estimated_hours: 8-12
      deliverables:
      - Trace ID generation using 128-bit random identifiers
      - Span ID generation using 64-bit random identifiers
      - Context propagation via W3C Trace Context or B3 headers
      - Baggage items for cross-service key-value metadata propagation
    - id: 2
      name: Span Recording
      description: Record span data with timing, tags, and logs.
      acceptance_criteria:
      - Start and end spans with precise timing for duration calculation
      - Add tags and attributes to spans for filtering and searching
      - Add log events within spans to record notable activities and data
      - Record errors and exceptions with status and detailed messages on spans
      - Span status correctly reflects OK or Error outcome of the operation
      hints:
        level1: Span = name + start time + end time + attributes + events.
        level2: Buffer spans in memory, flush to collector periodically or on threshold.
        level3: |-
          type Span struct {
              TraceID     TraceID
              SpanID      SpanID
              ParentID    SpanID
              Name        string
              StartTime   time.Time
              EndTime     time.Time
              Status      SpanStatus
              Attributes  map[string]interface{}
              Events      []SpanEvent
              mu          sync.Mutex
          }

          type SpanStatus struct {
              Code    StatusCode
              Message string
          }

          type StatusCode int

          const (
              StatusUnset StatusCode = iota
              StatusOK
              StatusError
          )

          type SpanEvent struct {
              Name       string
              Timestamp  time.Time
              Attributes map[string]interface{}
          }

          type Tracer struct {
              serviceName string
              exporter    SpanExporter
              spans       chan *Span
          }

          func NewTracer(serviceName string, exporter SpanExporter) *Tracer {
              t := &Tracer{
                  serviceName: serviceName,
                  exporter:    exporter,
                  spans:       make(chan *Span, 1000),
              }

              // Background exporter
              go t.exportLoop()

              return t
          }

          func (t *Tracer) StartSpan(ctx context.Context, name string) (context.Context, *Span) {
              parentSC := SpanContextFromContext(ctx)

              span := &Span{
                  Name:       name,
                  StartTime:  time.Now(),
                  Attributes: make(map[string]interface{}),
              }

              if parentSC != nil {
                  span.TraceID = parentSC.TraceID
                  span.ParentID = parentSC.SpanID
              } else {
                  span.TraceID = NewTraceID()
              }
              span.SpanID = NewSpanID()

              // Add default attributes
              span.SetAttribute("service.name", t.serviceName)

              sc := &SpanContext{
                  TraceID:  span.TraceID,
                  SpanID:   span.SpanID,
                  ParentID: span.ParentID,
              }

              return ContextWithSpanContext(ctx, sc), span
          }

          func (s *Span) SetAttribute(key string, value interface{}) {
              s.mu.Lock()
              s.Attributes[key] = value
              s.mu.Unlock()
          }

          func (s *Span) AddEvent(name string, attrs map[string]interface{}) {
              s.mu.Lock()
              s.Events = append(s.Events, SpanEvent{
                  Name:       name,
                  Timestamp:  time.Now(),
                  Attributes: attrs,
              })
              s.mu.Unlock()
          }

          func (s *Span) RecordError(err error) {
              s.mu.Lock()
              s.Status = SpanStatus{Code: StatusError, Message: err.Error()}
              s.Events = append(s.Events, SpanEvent{
                  Name:      "exception",
                  Timestamp: time.Now(),
                  Attributes: map[string]interface{}{
                      "exception.type":    fmt.Sprintf("%T", err),
                      "exception.message": err.Error(),
                  },
              })
              s.mu.Unlock()
          }

          func (s *Span) End() {
              s.mu.Lock()
              s.EndTime = time.Now()
              s.mu.Unlock()

              // Send to exporter
              tracer.spans <- s
          }

          func (t *Tracer) exportLoop() {
              batch := make([]*Span, 0, 100)
              ticker := time.NewTicker(5 * time.Second)

              for {
                  select {
                  case span := <-t.spans:
                      batch = append(batch, span)
                      if len(batch) >= 100 {
                          t.exporter.ExportSpans(batch)
                          batch = batch[:0]
                      }

                  case <-ticker.C:
                      if len(batch) > 0 {
                          t.exporter.ExportSpans(batch)
                          batch = batch[:0]
                      }
                  }
              }
          }
      pitfalls:
      - Span End() never called (memory leak)
      - Too many attributes causes large payloads
      - High cardinality attribute values
      - Blocking on export channel
      concepts:
      - Span recording
      - Attributes and events
      - Error recording
      - Batch export
      estimated_hours: 8-12
      deliverables:
      - Span start and finish recording with precise timestamps
      - Span attributes for tagging with key-value metadata pairs
      - Span events and logs for recording in-span activities
      - Error recording with exception details and stack traces
    - id: 3
      name: Collector & Storage
      description: Build collector to receive, process, and store traces.
      acceptance_criteria:
      - Receive spans via HTTP or gRPC ingestion endpoint from instrumented services
      - Process and enrich spans with service metadata before storage
      - Store span data in time-series database with efficient indexing for queries
      - Head-based and tail-based sampling strategies reduce storage volume effectively
      - Tail-based sampling retains complete traces for slow or error requests
      hints:
        level1: Collector receives spans, buffers, writes to storage. Use Cassandra/ClickHouse for traces.
        level2: 'Tail sampling: wait for trace completion, sample based on latency/errors.'
        level3: |-
          // Collector service
          type Collector struct {
              storage   TraceStorage
              processor SpanProcessor
              sampler   Sampler
          }

          type SpanProcessor interface {
              Process(span *Span) *Span
          }

          type EnrichmentProcessor struct {
              geoIP   *geoip.DB
          }

          func (p *EnrichmentProcessor) Process(span *Span) *Span {
              // Add derived attributes
              if ip, ok := span.Attributes["http.client_ip"].(string); ok {
                  if location, err := p.geoIP.Lookup(ip); err == nil {
                      span.SetAttribute("geo.country", location.Country)
                      span.SetAttribute("geo.city", location.City)
                  }
              }

              // Calculate duration
              span.SetAttribute("duration_ms", span.EndTime.Sub(span.StartTime).Milliseconds())

              return span
          }

          // Sampling
          type Sampler interface {
              ShouldSample(span *Span) bool
          }

          type ProbabilisticSampler struct {
              rate float64
          }

          func (s *ProbabilisticSampler) ShouldSample(span *Span) bool {
              // Use trace ID for deterministic sampling
              hash := fnv.New64a()
              hash.Write(span.TraceID[:])
              return float64(hash.Sum64()%10000)/10000 < s.rate
          }

          // Tail-based sampling - sample complete traces based on their properties
          type TailSampler struct {
              pending     map[TraceID]*pendingTrace
              sampleRules []SampleRule
              mu          sync.Mutex
          }

          type pendingTrace struct {
              spans       []*Span
              firstSeen   time.Time
              rootSpan    *Span
          }

          type SampleRule struct {
              Name      string
              Condition func(trace *pendingTrace) bool
              Rate      float64
          }

          func (s *TailSampler) AddSpan(span *Span) {
              s.mu.Lock()
              defer s.mu.Unlock()

              pt, ok := s.pending[span.TraceID]
              if !ok {
                  pt = &pendingTrace{firstSeen: time.Now()}
                  s.pending[span.TraceID] = pt
              }

              pt.spans = append(pt.spans, span)

              if span.ParentID == (SpanID{}) {
                  pt.rootSpan = span
              }
          }

          func (s *TailSampler) Evaluate() []*Span {
              s.mu.Lock()
              defer s.mu.Unlock()

              var sampled []*Span

              for traceID, pt := range s.pending {
                  // Wait for trace to complete (no new spans for 10s)
                  if time.Since(pt.firstSeen) < 10*time.Second {
                      continue
                  }

                  // Evaluate sampling rules
                  shouldSample := false
                  for _, rule := range s.sampleRules {
                      if rule.Condition(pt) && rand.Float64() < rule.Rate {
                          shouldSample = true
                          break
                      }
                  }

                  if shouldSample {
                      sampled = append(sampled, pt.spans...)
                  }

                  delete(s.pending, traceID)
              }

              return sampled
          }

          // Example rules
          func ErrorTraceRule(pt *pendingTrace) bool {
              for _, span := range pt.spans {
                  if span.Status.Code == StatusError {
                      return true
                  }
              }
              return false
          }

          func SlowTraceRule(threshold time.Duration) func(*pendingTrace) bool {
              return func(pt *pendingTrace) bool {
                  if pt.rootSpan == nil {
                      return false
                  }
                  return pt.rootSpan.EndTime.Sub(pt.rootSpan.StartTime) > threshold
              }
          }
      pitfalls:
      - Memory exhaustion holding pending traces
      - Sampling bias toward short traces
      - Clock skew affecting duration calculations
      - Hot partition in storage by trace ID
      concepts:
      - Span collection
      - Enrichment
      - Head vs tail sampling
      - Trace storage
      estimated_hours: 12-18
      deliverables:
      - Span ingestion API accepting spans via HTTP and gRPC endpoints
      - Storage backend for persisting spans in a time-series database
      - Sampling strategies to reduce volume while preserving important traces
      - Batch processing for efficient span ingestion and storage writes
    - id: 4
      name: Query & Visualization
      description: Query traces and visualize in timeline view.
      acceptance_criteria:
      - Search traces by service name, operation name, and custom tag values
      - Time range queries filter traces within specified start and end timestamps
      - Trace timeline visualization displays span hierarchy with duration bars
      - Service dependency graph shows directed edges between calling and called services
      - Latency percentile calculations (p50, p95, p99) identify slow operations
      hints:
        level1: Index by service, operation, and time. Use ClickHouse for fast queries.
        level2: 'Dependency graph: aggregate parent -> child service pairs from spans.'
        level3: |-
          // Query API
          type TraceQuery struct {
              Service     string
              Operation   string
              Tags        map[string]string
              MinDuration time.Duration
              MaxDuration time.Duration
              StartTime   time.Time
              EndTime     time.Time
              Limit       int
          }

          type TraceQueryService struct {
              storage TraceStorage
          }

          func (s *TraceQueryService) Search(q TraceQuery) ([]*Trace, error) {
              // Build query
              query := `
                  SELECT trace_id, span_id, parent_id, name, start_time, end_time, attributes
                  FROM spans
                  WHERE start_time >= ? AND start_time <= ?
              `
              args := []interface{}{q.StartTime, q.EndTime}

              if q.Service != "" {
                  query += " AND service_name = ?"
                  args = append(args, q.Service)
              }

              if q.Operation != "" {
                  query += " AND name = ?"
                  args = append(args, q.Operation)
              }

              if q.MinDuration > 0 {
                  query += " AND duration_ms >= ?"
                  args = append(args, q.MinDuration.Milliseconds())
              }

              for key, value := range q.Tags {
                  query += fmt.Sprintf(" AND attributes['%s'] = ?", key)
                  args = append(args, value)
              }

              query += " ORDER BY start_time DESC LIMIT ?"
              args = append(args, q.Limit)

              // Execute and group by trace
              rows, _ := s.storage.Query(query, args...)
              return s.groupSpansToTraces(rows)
          }

          // Dependency graph
          type ServiceDependency struct {
              Parent      string
              Child       string
              CallCount   int64
              ErrorCount  int64
              AvgDuration float64
          }

          func (s *TraceQueryService) GetDependencies(startTime, endTime time.Time) ([]ServiceDependency, error) {
              query := `
                  SELECT
                      parent.service_name as parent,
                      child.service_name as child,
                      count(*) as call_count,
                      countIf(child.status_code = 'ERROR') as error_count,
                      avg(child.duration_ms) as avg_duration
                  FROM spans child
                  JOIN spans parent ON child.parent_id = parent.span_id
                      AND child.trace_id = parent.trace_id
                  WHERE child.start_time >= ? AND child.start_time <= ?
                      AND parent.service_name != child.service_name
                  GROUP BY parent.service_name, child.service_name
              `

              // Return for D3.js force-directed graph
              return s.storage.Query(query, startTime, endTime)
          }

          // Timeline data for visualization
          type TimelineSpan struct {
              SpanID    string        `json:"spanId"`
              ParentID  string        `json:"parentId"`
              Name      string        `json:"name"`
              Service   string        `json:"service"`
              StartTime int64         `json:"startTime"` // microseconds from trace start
              Duration  int64         `json:"duration"`  // microseconds
              Status    string        `json:"status"`
              Tags      []TagPair     `json:"tags"`
              Logs      []LogEntry    `json:"logs"`
              Depth     int           `json:"depth"`
              Children  []*TimelineSpan `json:"children,omitempty"`
          }

          func (s *TraceQueryService) GetTraceTimeline(traceID string) (*TimelineSpan, error) {
              spans, _ := s.storage.GetSpans(traceID)

              // Build tree
              spanMap := make(map[string]*TimelineSpan)
              var root *TimelineSpan

              for _, span := range spans {
                  ts := &TimelineSpan{
                      SpanID:   span.SpanID.String(),
                      ParentID: span.ParentID.String(),
                      Name:     span.Name,
                      Service:  span.Attributes["service.name"].(string),
                      Duration: span.EndTime.Sub(span.StartTime).Microseconds(),
                      Status:   span.Status.Code.String(),
                  }
                  spanMap[ts.SpanID] = ts

                  if span.ParentID == (SpanID{}) {
                      root = ts
                  }
              }

              // Link parents
              for _, ts := range spanMap {
                  if ts.ParentID != "" {
                      if parent, ok := spanMap[ts.ParentID]; ok {
                          parent.Children = append(parent.Children, ts)
                      }
                  }
              }

              // Calculate relative times and depths
              if root != nil {
                  s.calculateDepths(root, 0, root.StartTime)
              }

              return root, nil
          }
      pitfalls:
      - Query without time range scans entire database
      - Deep traces cause stack overflow in tree building
      - Timezone issues in timestamp display
      - Missing spans leave gaps in visualization
      concepts:
      - Trace querying
      - Service dependencies
      - Timeline visualization
      - Aggregations
      estimated_hours: 12-16
      deliverables:
      - Trace search API supporting service, operation, and tag queries
      - Trace timeline view showing span hierarchy and durations
      - Service dependency map derived from observed inter-service traces
      - Latency analysis with percentile distribution across operations
  ecommerce-basic:
    id: ecommerce-basic
    name: E-commerce Store (Basic)
    description: Build a basic online store with product catalog, cart, and checkout. Learn full-stack development patterns.
    difficulty: beginner
    estimated_hours: 20-35
    prerequisites:
    - HTML/CSS/JS
    - REST API basics
    - Database basics
    languages:
      recommended:
      - JavaScript/Node.js
      - Python/Flask
      - Ruby/Rails
      also_possible:
      - PHP
      - Go
      - Java/Spring
    resources:
    - type: tutorial
      name: Full Stack E-commerce
      url: https://www.freecodecamp.org/news/how-to-build-an-e-commerce-app/
    - type: video
      name: Build an Online Store
      url: https://www.youtube.com/results?search_query=build+ecommerce+site+tutorial
    milestones:
    - id: 1
      name: Product Catalog
      description: Display products with images, prices, and descriptions.
      acceptance_criteria:
      - Product listing page displays paginated catalog with sort options
      - Product detail page shows full product information and availability
      - Category filtering narrows product listing to selected category branch
      - Search functionality matches products by name, description, and attributes
      hints:
        level1: Start with static JSON data. Add database later.
        level2: Use grid layout for products. Store images as URLs initially.
        level3: "// Product schema\nconst productSchema = {\n    id: 'string',\n    name: 'string',\n    description: 'string',\n\
          \    price: 'number',  // in cents\n    imageUrl: 'string',\n    category: 'string',\n    stock: 'number'\n};\n\n\
          // API endpoints\napp.get('/api/products', (req, res) => {\n    let products = [...allProducts];\n    \n    if (req.query.category)\
          \ {\n        products = products.filter(p => \n            p.category === req.query.category\n        );\n    }\n\
          \    \n    if (req.query.search) {\n        const term = req.query.search.toLowerCase();\n        products = products.filter(p\
          \ =>\n            p.name.toLowerCase().includes(term) ||\n            p.description.toLowerCase().includes(term)\n\
          \        );\n    }\n    \n    res.json(products);\n});\n\napp.get('/api/products/:id', (req, res) => {\n    const\
          \ product = allProducts.find(p => p.id === req.params.id);\n    if (!product) return res.status(404).json({error:\
          \ 'Not found'});\n    res.json(product);\n});"
      pitfalls:
      - Price precision (use cents)
      - Large image handling
      - N+1 queries
      concepts:
      - CRUD operations
      - Filtering/search
      - Data modeling
      estimated_hours: 4-6
      deliverables:
      - Product model with name, description, price, and inventory fields
      - Category hierarchy supporting nested subcategories for organization
      - Product listing API with pagination and sorting options
      - Product search functionality with keyword and filter matching
    - id: 2
      name: Shopping Cart
      description: Implement add to cart, update quantities, and remove items.
      acceptance_criteria:
      - Add and remove items from cart with correct subtotal recalculation
      - Update quantities with validation against available inventory stock levels
      - Cart persistence survives page refresh and browser session restoration
      - Price calculations correctly apply quantities and reflect current product prices
      hints:
        level1: Store cart in session/localStorage for guests, database for logged-in users.
        level2: Cart items reference product IDs. Calculate totals server-side.
        level3: "// Cart in localStorage (guest)\nconst Cart = {\n    items: [],\n    \n    load() {\n        const saved\
          \ = localStorage.getItem('cart');\n        this.items = saved ? JSON.parse(saved) : [];\n    },\n    \n    save()\
          \ {\n        localStorage.setItem('cart', JSON.stringify(this.items));\n    },\n    \n    add(productId, quantity\
          \ = 1) {\n        const existing = this.items.find(i => i.productId === productId);\n        if (existing) {\n \
          \           existing.quantity += quantity;\n        } else {\n            this.items.push({ productId, quantity\
          \ });\n        }\n        this.save();\n    },\n    \n    update(productId, quantity) {\n        const item = this.items.find(i\
          \ => i.productId === productId);\n        if (item) {\n            if (quantity <= 0) {\n                this.remove(productId);\n\
          \            } else {\n                item.quantity = quantity;\n            }\n        }\n        this.save();\n\
          \    },\n    \n    remove(productId) {\n        this.items = this.items.filter(i => i.productId !== productId);\n\
          \        this.save();\n    },\n    \n    async getTotal() {\n        // Fetch current prices from server\n     \
          \   const response = await fetch('/api/cart/calculate', {\n            method: 'POST',\n            body: JSON.stringify({\
          \ items: this.items })\n        });\n        return response.json();\n    }\n};"
      pitfalls:
      - Price changes between add and checkout
      - Stock validation
      - Session expiry
      concepts:
      - State management
      - Session storage
      - Data synchronization
      estimated_hours: 4-6
      deliverables:
      - Cart session management linking cart to user or anonymous session
      - Add and remove items operations with proper validation
      - Quantity update operations with inventory availability checking
      - Cart persistence across page loads and browser sessions
    - id: 3
      name: User Authentication
      description: Implement user registration, login, and profile management.
      acceptance_criteria:
      - Registration validates email format and password strength requirements
      - Login authenticates credentials and creates secure user session
      - Password hashing uses bcrypt or argon2 to prevent plaintext storage
      - Session management tracks authenticated state with secure cookie handling
      hints:
        level1: Use bcrypt for passwords. Never store plaintext.
        level2: JWT for API auth, session cookies for web. Add email validation.
        level3: "// Password hashing\nconst bcrypt = require('bcrypt');\n\nasync function register(email, password) {\n  \
          \  // Validate\n    if (!email || !password) throw new Error('Required');\n    if (password.length < 8) throw new\
          \ Error('Password too short');\n    \n    // Check existing\n    const existing = await db.users.findOne({ email\
          \ });\n    if (existing) throw new Error('Email exists');\n    \n    // Hash password\n    const hash = await bcrypt.hash(password,\
          \ 12);\n    \n    // Create user\n    const user = await db.users.create({\n        email,\n        passwordHash:\
          \ hash,\n        createdAt: new Date()\n    });\n    \n    return { id: user.id, email: user.email };\n}\n\nasync\
          \ function login(email, password) {\n    const user = await db.users.findOne({ email });\n    if (!user) throw new\
          \ Error('Invalid credentials');\n    \n    const valid = await bcrypt.compare(password, user.passwordHash);\n  \
          \  if (!valid) throw new Error('Invalid credentials');\n    \n    // Create session/token\n    const token = jwt.sign(\n\
          \        { userId: user.id },\n        process.env.JWT_SECRET,\n        { expiresIn: '7d' }\n    );\n    \n    return\
          \ { token, user: { id: user.id, email: user.email } };\n}"
      pitfalls:
      - Timing attacks on login
      - Password requirements
      - Token invalidation
      concepts:
      - Authentication
      - Password security
      - Session management
      estimated_hours: 5-8
      deliverables:
      - User registration with email and password validation
      - Login and logout flow with proper session management
      - Password hashing using bcrypt or argon2 for secure storage
      - Session management with secure cookies and expiration handling
    - id: 4
      name: Checkout Process
      description: Implement checkout flow with order creation.
      acceptance_criteria:
      - Address collection validates required shipping fields before proceeding
      - Order summary displays itemized list with quantities and total price
      - Order creation persists order record and clears the shopping cart
      - Inventory updates decrement stock for each purchased item at checkout
      hints:
        level1: Validate stock before creating order. Use transactions for consistency.
        level2: Create order in 'pending' state. Update to 'confirmed' after payment.
        level3: "async function createOrder(userId, cartItems, shippingAddress) {\n    // Start transaction\n    const session\
          \ = await db.startTransaction();\n    \n    try {\n        // Fetch and validate products\n        const products\
          \ = await db.products.find({\n            id: { $in: cartItems.map(i => i.productId) }\n        });\n        \n\
          \        // Check stock and calculate total\n        let total = 0;\n        const orderItems = [];\n        \n\
          \        for (const cartItem of cartItems) {\n            const product = products.find(p => p.id === cartItem.productId);\n\
          \            if (!product) throw new Error(`Product ${cartItem.productId} not found`);\n            if (product.stock\
          \ < cartItem.quantity) {\n                throw new Error(`Insufficient stock for ${product.name}`);\n         \
          \   }\n            \n            orderItems.push({\n                productId: product.id,\n                name:\
          \ product.name,\n                price: product.price,\n                quantity: cartItem.quantity\n          \
          \  });\n            \n            total += product.price * cartItem.quantity;\n            \n            // Decrement\
          \ stock\n            await db.products.updateOne(\n                { id: product.id },\n                { $inc:\
          \ { stock: -cartItem.quantity } }\n            );\n        }\n        \n        // Create order\n        const order\
          \ = await db.orders.create({\n            userId,\n            items: orderItems,\n            total,\n        \
          \    shippingAddress,\n            status: 'pending',\n            createdAt: new Date()\n        });\n        \n\
          \        await session.commit();\n        return order;\n    } catch (error) {\n        await session.rollback();\n\
          \        throw error;\n    }\n}"
      pitfalls:
      - Race conditions on stock
      - Abandoned checkouts
      - Partial order failures
      concepts:
      - Transactions
      - Inventory management
      - Order lifecycle
      estimated_hours: 7-15
      deliverables:
      - Address collection form with validation for shipping details
      - Order creation assembling cart items into a persistent order record
      - Payment integration stub simulating payment gateway interaction
      - Order confirmation page displaying summary and order number
  ecs-arch:
    id: ecs-arch
    name: ECS Architecture
    description: Implement an Entity-Component-System architecture. Learn data-oriented design for games.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - Game programming basics
    - Data structures
    - Performance concepts
    languages:
      recommended:
      - C++
      - Rust
      - C
      also_possible:
      - C#
      - Go
    resources:
    - name: ECS FAQ
      url: https://github.com/SanderMertens/ecs-faq
      type: article
    - name: Overwatch GDC Talk
      url: https://www.youtube.com/watch?v=W3aieHjyNvw
      type: video
    milestones:
    - id: 1
      name: Entity Manager
      description: Create entity ID management system.
      acceptance_criteria:
      - Generate unique entity IDs that are never duplicated during runtime
      - Track alive and dead entities with efficient status lookup support
      - Recycle deleted entity IDs for memory-efficient long-running systems
      - Generation counter prevents stale entity ID references from resolving incorrectly
      hints:
        level1: Entity = just an ID (integer). Components stored separately.
        level2: Use generation to detect stale references.
        level3: "struct EntityId {\n    uint32_t index;\n    uint32_t generation;\n};\n\nclass EntityManager {\n    std::vector<uint32_t>\
          \ generations;\n    std::queue<uint32_t> free_indices;\n    \npublic:\n    EntityId create() {\n        uint32_t\
          \ index;\n        if (!free_indices.empty()) {\n            index = free_indices.front();\n            free_indices.pop();\n\
          \        } else {\n            index = generations.size();\n            generations.push_back(0);\n        }\n \
          \       return {index, generations[index]};\n    }\n    \n    void destroy(EntityId id) {\n        if (is_alive(id))\
          \ {\n            generations[id.index]++;\n            free_indices.push(id.index);\n        }\n    }\n    \n  \
          \  bool is_alive(EntityId id) {\n        return id.index < generations.size() && \n               generations[id.index]\
          \ == id.generation;\n    }\n};"
      pitfalls:
      - Stale entity references
      - Index overflow
      - Not recycling IDs
      concepts:
      - Entity IDs
      - Generation counters
      - ID recycling
      estimated_hours: 3-4
      deliverables:
      - Entity ID generation with unique identifier assignment
      - Entity creation and destruction with lifecycle tracking
      - Entity lookup by ID with constant-time access
      - Entity iteration across all alive entities efficiently
    - id: 2
      name: Component Storage
      description: Implement cache-friendly component storage.
      acceptance_criteria:
      - Contiguous array storage enables cache-friendly iteration over component data
      - Sparse set maps entity IDs to component indices for constant-time lookup
      - Add and remove components dynamically without affecting other entity components
      - Type-safe component access prevents runtime errors from mismatched types
      hints:
        level1: 'Sparse set: sparse[entity] = dense_index, dense[index] = component.'
        level2: Dense array is contiguous for cache efficiency.
        level3: "template<typename T>\nclass ComponentArray {\n    std::vector<T> dense;           // Contiguous component\
          \ data\n    std::vector<uint32_t> sparse;   // Entity ID -> dense index\n    std::vector<uint32_t> dense_to_entity;\
          \  // dense index -> entity ID\n    \npublic:\n    void add(uint32_t entity, T component) {\n        if (entity\
          \ >= sparse.size())\n            sparse.resize(entity + 1, INVALID);\n        sparse[entity] = dense.size();\n \
          \       dense_to_entity.push_back(entity);\n        dense.push_back(component);\n    }\n    \n    void remove(uint32_t\
          \ entity) {\n        uint32_t index = sparse[entity];\n        uint32_t last = dense.size() - 1;\n        \n   \
          \     // Swap with last\n        std::swap(dense[index], dense[last]);\n        std::swap(dense_to_entity[index],\
          \ dense_to_entity[last]);\n        \n        // Update sparse\n        sparse[dense_to_entity[index]] = index;\n\
          \        sparse[entity] = INVALID;\n        \n        dense.pop_back();\n        dense_to_entity.pop_back();\n \
          \   }\n    \n    T* get(uint32_t entity) {\n        if (entity >= sparse.size() || sparse[entity] == INVALID)\n\
          \            return nullptr;\n        return &dense[sparse[entity]];\n    }\n};"
      pitfalls:
      - Swap-remove ordering
      - Sparse array growth
      - Invalid index sentinel
      concepts:
      - Sparse sets
      - Data-oriented design
      - Cache efficiency
      estimated_hours: 5-7
      deliverables:
      - Component type registry mapping types to storage arrays
      - Sparse set storage for efficient entity-to-component mapping
      - Component add and remove operations with dynamic attachment
      - Component lookup by entity ID with type-safe access interface
    - id: 3
      name: System Interface
      description: Create system execution framework.
      acceptance_criteria:
      - Systems operate on entities matching their required component set each frame
      - Query entities having specific component combinations with efficient iteration
      - System registration and ordering ensures correct execution sequence per frame
      - Delta time passing provides frame time to systems for time-based updates
      hints:
        level1: System = function that iterates entities with certain components.
        level2: 'Query: find all entities with Position AND Velocity.'
        level3: "class World {\n    std::unordered_map<std::type_index, std::unique_ptr<ComponentArrayBase>> components;\n\
          \    \n    template<typename... Components>\n    class View {\n        World& world;\n    public:\n        class\
          \ Iterator {\n            // Iterate entities that have all Components\n        };\n        \n        void each(std::function<void(EntityId,\
          \ Components&...)> func) {\n            // For each entity with all components, call func\n            for (auto\
          \ entity : get_matching_entities()) {\n                func(entity, *world.get<Components>(entity)...);\n      \
          \      }\n        }\n    };\n    \n    template<typename... Components>\n    View<Components...> view() {\n    \
          \    return View<Components...>(*this);\n    }\n};\n\n// Usage:\nvoid movement_system(World& world, float dt) {\n\
          \    world.view<Position, Velocity>().each([dt](EntityId e, Position& pos, Velocity& vel) {\n        pos.x += vel.x\
          \ * dt;\n        pos.y += vel.y * dt;\n    });\n}"
      pitfalls:
      - Component iteration invalidation
      - System ordering dependencies
      - Thread safety
      concepts:
      - System execution
      - Component queries
      - Iteration patterns
      estimated_hours: 5-7
      deliverables:
      - System base class defining the update and query interface
      - Component queries selecting entities matching required component sets
      - System execution order with configurable priority scheduling
      - System dependency declaration for correct ordering guarantees
    - id: 4
      name: Archetypes (Optional Advanced)
      description: Implement archetype-based storage for better performance.
      acceptance_criteria:
      - Group entities by their component combination into shared archetype tables
      - Move entities between archetypes when components are added or removed
      - Archetype graph enables fast lookup of matching archetypes for queries
      - Chunk-based storage within archetypes enables cache-efficient linear iteration
      hints:
        level1: Archetype = unique combination of component types.
        level2: Entities with same components stored together.
        level3: "struct Archetype {\n    std::set<std::type_index> component_types;\n    std::vector<void*> component_arrays;\
          \  // One array per type\n    std::vector<EntityId> entities;\n    \n    // All entities in same archetype have\
          \ identical component layout\n};\n\nclass ArchetypeWorld {\n    std::unordered_map<ComponentMask, Archetype> archetypes;\n\
          \    std::unordered_map<EntityId, ArchetypeLocation> entity_locations;\n    \n    void add_component(EntityId entity,\
          \ Component c) {\n        // 1. Find current archetype\n        // 2. Find or create target archetype (current +\
          \ new component)\n        // 3. Move entity data to new archetype\n    }\n};"
      pitfalls:
      - Archetype explosion
      - Move overhead
      - Complex implementation
      concepts:
      - Archetypes
      - Data locality
      - Component masks
      estimated_hours: 6-10
      deliverables:
      - Archetype identification grouping entities by component combination
      - Archetype-based storage co-locating components of same archetype
      - Archetype transitions moving entities between groups on component changes
      - Cache-friendly iteration over archetype storage for performance
  gossip-protocol:
    id: gossip-protocol
    name: Gossip Protocol
    description: Implement a gossip-based protocol for eventually consistent data dissemination. Learn epidemic algorithms.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Networking basics
    - Distributed systems concepts
    - Probability
    languages:
      recommended:
      - Go
      - Rust
      - Python
      also_possible:
      - Java
      - Erlang
    resources:
    - name: Gossip Protocol Explained
      url: https://www.cs.cornell.edu/home/rvr/papers/flowgossip.pdf
      type: paper
    - name: SWIM Paper
      url: https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf
      type: paper
    milestones:
    - id: 1
      name: Peer Management
      description: Manage cluster membership and peer list.
      acceptance_criteria:
      - Maintain list of known peers with address, port, and last-seen timestamp
      - Random peer selection picks k random alive peers for each gossip round
      - Handle peer join and leave events updating the membership list correctly
      - Periodic peer list exchange synchronizes membership across all cluster nodes
      hints:
        level1: Each node maintains list of known peers. Gossip updates this list.
        level2: 'Random selection: pick k random peers to gossip with each round.'
        level3: "import random\nimport threading\nfrom dataclasses import dataclass\nfrom typing import Dict, Set\nimport\
          \ time\n\n@dataclass\nclass Peer:\n    address: str\n    port: int\n    last_seen: float\n    state: str = 'alive'\
          \  # alive, suspected, dead\n\nclass PeerManager:\n    def __init__(self, self_addr: str, self_port: int):\n   \
          \     self.self_id = f'{self_addr}:{self_port}'\n        self.peers: Dict[str, Peer] = {}\n        self.lock = threading.Lock()\n\
          \        self.fanout = 3  # Number of peers to gossip with\n    \n    def add_peer(self, addr: str, port: int):\n\
          \        peer_id = f'{addr}:{port}'\n        with self.lock:\n            if peer_id != self.self_id and peer_id\
          \ not in self.peers:\n                self.peers[peer_id] = Peer(addr, port, time.time())\n    \n    def select_random_peers(self,\
          \ k: int = None) -> list:\n        '''Select k random alive peers'''\n        k = k or self.fanout\n        with\
          \ self.lock:\n            alive = [p for p in self.peers.values() if p.state == 'alive']\n            return random.sample(alive,\
          \ min(k, len(alive)))\n    \n    def mark_alive(self, peer_id: str):\n        with self.lock:\n            if peer_id\
          \ in self.peers:\n                self.peers[peer_id].last_seen = time.time()\n                self.peers[peer_id].state\
          \ = 'alive'\n    \n    def check_timeouts(self, timeout: float = 30.0):\n        '''Mark peers as dead if not seen\
          \ recently'''\n        now = time.time()\n        with self.lock:\n            for peer in self.peers.values():\n\
          \                if now - peer.last_seen > timeout:\n                    peer.state = 'dead'"
      pitfalls:
      - Not handling own address
      - Thread safety
      - Stale peer info
      concepts:
      - Membership
      - Random selection
      - Peer state
      estimated_hours: 3-4
      deliverables:
      - Peer list maintenance tracking known cluster members and their state
      - Peer discovery mechanism for finding new nodes in the cluster
      - Peer state tracking recording alive, suspected, and dead status
      - Peer removal cleaning up dead peers from the membership list
    - id: 2
      name: Push Gossip
      description: Implement push-based gossip dissemination.
      acceptance_criteria:
      - Periodic gossip rounds execute at configurable interval sending updates to peers
      - Push updates containing state deltas to randomly selected peer nodes each round
      - Version or timestamp tracking prevents old data from overwriting newer values
      - Infection-style spreading ensures updates reach all nodes within O(log N) rounds
      hints:
        level1: 'Push: send your data to random peers. They forward to others.'
        level2: Version numbers prevent old data from overwriting new.
        level3: "import json\nimport socket\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass GossipMessage:\n\
          \    sender: str\n    msg_type: str  # 'push', 'pull', 'ack'\n    key: str\n    value: str\n    version: int\n \
          \   \n    def to_json(self):\n        return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls,\
          \ data):\n        return cls(**json.loads(data))\n\nclass GossipNode:\n    def __init__(self, addr, port):\n   \
          \     self.addr = addr\n        self.port = port\n        self.peer_manager = PeerManager(addr, port)\n        self.data:\
          \ Dict[str, tuple] = {}  # key -> (value, version)\n        self.lock = threading.Lock()\n    \n    def set(self,\
          \ key: str, value: str):\n        '''Set local value and prepare for gossip'''\n        with self.lock:\n      \
          \      current = self.data.get(key, (None, 0))\n            self.data[key] = (value, current[1] + 1)\n    \n   \
          \ def gossip_round(self):\n        '''Push all data to random peers'''\n        peers = self.peer_manager.select_random_peers()\n\
          \        \n        with self.lock:\n            items = list(self.data.items())\n        \n        for peer in peers:\n\
          \            for key, (value, version) in items:\n                msg = GossipMessage(\n                    sender=f'{self.addr}:{self.port}',\n\
          \                    msg_type='push',\n                    key=key,\n                    value=value,\n        \
          \            version=version\n                )\n                self.send_to_peer(peer, msg)\n    \n    def handle_push(self,\
          \ msg: GossipMessage):\n        '''Receive pushed data'''\n        with self.lock:\n            current = self.data.get(msg.key,\
          \ (None, 0))\n            if msg.version > current[1]:\n                self.data[msg.key] = (msg.value, msg.version)\n\
          \        \n        self.peer_manager.mark_alive(msg.sender)"
      pitfalls:
      - Version conflicts
      - Infinite propagation
      - Network floods
      concepts:
      - Push gossip
      - Versioning
      - Epidemic spread
      estimated_hours: 4-5
      deliverables:
      - Random peer selection for fanout-based message dissemination
      - State delta calculation identifying changes since last gossip round
      - Push message sending transmitting updates to selected peer nodes
      - Infection-style spread propagating updates through transitive gossip rounds
    - id: 3
      name: Pull Gossip & Anti-Entropy
      description: Add pull-based reconciliation for consistency.
      acceptance_criteria:
      - Pull mechanism requests and receives missing data from selected peer nodes
      - Anti-entropy performs periodic full state synchronization between random peer pairs
      - Combine push and pull gossip for efficient bidirectional data dissemination
      - Handle network partitions gracefully by converging state after partition heals
      hints:
        level1: 'Pull: request data you''re missing. Anti-entropy: compare full state.'
        level2: 'Push-pull: push digest, peer responds with missing items.'
        level3: "def create_digest(self) -> Dict[str, int]:\n    '''Create digest of local data (key -> version)'''\n    with\
          \ self.lock:\n        return {k: v[1] for k, v in self.data.items()}\n\ndef pull_from_peer(self, peer: Peer):\n\
          \    '''Request peer's digest and pull missing/newer items'''\n    # Send our digest\n    our_digest = self.create_digest()\n\
          \    msg = GossipMessage(\n        sender=f'{self.addr}:{self.port}',\n        msg_type='pull_request',\n      \
          \  key='__digest__',\n        value=json.dumps(our_digest),\n        version=0\n    )\n    self.send_to_peer(peer,\
          \ msg)\n\ndef handle_pull_request(self, msg: GossipMessage):\n    '''Respond with items peer is missing or has old\
          \ versions of'''\n    their_digest = json.loads(msg.value)\n    our_digest = self.create_digest()\n    \n    # Find\
          \ items to send\n    to_send = []\n    with self.lock:\n        for key, (value, version) in self.data.items():\n\
          \            their_version = their_digest.get(key, 0)\n            if version > their_version:\n               \
          \ to_send.append((key, value, version))\n    \n    # Send missing items\n    for key, value, version in to_send:\n\
          \        response = GossipMessage(\n            sender=f'{self.addr}:{self.port}',\n            msg_type='pull_response',\n\
          \            key=key,\n            value=value,\n            version=version\n        )\n        # Send to requester\n\
          \ndef anti_entropy(self):\n    '''Periodic full state synchronization'''\n    peer = self.peer_manager.select_random_peers(1)\n\
          \    if peer:\n        self.pull_from_peer(peer[0])"
      pitfalls:
      - Digest size
      - Sync storms
      - Stale data during partition
      concepts:
      - Pull gossip
      - Anti-entropy
      - State reconciliation
      estimated_hours: 4-5
      deliverables:
      - Pull request and response protocol for data reconciliation between peers
      - State reconciliation comparing digests and exchanging missing entries
      - Anti-entropy repair mechanism for periodic full state synchronization
      - Consistency convergence ensuring all nodes reach same state eventually
    - id: 4
      name: Failure Detection
      description: Implement gossip-based failure detection (SWIM-style).
      acceptance_criteria:
      - Probe random peers periodically and mark responsive nodes as alive
      - Indirect probing through helper peers when direct probe fails to respond
      - Suspicion mechanism waits for confirmation before declaring peer as dead
      - Disseminate membership changes by piggybacking on regular gossip messages
      hints:
        level1: 'SWIM: ping peer, if no response, ask others to ping (indirect probe).'
        level2: 'Suspicion: don''t immediately mark dead. Wait for confirmation.'
        level3: "def failure_detection_round(self):\n    '''SWIM-style failure detection'''\n    peer = self.peer_manager.select_random_peers(1)\n\
          \    if not peer:\n        return\n    \n    target = peer[0]\n    \n    # Direct probe\n    if self.ping(target):\n\
          \        self.peer_manager.mark_alive(f'{target.address}:{target.port}')\n        return\n    \n    # Direct probe\
          \ failed, try indirect\n    helpers = self.peer_manager.select_random_peers(3)\n    responses = []\n    \n    for\
          \ helper in helpers:\n        # Ask helper to ping target\n        msg = GossipMessage(\n            sender=f'{self.addr}:{self.port}',\n\
          \            msg_type='ping_req',\n            key='target',\n            value=f'{target.address}:{target.port}',\n\
          \            version=0\n        )\n        if self.send_and_wait(helper, msg, timeout=2.0):\n            responses.append(True)\n\
          \    \n    if any(responses):\n        # Target reachable through helper\n        self.peer_manager.mark_alive(f'{target.address}:{target.port}')\n\
          \    else:\n        # Start suspicion\n        self.suspect_peer(target)\n\ndef suspect_peer(self, peer: Peer):\n\
          \    '''Mark peer as suspected, wait before declaring dead'''\n    peer_id = f'{peer.address}:{peer.port}'\n   \
          \ with self.lock:\n        if peer_id in self.peers:\n            self.peers[peer_id].state = 'suspected'\n    \n\
          \    # Schedule dead declaration if not refuted\n    threading.Timer(10.0, lambda: self.confirm_dead(peer_id)).start()\n\
          \ndef confirm_dead(self, peer_id: str):\n    '''Confirm death if still suspected'''\n    with self.lock:\n     \
          \   if peer_id in self.peers and self.peers[peer_id].state == 'suspected':\n            self.peers[peer_id].state\
          \ = 'dead'\n            # Gossip membership change\n            self.broadcast_membership_change(peer_id, 'dead')"
      pitfalls:
      - False positives
      - Suspicion timeout tuning
      - Split brain
      concepts:
      - SWIM protocol
      - Failure detection
      - Suspicion
      estimated_hours: 4-6
      deliverables:
      - Heartbeat mechanism with configurable probe interval for liveness checking
      - Failure suspicion protocol using indirect probing before declaring dead
      - SWIM protocol basics implementing probe, probe-req, and suspicion phases
      - Failure dissemination broadcasting membership changes via gossip piggyback
  hash-impl:
    id: hash-impl
    name: Hash Function (SHA-256)
    description: Implement SHA-256 hash function from the NIST specification. Learn about cryptographic primitives and bitwise
      operations.
    difficulty: beginner
    estimated_hours: 10-15
    prerequisites:
    - Binary and hexadecimal representation
    - Bitwise operations
    - Basic understanding of cryptography
    languages:
      recommended:
      - Python
      - JavaScript
      - C
      also_possible:
      - Rust
      - Go
      - Java
    resources:
    - name: SHA-256 Step by Step
      url: https://blog.boot.dev/cryptography/how-sha-2-works-step-by-step-sha-256/
      type: tutorial
    - name: NIST SHA-256 Specification
      url: https://csrc.nist.gov/publications/detail/fips/180/4/final
      type: specification
    milestones:
    - id: 1
      name: Message Preprocessing
      description: Implement message padding and parsing.
      acceptance_criteria:
      - Convert message to binary representation and append '1' padding bit
      - Append '1' bit followed by zeros to reach 448 mod 512 bit alignment
      - Pad with zeros to 448 mod 512 bits then append original length as 64-bit big-endian
      - Append original message length as 64-bit big-endian integer at block end
      - Parse padded message into array of 512-bit blocks for processing
      hints:
        level1: Message length in bits must end at 64 bits from 512 boundary.
        level2: 'Padding: 1 + zeros + 64-bit length. Total = multiple of 512.'
        level3: |-
          def preprocess(message):
              if isinstance(message, str):
                  message = message.encode()
              original_bit_len = len(message) * 8
              message += b'\x80'
              while (len(message) % 64) != 56:
                  message += b'\x00'
              message += original_bit_len.to_bytes(8, 'big')
              return [message[i:i+64] for i in range(0, len(message), 64)]
      pitfalls:
      - Wrong bit/byte conversion
      - Endianness errors
      - Off-by-one in padding calculation
      concepts:
      - Bit padding
      - Message blocks
      - Binary representation
      estimated_hours: 2-3
      deliverables:
      - Padding to block size with '1' bit and zero-fill alignment
      - Length encoding appending original message length as 64-bit integer
      - Block parsing splitting padded message into 512-bit chunks
      - Endianness handling using big-endian byte ordering throughout
    - id: 2
      name: Message Schedule
      description: Generate the message schedule from each 512-bit block.
      acceptance_criteria:
      - Parse 512-bit block into 16 initial 32-bit words in big-endian order
      - Extend schedule to 64 words using the SHA-256 schedule recurrence formula
      - Implement lower-sigma-0 and lower-sigma-1 functions using rotate and shift operations
      - Words stored as 32-bit unsigned integers with overflow masked to 32 bits
      hints:
        level1: 'First 16 words: direct from message block (big-endian).'
        level2: 'Words 16-63: W[i] = Ïƒ1(W[i-2]) + W[i-7] + Ïƒ0(W[i-15]) + W[i-16].'
        level3: |-
          def rotr(x, n):
              return ((x >> n) | (x << (32 - n))) & 0xffffffff

          def sigma0(x):
              return rotr(x, 7) ^ rotr(x, 18) ^ (x >> 3)

          def sigma1(x):
              return rotr(x, 17) ^ rotr(x, 19) ^ (x >> 10)

          def message_schedule(block):
              W = [int.from_bytes(block[i*4:(i+1)*4], 'big') for i in range(16)]
              for i in range(16, 64):
                  W.append((sigma1(W[i-2]) + W[i-7] + sigma0(W[i-15]) + W[i-16]) & 0xffffffff)
              return W
      pitfalls:
      - Not masking to 32 bits
      - Right rotate vs right shift
      - Wrong Ïƒ function parameters
      concepts:
      - Rotate operations
      - XOR operations
      - Word expansion
      estimated_hours: 2-3
      deliverables:
      - Word expansion from 16 initial words to 64 schedule entries
      - Schedule array construction using SHA-256 recurrence relation
      - Bit rotation functions implementing right-rotate for 32-bit words
      - XOR operations combining rotated and shifted values for sigma functions
    - id: 3
      name: Compression Function
      description: Implement the main compression function.
      acceptance_criteria:
      - Initialize working variables (a through h) from current hash values per block
      - Execute 64 rounds of compression using schedule words and round constants
      - Implement upper-Sigma-0, upper-Sigma-1, Ch, and Maj functions using bitwise operations
      - Use correct K constants array with all 64 round constant values from specification
      - Update hash values by adding compressed working variables after all rounds complete
      hints:
        level1: 'Each round: T1 = h + Î£1(e) + Ch(e,f,g) + K[i] + W[i].'
        level2: Ch(x,y,z) = (x AND y) XOR (NOT x AND z). Maj(x,y,z) = (x AND y) XOR (x AND z) XOR (y AND z).
        level3: |-
          def compress(H, W):
              a, b, c, d, e, f, g, h = H
              for i in range(64):
                  S1 = rotr(e, 6) ^ rotr(e, 11) ^ rotr(e, 25)
                  ch = (e & f) ^ (~e & g)
                  T1 = (h + S1 + ch + K[i] + W[i]) & 0xffffffff
                  S0 = rotr(a, 2) ^ rotr(a, 13) ^ rotr(a, 22)
                  maj = (a & b) ^ (a & c) ^ (b & c)
                  T2 = (S0 + maj) & 0xffffffff
                  h, g, f, e, d, c, b, a = g, f, e, (d + T1) & 0xffffffff, c, b, a, (T1 + T2) & 0xffffffff
              return [(H[i] + v) & 0xffffffff for i, v in enumerate([a,b,c,d,e,f,g,h])]
      pitfalls:
      - Mixing up Î£ and Ïƒ functions
      - Wrong K constant values
      - Not masking intermediate results
      concepts:
      - Compression function
      - Round functions
      - Hash state
      estimated_hours: 4-5
      deliverables:
      - Working variables initialization from current hash state values
      - Round function iteration executing 64 compression rounds per block
      - Bitwise operations implementing Ch, Maj, Sigma-0, and Sigma-1 functions
      - Intermediate hash update adding compressed values to running hash state
    - id: 4
      name: Final Hash Output
      description: Produce the final 256-bit hash output.
      acceptance_criteria:
      - Process all message blocks sequentially updating hash state after each block
      - Concatenate all eight final 32-bit hash values into 256-bit output
      - Output hash as 64-character lowercase hexadecimal string representation
      - Test output against NIST known test vectors for empty string and 'abc' inputs
      - Handle empty input correctly producing the standard SHA-256 empty hash value
      hints:
        level1: Process blocks sequentially, each updates hash state.
        level2: 'Final hash: concatenate all 8 hash values as big-endian.'
        level3: |-
          def sha256(message):
              blocks = preprocess(message)
              H = initial_hash_values.copy()
              for block in blocks:
                  W = message_schedule(block)
                  H = compress(H, W)
              return ''.join(h.to_bytes(4, 'big').hex() for h in H)

          # Test vectors
          assert sha256('') == 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
          assert sha256('abc') == 'ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad'
      pitfalls:
      - Wrong hash for empty string
      - Endianness in output
      - Not resetting state between calls
      concepts:
      - Hash finalization
      - Test vectors
      - Hex encoding
      estimated_hours: 2-3
      deliverables:
      - Final hash construction by concatenating all eight 32-bit hash values
      - Hex string formatting converting binary hash to 64-character hexadecimal output
      - Byte array output option for binary hash representation
      - Test vector verification against known SHA-256 reference values
  hexdump:
    id: hexdump
    name: Hexdump Utility
    description: Build a hexdump utility to display binary file contents in hexadecimal and ASCII. Learn binary file handling
      and formatted output.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - File I/O
    - Binary vs text
    - Formatted output
    languages:
      recommended:
      - C
      - Python
      - Rust
      also_possible:
      - Go
      - JavaScript
    resources:
    - name: Let's Build a Hexdump Utility in C
      url: http://www.dmulholl.com/blog/lets-build-a-hexdump-utility.html
      type: tutorial
    - name: Hexdump man page
      url: https://man7.org/linux/man-pages/man1/hexdump.1.html
      type: reference
    milestones:
    - id: 1
      name: Basic Hex Output
      description: Read binary file and output bytes in hexadecimal format.
      acceptance_criteria:
      - Read file in binary mode to preserve exact byte content without encoding
      - Output 16 bytes per line formatted as space-separated hex values
      - Show file offset in hexadecimal at the start of each output line
      - Handle files of any size by reading and processing in streaming chunks
      hints:
        level1: Read file in chunks of 16 bytes. Use format specifier %02x for hex output.
        level2: Track offset, incrementing by 16 each line. Handle partial last line.
        level3: |-
          def hexdump_basic(filepath: str):
              offset = 0
              with open(filepath, 'rb') as f:
                  while True:
                      chunk = f.read(16)
                      if not chunk:
                          break

                      # Format: offset: hex bytes
                      hex_str = ' '.join(f'{b:02x}' for b in chunk)
                      print(f'{offset:08x}: {hex_str}')

                      offset += len(chunk)

          # Output:
          # 00000000: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
          # 00000010: 03 00 3e 00 01 00 00 00 40 10 40 00 00 00 00 00
      pitfalls:
      - Opening file in text mode mangles binary data
      - Last chunk may have fewer than 16 bytes
      - Large files should be streamed, not loaded entirely
      concepts:
      - Binary file I/O
      - Hexadecimal formatting
      - Chunked reading
      estimated_hours: 2-3
      deliverables:
      - File reading in fixed-size chunks for streaming binary processing
      - Byte to hex conversion formatting each byte as two-digit hexadecimal
      - Offset display showing file position at start of each output line
      - Byte grouping with space-separated hex pairs per output line
    - id: 2
      name: ASCII Column
      description: Add ASCII representation alongside hex output.
      acceptance_criteria:
      - Show printable ASCII characters (0x20-0x7E) in the right-side column
      - Replace non-printable bytes with '.' to prevent terminal corruption
      - Align ASCII column properly even on partial lines with fewer than 16 bytes
      - Handle partial final lines with padding to maintain column alignment
      hints:
        level1: 'Printable ASCII: 0x20-0x7E. Use chr() to convert, ''.'' for others.'
        level2: Pad hex section with spaces for short lines to keep ASCII aligned.
        level3: |-
          def hexdump_with_ascii(filepath: str):
              offset = 0
              with open(filepath, 'rb') as f:
                  while True:
                      chunk = f.read(16)
                      if not chunk:
                          break

                      # Hex part
                      hex_parts = [f'{b:02x}' for b in chunk]
                      hex_str = ' '.join(hex_parts)

                      # Pad for alignment (each byte = 3 chars: "xx ")
                      padding = '   ' * (16 - len(chunk))

                      # ASCII part
                      ascii_str = ''.join(
                          chr(b) if 0x20 <= b <= 0x7e else '.'
                          for b in chunk
                      )

                      print(f'{offset:08x}: {hex_str}{padding}  |{ascii_str}|')
                      offset += len(chunk)

          # Output:
          # 00000000: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00  |.ELF............|
          # 00000010: 03 00 3e 00 01 00 00 00 40 10 40 00 00 00 00 00  |..>.....@.@.....|
      pitfalls:
      - Don't print control characters (newlines, tabs corrupt output)
      - Alignment breaks on partial lines without padding
      - Unicode handling varies by terminal
      concepts:
      - ASCII encoding
      - String formatting
      - Column alignment
      estimated_hours: 2-3
      deliverables:
      - Printable character display showing ASCII representation of each byte
      - Non-printable character replacement using dot for control characters
      - Column alignment ensuring ASCII column lines up across all output lines
      - Line formatting combining offset, hex, and ASCII sections properly
    - id: 3
      name: Grouped Output
      description: Group hex bytes (2, 4, or 8 byte groupings) for better readability.
      acceptance_criteria:
      - Support -g option for group size accepting values of 1, 2, 4, or 8 bytes
      - Add extra space between groups while keeping bytes within groups adjacent
      - Default to 2-byte groups similar to xxd output format
      - Handle endianness display option for big-endian or little-endian multi-byte groups
      hints:
        level1: 'Split 16-byte line into groups. For 2-byte groups: 8 groups per line.'
        level2: Join bytes within group without space, separate groups with space.
        level3: |-
          def hexdump_grouped(filepath: str, group_size: int = 2):
              offset = 0
              with open(filepath, 'rb') as f:
                  while True:
                      chunk = f.read(16)
                      if not chunk:
                          break

                      # Group bytes
                      groups = []
                      for i in range(0, len(chunk), group_size):
                          group = chunk[i:i+group_size]
                          groups.append(''.join(f'{b:02x}' for b in group))

                      hex_str = ' '.join(groups)

                      # Calculate padding (each group = group_size*2 + 1 for space)
                      full_groups = 16 // group_size
                      actual_groups = (len(chunk) + group_size - 1) // group_size
                      padding = ' ' * ((full_groups - actual_groups) * (group_size * 2 + 1))

                      ascii_str = ''.join(
                          chr(b) if 0x20 <= b <= 0x7e else '.'
                          for b in chunk
                      )

                      print(f'{offset:08x}: {hex_str}{padding}  {ascii_str}')
                      offset += len(chunk)

          # With group_size=2:
          # 00000000: 7f45 4c46 0201 0100 0000 0000 0000 0000  .ELF............
          # 00000010: 0300 3e00 0100 0000 4010 4000 0000 0000  ..>.....@.@.....
      pitfalls:
      - Group boundaries at end of file need special handling
      - Endianness affects display for multi-byte groups
      - Different tools use different default groupings
      concepts:
      - Data grouping
      - Byte order
      - Flexible formatting
      estimated_hours: 2-3
      deliverables:
      - 2-byte grouping option displaying hex pairs without internal spaces
      - 4-byte grouping option displaying hex quads for word-aligned viewing
      - Endianness display option for interpreting multi-byte group values
      - Custom grouping via command-line flag for flexible byte group sizes
    - id: 4
      name: CLI Options
      description: 'Add command-line options: offset, length, output format.'
      acceptance_criteria:
      - -s/--skip flag seeks to specified byte offset before starting hexdump output
      - -n/--length flag limits output to specified number of bytes from file
      - -C flag produces canonical hex+ASCII format matching standard hexdump -C output
      - Read from stdin when no file path is specified using dash as filename convention
      hints:
        level1: Use argparse (Python) or getopt (C). Seek to offset before reading.
        level2: Handle - as stdin. Count bytes for length limit.
        level3: |-
          import argparse
          import sys

          def main():
              parser = argparse.ArgumentParser(description='Hexadecimal file dump')
              parser.add_argument('file', nargs='?', default='-',
                                  help='File to dump (default: stdin)')
              parser.add_argument('-C', '--canonical', action='store_true',
                                  help='Canonical hex+ASCII display')
              parser.add_argument('-g', '--group', type=int, default=2,
                                  help='Bytes per group (default: 2)')
              parser.add_argument('-s', '--skip', type=int, default=0,
                                  help='Skip bytes at start')
              parser.add_argument('-n', '--length', type=int, default=None,
                                  help='Dump only N bytes')
              args = parser.parse_args()

              # Open file or stdin
              if args.file == '-':
                  f = sys.stdin.buffer
              else:
                  f = open(args.file, 'rb')

              try:
                  # Skip to offset
                  if args.skip and args.file != '-':
                      f.seek(args.skip)
                  elif args.skip:
                      f.read(args.skip)  # Can't seek stdin

                  bytes_read = 0
                  while True:
                      remaining = None
                      if args.length:
                          remaining = args.length - bytes_read
                          if remaining <= 0:
                              break

                      chunk_size = min(16, remaining) if remaining else 16
                      chunk = f.read(chunk_size)
                      if not chunk:
                          break

                      # Output...
                      bytes_read += len(chunk)
              finally:
                  if f != sys.stdin.buffer:
                      f.close()
      pitfalls:
      - Can't seek on stdin (must read and discard)
      - Length limit interacts with offset
      - Binary mode required for stdin
      concepts:
      - CLI design
      - Standard input
      - Argument parsing
      estimated_hours: 2-3
      deliverables:
      - Argument parsing supporting flags, options, and positional file path
      - Length limit option restricting output to specified number of bytes
      - Skip offset option starting hexdump at given byte offset in file
      - Output format selection choosing between canonical and grouped display
  http-server-basic:
    id: http-server-basic
    name: HTTP Server (Basic)
    description: Build a static file HTTP server. Learn TCP networking, HTTP protocol, and concurrent request handling.
    difficulty: intermediate
    estimated_hours: 12-20
    prerequisites:
    - TCP/IP basics
    - Socket programming
    - File I/O
    languages:
      recommended:
      - C
      - Go
      - Rust
      also_possible:
      - Python
      - Java
    resources:
    - name: HTTP/1.1 Specification (RFC 7230)
      url: https://tools.ietf.org/html/rfc7230
      type: specification
    - name: Beej's Guide to Network Programming
      url: https://beej.us/guide/bgnet/
      type: tutorial
    milestones:
    - id: 1
      name: TCP Server Basics
      description: Create a listening TCP server.
      acceptance_criteria:
      - Bind to port 8080 and listen for incoming TCP connections
      - Accept connections from multiple sequential client requests
      - Read complete request data from the client socket buffer
      - Send hardcoded HTTP response with proper status line and headers
      - Close connection and free resources after response is sent
      hints:
        level1: socket(), bind(), listen(), accept() sequence.
        level2: Set SO_REUSEADDR to avoid 'address already in use'.
        level3: |-
          int server_fd = socket(AF_INET, SOCK_STREAM, 0);
          int opt = 1;
          setsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));
          struct sockaddr_in addr = { .sin_family = AF_INET, .sin_addr.s_addr = INADDR_ANY, .sin_port = htons(8080) };
          bind(server_fd, (struct sockaddr*)&addr, sizeof(addr));
          listen(server_fd, 10);
          while (1) {
              int client_fd = accept(server_fd, NULL, NULL);
              char buf[1024];
              read(client_fd, buf, sizeof(buf));
              char *resp = "HTTP/1.1 200 OK\r\nContent-Length: 5\r\n\r\nHello";
              write(client_fd, resp, strlen(resp));
              close(client_fd);
          }
      pitfalls:
      - Byte order (htons)
      - Not handling partial reads
      - Forgetting to close client fd
      concepts:
      - Sockets
      - TCP connection lifecycle
      - Bind/Listen/Accept
      estimated_hours: 2-3
      deliverables:
      - Socket creation and binding to configurable port address
      - Connection acceptance loop handling incoming client connections
      - Client handling reading request data and sending response
      - Connection cleanup closing client sockets after response delivery
    - id: 2
      name: HTTP Request Parsing
      description: Parse HTTP request line and headers.
      acceptance_criteria:
      - Parse method, path, and version from HTTP request line correctly
      - Parse headers into key-value pairs handling whitespace around values
      - Handle GET requests and return appropriate responses based on path
      - Extract Host header value for virtual host routing support
      hints:
        level1: 'Request line: GET /path HTTP/1.1\r\n'
        level2: Headers end at \r\n\r\n (blank line).
        level3: |-
          // Parse request line
          char method[16], path[256], version[16];
          sscanf(request, "%s %s %s", method, path, version);

          // Parse headers
          char *line = strstr(request, "\r\n") + 2;
          while (line && strncmp(line, "\r\n", 2) != 0) {
              char *colon = strchr(line, ':');
              if (colon) {
                  *colon = '\0';
                  char *value = colon + 1;
                  while (*value == ' ') value++;
                  // Store header: line -> value
              }
              line = strstr(line, "\r\n");
              if (line) line += 2;
          }
      pitfalls:
      - CRLF vs LF line endings
      - Whitespace around header values
      - Buffer overflow on long lines
      concepts:
      - HTTP message format
      - Request parsing
      - Header handling
      estimated_hours: 2-3
      deliverables:
      - Request line parsing extracting method, path, and HTTP version
      - Header parsing converting header lines into key-value pair structure
      - Body handling reading content based on Content-Length header value
      - Error responses for malformed requests with appropriate status codes
    - id: 3
      name: Static File Serving
      description: Serve files from a directory.
      acceptance_criteria:
      - Map URL path to filesystem path within configured document root directory
      - Detect and set Content-Type header based on file extension or magic bytes
      - Send file contents in response body with correct Content-Length header
      - Handle 404 Not Found when requested file does not exist in document root
      - Prevent directory traversal attacks by validating resolved path stays within root
      hints:
        level1: Concatenate document root with URL path.
        level2: Use realpath() to resolve and validate path.
        level3: |-
          char filepath[512];
          snprintf(filepath, sizeof(filepath), "%s%s", docroot, path);
          char resolved[PATH_MAX];
          if (!realpath(filepath, resolved) || strncmp(resolved, docroot, strlen(docroot)) != 0) {
              send_404(client_fd);
              return;
          }
          FILE *f = fopen(resolved, "rb");
          if (!f) { send_404(client_fd); return; }
          fseek(f, 0, SEEK_END);
          long size = ftell(f);
          fseek(f, 0, SEEK_SET);
          char *mime = get_mime_type(resolved);
          dprintf(client_fd, "HTTP/1.1 200 OK\r\nContent-Type: %s\r\nContent-Length: %ld\r\n\r\n", mime, size);
          char buf[8192];
          size_t n;
          while ((n = fread(buf, 1, sizeof(buf), f)) > 0) {
              write(client_fd, buf, n);
          }
      pitfalls:
      - Directory traversal (../)
      - Missing Content-Type
      - Binary file handling
      concepts:
      - Path resolution
      - MIME types
      - Security validation
      estimated_hours: 3-4
      deliverables:
      - File reading from document root mapped to URL path
      - MIME type detection based on file extension for Content-Type header
      - 404 handling returning Not Found response for missing files
      - Directory listing displaying files when path maps to a directory
    - id: 4
      name: Concurrent Connections
      description: Handle multiple clients concurrently.
      acceptance_criteria:
      - Thread-per-connection model handles multiple simultaneous client connections
      - Thread pool limits concurrency to prevent resource exhaustion under load
      - Non-blocking with select or poll handles connections without one-thread-per-client
      - Graceful shutdown completes in-flight requests before stopping the server
      hints:
        level1: 'Simple: fork() per connection or spawn thread.'
        level2: 'Better: thread pool with work queue.'
        level3: |-
          // Thread per connection
          while (1) {
              int client_fd = accept(server_fd, NULL, NULL);
              pthread_t thread;
              int *arg = malloc(sizeof(int));
              *arg = client_fd;
              pthread_create(&thread, NULL, handle_client, arg);
              pthread_detach(thread);
          }

          void *handle_client(void *arg) {
              int client_fd = *(int*)arg;
              free(arg);
              // ... handle request ...
              close(client_fd);
              return NULL;
          }
      pitfalls:
      - Thread safety
      - File descriptor leaks
      - Resource exhaustion
      concepts:
      - Concurrency models
      - Threading
      - Connection handling
      estimated_hours: 4-6
      deliverables:
      - Thread per connection model spawning handler for each client
      - Thread pool option limiting concurrent handlers for resource control
      - Non-blocking I/O option using select or poll for event-driven handling
      - Connection limits preventing resource exhaustion from too many clients
  http2-server:
    id: http2-server
    name: HTTP/2 Server
    description: Build an HTTP/2 server with multiplexing. Learn binary framing, HPACK compression, and stream management.
    difficulty: advanced
    estimated_hours: 30-50
    prerequisites:
    - HTTP/1.1 server
    - TLS
    - Binary protocols
    languages:
      recommended:
      - Go
      - Rust
      - C
      also_possible:
      - Java
      - Python
    resources:
    - type: spec
      name: RFC 7540 - HTTP/2
      url: https://httpwg.org/specs/rfc7540.html
    - type: spec
      name: RFC 7541 - HPACK
      url: https://httpwg.org/specs/rfc7541.html
    milestones:
    - id: 1
      name: Binary Framing
      description: Parse and emit HTTP/2 frames.
      acceptance_criteria:
      - Frame header parsing correctly reads 9-byte header with length, type, flags, and stream ID
      - Frame type handling supports DATA, HEADERS, SETTINGS, PING, GOAWAY, and WINDOW_UPDATE
      - Frame flags correctly interpreted per frame type for END_STREAM and END_HEADERS
      - Payload extraction reads correct number of bytes based on frame length field
      hints:
        level1: 'All frames start with 9-byte header: length(3), type(1), flags(1), stream_id(4).'
        level2: Length is 24-bit big-endian. Stream ID has reserved high bit.
        level3: "class Frame:\n    def __init__(self, type, flags, stream_id, payload=b''):\n        self.type = type\n  \
          \      self.flags = flags\n        self.stream_id = stream_id\n        self.payload = payload\n\ndef parse_frame(data):\n\
          \    if len(data) < 9:\n        return None, data  # Need more data\n    \n    length = int.from_bytes(data[0:3],\
          \ 'big')\n    type_ = data[3]\n    flags = data[4]\n    stream_id = int.from_bytes(data[5:9], 'big') & 0x7FFFFFFF\n\
          \    \n    if len(data) < 9 + length:\n        return None, data  # Need more data\n    \n    payload = data[9:9+length]\n\
          \    remaining = data[9+length:]\n    \n    return Frame(type_, flags, stream_id, payload), remaining\n\ndef encode_frame(frame):\n\
          \    header = (\n        frame.length.to_bytes(3, 'big') +\n        bytes([frame.type, frame.flags]) +\n       \
          \ (frame.stream_id & 0x7FFFFFFF).to_bytes(4, 'big')\n    )\n    return header + frame.payload\n\n# Frame types\n\
          DATA = 0x0\nHEADERS = 0x1\nPRIORITY = 0x2\nRST_STREAM = 0x3\nSETTINGS = 0x4\nPUSH_PROMISE = 0x5\nPING = 0x6\nGOAWAY\
          \ = 0x7\nWINDOW_UPDATE = 0x8\nCONTINUATION = 0x9"
      pitfalls:
      - Endianness issues
      - Frame length limits
      - Reserved bit handling
      concepts:
      - Binary protocols
      - Frame-based messaging
      - Protocol parsing
      estimated_hours: 5-8
      deliverables:
      - Frame header parsing extracting 9-byte header with length, type, and flags
      - Frame types implementation for DATA, HEADERS, SETTINGS, and more
      - Frame serialization encoding frames into binary wire format
      - Frame validation checking length limits and reserved bit constraints
    - id: 2
      name: HPACK Compression
      description: Implement header compression with static/dynamic tables.
      acceptance_criteria:
      - Static table lookup resolves indexed headers from the predefined 61-entry table
      - Dynamic table management adds and evicts entries maintaining size constraint
      - Huffman decoding decompresses encoded header values to original string form
      - Integer encoding and decoding handles 5, 6, and 7-bit prefix formats correctly
      hints:
        level1: HPACK uses indexed references + Huffman. Static table has 61 entries.
        level2: Dynamic table is FIFO with size limit. Integers use 5/6/7-bit prefixes.
        level3: "STATIC_TABLE = [\n    (None, None),  # Index 0 unused\n    (':authority', ''),\n    (':method', 'GET'),\n\
          \    (':method', 'POST'),\n    (':path', '/'),\n    (':path', '/index.html'),\n    (':scheme', 'http'),\n    (':scheme',\
          \ 'https'),\n    (':status', '200'),\n    # ... 61 entries total\n]\n\nclass HPACKDecoder:\n    def __init__(self,\
          \ max_size=4096):\n        self.dynamic_table = []\n        self.max_size = max_size\n        self.size = 0\n  \
          \  \n    def decode(self, data):\n        headers = []\n        i = 0\n        \n        while i < len(data):\n\
          \            byte = data[i]\n            \n            if byte & 0x80:  # Indexed header (7-bit index)\n       \
          \         idx, i = self.decode_int(data, i, 7)\n                name, value = self.get_indexed(idx)\n          \
          \      headers.append((name, value))\n            \n            elif byte & 0x40:  # Literal with indexing\n   \
          \             idx, i = self.decode_int(data, i, 6)\n                if idx > 0:\n                    name, _ = self.get_indexed(idx)\n\
          \                else:\n                    name, i = self.decode_string(data, i)\n                value, i = self.decode_string(data,\
          \ i)\n                self.add_to_dynamic(name, value)\n                headers.append((name, value))\n        \
          \    \n            # ... handle other cases\n        \n        return headers\n    \n    def get_indexed(self, idx):\n\
          \        if idx <= 61:\n            return STATIC_TABLE[idx]\n        return self.dynamic_table[idx - 62]"
      pitfalls:
      - Table size eviction
      - Index off-by-one errors
      - Huffman boundary handling
      concepts:
      - Header compression
      - Table-based encoding
      - Huffman coding
      estimated_hours: 8-12
      deliverables:
      - Static table with predefined 61 header name-value entries
      - Dynamic table with FIFO eviction and configurable size limit
      - Header encoding compressing headers using indexed and literal representations
      - Header decoding reconstructing headers from HPACK-compressed wire format
    - id: 3
      name: Stream Management
      description: Handle multiplexed streams with proper state machines.
      acceptance_criteria:
      - Stream states transition correctly between idle, open, half-closed, and closed
      - Stream ID allocation uses odd numbers for client and even for server streams
      - Priority handling schedules stream data based on weight and dependency tree
      - Concurrent stream limits enforce maximum active streams per connection setting
      hints:
        level1: Client streams are odd, server (push) are even. Each has 5 states.
        level2: HEADERS opens stream, END_STREAM half-closes. Track dependencies for priority.
        level3: "from enum import Enum\n\nclass StreamState(Enum):\n    IDLE = 'idle'\n    RESERVED_LOCAL = 'reserved_local'\n\
          \    RESERVED_REMOTE = 'reserved_remote'\n    OPEN = 'open'\n    HALF_CLOSED_LOCAL = 'half_closed_local'\n    HALF_CLOSED_REMOTE\
          \ = 'half_closed_remote'\n    CLOSED = 'closed'\n\nclass Stream:\n    def __init__(self, stream_id):\n        self.id\
          \ = stream_id\n        self.state = StreamState.IDLE\n        self.window = 65535  # Initial flow control window\n\
          \        self.request_headers = None\n        self.request_body = b''\n        self.response_headers = None\n  \
          \      self.response_body = b''\n    \n    def recv_headers(self, end_stream):\n        if self.state == StreamState.IDLE:\n\
          \            self.state = StreamState.OPEN\n        \n        if end_stream:\n            if self.state == StreamState.OPEN:\n\
          \                self.state = StreamState.HALF_CLOSED_REMOTE\n    \n    def send_headers(self, end_stream):\n  \
          \      if end_stream:\n            if self.state == StreamState.OPEN:\n                self.state = StreamState.HALF_CLOSED_LOCAL\n\
          \            elif self.state == StreamState.HALF_CLOSED_REMOTE:\n                self.state = StreamState.CLOSED\n\
          \nclass Connection:\n    def __init__(self):\n        self.streams = {}\n        self.next_server_stream_id = 2\n\
          \        self.settings = {...}\n    \n    def get_or_create_stream(self, stream_id):\n        if stream_id not in\
          \ self.streams:\n            self.streams[stream_id] = Stream(stream_id)\n        return self.streams[stream_id]"
      pitfalls:
      - State machine violations
      - Stream ID exhaustion
      - Dependency cycles
      concepts:
      - Multiplexing
      - State machines
      - Concurrency
      estimated_hours: 8-12
      deliverables:
      - Stream state machine implementing idle, open, half-closed, and closed states
      - Stream prioritization with weight and dependency configuration
      - Stream creation assigning IDs for client and server-initiated streams
      - RST_STREAM handling for stream cancellation and error signaling
    - id: 4
      name: Flow Control
      description: Implement connection and stream-level flow control.
      acceptance_criteria:
      - Window sizes track available send capacity at connection and stream levels
      - WINDOW_UPDATE frames correctly increment the receiver's available send window
      - Connection-level windows limit total DATA frame bytes across all streams combined
      - Backpressure handling queues data when send window hits zero until window update arrives
      hints:
        level1: Each stream AND connection has a window. Both must have space to send.
        level2: WINDOW_UPDATE increments window. Sender blocks when window hits 0.
        level3: "class FlowController:\n    def __init__(self, initial_window=65535):\n        self.connection_window = initial_window\n\
          \        self.stream_windows = {}  # stream_id -> window\n        self.pending_data = {}  # stream_id -> [(data,\
          \ callback)]\n    \n    def can_send(self, stream_id, size):\n        stream_window = self.stream_windows.get(stream_id,\
          \ 65535)\n        return self.connection_window >= size and stream_window >= size\n    \n    def send_data(self,\
          \ stream_id, data):\n        size = len(data)\n        if not self.can_send(stream_id, size):\n            # Queue\
          \ for later\n            if stream_id not in self.pending_data:\n                self.pending_data[stream_id] =\
          \ []\n            self.pending_data[stream_id].append(data)\n            return False\n        \n        # Decrement\
          \ windows\n        self.connection_window -= size\n        self.stream_windows[stream_id] -= size\n        return\
          \ True\n    \n    def recv_window_update(self, stream_id, increment):\n        if stream_id == 0:\n            self.connection_window\
          \ += increment\n        else:\n            self.stream_windows[stream_id] = \\\n                self.stream_windows.get(stream_id,\
          \ 65535) + increment\n        \n        # Check if we can now send pending data\n        self.flush_pending()\n\
          \    \n    def send_window_update(self, stream_id, increment):\n        frame = Frame(\n            type=WINDOW_UPDATE,\n\
          \            flags=0,\n            stream_id=stream_id,\n            payload=increment.to_bytes(4, 'big')\n    \
          \    )\n        return encode_frame(frame)"
      pitfalls:
      - Window underflow/overflow
      - Deadlocks from exhausted windows
      - Forgetting connection window
      concepts:
      - Flow control
      - Backpressure
      - Resource management
      estimated_hours: 9-18
      deliverables:
      - Window update frame handling for adjusting flow control window sizes
      - Connection-level flow control tracking aggregate send window across streams
      - Stream-level flow control tracking per-stream send window independently
      - Backpressure handling queueing data when send window is exhausted
  https-client:
    id: https-client
    name: HTTPS Client
    description: Build an HTTPS client that performs TLS handshake. Learn certificate validation and encrypted communication.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - TCP sockets
    - Cryptography basics
    - X.509 certificates
    languages:
      recommended:
      - Python
      - Go
      - Rust
      also_possible:
      - C
      - Java
    resources:
    - name: TLS 1.3 RFC 8446
      url: https://datatracker.ietf.org/doc/html/rfc8446
      type: specification
    - name: Illustrated TLS 1.3
      url: https://tls13.xargs.org/
      type: tutorial
    milestones:
    - id: 1
      name: TCP Socket & Record Layer
      description: Establish TCP connection and implement TLS record layer.
      acceptance_criteria:
      - TCP connect to remote host on port 443 for TLS communication
      - TLS record format correctly encodes type, version, length, and payload fields
      - Fragment handling reassembles records split across multiple TCP segments
      - Record layer parsing correctly reads and classifies incoming TLS records
      hints:
        level1: 'TLS record: 1 byte type, 2 bytes version, 2 bytes length, then payload.'
        level2: 'Content types: 20=ChangeCipherSpec, 21=Alert, 22=Handshake, 23=Application.'
        level3: "import socket\nimport struct\n\nclass TLSRecord:\n    CHANGE_CIPHER_SPEC = 20\n    ALERT = 21\n    HANDSHAKE\
          \ = 22\n    APPLICATION_DATA = 23\n\nclass TLSConnection:\n    def __init__(self, host, port=443):\n        self.sock\
          \ = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.connect((host, port))\n        self.host\
          \ = host\n    \n    def send_record(self, content_type, data):\n        # TLS 1.2 version for compatibility (actual\
          \ version in handshake)\n        version = (3, 3)\n        header = struct.pack('!BHH', content_type, (version[0]\
          \ << 8) | version[1], len(data))\n        self.sock.sendall(header + data)\n    \n    def recv_record(self):\n \
          \       header = self.sock.recv(5)\n        if len(header) < 5:\n            raise ConnectionError('Connection closed')\n\
          \        \n        content_type, version, length = struct.unpack('!BHH', header)\n        data = b''\n        while\
          \ len(data) < length:\n            chunk = self.sock.recv(length - len(data))\n            if not chunk:\n     \
          \           raise ConnectionError('Connection closed')\n            data += chunk\n        \n        return content_type,\
          \ data"
      pitfalls:
      - Endianness
      - Partial reads
      - Record fragmentation
      concepts:
      - TLS records
      - Network protocols
      - Binary parsing
      estimated_hours: 3-4
      deliverables:
      - TCP connection establishment to remote server on port 443
      - TLS record layer implementing type, version, length, and data framing
      - Record framing with proper fragment boundary handling on receive
      - Record types for handshake, alert, change-cipher-spec, and application data
    - id: 2
      name: ClientHello
      description: Send ClientHello with supported cipher suites.
      acceptance_criteria:
      - Random bytes generation produces 32 cryptographically secure random bytes for ClientHello
      - Session ID field correctly included for TLS 1.2 compatibility in ClientHello message
      - Cipher suite list includes at least TLS_AES_128_GCM_SHA256 for TLS 1.3 support
      - SNI extension includes target hostname so server selects correct certificate
      hints:
        level1: ClientHello starts handshake. Include TLS version, random, extensions.
        level2: SNI (Server Name Indication) tells server which hostname you want.
        level3: "import os\n\ndef build_client_hello(hostname):\n    # Random (32 bytes)\n    client_random = os.urandom(32)\n\
          \    \n    # Session ID (empty for new connection)\n    session_id = b''\n    \n    # Cipher suites (TLS 1.3)\n\
          \    cipher_suites = struct.pack('!H', 2)  # Length\n    cipher_suites += struct.pack('!H', 0x1301)  # TLS_AES_128_GCM_SHA256\n\
          \    \n    # Compression methods (null only)\n    compression = struct.pack('!B', 1) + struct.pack('!B', 0)\n  \
          \  \n    # Extensions\n    extensions = build_sni_extension(hostname)\n    extensions += build_supported_versions_extension()\n\
          \    extensions += build_key_share_extension()\n    \n    # Build handshake message\n    body = struct.pack('!H',\
          \ 0x0303)  # Legacy version TLS 1.2\n    body += client_random\n    body += struct.pack('!B', len(session_id)) +\
          \ session_id\n    body += cipher_suites\n    body += compression\n    body += struct.pack('!H', len(extensions))\
          \ + extensions\n    \n    # Handshake header: type (1) + length (3)\n    handshake = struct.pack('!B', 1)  # ClientHello\n\
          \    handshake += struct.pack('!I', len(body))[1:]  # 3-byte length\n    handshake += body\n    \n    return handshake,\
          \ client_random\n\ndef build_sni_extension(hostname):\n    hostname_bytes = hostname.encode()\n    # SNI list entry\n\
          \    entry = struct.pack('!B', 0)  # DNS hostname type\n    entry += struct.pack('!H', len(hostname_bytes))\n  \
          \  entry += hostname_bytes\n    # SNI list\n    sni_list = struct.pack('!H', len(entry)) + entry\n    # Extension\n\
          \    ext = struct.pack('!H', 0)  # SNI extension type\n    ext += struct.pack('!H', len(sni_list)) + sni_list\n\
          \    return ext"
      pitfalls:
      - Extension ordering
      - Length field sizes
      - Version negotiation
      concepts:
      - TLS handshake
      - Cipher negotiation
      - Extensions
      estimated_hours: 4-6
      deliverables:
      - ClientHello construction with TLS version, random, and cipher suites
      - Cipher suite list specifying supported encryption algorithm combinations
      - Extensions including SNI, supported versions, and key share
      - ServerHello parsing extracting server's chosen parameters and key share
    - id: 3
      name: Key Exchange
      description: Perform ECDHE key exchange and derive keys.
      acceptance_criteria:
      - Generate ephemeral ECDHE key pair using X25519 or P-256 curve
      - Parse ServerHello and extract server's key share for shared secret computation
      - ECDH shared secret computation derives pre-master secret from key exchange
      - Key derivation via HKDF produces client and server traffic keys and IVs
      hints:
        level1: 'ECDHE: both sides generate key pairs, share public keys, compute shared secret.'
        level2: TLS 1.3 uses HKDF for key derivation from shared secret.
        level3: "from cryptography.hazmat.primitives.asymmetric import x25519\nfrom cryptography.hazmat.primitives import\
          \ hashes\nfrom cryptography.hazmat.primitives.kdf.hkdf import HKDFExpand, HKDF\nimport hashlib\n\nclass KeyExchange:\n\
          \    def __init__(self):\n        self.private_key = x25519.X25519PrivateKey.generate()\n        self.public_key\
          \ = self.private_key.public_key()\n    \n    def get_public_bytes(self):\n        return self.public_key.public_bytes_raw()\n\
          \    \n    def compute_shared_secret(self, peer_public_bytes):\n        peer_public = x25519.X25519PublicKey.from_public_bytes(peer_public_bytes)\n\
          \        return self.private_key.exchange(peer_public)\n\ndef derive_keys(shared_secret, client_hello_hash, server_hello_hash):\n\
          \    # Early secret\n    early_secret = hkdf_extract(b'\\x00' * 32, b'\\x00' * 32)\n    \n    # Handshake secret\n\
          \    derived = hkdf_expand_label(early_secret, b'derived', b'', 32)\n    handshake_secret = hkdf_extract(derived,\
          \ shared_secret)\n    \n    # Traffic secrets\n    messages_hash = hashlib.sha256(client_hello_hash + server_hello_hash).digest()\n\
          \    \n    client_handshake_traffic_secret = hkdf_expand_label(\n        handshake_secret, b'c hs traffic', messages_hash,\
          \ 32\n    )\n    server_handshake_traffic_secret = hkdf_expand_label(\n        handshake_secret, b's hs traffic',\
          \ messages_hash, 32\n    )\n    \n    # Derive keys and IVs\n    client_key = hkdf_expand_label(client_handshake_traffic_secret,\
          \ b'key', b'', 16)\n    client_iv = hkdf_expand_label(client_handshake_traffic_secret, b'iv', b'', 12)\n    \n \
          \   return client_key, client_iv"
      pitfalls:
      - Curve negotiation
      - Key derivation order
      - Hash transcript
      concepts:
      - ECDHE
      - Key derivation
      - Forward secrecy
      estimated_hours: 6-8
      deliverables:
      - ECDHE key exchange generating ephemeral key pair for forward secrecy
      - Key derivation using HKDF to produce traffic encryption keys from shared secret
      - Certificate validation verifying server identity through certificate chain
      - Finished message verification confirming handshake integrity on both sides
    - id: 4
      name: Encrypted Communication
      description: Encrypt/decrypt application data with derived keys.
      acceptance_criteria:
      - AEAD encryption using AES-GCM encrypts application data with authentication tag
      - Sequence number handling computes unique nonce by XORing IV with incrementing counter
      - Finished message verification confirms handshake transcript hash matches expected value
      - Send and receive application data over the encrypted TLS channel successfully
      hints:
        level1: TLS 1.3 uses AEAD. Nonce = IV XOR sequence number.
        level2: Record header is additional authenticated data (AAD).
        level3: "from cryptography.hazmat.primitives.ciphers.aead import AESGCM\n\nclass TLS13Cipher:\n    def __init__(self,\
          \ key, iv):\n        self.aesgcm = AESGCM(key)\n        self.iv = iv\n        self.seq_num = 0\n    \n    def _compute_nonce(self):\n\
          \        # XOR IV with sequence number (padded to IV length)\n        seq_bytes = self.seq_num.to_bytes(len(self.iv),\
          \ 'big')\n        nonce = bytes(a ^ b for a, b in zip(self.iv, seq_bytes))\n        self.seq_num += 1\n        return\
          \ nonce\n    \n    def encrypt(self, plaintext, content_type):\n        # Add content type byte (inner plaintext)\n\
          \        inner = plaintext + bytes([content_type])\n        \n        # Build AAD (record header with encrypted\
          \ content type)\n        aad = struct.pack('!BHH', 23, 0x0303, len(inner) + 16)  # +16 for auth tag\n        \n\
          \        nonce = self._compute_nonce()\n        ciphertext = self.aesgcm.encrypt(nonce, inner, aad)\n        \n\
          \        return aad + ciphertext\n    \n    def decrypt(self, record_data):\n        # AAD is the record header\n\
          \        aad = record_data[:5]\n        ciphertext = record_data[5:]\n        \n        nonce = self._compute_nonce()\n\
          \        plaintext = self.aesgcm.decrypt(nonce, ciphertext, aad)\n        \n        # Last byte is real content\
          \ type\n        content_type = plaintext[-1]\n        data = plaintext[:-1].rstrip(b'\\x00')  # Remove padding\n\
          \        \n        return content_type, data"
      pitfalls:
      - Nonce reuse
      - Padding removal
      - Alert handling
      concepts:
      - AEAD encryption
      - Sequence numbers
      - TLS records
      estimated_hours: 6-8
      deliverables:
      - Application data encryption using AEAD cipher with derived traffic keys
      - Request sending constructing and encrypting HTTP request for transmission
      - Response receiving decrypting and parsing HTTP response from encrypted records
      - Connection close with proper TLS close_notify alert exchange
  integration-testing:
    id: integration-testing
    name: Integration Testing Suite
    description: Build comprehensive integration tests. Learn to test components working together with real dependencies.
    difficulty: advanced
    estimated_hours: 12-20
    prerequisites:
    - Unit testing
    - Docker basics
    - Database knowledge
    languages:
      recommended:
      - Python
      - JavaScript
      - Java
      also_possible:
      - Go
      - Ruby
    resources:
    - name: Testcontainers
      url: https://www.testcontainers.org/
      type: documentation
    milestones:
    - id: 1
      name: Test Database Setup
      description: Set up isolated database for testing.
      acceptance_criteria:
      - Spin up test database in Docker container with configured credentials and port
      - Migrations run before test suite executes to create required schema tables
      - Clean state between tests using transaction rollback or table truncation
      - Tear down database container after test suite completes to free resources
      hints:
        level1: Use Docker Compose or Testcontainers.
        level2: Transaction rollback for fast cleanup.
        level3: |-
          import pytest
          import docker

          @pytest.fixture(scope='session')
          def postgres_container():
              client = docker.from_env()
              container = client.containers.run(
                  'postgres:15',
                  environment={'POSTGRES_PASSWORD': 'test'},
                  ports={'5432/tcp': None},
                  detach=True
              )
              # Wait for ready
              import time
              time.sleep(3)
              port = container.ports['5432/tcp'][0]['HostPort']
              yield f'postgresql://postgres:test@localhost:{port}/postgres'
              container.stop()
              container.remove()

          @pytest.fixture
          def db_session(postgres_container):
              engine = create_engine(postgres_container)
              # Run migrations
              Base.metadata.create_all(engine)
              session = Session(engine)
              yield session
              session.rollback()  # Clean up
              session.close()
      pitfalls:
      - Port conflicts
      - Container startup time
      - Data leaking between tests
      concepts:
      - Test isolation
      - Containers in testing
      - Database fixtures
      estimated_hours: 3-4
      deliverables:
      - Database container setup using Docker for isolated test environment
      - Schema migrations running before test suite to prepare database structure
      - Test data seeding inserting fixture records for deterministic test scenarios
      - Database cleanup resetting state between tests to prevent data leaks
    - id: 2
      name: API Integration Tests
      description: Test API endpoints with real HTTP requests.
      acceptance_criteria:
      - Start test server with test configuration and database connection
      - Make real HTTP requests to running server and capture full responses
      - Verify response status codes and body content match expected values
      - Test complete authentication flow including registration, login, and protected access
      hints:
        level1: Use requests library or test client.
        level2: Fixtures for authenticated users.
        level3: "import pytest\nfrom fastapi.testclient import TestClient\nfrom app import app, get_db\n\n@pytest.fixture\n\
          def client(db_session):\n    def override_get_db():\n        yield db_session\n    app.dependency_overrides[get_db]\
          \ = override_get_db\n    yield TestClient(app)\n    app.dependency_overrides.clear()\n\n@pytest.fixture\ndef auth_client(client,\
          \ db_session):\n    # Create test user\n    user = User(email='test@example.com')\n    user.set_password('password')\n\
          \    db_session.add(user)\n    db_session.commit()\n    \n    # Login and get token\n    response = client.post('/auth/login',\
          \ json={\n        'email': 'test@example.com',\n        'password': 'password'\n    })\n    token = response.json()['access_token']\n\
          \    client.headers['Authorization'] = f'Bearer {token}'\n    return client\n\ndef test_create_post(auth_client):\n\
          \    response = auth_client.post('/posts', json={'title': 'Test', 'content': 'Content'})\n    assert response.status_code\
          \ == 201\n    assert response.json()['title'] == 'Test'"
      pitfalls:
      - Test order dependencies
      - Shared state
      - Slow tests
      concepts:
      - API testing
      - Test clients
      - Authentication in tests
      estimated_hours: 4-5
      deliverables:
      - HTTP client for tests making real requests to running test server
      - Request and response assertions verifying status codes and response bodies
      - Authentication in tests with fixture users and valid session tokens
      - Error case testing verifying proper error responses for invalid requests
    - id: 3
      name: External Service Mocking
      description: Mock external APIs while testing real internal integration.
      acceptance_criteria:
      - Mock HTTP responses for external APIs returning predefined response data
      - Verify external API was called with correct URL, method, headers, and body
      - Simulate error responses from external services to test error handling paths
      - Test retry logic by configuring mock to fail initially then succeed on retry
      hints:
        level1: Use responses (Python) or nock (Node.js) to mock HTTP.
        level2: Record real responses for realistic mocks.
        level3: "import responses\n\n@responses.activate\ndef test_payment_integration(auth_client):\n    # Mock Stripe API\n\
          \    responses.add(\n        responses.POST,\n        'https://api.stripe.com/v1/charges',\n        json={'id':\
          \ 'ch_123', 'status': 'succeeded'},\n        status=200\n    )\n    \n    # Make request that triggers Stripe call\n\
          \    response = auth_client.post('/orders/123/pay', json={\n        'card_token': 'tok_visa'\n    })\n    \n   \
          \ assert response.status_code == 200\n    assert response.json()['payment_status'] == 'paid'\n    \n    # Verify\
          \ Stripe was called correctly\n    assert len(responses.calls) == 1\n    assert 'amount' in responses.calls[0].request.body\n\
          \n@responses.activate\ndef test_payment_failure_handling(auth_client):\n    # Mock Stripe failure\n    responses.add(\n\
          \        responses.POST,\n        'https://api.stripe.com/v1/charges',\n        json={'error': {'message': 'Card\
          \ declined'}},\n        status=402\n    )\n    \n    response = auth_client.post('/orders/123/pay', json={'card_token':\
          \ 'tok_bad'})\n    assert response.status_code == 400\n    assert 'declined' in response.json()['error']"
      pitfalls:
      - Missing mock causes real API call
      - Mock doesn't match real API
      - Order of mock setup
      concepts:
      - HTTP mocking
      - Service virtualization
      - Error simulation
      estimated_hours: 3-4
      deliverables:
      - Mock server setup intercepting outbound HTTP calls to external APIs
      - Response stubbing returning predefined responses for matched API requests
      - Request verification confirming external API was called with correct parameters
      - Network isolation ensuring no real external API calls escape during tests
  json-parser:
    id: json-parser
    name: JSON Parser
    description: Build a JSON parser using recursive descent parsing. Learn about tokenization and AST construction.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - Basic programming
    - Understanding of JSON format
    languages:
      recommended:
      - Python
      - JavaScript
      - C
      also_possible:
      - Rust
      - Go
    resources:
    - name: JSON Specification
      url: https://www.json.org/json-en.html
      type: specification
    - name: Crafting Interpreters
      url: https://craftinginterpreters.com/
      type: book
    milestones:
    - id: 1
      name: Tokenizer
      description: Build a lexer that converts JSON string into tokens.
      acceptance_criteria:
      - Tokenizer correctly identifies and categorizes strings, numbers, booleans, and null literals
      - Punctuation tokens for braces, brackets, colons, and commas are emitted individually
      - Whitespace characters including spaces, tabs, newlines, and carriage returns are skipped between tokens
      - Escape sequences including backslash-n, backslash-t, backslash-u hex, and backslash-quote are handled in strings
      hints:
        level1: Read char by char, emit tokens based on current char.
        level2: 'Handle escape sequences in strings: \n, \t, \\, etc.'
        level3: |-
          class Tokenizer:
              def __init__(self, text):
                  self.text = text
                  self.pos = 0

              def tokenize(self):
                  tokens = []
                  while self.pos < len(self.text):
                      char = self.text[self.pos]
                      if char.isspace():
                          self.pos += 1
                      elif char == '"':
                          tokens.append(self.read_string())
                      elif char.isdigit() or char == '-':
                          tokens.append(self.read_number())
                      elif char in '{}[]:,':
                          tokens.append(('PUNCT', char))
                          self.pos += 1
                      elif self.text[self.pos:self.pos+4] == 'true':
                          tokens.append(('BOOL', True))
                          self.pos += 4
                  return tokens
      pitfalls:
      - Escape sequences in strings
      - Negative numbers
      - Scientific notation
      concepts:
      - Lexical analysis
      - Token types
      - State machine
      estimated_hours: 2-3
      deliverables:
      - Token type definitions for string, number, boolean, null, and structural characters
      - String tokenizer that correctly handles escape sequences including unicode escapes
      - Number parser that recognizes integers, floats, and scientific notation formats
      - Delimiter token recognizer for braces, brackets, colons, and commas
    - id: 2
      name: Parser
      description: Parse tokens into native data structures.
      acceptance_criteria:
      - Parser correctly builds native data structures from object and array token sequences
      - Deeply nested structures with mixed objects and arrays parse without stack overflow up to configurable depth
      - Parsed values are returned as native language types (dict, list, string, number, bool, null)
      - Syntax errors produce descriptive error messages indicating the unexpected token and its position
      hints:
        level1: 'Recursive descent: parse_value calls parse_object/parse_array.'
        level2: 'Object: {key: value, ...}. Array: [value, ...].'
        level3: |-
          class Parser:
              def parse_value(self):
                  token = self.current()
                  if token[0] == 'PUNCT' and token[1] == '{':
                      return self.parse_object()
                  elif token[0] == 'PUNCT' and token[1] == '[':
                      return self.parse_array()
                  elif token[0] == 'STRING':
                      self.advance()
                      return token[1]
                  elif token[0] == 'NUMBER':
                      self.advance()
                      return token[1]
                  elif token[0] == 'BOOL':
                      self.advance()
                      return token[1]

              def parse_object(self):
                  self.expect('{')
                  obj = {}
                  while self.current()[1] != '}':
                      key = self.expect_string()
                      self.expect(':')
                      value = self.parse_value()
                      obj[key] = value
                      if self.current()[1] == ',':
                          self.advance()
                  self.expect('}')
                  return obj
      pitfalls:
      - Trailing comma handling
      - Deep nesting stack overflow
      - Empty object/array edge case
      concepts:
      - Recursive descent
      - Grammar rules
      - AST construction
      estimated_hours: 3-4
      deliverables:
      - Recursive descent parser that consumes tokens and builds a structured document tree
      - Object parser that reads key-value pairs delimited by colons and separated by commas
      - Array parser that reads ordered sequences of values within bracket delimiters
      - Nested structure handler that recursively processes objects and arrays to arbitrary depth
    - id: 3
      name: Error Handling & Edge Cases
      description: Add robust error handling and edge case support.
      acceptance_criteria:
      - Error messages include line number, column offset, and a description of the expected versus found token
      - All JSON value types including nested empty objects and empty arrays parse correctly
      - Number format validation rejects leading zeros, lone minus signs, and trailing decimal points
      - Unicode escape sequences in strings including surrogate pairs are decoded to proper characters
      hints:
        level1: Track line and column for error messages.
        level2: 'Number format: optional minus, integer or decimal, optional exponent.'
        level3: |-
          class JSONError(Exception):
              def __init__(self, message, line, col):
                  super().__init__(f'{message} at line {line}, column {col}')
                  self.line = line
                  self.col = col

          def read_number(self):
              start = self.pos
              if self.current_char() == '-':
                  self.advance()
              if self.current_char() == '0':
                  self.advance()
              else:
                  while self.current_char().isdigit():
                      self.advance()
              if self.current_char() == '.':
                  self.advance()
                  while self.current_char().isdigit():
                      self.advance()
              if self.current_char() in 'eE':
                  self.advance()
                  if self.current_char() in '+-':
                      self.advance()
                  while self.current_char().isdigit():
                      self.advance()
              return float(self.text[start:self.pos])
      pitfalls:
      - Numbers with leading zeros (invalid in JSON)
      - Unicode escapes \u0000
      - Duplicate keys in objects
      concepts:
      - Error recovery
      - JSON specification compliance
      - Unicode handling
      estimated_hours: 2-3
      deliverables:
      - Syntax error reporter that outputs line number, column, and description of the parsing failure
      - Trailing comma handler that detects and reports trailing commas in objects and arrays
      - Unicode escape sequence processor that converts backslash-u hex sequences to characters
      - Nested depth limiter that raises an error when document nesting exceeds a configurable maximum
  jwt-impl:
    id: jwt-impl
    name: JWT Library
    description: Implement JSON Web Token signing and verification. Learn about authentication tokens and cryptographic signatures.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - JSON
    - Base64
    - HMAC basics
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - Java
      - Rust
    resources:
    - name: JWT Specification (RFC 7519)
      url: https://tools.ietf.org/html/rfc7519
      type: specification
    - name: JWT.io Debugger
      url: https://jwt.io/
      type: tool
    milestones:
    - id: 1
      name: JWT Structure
      description: Implement JWT encoding without signing.
      acceptance_criteria:
      - Header JSON contains the correct algorithm identifier and token type fields
      - Payload claims including iss, sub, aud, exp, and custom fields are encoded as valid JSON
      - Base64URL encoding produces URL-safe output without plus, slash, or equals padding characters
      - Final token follows the three-part header.payload.signature dot-separated format
      hints:
        level1: JWT = base64url(header).base64url(payload).signature
        level2: 'Base64url: base64 with + -> -, / -> _, no padding.'
        level3: "import json\nimport base64\n\ndef base64url_encode(data: bytes) -> str:\n    return base64.urlsafe_b64encode(data).rstrip(b'=').decode('ascii')\n\
          \ndef base64url_decode(data: str) -> bytes:\n    padding = 4 - len(data) % 4\n    if padding != 4:\n        data\
          \ += '=' * padding\n    return base64.urlsafe_b64decode(data)\n\ndef encode_jwt(payload: dict, secret: str, algorithm='HS256')\
          \ -> str:\n    header = {'alg': algorithm, 'typ': 'JWT'}\n    header_b64 = base64url_encode(json.dumps(header).encode())\n\
          \    payload_b64 = base64url_encode(json.dumps(payload).encode())\n    \n    message = f'{header_b64}.{payload_b64}'\n\
          \    signature = sign(message, secret, algorithm)\n    signature_b64 = base64url_encode(signature)\n    \n    return\
          \ f'{message}.{signature_b64}'"
      pitfalls:
      - Regular base64 vs base64url
      - Padding removal/addition
      - JSON key ordering
      concepts:
      - JWT structure
      - Base64url
      - Token format
      estimated_hours: 2-3
      deliverables:
      - Header encoder that produces a JSON object with alg and typ fields for the JWT header
      - Payload encoder that serializes registered and custom claims into a JSON payload section
      - Base64URL encoder that converts binary data to URL-safe base64 without padding characters
      - Token assembler that concatenates the encoded header, payload, and signature with dot separators
    - id: 2
      name: HMAC Signing
      description: Implement HS256 signing and verification.
      acceptance_criteria:
      - Tokens signed with HMAC-SHA256 produce signatures matching RFC 7515 test vectors
      - Signature verification returns true only when the recomputed signature matches the token signature
      - Constant-time comparison is used for signature verification to prevent timing side-channel attacks
      - Tokens with invalid, truncated, or tampered signatures are correctly rejected with an error
      hints:
        level1: 'HMAC-SHA256: hmac.new(key, message, sha256).'
        level2: Compare signatures with constant-time function.
        level3: "import hmac\nimport hashlib\n\ndef sign(message: str, secret: str, algorithm: str) -> bytes:\n    if algorithm\
          \ != 'HS256':\n        raise ValueError(f'Unsupported algorithm: {algorithm}')\n    return hmac.new(\n        secret.encode(),\n\
          \        message.encode(),\n        hashlib.sha256\n    ).digest()\n\ndef verify_jwt(token: str, secret: str) ->\
          \ dict:\n    parts = token.split('.')\n    if len(parts) != 3:\n        raise ValueError('Invalid token format')\n\
          \    \n    header_b64, payload_b64, signature_b64 = parts\n    \n    # Decode header to get algorithm\n    header\
          \ = json.loads(base64url_decode(header_b64))\n    algorithm = header.get('alg')\n    \n    # Compute expected signature\n\
          \    message = f'{header_b64}.{payload_b64}'\n    expected_sig = sign(message, secret, algorithm)\n    actual_sig\
          \ = base64url_decode(signature_b64)\n    \n    # Constant-time comparison\n    if not hmac.compare_digest(expected_sig,\
          \ actual_sig):\n        raise ValueError('Invalid signature')\n    \n    return json.loads(base64url_decode(payload_b64))"
      pitfalls:
      - Timing attacks in comparison
      - Algorithm confusion attacks
      - Key encoding
      concepts:
      - HMAC
      - Digital signatures
      - Token verification
      estimated_hours: 2-3
      deliverables:
      - HMAC-SHA256 implementation that computes a keyed hash over the signing input string
      - Signature generation module that signs the header.payload string using the secret key
      - Signature verification module that recomputes and compares the signature for a given token
      - Secret key handler that validates key length and securely stores the signing key in memory
    - id: 3
      name: Claims Validation
      description: Implement standard JWT claim validation.
      acceptance_criteria:
      - Tokens with an exp claim in the past are rejected as expired with appropriate error
      - Tokens with an nbf claim in the future are rejected as not-yet-valid with appropriate error
      - The iat claim is validated to ensure it is not unreasonably far in the past or future
      - The iss claim is verified against a configured allowlist and rejects unknown issuers
      - The aud claim is validated to ensure it matches the expected audience identifier
      hints:
        level1: exp and nbf are Unix timestamps.
        level2: Add clock skew tolerance for exp/nbf.
        level3: "import time\n\ndef validate_claims(payload: dict, audience: str = None, issuer: str = None, clock_skew: int\
          \ = 60):\n    now = time.time()\n    \n    # Expiration\n    if 'exp' in payload:\n        if now > payload['exp']\
          \ + clock_skew:\n            raise ValueError('Token expired')\n    \n    # Not Before\n    if 'nbf' in payload:\n\
          \        if now < payload['nbf'] - clock_skew:\n            raise ValueError('Token not yet valid')\n    \n    #\
          \ Issuer\n    if issuer and payload.get('iss') != issuer:\n        raise ValueError(f'Invalid issuer: {payload.get(\"\
          iss\")}')\n    \n    # Audience\n    if audience:\n        token_aud = payload.get('aud')\n        if isinstance(token_aud,\
          \ list):\n            if audience not in token_aud:\n                raise ValueError('Invalid audience')\n    \
          \    elif token_aud != audience:\n            raise ValueError('Invalid audience')\n\ndef decode_jwt(token: str,\
          \ secret: str, audience: str = None, issuer: str = None) -> dict:\n    payload = verify_jwt(token, secret)\n   \
          \ validate_claims(payload, audience, issuer)\n    return payload"
      pitfalls:
      - Clock skew handling
      - Audience as list
      - Missing required claims
      concepts:
      - JWT claims
      - Token validation
      - Time-based security
      estimated_hours: 2-3
      deliverables:
      - Expiration check that rejects tokens whose exp claim is in the past
      - Not-before check that rejects tokens whose nbf claim is in the future
      - Issuer validation that verifies the iss claim matches a list of allowed issuers
      - Custom claims extractor that parses and returns application-specific payload fields
  knn:
    id: knn
    name: KNN Classifier
    description: Implement K-Nearest Neighbors classification algorithm. Learn about distance metrics and non-parametric models.
    difficulty: beginner
    estimated_hours: 6-10
    prerequisites:
    - Basic Python/NumPy
    - Distance formulas
    languages:
      recommended:
      - Python
      also_possible:
      - JavaScript
      - Julia
    resources:
    - name: KNN Explained
      url: https://scikit-learn.org/stable/modules/neighbors.html
      type: documentation
    - name: KNN from Scratch
      url: https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/
      type: tutorial
    milestones:
    - id: 1
      name: Distance Calculation
      description: Implement distance metrics for KNN.
      acceptance_criteria:
      - Euclidean distance correctly returns the square root of the sum of squared differences
      - Manhattan distance correctly returns the sum of absolute differences across all dimensions
      - Distance to all training points is computed and returned as a sorted array of distances
      - Distance computation runs efficiently using vectorized operations to avoid Python-level loops
      hints:
        level1: 'Euclidean: sqrt(sum((a-b)Â²))'
        level2: Use NumPy's broadcasting for efficient vectorized distance computation across all training samples. Manhattan
          distance (L1 norm) sums absolute differences while Euclidean (L2 norm) uses squared differences with square root.
        level3: |-
          def euclidean_distance(a, b):
              return np.sqrt(np.sum((a - b) ** 2))

          def manhattan_distance(a, b):
              return np.sum(np.abs(a - b))

          def compute_distances(X_train, x_test, metric='euclidean'):
              distances = []
              for x_train in X_train:
                  if metric == 'euclidean':
                      d = euclidean_distance(x_train, x_test)
                  else:
                      d = manhattan_distance(x_train, x_test)
                  distances.append(d)
              return np.array(distances)
      pitfalls:
      - Negative under square root
      - Feature scaling differences
      - Integer vs float types
      concepts:
      - Distance metrics
      - Vector operations
      - NumPy arrays
      estimated_hours: 1-2
      deliverables:
      - Euclidean distance function that computes the L2 norm between two feature vectors
      - Manhattan distance function that computes the L1 norm (sum of absolute differences) between vectors
      - Cosine similarity function that measures the angular similarity between two feature vectors
      - Distance matrix builder that precomputes pairwise distances between all training samples
    - id: 2
      name: K-Nearest Neighbors Classification
      description: Implement the full KNN classifier.
      acceptance_criteria:
      - Find-k-nearest-neighbors returns exactly K neighbors sorted by ascending distance
      - Majority voting assigns the class that appears most frequently among the K neighbors
      - Prediction on new unseen data points returns the correct class label from the trained model
      - Classification accuracy is calculated as the ratio of correct predictions to total predictions
      hints:
        level1: Sort by distance, take first k, count labels.
        level2: Use Counter for majority voting.
        level3: |-
          from collections import Counter

          class KNN:
              def __init__(self, k=3):
                  self.k = k

              def fit(self, X, y):
                  self.X_train = X
                  self.y_train = y

              def predict_one(self, x):
                  distances = compute_distances(self.X_train, x)
                  k_indices = np.argsort(distances)[:self.k]
                  k_labels = self.y_train[k_indices]
                  most_common = Counter(k_labels).most_common(1)
                  return most_common[0][0]

              def predict(self, X):
                  return [self.predict_one(x) for x in X]

              def score(self, X, y):
                  predictions = self.predict(X)
                  return np.mean(predictions == y)
      pitfalls:
      - K larger than dataset
      - Ties in voting
      - Empty neighbors
      concepts:
      - Classification
      - Majority voting
      - Accuracy metric
      estimated_hours: 2-3
      deliverables:
      - K selection parameter that allows configuring the number of neighbors to consider
      - Neighbor finder that returns the K training samples closest to the query point
      - Majority voting classifier that assigns the most common class among K neighbors
      - Weighted voting classifier that weighs each neighbor's vote by the inverse of its distance
    - id: 3
      name: Improvements & Evaluation
      description: Add improvements and proper evaluation.
      acceptance_criteria:
      - Weighted voting by inverse distance gives closer neighbors proportionally more influence
      - K-fold cross-validation partitions data into K folds and averages metrics across all folds
      - Confusion matrix correctly shows true positives, false positives, true negatives, and false negatives per class
      - Optimal K is found by evaluating multiple values and selecting the one with highest cross-validated accuracy
      hints:
        level1: Weight by 1/distance for closer neighbors.
        level2: Use k-fold cross-validation to find best k.
        level3: |-
          def weighted_predict(self, x):
              distances = compute_distances(self.X_train, x)
              k_indices = np.argsort(distances)[:self.k]
              k_distances = distances[k_indices]
              k_labels = self.y_train[k_indices]
              # Weighted voting
              weights = 1 / (k_distances + 1e-8)  # Avoid div by zero
              label_weights = {}
              for label, weight in zip(k_labels, weights):
                  label_weights[label] = label_weights.get(label, 0) + weight
              return max(label_weights, key=label_weights.get)

          def find_best_k(X, y, k_range=range(1, 21)):
              best_k, best_score = 1, 0
              for k in k_range:
                  knn = KNN(k=k)
                  score = cross_val_score(knn, X, y)  # Implement k-fold CV
                  if score > best_score:
                      best_k, best_score = k, score
              return best_k
      pitfalls:
      - Overfitting with small k
      - Underfitting with large k
      - Computational cost
      concepts:
      - Weighted voting
      - Cross-validation
      - Hyperparameter tuning
      estimated_hours: 2-3
      deliverables:
      - Cross-validation module that evaluates model performance using K-fold data splitting
      - K optimization routine that tests multiple K values and selects the best by validation accuracy
      - KD-tree spatial index that accelerates neighbor lookup from O(n) to O(log n) per query
      - Accuracy metrics module that computes precision, recall, F1-score, and confusion matrix
  leader-election:
    id: leader-election
    name: Leader Election
    description: Implement leader election algorithms. Learn distributed coordination and failure handling.
    difficulty: intermediate
    estimated_hours: 12-20
    prerequisites:
    - Distributed systems basics
    - Network programming
    - Failure detection
    languages:
      recommended:
      - Go
      - Python
      - Java
      also_possible:
      - Rust
      - Erlang
    resources:
    - name: Bully Algorithm
      url: https://en.wikipedia.org/wiki/Bully_algorithm
      type: article
    - name: Ring Election
      url: https://www.cs.colostate.edu/~cs551/CourseNotes/Synchronization/LeijdElect.html
      type: article
    milestones:
    - id: 1
      name: Node Communication
      description: Set up inter-node messaging.
      acceptance_criteria:
      - Each node has a unique numeric ID that is used for comparison during election algorithms
      - Point-to-point messaging delivers a message reliably from one specific node to another
      - Broadcast messaging delivers a message to all currently known live nodes in the cluster
      - Node failure is detected within a configurable timeout and triggers re-election of the leader
      hints:
        level1: Each node has unique ID and knows other nodes' addresses.
        level2: Use TCP or UDP for messaging. Handle connection failures.
        level3: "import socket\nimport threading\nimport json\nfrom typing import Dict, Callable\n\nclass Node:\n    def __init__(self,\
          \ node_id: int, port: int, peers: Dict[int, tuple]):\n        self.node_id = node_id\n        self.port = port\n\
          \        self.peers = peers  # {node_id: (host, port)}\n        self.leader_id = None\n        self.handlers: Dict[str,\
          \ Callable] = {}\n        self.running = True\n    \n    def send_to(self, target_id: int, msg_type: str, data:\
          \ dict) -> bool:\n        if target_id not in self.peers:\n            return False\n        \n        host, port\
          \ = self.peers[target_id]\n        message = json.dumps({'type': msg_type, 'from': self.node_id, **data})\n    \
          \    \n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.settimeout(2.0)\n\
          \            sock.connect((host, port))\n            sock.sendall(message.encode())\n            sock.close()\n\
          \            return True\n        except:\n            return False  # Node unreachable\n    \n    def broadcast(self,\
          \ msg_type: str, data: dict):\n        for peer_id in self.peers:\n            self.send_to(peer_id, msg_type, data)\n\
          \    \n    def start_server(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.setsockopt(socket.SOL_SOCKET,\
          \ socket.SO_REUSEADDR, 1)\n        sock.bind(('0.0.0.0', self.port))\n        sock.listen(10)\n        \n      \
          \  while self.running:\n            conn, addr = sock.accept()\n            data = conn.recv(4096).decode()\n  \
          \          msg = json.loads(data)\n            \n            if msg['type'] in self.handlers:\n                self.handlers[msg['type']](msg)\n\
          \            conn.close()"
      pitfalls:
      - Message ordering
      - Partial failures
      - Network partitions
      concepts:
      - Node identity
      - Message passing
      - Failure detection
      estimated_hours: 3-4
      deliverables:
      - Node discovery mechanism that detects peer nodes via configuration or multicast announcement
      - Message passing layer that sends and receives typed messages between nodes over TCP or UDP
      - Network partition handler that detects and responds to unreachable peers with split-brain avoidance
      - Node failure detector that identifies unresponsive nodes using heartbeat timeouts and suspicion levels
    - id: 2
      name: Bully Algorithm
      description: Implement the bully election algorithm.
      acceptance_criteria:
      - The node with the highest ID among all responding nodes wins the election every time
      - ELECTION messages are sent only to nodes with IDs higher than the initiating node
      - An OK response from any higher-ID node causes the lower-ID initiator to stop its election
      - COORDINATOR announcement is broadcast to all nodes after the winner is determined
      hints:
        level1: 'On timeout/failure: send ELECTION to higher IDs. If no OK, become leader.'
        level2: When receiving ELECTION from lower ID, send OK and start own election.
        level3: "class BullyElection(Node):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\
          \        self.election_in_progress = False\n        self.handlers['ELECTION'] = self.on_election\n        self.handlers['OK']\
          \ = self.on_ok\n        self.handlers['COORDINATOR'] = self.on_coordinator\n    \n    def start_election(self):\n\
          \        self.election_in_progress = True\n        higher_nodes = [nid for nid in self.peers if nid > self.node_id]\n\
          \        \n        if not higher_nodes:\n            # I'm the highest, become leader\n            self.become_leader()\n\
          \            return\n        \n        # Send ELECTION to all higher nodes\n        got_ok = False\n        for\
          \ nid in higher_nodes:\n            if self.send_to(nid, 'ELECTION', {}):\n                got_ok = True\n     \
          \   \n        if not got_ok:\n            # No higher node responded, become leader\n            self.become_leader()\n\
          \        else:\n            # Wait for COORDINATOR or timeout\n            threading.Timer(5.0, self.check_election_timeout).start()\n\
          \    \n    def on_election(self, msg):\n        sender = msg['from']\n        if sender < self.node_id:\n      \
          \      # Send OK to stop their election\n            self.send_to(sender, 'OK', {})\n            # Start my own\
          \ election\n            self.start_election()\n    \n    def on_ok(self, msg):\n        # Someone higher is alive,\
          \ wait for coordinator\n        pass\n    \n    def on_coordinator(self, msg):\n        self.leader_id = msg['leader']\n\
          \        self.election_in_progress = False\n        print(f'Node {self.node_id}: New leader is {self.leader_id}')\n\
          \    \n    def become_leader(self):\n        self.leader_id = self.node_id\n        self.election_in_progress =\
          \ False\n        self.broadcast('COORDINATOR', {'leader': self.node_id})\n        print(f'Node {self.node_id}: I\
          \ am the leader!')"
      pitfalls:
      - Split brain
      - Message loss
      - Concurrent elections
      concepts:
      - Bully algorithm
      - Leader election
      - Distributed coordination
      estimated_hours: 4-5
      deliverables:
      - Election initiation logic that starts a new election when the current leader is detected as failed
      - Higher-ID responder that sends OK messages to lower-ID nodes attempting to claim leadership
      - Coordinator announcement broadcaster that notifies all nodes of the newly elected leader
      - Election timeout handler that declares victory when no higher-ID node responds within the deadline
    - id: 3
      name: Ring Election
      description: Implement ring-based election algorithm.
      acceptance_criteria:
      - Nodes are arranged in a logical ring where each node knows its successor in ID order
      - Election messages are forwarded around the ring collecting all live node IDs along the way
      - All live node IDs are collected in the election message as it traverses the full ring
      - The node with the highest ID in the collected set becomes the new leader
      hints:
        level1: Pass ELECTION with list of IDs. When it returns, highest ID wins.
        level2: Each node adds its ID and forwards. When message returns to initiator, broadcast result.
        level3: "class RingElection(Node):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\
          \        self.ring = sorted(self.peers.keys())  # Ring order\n        self.handlers['ELECTION'] = self.on_election\n\
          \        self.handlers['ELECTED'] = self.on_elected\n    \n    def next_node(self):\n        '''Get next node in\
          \ ring'''\n        idx = self.ring.index(self.node_id)\n        for i in range(1, len(self.ring)):\n           \
          \ next_idx = (idx + i) % len(self.ring)\n            next_id = self.ring[next_idx]\n            if self.send_to(next_id,\
          \ 'PING', {}):\n                return next_id\n        return None  # All nodes dead\n    \n    def start_election(self):\n\
          \        # Start election message with my ID\n        self.forward_election([self.node_id], self.node_id)\n    \n\
          \    def forward_election(self, participants: list, initiator: int):\n        next_id = self.next_node()\n     \
          \   if next_id:\n            self.send_to(next_id, 'ELECTION', {\n                'participants': participants,\n\
          \                'initiator': initiator\n            })\n    \n    def on_election(self, msg):\n        participants\
          \ = msg['participants']\n        initiator = msg['initiator']\n        \n        if self.node_id in participants:\n\
          \            # Message completed the ring\n            if initiator == self.node_id:\n                # I started\
          \ this, announce winner\n                winner = max(participants)\n                self.broadcast('ELECTED', {'leader':\
          \ winner})\n                self.leader_id = winner\n        else:\n            # Add myself and forward\n     \
          \       participants.append(self.node_id)\n            self.forward_election(participants, initiator)\n    \n  \
          \  def on_elected(self, msg):\n        self.leader_id = msg['leader']\n        print(f'Node {self.node_id}: Leader\
          \ is {self.leader_id}')"
      pitfalls:
      - Ring breaks
      - Multiple elections
      - Node rejoins
      concepts:
      - Ring topology
      - Token passing
      - Distributed election
      estimated_hours: 4-5
      deliverables:
      - Ring topology manager that arranges nodes in a logical ring ordered by their IDs
      - Election message forwarder that passes election tokens along the ring to the next live node
      - Coordinator selector that picks the highest-ID node from the collected set of live node IDs
      - Ring repair mechanism that skips failed nodes and reconnects the ring to maintain message flow
  linear-regression:
    id: linear-regression
    name: Linear Regression
    description: Implement linear regression with gradient descent from scratch. Understand the fundamentals of machine learning
      optimization.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - Basic Python/NumPy
    - High school math (derivatives)
    languages:
      recommended:
      - Python
      also_possible:
      - JavaScript
      - Julia
    resources:
    - name: Linear Regression Tutorial
      url: https://scikit-learn.org/stable/modules/linear_model.html
      type: documentation
    - name: Gradient Descent Explained
      url: https://www.youtube.com/watch?v=sDv4f4s2SB8
      type: video
    milestones:
    - id: 1
      name: Simple Linear Regression
      description: Implement single-variable linear regression.
      acceptance_criteria:
      - Model fits the equation y = mx + b using the ordinary least squares closed-form formula
      - Closed-form solution computes slope and intercept directly from the normal equation
      - Predictions for new input values are calculated correctly using the fitted slope and intercept
      - R-squared score is computed and falls between 0 and 1 for well-specified models
      hints:
        level1: 'Closed-form: m = cov(x,y)/var(x), b = mean(y) - m*mean(x)'
        level2: Use numpy for vectorized operations.
        level3: |-
          def fit(X, y):
              x_mean, y_mean = np.mean(X), np.mean(y)
              numerator = np.sum((X - x_mean) * (y - y_mean))
              denominator = np.sum((X - x_mean) ** 2)
              m = numerator / denominator
              b = y_mean - m * x_mean
              return m, b

          def r2_score(y_true, y_pred):
              ss_res = np.sum((y_true - y_pred) ** 2)
              ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
              return 1 - (ss_res / ss_tot)
      pitfalls:
      - Division by zero with constant X
      - Data not being proper arrays
      - Using float instead of numpy
      concepts:
      - Linear relationship
      - Least squares
      - Model evaluation
      estimated_hours: 2-3
      deliverables:
      - Data loading module that reads feature and target columns from CSV or array input
      - Closed-form solution implementing the ordinary least squares formula for slope and intercept
      - Prediction function that computes y-hat given new x values using the fitted parameters
      - R-squared calculator that measures the proportion of variance explained by the model
    - id: 2
      name: Gradient Descent
      description: Implement gradient descent optimization.
      acceptance_criteria:
      - Cost function computes mean squared error as the average of squared prediction residuals
      - Gradient calculation returns the correct partial derivatives for slope and intercept parameters
      - Iterative parameter updates reduce the cost function monotonically with an appropriate learning rate
      - Convergence is detected when the change in cost between consecutive iterations falls below epsilon
      hints:
        level1: Cost = (1/n) * sum((y_pred - y)Â²)
        level2: 'Update: param = param - learning_rate * gradient'
        level3: |-
          def gradient_descent(X, y, learning_rate=0.01, iterations=1000):
              m, b = 0, 0
              n = len(X)
              for _ in range(iterations):
                  y_pred = m * X + b
                  dm = -(2/n) * np.sum(X * (y - y_pred))
                  db = -(2/n) * np.sum(y - y_pred)
                  m -= learning_rate * dm
                  b -= learning_rate * db
              return m, b
      pitfalls:
      - Learning rate too high (divergence)
      - Not normalizing features
      - Stopping too early
      concepts:
      - Gradient descent
      - Learning rate
      - Cost function
      estimated_hours: 2-3
      deliverables:
      - Cost function implementing mean squared error over the entire training dataset
      - Gradient computation module that calculates partial derivatives of cost with respect to each parameter
      - Parameter update rule that subtracts the learning rate times gradient from current parameters
      - Convergence checker that stops training when cost improvement falls below a threshold
    - id: 3
      name: Multiple Linear Regression
      description: Extend to multiple input features.
      acceptance_criteria:
      - Model handles multiple input features by fitting a weight vector instead of a single slope
      - Matrix form y = Xw is used to express predictions as a matrix-vector product
      - Vectorized gradient descent updates all weights in a single matrix operation per iteration
      - Feature normalization scales each feature to zero mean and unit variance before fitting
      hints:
        level1: Add bias term as column of 1s in X matrix.
        level2: 'Gradient: âˆ‡J = (1/n) * X^T * (Xw - y)'
        level3: |-
          def multi_gradient_descent(X, y, lr=0.01, iters=1000):
              X = np.c_[np.ones(len(X)), X]  # Add bias column
              w = np.zeros(X.shape[1])
              n = len(y)
              for _ in range(iters):
                  y_pred = X @ w
                  gradient = (1/n) * (X.T @ (y_pred - y))
                  w -= lr * gradient
              return w
      pitfalls:
      - Features on different scales
      - Matrix dimension mismatches
      - Forgetting bias term
      concepts:
      - Matrix operations
      - Feature scaling
      - Vectorization
      estimated_hours: 2-3
      deliverables:
      - Feature matrix constructor that builds the design matrix X with an intercept column of ones
      - Multi-variable gradient descent that updates all weight parameters simultaneously each iteration
      - Feature normalization using z-score standardization to ensure all features have comparable scales
      - L2 regularization (Ridge) option that adds a penalty term to the cost function to prevent overfitting
  lisp-interp:
    id: lisp-interp
    name: Lisp Interpreter
    description: Build an interpreter for a minimal Lisp. Learn S-expressions, environments, and functional programming concepts.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Recursion
    - Basic parsing
    - Functional programming concepts
    languages:
      recommended:
      - Python
      - JavaScript
      - Ruby
      also_possible:
      - Go
      - Rust
      - C
    resources:
    - name: Make a Lisp
      url: https://github.com/kanaka/mal
      type: tutorial
    - name: SICP
      url: https://mitpress.mit.edu/sites/default/files/sicp/index.html
      type: book
    milestones:
    - id: 1
      name: S-Expression Parser
      description: Parse Lisp S-expressions into data structures.
      acceptance_criteria:
      - Atoms including numbers, symbols, and string literals are parsed into their correct types
      - Nested parenthesized lists are parsed into nested list data structures at arbitrary depth
      - Whitespace and semicolon-initiated line comments are correctly ignored during tokenization
      - Parser returns native data structures (numbers as numbers, symbols as strings, lists as arrays)
      hints:
        level1: 'Tokenize first: (, ), atoms. Then parse recursively.'
        level2: ( starts list, ) ends it, atoms are leaf nodes.
        level3: |-
          def tokenize(code):
              tokens = []
              i = 0
              while i < len(code):
                  if code[i] in '()':
                      tokens.append(code[i])
                      i += 1
                  elif code[i].isspace():
                      i += 1
                  elif code[i] == ';':  # Comment
                      while i < len(code) and code[i] != '\n':
                          i += 1
                  else:
                      j = i
                      while j < len(code) and code[j] not in '() \t\n;':
                          j += 1
                      tokens.append(code[i:j])
                      i = j
              return tokens

          def parse(tokens):
              token = tokens.pop(0)
              if token == '(':
                  lst = []
                  while tokens[0] != ')':
                      lst.append(parse(tokens))
                  tokens.pop(0)  # Remove ')'
                  return lst
              elif token == ')':
                  raise SyntaxError('Unexpected )')
              else:
                  return parse_atom(token)

          def parse_atom(token):
              try: return int(token)
              except: pass
              try: return float(token)
              except: pass
              return token  # Symbol
      pitfalls:
      - Unbalanced parentheses
      - Quote syntax
      - Negative numbers
      concepts:
      - S-expressions
      - Tokenization
      - Recursive parsing
      estimated_hours: 2-4
      deliverables:
      - Tokenizer that splits input into parentheses, atoms, string literals, and whitespace tokens
      - Recursive descent parser that builds nested list structures from the token stream
      - List constructor that creates the internal representation for parenthesized sequences of expressions
      - Quote handler that transforms 'expr shorthand into the equivalent (quote expr) list form
    - id: 2
      name: Basic Evaluation
      description: Evaluate arithmetic and conditionals.
      acceptance_criteria:
      - Number literals evaluate to themselves without any environment lookup or transformation
      - Arithmetic operators +, -, *, / return correct results for integers and floating-point operands
      - Comparison operators <, >, =, <=, >= return boolean results for numeric comparisons
      - Conditional if-expressions evaluate the consequent or alternative branch based on the test value
      hints:
        level1: 'Numbers self-evaluate. Lists: first element is operator.'
        level2: Environment maps symbols to values.
        level3: "def eval_expr(expr, env):\n    if isinstance(expr, (int, float)):\n        return expr\n    if isinstance(expr,\
          \ str):  # Symbol\n        if expr not in env:\n            raise NameError(f'Undefined: {expr}')\n        return\
          \ env[expr]\n    if isinstance(expr, list):\n        if len(expr) == 0:\n            raise SyntaxError('Empty list')\n\
          \        \n        op = expr[0]\n        \n        # Special forms\n        if op == 'if':\n            _, test,\
          \ then, else_ = expr\n            return eval_expr(then if eval_expr(test, env) else else_, env)\n        \n   \
          \     # Function call\n        func = eval_expr(op, env)\n        args = [eval_expr(arg, env) for arg in expr[1:]]\n\
          \        return func(*args)\n\n# Built-in environment\nimport operator\nglobal_env = {\n    '+': operator.add,\n\
          \    '-': operator.sub,\n    '*': operator.mul,\n    '/': operator.truediv,\n    '<': operator.lt,\n    '>': operator.gt,\n\
          \    '=': operator.eq,\n}"
      pitfalls:
      - Special forms vs functions
      - Short-circuit evaluation
      - Environment lookup
      concepts:
      - Evaluation rules
      - Environments
      - Special forms
      estimated_hours: 3-4
      deliverables:
      - Number evaluator that returns numeric atoms as their literal value without transformation
      - Arithmetic primitives implementing addition, subtraction, multiplication, and division operators
      - Comparison primitives implementing less-than, greater-than, equal, and their combinations
      - Boolean primitives implementing logical and, or, and not operations on truth values
    - id: 3
      name: Variables and Functions
      description: Add define, lambda, and lexical scope.
      acceptance_criteria:
      - Define form binds a variable name to the evaluated value in the current environment
      - Lambda form creates a first-class function value that captures its lexical environment
      - Lexical scoping ensures closures access variables from their definition scope, not the call site
      - Let form creates local bindings that are visible only within the let body expressions
      hints:
        level1: Lambda captures its defining environment.
        level2: Closure = (params, body, env).
        level3: "class Closure:\n    def __init__(self, params, body, env):\n        self.params = params\n        self.body\
          \ = body\n        self.env = env\n    \n    def __call__(self, *args):\n        # Create new env with params bound\
          \ to args\n        local_env = dict(self.env)\n        local_env.update(zip(self.params, args))\n        return\
          \ eval_expr(self.body, local_env)\n\ndef eval_expr(expr, env):\n    # ... previous code ...\n    \n    if op ==\
          \ 'define':\n        _, name, value = expr\n        env[name] = eval_expr(value, env)\n        return None\n   \
          \ \n    if op == 'lambda':\n        _, params, body = expr\n        return Closure(params, body, env.copy())\n \
          \   \n    if op == 'let':\n        # (let ((x 1) (y 2)) body)\n        _, bindings, body = expr\n        local_env\
          \ = dict(env)\n        for name, value in bindings:\n            local_env[name] = eval_expr(value, env)\n     \
          \   return eval_expr(body, local_env)"
      pitfalls:
      - Mutation vs shadowing
      - Closure capturing
      - Let vs let*
      concepts:
      - Closures
      - Lexical scope
      - Variable binding
      estimated_hours: 4-6
      deliverables:
      - Environment data structure mapping symbol names to their bound values with parent scope chain
      - Define special form that binds a name to a value in the current environment scope
      - Lambda special form that creates a closure capturing the defining environment and parameter list
      - Function application logic that evaluates arguments and calls the function with a new scope
    - id: 4
      name: List Operations & Recursion
      description: Add list primitives and support recursion.
      acceptance_criteria:
      - Cons creates a pair, car returns the first element, and cdr returns the rest of the list
      - List constructor builds a proper nil-terminated list from its arguments in order
      - Null? predicate returns true for the empty list and false for all other values
      - Recursive functions execute correctly without stack overflow for reasonable recursion depths
      hints:
        level1: cons builds pairs, car/cdr destructure them.
        level2: For recursion, function must be in scope when called.
        level3: |-
          # List operations
          global_env.update({
              'cons': lambda a, b: [a] + (b if isinstance(b, list) else [b]),
              'car': lambda lst: lst[0],
              'cdr': lambda lst: lst[1:],
              'list': lambda *args: list(args),
              'null?': lambda lst: lst == [],
              'length': len,
          })

          # Example: factorial
          # (define fact (lambda (n) (if (= n 0) 1 (* n (fact (- n 1))))))

          # Example: map
          # (define map (lambda (f lst)
          #   (if (null? lst)
          #       (list)
          #       (cons (f (car lst)) (map f (cdr lst))))))
      pitfalls:
      - Proper vs improper lists
      - Stack overflow on deep recursion
      - Empty list handling
      concepts:
      - List processing
      - Recursion
      - Higher-order functions
      estimated_hours: 3-4
      deliverables:
      - Car, cdr, and cons primitives that access the head, tail, and construct new pairs respectively
      - List construction function that builds a proper list from a variable number of arguments
      - Recursive function support allowing functions to call themselves by name within their body
      - Tail call optimization that reuses the current stack frame for calls in tail position
  load-balancer-basic:
    id: load-balancer-basic
    name: Load Balancer (Basic)
    description: Build a basic application load balancer with round-robin distribution. Learn about reverse proxying and server
      health management.
    difficulty: intermediate
    estimated_hours: 15-20
    prerequisites:
    - HTTP protocol
    - TCP networking
    - Concurrency basics
    languages:
      recommended:
      - Go
      - Python
      - JavaScript
      also_possible:
      - Rust
      - Java
    resources:
    - name: Build Your Own Load Balancer
      url: https://codingchallenges.fyi/challenges/challenge-load-balancer/
      type: tutorial
    - name: Load Balancer in Go
      url: https://kasvith.me/posts/lets-create-a-simple-lb-go/
      type: tutorial
    milestones:
    - id: 1
      name: HTTP Proxy Foundation
      description: Build basic HTTP reverse proxy functionality.
      acceptance_criteria:
      - Incoming HTTP requests on the proxy port are accepted and parsed with method, path, and headers
      - Requests are forwarded to a single configured backend server preserving method, path, and body
      - Backend responses including status code, headers, and body are returned to the originating client
      - Connection errors to the backend return a 502 Bad Gateway response to the client
      - All proxied requests are logged with timestamp, method, path, backend, and response status
      hints:
        level1: Read request from client, write to backend, read response, write to client.
        level2: Use HTTP library to handle request/response parsing.
        level3: |-
          from flask import Flask, request, Response
          import requests

          app = Flask(__name__)
          BACKEND = 'http://localhost:8001'

          @app.route('/<path:path>', methods=['GET', 'POST', 'PUT', 'DELETE'])
          def proxy(path):
              url = f'{BACKEND}/{path}'
              resp = requests.request(
                  method=request.method,
                  url=url,
                  headers={k: v for k, v in request.headers if k != 'Host'},
                  data=request.get_data(),
                  allow_redirects=False
              )
              return Response(resp.content, status=resp.status_code, headers=dict(resp.headers))
      pitfalls:
      - Not forwarding all headers
      - Not handling request body
      - Connection timeouts
      concepts:
      - Reverse proxy
      - HTTP forwarding
      - Request/response handling
      estimated_hours: 3-4
      deliverables:
      - Request forwarder that relays incoming HTTP requests to the configured backend server
      - Response forwarder that returns the backend server's response to the original client
      - Header manipulation module that adds X-Forwarded-For and X-Forwarded-Proto headers to proxied requests
      - Connection handler that manages keep-alive, timeouts, and connection pooling to backends
    - id: 2
      name: Round Robin Distribution
      description: Distribute requests across multiple backends.
      acceptance_criteria:
      - Backend server list is configurable via a configuration file or API with hot-reload support
      - Round-robin algorithm selects each backend in turn, cycling back to the first after the last
      - Requests are distributed evenly across all healthy backends with less than 5% skew over 1000 requests
      - Counter increment is thread-safe using atomic operations to prevent race conditions under concurrency
      - Backends marked as unhealthy are skipped and the next healthy backend is selected instead
      hints:
        level1: Keep counter, increment mod number of backends.
        level2: Use atomic operations for thread safety.
        level3: |-
          class RoundRobinBalancer:
              def __init__(self, backends):
                  self.backends = backends
                  self.current = 0
                  self.lock = threading.Lock()

              def get_next(self):
                  with self.lock:
                      backend = self.backends[self.current]
                      self.current = (self.current + 1) % len(self.backends)
                      return backend
      pitfalls:
      - Race condition in counter
      - Modulo with zero backends
      - Uneven distribution after backend changes
      concepts:
      - Round robin algorithm
      - Atomic operations
      - Backend pool
      estimated_hours: 2-3
      deliverables:
      - Backend server list manager that stores and validates the pool of available backend addresses
      - Round robin selector that cycles through backend servers in sequential order for each request
      - Request router that directs each incoming request to the backend chosen by the active algorithm
      - Backend cycling counter that wraps around to the first server after reaching the last one
    - id: 3
      name: Health Checks
      description: Implement active health checking of backends.
      acceptance_criteria:
      - Health checks run periodically at a configurable interval sending HTTP requests to each backend
      - Backends failing more than N consecutive health checks are marked unhealthy and removed from rotation
      - Unhealthy backends are excluded from the load balancing rotation until recovery is confirmed
      - Backends are restored to rotation after passing a configurable number of consecutive health checks
      - Health check interval, timeout, healthy threshold, and unhealthy threshold are all configurable
      hints:
        level1: Background thread pings each backend periodically.
        level2: Track consecutive failures, mark unhealthy after threshold.
        level3: |-
          class HealthChecker:
              def __init__(self, backends, threshold=3):
                  self.backends = backends
                  self.healthy = {b: True for b in backends}
                  self.failures = {b: 0 for b in backends}
                  self.threshold = threshold

              def check_backend(self, backend):
                  try:
                      resp = requests.get(f'{backend}/health', timeout=2)
                      if resp.status_code == 200:
                          self.failures[backend] = 0
                          self.healthy[backend] = True
                          return
                  except: pass
                  self.failures[backend] += 1
                  if self.failures[backend] >= self.threshold:
                      self.healthy[backend] = False
      pitfalls:
      - Health checks overwhelming backends
      - No healthy backends available
      - Thundering herd on recovery
      concepts:
      - Health checking
      - Failure detection
      - Graceful degradation
      estimated_hours: 3-4
      deliverables:
      - Health check endpoint prober that sends HTTP GET requests to each backend's health path
      - Periodic health checker that runs checks at a configurable interval on all registered backends
      - Unhealthy backend remover that takes backends out of rotation after consecutive check failures
      - Recovery detector that restores backends to the rotation after consecutive successful health checks
    - id: 4
      name: Additional Algorithms
      description: Implement additional load balancing algorithms.
      acceptance_criteria:
      - Weighted round robin distributes requests proportionally to backend weights over many requests
      - Least connections tracks active connections per backend and routes to the one with the fewest
      - IP hash routes the same client IP to the same backend consistently enabling sticky sessions
      - Random selection distributes requests approximately evenly across backends over many requests
      - The active load balancing algorithm is configurable and can be switched at runtime without restart
      hints:
        level1: 'Least connections: track active requests per backend.'
        level2: 'IP hash: hash(client_ip) % len(backends) for consistency.'
        level3: |-
          class LeastConnectionsBalancer:
              def __init__(self, backends):
                  self.backends = backends
                  self.connections = {b: 0 for b in backends}
                  self.lock = threading.Lock()

              def get_next(self):
                  with self.lock:
                      healthy = [b for b in self.backends if health_checker.healthy[b]]
                      return min(healthy, key=lambda b: self.connections[b])

          class IPHashBalancer:
              def get_next(self, client_ip):
                  healthy = [b for b in self.backends if health_checker.healthy[b]]
                  return healthy[hash(client_ip) % len(healthy)]
      pitfalls:
      - Least connections not updating on response
      - IP hash inconsistent after backend changes
      - Weighted round robin integer overflow
      concepts:
      - Load balancing algorithms
      - Session affinity
      - Algorithm tradeoffs
      estimated_hours: 4-5
      deliverables:
      - Least connections algorithm that routes each request to the backend with the fewest active connections
      - Weighted round robin algorithm that distributes requests proportionally to each backend's configured weight
      - IP hash algorithm that consistently maps a client IP address to the same backend server
      - Random selection algorithm that picks a backend uniformly at random for each request
  logging-structured:
    id: logging-structured
    name: Structured Logging System
    description: Implement production-grade structured logging. Learn log levels, formats, and log aggregation patterns.
    difficulty: intermediate
    estimated_hours: 10-15
    prerequisites:
    - JSON
    - File I/O
    - Basic debugging concepts
    languages:
      recommended:
      - Python
      - Go
      - Java
      also_possible:
      - JavaScript
      - Rust
    resources:
    - name: 12 Factor App - Logs
      url: https://12factor.net/logs
      type: article
    - name: Structured Logging Guide
      url: https://www.honeycomb.io/blog/structured-logging
      type: article
    milestones:
    - id: 1
      name: Logger Core
      description: Build the core logging infrastructure.
      acceptance_criteria:
      - Log levels DEBUG, INFO, WARN, ERROR, and FATAL filter messages below the configured minimum level
      - Minimum log level is configurable per logger and can be changed at runtime without restart
      - Logging operations are thread-safe and do not corrupt output when called from multiple threads simultaneously
      - Log records are dispatched to multiple output destinations such as stdout, file, and remote collector
      hints:
        level1: Logger accepts level + message + context. Filters by min level.
        level2: Use mutex/lock for thread safety. Handler pattern for outputs.
        level3: "import threading\nimport sys\nfrom enum import IntEnum\nfrom typing import Dict, Any, List, Callable\nfrom\
          \ datetime import datetime\n\nclass Level(IntEnum):\n    DEBUG = 10\n    INFO = 20\n    WARN = 30\n    ERROR = 40\n\
          \    FATAL = 50\n\nclass LogRecord:\n    def __init__(self, level: Level, message: str, **context):\n        self.timestamp\
          \ = datetime.utcnow()\n        self.level = level\n        self.message = message\n        self.context = context\n\
          \nclass Handler:\n    def emit(self, record: LogRecord):\n        raise NotImplementedError\n\nclass Logger:\n \
          \   def __init__(self, name: str = 'root', level: Level = Level.INFO):\n        self.name = name\n        self.level\
          \ = level\n        self.handlers: List[Handler] = []\n        self._lock = threading.Lock()\n    \n    def add_handler(self,\
          \ handler: Handler):\n        self.handlers.append(handler)\n    \n    def log(self, level: Level, message: str,\
          \ **context):\n        if level < self.level:\n            return\n        \n        record = LogRecord(level, message,\
          \ logger=self.name, **context)\n        \n        with self._lock:\n            for handler in self.handlers:\n\
          \                handler.emit(record)\n    \n    def debug(self, message: str, **ctx): self.log(Level.DEBUG, message,\
          \ **ctx)\n    def info(self, message: str, **ctx): self.log(Level.INFO, message, **ctx)\n    def warn(self, message:\
          \ str, **ctx): self.log(Level.WARN, message, **ctx)\n    def error(self, message: str, **ctx): self.log(Level.ERROR,\
          \ message, **ctx)"
      pitfalls:
      - Race conditions
      - Blocking I/O in hot path
      - Missing context
      concepts:
      - Log levels
      - Thread safety
      - Handler pattern
      estimated_hours: 3-4
      deliverables:
      - Log level manager supporting DEBUG, INFO, WARN, ERROR, and FATAL with numeric ordering
      - Logger hierarchy system that creates child loggers inheriting parent configuration and context
      - Log record factory that constructs structured records with timestamp, level, message, and metadata
      - Handler dispatch mechanism that routes log records to one or more configured output destinations
    - id: 2
      name: Structured Output
      description: Format logs as structured JSON.
      acceptance_criteria:
      - JSON output format produces valid single-line JSON with consistent field names for each log record
      - Each log record includes timestamp, level, message, logger name, and any attached context fields
      - Custom formatters can be registered and selected by name to control log output appearance
      - Pretty-print mode formats JSON with indentation and colors for readable developer console output
      hints:
        level1: JSON format allows log aggregation tools to parse logs.
        level2: 'Include: timestamp (ISO8601), level, message, all context fields.'
        level3: "import json\n\nclass JSONHandler(Handler):\n    def __init__(self, stream=sys.stdout):\n        self.stream\
          \ = stream\n    \n    def emit(self, record: LogRecord):\n        log_dict = {\n            'timestamp': record.timestamp.isoformat()\
          \ + 'Z',\n            'level': record.level.name,\n            'message': record.message,\n            **record.context\n\
          \        }\n        line = json.dumps(log_dict, default=str) + '\\n'\n        self.stream.write(line)\n        self.stream.flush()\n\
          \nclass PrettyHandler(Handler):\n    '''Human-readable format for development'''\n    COLORS = {\n        Level.DEBUG:\
          \ '\\033[36m',  # Cyan\n        Level.INFO: '\\033[32m',   # Green\n        Level.WARN: '\\033[33m',   # Yellow\n\
          \        Level.ERROR: '\\033[31m',  # Red\n    }\n    RESET = '\\033[0m'\n    \n    def emit(self, record: LogRecord):\n\
          \        color = self.COLORS.get(record.level, '')\n        ts = record.timestamp.strftime('%H:%M:%S.%f')[:-3]\n\
          \        \n        ctx_str = ''\n        if record.context:\n            ctx_parts = [f'{k}={v!r}' for k, v in record.context.items()\
          \ if k != 'logger']\n            if ctx_parts:\n                ctx_str = f' | {\" \".join(ctx_parts)}'\n      \
          \  \n        print(f'{ts} {color}{record.level.name:5}{self.RESET} {record.message}{ctx_str}')"
      pitfalls:
      - Non-serializable values
      - Missing flush
      - Large context objects
      concepts:
      - Structured logging
      - JSON format
      - Formatters
      estimated_hours: 2-3
      deliverables:
      - JSON formatter that serializes log records as single-line JSON objects with consistent field ordering
      - Key-value pair enricher that attaches arbitrary context fields to each log record
      - Timestamp formatter supporting ISO 8601, Unix epoch, and custom strftime patterns
      - Custom formatter plugin system that allows users to register their own output format functions
    - id: 3
      name: Context & Correlation
      description: Add request context and correlation IDs.
      acceptance_criteria:
      - A unique request ID is automatically injected into every log record within the same request scope
      - Context key-value pairs propagate through nested function calls without explicit parameter passing
      - Child loggers inherit all context fields from their parent and can add their own additional fields
      - Logging context is preserved correctly across async/await boundaries and coroutine switches
      hints:
        level1: 'Correlation ID: unique ID per request, included in all logs.'
        level2: Use thread-local or context variables for automatic injection.
        level3: "import contextvars\nimport uuid\n\n# Context variable for request context\nrequest_context: contextvars.ContextVar[Dict[str,\
          \ Any]] = contextvars.ContextVar('request_context', default={})\n\nclass ContextLogger(Logger):\n    def log(self,\
          \ level: Level, message: str, **context):\n        # Merge request context\n        ctx = {**request_context.get(),\
          \ **context}\n        super().log(level, message, **ctx)\n\ndef with_request_context(**ctx):\n    '''Decorator to\
          \ set request context'''\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            token = request_context.set({**request_context.get(),\
          \ **ctx})\n            try:\n                return func(*args, **kwargs)\n            finally:\n              \
          \  request_context.reset(token)\n        return wrapper\n    return decorator\n\n# Middleware example\nclass LoggingMiddleware:\n\
          \    def __call__(self, request, next_handler):\n        request_id = request.headers.get('X-Request-ID', str(uuid.uuid4()))\n\
          \        \n        token = request_context.set({\n            'request_id': request_id,\n            'method': request.method,\n\
          \            'path': request.path,\n        })\n        \n        try:\n            logger.info('Request started')\n\
          \            response = next_handler(request)\n            logger.info('Request completed', status=response.status)\n\
          \            return response\n        except Exception as e:\n            logger.error('Request failed', error=str(e))\n\
          \            raise\n        finally:\n            request_context.reset(token)"
      pitfalls:
      - Context not propagating
      - Memory leaks
      - Async context issues
      concepts:
      - Correlation IDs
      - Context propagation
      - Request tracing
      estimated_hours: 3-4
      deliverables:
      - Thread-local context storage that maintains key-value pairs scoped to the current execution context
      - Correlation ID propagator that injects and carries a unique request ID across function boundaries
      - Request context middleware that extracts request metadata and attaches it to the logging context
      - Async context bridge that preserves logging context when crossing async/await task boundaries
  markdown-renderer:
    id: markdown-renderer
    name: Markdown Renderer
    description: Build a Markdown to HTML converter. Learn text parsing, regular expressions, and document transformation.
    difficulty: beginner
    estimated_hours: 12-18
    prerequisites:
    - Regular expressions
    - String manipulation
    - HTML basics
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - Rust
      - Ruby
    resources:
    - name: CommonMark Spec
      url: https://spec.commonmark.org/
      type: specification
    - name: Building a Markdown Parser
      url: https://dev.to/kawaljain/building-my-own-markdown-parser-a-developers-journey-3b26
      type: tutorial
    - name: Sarvasv's MD Parser Notes
      url: https://sarvasvkulpati.com/mdparser
      type: tutorial
    milestones:
    - id: 1
      name: Block Elements
      description: 'Parse block-level elements: headings, paragraphs, code blocks, horizontal rules.'
      acceptance_criteria:
      - 'Headings from # through ###### are converted to the corresponding h1 through h6 HTML elements'
      - Consecutive lines separated by blank lines are wrapped in paragraph p elements
      - Fenced code blocks with triple backticks and indented code blocks both render as pre and code elements
      - Horizontal rules from three or more dashes or asterisks are rendered as hr elements
      hints:
        level1: Process line by line. Blank lines separate blocks. Accumulate paragraph lines.
        level2: 'Headings: /^(#{1,6})\s+(.+)$/. Code blocks: track if inside ``` fence.'
        level3: |-
          def parse_blocks(text: str) -> list[dict]:
              '''Parse text into block elements'''
              lines = text.split('\n')
              blocks = []
              current_para = []
              in_code = False
              code_lines = []

              def flush_para():
                  if current_para:
                      blocks.append({'type': 'paragraph', 'content': ' '.join(current_para)})
                      current_para.clear()

              for line in lines:
                  # Code fence
                  if line.startswith('```'):
                      if in_code:
                          blocks.append({'type': 'code', 'content': '\n'.join(code_lines)})
                          code_lines.clear()
                          in_code = False
                      else:
                          flush_para()
                          in_code = True
                      continue

                  if in_code:
                      code_lines.append(line)
                      continue

                  # Heading
                  match = re.match(r'^(#{1,6})\s+(.+)$', line)
                  if match:
                      flush_para()
                      level = len(match.group(1))
                      blocks.append({'type': f'h{level}', 'content': match.group(2)})
                      continue

                  # Horizontal rule
                  if re.match(r'^([-*_])\1{2,}\s*$', line):
                      flush_para()
                      blocks.append({'type': 'hr'})
                      continue

                  # Blank line
                  if not line.strip():
                      flush_para()
                      continue

                  # Paragraph continuation
                  current_para.append(line)

              flush_para()
              return blocks
      pitfalls:
      - Setext headings (underline style) need lookahead
      - Indented code vs list continuation is complex
      - Fenced code can have language identifier
      concepts:
      - Block parsing
      - State machines
      - Line-by-line processing
      estimated_hours: 4-5
      deliverables:
      - 'Heading parser that recognizes ATX-style hash prefixes from single # through ######'
      - Paragraph detector that groups consecutive non-blank lines into paragraph blocks
      - Fenced code block parser that extracts content between triple-backtick delimiters with optional language hints
      - Blockquote parser that handles greater-than prefixed lines including nested blockquotes
    - id: 2
      name: Inline Elements
      description: 'Parse inline formatting: bold, italic, code, links, images.'
      acceptance_criteria:
      - Double asterisks and double underscores around text produce strong (bold) HTML elements
      - Single asterisks and single underscores around text produce em (italic) HTML elements
      - Backtick-delimited inline code renders as code HTML elements preserving inner whitespace
      - Bracket-parenthesis link syntax [text](url) renders as anchor elements with href attributes
      - Exclamation-bracket-parenthesis image syntax ![alt](url) renders as img elements with src and alt attributes
      hints:
        level1: Process inline after block parsing. Use regex or character-by-character scan.
        level2: 'Handle nested: **bold _and italic_**. Process outer markers first.'
        level3: |-
          def parse_inline(text: str) -> str:
              '''Convert inline markdown to HTML'''
              # Process in order: code (literal), then links, then emphasis

              # Inline code (must be first - content is literal)
              text = re.sub(r'`([^`]+)`', r'<code>\1</code>', text)

              # Images (before links - similar syntax)
              text = re.sub(r'!\[([^\]]*)\]\(([^)]+)\)', r'<img src="\2" alt="\1">', text)

              # Links
              text = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2">\1</a>', text)

              # Bold (** or __)
              text = re.sub(r'\*\*([^*]+)\*\*', r'<strong>\1</strong>', text)
              text = re.sub(r'__([^_]+)__', r'<strong>\1</strong>', text)

              # Italic (* or _)
              text = re.sub(r'\*([^*]+)\*', r'<em>\1</em>', text)
              text = re.sub(r'_([^_]+)_', r'<em>\1</em>', text)

              return text

          # Better approach: tokenize then render
          def tokenize_inline(text: str) -> list:
              tokens = []
              i = 0
              while i < len(text):
                  if text[i:i+2] == '**':
                      # Find closing **
                      end = text.find('**', i+2)
                      if end != -1:
                          tokens.append(('strong', text[i+2:end]))
                          i = end + 2
                          continue
                  # ... handle other cases
                  tokens.append(('text', text[i]))
                  i += 1
              return tokens
      pitfalls:
      - Underscore in middle_of_words shouldn't trigger emphasis
      - 'Mismatched delimiters: **bold* is invalid'
      - 'Escaping: \* should be literal asterisk'
      concepts:
      - Inline parsing
      - Regular expressions
      - Nested formatting
      estimated_hours: 4-5
      deliverables:
      - Bold and italic parser that detects asterisk and underscore emphasis markers at inline level
      - Inline code parser that extracts text between single backtick delimiters
      - Link parser that recognizes bracket-parenthesis syntax for hyperlink text and URL
      - Image parser that recognizes exclamation-bracket-parenthesis syntax for image alt text and URL
    - id: 3
      name: Lists
      description: Parse ordered and unordered lists with nesting.
      acceptance_criteria:
      - Dash or asterisk prefixed lines render as unordered list ul with li items
      - Numeric-dot prefixed lines like 1. 2. 3. render as ordered list ol with li items
      - Indented list items nested under a parent item create properly nested ul or ol sub-lists
      - Tight lists without blank lines between items and loose lists with blank lines render differently per CommonMark spec
      hints:
        level1: Track indentation level. Deeper indent = nested list. Same indent = sibling.
        level2: Use a stack to track nested lists. Push on deeper indent, pop on shallower.
        level3: |-
          def parse_list(lines: list[str], start_idx: int) -> tuple[dict, int]:
              '''Parse list starting at start_idx, return (list_node, next_idx)'''
              first_line = lines[start_idx]

              # Determine list type
              if re.match(r'^\d+\.\s', first_line.lstrip()):
                  list_type = 'ol'
                  pattern = r'^(\s*)(\d+)\.\s+(.*)$'
              else:
                  list_type = 'ul'
                  pattern = r'^(\s*)([-*+])\s+(.*)$'

              base_indent = len(first_line) - len(first_line.lstrip())
              items = []
              idx = start_idx

              while idx < len(lines):
                  line = lines[idx]
                  if not line.strip():
                      idx += 1
                      continue

                  match = re.match(pattern, line)
                  if not match:
                      break

                  indent = len(match.group(1))
                  content = match.group(3)

                  if indent < base_indent:
                      break
                  elif indent > base_indent:
                      # Nested list - recursively parse
                      nested, idx = parse_list(lines, idx)
                      items[-1]['children'].append(nested)
                  else:
                      items.append({'content': content, 'children': []})
                      idx += 1

              return {'type': list_type, 'items': items}, idx
      pitfalls:
      - Mixed list types in same level is invalid
      - Continuation lines must be indented
      - Loose lists have paragraphs in items
      concepts:
      - Recursive parsing
      - Indentation tracking
      - Tree structures
      estimated_hours: 4-5
      deliverables:
      - Unordered list parser that recognizes dash and asterisk bullet markers at the start of lines
      - Ordered list parser that recognizes numeric-dot prefixes for sequentially numbered items
      - Nested list handler that detects indentation levels to create hierarchical list structures
      - List item content parser that handles multi-line items and inline formatting within list items
    - id: 4
      name: HTML Generation
      description: Convert parsed structure to valid HTML output.
      acceptance_criteria:
      - Generated output is valid HTML5 that passes W3C validation without structural errors
      - Special characters including ampersand, less-than, greater-than, and quotes are escaped to HTML entities
      - HTML elements are properly nested with correct opening and closing tags and indentation
      - Optional wrapper template adds doctype, head, and body elements around the rendered content
      hints:
        level1: Walk the parsed tree, emit HTML tags. Escape content but not generated tags.
        level2: Use html.escape() for content. Track indent level for pretty printing.
        level3: |-
          import html

          class HtmlRenderer:
              def __init__(self, pretty: bool = True):
                  self.pretty = pretty
                  self.indent = 0

              def render(self, blocks: list[dict]) -> str:
                  output = []
                  for block in blocks:
                      output.append(self.render_block(block))
                  return '\n'.join(output)

              def render_block(self, block: dict) -> str:
                  t = block['type']

                  if t == 'hr':
                      return self._line('<hr>')

                  if t.startswith('h'):
                      content = self.render_inline(block['content'])
                      return self._line(f'<{t}>{content}</{t}>')

                  if t == 'paragraph':
                      content = self.render_inline(block['content'])
                      return self._line(f'<p>{content}</p>')

                  if t == 'code':
                      escaped = html.escape(block['content'])
                      return self._line(f'<pre><code>{escaped}</code></pre>')

                  if t in ('ul', 'ol'):
                      return self.render_list(block)

                  return ''

              def render_inline(self, text: str) -> str:
                  # First escape HTML entities in the text
                  text = html.escape(text)
                  # Then apply inline markdown (careful not to escape our generated tags)
                  # ... apply inline patterns
                  return text

              def _line(self, content: str) -> str:
                  indent = '  ' * self.indent if self.pretty else ''
                  return f'{indent}{content}'
      pitfalls:
      - 'Double-escaping: escape content before inline parsing'
      - 'Self-closing tags: <hr> not <hr></hr>'
      - Inline HTML in markdown should pass through
      concepts:
      - HTML generation
      - Character escaping
      - Tree traversal
      estimated_hours: 3-4
      deliverables:
      - HTML tag generator that wraps parsed content in the appropriate semantic HTML elements
      - Special character escaper that converts ampersands, angle brackets, and quotes to HTML entities
      - Pretty printer that outputs indented HTML with consistent formatting for human readability
      - Custom renderer plugin interface that allows users to override the output for specific element types
  memory-pool:
    id: memory-pool
    name: Memory Pool Allocator
    description: Build a fixed-size block memory allocator. Learn memory management, fragmentation, and allocation strategies.
    difficulty: intermediate
    estimated_hours: 10-15
    prerequisites:
    - C pointers
    - Memory layout
    - Data structures
    languages:
      recommended:
      - C
      - Rust
      - Zig
      also_possible:
      - C++
    resources:
    - name: Memory Pool Design
      url: https://en.wikipedia.org/wiki/Memory_pool
      type: article
    milestones:
    - id: 1
      name: Fixed-Size Pool
      description: Allocate a pool of fixed-size blocks.
      acceptance_criteria:
      - Pool initializes with N blocks of size S bytes carved from a single contiguous memory allocation
      - Allocate returns a pointer to a free block or NULL when the pool is completely exhausted
      - Free returns the block to the pool making it available for subsequent allocation requests
      - Both allocate and free operations complete in O(1) constant time regardless of pool size
      hints:
        level1: 'Use free list: each free block points to next free block.'
        level2: Store next pointer in the free block itself (no extra memory).
        level3: |-
          typedef struct Pool {
              void *memory;
              void *free_list;
              size_t block_size;
              size_t count;
          } Pool;

          void pool_init(Pool *p, size_t block_size, size_t count) {
              p->block_size = block_size < sizeof(void*) ? sizeof(void*) : block_size;
              p->count = count;
              p->memory = malloc(p->block_size * count);
              p->free_list = p->memory;
              // Chain all blocks
              char *block = p->memory;
              for (size_t i = 0; i < count - 1; i++) {
                  *(void**)(block) = block + p->block_size;
                  block += p->block_size;
              }
              *(void**)block = NULL;
          }

          void *pool_alloc(Pool *p) {
              if (!p->free_list) return NULL;
              void *block = p->free_list;
              p->free_list = *(void**)block;
              return block;
          }

          void pool_free(Pool *p, void *block) {
              *(void**)block = p->free_list;
              p->free_list = block;
          }
      pitfalls:
      - Block size smaller than pointer
      - Double free
      - Use after free
      concepts:
      - Free list
      - Pointer aliasing
      - Memory layout
      estimated_hours: 3-4
      deliverables:
      - Pool initializer that pre-allocates a contiguous memory region divided into N equal-sized blocks
      - Block allocator that returns the next available block from the free list in constant time
      - Block deallocator that returns a freed block to the front of the free list in constant time
      - Free list manager that maintains a singly-linked list threading through the unused blocks
    - id: 2
      name: Pool Growing
      description: Allow pool to grow when exhausted.
      acceptance_criteria:
      - A new chunk is automatically allocated when the pool's free list is empty and an alloc is requested
      - Multiple chunks are chained together in a linked list for unified pool management and cleanup
      - All allocated chunks are tracked and freed during pool destruction to prevent memory leaks
      - Optional maximum pool size limit prevents unbounded growth and returns NULL when the limit is reached
      hints:
        level1: Keep linked list of chunks.
        level2: Each chunk contains blocks, chain new blocks to free list.
        level3: |-
          typedef struct Chunk {
              void *memory;
              struct Chunk *next;
          } Chunk;

          typedef struct GrowablePool {
              Chunk *chunks;
              void *free_list;
              size_t block_size;
              size_t chunk_blocks;
          } GrowablePool;

          void *gpool_alloc(GrowablePool *p) {
              if (!p->free_list) {
                  // Allocate new chunk
                  Chunk *chunk = malloc(sizeof(Chunk));
                  chunk->memory = malloc(p->block_size * p->chunk_blocks);
                  chunk->next = p->chunks;
                  p->chunks = chunk;
                  // Add blocks to free list
                  char *block = chunk->memory;
                  for (size_t i = 0; i < p->chunk_blocks; i++) {
                      *(void**)block = p->free_list;
                      p->free_list = block;
                      block += p->block_size;
                  }
              }
              void *block = p->free_list;
              p->free_list = *(void**)block;
              return block;
          }
      pitfalls:
      - Memory leak on destroy
      - Infinite growth
      - Fragmentation across chunks
      concepts:
      - Dynamic allocation
      - Chunk management
      - Resource limits
      estimated_hours: 2-3
      deliverables:
      - Automatic pool expander that allocates a new chunk of blocks when the free list is empty
      - Chunk allocator that creates additional memory regions with the same block size as the original pool
      - Memory tracker that maintains a list of all allocated chunks for cleanup during pool destruction
      - Pool statistics collector that reports total blocks, used blocks, free blocks, and chunk count
    - id: 3
      name: Thread Safety & Debugging
      description: Add thread safety and debugging features.
      acceptance_criteria:
      - Mutex or atomic CAS protects alloc and free operations from data corruption under concurrent access
      - Double free attempts are detected and reported with the block address and call site information
      - Memory poisoning fills freed blocks with a sentinel pattern to detect use-after-free access
      - Statistics tracking reports allocation count, free count, peak usage, and current utilization in real time
      hints:
        level1: Lock around free list operations.
        level2: Use magic number to detect double free.
        level3: |-
          #define MAGIC_FREE 0xDEADBEEF
          #define MAGIC_USED 0xBEEFCAFE

          typedef struct DebugBlock {
              uint32_t magic;
              char data[];
          } DebugBlock;

          void *pool_alloc_debug(Pool *p) {
              pthread_mutex_lock(&p->lock);
              DebugBlock *block = pool_alloc_internal(p);
              if (block) {
                  assert(block->magic == MAGIC_FREE);
                  block->magic = MAGIC_USED;
                  memset(block->data, 0xCC, p->block_size - sizeof(DebugBlock));
              }
              pthread_mutex_unlock(&p->lock);
              return block ? block->data : NULL;
          }

          void pool_free_debug(Pool *p, void *ptr) {
              DebugBlock *block = (DebugBlock*)((char*)ptr - offsetof(DebugBlock, data));
              pthread_mutex_lock(&p->lock);
              assert(block->magic == MAGIC_USED);  // Detect double free
              block->magic = MAGIC_FREE;
              memset(block->data, 0xDD, p->block_size - sizeof(DebugBlock));  // Poison
              pool_free_internal(p, block);
              pthread_mutex_unlock(&p->lock);
          }
      pitfalls:
      - Lock ordering deadlocks
      - Overhead of debugging
      - False positives in detection
      concepts:
      - Thread safety
      - Memory debugging
      - Defensive programming
      estimated_hours: 3-4
      deliverables:
      - Lock-free or mutex-protected allocation option that ensures thread-safe pool access under contention
      - Per-thread pool variant that gives each thread its own free list to eliminate cross-thread contention
      - Leak detector that reports allocated blocks not freed when the pool is destroyed
      - Double-free detector that identifies and reports attempts to free a block that is already free
  metrics-dashboard:
    id: metrics-dashboard
    name: Metrics & Alerting Dashboard
    description: Build a metrics collection and visualization system with alerting capabilities.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - HTTP APIs
    - Time-series data concepts
    - Basic statistics
    - Docker
    languages:
      recommended:
      - Go
      - Python
      also_possible:
      - JavaScript
      - Rust
    resources:
    - type: documentation
      name: Prometheus Documentation
      url: https://prometheus.io/docs/introduction/overview/
    - type: documentation
      name: Grafana Documentation
      url: https://grafana.com/docs/grafana/latest/
    - type: article
      name: Google SRE Book - Monitoring
      url: https://sre.google/sre-book/monitoring-distributed-systems/
    milestones:
    - id: 1
      name: Metrics Collection
      description: Implement metrics collection with counters, gauges, and histograms.
      acceptance_criteria:
      - Counter metrics are ingested and stored as monotonically increasing cumulative values
      - Gauge metrics are ingested and stored as point-in-time values that can increase or decrease
      - Histogram metrics bucket observations into configurable ranges and track count and sum
      - Labels and dimensions are parsed, validated, and indexed for multi-dimensional metric queries
      - Prometheus exposition format is supported for scraping metrics from compatible endpoints
      hints:
        level1: Start with counters (always increasing) and gauges (can go up/down).
        level2: 'Implement the Prometheus text format: metric_name{label="value"} value timestamp'
        level3: "## Metrics Types Implementation\n\n```go\npackage metrics\n\nimport (\n    \"fmt\"\n    \"sort\"\n    \"\
          strings\"\n    \"sync\"\n    \"time\"\n)\n\n// Counter - only increases\ntype Counter struct {\n    mu     sync.RWMutex\n\
          \    values map[string]float64  // key: serialized labels\n    labels []string\n}\n\nfunc NewCounter(labels ...string)\
          \ *Counter {\n    return &Counter{\n        values: make(map[string]float64),\n        labels: labels,\n    }\n\
          }\n\nfunc (c *Counter) Inc(labelValues ...string) {\n    c.Add(1, labelValues...)\n}\n\nfunc (c *Counter) Add(v\
          \ float64, labelValues ...string) {\n    key := strings.Join(labelValues, \"|\")\n    c.mu.Lock()\n    c.values[key]\
          \ += v\n    c.mu.Unlock()\n}\n\n// Gauge - can increase or decrease\ntype Gauge struct {\n    mu     sync.RWMutex\n\
          \    values map[string]float64\n    labels []string\n}\n\nfunc (g *Gauge) Set(v float64, labelValues ...string)\
          \ {\n    key := strings.Join(labelValues, \"|\")\n    g.mu.Lock()\n    g.values[key] = v\n    g.mu.Unlock()\n}\n\
          \n// Histogram - distribution of values\ntype Histogram struct {\n    mu      sync.RWMutex\n    buckets []float64\n\
          \    counts  map[string][]uint64  // per-label bucket counts\n    sums    map[string]float64\n    totals  map[string]uint64\n\
          }\n\nfunc NewHistogram(buckets []float64) *Histogram {\n    sort.Float64s(buckets)\n    return &Histogram{\n   \
          \     buckets: buckets,\n        counts:  make(map[string][]uint64),\n        sums:    make(map[string]float64),\n\
          \        totals:  make(map[string]uint64),\n    }\n}\n\nfunc (h *Histogram) Observe(v float64, labelValues ...string)\
          \ {\n    key := strings.Join(labelValues, \"|\")\n    h.mu.Lock()\n    defer h.mu.Unlock()\n    \n    if _, ok :=\
          \ h.counts[key]; !ok {\n        h.counts[key] = make([]uint64, len(h.buckets))\n    }\n    \n    for i, bucket :=\
          \ range h.buckets {\n        if v <= bucket {\n            h.counts[key][i]++\n        }\n    }\n    h.sums[key]\
          \ += v\n    h.totals[key]++\n}\n```\n\n```\n# Prometheus Exposition Format\nhttp_requests_total{method=\"GET\",status=\"\
          200\"} 1234\nhttp_requests_total{method=\"POST\",status=\"201\"} 567\nhttp_request_duration_seconds_bucket{le=\"\
          0.1\"} 500\nhttp_request_duration_seconds_bucket{le=\"0.5\"} 900\nhttp_request_duration_seconds_bucket{le=\"1.0\"\
          } 990\nhttp_request_duration_seconds_bucket{le=\"+Inf\"} 1000\nhttp_request_duration_seconds_sum 450.5\nhttp_request_duration_seconds_count\
          \ 1000\n```"
      pitfalls:
      - Not using labels effectively
      - Too many unique label values (cardinality explosion)
      - Forgetting thread safety
      concepts:
      - Time-series data
      - Metric types
      - Label cardinality
      estimated_hours: 6-10
      deliverables:
      - Metric ingestion API endpoint that accepts counter, gauge, and histogram data points via HTTP
      - Push and pull model support allowing metrics to be sent by clients or scraped from endpoints
      - Metric type handler that processes counter increments, gauge sets, and histogram observations differently
      - Label handler that parses and indexes label key-value pairs attached to each metric data point
    - id: 2
      name: Storage & Querying
      description: Implement time-series storage and basic query capabilities.
      acceptance_criteria:
      - Time-series storage persists millions of data points with efficient write throughput and compression
      - Data retention and compaction policies automatically delete or downsample data older than configured limits
      - Range queries return all data points within the specified start and end timestamps at the requested resolution
      - Aggregation functions correctly compute sum, average, rate, and percentile over grouped time series
      hints:
        level1: Store data points as (timestamp, value) pairs grouped by metric+labels.
        level2: Use a simple append-only log with periodic compaction.
        level3: "## Time-Series Storage\n\n```go\ntype DataPoint struct {\n    Timestamp int64\n    Value     float64\n}\n\
          \ntype TimeSeries struct {\n    mu     sync.RWMutex\n    points []DataPoint\n    maxAge time.Duration\n}\n\nfunc\
          \ (ts *TimeSeries) Append(t int64, v float64) {\n    ts.mu.Lock()\n    ts.points = append(ts.points, DataPoint{t,\
          \ v})\n    ts.mu.Unlock()\n}\n\nfunc (ts *TimeSeries) Range(start, end int64) []DataPoint {\n    ts.mu.RLock()\n\
          \    defer ts.mu.RUnlock()\n    \n    // Binary search for start\n    i := sort.Search(len(ts.points), func(i int)\
          \ bool {\n        return ts.points[i].Timestamp >= start\n    })\n    \n    result := []DataPoint{}\n    for ; i\
          \ < len(ts.points) && ts.points[i].Timestamp <= end; i++ {\n        result = append(result, ts.points[i])\n    }\n\
          \    return result\n}\n\n// Compaction - remove old data\nfunc (ts *TimeSeries) Compact() {\n    cutoff := time.Now().Add(-ts.maxAge).UnixMilli()\n\
          \    ts.mu.Lock()\n    defer ts.mu.Unlock()\n    \n    i := sort.Search(len(ts.points), func(i int) bool {\n   \
          \     return ts.points[i].Timestamp >= cutoff\n    })\n    ts.points = ts.points[i:]\n}\n```\n\n```go\n// Query\
          \ Language (simplified PromQL-like)\ntype Query struct {\n    MetricName string\n    Labels     map[string]string\n\
          \    Start      int64\n    End        int64\n    Step       int64  // resolution\n    Aggregation string // sum,\
          \ avg, max, min, rate\n}\n\nfunc (db *MetricsDB) Execute(q Query) []DataPoint {\n    series := db.findSeries(q.MetricName,\
          \ q.Labels)\n    points := series.Range(q.Start, q.End)\n    \n    switch q.Aggregation {\n    case \"rate\":\n\
          \        return calculateRate(points, q.Step)\n    case \"avg\":\n        return downsampleAvg(points, q.Step)\n\
          \    case \"sum\":\n        return downsampleSum(points, q.Step)\n    default:\n        return points\n    }\n}\n\
          \nfunc calculateRate(points []DataPoint, step int64) []DataPoint {\n    if len(points) < 2 {\n        return nil\n\
          \    }\n    \n    result := []DataPoint{}\n    for i := 1; i < len(points); i++ {\n        dt := float64(points[i].Timestamp\
          \ - points[i-1].Timestamp) / 1000\n        dv := points[i].Value - points[i-1].Value\n        result = append(result,\
          \ DataPoint{\n            Timestamp: points[i].Timestamp,\n            Value:     dv / dt,  // rate per second\n\
          \        })\n    }\n    return result\n}\n```"
      pitfalls:
      - Not implementing compaction
      - Inefficient range queries
      - Memory issues with large datasets
      concepts:
      - Time-series databases
      - Data compaction
      - Query optimization
      estimated_hours: 6-10
      deliverables:
      - Time-series storage engine that persists metric samples indexed by name, labels, and timestamp
      - Query language parser that supports metric selection, label filtering, and time range specification
      - Aggregation function library implementing sum, average, max, min, rate, and percentile calculations
      - Downsampling processor that reduces data resolution for older time ranges to save storage space
    - id: 3
      name: Visualization Dashboard
      description: Build a web dashboard for visualizing metrics.
      acceptance_criteria:
      - Line charts display time-series data with proper axis labels, legends, and tooltip details
      - Dashboard layout and panel configuration is saved and loaded from a persistent JSON schema
      - Auto-refresh updates all panels at the configured interval without requiring a manual page reload
      - Time range selector allows users to choose relative or absolute time windows for all dashboard panels
      hints:
        level1: Use Chart.js or similar for rendering. Fetch data via API.
        level2: Support multiple panels per dashboard, configurable via JSON.
        level3: "## Dashboard Implementation\n\n```html\n<!-- dashboard.html -->\n<div id=\"dashboard\">\n  <div class=\"\
          controls\">\n    <select id=\"timeRange\">\n      <option value=\"5m\">Last 5 minutes</option>\n      <option value=\"\
          1h\" selected>Last 1 hour</option>\n      <option value=\"24h\">Last 24 hours</option>\n    </select>\n    <button\
          \ onclick=\"refresh()\">Refresh</button>\n    <label>\n      <input type=\"checkbox\" id=\"autoRefresh\" checked>\n\
          \      Auto-refresh (30s)\n    </label>\n  </div>\n  <div id=\"panels\" class=\"panels-grid\"></div>\n</div>\n```\n\
          \n```javascript\n// Dashboard Configuration\nconst dashboardConfig = {\n  title: \"Application Metrics\",\n  refreshInterval:\
          \ 30000,\n  panels: [\n    {\n      title: \"Request Rate\",\n      query: \"rate(http_requests_total[5m])\",\n\
          \      type: \"line\",\n      yAxis: { label: \"req/s\" }\n    },\n    {\n      title: \"Error Rate\",\n      query:\
          \ \"rate(http_requests_total{status=~'5..'}[5m])\",\n      type: \"line\",\n      thresholds: [{ value: 0.01, color:\
          \ \"red\" }]\n    },\n    {\n      title: \"Response Time P99\",\n      query: \"histogram_quantile(0.99, http_request_duration_bucket)\"\
          ,\n      type: \"line\",\n      yAxis: { label: \"seconds\" }\n    }\n  ]\n};\n\nasync function renderPanel(panel,\
          \ container) {\n  const timeRange = document.getElementById('timeRange').value;\n  const { start, end } = parseTimeRange(timeRange);\n\
          \  \n  const data = await fetch(\n    `/api/query?q=${encodeURIComponent(panel.query)}&start=${start}&end=${end}`\n\
          \  ).then(r => r.json());\n  \n  const ctx = container.querySelector('canvas').getContext('2d');\n  new Chart(ctx,\
          \ {\n    type: 'line',\n    data: {\n      labels: data.map(p => new Date(p.timestamp)),\n      datasets: [{\n \
          \       label: panel.title,\n        data: data.map(p => p.value),\n        borderColor: '#3498db',\n        tension:\
          \ 0.1\n      }]\n    },\n    options: {\n      responsive: true,\n      scales: {\n        x: { type: 'time' },\n\
          \        y: { title: { text: panel.yAxis?.label || '' } }\n      }\n    }\n  });\n}\n```"
      pitfalls:
      - Not handling missing data points
      - Chart performance with many points
      - Time zone issues
      concepts:
      - Data visualization
      - Dashboard design
      - Real-time updates
      estimated_hours: 6-10
      deliverables:
      - Dashboard configuration system that defines panels, layout, and data source queries in a JSON schema
      - Chart renderer that draws line charts, bar charts, and heatmaps from time-series query results
      - Real-time update mechanism that refreshes chart data at a configurable auto-refresh interval
      - Dashboard sharing feature that generates shareable read-only links or embeddable iframe URLs
    - id: 4
      name: Alerting System
      description: Implement alerting rules with notifications.
      acceptance_criteria:
      - Alert rules define a metric query, comparison operator, threshold value, and evaluation interval
      - Threshold-based alerts fire when metric values exceed or drop below the configured threshold for a duration
      - Alert states transition correctly between pending, firing, and resolved with appropriate notifications at each stage
      - Notification channels deliver alert messages via the configured integration with customizable message templates
      hints:
        level1: Poll metrics at regular intervals, evaluate rules, track state changes.
        level2: Implement hysteresis (for duration) to avoid flapping alerts.
        level3: "## Alerting System\n\n```go\ntype AlertRule struct {\n    Name        string\n    Query       string\n  \
          \  Condition   string        // e.g., \"> 0.95\"\n    For         time.Duration // must be true for this long\n\
          \    Labels      map[string]string\n    Annotations map[string]string\n}\n\ntype AlertState int\nconst (\n    AlertInactive\
          \ AlertState = iota\n    AlertPending\n    AlertFiring\n)\n\ntype Alert struct {\n    Rule       *AlertRule\n  \
          \  State      AlertState\n    ActiveAt   time.Time\n    FiredAt    time.Time\n    ResolvedAt time.Time\n    Value\
          \      float64\n}\n\ntype AlertManager struct {\n    rules    []*AlertRule\n    alerts   map[string]*Alert  // rule\
          \ name -> alert\n    notifier Notifier\n}\n\nfunc (am *AlertManager) Evaluate(db *MetricsDB) {\n    for _, rule\
          \ := range am.rules {\n        value := db.QuerySingle(rule.Query)\n        triggered := evaluateCondition(value,\
          \ rule.Condition)\n        \n        alert, exists := am.alerts[rule.Name]\n        if !exists {\n            alert\
          \ = &Alert{Rule: rule, State: AlertInactive}\n            am.alerts[rule.Name] = alert\n        }\n        \n  \
          \      switch alert.State {\n        case AlertInactive:\n            if triggered {\n                alert.State\
          \ = AlertPending\n                alert.ActiveAt = time.Now()\n                alert.Value = value\n           \
          \ }\n            \n        case AlertPending:\n            if !triggered {\n                alert.State = AlertInactive\n\
          \            } else if time.Since(alert.ActiveAt) >= rule.For {\n                alert.State = AlertFiring\n   \
          \             alert.FiredAt = time.Now()\n                am.notifier.Send(alert)\n            }\n            \n\
          \        case AlertFiring:\n            if !triggered {\n                alert.State = AlertInactive\n         \
          \       alert.ResolvedAt = time.Now()\n                am.notifier.SendResolved(alert)\n            }\n        }\n\
          \    }\n}\n```\n\n```yaml\n# alert-rules.yaml\ngroups:\n  - name: application\n    rules:\n      - alert: HighErrorRate\n\
          \        query: rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m])\n        condition:\
          \ \"> 0.01\"\n        for: 5m\n        annotations:\n          summary: \"High error rate detected\"\n         \
          \ description: \"Error rate is {{ $value | printf \\\"%.2f\\\" }}%\"\n        \n      - alert: HighLatency\n   \
          \     query: histogram_quantile(0.99, http_request_duration_bucket)\n        condition: \"> 1.0\"\n        for:\
          \ 10m\n        annotations:\n          summary: \"P99 latency above 1 second\"\n```\n\n```go\n// Notification channels\n\
          type Notifier interface {\n    Send(alert *Alert) error\n    SendResolved(alert *Alert) error\n}\n\ntype SlackNotifier\
          \ struct {\n    webhookURL string\n}\n\nfunc (s *SlackNotifier) Send(alert *Alert) error {\n    payload := map[string]interface{}{\n\
          \        \"text\": fmt.Sprintf(\":rotating_light: ALERT: %s\\n%s\\nValue: %.4f\",\n            alert.Rule.Name,\n\
          \            alert.Rule.Annotations[\"description\"],\n            alert.Value),\n    }\n    body, _ := json.Marshal(payload)\n\
          \    _, err := http.Post(s.webhookURL, \"application/json\", bytes.NewReader(body))\n    return err\n}\n```"
      pitfalls:
      - Alert fatigue from too many alerts
      - Flapping alerts
      - Missing alert resolution notifications
      concepts:
      - Alert rules
      - State machines
      - Notification routing
      estimated_hours: 7-10
      deliverables:
      - Alert rule definition system that specifies metric conditions, thresholds, and evaluation intervals
      - Alert evaluator that periodically checks metric values against defined alert rule conditions
      - Notification channel integrations for email, Slack, PagerDuty, and webhook alert delivery
      - Alert silencing mechanism that suppresses notifications for specified alert rules during maintenance windows
  mini-shell:
    id: mini-shell
    name: Mini Shell
    description: Build a feature-rich shell with job control. Learn process groups, signals, and terminal handling.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - Process spawner
    - Signal handling
    - Unix process model
    languages:
      recommended:
      - C
      - Rust
      also_possible:
      - Go
      - Zig
    resources:
    - type: article
      name: Writing Your Own Shell
      url: https://brennan.io/2015/01/16/write-a-shell-in-c/
    - type: book
      name: Advanced Programming in Unix Environment - Ch 9
      url: https://www.apuebook.com/
    milestones:
    - id: 1
      name: Command Execution
      description: Parse and execute simple commands with arguments.
      acceptance_criteria:
      - Command line input is parsed into program name and argument tokens respecting whitespace boundaries
      - Quoted strings with single or double quotes are treated as single tokens preserving internal spaces
      - External commands are located via PATH search and executed in a forked child process
      - Built-in commands cd, exit, and pwd execute within the shell process without forking a child
      hints:
        level1: Use strtok or write custom tokenizer. Quote handling needs state machine.
        level2: Builtins must run in shell process, not forked. cd changes shell's cwd.
        level3: "char **parse_line(char *line) {\n    // Simple tokenizer (no quote handling)\n    char **tokens = malloc(64\
          \ * sizeof(char*));\n    int pos = 0;\n    char *token = strtok(line, \" \\t\\n\");\n    while (token) {\n     \
          \   tokens[pos++] = token;\n        token = strtok(NULL, \" \\t\\n\");\n    }\n    tokens[pos] = NULL;\n    return\
          \ tokens;\n}\n\nint execute_command(char **args) {\n    // Handle builtins\n    if (strcmp(args[0], \"cd\") == 0)\
          \ {\n        return chdir(args[1] ? args[1] : getenv(\"HOME\"));\n    }\n    if (strcmp(args[0], \"exit\") == 0)\
          \ exit(0);\n    \n    // External command\n    pid_t pid = fork();\n    if (pid == 0) {\n        execvp(args[0],\
          \ args);\n        perror(\"execvp\");\n        exit(1);\n    }\n    int status;\n    waitpid(pid, &status, 0);\n\
          \    return WEXITSTATUS(status);\n}"
      pitfalls:
      - Forgetting to null-terminate args
      - Not handling empty input
      - Memory leaks in tokenizer
      concepts:
      - Lexical analysis
      - fork/exec pattern
      - Process creation
      estimated_hours: 4-6
      deliverables:
      - Process launcher using fork and exec system calls to run external programs as child processes
      - PATH searcher that locates executable files by scanning directories listed in the PATH environment variable
      - Command-not-found error handler that prints a descriptive message when the executable cannot be located
      - Exit status capturer that stores the child process return code for use in conditionals and $? expansion
    - id: 2
      name: Pipes and Redirection
      description: Implement pipelines and I/O redirection.
      acceptance_criteria:
      - Input redirection with < reads the command's stdin from the specified file instead of the terminal
      - Output redirection with > writes the command's stdout to the specified file, creating or truncating it
      - Pipeline syntax cmd1 | cmd2 | cmd3 chains standard output to standard input across multiple commands
      - File creation respects umask and output redirection creates the file with standard permissions if it does not exist
      hints:
        level1: Parse redirection before executing. Use dup2 to redirect fds.
        level2: Pipes need careful fd management - close unused ends. Create all pipes before forking.
        level3: "void setup_pipeline(Command *cmds, int n) {\n    int pipes[n-1][2];  // n-1 pipes for n commands\n    \n\
          \    // Create all pipes first\n    for (int i = 0; i < n-1; i++) {\n        pipe(pipes[i]);\n    }\n    \n    for\
          \ (int i = 0; i < n; i++) {\n        pid_t pid = fork();\n        if (pid == 0) {\n            // Connect to previous\
          \ pipe (read end)\n            if (i > 0) {\n                dup2(pipes[i-1][0], STDIN_FILENO);\n            }\n\
          \            // Connect to next pipe (write end)\n            if (i < n-1) {\n                dup2(pipes[i][1],\
          \ STDOUT_FILENO);\n            }\n            \n            // Close all pipe fds in child\n            for (int\
          \ j = 0; j < n-1; j++) {\n                close(pipes[j][0]);\n                close(pipes[j][1]);\n           \
          \ }\n            \n            execvp(cmds[i].args[0], cmds[i].args);\n            exit(1);\n        }\n    }\n\
          \    \n    // Parent closes all pipes\n    for (int i = 0; i < n-1; i++) {\n        close(pipes[i][0]);\n      \
          \  close(pipes[i][1]);\n    }\n    \n    // Wait for all children\n    for (int i = 0; i < n; i++) {\n        wait(NULL);\n\
          \    }\n}"
      pitfalls:
      - Fd leaks causing hangs
      - Not closing pipe ends
      - Wrong redirection precedence
      concepts:
      - File descriptors
      - dup2
      - Pipe communication
      estimated_hours: 5-8
      deliverables:
      - Pipe connector that chains the stdout of one command to the stdin of the next using the pipe system call
      - Stdin redirector that opens a file and connects it to the command's standard input via dup2
      - Stdout redirector that creates or truncates a file and connects it to the command's standard output via dup2
      - Append redirector that opens a file in append mode and connects it to the command's standard output
    - id: 3
      name: Job Control
      description: Implement background jobs and job management.
      acceptance_criteria:
      - Appending & to a command line launches the command as a background job printing its job number and PID
      - The jobs built-in lists all current jobs with their job number, state, and command string
      - The fg and bg built-ins resume a stopped job in the foreground or background by job number
      - Process groups are set correctly so that signals from the terminal reach only the foreground job group
      hints:
        level1: Each job needs its own process group. Use setpgid in child and parent.
        level2: Shell must give terminal to foreground job. Use tcsetpgrp.
        level3: "typedef struct Job {\n    int id;\n    pid_t pgid;\n    char *command;\n    int status;  // RUNNING, STOPPED,\
          \ DONE\n    struct Job *next;\n} Job;\n\nJob *jobs = NULL;\nint next_job_id = 1;\n\nvoid launch_job(char **args,\
          \ int background) {\n    pid_t pid = fork();\n    if (pid == 0) {\n        // Child: create new process group\n\
          \        setpgid(0, 0);\n        \n        // If foreground, take terminal\n        if (!background) {\n       \
          \     tcsetpgrp(STDIN_FILENO, getpid());\n        }\n        \n        // Reset signal handlers\n        signal(SIGINT,\
          \ SIG_DFL);\n        signal(SIGTSTP, SIG_DFL);\n        \n        execvp(args[0], args);\n        exit(1);\n   \
          \ }\n    \n    // Parent\n    setpgid(pid, pid);  // Avoid race condition\n    \n    Job *job = add_job(pid, args);\n\
          \    \n    if (background) {\n        printf(\"[%d] %d\\n\", job->id, pid);\n    } else {\n        // Wait for foreground\
          \ job\n        wait_for_job(job);\n        // Take back terminal\n        tcsetpgrp(STDIN_FILENO, getpgrp());\n\
          \    }\n}"
      pitfalls:
      - Race conditions with setpgid
      - Terminal control issues
      - Zombie processes
      concepts:
      - Process groups
      - Session management
      - Terminal control
      estimated_hours: 8-12
      deliverables:
      - Foreground and background job manager that tracks running and stopped jobs with their process groups
      - SIGTSTP handler that suspends the foreground job when the user presses Ctrl+Z and returns to the prompt
      - Built-in fg and bg commands that resume stopped jobs in the foreground or background respectively
      - Job table manager that assigns job numbers and tracks each job's PID, state, and command string
    - id: 4
      name: Signal Handling
      description: Handle Ctrl+C, Ctrl+Z and job notifications properly.
      acceptance_criteria:
      - Ctrl+C sends SIGINT only to the foreground job's process group, not to the shell process
      - Ctrl+Z sends SIGTSTP to the foreground job's process group suspending it and returning to the prompt
      - SIGCHLD is handled to reap background jobs on completion and report their exit status to the user
      - Signal masks are saved before fork and restored to defaults in the child process after fork
      hints:
        level1: Shell ignores SIGINT/SIGTSTP, children restore defaults.
        level2: Use SIGCHLD handler to reap background jobs. Mask signals during critical sections.
        level3: "volatile sig_atomic_t sigchld_received = 0;\n\nvoid sigchld_handler(int sig) {\n    sigchld_received = 1;\n\
          }\n\nvoid reap_children() {\n    int status;\n    pid_t pid;\n    \n    while ((pid = waitpid(-1, &status, WNOHANG\
          \ | WUNTRACED | WCONTINUED)) > 0) {\n        Job *job = find_job_by_pid(pid);\n        if (!job) continue;\n   \
          \     \n        if (WIFEXITED(status) || WIFSIGNALED(status)) {\n            printf(\"\\n[%d] Done: %s\\n\", job->id,\
          \ job->command);\n            remove_job(job);\n        } else if (WIFSTOPPED(status)) {\n            job->status\
          \ = STOPPED;\n            printf(\"\\n[%d] Stopped: %s\\n\", job->id, job->command);\n        } else if (WIFCONTINUED(status))\
          \ {\n            job->status = RUNNING;\n        }\n    }\n}\n\nvoid setup_shell_signals() {\n    // Shell ignores\
          \ job-control signals\n    signal(SIGINT, SIG_IGN);\n    signal(SIGTSTP, SIG_IGN);\n    signal(SIGTTIN, SIG_IGN);\n\
          \    signal(SIGTTOU, SIG_IGN);\n    \n    // But catches SIGCHLD\n    struct sigaction sa;\n    sa.sa_handler =\
          \ sigchld_handler;\n    sigemptyset(&sa.sa_mask);\n    sa.sa_flags = SA_RESTART;\n    sigaction(SIGCHLD, &sa, NULL);\n\
          }"
      pitfalls:
      - Signal handler races
      - Async-signal-safe functions
      - Interrupted system calls
      concepts:
      - Signal masking
      - Async-signal safety
      - Job notification
      estimated_hours: 8-14
      deliverables:
      - SIGINT handler that delivers the interrupt signal to the foreground job only, not to the shell itself
      - SIGTSTP handler that delivers the stop signal to the foreground job and updates its state to stopped
      - Child process signal forwarder that relays terminal-generated signals to the foreground process group
      - Signal mask restorer that resets signal dispositions to default in child processes after fork
  neural-network-basic:
    id: neural-network-basic
    name: Neural Network (micrograd)
    description: Build a minimal neural network library with automatic differentiation. Inspired by Andrej Karpathy's micrograd.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Calculus (derivatives)
    - Python basics
    - Linear algebra basics
    languages:
      recommended:
      - Python
      also_possible:
      - Julia
      - JavaScript
    resources:
    - name: Micrograd Repository
      url: https://github.com/karpathy/micrograd
      type: code
    - name: Micrograd Video Tutorial
      url: https://www.youtube.com/watch?v=VMj-3S1tku0
      type: video
    milestones:
    - id: 1
      name: Value Class with Autograd
      description: Create Value class that tracks computation graph for automatic differentiation.
      acceptance_criteria:
      - Value class wraps a scalar float number and exposes it for arithmetic operations
      - Each Value tracks its child operands and the operation that produced it in the computational graph
      - Each Value tracks the operation type (add, multiply, etc.) that created it for backward dispatch
      - Gradient field is initialized to zero and accumulates derivatives during the backward pass
      - Value supports addition, multiplication, subtraction, division, and power operator overloads
      hints:
        level1: 'Value stores: data, grad, _backward function, _prev set.'
        level2: Each operation creates new Value with backward function.
        level3: "class Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad\
          \ = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n    \n\
          \    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out\
          \ = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n\
          \            other.grad += out.grad\n        out._backward = _backward\n        return out\n    \n    def __mul__(self,\
          \ other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data *\
          \ other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n  \
          \          other.grad += self.data * out.grad\n        out._backward = _backward\n        return out"
      pitfalls:
      - Forgetting += for gradients (accumulation)
      - Not handling scalar + Value
      - Backward called before forward
      concepts:
      - Computational graphs
      - Chain rule
      - Gradient accumulation
      estimated_hours: 4-6
      deliverables:
      - Value wrapper class that holds a scalar number along with its gradient and computational graph references
      - Operation tracker that records which operation and operands produced each Value for backpropagation
      - Gradient storage field on each Value that accumulates the derivative of the loss with respect to that Value
      - Parent reference links that connect each Value to the Values used as inputs in the operation that created it
    - id: 2
      name: Backward Pass
      description: Implement backpropagation through the computation graph.
      acceptance_criteria:
      - Topological sort produces a valid ordering where all inputs appear before the operations that use them
      - Backward pass iterates in reverse topological order computing gradients from output toward inputs
      - Gradients are zeroed before each backward pass to prevent accumulation from previous iterations
      - Values used as input to multiple operations correctly accumulate gradients from all downstream paths
      hints:
        level1: 'Topological sort: visit children before parents.'
        level2: Start with output.grad = 1, then propagate.
        level3: "def backward(self):\n    # Topological sort\n    topo = []\n    visited = set()\n    def build_topo(v):\n\
          \        if v not in visited:\n            visited.add(v)\n            for child in v._prev:\n                build_topo(child)\n\
          \            topo.append(v)\n    build_topo(self)\n    \n    # Backward pass\n    self.grad = 1\n    for v in reversed(topo):\n\
          \        v._backward()"
      pitfalls:
      - Not zeroing gradients
      - Wrong topological order
      - Gradient through constants
      concepts:
      - Backpropagation
      - Topological sort
      - Reverse-mode AD
      estimated_hours: 2-3
      deliverables:
      - Topological sort that orders all Values in the computational graph from inputs to output
      - Backward propagation engine that applies the chain rule in reverse topological order to compute gradients
      - Chain rule applicator that multiplies the upstream gradient by the local derivative at each operation node
      - Gradient accumulator that sums gradient contributions when a Value is used as input to multiple operations
    - id: 3
      name: Neuron and Layer
      description: Build neural network components using Value.
      acceptance_criteria:
      - Each neuron has a weight vector and bias term that are initialized randomly and are trainable Value objects
      - Activation function applies tanh or ReLU nonlinearity to the weighted sum producing a bounded output
      - Layer computes outputs for all its neurons producing a vector of activation values for each input
      - MLP (Multi-Layer Perceptron) chains multiple layers and propagates data through all of them sequentially
      hints:
        level1: 'Neuron: sum(w*x) + b, then activation.'
        level2: 'Layer: list of neurons. MLP: list of layers.'
        level3: "import random\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,\
          \ 1)) for _ in range(nin)]\n        self.b = Value(0)\n    \n    def __call__(self, x):\n        act = sum((wi*xi\
          \ for wi, xi in zip(self.w, x)), self.b)\n        return act.tanh()\n    \n    def parameters(self):\n        return\
          \ self.w + [self.b]\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for\
          \ _ in range(nout)]\n    \n    def __call__(self, x):\n        return [n(x) for n in self.neurons]\n    \n    def\
          \ parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\nclass MLP:\n    def __init__(self,\
          \ nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\
          \    \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x[0]\
          \ if len(x) == 1 else x"
      pitfalls:
      - Not initializing weights properly
      - Missing activation on last layer
      - Parameter collection incomplete
      concepts:
      - Neural network architecture
      - Activation functions
      - Parameter management
      estimated_hours: 3-4
      deliverables:
      - Neuron class that computes a weighted sum of inputs plus bias and applies a nonlinear activation function
      - Layer class that groups multiple neurons and computes all their outputs for a given input vector
      - Forward pass method that propagates input data through the neuron computing activation output
      - Parameter collection method that returns all trainable weights and biases from the network hierarchy
    - id: 4
      name: Training Loop
      description: Train the neural network with gradient descent.
      acceptance_criteria:
      - Forward pass computes predictions by propagating inputs through all layers of the network
      - MSE loss is computed as the mean of squared differences between predictions and target values
      - Backward pass computes gradients of the loss with respect to all trainable parameters
      - Gradient descent update subtracts learning rate times gradient from each parameter reducing the loss
      - Training over multiple epochs progressively reduces the loss demonstrating network learning
      hints:
        level1: loss = sum((ypred - ytrue)**2)
        level2: 'After backward: param.data -= learning_rate * param.grad'
        level3: "# Training data\nxs = [[2.0, 3.0], [-1.0, -2.0], [3.0, -1.0]]\nys = [1.0, -1.0, 1.0]\n\n# Training loop\n\
          model = MLP(2, [4, 4, 1])  # 2 inputs, 2 hidden layers of 4, 1 output\nlearning_rate = 0.05\n\nfor epoch in range(100):\n\
          \    # Forward pass\n    ypred = [model(x) for x in xs]\n    loss = sum((yp - yt)**2 for yp, yt in zip(ypred, ys))\n\
          \    \n    # Zero gradients\n    for p in model.parameters():\n        p.grad = 0\n    \n    # Backward pass\n \
          \   loss.backward()\n    \n    # Update weights\n    for p in model.parameters():\n        p.data -= learning_rate\
          \ * p.grad\n    \n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Loss: {loss.data}')"
      pitfalls:
      - Forgetting to zero gradients
      - Learning rate too high
      - Not enough epochs
      concepts:
      - Training loop
      - Gradient descent
      - Loss functions
      estimated_hours: 3-4
      deliverables:
      - Loss function that computes mean squared error between predicted and target output values
      - Gradient zeroing routine that resets all parameter gradients to zero before each backward pass
      - Parameter update step that adjusts each weight by subtracting learning rate times its gradient
      - Training iteration loop that repeats forward pass, loss, backward, and update for multiple epochs
  packet-sniffer:
    id: packet-sniffer
    name: Packet Sniffer
    description: Build a network packet capture and analysis tool using raw sockets or libpcap. Learn network protocols and
      packet parsing.
    difficulty: intermediate
    estimated_hours: 20-30
    prerequisites:
    - Networking basics
    - TCP/IP model
    - Binary parsing
    languages:
      recommended:
      - C
      - Python
      - Go
      also_possible:
      - Rust
    resources:
    - name: Libpcap Programming Tutorial
      url: https://www.tcpdump.org/pcap.html
      type: tutorial
    - name: Building a Packet Sniffer from Scratch
      url: https://aidanvidal.github.io/posts/Packet_Sniffer.html
      type: tutorial
    - name: Scapy Documentation
      url: https://scapy.readthedocs.io/
      type: reference
    milestones:
    - id: 1
      name: Packet Capture Setup
      description: Set up packet capture using raw sockets or libpcap.
      acceptance_criteria:
      - List all available network interfaces by name
      - Open selected interface for packet capture
      - Capture packets in promiscuous mode receiving all traffic
      - Handle root or admin permission requirements gracefully
      hints:
        level1: 'libpcap: pcap_findalldevs() lists interfaces, pcap_open_live() opens for capture.'
        level2: Promiscuous mode captures all traffic, not just for your host. Requires root.
        level3: |-
          # Python with scapy (simpler) or raw sockets
          from scapy.all import sniff, get_if_list

          def list_interfaces():
              return get_if_list()

          def capture_packets(interface: str, count: int = 10):
              '''Capture packets and return them'''
              packets = sniff(iface=interface, count=count, promisc=True)
              return packets

          # Or with raw sockets (Linux)
          import socket

          def raw_capture(interface: str):
              # Create raw socket
              sock = socket.socket(socket.AF_PACKET, socket.SOCK_RAW, socket.ntohs(0x0003))
              sock.bind((interface, 0))
              sock.setblocking(False)

              while True:
                  try:
                      packet, addr = sock.recvfrom(65535)
                      yield packet
                  except BlockingIOError:
                      continue

          # With libpcap (C)
          '''
          pcap_t *handle;
          char errbuf[PCAP_ERRBUF_SIZE];
          handle = pcap_open_live("eth0", BUFSIZ, 1, 1000, errbuf);
          if (handle == NULL) {
              fprintf(stderr, "Couldn't open device: %s\n", errbuf);
              return 1;
          }
          '''
      pitfalls:
      - Requires root/admin privileges
      - Interface names vary by OS (eth0, en0, etc.)
      - Virtual interfaces may not support promiscuous mode
      concepts:
      - Raw sockets
      - Network interfaces
      - Privilege requirements
      estimated_hours: 4-5
      deliverables:
      - Raw socket creation with appropriate protocol family
      - Promiscuous mode activation on selected interface
      - BPF filter compilation and attachment option
      - Ring buffer for captured packet storage
    - id: 2
      name: Ethernet Frame Parsing
      description: Parse Ethernet frames to extract MAC addresses and protocol type.
      acceptance_criteria:
      - Extract destination MAC address from first 6 bytes
      - Extract source MAC address from bytes 6 through 12
      - Extract EtherType field from bytes 12 through 14
      - Format MAC addresses as colon-separated hex string
      hints:
        level1: 'Ethernet header is 14 bytes: dst(6) + src(6) + type(2). Type 0x0800 = IPv4.'
        level2: Use struct.unpack() for parsing. MAC is 6 bytes, format each as hex.
        level3: |-
          from struct import unpack

          class EthernetFrame:
              def __init__(self, data: bytes):
                  # Unpack ethernet header (14 bytes)
                  # ! = network byte order (big endian)
                  self.dst_mac = data[0:6]
                  self.src_mac = data[6:12]
                  self.ethertype = unpack('!H', data[12:14])[0]
                  self.payload = data[14:]

              @staticmethod
              def format_mac(mac: bytes) -> str:
                  return ':'.join(f'{b:02x}' for b in mac)

              def __str__(self):
                  proto = {0x0800: 'IPv4', 0x0806: 'ARP', 0x86dd: 'IPv6'}.get(
                      self.ethertype, f'0x{self.ethertype:04x}'
                  )
                  return (f"Ethernet: {self.format_mac(self.src_mac)} -> "
                          f"{self.format_mac(self.dst_mac)} ({proto})")

          # Parse captured packet
          frame = EthernetFrame(raw_packet)
          print(frame)
          # Ethernet: aa:bb:cc:dd:ee:ff -> 11:22:33:44:55:66 (IPv4)
      pitfalls:
      - 802.1Q VLAN tags add 4 extra bytes
      - Jumbo frames can exceed 1500 bytes payload
      - Byte order is big-endian (network order)
      concepts:
      - Ethernet framing
      - MAC addresses
      - EtherType
      estimated_hours: 3-4
      deliverables:
      - Ethernet header parsing extracting all 14 bytes
      - Source and destination MAC address extraction and formatting
      - EtherType field handling for protocol identification
      - Frame validation including minimum size and CRC checks
    - id: 3
      name: IP Header Parsing
      description: Parse IPv4 headers to extract addresses and protocol.
      acceptance_criteria:
      - Extract IP version and header length from first byte
      - Extract total length, TTL, and protocol number fields
      - Extract source and destination IP addresses correctly
      - Handle variable header length when IP options are present
      hints:
        level1: 'IPv4 header: first nibble = version (4), second nibble = IHL (header length in 32-bit words).'
        level2: 'Protocol field: 1=ICMP, 6=TCP, 17=UDP. IPs are 4 bytes each at offset 12 and 16.'
        level3: |-
          class IPv4Packet:
              def __init__(self, data: bytes):
                  # First byte: version (4 bits) + IHL (4 bits)
                  version_ihl = data[0]
                  self.version = version_ihl >> 4
                  self.ihl = version_ihl & 0x0F  # Header length in 32-bit words
                  self.header_length = self.ihl * 4

                  self.dscp_ecn = data[1]
                  self.total_length = unpack('!H', data[2:4])[0]
                  self.identification = unpack('!H', data[4:6])[0]

                  flags_frag = unpack('!H', data[6:8])[0]
                  self.flags = flags_frag >> 13
                  self.fragment_offset = flags_frag & 0x1FFF

                  self.ttl = data[8]
                  self.protocol = data[9]
                  self.checksum = unpack('!H', data[10:12])[0]

                  self.src_ip = data[12:16]
                  self.dst_ip = data[16:20]

                  # Options (if any)
                  self.options = data[20:self.header_length] if self.ihl > 5 else b''

                  self.payload = data[self.header_length:]

              @staticmethod
              def format_ip(ip: bytes) -> str:
                  return '.'.join(str(b) for b in ip)

              def __str__(self):
                  proto = {1: 'ICMP', 6: 'TCP', 17: 'UDP'}.get(self.protocol, str(self.protocol))
                  return (f"IPv4: {self.format_ip(self.src_ip)} -> "
                          f"{self.format_ip(self.dst_ip)} ({proto}, TTL={self.ttl})")
      pitfalls:
      - Header length is in 32-bit words, not bytes
      - Fragmented packets need reassembly
      - Options make header length variable
      concepts:
      - IP addressing
      - Protocol numbers
      - Header fields
      estimated_hours: 4-5
      deliverables:
      - IPv4 header parsing with all standard fields
      - IPv6 header parsing with next header chain support
      - Protocol field extraction identifying transport layer
      - Source and destination IP address extraction and formatting
    - id: 4
      name: TCP/UDP Parsing
      description: Parse TCP and UDP headers for port information and flags.
      acceptance_criteria:
      - Extract source and destination port numbers correctly
      - Extract TCP flags including SYN, ACK, FIN, RST, and PSH
      - Extract TCP sequence and acknowledgment number fields
      - Identify common services by well-known port numbers
      hints:
        level1: 'TCP header: ports(4) + seq(4) + ack(4) + flags(2) + window(2) + checksum(2) + urgent(2) = 20+ bytes'
        level2: 'UDP is simpler: ports(4) + length(2) + checksum(2) = 8 bytes total.'
        level3: |-
          class TCPSegment:
              def __init__(self, data: bytes):
                  self.src_port = unpack('!H', data[0:2])[0]
                  self.dst_port = unpack('!H', data[2:4])[0]
                  self.seq_num = unpack('!I', data[4:8])[0]
                  self.ack_num = unpack('!I', data[8:12])[0]

                  data_offset_flags = unpack('!H', data[12:14])[0]
                  self.data_offset = (data_offset_flags >> 12) * 4  # In bytes
                  self.flags = data_offset_flags & 0x1FF

                  self.window = unpack('!H', data[14:16])[0]
                  self.checksum = unpack('!H', data[16:18])[0]
                  self.urgent = unpack('!H', data[18:20])[0]

                  self.payload = data[self.data_offset:]

              @property
              def flag_str(self) -> str:
                  flags = []
                  if self.flags & 0x001: flags.append('FIN')
                  if self.flags & 0x002: flags.append('SYN')
                  if self.flags & 0x004: flags.append('RST')
                  if self.flags & 0x008: flags.append('PSH')
                  if self.flags & 0x010: flags.append('ACK')
                  if self.flags & 0x020: flags.append('URG')
                  return ','.join(flags) or 'none'

          class UDPDatagram:
              def __init__(self, data: bytes):
                  self.src_port = unpack('!H', data[0:2])[0]
                  self.dst_port = unpack('!H', data[2:4])[0]
                  self.length = unpack('!H', data[4:6])[0]
                  self.checksum = unpack('!H', data[6:8])[0]
                  self.payload = data[8:]

          # Common ports
          SERVICES = {80: 'HTTP', 443: 'HTTPS', 22: 'SSH', 53: 'DNS', 25: 'SMTP'}
      pitfalls:
      - TCP data offset is in 32-bit words
      - Port numbers are unsigned 16-bit
      - Don't confuse TCP and UDP despite similar port fields
      concepts:
      - TCP flags
      - Port numbers
      - Transport layer
      estimated_hours: 4-5
      deliverables:
      - TCP header parsing including flags and sequence numbers
      - UDP header parsing with length and checksum fields
      - Source and destination port extraction for both protocols
      - Payload extraction after transport header boundary
    - id: 5
      name: Filtering and Output
      description: Add BPF filters and formatted output display.
      acceptance_criteria:
      - Support BPF filter expressions for capture-time filtering
      - Filter captured packets by protocol type (TCP, UDP, ICMP)
      - Filter captured packets by port number or IP address
      - Display formatted packet summary with protocol details
      hints:
        level1: 'BPF syntax: ''tcp port 80'', ''host 192.168.1.1'', ''udp and port 53'''
        level2: pcap_compile() and pcap_setfilter() apply BPF filters before capture.
        level3: |-
          # Using scapy's BPF filter
          from scapy.all import sniff

          def capture_with_filter(interface: str, bpf_filter: str, callback):
              '''Capture packets matching BPF filter'''
              sniff(iface=interface, filter=bpf_filter, prn=callback)

          def packet_callback(packet):
              '''Process and display packet'''
              timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]

              # Build summary
              summary = f"{timestamp} "

              if packet.haslayer('Ether'):
                  eth = packet['Ether']

              if packet.haslayer('IP'):
                  ip = packet['IP']
                  summary += f"{ip.src} -> {ip.dst} "

                  if packet.haslayer('TCP'):
                      tcp = packet['TCP']
                      flags = tcp.sprintf('%TCP.flags%')
                      summary += f"TCP {tcp.sport} -> {tcp.dport} [{flags}]"
                  elif packet.haslayer('UDP'):
                      udp = packet['UDP']
                      summary += f"UDP {udp.sport} -> {udp.dport}"
                  elif packet.haslayer('ICMP'):
                      icmp = packet['ICMP']
                      summary += f"ICMP type={icmp.type}"

              print(summary)

          # Example filters:
          # "tcp port 80"       - HTTP traffic
          # "udp port 53"       - DNS queries
          # "host 192.168.1.1"  - All traffic to/from IP
          # "tcp[tcpflags] & tcp-syn != 0"  - TCP SYN packets
      pitfalls:
      - BPF syntax errors are cryptic
      - High traffic can overwhelm display
      - Timestamps should be high-resolution
      concepts:
      - Berkeley Packet Filter
      - Traffic filtering
      - Packet analysis
      estimated_hours: 5-6
      deliverables:
      - Display filter supporting protocol and address criteria
      - Formatted packet summary with timestamp and key fields
      - Hex dump output option showing raw packet bytes
      - PCAP file export for offline analysis in Wireshark
  password-hashing:
    id: password-hashing
    name: Password Hashing
    description: Implement secure password hashing with salt. Learn cryptographic security concepts and why plain hashing
      is insufficient.
    difficulty: beginner
    estimated_hours: 4-6
    prerequisites:
    - Basic programming
    - Understanding of hashing
    languages:
      recommended:
      - Python
      - Go
      - JavaScript
      also_possible:
      - Java
      - C#
    resources:
    - name: How to Safely Store Passwords
      url: https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html
      type: documentation
    milestones:
    - id: 1
      name: Basic Hashing with Salt
      description: Implement salted password hashing.
      acceptance_criteria:
      - Generate cryptographically random salt of at least 16 bytes
      - Hash password concatenated with salt using SHA-256
      - Store salt alongside hash for later verification
      - Verify password against stored salt and hash pair
      hints:
        level1: Salt prevents rainbow table attacks.
        level2: Concatenate salt + password, then hash.
        level3: |-
          import hashlib
          import os

          def hash_password(password):
              salt = os.urandom(16)  # 16 bytes of random salt
              combined = salt + password.encode()
              hash_bytes = hashlib.sha256(combined).digest()
              # Store salt + hash together
              return salt + hash_bytes

          def verify_password(password, stored):
              salt = stored[:16]
              stored_hash = stored[16:]
              combined = salt + password.encode()
              computed_hash = hashlib.sha256(combined).digest()
              return computed_hash == stored_hash
      pitfalls:
      - Using predictable salt
      - Not storing salt
      - Timing attacks in comparison
      concepts:
      - Salting
      - Rainbow tables
      - Secure random generation
      estimated_hours: 1-2
      deliverables:
      - Cryptographically secure random salt generation function
      - SHA-256 hash computation combining salt and password
      - Combined salt and hash storage in single output format
      - Password verification function comparing against stored hash
    - id: 2
      name: Key Stretching
      description: Implement iterated hashing for slow verification.
      acceptance_criteria:
      - Apply multiple hash iterations to slow brute force attacks
      - Support configurable iteration count (minimum 100,000)
      - Implement PBKDF2 key derivation with HMAC-SHA256
      - Use constant-time comparison to prevent timing side channels
      hints:
        level1: More iterations = slower brute force.
        level2: PBKDF2 is standard key derivation function.
        level3: |-
          import hashlib
          import hmac

          def pbkdf2_hash(password, salt, iterations=100000):
              # PBKDF2-HMAC-SHA256
              return hashlib.pbkdf2_hmac(
                  'sha256',
                  password.encode(),
                  salt,
                  iterations
              )

          def constant_time_compare(a, b):
              """Prevent timing attacks"""
              if len(a) != len(b):
                  return False
              result = 0
              for x, y in zip(a, b):
                  result |= x ^ y
              return result == 0
      pitfalls:
      - Too few iterations
      - Not using constant-time compare
      - iteration count not stored
      concepts:
      - Key stretching
      - PBKDF2
      - Timing attacks
      estimated_hours: 1-2
      deliverables:
      - PBKDF2-HMAC-SHA256 implementation with configurable parameters
      - Configurable iteration count stored with output
      - Derived key of configurable length in bytes
      - Constant-time comparison function preventing timing attacks
    - id: 3
      name: Modern Password Hashing
      description: Implement or use bcrypt/Argon2.
      acceptance_criteria:
      - Understand and parse bcrypt output format including version and cost
      - Configure bcrypt work factor appropriate for current hardware
      - Optionally support Argon2id as memory-hard alternative
      - Implement migration strategy for upgrading legacy password hashes
      hints:
        level1: Bcrypt includes salt and cost in output.
        level2: Argon2 is memory-hard, resistant to GPU attacks.
        level3: |-
          # Using bcrypt library (recommended for production)
          import bcrypt

          def hash_password_bcrypt(password):
              # Cost factor of 12 is reasonable for 2024
              return bcrypt.hashpw(password.encode(), bcrypt.gensalt(rounds=12))

          def verify_password_bcrypt(password, hashed):
              return bcrypt.checkpw(password.encode(), hashed)

          # Bcrypt output format: $2b$12$salthere...hashhere
          # $2b$ = algorithm, 12 = cost factor, then 22-char salt + 31-char hash

          # Migration strategy
          def verify_with_migration(password, stored_hash):
              if stored_hash.startswith('$2b$'):
                  return verify_password_bcrypt(password, stored_hash)
              else:
                  # Old format - verify then upgrade
                  if verify_old_format(password, stored_hash):
                      return True, hash_password_bcrypt(password)  # New hash to store
                  return False, None
      pitfalls:
      - Implementing crypto yourself
      - Too low work factor
      - Not planning for algorithm upgrades
      concepts:
      - Bcrypt
      - Argon2
      - Algorithm agility
      estimated_hours: 1-2
      deliverables:
      - Bcrypt integration with configurable cost factor
      - Argon2id integration with memory and time parameters
      - Cost factor tuning utility measuring hash computation time
      - Migration support for upgrading old hash formats transparently
  platformer:
    id: platformer
    name: Platformer
    description: Build a 2D platformer with physics, jumping, and collision detection. Learn game physics and level design.
    difficulty: intermediate
    estimated_hours: 20-30
    prerequisites:
    - Basic game loop
    - 2D graphics
    - Basic physics
    languages:
      recommended:
      - JavaScript
      - Python
      - C#
      also_possible:
      - C++
      - Lua
    resources:
    - name: 2D Platformer Tutorial
      url: https://www.youtube.com/results?search_query=2d+platformer+tutorial
      type: video
    milestones:
    - id: 1
      name: Basic Movement and Gravity
      description: Implement player movement with gravity.
      acceptance_criteria:
      - Move player left and right with arrow key input
      - Apply downward gravity acceleration each physics frame
      - Detect ground collision and stop player from falling through
      - Use velocity-based movement scaled by delta time
      hints:
        level1: velocity.y += gravity each frame. position += velocity.
        level2: Check if player bottom > ground level to detect landing.
        level3: "class Player:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        self.vx = 0\n\
          \        self.vy = 0\n        self.width = 32\n        self.height = 48\n        self.on_ground = False\n    \n\
          \    def update(self, dt):\n        # Gravity\n        self.vy += GRAVITY * dt\n        \n        # Horizontal movement\n\
          \        if keys['left']:\n            self.vx = -MOVE_SPEED\n        elif keys['right']:\n            self.vx =\
          \ MOVE_SPEED\n        else:\n            self.vx = 0\n        \n        # Apply velocity\n        self.x += self.vx\
          \ * dt\n        self.y += self.vy * dt\n        \n        # Ground collision\n        if self.y + self.height >\
          \ GROUND_Y:\n            self.y = GROUND_Y - self.height\n            self.vy = 0\n            self.on_ground =\
          \ True"
      pitfalls:
      - Gravity too strong or weak
      - Not using delta time
      - Ground clipping
      concepts:
      - Physics simulation
      - Velocity and acceleration
      - Delta time
      estimated_hours: 3-4
      deliverables:
      - Player entity with position, velocity, and dimensions
      - Horizontal movement responding to left and right input
      - Gravity acceleration applied each frame using delta time
      - Terminal velocity cap preventing infinite falling speed
    - id: 2
      name: Jumping
      description: Implement jump mechanics with variable height.
      acceptance_criteria:
      - Allow jump only when player is standing on ground
      - Support variable jump height by holding the jump button longer
      - Implement coyote time permitting jump within 100ms of leaving edge
      - Implement jump buffer accepting input within 100ms before landing
      hints:
        level1: Jump = set negative vy. Only allow when on_ground.
        level2: Cut jump short by reducing vy when button released.
        level3: "JUMP_VELOCITY = -300\nJUMP_CUT_MULTIPLIER = 0.5\nCOYOTE_TIME = 0.1  # seconds\nJUMP_BUFFER = 0.1\n\nclass\
          \ Player:\n    def __init__(self):\n        self.coyote_timer = 0\n        self.jump_buffer_timer = 0\n    \n  \
          \  def update(self, dt):\n        # Coyote time\n        if self.on_ground:\n            self.coyote_timer = COYOTE_TIME\n\
          \        else:\n            self.coyote_timer -= dt\n        \n        # Jump buffer\n        if keys_pressed['jump']:\n\
          \            self.jump_buffer_timer = JUMP_BUFFER\n        else:\n            self.jump_buffer_timer -= dt\n   \
          \     \n        # Jump\n        can_jump = self.coyote_timer > 0\n        want_jump = self.jump_buffer_timer > 0\n\
          \        if can_jump and want_jump:\n            self.vy = JUMP_VELOCITY\n            self.coyote_timer = 0\n  \
          \          self.jump_buffer_timer = 0\n        \n        # Variable jump height\n        if keys_released['jump']\
          \ and self.vy < 0:\n            self.vy *= JUMP_CUT_MULTIPLIER"
      pitfalls:
      - Double jump without intending
      - Jump feels floaty
      - Coyote time too generous
      concepts:
      - Jump mechanics
      - Coyote time
      - Input buffering
      estimated_hours: 3-4
      deliverables:
      - Jump initiation applying upward velocity impulse
      - Variable jump height based on button hold duration
      - Coyote time allowing jump shortly after leaving edge
      - Jump buffering registering input just before landing
    - id: 3
      name: Tile-based Collision
      description: Implement collision with tile-based level.
      acceptance_criteria:
      - Load and render tile map from level data file
      - Detect collision between player and solid tiles accurately
      - Resolve X and Y collisions separately to handle corners
      - Optionally support slope tiles for angled surfaces
      hints:
        level1: Check which tiles player overlaps with.
        level2: Resolve X and Y separately to handle corners.
        level3: "def resolve_collisions(self, tilemap):\n    # Move X, resolve X collisions\n    self.x += self.vx * dt\n\
          \    for tile in tilemap.get_colliding_tiles(self.rect):\n        if self.vx > 0:  # Moving right\n            self.x\
          \ = tile.left - self.width\n        elif self.vx < 0:  # Moving left\n            self.x = tile.right\n        self.vx\
          \ = 0\n    \n    # Move Y, resolve Y collisions\n    self.y += self.vy * dt\n    self.on_ground = False\n    for\
          \ tile in tilemap.get_colliding_tiles(self.rect):\n        if self.vy > 0:  # Falling\n            self.y = tile.top\
          \ - self.height\n            self.on_ground = True\n        elif self.vy < 0:  # Jumping\n            self.y = tile.bottom\n\
          \        self.vy = 0\n\nclass Tilemap:\n    def get_colliding_tiles(self, rect):\n        tiles = []\n        x1,\
          \ y1 = int(rect.left // TILE_SIZE), int(rect.top // TILE_SIZE)\n        x2, y2 = int(rect.right // TILE_SIZE), int(rect.bottom\
          \ // TILE_SIZE)\n        for y in range(y1, y2 + 1):\n            for x in range(x1, x2 + 1):\n                if\
          \ self.is_solid(x, y):\n                    tiles.append(Rect(x * TILE_SIZE, y * TILE_SIZE, TILE_SIZE, TILE_SIZE))\n\
          \        return tiles"
      pitfalls:
      - Corner clipping
      - Tunneling at high speeds
      - Off-by-one in tile lookup
      concepts:
      - AABB collision
      - Collision resolution
      - Tile maps
      estimated_hours: 5-6
      deliverables:
      - Tilemap data structure representing level geometry
      - AABB collision detection against solid tile boundaries
      - Separate X and Y axis collision resolution logic
      - One-way platform support allowing upward pass-through
    - id: 4
      name: Enemies and Hazards
      description: Add enemies and death/respawn system.
      acceptance_criteria:
      - Create enemies with simple left-right patrol behavior
      - Kill player on contact with enemy from the side
      - Respawn player at last activated checkpoint on death
      - Optionally defeat enemies by stomping from above
      hints:
        level1: Enemy walks left/right, reverses at edges or walls.
        level2: Check player-enemy overlap for damage.
        level3: "class Enemy:\n    def __init__(self, x, y, patrol_distance):\n        self.x = x\n        self.y = y\n  \
          \      self.start_x = x\n        self.direction = 1\n        self.patrol_distance = patrol_distance\n    \n    def\
          \ update(self, dt, tilemap):\n        self.x += ENEMY_SPEED * self.direction * dt\n        \n        # Reverse at\
          \ patrol bounds\n        if abs(self.x - self.start_x) > self.patrol_distance:\n            self.direction *= -1\n\
          \        \n        # Reverse at walls or edges\n        next_tile_x = int((self.x + self.width/2 + self.direction\
          \ * self.width/2) / TILE_SIZE)\n        floor_tile_y = int((self.y + self.height + 1) / TILE_SIZE)\n        if not\
          \ tilemap.is_solid(next_tile_x, floor_tile_y):  # No floor ahead\n            self.direction *= -1\n\nclass Player:\n\
          \    def check_enemy_collision(self, enemies):\n        for enemy in enemies:\n            if self.rect.colliderect(enemy.rect):\n\
          \                if self.vy > 0 and self.y + self.height < enemy.y + 10:\n                    # Stomped enemy\n\
          \                    enemy.die()\n                    self.vy = BOUNCE_VELOCITY\n                else:\n       \
          \             # Player takes damage\n                    self.die()"
      pitfalls:
      - Stomp detection window
      - Death during invincibility
      - Enemy stuck at patrol bounds
      concepts:
      - AI patrol behavior
      - Health systems
      - Collision types
      estimated_hours: 4-6
      deliverables:
      - Enemy entity class with position and movement behavior
      - Basic patrol AI walking between defined waypoints
      - Player-enemy collision detection and response system
      - Death and respawn system returning player to checkpoint
  process-spawner:
    id: process-spawner
    name: Process Spawner
    description: Build a process manager using fork/exec. Learn Unix process lifecycle and IPC.
    difficulty: intermediate
    estimated_hours: 10-15
    prerequisites:
    - Basic C programming
    - Unix basics
    - System calls
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
    resources:
    - name: fork(2) man page
      url: https://man7.org/linux/man-pages/man2/fork.2.html
      type: documentation
    - name: Advanced Programming in Unix
      url: https://www.apuebook.com/
      type: book
    milestones:
    - id: 1
      name: Basic Fork/Exec
      description: Spawn a child process to run a command.
      acceptance_criteria:
      - Create child process using fork() system call
      - Execute external command in child using exec() family
      - Parent process waits for child to complete using waitpid()
      - Handle fork and exec errors with informative messages
      hints:
        level1: fork() returns 0 in child, child PID in parent. exec() replaces process.
        level2: Always check return values. Child should exec or _exit.
        level3: "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/wait.h>\n\nint spawn(char *program,\
          \ char **args) {\n    pid_t pid = fork();\n    \n    if (pid < 0) {\n        perror(\"fork\");\n        return -1;\n\
          \    }\n    \n    if (pid == 0) {\n        // Child process\n        execvp(program, args);\n        // If exec\
          \ returns, it failed\n        perror(\"exec\");\n        _exit(127);  // Use _exit, not exit\n    }\n    \n    //\
          \ Parent process\n    int status;\n    if (waitpid(pid, &status, 0) < 0) {\n        perror(\"waitpid\");\n     \
          \   return -1;\n    }\n    \n    if (WIFEXITED(status)) {\n        return WEXITSTATUS(status);\n    } else if (WIFSIGNALED(status))\
          \ {\n        printf(\"Killed by signal %d\\n\", WTERMSIG(status));\n        return -1;\n    }\n    \n    return\
          \ -1;\n}\n\nint main() {\n    char *args[] = {\"ls\", \"-la\", NULL};\n    int result = spawn(\"ls\", args);\n \
          \   printf(\"Exit code: %d\\n\", result);\n    return 0;\n}"
      pitfalls:
      - Forgetting to exec
      - Using exit() instead of _exit()
      - Not handling signals
      concepts:
      - fork()
      - exec()
      - Process creation
      estimated_hours: 2-3
      deliverables:
      - fork() call creating child process from parent
      - exec() family function replacing child process image
      - Exit status collection using waitpid() in parent
      - Error handling for failed fork and exec system calls
    - id: 2
      name: Pipe Communication
      description: Set up pipes for parent-child communication.
      acceptance_criteria:
      - Create pipe file descriptors before forking child process
      - Redirect child stdin and stdout to pipe endpoints using dup2
      - Parent reads from and writes to child via pipe descriptors
      - Close unused pipe ends in both parent and child processes
      hints:
        level1: pipe() before fork. dup2() redirects file descriptors.
        level2: Close unused ends to avoid deadlock. EOF when all writers close.
        level3: "#include <unistd.h>\n\nint spawn_with_pipe(char *program, char **args, int *fd_in, int *fd_out) {\n    int\
          \ pipe_in[2], pipe_out[2];\n    \n    if (pipe(pipe_in) < 0 || pipe(pipe_out) < 0) {\n        perror(\"pipe\");\n\
          \        return -1;\n    }\n    \n    pid_t pid = fork();\n    if (pid < 0) {\n        perror(\"fork\");\n     \
          \   return -1;\n    }\n    \n    if (pid == 0) {\n        // Child\n        // pipe_in: parent writes, child reads\n\
          \        close(pipe_in[1]);  // Close write end\n        dup2(pipe_in[0], STDIN_FILENO);\n        close(pipe_in[0]);\n\
          \        \n        // pipe_out: child writes, parent reads\n        close(pipe_out[0]);  // Close read end\n   \
          \     dup2(pipe_out[1], STDOUT_FILENO);\n        close(pipe_out[1]);\n        \n        execvp(program, args);\n\
          \        _exit(127);\n    }\n    \n    // Parent\n    close(pipe_in[0]);   // Close read end\n    close(pipe_out[1]);\
          \  // Close write end\n    \n    *fd_in = pipe_in[1];   // Parent writes here\n    *fd_out = pipe_out[0]; // Parent\
          \ reads here\n    \n    return pid;\n}\n\n// Usage:\nint fd_in, fd_out;\npid_t pid = spawn_with_pipe(\"cat\", args,\
          \ &fd_in, &fd_out);\nwrite(fd_in, \"hello\\n\", 6);\nclose(fd_in);  // Signal EOF\nchar buf[1024];\nread(fd_out,\
          \ buf, sizeof(buf));"
      pitfalls:
      - Pipe deadlock
      - Forgetting to close ends
      - Buffer full blocking
      concepts:
      - Pipes
      - IPC
      - File descriptor redirection
      estimated_hours: 3-4
      deliverables:
      - Pipe creation before fork for parent-child communication
      - Parent-to-child and child-to-parent data transfer channels
      - Bi-directional pipe setup using two pipe pairs
      - Proper pipe cleanup closing all unused file descriptor ends
    - id: 3
      name: Process Pool
      description: Manage a pool of worker processes.
      acceptance_criteria:
      - Spawn configurable number of worker processes at startup
      - Distribute incoming work items to available worker processes
      - Handle worker process crashes by respawning replacements
      - Perform clean shutdown terminating all workers gracefully
      hints:
        level1: Pre-fork workers, send work via pipes or shared memory.
        level2: Parent tracks worker PIDs, respawns on SIGCHLD.
        level3: "#include <signal.h>\n\n#define MAX_WORKERS 4\n\ntypedef struct {\n    pid_t pid;\n    int fd_in;\n    int\
          \ fd_out;\n    int busy;\n} Worker;\n\nWorker workers[MAX_WORKERS];\nint num_workers = 0;\n\nvoid sigchld_handler(int\
          \ sig) {\n    int status;\n    pid_t pid;\n    while ((pid = waitpid(-1, &status, WNOHANG)) > 0) {\n        // Find\
          \ and respawn crashed worker\n        for (int i = 0; i < num_workers; i++) {\n            if (workers[i].pid ==\
          \ pid) {\n                printf(\"Worker %d (PID %d) exited\\n\", i, pid);\n                spawn_worker(i);  //\
          \ Respawn\n                break;\n            }\n        }\n    }\n}\n\nvoid spawn_worker(int idx) {\n    char\
          \ *args[] = {\"./worker\", NULL};\n    workers[idx].pid = spawn_with_pipe(\"./worker\", args,\n                \
          \                        &workers[idx].fd_in,\n                                        &workers[idx].fd_out);\n\
          \    workers[idx].busy = 0;\n}\n\nvoid init_pool() {\n    signal(SIGCHLD, sigchld_handler);\n    \n    for (int\
          \ i = 0; i < MAX_WORKERS; i++) {\n        spawn_worker(i);\n    }\n    num_workers = MAX_WORKERS;\n}\n\nWorker*\
          \ get_available_worker() {\n    for (int i = 0; i < num_workers; i++) {\n        if (!workers[i].busy) {\n     \
          \       workers[i].busy = 1;\n            return &workers[i];\n        }\n    }\n    return NULL;  // All busy\n\
          }\n\nvoid shutdown_pool() {\n    for (int i = 0; i < num_workers; i++) {\n        close(workers[i].fd_in);\n   \
          \     kill(workers[i].pid, SIGTERM);\n    }\n}"
      pitfalls:
      - Zombie processes
      - Signal handling races
      - Resource cleanup
      concepts:
      - Process pools
      - SIGCHLD
      - Worker management
      estimated_hours: 4-5
      deliverables:
      - Worker process creation spawning N child processes
      - Task distribution mechanism sending work to idle workers
      - Result collection aggregating output from worker processes
      - Pool lifecycle management including startup and shutdown
  protocol-buffer:
    id: protocol-buffer
    name: Protocol Buffer
    description: Implement a binary serialization format similar to Protocol Buffers. Learn varints, wire types, and schema-driven
      encoding.
    difficulty: advanced
    estimated_hours: 25-35
    prerequisites:
    - Binary encoding
    - Schema concepts
    - Data structures
    languages:
      recommended:
      - Python
      - Go
      - Rust
      also_possible:
      - C
      - Java
    resources:
    - name: Protocol Buffers Encoding
      url: https://protobuf.dev/programming-guides/encoding/
      type: reference
    - name: Varint Encoding
      url: https://developers.google.com/protocol-buffers/docs/encoding#varints
      type: tutorial
    milestones:
    - id: 1
      name: Varint Encoding
      description: Implement variable-length integer encoding used in Protocol Buffers.
      acceptance_criteria:
      - Encode unsigned integers as variable-length varint bytes
      - Encode signed integers using ZigZag mapping to unsigned
      - Decode varints from byte stream returning value and bytes consumed
      - Handle all values up to 64-bit integer range correctly
      hints:
        level1: Varint uses 7 bits per byte for data, MSB=1 means more bytes follow.
        level2: 'ZigZag maps signed to unsigned: 0->0, -1->1, 1->2, -2->3, etc. Formula: (n << 1) ^ (n >> 63)'
        level3: |-
          def encode_varint(value: int) -> bytes:
              '''Encode unsigned integer as varint'''
              result = []
              while value > 127:
                  result.append((value & 0x7F) | 0x80)  # 7 bits + continuation
                  value >>= 7
              result.append(value)
              return bytes(result)

          def decode_varint(data: bytes, offset: int = 0) -> tuple[int, int]:
              '''Decode varint, return (value, bytes_consumed)'''
              result = 0
              shift = 0
              pos = offset

              while True:
                  byte = data[pos]
                  result |= (byte & 0x7F) << shift
                  pos += 1

                  if not (byte & 0x80):  # No continuation bit
                      break
                  shift += 7

              return result, pos - offset

          def zigzag_encode(value: int) -> int:
              '''Encode signed int using ZigZag'''
              return (value << 1) ^ (value >> 63)

          def zigzag_decode(value: int) -> int:
              '''Decode ZigZag to signed int'''
              return (value >> 1) ^ -(value & 1)

          # Test
          assert encode_varint(1) == b'\x01'
          assert encode_varint(300) == b'\xac\x02'  # 300 = 0b100101100
          assert zigzag_encode(-1) == 1
          assert zigzag_encode(1) == 2
      pitfalls:
      - Signed integers need ZigZag, not two's complement
      - Varint can be up to 10 bytes for 64-bit values
      - Overflow possible if not handling properly
      concepts:
      - Variable-length encoding
      - Integer representation
      - ZigZag encoding
      estimated_hours: 4-5
      deliverables:
      - Unsigned integer varint encoding using 7-bit groups
      - Varint decoding reading bytes until continuation bit clears
      - Signed integer ZigZag encoding for efficient negative values
      - Full 64-bit integer support for both signed and unsigned
    - id: 2
      name: Wire Types
      description: Implement the wire type system for field encoding.
      acceptance_criteria:
      - Support Type 0 varint for int32, int64, uint, bool, and enum
      - Support Type 1 fixed 64-bit for double and sfixed64 values
      - Support Type 2 length-delimited for string, bytes, and messages
      - Support Type 5 fixed 32-bit for float and sfixed32 values
      hints:
        level1: Field key = (field_number << 3) | wire_type. Wire type is in lower 3 bits.
        level2: 'Length-delimited: encode length as varint, then raw bytes.'
        level3: |-
          from enum import IntEnum
          from struct import pack, unpack

          class WireType(IntEnum):
              VARINT = 0
              FIXED64 = 1
              LENGTH_DELIMITED = 2
              # 3, 4 deprecated (groups)
              FIXED32 = 5

          def encode_field_key(field_number: int, wire_type: WireType) -> bytes:
              '''Encode field key'''
              key = (field_number << 3) | wire_type
              return encode_varint(key)

          def decode_field_key(data: bytes, offset: int) -> tuple[int, WireType, int]:
              '''Decode field key, return (field_number, wire_type, bytes_consumed)'''
              key, consumed = decode_varint(data, offset)
              field_number = key >> 3
              wire_type = WireType(key & 0x07)
              return field_number, wire_type, consumed

          def encode_field(field_number: int, value, value_type: str) -> bytes:
              '''Encode a field with type'''
              if value_type in ('int32', 'int64', 'uint32', 'uint64', 'bool'):
                  wire_type = WireType.VARINT
                  val_bytes = encode_varint(value if not isinstance(value, bool) else int(value))
              elif value_type in ('sint32', 'sint64'):
                  wire_type = WireType.VARINT
                  val_bytes = encode_varint(zigzag_encode(value))
              elif value_type == 'double':
                  wire_type = WireType.FIXED64
                  val_bytes = pack('<d', value)
              elif value_type == 'float':
                  wire_type = WireType.FIXED32
                  val_bytes = pack('<f', value)
              elif value_type in ('string', 'bytes'):
                  wire_type = WireType.LENGTH_DELIMITED
                  data = value.encode() if isinstance(value, str) else value
                  val_bytes = encode_varint(len(data)) + data
              else:
                  raise ValueError(f"Unknown type: {value_type}")

              return encode_field_key(field_number, wire_type) + val_bytes
      pitfalls:
      - Wire types 3 and 4 are deprecated (start/end group)
      - Fixed types are little-endian
      - Unknown fields should be preserved, not discarded
      concepts:
      - Wire types
      - Type-length-value encoding
      - Binary protocols
      estimated_hours: 5-6
      deliverables:
      - Wire type encoding and decoding for all supported types
      - Length-delimited field encoding for strings and nested messages
      - Fixed 32-bit and 64-bit field encoding in little-endian format
      - Field number extraction from wire format key bytes
    - id: 3
      name: Schema Parser
      description: Parse a simple schema definition to drive encoding/decoding.
      acceptance_criteria:
      - Parse message definitions with name and body braces
      - Parse field declarations with type, name, and field number
      - Support repeated field modifier for array-type fields
      - Support nested message definitions within parent messages
      hints:
        level1: 'Schema: message Name { type name = number; }. Start simple, then add features.'
        level2: Build AST with Message nodes containing Field nodes.
        level3: |-
          from dataclasses import dataclass
          from typing import Optional
          import re

          @dataclass
          class Field:
              name: str
              type: str
              number: int
              repeated: bool = False

          @dataclass
          class Message:
              name: str
              fields: list[Field]
              nested: list['Message']

          def parse_schema(text: str) -> list[Message]:
              '''Parse protobuf-like schema'''
              messages = []

              # Find all message definitions
              message_pattern = r'message\s+(\w+)\s*\{([^}]+)\}'

              for match in re.finditer(message_pattern, text, re.DOTALL):
                  name = match.group(1)
                  body = match.group(2)

                  fields = []
                  # Field pattern: [repeated] type name = number;
                  field_pattern = r'(repeated\s+)?(\w+)\s+(\w+)\s*=\s*(\d+)\s*;'

                  for field_match in re.finditer(field_pattern, body):
                      repeated = bool(field_match.group(1))
                      field_type = field_match.group(2)
                      field_name = field_match.group(3)
                      field_number = int(field_match.group(4))

                      fields.append(Field(field_name, field_type, field_number, repeated))

                  messages.append(Message(name, fields, []))

              return messages

          # Example schema
          schema = '''
          message Person {
              string name = 1;
              int32 age = 2;
              repeated string emails = 3;
          }
          '''

          messages = parse_schema(schema)
          # Message(name='Person', fields=[
          #     Field(name='name', type='string', number=1),
          #     Field(name='age', type='int32', number=2),
          #     Field(name='emails', type='string', number=3, repeated=True)
          # ])
      pitfalls:
      - Field numbers must be unique within a message
      - Field numbers 19000-19999 are reserved
      - Nested messages need recursive parsing
      concepts:
      - Schema languages
      - Code generation
      - Parsing
      estimated_hours: 5-6
      deliverables:
      - Lexer tokenizing .proto file into meaningful tokens
      - Message definition parser extracting name and field list
      - Field definition parser extracting type, name, and number
      - Import statement handling for multi-file schemas
    - id: 4
      name: Message Serialization
      description: Serialize and deserialize messages according to schema.
      acceptance_criteria:
      - Serialize Python dict to binary bytes following schema definition
      - Deserialize binary bytes back to Python dict using schema
      - Handle missing optional fields by skipping during serialization
      - Handle repeated fields serializing each array element separately
      hints:
        level1: Fields can appear in any order. Repeated fields appear multiple times.
        level2: During decode, collect all values for repeated fields into a list.
        level3: |-
          class Serializer:
              def __init__(self, messages: list[Message]):
                  self.messages = {m.name: m for m in messages}

              def serialize(self, message_name: str, data: dict) -> bytes:
                  '''Serialize dict to protobuf bytes'''
                  message = self.messages[message_name]
                  result = b''

                  for field in message.fields:
                      value = data.get(field.name)
                      if value is None:
                          continue

                      if field.repeated:
                          for item in value:
                              result += encode_field(field.number, item, field.type)
                      else:
                          result += encode_field(field.number, value, field.type)

                  return result

              def deserialize(self, message_name: str, data: bytes) -> dict:
                  '''Deserialize protobuf bytes to dict'''
                  message = self.messages[message_name]
                  field_map = {f.number: f for f in message.fields}
                  result = {}

                  offset = 0
                  while offset < len(data):
                      field_number, wire_type, consumed = decode_field_key(data, offset)
                      offset += consumed

                      field = field_map.get(field_number)
                      if not field:
                          # Skip unknown field
                          offset += skip_field(data, offset, wire_type)
                          continue

                      value, consumed = decode_value(data, offset, wire_type, field.type)
                      offset += consumed

                      if field.repeated:
                          if field.name not in result:
                              result[field.name] = []
                          result[field.name].append(value)
                      else:
                          result[field.name] = value

                  return result

          # Usage
          serializer = Serializer(parse_schema(schema))
          data = {'name': 'Alice', 'age': 30, 'emails': ['a@b.com', 'c@d.com']}
          encoded = serializer.serialize('Person', data)
          decoded = serializer.deserialize('Person', encoded)
      pitfalls:
      - Field order in wire format is arbitrary
      - Unknown fields should be skipped, not error
      - Empty repeated field = no entries (not encoded)
      concepts:
      - Serialization
      - Schema-driven encoding
      - Forward compatibility
      estimated_hours: 6-8
      deliverables:
      - Field encoding writing typed values with field key prefix
      - Complete message encoding serializing all fields to bytes
      - Nested message handling encoding sub-messages as length-delimited
      - Repeated field encoding writing each element with same field number
  query-optimizer:
    id: query-optimizer
    name: Query Optimizer
    description: Build a basic query optimizer. Learn cost estimation, join ordering, and query plans.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - SQL Parser
    - Database fundamentals
    - Algorithm complexity
    languages:
      recommended:
      - Python
      - Java
      - Go
      also_possible:
      - Rust
      - C++
    resources:
    - name: CMU Database Course
      url: https://15445.courses.cs.cmu.edu/
      type: course
    - name: Query Optimization Survey
      url: https://www.vldb.org/pvldb/vol14/p3025-yang.pdf
      type: paper
    milestones:
    - id: 1
      name: Query Plan Representation
      description: Define query plan tree structure.
      acceptance_criteria:
      - Represent plan as tree of operator nodes (Scan, Filter, Join, Project)
      - Support tree structure with parent-child operator relationships
      - Distinguish physical operators from logical operators
      - Pretty-print query plan as indented tree for debugging
      hints:
        level1: Plan = tree of operators. Each node has children and produces rows.
        level2: 'Logical: what to compute. Physical: how to compute (e.g., HashJoin vs NestedLoopJoin).'
        level3: "from dataclasses import dataclass\nfrom typing import List, Optional\nfrom abc import ABC, abstractmethod\n\
          \nclass PlanNode(ABC):\n    @abstractmethod\n    def children(self) -> List['PlanNode']:\n        pass\n    \n \
          \   @abstractmethod\n    def __str__(self) -> str:\n        pass\n\n@dataclass\nclass SeqScan(PlanNode):\n    table:\
          \ str\n    alias: str = None\n    \n    def children(self): return []\n    def __str__(self): return f'SeqScan({self.table})'\n\
          \n@dataclass\nclass IndexScan(PlanNode):\n    table: str\n    index: str\n    predicate: 'Expr'\n    \n    def children(self):\
          \ return []\n    def __str__(self): return f'IndexScan({self.table}, {self.index})'\n\n@dataclass\nclass Filter(PlanNode):\n\
          \    predicate: 'Expr'\n    child: PlanNode\n    \n    def children(self): return [self.child]\n    def __str__(self):\
          \ return f'Filter({self.predicate})'\n\n@dataclass\nclass HashJoin(PlanNode):\n    left: PlanNode\n    right: PlanNode\n\
          \    condition: 'Expr'\n    \n    def children(self): return [self.left, self.right]\n    def __str__(self): return\
          \ f'HashJoin({self.condition})'\n\n@dataclass\nclass NestedLoopJoin(PlanNode):\n    left: PlanNode\n    right: PlanNode\n\
          \    condition: 'Expr'\n    \n    def children(self): return [self.left, self.right]\n    def __str__(self): return\
          \ f'NLJoin({self.condition})'\n\n@dataclass\nclass Project(PlanNode):\n    columns: List[str]\n    child: PlanNode\n\
          \    \n    def children(self): return [self.child]\n    def __str__(self): return f'Project({self.columns})'\n\n\
          def print_plan(node: PlanNode, indent=0):\n    print(' ' * indent + str(node))\n    for child in node.children():\n\
          \        print_plan(child, indent + 2)"
      pitfalls:
      - Missing operators
      - Tree vs DAG
      - Operator semantics
      concepts:
      - Query plans
      - Operators
      - Plan trees
      estimated_hours: 3-4
      deliverables:
      - Plan tree data structure with operator node hierarchy
      - Physical operator nodes for Scan, Filter, Join, and Project
      - Cost annotation attached to each plan node
      - Plan visualization printing indented tree representation
    - id: 2
      name: Cost Estimation
      description: Estimate the cost of query plans.
      acceptance_criteria:
      - Maintain table statistics including row count and distinct values per column
      - Estimate filter selectivity based on column statistics and operator type
      - Estimate join output cardinality using input sizes and selectivity
      - Compute total plan cost combining I/O pages and CPU operations
      hints:
        level1: Cost = f(cardinality, I/O, CPU). Need statistics about data.
        level2: 'Selectivity: fraction of rows that pass a filter. Default = 0.1 for unknown.'
        level3: "@dataclass\nclass TableStats:\n    row_count: int\n    distinct_values: dict  # column -> count\n    min_values:\
          \ dict       # column -> min\n    max_values: dict       # column -> max\n\nclass CostEstimator:\n    def __init__(self,\
          \ stats: dict):\n        self.stats = stats  # table_name -> TableStats\n    \n    def estimate_cardinality(self,\
          \ node: PlanNode) -> float:\n        if isinstance(node, SeqScan):\n            return self.stats[node.table].row_count\n\
          \        \n        elif isinstance(node, Filter):\n            child_card = self.estimate_cardinality(node.child)\n\
          \            selectivity = self.estimate_selectivity(node.predicate, node.child)\n            return child_card\
          \ * selectivity\n        \n        elif isinstance(node, (HashJoin, NestedLoopJoin)):\n            left_card = self.estimate_cardinality(node.left)\n\
          \            right_card = self.estimate_cardinality(node.right)\n            selectivity = self.estimate_join_selectivity(node.condition)\n\
          \            return left_card * right_card * selectivity\n        \n        return 1000  # Default\n    \n    def\
          \ estimate_selectivity(self, predicate, source) -> float:\n        # col = constant\n        if isinstance(predicate,\
          \ Comparison) and predicate.op == '=':\n            if isinstance(predicate.left, Identifier):\n               \
          \ col = predicate.left.name\n                table = self.get_table(source)\n                if col in self.stats[table].distinct_values:\n\
          \                    return 1.0 / self.stats[table].distinct_values[col]\n        \n        # col < constant (range\
          \ query)\n        if isinstance(predicate, Comparison) and predicate.op in ('<', '>'):\n            return 0.33\
          \  # Default for range\n        \n        return 0.1  # Default selectivity\n    \n    def estimate_cost(self, node:\
          \ PlanNode) -> float:\n        cardinality = self.estimate_cardinality(node)\n        \n        if isinstance(node,\
          \ SeqScan):\n            # I/O cost = pages to read\n            pages = cardinality / 100  # Assume 100 rows per\
          \ page\n            return pages\n        \n        elif isinstance(node, HashJoin):\n            left_cost = self.estimate_cost(node.left)\n\
          \            right_cost = self.estimate_cost(node.right)\n            # Build hash table + probe\n            return\
          \ left_cost + right_cost + cardinality * 0.01\n        \n        elif isinstance(node, NestedLoopJoin):\n      \
          \      left_cost = self.estimate_cost(node.left)\n            right_cost = self.estimate_cost(node.right)\n    \
          \        left_card = self.estimate_cardinality(node.left)\n            # Nested loop: scan right for each left row\n\
          \            return left_cost + left_card * right_cost\n        \n        # Sum children costs\n        return sum(self.estimate_cost(c)\
          \ for c in node.children())"
      pitfalls:
      - Statistics staleness
      - Correlation assumptions
      - Estimation errors
      concepts:
      - Cost models
      - Selectivity
      - Cardinality estimation
      estimated_hours: 5-7
      deliverables:
      - Statistics collection including row counts and column histograms
      - Selectivity estimation for filter predicates using statistics
      - I/O and CPU cost model with configurable weight parameters
      - Join cardinality estimation using column distinct value counts
    - id: 3
      name: Join Ordering
      description: Find optimal join order for multi-table queries.
      acceptance_criteria:
      - Enumerate all possible join orderings for multi-table queries
      - Use dynamic programming to find the minimum cost join order
      - Support both left-deep and bushy join tree plan shapes
      - Prune clearly suboptimal plans early to reduce search space
      hints:
        level1: Join order matters! n tables = n! possible orders.
        level2: 'DP: build optimal plans for subsets, combine for larger sets.'
        level3: "from itertools import combinations\n\nclass JoinOrderOptimizer:\n    def __init__(self, estimator: CostEstimator):\n\
          \        self.estimator = estimator\n        self.memo = {}  # frozenset(tables) -> best_plan\n    \n    def optimize_joins(self,\
          \ tables: List[str], predicates: List['Expr']) -> PlanNode:\n        '''Find optimal join order using dynamic programming'''\n\
          \        # Base case: single tables\n        for table in tables:\n            key = frozenset([table])\n      \
          \      self.memo[key] = SeqScan(table)\n        \n        # Build up from 2 tables to all tables\n        for size\
          \ in range(2, len(tables) + 1):\n            for subset in combinations(tables, size):\n                subset_key\
          \ = frozenset(subset)\n                self.find_best_join(subset_key, predicates)\n        \n        return self.memo[frozenset(tables)]\n\
          \    \n    def find_best_join(self, tables: frozenset, predicates: List['Expr']):\n        best_plan = None\n  \
          \      best_cost = float('inf')\n        \n        # Try all ways to split tables into two non-empty subsets\n \
          \       for i in range(1, len(tables)):\n            for left_tables in combinations(tables, i):\n             \
          \   left_key = frozenset(left_tables)\n                right_key = tables - left_key\n                \n       \
          \         if left_key not in self.memo or right_key not in self.memo:\n                    continue\n          \
          \      \n                left_plan = self.memo[left_key]\n                right_plan = self.memo[right_key]\n  \
          \              \n                # Find applicable join predicate\n                join_pred = self.find_join_predicate(left_key,\
          \ right_key, predicates)\n                \n                # Try different join algorithms\n                for\
          \ join_type in [HashJoin, NestedLoopJoin]:\n                    plan = join_type(left_plan, right_plan, join_pred)\n\
          \                    cost = self.estimator.estimate_cost(plan)\n                    \n                    if cost\
          \ < best_cost:\n                        best_cost = cost\n                        best_plan = plan\n        \n \
          \       self.memo[tables] = best_plan"
      pitfalls:
      - Exponential complexity
      - Cross joins
      - Missing predicates
      concepts:
      - Join ordering
      - Dynamic programming
      - Plan enumeration
      estimated_hours: 6-8
      deliverables:
      - Dynamic programming approach enumerating optimal join subsets
      - Cross product elimination pruning joins without predicates
      - Join cost estimation comparing different join pair orderings
      - Support for both bushy and left-deep plan tree shapes
    - id: 4
      name: Plan Selection
      description: Choose between physical operators and access methods.
      acceptance_criteria:
      - Select between sequential scan and index scan based on selectivity
      - Choose join algorithm (hash join vs nested loop) based on input sizes
      - Push filter predicates below joins to reduce intermediate result sizes
      - Generate final physical execution plan with all operators selected
      hints:
        level1: Use index if selective enough. Push filters down before joins.
        level2: HashJoin good for large joins. NestedLoop good with index on inner.
        level3: "class QueryOptimizer:\n    def __init__(self, stats: dict, indexes: dict):\n        self.estimator = CostEstimator(stats)\n\
          \        self.indexes = indexes  # table -> [index_info]\n    \n    def optimize(self, query: SelectStmt) -> PlanNode:\n\
          \        # 1. Generate initial logical plan\n        plan = self.logical_plan(query)\n        \n        # 2. Predicate\
          \ pushdown\n        plan = self.pushdown_predicates(plan)\n        \n        # 3. Join ordering\n        if self.has_joins(plan):\n\
          \            tables = self.extract_tables(plan)\n            predicates = self.extract_join_predicates(plan)\n \
          \           plan = JoinOrderOptimizer(self.estimator).optimize_joins(tables, predicates)\n        \n        # 4.\
          \ Physical operator selection\n        plan = self.select_physical_operators(plan)\n        \n        return plan\n\
          \    \n    def select_physical_operators(self, node: PlanNode) -> PlanNode:\n        if isinstance(node, SeqScan):\n\
          \            # Check if index scan is better\n            return self.select_access_method(node)\n        \n   \
          \     elif isinstance(node, Filter):\n            child = self.select_physical_operators(node.child)\n         \
          \   return Filter(node.predicate, child)\n        \n        elif isinstance(node, (HashJoin, NestedLoopJoin)):\n\
          \            left = self.select_physical_operators(node.left)\n            right = self.select_physical_operators(node.right)\n\
          \            return self.select_join_method(left, right, node.condition)\n        \n        return node\n    \n\
          \    def select_access_method(self, scan: SeqScan) -> PlanNode:\n        # Check for applicable indexes\n      \
          \  if scan.table in self.indexes:\n            for idx in self.indexes[scan.table]:\n                # If there's\
          \ a selective predicate on indexed column\n                # return IndexScan instead\n                pass\n  \
          \      return scan\n    \n    def select_join_method(self, left, right, condition) -> PlanNode:\n        # Hash\
          \ join for equi-joins on large tables\n        # Nested loop for small tables or with index\n        left_card =\
          \ self.estimator.estimate_cardinality(left)\n        right_card = self.estimator.estimate_cardinality(right)\n \
          \       \n        if left_card > 1000 and right_card > 1000:\n            return HashJoin(left, right, condition)\n\
          \        else:\n            return NestedLoopJoin(left, right, condition)"
      pitfalls:
      - Over-optimization
      - Plan caching
      - Statistics accuracy
      concepts:
      - Physical planning
      - Index selection
      - Predicate pushdown
      estimated_hours: 5-7
      deliverables:
      - Dynamic programming for optimal join order selection
      - Heuristic pruning rules eliminating poor plan candidates
      - Best plan selection comparing costs across physical alternatives
      - Plan caching storing optimized plans for repeated queries
  rate-limiter:
    id: rate-limiter
    name: Rate Limiter
    description: Build a rate limiter using the token bucket algorithm. Learn about request throttling and protecting services
      from abuse.
    difficulty: intermediate
    estimated_hours: 10-15
    prerequisites:
    - Basic web server knowledge
    - Concurrency basics
    - Time handling
    languages:
      recommended:
      - Python
      - Go
      - JavaScript
      also_possible:
      - Java
      - Rust
    resources:
    - name: Token Bucket Algorithm
      url: https://en.wikipedia.org/wiki/Token_bucket
      type: documentation
    - name: Rate Limiting Strategies
      url: https://blog.bytebytego.com/p/rate-limiting-fundamentals
      type: article
    milestones:
    - id: 1
      name: Token Bucket Implementation
      description: Implement the core token bucket algorithm.
      acceptance_criteria:
      - Configure bucket with maximum capacity and tokens-per-second rate
      - Add tokens at configurable rate based on elapsed time since last refill
      - Consume specified number of tokens per incoming request
      - Return allow or deny decision based on token availability
      - Implement thread-safe access using locks or atomic operations
      hints:
        level1: Track tokens and last refill time.
        level2: Refill tokens on each request based on elapsed time.
        level3: |-
          import time, threading

          class TokenBucket:
              def __init__(self, capacity, refill_rate):
                  self.capacity = capacity
                  self.tokens = capacity
                  self.refill_rate = refill_rate
                  self.last_refill = time.time()
                  self.lock = threading.Lock()

              def _refill(self):
                  now = time.time()
                  elapsed = now - self.last_refill
                  tokens_to_add = elapsed * self.refill_rate
                  self.tokens = min(self.capacity, self.tokens + tokens_to_add)
                  self.last_refill = now

              def consume(self, tokens=1):
                  with self.lock:
                      self._refill()
                      if self.tokens >= tokens:
                          self.tokens -= tokens
                          return True
                      return False
      pitfalls:
      - Race conditions without locking
      - Integer overflow in token calculation
      - Clock drift issues
      concepts:
      - Token bucket algorithm
      - Thread safety
      - Rate calculations
      estimated_hours: 3-4
      deliverables:
      - Configurable bucket capacity and refill rate parameters
      - Token consumption function deducting from available tokens
      - Token refill logic calculating additions based on elapsed time
      - Burst handling allowing short bursts up to bucket capacity
    - id: 2
      name: Per-Client Rate Limiting
      description: Track rate limits per client (IP or API key).
      acceptance_criteria:
      - Maintain separate rate limit bucket for each client
      - Identify clients by IP address or API key header
      - Clean up stale buckets not accessed within timeout period
      - Store buckets efficiently to handle large numbers of clients
      - Support configurable per-client limit overrides for premium tiers
      hints:
        level1: 'Use dict/map: client_id -> TokenBucket.'
        level2: Expire buckets not used for N minutes to save memory.
        level3: |-
          class RateLimiter:
              def __init__(self, capacity, refill_rate):
                  self.capacity = capacity
                  self.refill_rate = refill_rate
                  self.buckets = {}
                  self.lock = threading.Lock()

              def is_allowed(self, client_id):
                  with self.lock:
                      if client_id not in self.buckets:
                          self.buckets[client_id] = TokenBucket(self.capacity, self.refill_rate)
                      return self.buckets[client_id].consume()
      pitfalls:
      - Memory leak from never cleaning buckets
      - Lock contention under load
      - Client spoofing bypassing limits
      concepts:
      - Per-client tracking
      - Memory management
      - Background cleanup
      estimated_hours: 2-3
      deliverables:
      - Client identification by IP address or API key
      - Separate token bucket instance per unique client
      - Memory-efficient storage with automatic stale bucket cleanup
      - Background cleanup task removing expired client buckets
    - id: 3
      name: HTTP Middleware Integration
      description: Integrate rate limiter as HTTP middleware.
      acceptance_criteria:
      - Middleware intercepts all HTTP requests and checks rate limit
      - Return HTTP 429 Too Many Requests when client exceeds limit
      - Include Retry-After header indicating seconds until next allowed request
      - Include X-RateLimit-Limit and X-RateLimit-Remaining headers in responses
      - Integrate with Express, Flask, or equivalent web framework
      hints:
        level1: Middleware checks rate limiter, proceeds or returns 429.
        level2: Calculate Retry-After from time until next token.
        level3: |-
          # Flask middleware example
          @app.before_request
          def rate_limit():
              client_ip = request.remote_addr
              if not limiter.is_allowed(client_ip):
                  response = jsonify({'error': 'Too many requests'})
                  response.status_code = 429
                  response.headers['Retry-After'] = str(limiter.get_retry_after(client_ip))
                  response.headers['X-RateLimit-Limit'] = str(limiter.capacity)
                  response.headers['X-RateLimit-Remaining'] = '0'
                  return response
      pitfalls:
      - Not returning proper status code
      - Missing Retry-After header
      - Rate limit headers only on 429
      concepts:
      - HTTP middleware
      - Rate limit headers
      - 429 response
      estimated_hours: 2-3
      deliverables:
      - Rate limit middleware function intercepting HTTP requests
      - Standard rate limit headers in every HTTP response
      - HTTP 429 Too Many Requests response with error body
      - Configurable limit rules per endpoint or route pattern
    - id: 4
      name: Distributed Rate Limiting
      description: Scale rate limiter across multiple server instances.
      acceptance_criteria:
      - Share rate limit state across multiple server instances via Redis
      - Use Redis-backed storage for token bucket or sliding window state
      - Perform atomic read-modify-write operations using Lua scripts
      - Handle Redis connection failures gracefully with local fallback
      - Maintain consistency under high concurrent request load
      hints:
        level1: Store bucket state in Redis instead of memory.
        level2: Use Redis transactions (MULTI/EXEC) or Lua scripts.
        level3: |-
          # Redis Lua script for atomic token bucket
          SCRIPT = '''
          local key = KEYS[1]
          local capacity = tonumber(ARGV[1])
          local refill_rate = tonumber(ARGV[2])
          local now = tonumber(ARGV[3])
          local requested = tonumber(ARGV[4])

          local bucket = redis.call('HMGET', key, 'tokens', 'last_refill')
          local tokens = tonumber(bucket[1]) or capacity
          local last_refill = tonumber(bucket[2]) or now

          local elapsed = now - last_refill
          tokens = math.min(capacity, tokens + elapsed * refill_rate)

          local allowed = 0
          if tokens >= requested then
              tokens = tokens - requested
              allowed = 1
          end

          redis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)
          redis.call('EXPIRE', key, 3600)
          return {allowed, tokens}
          '''
      pitfalls:
      - Non-atomic read-modify-write
      - Redis connection failures
      - Clock sync between servers
      concepts:
      - Distributed state
      - Redis Lua scripts
      - Atomic operations
      estimated_hours: 3-4
      deliverables:
      - Redis-based token counter storage shared across instances
      - Atomic increment operations using Redis Lua scripts
      - Sliding window rate limit implementation in Redis
      - Cluster-wide rate limits consistent across all server instances
  replicated-log:
    id: replicated-log
    name: Replicated Log
    description: Build a replicated append-only log. Learn distributed replication, consistency, and failure handling.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - RPC basics
    - Networking
    - Log-based storage
    languages:
      recommended:
      - Go
      - Rust
      - Java
      also_possible:
      - Python
      - C
    resources:
    - type: paper
      name: Viewstamped Replication
      url: https://pmg.csail.mit.edu/papers/vr-revisited.pdf
    - type: blog
      name: Distributed Log 101
      url: https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying
    milestones:
    - id: 1
      name: Log Storage
      description: Implement an append-only log with indexing.
      acceptance_criteria:
      - Append entries with monotonically increasing sequence numbers
      - Read entries by sequence index with O(1) lookup time
      - Persist all entries to disk surviving process restarts
      - Handle log compaction to reclaim disk space from old entries
      hints:
        level1: Each entry has monotonic sequence number. Store entries sequentially in file.
        level2: Use index file for O(1) lookups. mmap for efficient reads.
        level3: "import struct\nimport os\n\nclass Log:\n    HEADER_SIZE = 12  # seq(8) + len(4)\n    \n    def __init__(self,\
          \ path):\n        self.path = path\n        self.data_file = open(f'{path}/data', 'ab+')\n        self.index = {}\
          \  # seq -> file_offset\n        self.next_seq = 0\n        self._rebuild_index()\n    \n    def _rebuild_index(self):\n\
          \        self.data_file.seek(0)\n        offset = 0\n        while True:\n            header = self.data_file.read(self.HEADER_SIZE)\n\
          \            if len(header) < self.HEADER_SIZE:\n                break\n            seq, length = struct.unpack('>QI',\
          \ header)\n            self.index[seq] = offset\n            self.next_seq = seq + 1\n            offset += self.HEADER_SIZE\
          \ + length\n            self.data_file.seek(offset)\n    \n    def append(self, data: bytes) -> int:\n        seq\
          \ = self.next_seq\n        self.next_seq += 1\n        \n        offset = self.data_file.seek(0, 2)  # EOF\n   \
          \     header = struct.pack('>QI', seq, len(data))\n        self.data_file.write(header + data)\n        self.data_file.flush()\n\
          \        os.fsync(self.data_file.fileno())\n        \n        self.index[seq] = offset\n        return seq\n   \
          \ \n    def read(self, seq: int) -> bytes:\n        if seq not in self.index:\n            return None\n       \
          \ offset = self.index[seq]\n        self.data_file.seek(offset)\n        header = self.data_file.read(self.HEADER_SIZE)\n\
          \        _, length = struct.unpack('>QI', header)\n        return self.data_file.read(length)"
      pitfalls:
      - Partial writes on crash
      - Index corruption
      - File handle limits
      concepts:
      - Append-only logs
      - Sequence numbers
      - Durability
      estimated_hours: 3-5
      deliverables:
      - Append-only log structure with sequential write semantics
      - Log entry format with sequence number, length, and data payload
      - Durable log persistence with fsync to disk after each write
      - Log compaction removing old entries beyond retention window
    - id: 2
      name: Replication Protocol
      description: Implement primary-backup replication with follower sync.
      acceptance_criteria:
      - Primary node accepts all write operations from clients
      - Replicate each new log entry to all follower nodes
      - Acknowledge writes only after quorum of replicas confirm
      - Handle follower lag by sending missing entries on reconnect
      hints:
        level1: Primary assigns sequence, sends to followers. Wait for majority before ack.
        level2: Followers send their last seq. Primary sends missing entries.
        level3: "class Primary:\n    def __init__(self, log, followers):\n        self.log = log\n        self.followers =\
          \ followers  # List of follower RPC clients\n        self.follower_progress = {f.id: 0 for f in followers}\n   \
          \ \n    async def append(self, data: bytes) -> int:\n        # Append locally\n        seq = self.log.append(data)\n\
          \        \n        # Replicate to followers\n        acks = 1  # Self\n        tasks = []\n        for follower\
          \ in self.followers:\n            tasks.append(self._replicate_to(follower, seq, data))\n        \n        results\
          \ = await asyncio.gather(*tasks, return_exceptions=True)\n        acks += sum(1 for r in results if r is True)\n\
          \        \n        # Check quorum (majority)\n        quorum = (len(self.followers) + 1) // 2 + 1\n        if acks\
          \ >= quorum:\n            return seq\n        raise ReplicationError('Failed to reach quorum')\n    \n    async\
          \ def _replicate_to(self, follower, seq, data):\n        try:\n            # Check if follower is behind\n     \
          \       follower_seq = self.follower_progress[follower.id]\n            if follower_seq < seq - 1:\n           \
          \     # Send missing entries\n                for s in range(follower_seq + 1, seq):\n                    entry\
          \ = self.log.read(s)\n                    await follower.append(s, entry)\n            \n            # Send current\
          \ entry\n            await follower.append(seq, data)\n            self.follower_progress[follower.id] = seq\n \
          \           return True\n        except Exception as e:\n            return False\n\nclass Follower:\n    def __init__(self,\
          \ log):\n        self.log = log\n    \n    def append(self, seq, data):\n        expected = self.log.next_seq\n\
          \        if seq != expected:\n            raise OutOfOrderError(f'Expected {expected}, got {seq}')\n        self.log.append(data)\n\
          \        return True"
      pitfalls:
      - Split brain without leader election
      - Replication lag
      - Network partitions
      concepts:
      - Primary-backup
      - Quorum
      - Replication lag
      estimated_hours: 5-8
      deliverables:
      - Leader election mechanism selecting primary from cluster nodes
      - Log replication sending entries from primary to follower nodes
      - Consistency guarantees ensuring all replicas converge to same state
      - Quorum write acknowledgment requiring majority before confirming
    - id: 3
      name: Failure Detection
      description: Detect node failures and handle recovery.
      acceptance_criteria:
      - Send periodic heartbeat messages between cluster nodes
      - Detect node failure by heartbeat timeout expiration
      - Catch up follower after recovery by replaying missed entries
      - Handle primary failure by notifying remaining cluster members
      hints:
        level1: Followers send periodic heartbeats. Primary tracks last seen time.
        level2: On recovery, follower requests entries from last known seq.
        level3: "class FailureDetector:\n    def __init__(self, timeout_ms=5000):\n        self.timeout = timeout_ms / 1000\n\
          \        self.last_heartbeat = {}  # node_id -> timestamp\n        self.suspected = set()\n    \n    def heartbeat_received(self,\
          \ node_id):\n        self.last_heartbeat[node_id] = time.time()\n        if node_id in self.suspected:\n       \
          \     self.suspected.remove(node_id)\n            self.on_node_recovered(node_id)\n    \n    def check_timeouts(self):\n\
          \        now = time.time()\n        for node_id, last in self.last_heartbeat.items():\n            if now - last\
          \ > self.timeout and node_id not in self.suspected:\n                self.suspected.add(node_id)\n             \
          \   self.on_node_suspected(node_id)\n\nclass RecoveringFollower:\n    def __init__(self, log, primary_client):\n\
          \        self.log = log\n        self.primary = primary_client\n    \n    async def catch_up(self):\n        # Get\
          \ our last sequence\n        my_last = self.log.next_seq - 1 if self.log.next_seq > 0 else -1\n        \n      \
          \  # Request entries from primary\n        entries = await self.primary.get_entries_since(my_last + 1)\n       \
          \ \n        for seq, data in entries:\n            if seq != self.log.next_seq:\n                raise ConsistencyError('Gap\
          \ in log')\n            self.log.append(data)\n        \n        print(f'Caught up to seq {self.log.next_seq - 1}')"
      pitfalls:
      - False positives in detection
      - Thundering herd on recovery
      - Consistency after catch-up
      concepts:
      - Failure detection
      - Heartbeats
      - Recovery protocols
      estimated_hours: 4-6
      deliverables:
      - Heartbeat mechanism with periodic liveness signals between nodes
      - Failure timeout triggering node suspected status after missed heartbeats
      - Leader takeover process promoting follower when primary fails
      - Split-brain prevention ensuring only one active primary at any time
    - id: 4
      name: Client Interface
      description: Implement a client that handles primary discovery and failover.
      acceptance_criteria:
      - Discover and connect to current primary node automatically
      - Retry failed operations with automatic primary rediscovery
      - Optionally read from follower replicas for lower latency
      - Provide consistent reads by verifying data freshness against primary
      hints:
        level1: Client caches primary address. On error, re-discover.
        level2: For reads, can go to any replica if staleness is acceptable.
        level3: "class LogClient:\n    def __init__(self, cluster_nodes):\n        self.nodes = cluster_nodes\n        self.primary\
          \ = None\n        self.primary_addr = None\n    \n    async def discover_primary(self):\n        for addr in self.nodes:\n\
          \            try:\n                client = await connect(addr)\n                info = await client.get_info()\n\
          \                if info['role'] == 'primary':\n                    self.primary = client\n                    self.primary_addr\
          \ = addr\n                    return\n                elif info.get('primary_addr'):\n                    # Follower\
          \ knows who primary is\n                    self.primary = await connect(info['primary_addr'])\n               \
          \     self.primary_addr = info['primary_addr']\n                    return\n            except Exception:\n    \
          \            continue\n        raise NoPrimaryError('Cannot find primary')\n    \n    async def append(self, data:\
          \ bytes, retries=3) -> int:\n        for attempt in range(retries):\n            try:\n                if not self.primary:\n\
          \                    await self.discover_primary()\n                return await self.primary.append(data)\n   \
          \         except (ConnectionError, PrimaryChangedError):\n                self.primary = None\n                if\
          \ attempt == retries - 1:\n                    raise\n    \n    async def read(self, seq: int, allow_stale=False)\
          \ -> bytes:\n        if allow_stale:\n            # Read from any node\n            for addr in self.nodes:\n  \
          \              try:\n                    client = await connect(addr)\n                    return await client.read(seq)\n\
          \                except Exception:\n                    continue\n        \n        # Read from primary for consistency\n\
          \        if not self.primary:\n            await self.discover_primary()\n        return await self.primary.read(seq)"
      pitfalls:
      - Stale primary cache
      - Read-your-writes consistency
      - Infinite retry loops
      concepts:
      - Service discovery
      - Client-side failover
      - Consistency levels
      estimated_hours: 3-6
      deliverables:
      - Append API for writing new entries to the replicated log
      - Read API for fetching entries by sequence number or range
      - Subscribe-to-updates API streaming new entries to consumers
      - Consistency options supporting strong and eventual read modes
  rpc-basic:
    id: rpc-basic
    name: RPC Framework (Basic)
    description: Build a simple Remote Procedure Call framework. Learn serialization, network protocols, and client-server
      patterns.
    difficulty: beginner
    estimated_hours: 10-15
    prerequisites:
    - TCP sockets
    - JSON/serialization
    - Client-server architecture
    languages:
      recommended:
      - Python
      - Go
      - Java
      also_possible:
      - JavaScript
      - Rust
    resources:
    - name: gRPC Concepts
      url: https://grpc.io/docs/what-is-grpc/core-concepts/
      type: documentation
    - name: JSON-RPC Spec
      url: https://www.jsonrpc.org/specification
      type: specification
    milestones:
    - id: 1
      name: Message Protocol
      description: Define request/response message format.
      acceptance_criteria:
      - Request includes method name, parameter list, and unique request ID
      - Response includes result or error paired with matching request ID
      - Serialize and deserialize messages using JSON encoding
      - Handle different parameter types including strings, numbers, and objects
      hints:
        level1: Request = {method, params, id}. Response = {result, error, id}.
        level2: Use JSON for simplicity. Match request ID in response.
        level3: "import json\nfrom dataclasses import dataclass, asdict\nfrom typing import Any, Optional, List\n\n@dataclass\n\
          class RPCRequest:\n    method: str\n    params: List[Any]\n    id: int\n    \n    def to_json(self) -> str:\n  \
          \      return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls, data: str) -> 'RPCRequest':\n\
          \        d = json.loads(data)\n        return cls(d['method'], d['params'], d['id'])\n\n@dataclass\nclass RPCResponse:\n\
          \    id: int\n    result: Any = None\n    error: Optional[str] = None\n    \n    def to_json(self) -> str:\n   \
          \     return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls, data: str) -> 'RPCResponse':\n\
          \        d = json.loads(data)\n        return cls(d['id'], d.get('result'), d.get('error'))"
      pitfalls:
      - ID mismatch
      - Serialization errors
      - Missing error handling
      concepts:
      - RPC protocol
      - Message serialization
      - Request-response pattern
      estimated_hours: 2-3
      deliverables:
      - Request message format with method name, parameters, and ID
      - Response message format with result, error, and matching ID
      - Error format with code, message, and optional data fields
      - Serialization using JSON or msgpack for wire encoding
    - id: 2
      name: Server Implementation
      description: Build RPC server that handles method calls.
      acceptance_criteria:
      - Register callable functions by name in method registry
      - Parse incoming request messages extracting method and parameters
      - Execute registered method with provided parameters and return result
      - Handle method-not-found and execution errors with error responses
      hints:
        level1: Server maintains registry of method_name -> function.
        level2: Receive request, lookup method, call with params, send response.
        level3: "import socket\nimport threading\n\nclass RPCServer:\n    def __init__(self, host='localhost', port=8000):\n\
          \        self.host = host\n        self.port = port\n        self.methods = {}\n    \n    def register(self, name:\
          \ str, func):\n        self.methods[name] = func\n    \n    def handle_client(self, conn, addr):\n        try:\n\
          \            while True:\n                data = conn.recv(4096)\n                if not data:\n               \
          \     break\n                \n                request = RPCRequest.from_json(data.decode())\n                response\
          \ = self.execute(request)\n                conn.sendall(response.to_json().encode())\n        finally:\n       \
          \     conn.close()\n    \n    def execute(self, request: RPCRequest) -> RPCResponse:\n        try:\n           \
          \ if request.method not in self.methods:\n                return RPCResponse(request.id, error=f'Method not found:\
          \ {request.method}')\n            \n            func = self.methods[request.method]\n            result = func(*request.params)\n\
          \            return RPCResponse(request.id, result=result)\n        except Exception as e:\n            return RPCResponse(request.id,\
          \ error=str(e))\n    \n    def serve(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\
          \        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        sock.bind((self.host, self.port))\n\
          \        sock.listen(5)\n        print(f'RPC Server listening on {self.host}:{self.port}')\n        \n        while\
          \ True:\n            conn, addr = sock.accept()\n            thread = threading.Thread(target=self.handle_client,\
          \ args=(conn, addr))\n            thread.start()"
      pitfalls:
      - Method not found
      - Parameter count mismatch
      - Blocking calls
      concepts:
      - Method registry
      - Request dispatching
      - Error handling
      estimated_hours: 3-4
      deliverables:
      - TCP server listening for incoming client connections
      - Method registry mapping function names to callable handlers
      - Request dispatch parsing message and invoking registered method
      - Response serialization and sending back to connected client
    - id: 3
      name: Client Implementation
      description: Build RPC client with method proxying.
      acceptance_criteria:
      - Connect to RPC server via TCP socket connection
      - Send request messages and wait for matching response by ID
      - Provide proxy object allowing natural method call syntax
      - Handle call timeouts raising exception after deadline expires
      hints:
        level1: Client sends request, blocks until response with matching ID.
        level2: Use __getattr__ to proxy method calls dynamically.
        level3: "class RPCClient:\n    def __init__(self, host='localhost', port=8000):\n        self.host = host\n      \
          \  self.port = port\n        self.sock = None\n        self.request_id = 0\n    \n    def connect(self):\n     \
          \   self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.connect((self.host, self.port))\n\
          \    \n    def close(self):\n        if self.sock:\n            self.sock.close()\n    \n    def call(self, method:\
          \ str, *params, timeout=30):\n        self.request_id += 1\n        request = RPCRequest(method, list(params), self.request_id)\n\
          \        \n        self.sock.settimeout(timeout)\n        self.sock.sendall(request.to_json().encode())\n      \
          \  \n        data = self.sock.recv(4096)\n        response = RPCResponse.from_json(data.decode())\n        \n  \
          \      if response.id != self.request_id:\n            raise Exception('Response ID mismatch')\n        \n     \
          \   if response.error:\n            raise Exception(response.error)\n        \n        return response.result\n\
          \    \n    def __getattr__(self, name):\n        '''Proxy method calls: client.add(1, 2) -> client.call('add', 1,\
          \ 2)'''\n        def method(*args):\n            return self.call(name, *args)\n        return method\n\n# Usage:\n\
          client = RPCClient()\nclient.connect()\nresult = client.add(1, 2)  # Calls remote 'add' method\nprint(result)  #\
          \ 3"
      pitfalls:
      - Connection management
      - Timeout handling
      - ID tracking
      concepts:
      - RPC client
      - Method proxying
      - Connection pooling
      estimated_hours: 3-4
      deliverables:
      - TCP client connecting to remote RPC server endpoint
      - Method call API sending request and blocking for response
      - Transparent response handling returning result or raising error
      - Configurable timeout aborting calls that take too long
  service-discovery:
    id: service-discovery
    name: Service Discovery
    description: Build a service registry with health checking. Learn how microservices find and communicate with each other.
    difficulty: beginner
    estimated_hours: 10-15
    prerequisites:
    - HTTP basics
    - Networking concepts
    languages:
      recommended:
      - Go
      - Python
      - JavaScript
      also_possible:
      - Java
      - Rust
    resources:
    - name: Service Discovery Patterns
      url: https://microservices.io/patterns/service-registry.html
      type: article
    milestones:
    - id: 1
      name: Service Registry
      description: Build basic service registration and lookup.
      acceptance_criteria:
      - Register service instance with name, host, port, and metadata
      - Deregister service instance by unique instance identifier
      - List all healthy instances of a service by service name
      - Return complete list of all registered services and instances
      hints:
        level1: 'Store services in a dict: name -> list of instances.'
        level2: Each instance has unique ID, host, port, metadata.
        level3: "import uuid\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n@dataclass\nclass ServiceInstance:\n\
          \    id: str\n    name: str\n    host: str\n    port: int\n    metadata: dict = None\n\nclass ServiceRegistry:\n\
          \    def __init__(self):\n        self.services: Dict[str, Dict[str, ServiceInstance]] = {}\n    \n    def register(self,\
          \ name: str, host: str, port: int, metadata: dict = None) -> str:\n        instance_id = str(uuid.uuid4())\n   \
          \     instance = ServiceInstance(instance_id, name, host, port, metadata or {})\n        if name not in self.services:\n\
          \            self.services[name] = {}\n        self.services[name][instance_id] = instance\n        return instance_id\n\
          \    \n    def deregister(self, name: str, instance_id: str):\n        if name in self.services and instance_id\
          \ in self.services[name]:\n            del self.services[name][instance_id]\n    \n    def get_instances(self, name:\
          \ str) -> List[ServiceInstance]:\n        return list(self.services.get(name, {}).values())"
      pitfalls:
      - Not handling duplicate registrations
      - Race conditions
      - Memory growth without cleanup
      concepts:
      - Service registry pattern
      - Instance identity
      - Lookup by name
      estimated_hours: 2-3
      deliverables:
      - Registration API for services to announce their availability
      - Service metadata storage including name, host, port, and tags
      - TTL-based automatic deregistration after heartbeat timeout
      - Health check integration marking unhealthy instances as unavailable
    - id: 2
      name: Health Checking
      description: Add health checks to detect failed services.
      acceptance_criteria:
      - Expose HTTP /health endpoint on each registered service
      - Poll health endpoints at configurable check intervals
      - Remove instances failing consecutive health checks from registry
      - Configure health check interval and failure threshold per service
      hints:
        level1: Each service exposes /health returning 200.
        level2: Background thread checks all instances periodically.
        level3: "import threading\nimport requests\nimport time\n\nclass ServiceRegistry:\n    def __init__(self, health_check_interval=10):\n\
          \        self.services = {}\n        self.health_check_interval = health_check_interval\n        self._start_health_checker()\n\
          \    \n    def _start_health_checker(self):\n        def check_loop():\n            while True:\n              \
          \  self._check_all_health()\n                time.sleep(self.health_check_interval)\n        thread = threading.Thread(target=check_loop,\
          \ daemon=True)\n        thread.start()\n    \n    def _check_all_health(self):\n        for name, instances in list(self.services.items()):\n\
          \            for instance_id, instance in list(instances.items()):\n                try:\n                    url\
          \ = f'http://{instance.host}:{instance.port}/health'\n                    resp = requests.get(url, timeout=5)\n\
          \                    if resp.status_code != 200:\n                        self._mark_unhealthy(name, instance_id)\n\
          \                except Exception:\n                    self._mark_unhealthy(name, instance_id)\n    \n    def _mark_unhealthy(self,\
          \ name, instance_id):\n        # Could implement retry logic or immediate removal\n        self.deregister(name,\
          \ instance_id)\n        print(f'Removed unhealthy instance: {name}/{instance_id}')"
      pitfalls:
      - Network partition false positives
      - Check timeout too short
      - Not handling transient failures
      concepts:
      - Health checking
      - Failure detection
      - Background tasks
      estimated_hours: 3-4
      deliverables:
      - Health check execution polling service endpoints periodically
      - Support for HTTP, TCP, and gRPC health check protocols
      - Unhealthy instance removal from active service registry
      - Health status API exposing current health of all instances
    - id: 3
      name: HTTP API
      description: Expose registry operations via HTTP API.
      acceptance_criteria:
      - POST /services registers new service instance and returns ID
      - DELETE /services/{name}/{id} deregisters the specified service instance
      - GET /services/{name} returns list of healthy service instances
      - GET /services returns complete list of all registered services
      hints:
        level1: Use Flask/FastAPI/Express for HTTP server.
        level2: Return JSON responses with proper status codes.
        level3: |-
          from flask import Flask, request, jsonify

          app = Flask(__name__)
          registry = ServiceRegistry()

          @app.route('/services', methods=['POST'])
          def register_service():
              data = request.json
              instance_id = registry.register(
                  name=data['name'],
                  host=data['host'],
                  port=data['port'],
                  metadata=data.get('metadata')
              )
              return jsonify({'id': instance_id}), 201

          @app.route('/services/<name>/<instance_id>', methods=['DELETE'])
          def deregister_service(name, instance_id):
              registry.deregister(name, instance_id)
              return '', 204

          @app.route('/services/<name>', methods=['GET'])
          def get_service(name):
              instances = registry.get_instances(name)
              return jsonify([{
                  'id': i.id,
                  'host': i.host,
                  'port': i.port,
                  'metadata': i.metadata
              } for i in instances])

          @app.route('/services', methods=['GET'])
          def list_all_services():
              return jsonify(list(registry.services.keys()))
      pitfalls:
      - Missing validation
      - Not thread-safe
      - Error handling
      concepts:
      - REST API design
      - HTTP methods
      - JSON serialization
      estimated_hours: 2-3
      deliverables:
      - Service lookup endpoint returning instances by service name
      - Health status endpoint showing current state of all services
      - Service registration endpoint accepting new instance announcements
      - Query filtering supporting lookup by tags or metadata attributes
  signal-handler:
    id: signal-handler
    name: Signal Handler
    description: Master Unix signal handling. Learn signal safety, masks, and graceful shutdown.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - C programming
    - Unix basics
    - Process concepts
    languages:
      recommended:
      - C
      - Rust
      also_possible:
      - Go
      - Python
    resources:
    - name: signal(7) man page
      url: https://man7.org/linux/man-pages/man7/signal.7.html
      type: documentation
    - name: Signal Safety
      url: https://man7.org/linux/man-pages/man7/signal-safety.7.html
      type: documentation
    milestones:
    - id: 1
      name: Basic Signal Handling
      description: Install signal handlers for common signals.
      acceptance_criteria:
      - Handle SIGINT signal triggered by Ctrl+C keyboard interrupt
      - Handle SIGTERM signal for graceful process termination
      - Use sigaction() instead of signal() for portable reliable handling
      - Set SA_RESTART flag so interrupted system calls restart automatically
      hints:
        level1: sigaction() is portable and reliable. signal() has race conditions.
        level2: SA_RESTART makes interrupted syscalls restart automatically.
        level3: "#include <signal.h>\n#include <stdio.h>\n#include <unistd.h>\n\nvolatile sig_atomic_t running = 1;\n\nvoid\
          \ handle_signal(int sig) {\n    // Only async-signal-safe operations here!\n    if (sig == SIGINT || sig == SIGTERM)\
          \ {\n        running = 0;\n    }\n}\n\nint setup_signal_handler(int sig, void (*handler)(int)) {\n    struct sigaction\
          \ sa;\n    sa.sa_handler = handler;\n    sigemptyset(&sa.sa_mask);\n    sa.sa_flags = SA_RESTART;  // Restart interrupted\
          \ syscalls\n    \n    if (sigaction(sig, &sa, NULL) < 0) {\n        perror(\"sigaction\");\n        return -1;\n\
          \    }\n    return 0;\n}\n\nint main() {\n    setup_signal_handler(SIGINT, handle_signal);\n    setup_signal_handler(SIGTERM,\
          \ handle_signal);\n    \n    printf(\"Running... Press Ctrl+C to stop\\n\");\n    \n    while (running) {\n    \
          \    // Do work\n        sleep(1);\n        printf(\".\");\n        fflush(stdout);\n    }\n    \n    printf(\"\\\
          nGraceful shutdown\\n\");\n    return 0;\n}"
      pitfalls:
      - Non-reentrant functions in handler
      - Race conditions
      - Missing SA_RESTART
      concepts:
      - Signal handlers
      - sigaction()
      - Async-signal safety
      estimated_hours: 2-3
      deliverables:
      - Signal handler registration using sigaction() system call
      - Signal delivery handling for SIGINT, SIGTERM, and SIGHUP
      - Async-signal-safe handler function using only safe operations
      - SA_RESTART flag configuration ensuring interrupted syscalls restart
    - id: 2
      name: Signal Masking
      description: Block and unblock signals for critical sections.
      acceptance_criteria:
      - Block signals during critical code sections using sigprocmask()
      - Manipulate signal masks using sigprocmask() with SIG_BLOCK and SIG_UNBLOCK
      - Handle pending signals delivered when mask is removed
      - Support per-thread signal masks in multithreaded programs
      hints:
        level1: sigprocmask() blocks signals temporarily. Pending signals delivered when unblocked.
        level2: Block signals before modifying shared data, unblock after.
        level3: "#include <signal.h>\n\nvoid block_signals(sigset_t *oldmask) {\n    sigset_t blockmask;\n    sigemptyset(&blockmask);\n\
          \    sigaddset(&blockmask, SIGINT);\n    sigaddset(&blockmask, SIGTERM);\n    sigaddset(&blockmask, SIGCHLD);\n\
          \    \n    if (sigprocmask(SIG_BLOCK, &blockmask, oldmask) < 0) {\n        perror(\"sigprocmask\");\n    }\n}\n\n\
          void unblock_signals(sigset_t *oldmask) {\n    if (sigprocmask(SIG_SETMASK, oldmask, NULL) < 0) {\n        perror(\"\
          sigprocmask\");\n    }\n}\n\nvoid critical_section() {\n    sigset_t oldmask;\n    \n    block_signals(&oldmask);\n\
          \    \n    // Critical code - signals blocked\n    // Modify shared data structures safely\n    update_shared_state();\n\
          \    \n    unblock_signals(&oldmask);\n    // Pending signals delivered here\n}\n\n// Check for pending signals\n\
          void check_pending() {\n    sigset_t pending;\n    sigpending(&pending);\n    \n    if (sigismember(&pending, SIGINT))\
          \ {\n        printf(\"SIGINT is pending\\n\");\n    }\n}"
      pitfalls:
      - Forgetting to unblock
      - Deadlock with nested blocking
      - Thread safety
      concepts:
      - Signal masks
      - Critical sections
      - Pending signals
      estimated_hours: 2-3
      deliverables:
      - Signal mask manipulation blocking and unblocking specific signals
      - Critical section signal blocking preventing interruption of sensitive code
      - Pending signal inspection checking which signals are queued
      - Per-thread signal mask support using pthread_sigmask()
    - id: 3
      name: Self-Pipe Trick
      description: Integrate signals with select/poll event loop.
      acceptance_criteria:
      - Create non-blocking self-pipe for signal notification delivery
      - Write signal number byte to pipe inside async-signal-safe handler
      - Monitor pipe read end using select() or poll() in main event loop
      - Handle signal safely in main loop context after event notification
      hints:
        level1: Signal handlers can't do much safely. Write byte to pipe, handle in event loop.
        level2: Make pipe non-blocking. select() returns when pipe readable.
        level3: "#include <fcntl.h>\n#include <sys/select.h>\n\nint signal_pipe[2];\n\nvoid signal_handler(int sig) {\n  \
          \  // Async-signal-safe: write single byte\n    int saved_errno = errno;\n    char c = sig;\n    write(signal_pipe[1],\
          \ &c, 1);\n    errno = saved_errno;\n}\n\nvoid setup_self_pipe() {\n    if (pipe(signal_pipe) < 0) {\n        perror(\"\
          pipe\");\n        exit(1);\n    }\n    \n    // Make non-blocking\n    fcntl(signal_pipe[0], F_SETFL, O_NONBLOCK);\n\
          \    fcntl(signal_pipe[1], F_SETFL, O_NONBLOCK);\n    \n    setup_signal_handler(SIGINT, signal_handler);\n    setup_signal_handler(SIGTERM,\
          \ signal_handler);\n    setup_signal_handler(SIGCHLD, signal_handler);\n}\n\nvoid event_loop() {\n    fd_set readfds;\n\
          \    \n    while (running) {\n        FD_ZERO(&readfds);\n        FD_SET(signal_pipe[0], &readfds);\n        FD_SET(client_socket,\
          \ &readfds);  // Other FDs\n        \n        int maxfd = signal_pipe[0] > client_socket ? signal_pipe[0] : client_socket;\n\
          \        \n        int ret = select(maxfd + 1, &readfds, NULL, NULL, NULL);\n        if (ret < 0 && errno != EINTR)\
          \ {\n            perror(\"select\");\n            break;\n        }\n        \n        if (FD_ISSET(signal_pipe[0],\
          \ &readfds)) {\n            // Signal received - handle safely here\n            char sig;\n            while (read(signal_pipe[0],\
          \ &sig, 1) > 0) {\n                handle_signal_safely(sig);\n            }\n        }\n        \n        if (FD_ISSET(client_socket,\
          \ &readfds)) {\n            handle_client();\n        }\n    }\n}"
      pitfalls:
      - Pipe buffer full
      - Non-blocking write
      - Multiple signals coalescing
      concepts:
      - Self-pipe trick
      - Event loops
      - Signal integration
      estimated_hours: 3-4
      deliverables:
      - Non-blocking pipe pair for signal-to-event-loop notification
      - Signal handler writing single byte to pipe on signal receipt
      - Event loop integration reading pipe alongside other I/O descriptors
      - Multiplexed I/O combining signal pipe with socket descriptors in select/poll
  simple-gc:
    id: simple-gc
    name: Simple Garbage Collector
    description: Implement a mark-sweep garbage collector. Learn memory management and object traversal.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Memory management basics
    - Graph traversal
    - C pointers
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python (educational)
    resources:
    - name: The Garbage Collection Handbook
      url: https://gchandbook.org/
      type: book
    - name: Baby's First GC
      url: https://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/
      type: article
    milestones:
    - id: 1
      name: Object Model
      description: Define object representation with GC metadata.
      acceptance_criteria:
      - Object header includes type information for runtime dispatch
      - Store mark bit in each object header for garbage collection
      - Track total allocated object size for GC threshold triggering
      - Support at least three different object types (int, pair, string)
      hints:
        level1: 'Every object needs header: marked flag, type, size.'
        level2: Use a common header struct that precedes object data.
        level3: "typedef enum {\n    OBJ_INT,\n    OBJ_PAIR,\n    OBJ_STRING,\n} ObjectType;\n\ntypedef struct Object {\n\
          \    ObjectType type;\n    unsigned char marked;\n    struct Object* next;  // For allocation list\n    \n    union\
          \ {\n        int int_value;\n        struct {\n            struct Object* head;\n            struct Object* tail;\n\
          \        } pair;\n        struct {\n            char* chars;\n            int length;\n        } string;\n    }\
          \ as;\n} Object;\n\ntypedef struct {\n    Object* first_object;  // Head of all allocated objects\n    Object**\
          \ stack;        // Root stack\n    int stack_size;\n    int stack_capacity;\n    int num_objects;\n    int max_objects;\
          \       // Threshold for GC\n} VM;\n\nObject* new_object(VM* vm, ObjectType type) {\n    if (vm->num_objects >=\
          \ vm->max_objects) {\n        gc(vm);\n    }\n    \n    Object* obj = malloc(sizeof(Object));\n    obj->type = type;\n\
          \    obj->marked = 0;\n    obj->next = vm->first_object;\n    vm->first_object = obj;\n    vm->num_objects++;\n\
          \    \n    return obj;\n}"
      pitfalls:
      - Header alignment
      - Type safety
      - Object size calculation
      concepts:
      - Object headers
      - Type tags
      - Allocation lists
      estimated_hours: 3-4
      deliverables:
      - Object header structure containing type tag and mark bit
      - Reference tracking linking objects that point to other objects
      - Object allocation function creating new objects on the heap
      - Type information system distinguishing integers, pairs, and strings
    - id: 2
      name: Root Discovery
      description: Identify and traverse GC roots.
      acceptance_criteria:
      - Identify and enumerate all GC root references on the stack
      - Include global variable references in the root set
      - Support register-based roots when applicable to target architecture
      - Distinguish precise roots (known pointers) from conservative scanning
      hints:
        level1: Roots = live references not reachable from heap. Stack, globals.
        level2: Maintain explicit stack of roots for simplicity.
        level3: "void push_root(VM* vm, Object* obj) {\n    if (vm->stack_size >= vm->stack_capacity) {\n        vm->stack_capacity\
          \ *= 2;\n        vm->stack = realloc(vm->stack, sizeof(Object*) * vm->stack_capacity);\n    }\n    vm->stack[vm->stack_size++]\
          \ = obj;\n}\n\nObject* pop_root(VM* vm) {\n    return vm->stack[--vm->stack_size];\n}\n\nvoid mark_roots(VM* vm)\
          \ {\n    // Mark all objects on the stack\n    for (int i = 0; i < vm->stack_size; i++) {\n        mark_object(vm->stack[i]);\n\
          \    }\n    \n    // Mark globals (if any)\n    // mark_object(vm->globals);\n}\n\n// For conservative GC (scan\
          \ memory for pointers)\nvoid scan_stack_conservative(VM* vm, void* stack_top, void* stack_bottom) {\n    for (void**\
          \ p = stack_top; p < stack_bottom; p++) {\n        void* potential_ptr = *p;\n        // Check if this looks like\
          \ a valid object pointer\n        Object* obj = find_object_at_address(vm, potential_ptr);\n        if (obj) {\n\
          \            mark_object(obj);\n        }\n    }\n}"
      pitfalls:
      - Missing roots
      - Stack scanning direction
      - Interior pointers
      concepts:
      - Root set
      - Precise vs conservative
      - Stack scanning
      estimated_hours: 3-4
      deliverables:
      - Stack scanning identifying root references from call stack frames
      - Global variable roots tracking references in static or global scope
      - Register contents inspection for architectures storing roots in registers
      - Root set construction building complete list of live reference origins
    - id: 3
      name: Mark Phase
      description: Traverse and mark all reachable objects.
      acceptance_criteria:
      - Recursively mark all objects reachable from root set
      - Handle reference cycles without infinite loop using mark bits
      - Set and check mark bits to avoid revisiting already-marked objects
      - Optionally use explicit worklist instead of recursion to avoid stack overflow
      hints:
        level1: 'Mark: start from roots, follow pointers, set mark bit.'
        level2: Use recursion or explicit worklist to avoid stack overflow.
        level3: "void mark_object(Object* obj) {\n    if (obj == NULL) return;\n    if (obj->marked) return;  // Already visited\n\
          \    \n    obj->marked = 1;\n    \n    // Recursively mark referenced objects\n    switch (obj->type) {\n      \
          \  case OBJ_INT:\n            // No references\n            break;\n        case OBJ_PAIR:\n            mark_object(obj->as.pair.head);\n\
          \            mark_object(obj->as.pair.tail);\n            break;\n        case OBJ_STRING:\n            // No object\
          \ references (just char*)\n            break;\n    }\n}\n\n// Worklist version (avoids deep recursion)\nvoid mark_object_worklist(VM*\
          \ vm, Object* obj) {\n    if (obj == NULL || obj->marked) return;\n    \n    Object** worklist = malloc(sizeof(Object*)\
          \ * vm->num_objects);\n    int worklist_size = 0;\n    \n    worklist[worklist_size++] = obj;\n    \n    while (worklist_size\
          \ > 0) {\n        Object* current = worklist[--worklist_size];\n        if (current->marked) continue;\n       \
          \ current->marked = 1;\n        \n        if (current->type == OBJ_PAIR) {\n            if (current->as.pair.head\
          \ && !current->as.pair.head->marked)\n                worklist[worklist_size++] = current->as.pair.head;\n     \
          \       if (current->as.pair.tail && !current->as.pair.tail->marked)\n                worklist[worklist_size++]\
          \ = current->as.pair.tail;\n        }\n    }\n    \n    free(worklist);\n}"
      pitfalls:
      - Stack overflow on deep structures
      - Forgetting object types
      - Double marking
      concepts:
      - Graph traversal
      - Mark bits
      - Tri-color marking
      estimated_hours: 3-4
      deliverables:
      - Object graph traversal starting from root set references
      - Mark bit setting flagging each reachable object as live
      - Explicit worklist processing avoiding deep recursion stack overflow
      - Reachability analysis determining which objects are still in use
    - id: 4
      name: Sweep Phase
      description: Reclaim unmarked objects and reset marks.
      acceptance_criteria:
      - Walk the complete allocation list checking each object's mark bit
      - Free all unmarked objects and reclaim their memory
      - Reset mark bits on surviving objects for next collection cycle
      - Update allocation linked list to skip freed object entries
      hints:
        level1: 'Sweep: walk all objects, free unmarked, clear marks on marked.'
        level2: Update linked list as you go - skip freed objects.
        level3: "void sweep(VM* vm) {\n    Object** obj_ptr = &vm->first_object;\n    \n    while (*obj_ptr != NULL) {\n \
          \       if (!(*obj_ptr)->marked) {\n            // Unreachable - free it\n            Object* unreached = *obj_ptr;\n\
          \            *obj_ptr = unreached->next;  // Unlink\n            \n            // Free any additional memory\n \
          \           if (unreached->type == OBJ_STRING) {\n                free(unreached->as.string.chars);\n          \
          \  }\n            \n            free(unreached);\n            vm->num_objects--;\n        } else {\n           \
          \ // Reachable - clear mark for next GC\n            (*obj_ptr)->marked = 0;\n            obj_ptr = &(*obj_ptr)->next;\n\
          \        }\n    }\n}\n\nvoid gc(VM* vm) {\n    int num_before = vm->num_objects;\n    \n    // Mark phase\n    mark_roots(vm);\n\
          \    \n    // Sweep phase\n    sweep(vm);\n    \n    // Adjust threshold\n    vm->max_objects = vm->num_objects\
          \ * 2;\n    \n    printf(\"Collected %d objects, %d remaining.\\n\",\n           num_before - vm->num_objects, vm->num_objects);\n\
          }"
      pitfalls:
      - List corruption
      - Forgetting to clear marks
      - Memory leaks in objects
      concepts:
      - Memory reclamation
      - Linked list manipulation
      - GC thresholds
      estimated_hours: 3-4
      deliverables:
      - Heap traversal walking entire allocation list checking mark bits
      - Unmarked object collection freeing memory of unreachable objects
      - Free list update relinking allocation list after removing freed objects
      - Memory reclamation returning freed bytes to allocator for reuse
  social-network:
    id: social-network
    name: Social Network
    description: Build a social network with user profiles, feeds, followers, and notifications. Learn about complex data
      relationships and real-time features at scale.
    difficulty: advanced
    estimated_hours: 60-80
    prerequisites:
    - Full-stack web development
    - Database design
    - Caching concepts
    - Real-time systems
    languages:
      recommended:
      - JavaScript
      - Python
      - Go
      also_possible:
      - Ruby
      - Java
      - Elixir
    resources:
    - name: Feed Architecture
      url: https://www.youtube.com/watch?v=QmX2NPkJTKg
      type: video
    - name: System Design Social Network
      url: https://www.youtube.com/results?search_query=system+design+social+network
      type: video
    milestones:
    - id: 1
      name: User Profiles & Follow System
      description: Build user profiles and the follow/follower relationship.
      acceptance_criteria:
      - User profile with bio, avatar, links
      - Follow and unfollow toggles update relationship records atomically
      - Follower and following lists return paginated results with counts
      - Follower and following counts are accurate after bulk operations
      - Profile editing persists changes and validates field constraints
      hints:
        level1: Follow is a self-referential many-to-many relationship.
        level2: Store counts denormalized for performance.
        level3: |-
          // Follow relationship
          model Follow {
            id          Int      @id @default(autoincrement())
            followerId  Int
            followingId Int
            createdAt   DateTime @default(now())
            follower    User     @relation("follower", ...)
            following   User     @relation("following", ...)
            @@unique([followerId, followingId])
            @@index([followingId])
          }

          // Denormalized counts on User
          model User {
            followerCount  Int @default(0)
            followingCount Int @default(0)
          }
      pitfalls:
      - Self-follow allowed
      - Count drift from denormalization
      - N+1 queries for follower lists
      concepts:
      - Self-referential relations
      - Denormalization
      - Count caching
      estimated_hours: 8-10
      deliverables:
      - User profile CRUD API supporting bio, avatar, and links fields
      - Follow and unfollow endpoints with duplicate-follow prevention
      - Paginated follower and following list retrieval endpoints
      - Profile privacy settings controlling visibility of posts and follower lists
    - id: 2
      name: Posts & Feed (Fan-out on Write)
      description: Implement posts and the home feed using fan-out on write.
      acceptance_criteria:
      - Create text and image posts with validation and storage
      - User's own post list returns all authored posts in order
      - Home feed (posts from followed users)
      - Feed pagination uses cursor-based approach for stable page results
      - Post timestamps display correctly in multiple time zones
      hints:
        level1: 'Fan-out on write: when user posts, add to each follower''s feed.'
        level2: 'Use a Feed table: { userId, postId, createdAt }.'
        level3: |-
          async function createPost(authorId, content) {
            const post = await Post.create({ authorId, content });
            const followers = await Follow.findMany({
              where: { followingId: authorId },
              select: { followerId: true }
            });
            await FeedItem.createMany({
              data: followers.map(f => ({
                userId: f.followerId,
                postId: post.id,
                createdAt: post.createdAt
              }))
            });
            return post;
          }
      pitfalls:
      - Fan-out blocking post creation (use queue)
      - Celebrity problem (millions of followers)
      - Offset pagination performance
      concepts:
      - Fan-out on write vs read
      - Feed generation
      - Cursor pagination
      estimated_hours: 10-12
      deliverables:
      - Post creation endpoint supporting text and image content types
      - Fan-out service that writes new posts to each follower's feed
      - Feed retrieval endpoint returning chronologically ordered posts
      - Cursor-based feed pagination with stable ordering across pages
    - id: 3
      name: Likes, Comments & Interactions
      description: Add social interactions to posts.
      acceptance_criteria:
      - Like and unlike toggle updates count atomically without race conditions
      - Comment on posts with text content and author attribution
      - Like counts update in real-time and reflect accurate totals
      - Comment threads support nested replies with correct parent-child ordering
      - Share and repost duplicates content with attribution to original author
      hints:
        level1: 'Like: simple user-post relation. Comment: text + user + post.'
        level2: Use optimistic UI updates for likes.
        level3: |-
          async function likePost(postId) {
            // Optimistic update
            setLiked(true);
            setLikeCount(prev => prev + 1);
            try {
              await api.post(`/posts/${postId}/like`);
            } catch (error) {
              // Rollback on error
              setLiked(false);
              setLikeCount(prev => prev - 1);
            }
          }
      pitfalls:
      - Double-like race condition
      - Comment ordering (newest vs oldest)
      - Deep nested replies complexity
      concepts:
      - Social interactions
      - Optimistic updates
      - Comment systems
      estimated_hours: 8-10
      deliverables:
      - Like and unlike functionality with idempotent toggle behavior
      - Threaded comment system supporting nested replies to posts
      - Real-time interaction count aggregation for likes and comments
      - Notification trigger events emitted on like, comment, and share actions
    - id: 4
      name: Notifications
      description: Build a notification system for social activity.
      acceptance_criteria:
      - Notification fires when a new user follows the recipient
      - Notification fires for likes and comments on user's posts
      - Notification badge count reflects accurate unread total in real-time
      - Mark as read updates status and decrements badge count
      - Real-time notification delivery pushes events within two seconds
      hints:
        level1: 'Notification table: { userId, type, data, read, createdAt }.'
        level2: Use WebSocket or SSE for real-time delivery.
        level3: |-
          model Notification {
            id        Int      @id
            userId    Int      // recipient
            type      String   // 'follow', 'like', 'comment'
            actorId   Int      // who triggered it
            targetId  Int?     // post id for likes/comments
            read      Boolean  @default(false)
            createdAt DateTime @default(now())
            @@index([userId, read, createdAt])
          }

          async function createLikeNotification(postId, likerId) {
            const post = await Post.findUnique({ where: { id: postId } });
            if (post.authorId === likerId) return;
            await Notification.create({
              data: { userId: post.authorId, type: 'like', actorId: likerId, targetId: postId }
            });
            notificationService.push(post.authorId, { type: 'like', ... });
          }
      pitfalls:
      - Notification spam (batch similar notifications)
      - Notifying yourself
      - Notification count going negative
      concepts:
      - Notification system
      - Real-time delivery
      - Batching
      estimated_hours: 8-10
      deliverables:
      - Notification type registry for follow, like, comment, and mention events
      - Notification delivery pipeline routing events to recipient inboxes
      - Read and unread status tracking with bulk mark-as-read support
      - Notification preference settings allowing per-type opt-in and opt-out
    - id: 5
      name: Search & Discovery
      description: Add search and content discovery features.
      acceptance_criteria:
      - Search users by name or username with partial match support
      - Search posts by content keywords returning ranked results
      - Trending posts and hashtags update based on recent engagement metrics
      - Suggested users to follow are based on mutual connections or interests
      - Explore page surfaces popular content from outside the user's network
      hints:
        level1: Simple search with LIKE. Use Elasticsearch for scale.
        level2: 'Trending: count hashtags/posts in sliding window.'
        level3: |-
          async function getSuggestedUsers(userId) {
            // Users followed by people I follow, that I don't follow
            const suggestions = await prisma.$queryRaw`
              SELECT u.*, COUNT(*) as mutual
              FROM users u
              JOIN follows f1 ON f1.following_id = u.id
              JOIN follows f2 ON f2.follower_id = f1.follower_id
              WHERE f2.following_id = ${userId}
                AND u.id != ${userId}
                AND u.id NOT IN (
                  SELECT following_id FROM follows WHERE follower_id = ${userId}
                )
              GROUP BY u.id
              ORDER BY mutual DESC
              LIMIT 10
            `;
            return suggestions;
          }
      pitfalls:
      - Search too slow on large datasets
      - Trending manipulation (spam)
      - Recommendation bubbles
      concepts:
      - Search implementation
      - Trending algorithms
      - Recommendation systems
      estimated_hours: 10-12
      deliverables:
      - User search endpoint with full-text matching on name and username
      - Post search endpoint with keyword and hashtag content matching
      - Hashtag extraction and indexing system for post content
      - Trending topics aggregation based on recent hashtag and engagement volume
    - id: 6
      name: Performance & Scaling
      description: Optimize for performance and prepare for scale.
      acceptance_criteria:
      - Redis caching for feeds reduces database read load by 80 percent
      - Background job processing handles fan-out and notification delivery asynchronously
      - CDN serves media assets with cache-control headers and low latency
      - Database indexing is optimized for common query patterns on feeds and profiles
      - Load testing confirms system handles target concurrent user count
      hints:
        level1: 'Cache hot data: user profiles, feed, follower counts.'
        level2: Use background jobs for fan-out, notifications, emails.
        level3: |-
          async function getFeed(userId, cursor) {
            const cacheKey = `feed:${userId}`;
            let feedIds = await redis.zrevrange(cacheKey, 0, 19);
            if (!feedIds.length) {
              const feed = await FeedItem.findMany({
                where: { userId },
                orderBy: { createdAt: 'desc' },
                take: 100
              });
              await redis.zadd(cacheKey, ...feed.flatMap(f => [f.createdAt.getTime(), f.postId]));
              feedIds = feed.map(f => f.postId).slice(0, 20);
            }
            return Post.findMany({ where: { id: { in: feedIds } } });
          }
      pitfalls:
      - Cache invalidation complexity
      - Celebrity user problem
      - Hot partition on popular content
      concepts:
      - Caching strategies
      - Background jobs
      - Performance optimization
      estimated_hours: 12-15
      deliverables:
      - Caching strategy using Redis for hot feed and profile data
      - Database sharding scheme partitioning data by user ID range
      - CDN integration for serving uploaded media assets efficiently
      - Rate limiting middleware protecting API endpoints from abuse
  software-3d:
    id: software-3d
    name: Software 3D Renderer
    description: Build a 3D renderer without GPU acceleration. Learn the math behind 3D graphics from scratch.
    difficulty: advanced
    estimated_hours: 30-50
    prerequisites:
    - Linear algebra (matrices, vectors)
    - Basic trigonometry
    - 2D graphics basics
    languages:
      recommended:
      - C
      - C++
      - Rust
      also_possible:
      - Python
      - JavaScript
    resources:
    - name: tinyrenderer
      url: https://github.com/ssloy/tinyrenderer/wiki
      type: tutorial
    - name: 3D Math Primer
      url: https://gamemath.com/
      type: book
    milestones:
    - id: 1
      name: Line Drawing
      description: Implement Bresenham's line algorithm.
      acceptance_criteria:
      - Draw lines between any two points
      - Handle all octants correctly including horizontal and vertical lines
      - Optimize for integer-only math with no floating-point operations
      - Draw pixels to a memory buffer that can be displayed on screen
      hints:
        level1: 'Bresenham: use error accumulation instead of floats.'
        level2: Handle steep lines by swapping x/y. Handle direction with step.
        level3: "void draw_line(int x0, int y0, int x1, int y1, uint32_t *buffer, int width) {\n    int steep = abs(y1 - y0)\
          \ > abs(x1 - x0);\n    if (steep) {\n        // Swap x and y\n        int tmp = x0; x0 = y0; y0 = tmp;\n       \
          \ tmp = x1; x1 = y1; y1 = tmp;\n    }\n    if (x0 > x1) {\n        int tmp = x0; x0 = x1; x1 = tmp;\n        tmp\
          \ = y0; y0 = y1; y1 = tmp;\n    }\n    \n    int dx = x1 - x0;\n    int dy = abs(y1 - y0);\n    int error = dx /\
          \ 2;\n    int ystep = (y0 < y1) ? 1 : -1;\n    int y = y0;\n    \n    for (int x = x0; x <= x1; x++) {\n       \
          \ if (steep)\n            buffer[x * width + y] = 0xFFFFFF;\n        else\n            buffer[y * width + x] = 0xFFFFFF;\n\
          \        \n        error -= dy;\n        if (error < 0) {\n            y += ystep;\n            error += dx;\n \
          \       }\n    }\n}"
      pitfalls:
      - Integer overflow
      - Division by zero
      - Missing octants
      concepts:
      - Rasterization
      - Bresenham's algorithm
      - Pixel buffers
      estimated_hours: 3-4
      deliverables:
      - Bresenham's line algorithm implementation supporting all slope octants
      - Cohen-Sutherland line clipping against viewport rectangle boundaries
      - Anti-aliased line drawing using Xiaolin Wu's algorithm for smooth edges
      - Framebuffer write interface that sets pixel color at x,y coordinates
    - id: 2
      name: Triangle Rasterization
      description: Fill triangles using scanline or barycentric methods.
      acceptance_criteria:
      - Fill solid-colored triangles with correct pixel coverage
      - Handle edge cases including flat-top, flat-bottom, and degenerate triangles
      - Implement barycentric coordinates for smooth attribute interpolation across triangles
      - Anti-aliasing smooths triangle edges using sub-pixel sampling (optional)
      hints:
        level1: 'Barycentric: point P in triangle if all weights positive.'
        level2: Compute bounding box, test each pixel with barycentric coords.
        level3: "typedef struct { float x, y; } Vec2;\n\nfloat edge_function(Vec2 a, Vec2 b, Vec2 c) {\n    return (c.x -\
          \ a.x) * (b.y - a.y) - (c.y - a.y) * (b.x - a.x);\n}\n\nvoid draw_triangle(Vec2 v0, Vec2 v1, Vec2 v2, uint32_t color,\
          \ uint32_t *buffer, int w, int h) {\n    // Bounding box\n    int minX = max(0, (int)min(v0.x, min(v1.x, v2.x)));\n\
          \    int maxX = min(w-1, (int)max(v0.x, max(v1.x, v2.x)));\n    int minY = max(0, (int)min(v0.y, min(v1.y, v2.y)));\n\
          \    int maxY = min(h-1, (int)max(v0.y, max(v1.y, v2.y)));\n    \n    float area = edge_function(v0, v1, v2);\n\
          \    \n    for (int y = minY; y <= maxY; y++) {\n        for (int x = minX; x <= maxX; x++) {\n            Vec2\
          \ p = {x + 0.5f, y + 0.5f};\n            \n            float w0 = edge_function(v1, v2, p);\n            float w1\
          \ = edge_function(v2, v0, p);\n            float w2 = edge_function(v0, v1, p);\n            \n            // Point\
          \ inside triangle if all same sign\n            if (w0 >= 0 && w1 >= 0 && w2 >= 0) {\n                // Barycentric\
          \ coordinates\n                w0 /= area; w1 /= area; w2 /= area;\n                buffer[y * w + x] = color;\n\
          \            }\n        }\n    }\n}"
      pitfalls:
      - Winding order
      - Gaps between triangles
      - Subpixel precision
      concepts:
      - Barycentric coordinates
      - Rasterization
      - Fill rules
      estimated_hours: 5-7
      deliverables:
      - Edge function evaluator determining if a point is inside a triangle
      - Scanline rasterization filling triangles row by row between edges
      - Barycentric coordinate interpolation for vertex attribute blending
      - Texture coordinate interpolation mapping UV values across triangle surfaces
    - id: 3
      name: 3D Transformations
      description: Implement model, view, and projection matrices.
      acceptance_criteria:
      - 4x4 matrix multiplication produces correct composite transformation results
      - Translation, rotation, and scaling transforms apply correctly to vertex positions
      - Look-at camera matrix orients the view toward a target point from eye position
      - Perspective projection produces correct foreshortening with near and far planes
      hints:
        level1: 'MVP: Model * View * Projection. Apply to each vertex.'
        level2: 'Perspective divide: after projection, divide x,y,z by w.'
        level3: "typedef struct { float m[4][4]; } Mat4;\n\nMat4 mat4_perspective(float fov, float aspect, float near, float\
          \ far) {\n    Mat4 m = {0};\n    float tanHalfFov = tan(fov / 2.0f);\n    m.m[0][0] = 1.0f / (aspect * tanHalfFov);\n\
          \    m.m[1][1] = 1.0f / tanHalfFov;\n    m.m[2][2] = -(far + near) / (far - near);\n    m.m[2][3] = -1.0f;\n   \
          \ m.m[3][2] = -(2.0f * far * near) / (far - near);\n    return m;\n}\n\nMat4 mat4_look_at(Vec3 eye, Vec3 target,\
          \ Vec3 up) {\n    Vec3 f = vec3_normalize(vec3_sub(target, eye));\n    Vec3 r = vec3_normalize(vec3_cross(f, up));\n\
          \    Vec3 u = vec3_cross(r, f);\n    \n    Mat4 m = mat4_identity();\n    m.m[0][0] = r.x; m.m[1][0] = r.y; m.m[2][0]\
          \ = r.z;\n    m.m[0][1] = u.x; m.m[1][1] = u.y; m.m[2][1] = u.z;\n    m.m[0][2] = -f.x; m.m[1][2] = -f.y; m.m[2][2]\
          \ = -f.z;\n    m.m[3][0] = -vec3_dot(r, eye);\n    m.m[3][1] = -vec3_dot(u, eye);\n    m.m[3][2] = vec3_dot(f, eye);\n\
          \    return m;\n}\n\nVec3 project_vertex(Vec3 v, Mat4 mvp, int width, int height) {\n    // Apply MVP\n    float\
          \ w = mvp.m[0][3]*v.x + mvp.m[1][3]*v.y + mvp.m[2][3]*v.z + mvp.m[3][3];\n    float x = mvp.m[0][0]*v.x + mvp.m[1][0]*v.y\
          \ + mvp.m[2][0]*v.z + mvp.m[3][0];\n    float y = mvp.m[0][1]*v.x + mvp.m[1][1]*v.y + mvp.m[2][1]*v.z + mvp.m[3][1];\n\
          \    \n    // Perspective divide\n    x /= w; y /= w;\n    \n    // Viewport transform\n    return (Vec3){\n   \
          \     (x + 1.0f) * width / 2.0f,\n        (1.0f - y) * height / 2.0f,\n        0\n    };\n}"
      pitfalls:
      - Matrix multiplication order
      - Homogeneous coordinates
      - Y-axis flip
      concepts:
      - Linear transformations
      - Projection
      - Camera space
      estimated_hours: 6-8
      deliverables:
      - Model matrix supporting translation, rotation, and scaling of objects
      - View matrix implementing camera positioning with look-at computation
      - Projection matrix implementing perspective and orthographic projections
      - Vertex transformation pipeline applying model-view-projection to each vertex
    - id: 4
      name: Depth Buffer & Lighting
      description: Add z-buffering and basic shading.
      acceptance_criteria:
      - Z-buffer correctly resolves hidden surfaces in overlapping geometry
      - Flat shading computes lighting from surface normals for each triangle face
      - Gouraud shading interpolates per-vertex lighting across triangle for smooth gradients
      - Basic diffuse lighting computes intensity from dot product of normal and light direction
      hints:
        level1: 'Z-buffer: store depth per pixel, only draw if closer.'
        level2: Interpolate z using barycentric coordinates.
        level3: "float *zbuffer;  // Initialize to infinity\n\nvoid draw_triangle_zbuf(Vec3 v0, Vec3 v1, Vec3 v2, uint32_t\
          \ color,\n                         uint32_t *buffer, float *zbuffer, int w, int h) {\n    // ... bounding box code\
          \ ...\n    \n    for (int y = minY; y <= maxY; y++) {\n        for (int x = minX; x <= maxX; x++) {\n          \
          \  Vec2 p = {x + 0.5f, y + 0.5f};\n            \n            float w0 = edge_function(v1, v2, p) / area;\n     \
          \       float w1 = edge_function(v2, v0, p) / area;\n            float w2 = edge_function(v0, v1, p) / area;\n \
          \           \n            if (w0 >= 0 && w1 >= 0 && w2 >= 0) {\n                // Interpolate z\n             \
          \   float z = w0 * v0.z + w1 * v1.z + w2 * v2.z;\n                \n                int idx = y * w + x;\n     \
          \           if (z < zbuffer[idx]) {\n                    zbuffer[idx] = z;\n                    buffer[idx] = color;\n\
          \                }\n            }\n        }\n    }\n}\n\n// Flat shading\nVec3 compute_normal(Vec3 v0, Vec3 v1,\
          \ Vec3 v2) {\n    Vec3 edge1 = vec3_sub(v1, v0);\n    Vec3 edge2 = vec3_sub(v2, v0);\n    return vec3_normalize(vec3_cross(edge1,\
          \ edge2));\n}\n\nfloat compute_lighting(Vec3 normal, Vec3 light_dir) {\n    float intensity = vec3_dot(normal, light_dir);\n\
          \    return max(0.0f, intensity);  // Clamp to [0,1]\n}"
      pitfalls:
      - Z-fighting
      - Normal direction
      - Interpolation artifacts
      concepts:
      - Z-buffering
      - Surface normals
      - Lighting models
      estimated_hours: 8-12
      deliverables:
      - Z-buffer implementation storing per-pixel depth values for occlusion testing
      - Depth testing logic rejecting fragments behind already-rendered surfaces
      - Flat shading computing one normal per triangle for uniform face lighting
      - Gouraud shading interpolating vertex colors across the triangle surface
  sql-parser:
    id: sql-parser
    name: SQL Parser
    description: Build a SQL parser for SELECT, INSERT, UPDATE queries. Learn query parsing and AST construction.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Tokenizer basics
    - Recursive descent parsing
    - Tree data structures
    languages:
      recommended:
      - Python
      - Go
      - Rust
      also_possible:
      - JavaScript
      - Java
    resources:
    - name: SQLite Grammar
      url: https://www.sqlite.org/lang.html
      type: documentation
    - name: Writing a SQL Parser
      url: https://blog.subnetzero.io/post/building-a-sql-parser/
      type: article
    milestones:
    - id: 1
      name: SQL Tokenizer
      description: Tokenize SQL statements into keywords, identifiers, literals.
      acceptance_criteria:
      - Recognize SQL keywords (SELECT, FROM, WHERE)
      - Handle identifiers and numbers with correct token type classification
      - Parse string literals enclosed in single or double quotes
      - Support operators (=, <, >, AND, OR)
      hints:
        level1: Keywords are case-insensitive. Identifiers can be quoted.
        level2: Handle both single and double quotes for strings. Whitespace separates tokens.
        level3: "import re\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\n\nclass TokenType(Enum):\n   \
          \ SELECT = auto()\n    FROM = auto()\n    WHERE = auto()\n    INSERT = auto()\n    INTO = auto()\n    VALUES = auto()\n\
          \    UPDATE = auto()\n    SET = auto()\n    DELETE = auto()\n    AND = auto()\n    OR = auto()\n    NOT = auto()\n\
          \    NULL = auto()\n    IDENTIFIER = auto()\n    NUMBER = auto()\n    STRING = auto()\n    STAR = auto()\n    COMMA\
          \ = auto()\n    LPAREN = auto()\n    RPAREN = auto()\n    EQ = auto()\n    NE = auto()\n    LT = auto()\n    GT\
          \ = auto()\n    LE = auto()\n    GE = auto()\n    EOF = auto()\n\nKEYWORDS = {\n    'SELECT': TokenType.SELECT,\
          \ 'FROM': TokenType.FROM,\n    'WHERE': TokenType.WHERE, 'AND': TokenType.AND,\n    'OR': TokenType.OR, 'INSERT':\
          \ TokenType.INSERT,\n    'INTO': TokenType.INTO, 'VALUES': TokenType.VALUES,\n    'UPDATE': TokenType.UPDATE, 'SET':\
          \ TokenType.SET,\n    'DELETE': TokenType.DELETE, 'NULL': TokenType.NULL,\n    'NOT': TokenType.NOT,\n}\n\n@dataclass\n\
          class Token:\n    type: TokenType\n    value: str\n    \nclass SQLTokenizer:\n    def __init__(self, sql: str):\n\
          \        self.sql = sql\n        self.pos = 0\n    \n    def tokenize(self) -> list:\n        tokens = []\n    \
          \    while self.pos < len(self.sql):\n            self.skip_whitespace()\n            if self.pos >= len(self.sql):\n\
          \                break\n            token = self.next_token()\n            if token:\n                tokens.append(token)\n\
          \        tokens.append(Token(TokenType.EOF, ''))\n        return tokens"
      pitfalls:
      - Case sensitivity
      - Escape sequences in strings
      - Multi-char operators
      concepts:
      - Lexical analysis
      - Token types
      - SQL syntax
      estimated_hours: 3-4
      deliverables:
      - Keyword token recognition for SELECT, FROM, WHERE, and other SQL reserved words
      - Identifier token extraction for table names, column names, and aliases
      - Literal token parsing for string, integer, and floating-point values
      - Operator token handling for comparison, arithmetic, and logical operators
    - id: 2
      name: SELECT Parser
      description: Parse SELECT statements into AST.
      acceptance_criteria:
      - Parse column list or star wildcard in SELECT clause correctly
      - Parse FROM clause with table name
      - Handle multiple comma-separated columns with correct AST node generation
      - Parse column and table aliases using AS keyword or implicit aliasing
      hints:
        level1: SELECT columns FROM table. columns = * | column_list.
        level2: column_list = column (, column)*. column can have alias.
        level3: "@dataclass\nclass SelectColumn:\n    name: str\n    alias: str = None\n\n@dataclass\nclass SelectStmt:\n\
          \    columns: list  # [SelectColumn] or ['*']\n    table: str\n    table_alias: str = None\n    where: 'Expr' =\
          \ None\n\nclass SQLParser:\n    def __init__(self, tokens: list):\n        self.tokens = tokens\n        self.pos\
          \ = 0\n    \n    def parse(self):\n        if self.current().type == TokenType.SELECT:\n            return self.parse_select()\n\
          \        # ... other statements\n    \n    def parse_select(self):\n        self.expect(TokenType.SELECT)\n    \
          \    \n        # Parse columns\n        if self.match(TokenType.STAR):\n            columns = ['*']\n        else:\n\
          \            columns = self.parse_column_list()\n        \n        # Parse FROM\n        self.expect(TokenType.FROM)\n\
          \        table = self.expect(TokenType.IDENTIFIER).value\n        \n        # Optional alias\n        table_alias\
          \ = None\n        if self.check(TokenType.IDENTIFIER) or self.match_keyword('AS'):\n            table_alias = self.expect(TokenType.IDENTIFIER).value\n\
          \        \n        # Optional WHERE\n        where = None\n        if self.match(TokenType.WHERE):\n           \
          \ where = self.parse_expr()\n        \n        return SelectStmt(columns, table, table_alias, where)\n    \n   \
          \ def parse_column_list(self):\n        columns = [self.parse_column()]\n        while self.match(TokenType.COMMA):\n\
          \            columns.append(self.parse_column())\n        return columns\n    \n    def parse_column(self):\n  \
          \      name = self.expect(TokenType.IDENTIFIER).value\n        alias = None\n        if self.match_keyword('AS')\
          \ or self.check(TokenType.IDENTIFIER):\n            alias = self.expect(TokenType.IDENTIFIER).value\n        return\
          \ SelectColumn(name, alias)"
      pitfalls:
      - Missing commas
      - Alias without AS
      - Reserved words as identifiers
      concepts:
      - Recursive descent
      - AST construction
      - Grammar rules
      estimated_hours: 4-5
      deliverables:
      - SELECT clause parser supporting column lists and wildcard star expressions
      - FROM clause parser resolving table references and optional aliases
      - Column list parser handling qualified names like table.column notation
      - Table reference parser supporting aliased and multi-table FROM clauses
    - id: 3
      name: WHERE Clause
      description: Parse WHERE expressions with operators and logic.
      acceptance_criteria:
      - Comparison operators (=, <, >, !=)
      - Boolean operators AND, OR, NOT bind with correct precedence and associativity
      - Parentheses override default operator precedence in expressions
      - NULL checks (IS NULL, IS NOT NULL)
      hints:
        level1: 'Precedence: NOT > AND > OR. Use recursive descent.'
        level2: or_expr -> and_expr (OR and_expr)*. and_expr -> not_expr (AND not_expr)*.
        level3: "@dataclass\nclass BinaryExpr:\n    left: 'Expr'\n    op: str\n    right: 'Expr'\n\n@dataclass\nclass UnaryExpr:\n\
          \    op: str\n    operand: 'Expr'\n\n@dataclass\nclass Comparison:\n    left: 'Expr'\n    op: str  # =, <, >, <=,\
          \ >=, !=\n    right: 'Expr'\n\n@dataclass\nclass IsNull:\n    expr: 'Expr'\n    negated: bool  # IS NOT NULL\n\n\
          def parse_expr(self):\n    return self.parse_or()\n\ndef parse_or(self):\n    left = self.parse_and()\n    while\
          \ self.match(TokenType.OR):\n        right = self.parse_and()\n        left = BinaryExpr(left, 'OR', right)\n  \
          \  return left\n\ndef parse_and(self):\n    left = self.parse_not()\n    while self.match(TokenType.AND):\n    \
          \    right = self.parse_not()\n        left = BinaryExpr(left, 'AND', right)\n    return left\n\ndef parse_not(self):\n\
          \    if self.match(TokenType.NOT):\n        return UnaryExpr('NOT', self.parse_not())\n    return self.parse_comparison()\n\
          \ndef parse_comparison(self):\n    left = self.parse_primary()\n    \n    # IS NULL / IS NOT NULL\n    if self.match_keyword('IS'):\n\
          \        negated = self.match(TokenType.NOT)\n        self.expect(TokenType.NULL)\n        return IsNull(left, negated)\n\
          \    \n    # Comparison operators\n    if self.current().type in (TokenType.EQ, TokenType.NE, TokenType.LT, TokenType.GT,\
          \ TokenType.LE, TokenType.GE):\n        op = self.advance().value\n        right = self.parse_primary()\n      \
          \  return Comparison(left, op, right)\n    \n    return left\n\ndef parse_primary(self):\n    if self.match(TokenType.LPAREN):\n\
          \        expr = self.parse_expr()\n        self.expect(TokenType.RPAREN)\n        return expr\n    \n    if self.check(TokenType.NUMBER):\n\
          \        return Literal(int(self.advance().value))\n    if self.check(TokenType.STRING):\n        return Literal(self.advance().value)\n\
          \    if self.check(TokenType.IDENTIFIER):\n        return Identifier(self.advance().value)\n    \n    raise ParseError(f'Unexpected\
          \ token: {self.current()}')"
      pitfalls:
      - Operator precedence
      - Unbalanced parentheses
      - NULL comparisons
      concepts:
      - Expression parsing
      - Precedence climbing
      - Boolean logic
      estimated_hours: 4-5
      deliverables:
      - Condition parser building expression trees from WHERE clause predicates
      - Comparison operator support for equals, not-equals, less-than, greater-than
      - Logical operator support combining conditions with AND, OR, and NOT
      - Expression tree builder producing correct AST with operator precedence
    - id: 4
      name: INSERT/UPDATE/DELETE
      description: Parse modification statements.
      acceptance_criteria:
      - INSERT INTO table (cols) VALUES (vals)
      - UPDATE table SET col=val WHERE condition parses into correct AST
      - DELETE FROM table WHERE condition parses with proper predicate tree
      - Handle multiple value rows in a single INSERT statement
      hints:
        level1: 'INSERT: columns optional. VALUES can have multiple tuples.'
        level2: 'UPDATE: SET clause is comma-separated assignments.'
        level3: "@dataclass\nclass InsertStmt:\n    table: str\n    columns: list  # Optional column list\n    values: list\
          \   # List of value tuples\n\n@dataclass\nclass UpdateStmt:\n    table: str\n    assignments: list  # [(column,\
          \ value), ...]\n    where: Expr = None\n\n@dataclass\nclass DeleteStmt:\n    table: str\n    where: Expr = None\n\
          \ndef parse_insert(self):\n    self.expect(TokenType.INSERT)\n    self.expect(TokenType.INTO)\n    table = self.expect(TokenType.IDENTIFIER).value\n\
          \    \n    # Optional column list\n    columns = None\n    if self.match(TokenType.LPAREN):\n        columns = []\n\
          \        columns.append(self.expect(TokenType.IDENTIFIER).value)\n        while self.match(TokenType.COMMA):\n \
          \           columns.append(self.expect(TokenType.IDENTIFIER).value)\n        self.expect(TokenType.RPAREN)\n   \
          \ \n    self.expect(TokenType.VALUES)\n    values = []\n    values.append(self.parse_value_tuple())\n    while self.match(TokenType.COMMA):\n\
          \        values.append(self.parse_value_tuple())\n    \n    return InsertStmt(table, columns, values)\n\ndef parse_update(self):\n\
          \    self.expect(TokenType.UPDATE)\n    table = self.expect(TokenType.IDENTIFIER).value\n    self.expect(TokenType.SET)\n\
          \    \n    assignments = []\n    col = self.expect(TokenType.IDENTIFIER).value\n    self.expect(TokenType.EQ)\n\
          \    val = self.parse_primary()\n    assignments.append((col, val))\n    \n    while self.match(TokenType.COMMA):\n\
          \        col = self.expect(TokenType.IDENTIFIER).value\n        self.expect(TokenType.EQ)\n        val = self.parse_primary()\n\
          \        assignments.append((col, val))\n    \n    where = None\n    if self.match(TokenType.WHERE):\n        where\
          \ = self.parse_expr()\n    \n    return UpdateStmt(table, assignments, where)"
      pitfalls:
      - Column/value count mismatch
      - Missing WHERE in DELETE
      - Type mismatches
      concepts:
      - DML parsing
      - Statement types
      - Data modification
      estimated_hours: 4-5
      deliverables:
      - INSERT statement parser supporting column lists and value expressions
      - UPDATE statement parser handling SET clauses and WHERE conditions
      - DELETE statement parser supporting WHERE-filtered row deletion
      - Value list parser handling multiple rows and typed literal expressions
  terminal-multiplexer:
    id: terminal-multiplexer
    name: Terminal Multiplexer
    description: Build a simple terminal multiplexer like tmux/screen. Learn PTY handling, terminal escape sequences, and
      process management.
    difficulty: advanced
    estimated_hours: 30-40
    prerequisites:
    - Unix processes
    - Terminal basics
    - File descriptors
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
    resources:
    - name: PTY Programming
      url: https://www.man7.org/linux/man-pages/man7/pty.7.html
      type: reference
    - name: ANSI Escape Codes
      url: https://en.wikipedia.org/wiki/ANSI_escape_code
      type: reference
    - name: Building a Terminal Emulator
      url: https://www.uninformativ.de/blog/postings/2018-02-24/0/POSTING-en.html
      type: tutorial
    milestones:
    - id: 1
      name: PTY Creation
      description: Create and manage pseudo-terminal pairs for running shell sessions.
      acceptance_criteria:
      - Open PTY master and slave pair with valid file descriptors
      - Fork child process with PTY as controlling terminal
      - Start shell in child process connected to the slave PTY device
      - Handle terminal size changes by forwarding TIOCSWINSZ to child process
      hints:
        level1: posix_openpt() or openpty() create PTY pairs. Child uses slave, parent uses master.
        level2: Child must setsid(), then open slave to make it controlling terminal.
        level3: |-
          import os
          import pty
          import fcntl
          import termios
          import struct

          def create_pty_shell() -> tuple[int, int]:
              '''Create PTY and spawn shell, return (master_fd, child_pid)'''
              master_fd, slave_fd = pty.openpty()

              pid = os.fork()

              if pid == 0:
                  # Child process
                  os.close(master_fd)

                  # Create new session
                  os.setsid()

                  # Slave becomes controlling terminal
                  os.dup2(slave_fd, 0)  # stdin
                  os.dup2(slave_fd, 1)  # stdout
                  os.dup2(slave_fd, 2)  # stderr

                  if slave_fd > 2:
                      os.close(slave_fd)

                  # Exec shell
                  shell = os.environ.get('SHELL', '/bin/sh')
                  os.execvp(shell, [shell])

              # Parent
              os.close(slave_fd)
              return master_fd, pid

          def set_terminal_size(fd: int, rows: int, cols: int):
              '''Set terminal size via ioctl'''
              winsize = struct.pack('HHHH', rows, cols, 0, 0)
              fcntl.ioctl(fd, termios.TIOCSWINSZ, winsize)

          # C version:
          '''
          int master_fd = posix_openpt(O_RDWR | O_NOCTTY);
          grantpt(master_fd);
          unlockpt(master_fd);
          char *slave_name = ptsname(master_fd);
          '''
      pitfalls:
      - Must call setsid() in child before opening slave
      - File descriptors need careful management across fork
      - SIGCHLD handling for child termination
      concepts:
      - Pseudo-terminals
      - Process groups
      - Controlling terminals
      estimated_hours: 6-8
      deliverables:
      - Pseudoterminal allocation using posix_openpt or equivalent system call
      - Master and slave file descriptor setup with correct terminal attributes
      - PTY I/O handling for reading output and writing input between processes
      - Window size management forwarding TIOCSWINSZ ioctl on terminal resize
    - id: 2
      name: Terminal Emulation
      description: Parse and render terminal escape sequences for display.
      acceptance_criteria:
      - Handle cursor movement (up, down, left, right)
      - Handle clear screen and clear line
      - Handle text attributes including bold, underline, and 256-color rendering
      - Maintain virtual screen buffer reflecting current terminal display state accurately
      hints:
        level1: ANSI escapes start with ESC[ (0x1B 0x5B). Parse parameters between ESC[ and command letter.
        level2: Maintain grid of cells with character and attributes. Render to real terminal.
        level3: |-
          from dataclasses import dataclass
          from enum import IntFlag

          class Attr(IntFlag):
              NORMAL = 0
              BOLD = 1
              DIM = 2
              UNDERLINE = 4
              REVERSE = 8

          @dataclass
          class Cell:
              char: str = ' '
              fg: int = 7  # White
              bg: int = 0  # Black
              attr: Attr = Attr.NORMAL

          class Screen:
              def __init__(self, rows: int, cols: int):
                  self.rows = rows
                  self.cols = cols
                  self.cursor_row = 0
                  self.cursor_col = 0
                  self.cells = [[Cell() for _ in range(cols)] for _ in range(rows)]
                  self.current_attr = Attr.NORMAL
                  self.current_fg = 7
                  self.current_bg = 0

              def write(self, data: bytes):
                  '''Process output from PTY'''
                  i = 0
                  while i < len(data):
                      if data[i] == 0x1B and i+1 < len(data) and data[i+1] == 0x5B:
                          # ESC[ sequence
                          end, params, cmd = self.parse_csi(data, i+2)
                          self.handle_csi(params, cmd)
                          i = end
                      elif data[i] == ord('\n'):
                          self.cursor_row = min(self.cursor_row + 1, self.rows - 1)
                          i += 1
                      elif data[i] == ord('\r'):
                          self.cursor_col = 0
                          i += 1
                      else:
                          self.put_char(chr(data[i]))
                          i += 1

              def handle_csi(self, params: list[int], cmd: str):
                  '''Handle CSI escape sequence'''
                  if cmd == 'H':  # Cursor position
                      row = params[0] - 1 if params else 0
                      col = params[1] - 1 if len(params) > 1 else 0
                      self.cursor_row = max(0, min(row, self.rows - 1))
                      self.cursor_col = max(0, min(col, self.cols - 1))
                  elif cmd == 'J':  # Clear screen
                      mode = params[0] if params else 0
                      if mode == 2:  # Clear all
                          self.cells = [[Cell() for _ in range(self.cols)]
                                        for _ in range(self.rows)]
                  elif cmd == 'm':  # SGR (text attributes)
                      self.handle_sgr(params)
      pitfalls:
      - Many escape sequences have optional parameters
      - UTF-8 characters can span multiple bytes
      - Some sequences are terminal-specific
      concepts:
      - Terminal emulation
      - ANSI escape codes
      - Screen buffers
      estimated_hours: 8-10
      deliverables:
      - ANSI escape sequence parser handling CSI, OSC, and control character codes
      - Cursor movement tracker maintaining row and column position state
      - Screen buffer storing character cells with attributes for each terminal line
      - Scrollback buffer retaining lines that scroll off the top of the visible area
    - id: 3
      name: Window Management
      description: Support multiple panes in a split-screen layout.
      acceptance_criteria:
      - Vertical and horizontal splits create new panes with independent PTY sessions
      - Switch focus between panes using keyboard shortcuts for directional navigation
      - Resize panes by adjusting borders while maintaining minimum pane dimensions
      - Each pane runs an independent shell with its own scrollback buffer
      hints:
        level1: Each pane has its own PTY, screen buffer, and render region.
        level2: 'Use a tree structure: nodes are either splits (container) or panes (leaf).'
        level3: |-
          from dataclasses import dataclass
          from typing import Optional, Union

          @dataclass
          class Rect:
              x: int
              y: int
              width: int
              height: int

          @dataclass
          class Pane:
              id: int
              master_fd: int
              pid: int
              screen: Screen
              rect: Rect

          @dataclass
          class Split:
              direction: str  # 'horizontal' or 'vertical'
              ratio: float  # 0.0-1.0, position of split
              first: Union['Split', Pane]
              second: Union['Split', Pane]

          class Multiplexer:
              def __init__(self, rows: int, cols: int):
                  self.rows = rows
                  self.cols = cols
                  self.root: Union[Split, Pane] = None
                  self.focused_pane: Optional[Pane] = None
                  self.panes: list[Pane] = []
                  self.next_pane_id = 0

              def create_pane(self, rect: Rect) -> Pane:
                  '''Create new pane with PTY'''
                  master_fd, pid = create_pty_shell()
                  screen = Screen(rect.height, rect.width)
                  set_terminal_size(master_fd, rect.height, rect.width)

                  pane = Pane(self.next_pane_id, master_fd, pid, screen, rect)
                  self.next_pane_id += 1
                  self.panes.append(pane)
                  return pane

              def split_pane(self, pane: Pane, direction: str):
                  '''Split pane horizontally or vertically'''
                  old_rect = pane.rect

                  if direction == 'vertical':
                      # Split left/right
                      w1 = old_rect.width // 2
                      w2 = old_rect.width - w1 - 1  # -1 for border
                      rect1 = Rect(old_rect.x, old_rect.y, w1, old_rect.height)
                      rect2 = Rect(old_rect.x + w1 + 1, old_rect.y, w2, old_rect.height)
                  else:
                      # Split top/bottom
                      h1 = old_rect.height // 2
                      h2 = old_rect.height - h1 - 1
                      rect1 = Rect(old_rect.x, old_rect.y, old_rect.width, h1)
                      rect2 = Rect(old_rect.x, old_rect.y + h1 + 1, old_rect.width, h2)

                  # Resize existing pane
                  pane.rect = rect1
                  pane.screen = Screen(rect1.height, rect1.width)
                  set_terminal_size(pane.master_fd, rect1.height, rect1.width)

                  # Create new pane
                  new_pane = self.create_pane(rect2)

                  # Update tree structure
                  # ...
      pitfalls:
      - Resize needs to update PTY size too
      - Border drawing reduces usable space
      - Focus tracking across splits is complex
      concepts:
      - Window management
      - Tree layouts
      - PTY multiplexing
      estimated_hours: 8-10
      deliverables:
      - Pane splitting logic creating vertical and horizontal subdivisions of the window
      - Pane resizing handler adjusting pane dimensions while respecting minimum sizes
      - Window switching mechanism cycling between multiple named window sessions
      - Layout algorithm distributing available space among panes proportionally
    - id: 4
      name: Key Bindings and UI
      description: Add command mode and key bindings for pane management.
      acceptance_criteria:
      - Prefix key (like Ctrl-b) enters command mode
      - Bindings for split, close, navigate, resize
      - Status bar displays current pane index, window name, and system time
      - Render all panes to the terminal with correct border characters between them
      hints:
        level1: Raw terminal mode to capture all keys. Check for prefix, then command key.
        level2: After prefix, next key is command. 'v' = vertical split, 's' = horizontal, arrows = navigate.
        level3: |-
          import select
          import tty
          import sys

          class InputHandler:
              def __init__(self, mux: Multiplexer):
                  self.mux = mux
                  self.prefix = b'\x02'  # Ctrl-B
                  self.in_command_mode = False

                  self.bindings = {
                      ord('v'): self.split_vertical,
                      ord('s'): self.split_horizontal,
                      ord('x'): self.close_pane,
                      ord('o'): self.next_pane,
                  }

              def handle_input(self, data: bytes):
                  '''Process keyboard input'''
                  if self.in_command_mode:
                      self.in_command_mode = False
                      if data in self.bindings:
                          self.bindings[data]()
                      return

                  if data == self.prefix:
                      self.in_command_mode = True
                      return

                  # Forward to focused pane
                  if self.mux.focused_pane:
                      os.write(self.mux.focused_pane.master_fd, data)

          def main_loop(mux: Multiplexer):
              '''Main event loop'''
              # Set terminal to raw mode
              old_settings = termios.tcgetattr(sys.stdin)
              tty.setraw(sys.stdin)

              handler = InputHandler(mux)

              try:
                  while True:
                      # Watch stdin and all PTY masters
                      fds = [sys.stdin] + [p.master_fd for p in mux.panes]
                      readable, _, _ = select.select(fds, [], [], 0.1)

                      for fd in readable:
                          if fd == sys.stdin:
                              data = os.read(sys.stdin.fileno(), 1024)
                              handler.handle_input(data)
                          else:
                              # Find pane and update its screen
                              pane = next(p for p in mux.panes if p.master_fd == fd)
                              data = os.read(fd, 4096)
                              if data:
                                  pane.screen.write(data)

                      # Render all panes
                      mux.render()
              finally:
                  termios.tcsetattr(sys.stdin, termios.TCSADRAIN, old_settings)
      pitfalls:
      - Raw mode disables Ctrl-C - need explicit handling
      - Must restore terminal settings on exit
      - Rendering must be efficient to avoid flicker
      concepts:
      - Raw terminal mode
      - Event loops
      - Key binding systems
      estimated_hours: 8-10
      deliverables:
      - Prefix key handler intercepting a configurable leader key for command mode
      - Custom key binding registry mapping key sequences to multiplexer commands
      - Status bar renderer displaying pane info, session name, and clock on screen
      - Copy mode enabling keyboard-driven text selection and clipboard copy from scrollback
  tetris:
    id: tetris
    name: Tetris
    description: Build the classic Tetris game. Learn about piece rotation, line clearing, and more complex game state management.
    difficulty: beginner
    estimated_hours: 15-20
    prerequisites:
    - Basic programming
    - 2D arrays
    - HTML5 Canvas or similar
    languages:
      recommended:
      - JavaScript
      - Python
      - C#
      also_possible:
      - Rust
      - Go
      - C++
    resources:
    - name: Tetris with JavaScript
      url: https://www.youtube.com/watch?v=rAUn1Lom6dw
      type: video
    - name: Coding Challenges Tetris
      url: https://codingchallenges.fyi/challenges/challenge-tetris/
      type: tutorial
    milestones:
    - id: 1
      name: Board & Tetrominoes
      description: Create the game board and define tetromino shapes.
      acceptance_criteria:
      - 10x20 game board is initialized with all cells empty
      - 7 standard tetromino shapes (I, O, T, S, Z, J, L)
      - Each shape has a distinct color visible on the board
      - Shapes are defined as 2D arrays with rotation offset data
      - Board renders the empty grid with visible cell boundaries
      hints:
        level1: 'Board: 2D array of cell states (0=empty, 1-7=piece colors).'
        level2: Define pieces as 2D arrays, use rotation matrices or store all rotations.
        level3: |-
          const PIECES = {
            I: { shape: [[1,1,1,1]], color: 'cyan' },
            O: { shape: [[1,1],[1,1]], color: 'yellow' },
            T: { shape: [[0,1,0],[1,1,1]], color: 'purple' },
            S: { shape: [[0,1,1],[1,1,0]], color: 'green' },
            Z: { shape: [[1,1,0],[0,1,1]], color: 'red' },
            J: { shape: [[1,0,0],[1,1,1]], color: 'blue' },
            L: { shape: [[0,0,1],[1,1,1]], color: 'orange' }
          };
      pitfalls:
      - Piece definition orientation inconsistent
      - Board dimensions swapped
      - Off-by-one in grid rendering
      concepts:
      - 2D arrays
      - Piece representation
      - Grid rendering
      estimated_hours: 2-3
      deliverables:
      - Game board grid data structure representing a 10-column by 20-row playing field
      - Tetromino shape definitions for all seven standard piece types
      - Tetromino rotation data storing four orientation states for each piece type
      - Color mapping table assigning a distinct color to each tetromino type
    - id: 2
      name: Piece Falling & Controls
      description: Implement falling pieces and player controls.
      acceptance_criteria:
      - Current piece falls automatically at a rate determined by the level
      - Left and right arrow keys move piece one cell horizontally
      - Down arrow accelerates the piece fall speed for soft drop
      - Space bar performs hard drop placing piece instantly at lowest valid position
      - Cannot move piece outside board boundaries or into occupied cells
      hints:
        level1: Track current piece position separately from board state.
        level2: 'Validate move before applying: check bounds and collisions.'
        level3: |-
          function isValidPosition(piece, offsetX = 0, offsetY = 0) {
            const shape = PIECES[piece.type].shape;
            for (let row = 0; row < shape.length; row++) {
              for (let col = 0; col < shape[row].length; col++) {
                if (shape[row][col]) {
                  const newX = piece.x + col + offsetX;
                  const newY = piece.y + row + offsetY;
                  if (newX < 0 || newX >= 10 || newY >= 20) return false;
                  if (newY >= 0 && board[newY][newX]) return false;
                }
              }
            }
            return true;
          }
      pitfalls:
      - Negative Y during spawn
      - Piece moving into placed blocks
      - Hard drop going through floor
      concepts:
      - Position validation
      - Input handling
      - Collision detection
      estimated_hours: 3-4
      deliverables:
      - Piece spawning logic placing new tetrominoes at the top center of the board
      - Gravity timer that automatically drops the current piece at interval ticks
      - Left and right movement handlers shifting the piece horizontally on key press
      - Soft drop and hard drop controls for accelerated and instant piece placement
    - id: 3
      name: Piece Rotation
      description: Implement piece rotation with wall kicks.
      acceptance_criteria:
      - Up arrow rotates the current piece clockwise by one state
      - Rotation works correctly for all seven piece types in all orientations
      - Cannot rotate into walls or pieces
      - 'Wall kick: try offset positions if direct rotation fails'
      - O piece does not rotate and remains in its single orientation
      hints:
        level1: Transpose and reverse rows to rotate 90Â° clockwise.
        level2: 'If rotation fails, try offsets: (1,0), (-1,0), (0,-1).'
        level3: |-
          function rotateShape(shape) {
            const rows = shape.length;
            const cols = shape[0].length;
            const rotated = Array(cols).fill(null).map(() => Array(rows).fill(0));
            for (let r = 0; r < rows; r++) {
              for (let c = 0; c < cols; c++) {
                rotated[c][rows - 1 - r] = shape[r][c];
              }
            }
            return rotated;
          }
      pitfalls:
      - O piece rotation changing position
      - I piece wall kick needs special handling
      - Rotating into ceiling
      concepts:
      - Matrix rotation
      - Wall kicks
      - SRS (Super Rotation System)
      estimated_hours: 3-4
      deliverables:
      - Rotation state machine cycling through four orientations per piece type
      - Wall kick test offsets attempted when rotation collides with boundaries
      - SRS (Super Rotation System) implementing the official rotation and kick tables
      - Collision checking verifying rotated piece fits within the board without overlap
    - id: 4
      name: Line Clearing & Scoring
      description: Implement line clearing and scoring system.
      acceptance_criteria:
      - Filled rows are detected immediately after a piece locks into place
      - Filled rows are removed and visually cleared from the board
      - Rows above cleared lines fall down to fill the gap
      - Score based on lines cleared (1/2/3/4 = 100/300/500/800)
      - Level increases after every ten lines cleared
      - Speed increases with each level reducing the gravity timer interval
      hints:
        level1: 'Check each row: if all cells filled, remove and shift above down.'
        level2: Track lines cleared, increase level every 10 lines.
        level3: |-
          function clearLines() {
            let linesCleared = 0;
            for (let row = board.length - 1; row >= 0; row--) {
              if (board[row].every(cell => cell !== 0)) {
                board.splice(row, 1);
                board.unshift(Array(10).fill(0));
                linesCleared++;
                row++;
              }
            }
            if (linesCleared > 0) {
              const points = [0, 100, 300, 500, 800][linesCleared];
              score += points * level;
            }
          }
      pitfalls:
      - Iterating wrong direction when removing
      - Not re-checking row after splice
      - Tetris (4 lines) scoring wrong
      concepts:
      - Row clearing
      - Score systems
      - Difficulty progression
      estimated_hours: 3-4
      deliverables:
      - Full line detection scanning each row for completely filled cells
      - Line removal logic clearing filled rows and shifting rows above downward
      - Scoring system awarding points based on number of lines cleared simultaneously
      - Level progression system increasing difficulty after a set number of lines
  tokenizer:
    id: tokenizer
    name: Tokenizer/Lexer
    description: Build a lexer that converts source code into tokens. Foundation for all compilers and interpreters.
    difficulty: beginner
    estimated_hours: 8-15
    prerequisites:
    - Regular expressions basics
    - String manipulation
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - Rust
      - C
    resources:
    - name: Crafting Interpreters - Scanning
      url: https://craftinginterpreters.com/scanning.html
      type: book
    - name: Let's Build a Compiler
      url: https://compilers.iecc.com/crenshaw/
      type: tutorial
    milestones:
    - id: 1
      name: Basic Token Types
      description: Define token types and basic structure.
      acceptance_criteria:
      - Token class with type and value
      - Support numbers and identifiers as distinct token types with correct classification
      - Support operators (+, -, *, /)
      - Track position with line and column numbers for each emitted token
      hints:
        level1: Token = (type, value, position). Start simple.
        level2: Use an enum or constants for token types.
        level3: "from enum import Enum, auto\nfrom dataclasses import dataclass\n\nclass TokenType(Enum):\n    # Literals\n\
          \    NUMBER = auto()\n    IDENTIFIER = auto()\n    STRING = auto()\n    \n    # Operators\n    PLUS = auto()\n \
          \   MINUS = auto()\n    STAR = auto()\n    SLASH = auto()\n    \n    # Delimiters\n    LPAREN = auto()\n    RPAREN\
          \ = auto()\n    SEMICOLON = auto()\n    \n    # Keywords\n    IF = auto()\n    ELSE = auto()\n    WHILE = auto()\n\
          \    \n    # Special\n    EOF = auto()\n\n@dataclass\nclass Token:\n    type: TokenType\n    value: str\n    line:\
          \ int\n    column: int\n    \n    def __repr__(self):\n        return f'Token({self.type.name}, {repr(self.value)},\
          \ {self.line}:{self.column})'"
      pitfalls:
      - Forgetting EOF token
      - Not tracking position
      - Inconsistent naming
      concepts:
      - Tokens
      - Lexemes
      - Token types
      estimated_hours: 1-2
      deliverables:
      - Token type enumeration defining all recognized token categories
      - Token structure holding type, lexeme value, and source position
      - Position tracking recording line number and column for each token
      - Token list output producing a complete ordered sequence of recognized tokens
    - id: 2
      name: Scanning Logic
      description: Implement the main scanning loop.
      acceptance_criteria:
      - Iterate through source code character by character producing tokens
      - Match single-character tokens and emit them with correct type
      - Handle whitespace and newlines by skipping without emitting tokens
      - Report lexical errors with line and column position of the offending character
      hints:
        level1: Maintain current position, peek ahead when needed.
        level2: advance() returns current char and moves forward. peek() looks ahead without moving.
        level3: "class Lexer:\n    def __init__(self, source):\n        self.source = source\n        self.pos = 0\n     \
          \   self.line = 1\n        self.column = 1\n        self.tokens = []\n    \n    def is_at_end(self):\n        return\
          \ self.pos >= len(self.source)\n    \n    def advance(self):\n        char = self.source[self.pos]\n        self.pos\
          \ += 1\n        if char == '\\n':\n            self.line += 1\n            self.column = 1\n        else:\n    \
          \        self.column += 1\n        return char\n    \n    def peek(self):\n        if self.is_at_end():\n      \
          \      return '\\0'\n        return self.source[self.pos]\n    \n    def add_token(self, type, value=''):\n    \
          \    self.tokens.append(Token(type, value, self.line, self.column))\n    \n    def scan_tokens(self):\n        while\
          \ not self.is_at_end():\n            self.scan_token()\n        self.add_token(TokenType.EOF)\n        return self.tokens\n\
          \    \n    def scan_token(self):\n        char = self.advance()\n        \n        if char in ' \\t\\r\\n':\n  \
          \          return  # Skip whitespace\n        elif char == '+':\n            self.add_token(TokenType.PLUS, char)\n\
          \        elif char == '-':\n            self.add_token(TokenType.MINUS, char)\n        # ... more cases\n      \
          \  else:\n            raise LexerError(f'Unexpected character: {char}', self.line, self.column)"
      pitfalls:
      - Off-by-one errors
      - Not handling newlines
      - Consuming too much
      concepts:
      - Scanning
      - Character stream
      - Position tracking
      estimated_hours: 2-3
      deliverables:
      - Character consumption function advancing the read position in source text
      - Peek and advance methods inspecting upcoming characters without consuming them
      - Whitespace handling logic skipping spaces, tabs, and newline characters
      - EOF handling logic emitting an end-of-file token when input is exhausted
    - id: 3
      name: Multi-character Tokens
      description: Handle numbers, identifiers, strings, and multi-char operators.
      acceptance_criteria:
      - Scan integers and floats including decimal point and digit sequences
      - Scan identifiers starting with a letter or underscore followed by alphanumeric characters
      - Distinguish keywords from identifiers using a reserved word lookup table
      - Handle ==, !=, <=, >= operators
      hints:
        level1: When you see a digit, keep consuming digits. Same for identifiers.
        level2: Use a keyword lookup table to convert identifiers to keywords.
        level3: "KEYWORDS = {\n    'if': TokenType.IF,\n    'else': TokenType.ELSE,\n    'while': TokenType.WHILE,\n    'for':\
          \ TokenType.FOR,\n    'return': TokenType.RETURN,\n    'true': TokenType.TRUE,\n    'false': TokenType.FALSE,\n\
          }\n\ndef scan_number(self):\n    start = self.pos - 1\n    while self.peek().isdigit():\n        self.advance()\n\
          \    \n    # Look for decimal part\n    if self.peek() == '.' and self.peek_next().isdigit():\n        self.advance()\
          \  # consume '.'\n        while self.peek().isdigit():\n            self.advance()\n    \n    value = self.source[start:self.pos]\n\
          \    self.add_token(TokenType.NUMBER, value)\n\ndef scan_identifier(self):\n    start = self.pos - 1\n    while\
          \ self.peek().isalnum() or self.peek() == '_':\n        self.advance()\n    \n    text = self.source[start:self.pos]\n\
          \    token_type = KEYWORDS.get(text, TokenType.IDENTIFIER)\n    self.add_token(token_type, text)\n\ndef match(self,\
          \ expected):\n    '''Consume next char if it matches expected'''\n    if self.is_at_end() or self.source[self.pos]\
          \ != expected:\n        return False\n    self.pos += 1\n    self.column += 1\n    return True\n\n# In scan_token:\n\
          if char == '=':\n    if self.match('='):\n        self.add_token(TokenType.EQUAL_EQUAL, '==')\n    else:\n     \
          \   self.add_token(TokenType.EQUAL, '=')"
      pitfalls:
      - Not handling '.' in floats correctly
      - Keywords vs identifiers
      - Multi-char operators
      concepts:
      - Maximal munch
      - Keyword tables
      - Lookahead
      estimated_hours: 3-4
      deliverables:
      - Multi-character operator scanning for ==, !=, <=, and >= tokens
      - Identifier scanner consuming alphanumeric characters and underscores into a single token
      - Keyword recognizer distinguishing reserved words from user-defined identifiers
      - Number literal scanner consuming integer and floating-point digit sequences
    - id: 4
      name: Strings and Comments
      description: Handle string literals and comments.
      acceptance_criteria:
      - String literals parse correctly including escape sequences like \n and \t
      - Single-line comments starting with // are skipped until end of line
      - Multi-line comments between /* and */ are skipped including nested newlines
      - Handle unterminated strings and comments by reporting an error with position
      hints:
        level1: 'Strings: consume until closing quote. Handle \n, \t, \\.'
        level2: For multi-line comments, track nesting or just find */.
        level3: "def scan_string(self):\n    start_line = self.line\n    value = ''\n    \n    while self.peek() != '\"' and\
          \ not self.is_at_end():\n        if self.peek() == '\\\\':\n            self.advance()  # consume backslash\n  \
          \          escape = self.advance()\n            if escape == 'n':\n                value += '\\n'\n            elif\
          \ escape == 't':\n                value += '\\t'\n            elif escape == '\\\\':\n                value += '\\\
          \\'\n            elif escape == '\"':\n                value += '\"'\n            else:\n                raise LexerError(f'Invalid\
          \ escape: \\\\{escape}', self.line, self.column)\n        else:\n            value += self.advance()\n    \n   \
          \ if self.is_at_end():\n        raise LexerError('Unterminated string', start_line, self.column)\n    \n    self.advance()\
          \  # closing \"\n    self.add_token(TokenType.STRING, value)\n\ndef skip_comment(self):\n    if self.peek() == '/':\n\
          \        # Single line comment\n        while self.peek() != '\\n' and not self.is_at_end():\n            self.advance()\n\
          \    elif self.peek() == '*':\n        # Multi-line comment\n        self.advance()  # consume *\n        while\
          \ True:\n            if self.is_at_end():\n                raise LexerError('Unterminated comment', self.line, self.column)\n\
          \            if self.peek() == '*' and self.peek_next() == '/':\n                self.advance()  # *\n         \
          \       self.advance()  # /\n                break\n            self.advance()"
      pitfalls:
      - Unterminated strings across lines
      - Nested comments
      - Escape sequences
      concepts:
      - String literals
      - Comments
      - Escape sequences
      estimated_hours: 2-3
      deliverables:
      - String literal scanner consuming characters between matching quote delimiters
      - Escape sequence handler interpreting backslash-prefixed special characters in strings
      - Single-line comment scanner skipping characters from // to end of line
      - Multi-line comment scanner skipping characters between /* and */ delimiters
  topdown-shooter:
    id: topdown-shooter
    name: Top-down Shooter
    description: Build a top-down shooter with enemies, projectiles, and waves. Learn game AI, collision systems, and game
      feel.
    difficulty: intermediate
    estimated_hours: 20-30
    prerequisites:
    - Basic game loop
    - 2D graphics
    - Vector math
    languages:
      recommended:
      - JavaScript
      - Python
      - C#
      also_possible:
      - C++
      - Lua
    resources:
    - name: Game Programming Patterns
      url: https://gameprogrammingpatterns.com/
      type: book
    milestones:
    - id: 1
      name: Player Movement & Aiming
      description: Implement player with 8-directional movement and mouse aiming.
      acceptance_criteria:
      - WASD movement supports eight directions including diagonal combinations
      - Mouse aiming rotates the player sprite to face the cursor in real time
      - Smooth movement with acceleration ramps up speed gradually on key hold
      - Screen bounds checking prevents the player from moving outside visible area
      hints:
        level1: Normalize diagonal movement to avoid faster speed.
        level2: 'Angle to mouse: atan2(mouse.y - player.y, mouse.x - player.x).'
        level3: "class Player:\n    def update(self, dt):\n        # Input\n        dx = (keys['d'] - keys['a'])\n       \
          \ dy = (keys['s'] - keys['w'])\n        \n        # Normalize diagonal\n        if dx != 0 and dy != 0:\n      \
          \      dx *= 0.707  # 1/sqrt(2)\n            dy *= 0.707\n        \n        # Apply movement\n        self.vx =\
          \ dx * SPEED\n        self.vy = dy * SPEED\n        self.x += self.vx * dt\n        self.y += self.vy * dt\n   \
          \     \n        # Aim at mouse\n        self.angle = math.atan2(mouse_y - self.y, mouse_x - self.x)\n        \n\
          \        # Bounds\n        self.x = max(0, min(SCREEN_WIDTH, self.x))\n        self.y = max(0, min(SCREEN_HEIGHT,\
          \ self.y))"
      pitfalls:
      - Diagonal speed boost
      - Angle in wrong units
      - Jittery movement
      concepts:
      - 8-directional movement
      - Mouse aiming
      - Vector normalization
      estimated_hours: 2-3
      deliverables:
      - WASD movement handler translating key presses into velocity vectors
      - Mouse aiming system rotating the player sprite toward the cursor position
      - Player sprite rotation rendering the character facing the aim direction
      - Movement speed configuration with acceleration and deceleration curves
    - id: 2
      name: Shooting & Projectiles
      description: Implement projectile system with firing and collision.
      acceptance_criteria:
      - Click to fire a projectile from the player's current position
      - Projectile travels in the aimed direction at a constant speed
      - Fire rate limiting enforces a cooldown period between consecutive shots
      - Projectile-enemy collision deals damage and destroys the projectile on hit
      - Remove off-screen projectiles to free memory and maintain performance
      hints:
        level1: Projectile velocity = (cos(angle), sin(angle)) * speed.
        level2: Track last fire time for fire rate.
        level3: "class Projectile:\n    def __init__(self, x, y, angle, speed=500):\n        self.x = x\n        self.y =\
          \ y\n        self.vx = math.cos(angle) * speed\n        self.vy = math.sin(angle) * speed\n        self.radius =\
          \ 5\n        self.damage = 10\n    \n    def update(self, dt):\n        self.x += self.vx * dt\n        self.y +=\
          \ self.vy * dt\n    \n    def is_offscreen(self):\n        return self.x < 0 or self.x > SCREEN_WIDTH or self.y\
          \ < 0 or self.y > SCREEN_HEIGHT\n\nclass Player:\n    def __init__(self):\n        self.fire_cooldown = 0\n    \
          \    self.fire_rate = 0.1  # seconds between shots\n    \n    def shoot(self, projectiles):\n        if self.fire_cooldown\
          \ <= 0:\n            proj = Projectile(self.x, self.y, self.angle)\n            projectiles.append(proj)\n     \
          \       self.fire_cooldown = self.fire_rate\n    \n    def update(self, dt):\n        self.fire_cooldown -= dt"
      pitfalls:
      - Projectile spawn position
      - Fire rate timing
      - Memory from many projectiles
      concepts:
      - Projectile physics
      - Fire rate
      - Object pooling
      estimated_hours: 3-4
      deliverables:
      - Projectile spawning system creating bullets at the player's weapon position
      - Projectile movement controller updating bullet positions each frame along their trajectory
      - Projectile collision detection checking hits against enemy bounding boxes
      - Fire rate limiter enforcing minimum cooldown between consecutive shots
    - id: 3
      name: Enemies & AI
      description: Add enemies with simple AI behaviors.
      acceptance_criteria:
      - Enemy chases the player using direct line-of-sight movement each frame
      - Different enemy types have varying speed, health, and damage attributes
      - Health and damage system correctly reduces HP on hit and triggers death at zero
      - Death removes the enemy and spawning places new enemies at wave intervals
      hints:
        level1: 'Chase: direction = normalize(player.pos - enemy.pos).'
        level2: 'Add variety: fast/weak, slow/strong, ranged.'
        level3: "class Enemy:\n    def __init__(self, x, y, enemy_type='basic'):\n        self.x = x\n        self.y = y\n\
          \        if enemy_type == 'basic':\n            self.speed = 100\n            self.health = 30\n            self.damage\
          \ = 10\n        elif enemy_type == 'fast':\n            self.speed = 200\n            self.health = 15\n       \
          \     self.damage = 5\n        elif enemy_type == 'tank':\n            self.speed = 50\n            self.health\
          \ = 100\n            self.damage = 25\n    \n    def update(self, dt, player):\n        # Chase player\n       \
          \ dx = player.x - self.x\n        dy = player.y - self.y\n        dist = math.sqrt(dx*dx + dy*dy)\n        if dist\
          \ > 0:\n            self.x += (dx/dist) * self.speed * dt\n            self.y += (dy/dist) * self.speed * dt\n \
          \   \n    def take_damage(self, amount):\n        self.health -= amount\n        return self.health <= 0  # Returns\
          \ True if dead"
      pitfalls:
      - Division by zero in normalize
      - Enemy stacking
      - Instant kill on spawn
      concepts:
      - Enemy AI
      - Health systems
      - Enemy types
      estimated_hours: 4-5
      deliverables:
      - Enemy spawning system placing new enemies at random positions outside the screen
      - Chase behavior AI moving enemies toward the player's current position each frame
      - Enemy health system tracking hit points and triggering death on zero HP
      - Enemy-player collision handler dealing contact damage when enemies reach the player
    - id: 4
      name: Waves & Scoring
      description: Implement wave system and scoring.
      acceptance_criteria:
      - Wave-based spawning releases enemies in defined groups with rest periods between
      - Difficulty increases each wave with more and tougher enemies
      - Score increments for each enemy kill based on enemy type value
      - High score tracking persists the best score across game sessions
      - Wave clear bonus awards extra points when all enemies in a wave are defeated
      hints:
        level1: Wave spawns N enemies. N increases each wave.
        level2: Track enemies alive, start next wave when zero.
        level3: "class WaveManager:\n    def __init__(self):\n        self.wave = 0\n        self.enemies_to_spawn = 0\n \
          \       self.spawn_timer = 0\n        self.spawn_interval = 1.0\n    \n    def start_wave(self, enemies):\n    \
          \    self.wave += 1\n        self.enemies_to_spawn = 5 + self.wave * 2\n        self.spawn_timer = 0\n    \n   \
          \ def update(self, dt, enemies):\n        if self.enemies_to_spawn > 0:\n            self.spawn_timer -= dt\n  \
          \          if self.spawn_timer <= 0:\n                self.spawn_enemy(enemies)\n                self.enemies_to_spawn\
          \ -= 1\n                self.spawn_timer = self.spawn_interval\n        elif len(enemies) == 0:\n            self.start_wave(enemies)\n\
          \    \n    def spawn_enemy(self, enemies):\n        # Spawn at random edge\n        side = random.randint(0, 3)\n\
          \        if side == 0: x, y = random.randint(0, WIDTH), 0\n        elif side == 1: x, y = WIDTH, random.randint(0,\
          \ HEIGHT)\n        # ...\n        enemy_type = random.choice(['basic', 'basic', 'fast', 'tank'])\n        enemies.append(Enemy(x,\
          \ y, enemy_type))"
      pitfalls:
      - Infinite spawn loop
      - No break between waves
      - Unfair difficulty spike
      concepts:
      - Wave systems
      - Difficulty scaling
      - Score systems
      estimated_hours: 3-4
      deliverables:
      - Wave system spawning groups of enemies with increasing count per wave
      - Score tracking system awarding points for each enemy kill and wave completion
      - Difficulty scaling increasing enemy count, speed, and health each wave
      - Game over condition triggered when the player's health reaches zero
  transformer-scratch:
    id: transformer-scratch
    name: Transformer from Scratch
    description: Implement the Transformer architecture from 'Attention Is All You Need'. Learn self-attention and modern
      NLP architectures.
    difficulty: advanced
    estimated_hours: 30-50
    prerequisites:
    - Neural networks
    - Linear algebra
    - Python/PyTorch basics
    languages:
      recommended:
      - Python
      also_possible:
      - Julia
    resources:
    - name: Attention Is All You Need Paper
      url: https://arxiv.org/abs/1706.03762
      type: paper
    - name: The Illustrated Transformer
      url: https://jalammar.github.io/illustrated-transformer/
      type: article
    - name: Annotated Transformer
      url: https://nlp.seas.harvard.edu/2018/04/03/attention.html
      type: tutorial
    milestones:
    - id: 1
      name: Scaled Dot-Product Attention
      description: Implement the core attention mechanism.
      acceptance_criteria:
      - Compute Q, K, V from input
      - Scaled dot-product computes softmax(QK^T / sqrt(d_k))V with correct dimensions
      - Handle attention mask for padding tokens and causal autoregressive decoding
      - Vectorized implementation processes full batch without explicit Python loops
      hints:
        level1: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V
        level2: 'Mask: set masked positions to -inf before softmax.'
        level3: "import torch\nimport torch.nn.functional as F\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n\
          \    # Q, K, V: (batch, seq_len, d_k)\n    d_k = Q.size(-1)\n    \n    # Compute attention scores\n    scores =\
          \ torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)\n    # scores: (batch, seq_len, seq_len)\n    \n    # Apply\
          \ mask\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    \n    # Softmax and\
          \ apply to values\n    attn_weights = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn_weights, V)\n  \
          \  \n    return output, attn_weights"
      pitfalls:
      - Forgetting to scale by sqrt(d_k)
      - Wrong mask application
      - Dimension mismatches
      concepts:
      - Attention mechanism
      - Query-Key-Value
      - Masking
      estimated_hours: 4-6
      deliverables:
      - Q, K, V matrix computation from input embeddings via learned linear projections
      - Attention score calculation using dot product between query and key matrices
      - Softmax normalization converting raw scores into a probability distribution
      - Weighted sum computation combining value vectors using attention probabilities
    - id: 2
      name: Multi-Head Attention
      description: Implement multi-head attention with multiple parallel heads.
      acceptance_criteria:
      - Split input into h attention heads with d_k = d_model / h dimensions each
      - Project Q, K, V for each head
      - Parallel attention computation processes all heads in a single batched operation
      - Concatenate head outputs and project through output linear layer
      hints:
        level1: Split d_model into h heads of d_k = d_model/h each.
        level2: 'Linear projections: W_q, W_k, W_v, W_o.'
        level3: "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n\
          \        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model,\
          \ d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n   \
          \     self.W_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, Q, K, V, mask=None):\n        batch_size\
          \ = Q.size(0)\n        \n        # Linear projections and reshape to (batch, heads, seq, d_k)\n        Q = self.W_q(Q).view(batch_size,\
          \ -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,\
          \ 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n       \
          \ # Attention\n        attn_output, _ = scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Concatenate\
          \ heads\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n\
          \        \n        return self.W_o(attn_output)"
      pitfalls:
      - Reshape vs view errors
      - Transpose dimensions wrong
      - Not using contiguous()
      concepts:
      - Multi-head attention
      - Parallel computation
      - Linear projections
      estimated_hours: 4-6
      deliverables:
      - Head splitting logic partitioning Q, K, V into h parallel attention heads
      - Parallel attention computation running scaled dot-product on each head independently
      - Head concatenation merging outputs from all attention heads into a single tensor
      - Output projection layer applying a linear transformation to concatenated head outputs
    - id: 3
      name: Position-wise Feed-Forward & Embeddings
      description: Implement FFN layer and positional embeddings.
      acceptance_criteria:
      - Two-layer FFN with ReLU activation and configurable hidden dimension
      - Positional encoding uses sinusoidal functions with period varying by dimension
      - Token embeddings map each vocabulary index to a learned dense vector
      - Embedding scaling multiplies embeddings by sqrt(d_model) before adding positional encoding
      hints:
        level1: 'FFN: Linear(d_model, d_ff) -> ReLU -> Linear(d_ff, d_model).'
        level2: 'Positional: PE(pos, 2i) = sin(pos/10000^(2i/d_model)).'
        level3: "class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n\
          \        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n\
          \        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        \n\
          \        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n  \
          \      \n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n    \n \
          \   def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass FeedForward(nn.Module):\n    def __init__(self,\
          \ d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n  \
          \      self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self,\
          \ x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
      pitfalls:
      - Positional encoding dimensions
      - Forgetting dropout
      - Embedding scale factor
      concepts:
      - Positional encoding
      - Feed-forward networks
      - Residual connections
      estimated_hours: 4-6
      deliverables:
      - Feed-forward network with two linear layers and a ReLU activation between them
      - Token embedding lookup table mapping vocabulary indices to dense vectors
      - Positional encoding using sinusoidal functions at different frequencies per dimension
      - Embedding combination layer adding token embeddings and positional encodings element-wise
    - id: 4
      name: Encoder & Decoder Layers
      description: Combine components into encoder/decoder layers.
      acceptance_criteria:
      - Encoder layer applies self-attention followed by feed-forward with residual connections
      - 'Decoder: masked self-attn + cross-attn + FFN'
      - Layer normalization is applied before or after each sublayer per configuration
      - Residual connections add sublayer input to its output before normalization
      hints:
        level1: 'Each sublayer: LayerNorm(x + Sublayer(x)).'
        level2: Decoder has extra cross-attention to encoder output.
        level3: "class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n\
          \        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model, d_ff,\
          \ dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout\
          \ = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Self attention with residual\n  \
          \      attn_out = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        # FFN\
          \ with residual\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_out))\n        return\
          \ x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n\
          \        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model,\
          \ num_heads)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n\
          \        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)"
      pitfalls:
      - Pre-norm vs post-norm
      - Causal mask in decoder
      - Cross-attention key/value source
      concepts:
      - Encoder-decoder architecture
      - Layer normalization
      - Residual connections
      estimated_hours: 5-8
      deliverables:
      - Encoder layer combining self-attention and feed-forward with residual connections
      - Decoder layer combining masked self-attention, cross-attention, and feed-forward sublayers
      - Layer stacking mechanism composing N identical layers into encoder and decoder stacks
      - Masked attention in decoder preventing positions from attending to future tokens
    - id: 5
      name: Full Transformer & Training
      description: Assemble full transformer and train on a task.
      acceptance_criteria:
      - Stack N encoder and decoder layers with shared or separate parameters
      - Output projection maps decoder output to vocabulary-sized logits for prediction
      - Label smoothing regularization distributes probability mass to non-target tokens (optional)
      - Train on translation or LM task
      hints:
        level1: Stack 6 encoder layers + 6 decoder layers (original paper).
        level2: For language modeling, decoder-only with causal mask.
        level3: "class Transformer(nn.Module):\n    def __init__(self, src_vocab, tgt_vocab, d_model=512, num_heads=8, num_layers=6,\
          \ d_ff=2048):\n        super().__init__()\n        self.encoder_embed = nn.Embedding(src_vocab, d_model)\n     \
          \   self.decoder_embed = nn.Embedding(tgt_vocab, d_model)\n        self.pos_encoding = PositionalEncoding(d_model)\n\
          \        \n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, 0.1) for _ in range(num_layers)])\n\
          \        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, 0.1) for _ in range(num_layers)])\n\
          \        \n        self.output_proj = nn.Linear(d_model, tgt_vocab)\n        self.scale = d_model ** 0.5\n    \n\
          \    def encode(self, src, src_mask):\n        x = self.pos_encoding(self.encoder_embed(src) * self.scale)\n   \
          \     for layer in self.encoder_layers:\n            x = layer(x, src_mask)\n        return x\n    \n    def decode(self,\
          \ tgt, enc_out, src_mask, tgt_mask):\n        x = self.pos_encoding(self.decoder_embed(tgt) * self.scale)\n    \
          \    for layer in self.decoder_layers:\n            x = layer(x, enc_out, src_mask, tgt_mask)\n        return self.output_proj(x)"
      pitfalls:
      - Mask generation
      - Teacher forcing
      - Learning rate schedule
      concepts:
      - Full transformer
      - Training loop
      - Inference/generation
      estimated_hours: 8-12
      deliverables:
      - Encoder-decoder architecture wiring together N-layer encoder and decoder stacks
      - Training loop with forward pass, loss computation, and backpropagation steps
      - Cross-entropy loss function computed over the output vocabulary logits
      - Inference module performing autoregressive decoding with greedy or beam search
  type-checker:
    id: type-checker
    name: Type Checker
    description: Build a type checker for a statically typed language. Learn type inference, type rules, and semantic analysis.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - AST Builder
    - Type systems basics
    - Recursive algorithms
    languages:
      recommended:
      - OCaml
      - Haskell
      - Rust
      also_possible:
      - Python
      - TypeScript
    resources:
    - name: Types and Programming Languages
      url: https://www.cis.upenn.edu/~bcpierce/tapl/
      type: book
    - name: Write You a Haskell
      url: http://dev.stephendiehl.com/fun/
      type: tutorial
    milestones:
    - id: 1
      name: Type Representation
      description: Define type representations and type environment.
      acceptance_criteria:
      - Primitive types int, bool, and string are represented as distinct type nodes
      - Function types represent parameter types and return type as a single arrow type
      - Type variables for generics are represented as named placeholder type nodes
      - Type environment maps variable names to their declared or inferred types
      hints:
        level1: 'Types are data structures: Int, Bool, Function(params, return), TypeVar(name).'
        level2: 'Type environment: name -> type mapping, with scoping.'
        level3: "from dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n@dataclass\nclass Type:\n \
          \   pass\n\n@dataclass\nclass TInt(Type):\n    def __repr__(self): return 'int'\n\n@dataclass\nclass TBool(Type):\n\
          \    def __repr__(self): return 'bool'\n\n@dataclass\nclass TString(Type):\n    def __repr__(self): return 'string'\n\
          \n@dataclass\nclass TFunction(Type):\n    params: List[Type]\n    ret: Type\n    def __repr__(self): return f'({\"\
          , \".join(map(str, self.params))}) -> {self.ret}'\n\n@dataclass\nclass TVar(Type):\n    '''Type variable for polymorphism'''\n\
          \    name: str\n    def __repr__(self): return self.name\n\nclass TypeEnv:\n    def __init__(self, parent=None):\n\
          \        self.bindings: Dict[str, Type] = {}\n        self.parent = parent\n    \n    def define(self, name: str,\
          \ type: Type):\n        self.bindings[name] = type\n    \n    def lookup(self, name: str) -> Optional[Type]:\n \
          \       if name in self.bindings:\n            return self.bindings[name]\n        if self.parent:\n           \
          \ return self.parent.lookup(name)\n        return None\n    \n    def child(self):\n        return TypeEnv(parent=self)"
      pitfalls:
      - Forgetting unit/void type
      - Mutable type variables
      - Scope handling
      concepts:
      - Type representations
      - Type environments
      - Scoping
      estimated_hours: 3-4
      deliverables:
      - Type AST (primitives, functions, generics)
      - Type environment (symbol to type)
      - Type equality and compatibility checks for structural or nominal comparison
      - Subtyping rules defining when one type can be used in place of another
    - id: 2
      name: Basic Type Checking
      description: Check types for expressions and statements.
      acceptance_criteria:
      - Literals have known types inferred from their syntactic form
      - Binary operators check that both operand types are compatible with the operator
      - Function calls verify that argument types match the declared parameter types
      - Assignments check that the right-hand side type is compatible with the variable type
      hints:
        level1: check(node, env) -> Type. Recursively check children.
        level2: 'Binary +: both operands int -> result int. ==: same types -> bool.'
        level3: "class TypeChecker:\n    def __init__(self):\n        self.errors = []\n    \n    def error(self, message,\
          \ node):\n        self.errors.append(TypeError(message, node))\n    \n    def check_expr(self, expr, env) -> Type:\n\
          \        if isinstance(expr, Literal):\n            if isinstance(expr.value, int):\n                return TInt()\n\
          \            elif isinstance(expr.value, bool):\n                return TBool()\n            elif isinstance(expr.value,\
          \ str):\n                return TString()\n        \n        elif isinstance(expr, Identifier):\n            type\
          \ = env.lookup(expr.name)\n            if type is None:\n                self.error(f'Undefined variable: {expr.name}',\
          \ expr)\n                return TInt()  # Return something to continue\n            return type\n        \n    \
          \    elif isinstance(expr, Binary):\n            left_type = self.check_expr(expr.left, env)\n            right_type\
          \ = self.check_expr(expr.right, env)\n            \n            if expr.operator in ['+', '-', '*', '/']:\n    \
          \            if not isinstance(left_type, TInt) or not isinstance(right_type, TInt):\n                    self.error(f'Operator\
          \ {expr.operator} requires int operands', expr)\n                return TInt()\n            \n            elif expr.operator\
          \ in ['<', '>', '<=', '>=']:\n                if not isinstance(left_type, TInt) or not isinstance(right_type, TInt):\n\
          \                    self.error(f'Comparison requires int operands', expr)\n                return TBool()\n   \
          \         \n            elif expr.operator in ['==', '!=']:\n                if type(left_type) != type(right_type):\n\
          \                    self.error(f'Cannot compare {left_type} and {right_type}', expr)\n                return TBool()\n\
          \        \n        elif isinstance(expr, Call):\n            callee_type = self.check_expr(expr.callee, env)\n \
          \           if not isinstance(callee_type, TFunction):\n                self.error('Cannot call non-function', expr)\n\
          \                return TInt()\n            \n            if len(expr.arguments) != len(callee_type.params):\n \
          \               self.error(f'Expected {len(callee_type.params)} arguments, got {len(expr.arguments)}', expr)\n \
          \           \n            for arg, param_type in zip(expr.arguments, callee_type.params):\n                arg_type\
          \ = self.check_expr(arg, env)\n                if not self.types_match(arg_type, param_type):\n                \
          \    self.error(f'Expected {param_type}, got {arg_type}', arg)\n            \n            return callee_type.ret"
      pitfalls:
      - Operator type rules
      - Function arity
      - Return type tracking
      concepts:
      - Type checking
      - Type rules
      - Error reporting
      estimated_hours: 5-7
      deliverables:
      - Type annotation parser extracting explicit type declarations from source AST
      - Expression type inference deducing types from literals, operators, and context
      - Type compatibility checker verifying assignment and argument type correctness
      - Error reporting module producing clear messages with source location for type mismatches
    - id: 3
      name: Type Inference
      description: Infer types where not explicitly annotated.
      acceptance_criteria:
      - Infer variable types from initializer expressions without explicit annotations
      - Infer function return types from the body expression or return statements
      - Unification algorithm finds most general unifier for a set of type constraints
      - Handle type constraints producing clear errors when constraints are unsatisfiable
      hints:
        level1: 'Inference: generate constraints, then solve (unify).'
        level2: 'Unification: TVar = concrete type, or TVar = TVar.'
        level3: "class TypeInferrer:\n    def __init__(self):\n        self.type_var_counter = 0\n        self.substitution\
          \ = {}  # TVar name -> Type\n    \n    def fresh_type_var(self):\n        self.type_var_counter += 1\n        return\
          \ TVar(f't{self.type_var_counter}')\n    \n    def unify(self, t1: Type, t2: Type):\n        '''Make t1 and t2 the\
          \ same type'''\n        t1 = self.apply_substitution(t1)\n        t2 = self.apply_substitution(t2)\n        \n \
          \       if isinstance(t1, TVar):\n            if t1.name != getattr(t2, 'name', None):\n                # Occurs\
          \ check\n                if self.occurs(t1, t2):\n                    raise TypeError(f'Infinite type: {t1} = {t2}')\n\
          \                self.substitution[t1.name] = t2\n        elif isinstance(t2, TVar):\n            self.unify(t2,\
          \ t1)\n        elif type(t1) == type(t2):\n            if isinstance(t1, TFunction):\n                if len(t1.params)\
          \ != len(t2.params):\n                    raise TypeError(f'Arity mismatch')\n                for p1, p2 in zip(t1.params,\
          \ t2.params):\n                    self.unify(p1, p2)\n                self.unify(t1.ret, t2.ret)\n        else:\n\
          \            raise TypeError(f'Cannot unify {t1} with {t2}')\n    \n    def apply_substitution(self, type: Type)\
          \ -> Type:\n        if isinstance(type, TVar):\n            if type.name in self.substitution:\n               \
          \ return self.apply_substitution(self.substitution[type.name])\n            return type\n        elif isinstance(type,\
          \ TFunction):\n            return TFunction(\n                [self.apply_substitution(p) for p in type.params],\n\
          \                self.apply_substitution(type.ret)\n            )\n        return type\n    \n    def occurs(self,\
          \ tvar: TVar, type: Type) -> bool:\n        '''Check if tvar occurs in type (infinite type check)'''\n        type\
          \ = self.apply_substitution(type)\n        if isinstance(type, TVar):\n            return tvar.name == type.name\n\
          \        elif isinstance(type, TFunction):\n            return any(self.occurs(tvar, p) for p in type.params) or\
          \ self.occurs(tvar, type.ret)\n        return False"
      pitfalls:
      - Infinite types
      - Substitution application
      - Occurs check
      concepts:
      - Type inference
      - Unification
      - Constraints
      estimated_hours: 6-10
      deliverables:
      - Constraint generation pass collecting type equality constraints from expressions
      - Unification algorithm solving type constraints by finding consistent type substitutions
      - Type variable substitution applying solved constraints to replace type variables with concrete types
      - Let-polymorphism generalization abstracting inferred types over free type variables at let bindings
    - id: 4
      name: Polymorphism
      description: Add support for generic/polymorphic types.
      acceptance_criteria:
      - Let polymorphism allows a single definition to be used at multiple types
      - Generalize types at let bindings by quantifying over free type variables
      - Instantiate polymorphic types at use sites
      - Type schemes with forall quantification represent polymorphic types correctly
      hints:
        level1: 'Polymorphism: ''id'' has type forall a. a -> a.'
        level2: 'Generalize: collect free type vars. Instantiate: replace with fresh vars.'
        level3: "@dataclass\nclass Scheme:\n    '''Type scheme: forall [vars] . type'''\n    vars: List[str]  # Quantified\
          \ variables\n    type: Type\n\nclass TypeInferrer:\n    def generalize(self, type: Type, env: TypeEnv) -> Scheme:\n\
          \        '''Generalize type by quantifying free variables not in env'''\n        type = self.apply_substitution(type)\n\
          \        free_in_type = self.free_vars(type)\n        free_in_env = self.free_vars_env(env)\n        quantified\
          \ = free_in_type - free_in_env\n        return Scheme(list(quantified), type)\n    \n    def instantiate(self, scheme:\
          \ Scheme) -> Type:\n        '''Create fresh type variables for quantified vars'''\n        subst = {var: self.fresh_type_var()\
          \ for var in scheme.vars}\n        return self.apply_subst_to_type(scheme.type, subst)\n    \n    def free_vars(self,\
          \ type: Type) -> set:\n        if isinstance(type, TVar):\n            return {type.name}\n        elif isinstance(type,\
          \ TFunction):\n            result = set()\n            for p in type.params:\n                result |= self.free_vars(p)\n\
          \            result |= self.free_vars(type.ret)\n            return result\n        return set()\n\n# Algorithm\
          \ W for let-polymorphism:\n# let x = e1 in e2\n# 1. Infer type of e1\n# 2. Generalize to get scheme\n# 3. Add x:\
          \ scheme to env\n# 4. Infer type of e2 in extended env"
      pitfalls:
      - Value restriction
      - Generalization timing
      - Monomorphization
      concepts:
      - Polymorphism
      - Type schemes
      - Generalization
      estimated_hours: 6-10
      deliverables:
      - Generic type parameters declared on functions and data type definitions
      - Type variable instantiation replacing generic parameters with fresh type variables at use sites
      - Constraint solving resolving type variable bindings during polymorphic function application
      - Generic function call type checking inferring type arguments from actual parameter types
  unit-testing-basics:
    id: unit-testing-basics
    name: Unit Testing Fundamentals
    description: Learn unit testing principles and practices. Write effective tests for confidence in your code.
    difficulty: beginner
    estimated_hours: 6-10
    prerequisites:
    - Basic programming
    - Functions and classes
    languages:
      recommended:
      - Python
      - JavaScript
      - Java
      also_possible:
      - Go
      - Rust
    resources:
    - name: pytest Documentation
      url: https://docs.pytest.org/
      type: documentation
    - name: Jest Documentation
      url: https://jestjs.io/docs/getting-started
      type: documentation
    milestones:
    - id: 1
      name: First Tests
      description: Write your first unit tests.
      acceptance_criteria:
      - Set up test framework with correct project configuration and imports
      - Write test for a pure function verifying expected return values
      - Use assertions assertEqual and assertTrue to verify function behavior
      - Run tests and see results showing pass count and failure details
      - Test edge cases including empty input, boundary values, and null arguments
      hints:
        level1: Test function should start with test_.
        level2: 'Follow the Arrange-Act-Assert pattern: set up test data (arrange), call the function under test (act), then
          verify the result with assertions (assert). Test both normal cases and edge cases like empty inputs.'
        level3: |-
          # calculator.py
          def add(a, b):
              return a + b

          def divide(a, b):
              if b == 0:
                  raise ValueError("Cannot divide by zero")
              return a / b

          # test_calculator.py
          import pytest
          from calculator import add, divide

          def test_add_positive_numbers():
              # Arrange
              a, b = 2, 3
              # Act
              result = add(a, b)
              # Assert
              assert result == 5

          def test_add_negative_numbers():
              assert add(-1, -1) == -2

          def test_add_zero():
              assert add(0, 5) == 5

          def test_divide_normal():
              assert divide(10, 2) == 5

          def test_divide_by_zero():
              with pytest.raises(ValueError):
                  divide(10, 0)
      pitfalls:
      - Testing implementation not behavior
      - Not testing edge cases
      - Fragile assertions
      concepts:
      - Unit testing
      - Assertions
      - Test structure
      estimated_hours: 2-3
      deliverables:
      - Test function creation using the framework's test annotation or naming convention
      - Assertion usage demonstrating assertEqual, assertTrue, and assertRaises patterns
      - Test execution runner discovering and running all test functions in the suite
      - Pass and fail reporting output showing test names, results, and error details
    - id: 2
      name: Test Organization
      description: Organize tests with fixtures and setup.
      acceptance_criteria:
      - Group related tests in classes or modules for logical organization
      - Use setup and teardown to initialize and clean up shared test fixtures
      - Share fixtures between tests using class-level setup or fixture factories
      - Parameterized tests run the same assertion logic across multiple input values
      hints:
        level1: Fixtures provide reusable test data.
        level2: Parameterize to test multiple inputs.
        level3: "import pytest\n\n# Fixtures\n@pytest.fixture\ndef user():\n    return {'name': 'John', 'email': 'john@example.com'}\n\
          \n@pytest.fixture\ndef database():\n    db = connect_test_db()\n    yield db  # Test runs here\n    db.close() \
          \ # Cleanup\n\ndef test_user_creation(user):\n    assert user['name'] == 'John'\n\n# Parameterized tests\n@pytest.mark.parametrize('input,expected',\
          \ [\n    (1, 1),\n    (2, 4),\n    (3, 9),\n    (-2, 4),\n])\ndef test_square(input, expected):\n    assert input\
          \ ** 2 == expected\n\n# Test class for grouping\nclass TestStringMethods:\n    def test_upper(self):\n        assert\
          \ 'hello'.upper() == 'HELLO'\n    \n    def test_split(self):\n        assert 'a,b,c'.split(',') == ['a', 'b', 'c']"
      pitfalls:
      - Fixtures with side effects
      - Shared mutable state
      - Over-complicated fixtures
      concepts:
      - Fixtures
      - Parameterization
      - Test isolation
      estimated_hours: 2-3
      deliverables:
      - Test file structure organized by module with clear naming conventions
      - Test suites and classes grouping related test cases logically
      - Setup and teardown methods running before and after each test or suite
      - Test naming convention using descriptive names that document expected behavior
    - id: 3
      name: Mocking and Isolation
      description: Test code with external dependencies.
      acceptance_criteria:
      - Mock external API calls to return controlled responses in tests
      - Mock file system operations to avoid real disk I/O during tests
      - Verify mock was called with expected arguments and expected call count
      - Patch at the correct import level so the mock replaces the right reference
      hints:
        level1: Mock replaces real object with fake.
        level2: Patch where it's used, not where it's defined.
        level3: "from unittest.mock import Mock, patch\n\n# Code under test\ndef get_user_data(user_id):\n    response = requests.get(f'https://api.example.com/users/{user_id}')\n\
          \    return response.json()\n\ndef process_user(user_id):\n    data = get_user_data(user_id)\n    return data['name'].upper()\n\
          \n# Test with mocking\n@patch('mymodule.requests.get')\ndef test_process_user(mock_get):\n    # Setup mock response\n\
          \    mock_response = Mock()\n    mock_response.json.return_value = {'name': 'john', 'id': 1}\n    mock_get.return_value\
          \ = mock_response\n    \n    # Act\n    result = process_user(1)\n    \n    # Assert\n    assert result == 'JOHN'\n\
          \    mock_get.assert_called_once_with('https://api.example.com/users/1')\n\n# Mock file operations\n@patch('builtins.open',\
          \ mock_open(read_data='file content'))\ndef test_read_config():\n    result = read_config('config.txt')\n    assert\
          \ result == 'file content'"
      pitfalls:
      - Mocking too much
      - Wrong patch location
      - Not verifying interactions
      concepts:
      - Mocking
      - Dependency injection
      - Test isolation
      estimated_hours: 2-3
      deliverables:
      - Mock objects replacing real dependencies with controllable test doubles
      - Dependency injection pattern allowing tests to substitute fake implementations
      - Stub functions returning predefined values for controlled test scenarios
      - Spy objects recording call arguments and counts for interaction verification
  vector-clocks:
    id: vector-clocks
    name: Vector Clocks
    description: Implement vector clocks for tracking causality in distributed systems. Learn about logical time and partial
      ordering.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - Distributed systems basics
    - Understanding of causality
    languages:
      recommended:
      - Python
      - Go
      - Java
      also_possible:
      - Rust
      - JavaScript
    resources:
    - name: Vector Clocks Paper
      url: https://en.wikipedia.org/wiki/Vector_clock
      type: article
    milestones:
    - id: 1
      name: Basic Vector Clock
      description: Implement vector clock data structure and operations.
      acceptance_criteria:
      - Initialize clock for N nodes with all counters set to zero
      - Increment local clock component on each local event at that node
      - Merge clocks on message receive by taking element-wise maximum values
      - Compare clocks returning happens-before, happens-after, or concurrent result
      hints:
        level1: Vector clock is dict/array of counters, one per node.
        level2: 'Merge: take max of each component.'
        level3: "class VectorClock:\n    def __init__(self, node_id, num_nodes):\n        self.node_id = node_id\n       \
          \ self.clock = [0] * num_nodes\n    \n    def increment(self):\n        \"\"\"Called on local event\"\"\"\n    \
          \    self.clock[self.node_id] += 1\n    \n    def update(self, other_clock):\n        \"\"\"Called when receiving\
          \ message\"\"\"\n        for i in range(len(self.clock)):\n            self.clock[i] = max(self.clock[i], other_clock[i])\n\
          \        self.increment()  # Receive is also an event\n    \n    def send(self):\n        \"\"\"Called when sending\
          \ message, returns clock to attach\"\"\"\n        self.increment()\n        return self.clock.copy()\n    \n   \
          \ def compare(self, other):\n        \"\"\"Returns 'before', 'after', 'concurrent', or 'equal'\"\"\"\n        less\
          \ = all(a <= b for a, b in zip(self.clock, other))\n        greater = all(a >= b for a, b in zip(self.clock, other))\n\
          \        if less and greater:\n            return 'equal'\n        if less:\n            return 'before'\n     \
          \   if greater:\n            return 'after'\n        return 'concurrent'"
      pitfalls:
      - Forgetting to increment on receive
      - Not copying clock on send
      - Comparison logic
      concepts:
      - Vector clocks
      - Causality
      - Partial ordering
      estimated_hours: 3-4
      deliverables:
      - Clock initialization creating a zero-valued vector for each participating node
      - Increment operation advancing the local node's counter in the vector on each event
      - Merge operation computing the element-wise maximum of two vector clock values
      - Clock comparison function determining before, after, or concurrent ordering
    - id: 2
      name: Conflict Detection
      description: Use vector clocks to detect conflicting updates.
      acceptance_criteria:
      - Detect concurrent writes to same key
      - Store each version alongside its vector clock for later comparison
      - Return all concurrent versions on read
      - Last-writer-wins resolution picks the version with the highest wall-clock timestamp (optional)
      hints:
        level1: Concurrent clocks = neither happened-before the other.
        level2: Keep list of (value, clock) pairs for conflicts.
        level3: "class VersionedStore:\n    def __init__(self):\n        self.data = {}  # key -> list of (value, clock)\n\
          \    \n    def put(self, key, value, clock):\n        if key not in self.data:\n            self.data[key] = []\n\
          \        \n        # Remove versions this one supersedes\n        new_versions = []\n        for old_value, old_clock\
          \ in self.data[key]:\n            relation = self._compare(old_clock, clock)\n            if relation == 'after'\
          \ or relation == 'equal':\n                # Old version supersedes or equals new, keep it\n                new_versions.append((old_value,\
          \ old_clock))\n            elif relation == 'concurrent':\n                # Conflict! Keep both\n             \
          \   new_versions.append((old_value, old_clock))\n            # 'before' means old is superseded, don't keep\n  \
          \      \n        new_versions.append((value, clock))\n        self.data[key] = new_versions\n    \n    def get(self,\
          \ key):\n        \"\"\"Returns list of (value, clock) - multiple means conflict\"\"\"\n        return self.data.get(key,\
          \ [])"
      pitfalls:
      - Growing version lists without cleanup
      - Comparison direction
      - Lost updates
      concepts:
      - Conflict detection
      - Version vectors
      - Multi-value registers
      estimated_hours: 3-4
      deliverables:
      - Concurrent event detection identifying writes with incomparable vector clocks
      - Happens-before relationship check determining causal ordering between two events
      - Conflict marking flag attached to values with concurrent version histories
      - Conflict resolution strategy supporting last-writer-wins and application-level merge
    - id: 3
      name: Version Pruning
      description: Implement strategies to limit unbounded growth of version history.
      acceptance_criteria:
      - Configurable max versions per key limits the retained history count
      - Prune oldest versions when limit exceeded
      - Optional pruning of dominated versions removes entries superseded by all current versions
      - Garbage collection of unreferenced clocks frees memory for departed nodes
      - Metrics report current version count and pruning activity per key
      hints:
        level1: Track timestamp with each version. Prune by age or count.
        level2: 'Dominated version: another version has higher clock values for all nodes.'
        level3: |-
          class PrunedVersionStore:
              def __init__(self, max_versions=10, max_age_seconds=3600):
                  self.data = {}  # key -> list of (value, clock, timestamp)
                  self.max_versions = max_versions
                  self.max_age = max_age_seconds

              def put(self, key, value, clock):
                  import time
                  now = time.time()

                  if key not in self.data:
                      self.data[key] = []

                  versions = self.data[key]

                  # Remove dominated versions
                  new_versions = []
                  for v, vc, ts in versions:
                      if not self._is_dominated(vc, clock):
                          new_versions.append((v, vc, ts))

                  # Add new version
                  new_versions.append((value, clock.copy(), now))

                  # Prune by age
                  new_versions = [(v, vc, ts) for v, vc, ts in new_versions
                                  if now - ts < self.max_age]

                  # Prune by count (keep most recent)
                  if len(new_versions) > self.max_versions:
                      new_versions.sort(key=lambda x: x[2])  # Sort by timestamp
                      new_versions = new_versions[-self.max_versions:]

                  self.data[key] = new_versions

              def _is_dominated(self, clock1, clock2):
                  '''Returns True if clock1 <= clock2 (clock2 dominates clock1)'''
                  if len(clock1) != len(clock2):
                      return False
                  return all(c1 <= c2 for c1, c2 in zip(clock1, clock2))

              def get_stats(self):
                  total_versions = sum(len(v) for v in self.data.values())
                  return {
                      'keys': len(self.data),
                      'total_versions': total_versions,
                      'avg_versions_per_key': total_versions / max(1, len(self.data))
                  }
      pitfalls:
      - Pruning too aggressively loses conflict info
      - Clock comparison must handle different lengths
      - Timestamp skew across nodes
      - Memory leaks from orphaned clocks
      concepts:
      - Version pruning strategies
      - Garbage collection
      - Dominated versions
      - 'Trade-off: consistency vs storage'
      estimated_hours: 2-3
      deliverables:
      - Clock garbage collection removing entries for nodes that have left the cluster
      - Minimum clock tracking recording the globally known safe pruning threshold
      - Pruning algorithm removing dominated versions that are causally superseded
      - Storage optimization reducing memory usage by compacting version history
    - id: 4
      name: Distributed Key-Value Store
      description: Integrate vector clocks into a working distributed key-value store.
      acceptance_criteria:
      - Multi-node setup with three or more nodes exchanging messages
      - PUT and GET operations include vector clocks for version tracking
      - Read-repair resolves detected conflicts by writing the merged result back
      - Configurable conflict resolution (LWW, merge, manual)
      - Simple replication (all nodes store all keys)
      hints:
        level1: Each node maintains its own vector clock. Attach clock to every write.
        level2: On GET, collect from multiple nodes. If clocks differ, return all versions.
        level3: |-
          import asyncio
          import aiohttp
          from aiohttp import web

          class DistributedKVNode:
              def __init__(self, node_id, peers):
                  self.node_id = node_id
                  self.peers = peers  # list of (host, port)
                  self.store = VersionedStore()
                  self.clock = VectorClock(node_id, len(peers) + 1)

              async def put(self, key, value):
                  '''Write to local and replicate to peers'''
                  clock = self.clock.send()
                  self.store.put(key, value, clock)

                  # Async replicate to peers
                  tasks = [self._replicate_to_peer(peer, key, value, clock)
                           for peer in self.peers]
                  await asyncio.gather(*tasks, return_exceptions=True)

                  return {'status': 'ok', 'clock': clock}

              async def _replicate_to_peer(self, peer, key, value, clock):
                  url = f'http://{peer[0]}:{peer[1]}/internal/replicate'
                  async with aiohttp.ClientSession() as session:
                      await session.post(url, json={
                          'key': key, 'value': value, 'clock': clock
                      })

              async def get(self, key, quorum=2):
                  '''Read from multiple nodes, detect conflicts'''
                  results = [self.store.get(key)]

                  # Query peers
                  tasks = [self._get_from_peer(peer, key) for peer in self.peers]
                  peer_results = await asyncio.gather(*tasks, return_exceptions=True)

                  for r in peer_results:
                      if not isinstance(r, Exception) and r:
                          results.append(r)

                  # Merge all versions
                  all_versions = []
                  for versions in results:
                      all_versions.extend(versions)

                  # Remove dominated versions
                  final = self._remove_dominated(all_versions)

                  if len(final) > 1:
                      return {'status': 'conflict', 'versions': final}
                  elif len(final) == 1:
                      return {'status': 'ok', 'value': final[0][0]}
                  else:
                      return {'status': 'not_found'}

              def _remove_dominated(self, versions):
                  result = []
                  for v1, c1 in versions:
                      dominated = False
                      for v2, c2 in versions:
                          if c1 != c2 and self._is_dominated(c1, c2):
                              dominated = True
                              break
                      if not dominated:
                          result.append((v1, c1))
                  return result

          # HTTP handlers
          routes = web.RouteTableDef()

          @routes.post('/kv/{key}')
          async def handle_put(request):
              key = request.match_info['key']
              data = await request.json()
              result = await node.put(key, data['value'])
              return web.json_response(result)

          @routes.get('/kv/{key}')
          async def handle_get(request):
              key = request.match_info['key']
              result = await node.get(key)
              return web.json_response(result)
      pitfalls:
      - Network partitions causing divergence
      - Replication lag during high write load
      - Clock synchronization across restarts
      - Handling node failures during writes
      concepts:
      - Distributed replication
      - Quorum reads/writes
      - Read repair
      - Eventual consistency
      estimated_hours: 4-6
      deliverables:
      - Versioned values stored alongside their vector clock metadata
      - Read operation returning the value together with its current vector clock
      - Write operation accepting a context vector clock for conflict detection on update
      - Conflict resolution on read merging or surfacing concurrent versions to the client
  video-streaming:
    id: video-streaming
    name: Video Streaming Platform
    description: Build a video streaming service with upload, transcoding, and adaptive playback.
    difficulty: intermediate
    estimated_hours: 30-50
    prerequisites:
    - HTTP server
    - File handling
    - Basic frontend
    languages:
      recommended:
      - Node.js
      - Python
      - Go
      also_possible:
      - Java
      - Rust
    resources:
    - type: article
      name: HLS Streaming Explained
      url: https://www.cloudflare.com/learning/video/what-is-hls-streaming/
    - type: tool
      name: FFmpeg Documentation
      url: https://ffmpeg.org/documentation.html
    milestones:
    - id: 1
      name: Video Upload
      description: Handle large video file uploads with progress tracking.
      acceptance_criteria:
      - Chunked upload support allows resumable uploads of large video files
      - Progress tracking reports bytes uploaded and estimated completion percentage
      - File validation rejects unsupported formats and files exceeding size limits
      - Storage management organizes uploaded files with unique identifiers and directory structure
      hints:
        level1: Use multipart upload for large files. Validate MIME types.
        level2: Resumable uploads with chunk tracking. Store metadata separately.
        level3: "// Chunked upload endpoint\nconst uploads = new Map();  // uploadId -> chunks\n\napp.post('/api/upload/init',\
          \ (req, res) => {\n    const { filename, fileSize, mimeType } = req.body;\n    \n    // Validate\n    if (!mimeType.startsWith('video/'))\
          \ {\n        return res.status(400).json({ error: 'Must be video' });\n    }\n    \n    const uploadId = crypto.randomUUID();\n\
          \    const chunkSize = 5 * 1024 * 1024;  // 5MB chunks\n    const totalChunks = Math.ceil(fileSize / chunkSize);\n\
          \    \n    uploads.set(uploadId, {\n        filename,\n        fileSize,\n        chunkSize,\n        totalChunks,\n\
          \        receivedChunks: new Set(),\n        tempPath: `/tmp/uploads/${uploadId}`\n    });\n    \n    fs.mkdirSync(`/tmp/uploads/${uploadId}`,\
          \ { recursive: true });\n    \n    res.json({ uploadId, chunkSize, totalChunks });\n});\n\napp.post('/api/upload/:uploadId/chunk/:chunkIndex',\
          \ async (req, res) => {\n    const { uploadId, chunkIndex } = req.params;\n    const upload = uploads.get(uploadId);\n\
          \    \n    if (!upload) return res.status(404).json({ error: 'Upload not found' });\n    \n    // Save chunk\n \
          \   const chunkPath = `${upload.tempPath}/chunk_${chunkIndex}`;\n    await pipeline(req, fs.createWriteStream(chunkPath));\n\
          \    \n    upload.receivedChunks.add(parseInt(chunkIndex));\n    \n    // Check if complete\n    if (upload.receivedChunks.size\
          \ === upload.totalChunks) {\n        // Trigger assembly\n        assembleChunks(uploadId);\n    }\n    \n    res.json({\n\
          \        received: upload.receivedChunks.size,\n        total: upload.totalChunks\n    });\n});"
      pitfalls:
      - Memory issues with large files
      - Incomplete uploads
      - Storage cleanup
      concepts:
      - Chunked transfer
      - Resumable uploads
      - File validation
      estimated_hours: 5-8
      deliverables:
      - Upload endpoint accepting video file submissions via multipart form POST
      - Chunked upload support allowing large files to be sent in sequential parts
      - Video storage service persisting uploaded files to object storage or local disk
      - Metadata extraction service reading duration, resolution, and codec from uploaded files
    - id: 2
      name: Video Transcoding
      description: Convert videos to streaming-friendly formats using FFmpeg.
      acceptance_criteria:
      - FFmpeg integration transcodes uploaded videos into target format and bitrate
      - Multiple quality levels are generated from a single source file
      - Progress monitoring reports transcoding percentage and estimated time remaining
      - Background processing runs transcoding jobs asynchronously without blocking uploads
      hints:
        level1: Use FFmpeg to create HLS segments. Start with 720p, add more qualities.
        level2: Run transcoding in background job queue. Track progress via FFmpeg output.
        level3: "const { spawn } = require('child_process');\n\nasync function transcodeToHLS(inputPath, outputDir, qualities)\
          \ {\n    // qualities: [{name: '720p', width: 1280, height: 720, bitrate: '3000k'}]\n    \n    for (const q of qualities)\
          \ {\n        const outputPath = `${outputDir}/${q.name}`;\n        fs.mkdirSync(outputPath, { recursive: true });\n\
          \        \n        await new Promise((resolve, reject) => {\n            const ffmpeg = spawn('ffmpeg', [\n    \
          \            '-i', inputPath,\n                '-vf', `scale=${q.width}:${q.height}`,\n                '-c:v', 'libx264',\n\
          \                '-b:v', q.bitrate,\n                '-c:a', 'aac',\n                '-b:a', '128k',\n         \
          \       '-hls_time', '10',\n                '-hls_list_size', '0',\n                '-hls_segment_filename', `${outputPath}/segment_%03d.ts`,\n\
          \                `${outputPath}/playlist.m3u8`\n            ]);\n            \n            ffmpeg.stderr.on('data',\
          \ (data) => {\n                // Parse progress from FFmpeg output\n                const match = data.toString().match(/time=(\\\
          d+:\\d+:\\d+)/);\n                if (match) {\n                    updateProgress(inputPath, q.name, match[1]);\n\
          \                }\n            });\n            \n            ffmpeg.on('close', (code) => {\n                code\
          \ === 0 ? resolve() : reject(new Error(`FFmpeg exit ${code}`));\n            });\n        });\n    }\n    \n   \
          \ // Create master playlist\n    createMasterPlaylist(outputDir, qualities);\n}\n\nfunction createMasterPlaylist(outputDir,\
          \ qualities) {\n    let content = '#EXTM3U\\n';\n    \n    for (const q of qualities) {\n        content += `#EXT-X-STREAM-INF:BANDWIDTH=${parseInt(q.bitrate)*1000},RESOLUTION=${q.width}x${q.height}\\\
          n`;\n        content += `${q.name}/playlist.m3u8\\n`;\n    }\n    \n    fs.writeFileSync(`${outputDir}/master.m3u8`,\
          \ content);\n}"
      pitfalls:
      - FFmpeg memory usage
      - Codec compatibility
      - Interrupted transcoding
      concepts:
      - Video codecs
      - HLS format
      - Background jobs
      estimated_hours: 8-12
      deliverables:
      - FFmpeg integration invoking transcoding commands as subprocess pipelines
      - Multiple quality level output producing 360p, 720p, and 1080p renditions
      - Codec selection logic choosing optimal output codec based on target device compatibility
      - Transcoding queue managing parallel jobs with configurable concurrency limits
    - id: 3
      name: Adaptive Streaming
      description: Serve HLS streams with quality adaptation.
      acceptance_criteria:
      - HLS manifest serving returns valid M3U8 playlist with segment references
      - Segment serving delivers TS files with correct MIME type headers
      - Byte-range requests support partial content delivery for efficient seeking
      - CDN-friendly headers include cache-control and content-length for edge caching
      hints:
        level1: Serve .m3u8 as application/vnd.apple.mpegurl, .ts as video/mp2t.
        level2: Support Range requests for seeking. Set proper cache headers.
        level3: "// Serve HLS content\napp.get('/api/videos/:videoId/stream/*', async (req, res) => {\n    const { videoId\
          \ } = req.params;\n    const filePath = req.params[0];  // e.g., 'master.m3u8' or '720p/segment_001.ts'\n    \n\
          \    const fullPath = path.join(VIDEO_DIR, videoId, filePath);\n    \n    if (!fs.existsSync(fullPath)) {\n    \
          \    return res.status(404).send('Not found');\n    }\n    \n    // Set content type\n    const ext = path.extname(filePath);\n\
          \    const contentTypes = {\n        '.m3u8': 'application/vnd.apple.mpegurl',\n        '.ts': 'video/mp2t'\n  \
          \  };\n    res.setHeader('Content-Type', contentTypes[ext] || 'application/octet-stream');\n    \n    // Cache headers\n\
          \    if (ext === '.m3u8') {\n        res.setHeader('Cache-Control', 'no-cache');  // Playlist can change\n    }\
          \ else {\n        res.setHeader('Cache-Control', 'public, max-age=31536000');  // Segments immutable\n    }\n  \
          \  \n    // Handle Range requests for segments\n    const stat = fs.statSync(fullPath);\n    const range = req.headers.range;\n\
          \    \n    if (range && ext === '.ts') {\n        const parts = range.replace(/bytes=/, '').split('-');\n      \
          \  const start = parseInt(parts[0], 10);\n        const end = parts[1] ? parseInt(parts[1], 10) : stat.size - 1;\n\
          \        \n        res.status(206);\n        res.setHeader('Content-Range', `bytes ${start}-${end}/${stat.size}`);\n\
          \        res.setHeader('Content-Length', end - start + 1);\n        \n        fs.createReadStream(fullPath, { start,\
          \ end }).pipe(res);\n    } else {\n        res.setHeader('Content-Length', stat.size);\n        fs.createReadStream(fullPath).pipe(res);\n\
          \    }\n});"
      pitfalls:
      - CORS for cross-origin players
      - Playlist caching issues
      - Seeking accuracy
      concepts:
      - HLS protocol
      - HTTP range requests
      - Caching strategies
      estimated_hours: 6-10
      deliverables:
      - HLS segment generation splitting transcoded video into fixed-duration TS segments
      - Manifest file creation producing M3U8 playlists listing all segments in order
      - Quality variant playlists referencing multiple rendition playlists for adaptive switching
      - Segment serving endpoint delivering TS files with correct content-type and caching headers
    - id: 4
      name: Video Player Integration
      description: Build frontend player with quality switching and progress tracking.
      acceptance_criteria:
      - HLS.js integration plays adaptive streams in browsers without native HLS support
      - Quality selector allows manual rendition switching during playback
      - Progress bar displays playback position and supports click-to-seek navigation
      - Playback analytics record view duration, quality switches, and buffering events
      hints:
        level1: Use HLS.js library for non-Safari browsers. Safari has native HLS.
        level2: Track watch progress for resume. Send analytics events.
        level3: "// Video player component\nclass VideoPlayer {\n    constructor(container, videoId) {\n        this.video\
          \ = document.createElement('video');\n        this.video.controls = true;\n        container.appendChild(this.video);\n\
          \        \n        this.videoId = videoId;\n        this.hls = null;\n        this.qualities = [];\n        \n \
          \       this.setupHLS();\n        this.setupAnalytics();\n    }\n    \n    setupHLS() {\n        const src = `/api/videos/${this.videoId}/stream/master.m3u8`;\n\
          \        \n        if (this.video.canPlayType('application/vnd.apple.mpegurl')) {\n            // Safari native\
          \ HLS\n            this.video.src = src;\n        } else if (Hls.isSupported()) {\n            this.hls = new Hls({\n\
          \                enableWorker: true,\n                lowLatencyMode: false\n            });\n            \n   \
          \         this.hls.loadSource(src);\n            this.hls.attachMedia(this.video);\n            \n            this.hls.on(Hls.Events.MANIFEST_PARSED,\
          \ (event, data) => {\n                this.qualities = data.levels.map((level, i) => ({\n                    index:\
          \ i,\n                    height: level.height,\n                    bitrate: level.bitrate\n                }));\n\
          \                this.renderQualitySelector();\n            });\n        }\n    }\n    \n    setQuality(levelIndex)\
          \ {\n        if (this.hls) {\n            this.hls.currentLevel = levelIndex;  // -1 for auto\n        }\n    }\n\
          \    \n    setupAnalytics() {\n        let lastReport = 0;\n        \n        this.video.addEventListener('timeupdate',\
          \ () => {\n            const now = Date.now();\n            if (now - lastReport > 10000) {  // Every 10 seconds\n\
          \                this.reportProgress(this.video.currentTime);\n                lastReport = now;\n            }\n\
          \        });\n        \n        this.video.addEventListener('ended', () => {\n            this.reportComplete();\n\
          \        });\n    }\n    \n    async reportProgress(time) {\n        await fetch(`/api/videos/${this.videoId}/progress`,\
          \ {\n            method: 'POST',\n            body: JSON.stringify({ time })\n        });\n    }\n}"
      pitfalls:
      - Browser compatibility
      - Memory leaks on unmount
      - Bandwidth estimation
      concepts:
      - Adaptive bitrate
      - Media APIs
      - Analytics
      estimated_hours: 11-20
      deliverables:
      - HLS.js integration initializing the player with a master playlist URL
      - Quality switching UI allowing manual selection or automatic adaptive bitrate
      - Playback controls providing play, pause, seek, and volume adjustment buttons
      - Progress tracking overlay displaying current playback position and total duration
  virtual-memory-sim:
    id: virtual-memory-sim
    name: Virtual Memory Simulator
    description: Simulate virtual memory with page tables and TLB. Learn memory management and address translation.
    difficulty: advanced
    estimated_hours: 20-30
    prerequisites:
    - Memory concepts
    - Binary/hex
    - Data structures
    languages:
      recommended:
      - C
      - Rust
      - Python
      also_possible:
      - Go
      - Java
    resources:
    - type: book
      name: OSTEP - Address Translation
      url: https://pages.cs.wisc.edu/~remzi/OSTEP/vm-mechanism.pdf
    - type: book
      name: OSTEP - Paging
      url: https://pages.cs.wisc.edu/~remzi/OSTEP/vm-paging.pdf
    milestones:
    - id: 1
      name: Page Table
      description: Implement single-level page table with address translation.
      acceptance_criteria:
      - Virtual to physical address translation returns correct frame number and offset
      - Page table entries store flags for valid, dirty, referenced, and read-write permissions
      - Handle page faults by detecting invalid page table entries on lookup
      - Valid and invalid bits are set and cleared correctly on page load and eviction
      hints:
        level1: Split virtual address into page number and offset. Look up frame in table.
        level2: PTE has frame number + flags (valid, dirty, accessed, protection).
        level3: "class PageTableEntry:\n    def __init__(self):\n        self.valid = False\n        self.frame_number = 0\n\
          \        self.dirty = False\n        self.accessed = False\n        self.protection = 0b111  # RWX\n\nclass PageTable:\n\
          \    def __init__(self, page_bits=12, addr_bits=32):\n        self.page_size = 1 << page_bits\n        self.offset_mask\
          \ = self.page_size - 1\n        self.page_bits = page_bits\n        self.num_pages = 1 << (addr_bits - page_bits)\n\
          \        self.entries = [PageTableEntry() for _ in range(self.num_pages)]\n    \n    def translate(self, virtual_addr,\
          \ write=False):\n        page_num = virtual_addr >> self.page_bits\n        offset = virtual_addr & self.offset_mask\n\
          \        \n        if page_num >= self.num_pages:\n            raise SegmentationFault(f'Invalid page {page_num}')\n\
          \        \n        pte = self.entries[page_num]\n        \n        if not pte.valid:\n            raise PageFault(page_num)\n\
          \        \n        # Check permissions\n        if write and not (pte.protection & 0b010):\n            raise ProtectionFault('Write\
          \ not allowed')\n        \n        # Update accessed/dirty bits\n        pte.accessed = True\n        if write:\n\
          \            pte.dirty = True\n        \n        physical_addr = (pte.frame_number << self.page_bits) | offset\n\
          \        return physical_addr\n    \n    def map_page(self, page_num, frame_num, protection=0b111):\n        pte\
          \ = self.entries[page_num]\n        pte.valid = True\n        pte.frame_number = frame_num\n        pte.protection\
          \ = protection\n        pte.dirty = False\n        pte.accessed = False"
      pitfalls:
      - Off-by-one in bit shifting
      - Forgetting offset
      - Protection check order
      concepts:
      - Address translation
      - Page tables
      - Memory protection
      estimated_hours: 4-6
      deliverables:
      - Page table entry structure containing frame number, valid bit, and permission flags
      - Page table construction initializing entries for each virtual page in the address space
      - Page table lookup translating a virtual page number to a physical frame number
      - Valid and invalid bits indicating whether a page is currently resident in memory
    - id: 2
      name: TLB
      description: Add Translation Lookaside Buffer for faster translations.
      acceptance_criteria:
      - TLB lookup is checked before consulting the full page table
      - TLB miss triggers a full page table walk and caches the result
      - TLB eviction uses LRU or random replacement when all slots are occupied
      - TLB flush clears all entries on process context switch to prevent stale mappings
      hints:
        level1: TLB is small cache of recent translations. Check TLB first.
        level2: On miss, walk page table and add to TLB. Track LRU for eviction.
        level3: "from collections import OrderedDict\n\nclass TLB:\n    def __init__(self, size=64):\n        self.size =\
          \ size\n        self.entries = OrderedDict()  # page_num -> (frame_num, protection)\n        self.hits = 0\n   \
          \     self.misses = 0\n    \n    def lookup(self, page_num):\n        if page_num in self.entries:\n           \
          \ self.hits += 1\n            # Move to end (most recently used)\n            self.entries.move_to_end(page_num)\n\
          \            return self.entries[page_num]\n        self.misses += 1\n        return None\n    \n    def insert(self,\
          \ page_num, frame_num, protection):\n        if page_num in self.entries:\n            del self.entries[page_num]\n\
          \        elif len(self.entries) >= self.size:\n            # Evict LRU (first item)\n            self.entries.popitem(last=False)\n\
          \        self.entries[page_num] = (frame_num, protection)\n    \n    def invalidate(self, page_num):\n        if\
          \ page_num in self.entries:\n            del self.entries[page_num]\n    \n    def flush(self):\n        self.entries.clear()\n\
          \    \n    def hit_rate(self):\n        total = self.hits + self.misses\n        return self.hits / total if total\
          \ > 0 else 0\n\nclass MMU:\n    def __init__(self, page_table):\n        self.page_table = page_table\n        self.tlb\
          \ = TLB()\n    \n    def translate(self, virtual_addr, write=False):\n        page_num = virtual_addr >> self.page_table.page_bits\n\
          \        offset = virtual_addr & self.page_table.offset_mask\n        \n        # TLB lookup\n        tlb_entry\
          \ = self.tlb.lookup(page_num)\n        if tlb_entry:\n            frame_num, protection = tlb_entry\n          \
          \  if write and not (protection & 0b010):\n                raise ProtectionFault('Write not allowed')\n        \
          \    return (frame_num << self.page_table.page_bits) | offset\n        \n        # TLB miss - walk page table\n\
          \        physical = self.page_table.translate(virtual_addr, write)\n        \n        # Add to TLB\n        pte\
          \ = self.page_table.entries[page_num]\n        self.tlb.insert(page_num, pte.frame_number, pte.protection)\n   \
          \     \n        return physical"
      pitfalls:
      - TLB coherency with page table
      - Context switch handling
      - ASID management
      concepts:
      - Caching
      - Locality
      - TLB shootdown
      estimated_hours: 4-6
      deliverables:
      - TLB structure implementing a small fast-lookup cache of recent page translations
      - TLB lookup function checking for a matching virtual page number entry
      - TLB miss handling logic triggering a page table walk and updating the TLB
      - TLB flush operation invalidating all entries on context switch or page table change
    - id: 3
      name: Multi-level Page Tables
      description: Implement hierarchical page tables to save memory.
      acceptance_criteria:
      - Two or three-level tables translate addresses through successive index lookups
      - Sparse address space handling avoids allocating tables for unmapped regions
      - On-demand table allocation creates second-level tables only when first accessed
      - Page table walks correctly traverse all levels and produce the final frame number
      hints:
        level1: Split page number into multiple indices. Each level points to next.
        level2: Only allocate tables for used regions. Check valid bit at each level.
        level3: "class MultiLevelPageTable:\n    def __init__(self, levels=2, bits_per_level=10, offset_bits=12):\n      \
          \  self.levels = levels\n        self.bits_per_level = bits_per_level\n        self.offset_bits = offset_bits\n\
          \        self.entries_per_table = 1 << bits_per_level\n        self.offset_mask = (1 << offset_bits) - 1\n     \
          \   \n        # Root table (always allocated)\n        self.root = self._new_table()\n        self.tables_allocated\
          \ = 1\n    \n    def _new_table(self):\n        return [None] * self.entries_per_table\n    \n    def _extract_indices(self,\
          \ virtual_addr):\n        indices = []\n        addr = virtual_addr >> self.offset_bits\n        for _ in range(self.levels):\n\
          \            indices.append(addr & (self.entries_per_table - 1))\n            addr >>= self.bits_per_level\n   \
          \     return list(reversed(indices))\n    \n    def translate(self, virtual_addr):\n        indices = self._extract_indices(virtual_addr)\n\
          \        offset = virtual_addr & self.offset_mask\n        \n        table = self.root\n        for i, idx in enumerate(indices[:-1]):\n\
          \            entry = table[idx]\n            if entry is None:\n                raise PageFault(virtual_addr)\n\
          \            table = entry  # Next level table\n        \n        # Last level is the PTE\n        pte = table[indices[-1]]\n\
          \        if pte is None or not pte.valid:\n            raise PageFault(virtual_addr)\n        \n        return (pte.frame_number\
          \ << self.offset_bits) | offset\n    \n    def map_page(self, virtual_addr, frame_num):\n        indices = self._extract_indices(virtual_addr)\n\
          \        \n        table = self.root\n        for i, idx in enumerate(indices[:-1]):\n            if table[idx]\
          \ is None:\n                table[idx] = self._new_table()\n                self.tables_allocated += 1\n       \
          \     table = table[idx]\n        \n        # Create PTE at leaf\n        pte = PageTableEntry()\n        pte.valid\
          \ = True\n        pte.frame_number = frame_num\n        table[indices[-1]] = pte"
      pitfalls:
      - Index extraction order
      - Table pointer vs PTE confusion
      - Memory overhead calculation
      concepts:
      - Hierarchical structures
      - Sparse data
      - Memory efficiency
      estimated_hours: 5-8
      deliverables:
      - Two-level page table splitting virtual address into directory and table indices
      - Page directory structure mapping directory entries to second-level page tables
      - Page table hierarchy supporting configurable number of translation levels
      - Memory savings achieved by allocating second-level tables only for mapped regions
    - id: 4
      name: Page Replacement
      description: Implement page replacement algorithms when memory is full.
      acceptance_criteria:
      - FIFO replacement evicts pages in the order they were loaded into memory
      - LRU replacement evicts the page with the oldest last-access timestamp
      - Clock algorithm sweeps reference bits and evicts the first unreferenced page found
      - Track working set size reporting the number of distinct pages accessed in a window
      hints:
        level1: 'FIFO: queue of page numbers, evict oldest. LRU: track last access time.'
        level2: 'Clock: circular list with reference bits. Give second chance.'
        level3: "class ClockPageReplacement:\n    def __init__(self, num_frames):\n        self.num_frames = num_frames\n\
          \        self.frames = [None] * num_frames  # page_num in each frame\n        self.reference_bits = [False] * num_frames\n\
          \        self.clock_hand = 0\n        self.page_to_frame = {}  # page_num -> frame_num\n    \n    def access(self,\
          \ page_num):\n        if page_num in self.page_to_frame:\n            # Page hit - set reference bit\n         \
          \   frame = self.page_to_frame[page_num]\n            self.reference_bits[frame] = True\n            return frame,\
          \ None  # No eviction\n        \n        # Page fault - find frame to use\n        frame, evicted = self._find_victim()\n\
          \        \n        # Update mappings\n        if evicted is not None:\n            del self.page_to_frame[evicted]\n\
          \        \n        self.frames[frame] = page_num\n        self.reference_bits[frame] = True\n        self.page_to_frame[page_num]\
          \ = frame\n        \n        return frame, evicted\n    \n    def _find_victim(self):\n        # Check for empty\
          \ frame first\n        for i in range(self.num_frames):\n            if self.frames[i] is None:\n              \
          \  return i, None\n        \n        # Clock algorithm - find victim\n        while True:\n            if not self.reference_bits[self.clock_hand]:\n\
          \                # Found victim\n                evicted = self.frames[self.clock_hand]\n                frame =\
          \ self.clock_hand\n                self.clock_hand = (self.clock_hand + 1) % self.num_frames\n                return\
          \ frame, evicted\n            \n            # Give second chance\n            self.reference_bits[self.clock_hand]\
          \ = False\n            self.clock_hand = (self.clock_hand + 1) % self.num_frames"
      pitfalls:
      - Belady's anomaly with FIFO
      - Dirty page handling
      - Thrashing
      concepts:
      - Page replacement
      - Working set
      - Thrashing
      estimated_hours: 7-10
      deliverables:
      - FIFO replacement policy evicting the oldest loaded page on memory full condition
      - LRU replacement policy evicting the least recently accessed page on memory full
      - Clock algorithm approximating LRU using a circular buffer with reference bits
      - Page fault handler loading the requested page from disk and updating the page table
  wal-impl:
    id: wal-impl
    name: WAL Implementation
    description: Implement Write-Ahead Logging for durability. Learn crash recovery and log-structured storage.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - File I/O
    - Database basics
    - Crash recovery concepts
    languages:
      recommended:
      - Rust
      - Go
      - C
      also_possible:
      - Python
      - Java
    resources:
    - name: ARIES Paper
      url: https://cs.stanford.edu/people/chr101/cs345/aries.pdf
      type: paper
    - name: SQLite WAL
      url: https://sqlite.org/wal.html
      type: documentation
    milestones:
    - id: 1
      name: Log Record Format
      description: Design and implement log record structure.
      acceptance_criteria:
      - LSN (Log Sequence Number) is monotonically increasing and unique per record
      - Transaction ID field links each log record to its originating transaction
      - Operation type field distinguishes INSERT, UPDATE, and DELETE log entries
      - Before and after images stored for undo and redo operations respectively
      hints:
        level1: Log record = LSN + TxID + Type + Data. Append-only file.
        level2: Include enough info to redo OR undo the operation.
        level3: "import struct\nfrom enum import IntEnum\n\nclass LogType(IntEnum):\n    BEGIN = 1\n    COMMIT = 2\n    ABORT\
          \ = 3\n    INSERT = 4\n    UPDATE = 5\n    DELETE = 6\n    CHECKPOINT = 7\n\nclass LogRecord:\n    def __init__(self,\
          \ lsn, tx_id, log_type, table=None, key=None, \n                 before_value=None, after_value=None):\n       \
          \ self.lsn = lsn\n        self.tx_id = tx_id\n        self.log_type = log_type\n        self.table = table\n   \
          \     self.key = key\n        self.before_value = before_value  # For undo\n        self.after_value = after_value\
          \    # For redo\n    \n    def serialize(self):\n        # Format: LSN(8) + TxID(4) + Type(1) + DataLen(4) + Data\n\
          \        data = b''\n        if self.table:\n            data += self.table.encode() + b'\\x00'\n        if self.key\
          \ is not None:\n            data += struct.pack('!I', self.key)\n        if self.before_value is not None:\n   \
          \         before_bytes = self.before_value.encode() if isinstance(self.before_value, str) else self.before_value\n\
          \            data += struct.pack('!I', len(before_bytes)) + before_bytes\n        if self.after_value is not None:\n\
          \            after_bytes = self.after_value.encode() if isinstance(self.after_value, str) else self.after_value\n\
          \            data += struct.pack('!I', len(after_bytes)) + after_bytes\n        \n        header = struct.pack('!QIB\
          \ I', self.lsn, self.tx_id, self.log_type, len(data))\n        return header + data"
      pitfalls:
      - Variable-length fields
      - Endianness
      - Corruption detection
      concepts:
      - Log records
      - LSN
      - Undo/redo logging
      estimated_hours: 3-4
      deliverables:
      - Record header (LSN, type, length)
      - Record types (redo, undo, checkpoint)
      - CRC integrity checksum appended to each record for corruption detection
      - Record serialization converting structured log entries to binary byte format
    - id: 2
      name: Log Writer
      description: Implement append-only log file with fsync.
      acceptance_criteria:
      - Append records atomically so partial writes are never visible after crash
      - Force log to disk via fsync before reporting commit to the client
      - Handle concurrent writers using locking or lock-free buffer for serialized append
      - Log file rotation creates new segment files at configurable size thresholds
      hints:
        level1: Append to log file, fsync before acknowledging commit.
        level2: 'Group commit: batch multiple commits into one fsync.'
        level3: "import os\nimport threading\n\nclass WALWriter:\n    def __init__(self, log_path):\n        self.log_path\
          \ = log_path\n        self.log_file = open(log_path, 'ab')\n        self.lock = threading.Lock()\n        self.current_lsn\
          \ = self._recover_lsn()\n        self.buffer = bytearray()\n        self.pending_commits = []\n    \n    def _recover_lsn(self):\n\
          \        # Read last LSN from log file\n        try:\n            size = os.path.getsize(self.log_path)\n      \
          \      if size == 0:\n                return 0\n            # Read last record's LSN\n            # ... implementation\
          \ ...\n        except FileNotFoundError:\n            return 0\n    \n    def append(self, record):\n        with\
          \ self.lock:\n            record.lsn = self.current_lsn\n            self.current_lsn += 1\n            \n     \
          \       data = record.serialize()\n            self.buffer.extend(data)\n            \n            if record.log_type\
          \ == LogType.COMMIT:\n                self.pending_commits.append(record.tx_id)\n            \n            return\
          \ record.lsn\n    \n    def flush(self):\n        '''Force buffered records to disk'''\n        with self.lock:\n\
          \            if self.buffer:\n                self.log_file.write(bytes(self.buffer))\n                self.log_file.flush()\n\
          \                os.fsync(self.log_file.fileno())\n                self.buffer.clear()\n                committed\
          \ = self.pending_commits.copy()\n                self.pending_commits.clear()\n                return committed\n\
          \            return []"
      pitfalls:
      - Partial writes
      - fsync semantics
      - Torn pages
      concepts:
      - Durability
      - fsync
      - Group commit
      estimated_hours: 4-5
      deliverables:
      - Sequential log writer appending records to the end of the active segment file
      - Force write mechanism calling fsync to ensure durability before acknowledging
      - Buffer management batching multiple log records before flushing to reduce I/O
      - Log segment rotation creating a new file when the current segment reaches size limit
    - id: 3
      name: Crash Recovery
      description: Implement ARIES-style recovery (redo then undo).
      acceptance_criteria:
      - Scan log from last checkpoint to identify committed and active transactions
      - Redo all committed changes restoring their effects to the database state
      - Undo incomplete transactions reverting their partial changes from the database
      - Track active transactions at crash time using the transaction table from the log
      hints:
        level1: 'Recovery: 1) Analysis - find active txs, 2) Redo - replay, 3) Undo - rollback.'
        level2: 'Redo: apply all operations. Undo: reverse uncommitted ops.'
        level3: "class WALRecovery:\n    def recover(self, wal_reader, database):\n        # Phase 1: Analysis\n        active_txs\
          \ = set()\n        committed_txs = set()\n        \n        for record in wal_reader.scan():\n            if record.log_type\
          \ == LogType.BEGIN:\n                active_txs.add(record.tx_id)\n            elif record.log_type == LogType.COMMIT:\n\
          \                active_txs.discard(record.tx_id)\n                committed_txs.add(record.tx_id)\n           \
          \ elif record.log_type == LogType.ABORT:\n                active_txs.discard(record.tx_id)\n        \n        #\
          \ Phase 2: Redo (all operations from committed transactions)\n        for record in wal_reader.scan():\n       \
          \     if record.tx_id in committed_txs:\n                if record.log_type == LogType.INSERT:\n               \
          \     database.insert(record.table, record.key, record.after_value)\n                elif record.log_type == LogType.UPDATE:\n\
          \                    database.update(record.table, record.key, record.after_value)\n                elif record.log_type\
          \ == LogType.DELETE:\n                    database.delete(record.table, record.key)\n        \n        # Phase 3:\
          \ Undo (reverse uncommitted transactions)\n        # Scan backwards\n        for record in wal_reader.scan_reverse():\n\
          \            if record.tx_id in active_txs:\n                if record.log_type == LogType.INSERT:\n           \
          \         database.delete(record.table, record.key)\n                elif record.log_type == LogType.UPDATE:\n \
          \                   database.update(record.table, record.key, record.before_value)\n                elif record.log_type\
          \ == LogType.DELETE:\n                    database.insert(record.table, record.key, record.before_value)\n     \
          \   \n        # Log abort records for incomplete transactions\n        for tx_id in active_txs:\n            wal_writer.append(LogRecord(0,\
          \ tx_id, LogType.ABORT))"
      pitfalls:
      - Idempotent operations
      - Log corruption
      - Partial recovery
      concepts:
      - ARIES recovery
      - Redo/undo
      - Crash consistency
      estimated_hours: 5-7
      deliverables:
      - Log scanning module reading records from the last checkpoint forward
      - Redo pass replaying all committed transaction changes to restore database state
      - Undo pass rolling back changes from transactions that were active at crash time
      - Recovery completion signal marking the database as consistent and ready for new transactions
    - id: 4
      name: Checkpointing
      description: Implement checkpoints to speed up recovery.
      acceptance_criteria:
      - Fuzzy checkpoints allow concurrent transactions to continue without blocking
      - Record dirty pages and active transactions in the checkpoint record
      - Truncate old log entries before the checkpoint to reclaim disk space
      - Restart from checkpoint reduces recovery time by limiting log scan range
      hints:
        level1: Checkpoint records state so recovery starts from there.
        level2: 'Fuzzy checkpoint: don''t block, just record dirty pages.'
        level3: "class Checkpoint:\n    def __init__(self, lsn, active_txs, dirty_pages):\n        self.lsn = lsn\n      \
          \  self.active_txs = active_txs  # {tx_id: first_lsn}\n        self.dirty_pages = dirty_pages  # {page_id: recovery_lsn}\n\
          \nclass WALManager:\n    def create_checkpoint(self):\n        with self.lock:\n            # Get current state\n\
          \            active_txs = dict(self.transaction_table)\n            dirty_pages = dict(self.dirty_page_table)\n\
          \            \n            # Write checkpoint begin\n            begin_record = LogRecord(0, 0, LogType.CHECKPOINT_BEGIN)\n\
          \            self.wal_writer.append(begin_record)\n            \n            # Flush dirty pages in background\n\
          \            # (fuzzy - don't wait)\n            \n            # Write checkpoint end with state\n            checkpoint\
          \ = Checkpoint(\n                begin_record.lsn,\n                active_txs,\n                dirty_pages\n \
          \           )\n            end_record = LogRecord(0, 0, LogType.CHECKPOINT_END,\n                              \
          \      data=checkpoint.serialize())\n            self.wal_writer.append(end_record)\n            self.wal_writer.flush()\n\
          \            \n            # Update master record\n            self._write_master_record(begin_record.lsn)\n   \
          \ \n    def recover_from_checkpoint(self):\n        # Read master record to find checkpoint\n        checkpoint_lsn\
          \ = self._read_master_record()\n        \n        if checkpoint_lsn:\n            # Start analysis from checkpoint\n\
          \            checkpoint = self._read_checkpoint(checkpoint_lsn)\n            self.transaction_table = checkpoint.active_txs\n\
          \            self.dirty_page_table = checkpoint.dirty_pages\n            start_lsn = checkpoint_lsn\n        else:\n\
          \            start_lsn = 0\n        \n        # Continue with normal recovery from start_lsn\n        self._redo_phase(start_lsn)\n\
          \        self._undo_phase()"
      pitfalls:
      - Checkpoint consistency
      - Log truncation timing
      - Master record
      concepts:
      - Checkpointing
      - Fuzzy checkpoints
      - Log truncation
      estimated_hours: 4-5
      deliverables:
      - Checkpoint record written to the log capturing current recovery state information
      - Active transaction list snapshot recorded at checkpoint for recovery starting point
      - Dirty page tracking recording which pages have uncommitted modifications in memory
      - Checkpoint coordination mechanism ensuring consistent state without blocking normal operations
  wasm-emitter:
    id: wasm-emitter
    name: WebAssembly Emitter
    description: Compile a simple language to WebAssembly binary. Learn WASM format and code generation.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - AST building
    - Binary formats
    - Stack machines
    languages:
      recommended:
      - Rust
      - Go
      - TypeScript
      also_possible:
      - Python
      - C
    resources:
    - type: spec
      name: WebAssembly Specification
      url: https://webassembly.github.io/spec/core/
    - type: tool
      name: WebAssembly Binary Toolkit
      url: https://github.com/WebAssembly/wabt
    milestones:
    - id: 1
      name: WASM Binary Format
      description: Understand and emit valid WASM module structure.
      acceptance_criteria:
      - Magic number 0x00 0x61 0x73 0x6D and version 1 header are emitted correctly
      - Section encoding writes section ID byte followed by LEB128-encoded content length
      - LEB128 integers encode both signed and unsigned values in variable-length format
      - Type section lists all function signatures used by the module
      hints:
        level1: WASM starts with magic (0x00 0x61 0x73 0x6d) + version (0x01 0x00 0x00 0x00).
        level2: Sections have ID, size (LEB128), content. Type section defines function signatures.
        level3: "def leb128_unsigned(n):\n    result = []\n    while True:\n        byte = n & 0x7f\n        n >>= 7\n   \
          \     if n != 0:\n            byte |= 0x80\n        result.append(byte)\n        if n == 0:\n            break\n\
          \    return bytes(result)\n\ndef leb128_signed(n):\n    result = []\n    while True:\n        byte = n & 0x7f\n\
          \        n >>= 7\n        if (n == 0 and byte & 0x40 == 0) or (n == -1 and byte & 0x40):\n            result.append(byte)\n\
          \            break\n        result.append(byte | 0x80)\n    return bytes(result)\n\nclass WasmModule:\n    MAGIC\
          \ = b'\\x00asm'\n    VERSION = b'\\x01\\x00\\x00\\x00'\n    \n    # Section IDs\n    TYPE_SECTION = 1\n    FUNC_SECTION\
          \ = 3\n    EXPORT_SECTION = 7\n    CODE_SECTION = 10\n    \n    def __init__(self):\n        self.types = []   \
          \   # Function signatures\n        self.functions = []  # Function type indices\n        self.exports = []    #\
          \ Exported items\n        self.code = []       # Function bodies\n    \n    def add_function_type(self, params,\
          \ results):\n        # params/results are lists of value types (0x7f=i32, 0x7e=i64, etc.)\n        sig = bytes([0x60,\
          \ len(params)] + params + [len(results)] + results)\n        if sig not in self.types:\n            self.types.append(sig)\n\
          \        return self.types.index(sig)\n    \n    def encode(self):\n        output = self.MAGIC + self.VERSION\n\
          \        \n        # Type section\n        if self.types:\n            content = bytes([len(self.types)]) + b''.join(self.types)\n\
          \            output += self._section(self.TYPE_SECTION, content)\n        \n        # ... other sections\n     \
          \   return output\n    \n    def _section(self, id, content):\n        return bytes([id]) + leb128_unsigned(len(content))\
          \ + content"
      pitfalls:
      - LEB128 edge cases
      - Section ordering
      - Size calculation
      concepts:
      - Binary formats
      - Variable-length encoding
      - Module structure
      estimated_hours: 5-8
      deliverables:
      - Module structure emitter writing the top-level WASM binary module wrapper
      - Section encoding logic writing section IDs, sizes, and payloads in binary format
      - LEB128 encoding functions converting integers to variable-length binary representation
      - Type section emitter writing function signature definitions with parameter and return types
    - id: 2
      name: Expression Compilation
      description: Compile arithmetic expressions to WASM instructions.
      acceptance_criteria:
      - i32 arithmetic operations produce correct results via WASM stack instructions
      - Stack-based code generation pushes operands and applies operators in correct order
      - Local variables are read and written using indexed local.get and local.set instructions
      - Constants emit correct i32.const or f64.const instructions with encoded immediate values
      hints:
        level1: WASM is stack-based. i32.const pushes, i32.add pops 2 pushes 1.
        level2: local.get/set for variables. Locals declared at function start.
        level3: "# WASM opcodes\nI32_CONST = 0x41\nI32_ADD = 0x6a\nI32_SUB = 0x6b\nI32_MUL = 0x6c\nI32_DIV_S = 0x6d\nLOCAL_GET\
          \ = 0x20\nLOCAL_SET = 0x21\nLOCAL_TEE = 0x22  # Set and keep on stack\n\nclass CodeGen:\n    def __init__(self):\n\
          \        self.code = []\n        self.locals = {}  # name -> index\n        self.local_types = []  # type of each\
          \ local\n    \n    def compile_expr(self, node):\n        if isinstance(node, NumberLit):\n            self.code.append(I32_CONST)\n\
          \            self.code.extend(leb128_signed(node.value))\n        \n        elif isinstance(node, BinaryOp):\n \
          \           self.compile_expr(node.left)\n            self.compile_expr(node.right)\n            ops = {'+': I32_ADD,\
          \ '-': I32_SUB, '*': I32_MUL, '/': I32_DIV_S}\n            self.code.append(ops[node.op])\n        \n        elif\
          \ isinstance(node, Variable):\n            idx = self.locals[node.name]\n            self.code.append(LOCAL_GET)\n\
          \            self.code.extend(leb128_unsigned(idx))\n        \n        elif isinstance(node, Assignment):\n    \
          \        self.compile_expr(node.value)\n            idx = self.locals[node.name]\n            self.code.append(LOCAL_TEE)\
          \  # Keep value on stack\n            self.code.extend(leb128_unsigned(idx))\n    \n    def declare_local(self,\
          \ name, type=0x7f):  # 0x7f = i32\n        idx = len(self.locals)\n        self.locals[name] = idx\n        self.local_types.append(type)\n\
          \        return idx"
      pitfalls:
      - Operand order for non-commutative ops
      - Signed vs unsigned
      - Stack imbalance
      concepts:
      - Stack machines
      - Instruction encoding
      - Local variables
      estimated_hours: 5-8
      deliverables:
      - Literal value emitter producing WASM const instructions for integer and float values
      - Binary operation compiler generating WASM i32.add, i32.sub, i32.mul instructions
      - Local variable compiler generating local.get and local.set instructions by index
      - Stack management logic ensuring operand stack depth is correct after each instruction
    - id: 3
      name: Control Flow
      description: Compile if/else and loops to WASM structured control flow.
      acceptance_criteria:
      - Block and end structure correctly nests and terminates structured control flow
      - If, else, and end instructions implement two-branch conditional execution correctly
      - Loop with br_if generates a backward branch that repeats the loop body conditionally
      - Break to labels correctly targets the right enclosing block by depth index
      hints:
        level1: 'WASM has structured control: block/loop/if all need end. br jumps to enclosing block.'
        level2: 'br 0 exits innermost block. loop: br goes back to start. block: br goes to end.'
        level3: "BLOCK = 0x02\nLOOP = 0x03\nIF = 0x04\nELSE = 0x05\nEND = 0x0b\nBR = 0x0c\nBR_IF = 0x0d\nRETURN = 0x0f\nVOID\
          \ = 0x40  # Empty block type\n\nclass CodeGen:\n    def __init__(self):\n        self.code = []\n        self.block_depth\
          \ = 0\n    \n    def compile_if(self, node):\n        # Condition leaves i32 on stack\n        self.compile_expr(node.condition)\n\
          \        \n        self.code.append(IF)\n        self.code.append(VOID)  # No result type\n        self.block_depth\
          \ += 1\n        \n        self.compile_stmt(node.then_branch)\n        \n        if node.else_branch:\n        \
          \    self.code.append(ELSE)\n            self.compile_stmt(node.else_branch)\n        \n        self.code.append(END)\n\
          \        self.block_depth -= 1\n    \n    def compile_while(self, node):\n        # block { loop { if !cond br 1;\
          \ body; br 0 } }\n        self.code.append(BLOCK)\n        self.code.append(VOID)\n        self.block_depth += 1\n\
          \        \n        self.code.append(LOOP)\n        self.code.append(VOID)\n        self.block_depth += 1\n     \
          \   \n        # Check condition, break if false\n        self.compile_expr(node.condition)\n        self.code.append(I32_EQZ)\
          \  # Invert\n        self.code.append(BR_IF)\n        self.code.extend(leb128_unsigned(1))  # Break to outer block\n\
          \        \n        # Body\n        self.compile_stmt(node.body)\n        \n        # Loop back\n        self.code.append(BR)\n\
          \        self.code.extend(leb128_unsigned(0))  # Back to loop start\n        \n        self.code.append(END)  #\
          \ End loop\n        self.block_depth -= 1\n        self.code.append(END)  # End block\n        self.block_depth\
          \ -= 1"
      pitfalls:
      - Label depth calculation
      - Block type annotations
      - Unreachable code after br
      concepts:
      - Structured control flow
      - Label indices
      - Block nesting
      estimated_hours: 6-10
      deliverables:
      - Block and loop structure emitter generating WASM block and loop with matching end markers
      - If-else emitter generating WASM if, else, and end instructions for conditional branches
      - Branch instruction emitter generating br and br_if for jumping to enclosing labels
      - Return instruction emitter generating WASM return to exit the current function
    - id: 4
      name: Functions and Exports
      description: Compile function definitions and export them.
      acceptance_criteria:
      - Function section maps each function index to its corresponding type signature
      - Code section format includes local count declarations followed by expression bytecode
      - Export section maps string names to function indices for host-callable entry points
      - Function calls emit the correct call instruction with the target function index
      hints:
        level1: 'Separate sections: Type (signatures), Function (type refs), Code (bodies), Export.'
        level2: Code section encodes local counts then instructions. Functions called by index.
        level3: "CALL = 0x10\n\nclass WasmCompiler:\n    def __init__(self):\n        self.module = WasmModule()\n       \
          \ self.function_indices = {}  # name -> index\n    \n    def compile_function(self, func):\n        # Get/create\
          \ type signature\n        param_types = [0x7f] * len(func.params)  # All i32\n        result_types = [0x7f] if func.returns\
          \ else []\n        type_idx = self.module.add_function_type(param_types, result_types)\n        \n        # Register\
          \ function\n        func_idx = len(self.module.functions)\n        self.module.functions.append(type_idx)\n    \
          \    self.function_indices[func.name] = func_idx\n        \n        # Compile body\n        codegen = CodeGen()\n\
          \        for i, param in enumerate(func.params):\n            codegen.locals[param] = i\n        \n        for stmt\
          \ in func.body:\n            codegen.compile_stmt(stmt)\n        \n        codegen.code.append(END)\n        \n\
          \        # Encode function body\n        # Local declarations: count of (count, type) pairs\n        if codegen.local_types:\n\
          \            # Group consecutive same types\n            local_decls = self._group_locals(codegen.local_types[len(func.params):])\n\
          \        else:\n            local_decls = []\n        \n        body = bytes([len(local_decls)])\n        for count,\
          \ type in local_decls:\n            body += leb128_unsigned(count) + bytes([type])\n        body += bytes(codegen.code)\n\
          \        \n        # Wrap with size\n        self.module.code.append(leb128_unsigned(len(body)) + body)\n      \
          \  \n        return func_idx\n    \n    def export_function(self, name, func_name):\n        idx = self.function_indices[func_name]\n\
          \        # name as UTF-8 string + kind (0=func) + index\n        name_bytes = name.encode('utf-8')\n        self.module.exports.append(\n\
          \            leb128_unsigned(len(name_bytes)) + name_bytes + bytes([0]) + leb128_unsigned(idx)\n        )"
      pitfalls:
      - Parameter vs local indices
      - Body size encoding
      - Export name encoding
      concepts:
      - Module linking
      - Function ABI
      - Export mechanisms
      estimated_hours: 9-14
      deliverables:
      - Function type declaration emitter writing signatures to the type section
      - Function body emission writing local declarations and instruction sequences to the code section
      - Export section emitter making selected functions visible by name to the host environment
      - Import section emitter declaring functions provided by the host runtime environment
  word2vec:
    id: word2vec
    name: Word Embeddings (Word2Vec)
    description: Implement Word2Vec from scratch using Skip-gram or CBOW. Learn how words become dense vectors capturing semantic
      meaning.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Neural network basics
    - Linear algebra
    - Python/NumPy
    languages:
      recommended:
      - Python
      also_possible:
      - Julia
      - C++
    resources:
    - name: Word2Vec Paper
      url: https://arxiv.org/abs/1301.3781
      type: paper
    - name: Word2Vec Tutorial
      url: https://www.tensorflow.org/tutorials/text/word2vec
      type: tutorial
    milestones:
    - id: 1
      name: Data Preprocessing
      description: Prepare text corpus for training.
      acceptance_criteria:
      - Tokenize text into words with lowercasing and punctuation removal
      - Build vocabulary with word-to-index mapping and minimum frequency threshold
      - Create training pairs of context and target words from sliding window over corpus
      - Implement subsampling of frequent words reducing their occurrence probability in training
      hints:
        level1: 'Skip-gram: given center word, predict context words.'
        level2: 'For window size 2: ''the cat sat'' -> (cat, the), (cat, sat).'
        level3: "import re\nfrom collections import Counter\n\ndef preprocess(text, min_count=5):\n    # Tokenize\n    words\
          \ = re.findall(r'\\w+', text.lower())\n    \n    # Build vocab\n    word_counts = Counter(words)\n    vocab = {w:\
          \ i for i, (w, c) in enumerate(word_counts.items()) if c >= min_count}\n    \n    # Convert to indices\n    data\
          \ = [vocab[w] for w in words if w in vocab]\n    return data, vocab\n\ndef generate_training_pairs(data, window_size=2):\n\
          \    pairs = []\n    for i, center in enumerate(data):\n        for j in range(max(0, i-window_size), min(len(data),\
          \ i+window_size+1)):\n            if i != j:\n                pairs.append((center, data[j]))\n    return pairs"
      pitfalls:
      - Vocabulary too large
      - Not removing rare words
      - Memory issues with large corpus
      concepts:
      - Tokenization
      - Vocabulary building
      - Skip-gram pairs
      estimated_hours: 3-4
      deliverables:
      - Text tokenization splitting raw input into individual word tokens with normalization
      - Vocabulary building collecting unique words with frequency counts above a threshold
      - Word to index mapping creating bidirectional lookup between words and integer IDs
      - Subsampling frequent words randomly discarding high-frequency tokens during training
    - id: 2
      name: Skip-gram Model
      description: Implement the Skip-gram neural network.
      acceptance_criteria:
      - Embedding layer maps each word index to a dense vector of configurable dimension
      - Output layer predicts context words given the target word embedding
      - Softmax over vocabulary computes probability distribution for context word prediction
      - Forward pass implementation computes prediction scores for all context candidates
      hints:
        level1: 'Two matrices: W_in (vocab x embed_dim), W_out (embed_dim x vocab).'
        level2: 'Forward: embed = W_in[word], scores = embed @ W_out, probs = softmax(scores).'
        level3: "import numpy as np\n\nclass SkipGram:\n    def __init__(self, vocab_size, embed_dim):\n        self.W_in\
          \ = np.random.randn(vocab_size, embed_dim) * 0.01\n        self.W_out = np.random.randn(embed_dim, vocab_size) *\
          \ 0.01\n    \n    def forward(self, center_word):\n        # Get embedding\n        self.embed = self.W_in[center_word]\
          \  # (embed_dim,)\n        # Compute scores\n        self.scores = self.embed @ self.W_out  # (vocab_size,)\n  \
          \      # Softmax\n        exp_scores = np.exp(self.scores - np.max(self.scores))\n        self.probs = exp_scores\
          \ / exp_scores.sum()\n        return self.probs"
      pitfalls:
      - Softmax numerical stability
      - Wrong matrix dimensions
      - Embedding lookup indexing
      concepts:
      - Word embeddings
      - Softmax classification
      - Neural network layers
      estimated_hours: 3-4
      deliverables:
      - Embedding layer mapping word indices to dense trainable vector representations
      - Context window parameter defining the radius of surrounding words for pair generation
      - Training pair generation producing target-context word pairs from the sliding window
      - Forward pass computing dot product between target and context embedding vectors
    - id: 3
      name: Training with Negative Sampling
      description: Implement efficient training with negative sampling.
      acceptance_criteria:
      - Cross-entropy loss computes correct gradients for positive and negative sample pairs
      - Negative sampling instead of full softmax
      - Gradient computation correctly updates both input and output embedding matrices
      - SGD parameter updates reduce loss over successive training iterations
      hints:
        level1: Full softmax is O(vocab_size). Negative sampling is O(k) where k ~ 5-20.
        level2: Sample negatives according to word frequency ^ 0.75.
        level3: "def negative_sampling_loss(self, center, context, neg_samples):\n    # Positive sample\n    pos_embed = self.W_in[center]\n\
          \    pos_context = self.W_out[:, context]\n    pos_score = sigmoid(pos_embed @ pos_context)\n    pos_loss = -np.log(pos_score\
          \ + 1e-10)\n    \n    # Negative samples\n    neg_loss = 0\n    for neg in neg_samples:\n        neg_context = self.W_out[:,\
          \ neg]\n        neg_score = sigmoid(-pos_embed @ neg_context)\n        neg_loss -= np.log(neg_score + 1e-10)\n \
          \   \n    return pos_loss + neg_loss\n\ndef train_step(self, center, context, neg_samples, lr=0.01):\n    # Compute\
          \ gradients and update\n    pos_embed = self.W_in[center]\n    pos_context = self.W_out[:, context]\n    \n    #\
          \ Gradient for positive\n    pos_score = sigmoid(pos_embed @ pos_context)\n    grad_pos = (pos_score - 1) * pos_context\n\
          \    \n    # Update embeddings\n    self.W_in[center] -= lr * grad_pos\n    self.W_out[:, context] -= lr * (pos_score\
          \ - 1) * pos_embed"
      pitfalls:
      - Sampling distribution
      - Gradient computation errors
      - Learning rate too high
      concepts:
      - Negative sampling
      - Cross-entropy loss
      - Stochastic gradient descent
      estimated_hours: 4-6
      deliverables:
      - Negative sample selection drawing non-context words weighted by frequency distribution
      - Loss function computing binary cross-entropy between positive and negative samples
      - Gradient computation deriving parameter updates from the negative sampling loss
      - Parameter update step applying stochastic gradient descent to embedding weights
    - id: 4
      name: Evaluation & Visualization
      description: Evaluate embeddings and visualize results.
      acceptance_criteria:
      - Find similar words by cosine similarity returning ranked nearest neighbors
      - 'Analogy task: king - man + woman = queen'
      - Visualize embedding clusters with t-SNE or PCA in a 2D scatter plot
      - Save and load embeddings to and from disk in a portable format
      hints:
        level1: Cosine similarity = dot(a, b) / (norm(a) * norm(b)).
        level2: 'Analogy: vec(king) - vec(man) + vec(woman) â‰ˆ vec(queen).'
        level3: "def most_similar(self, word, top_k=10):\n    if word not in self.vocab:\n        return []\n    word_vec\
          \ = self.W_in[self.vocab[word]]\n    word_vec = word_vec / np.linalg.norm(word_vec)\n    \n    similarities = self.W_in\
          \ @ word_vec\n    norms = np.linalg.norm(self.W_in, axis=1)\n    similarities = similarities / (norms + 1e-10)\n\
          \    \n    top_indices = similarities.argsort()[-top_k-1:-1][::-1]\n    idx_to_word = {i: w for w, i in self.vocab.items()}\n\
          \    return [(idx_to_word[i], similarities[i]) for i in top_indices]\n\ndef analogy(self, a, b, c):\n    # a is\
          \ to b as c is to ?\n    vec = self.W_in[self.vocab[b]] - self.W_in[self.vocab[a]] + self.W_in[self.vocab[c]]\n\
          \    return self.most_similar_vec(vec)"
      pitfalls:
      - Normalizing vectors
      - Including query word in results
      - Memory for large vocab
      concepts:
      - Cosine similarity
      - Word analogies
      - Dimensionality reduction
      estimated_hours: 3-4
      deliverables:
      - Word similarity function finding nearest neighbors by cosine similarity in embedding space
      - Analogy test evaluator computing vector arithmetic to solve word analogy tasks
      - t-SNE visualization projecting high-dimensional embeddings to 2D for visual inspection
      - Embedding export module saving trained vectors in standard text or binary format
  chatbot-intent:
    id: chatbot-intent
    name: Intent-Based Chatbot
    description: Build a chatbot with intent classification and entity extraction. Foundation for conversational AI without
      LLMs.
    difficulty: beginner
    estimated_hours: 15-25
    prerequisites:
    - Python basics
    - Text processing
    - Basic ML concepts
    languages:
      recommended:
      - Python
      also_possible:
      - JavaScript
      - Go
    resources:
    - name: Rasa Open Source
      url: https://rasa.com/docs/rasa/
      type: documentation
    - name: Intent Classification Tutorial
      url: https://www.tensorflow.org/tutorials/text/text_classification_rnn
      type: tutorial
    milestones:
    - id: 1
      name: Intent Classification
      description: Build intent classifier from training examples.
      acceptance_criteria:
      - Training data contains at least 10 examples per intent for adequate coverage
      - Classifier vectorizes text using TF-IDF and trains supervised classification model
      - Classifier predicts intent with probability score for each known intent category
      - Low-confidence predictions below threshold are reported as unknown intent
      - Classifier achieves at least 80% accuracy on held-out test set of labeled examples
      hints:
        level1: Start with TF-IDF + Naive Bayes. Simple but effective baseline.
        level2: Use scikit-learn Pipeline for clean preprocessing + classification.
        level3: |-
          from sklearn.pipeline import Pipeline
          from sklearn.feature_extraction.text import TfidfVectorizer
          from sklearn.naive_bayes import MultinomialNB
          from sklearn.model_selection import train_test_split

          class IntentClassifier:
              def __init__(self, confidence_threshold=0.5):
                  self.threshold = confidence_threshold
                  self.pipeline = Pipeline([
                      ('tfidf', TfidfVectorizer(ngram_range=(1, 2))),
                      ('clf', MultinomialNB())
                  ])
                  self.intents = []

              def train(self, texts, intents):
                  self.intents = list(set(intents))
                  self.pipeline.fit(texts, intents)

              def predict(self, text):
                  probs = self.pipeline.predict_proba([text])[0]
                  max_idx = probs.argmax()
                  confidence = probs[max_idx]

                  if confidence < self.threshold:
                      return {"intent": "unknown", "confidence": confidence}

                  return {
                      "intent": self.pipeline.classes_[max_idx],
                      "confidence": float(confidence)
                  }

          # Training data
          training_data = [
              ("What's the weather like?", "weather"),
              ("Is it going to rain?", "weather"),
              ("Set an alarm for 7am", "alarm"),
              ("Wake me up at 6", "alarm"),
              # ... more examples
          ]
          texts, intents = zip(*training_data)
          clf = IntentClassifier()
          clf.train(texts, intents)
      pitfalls:
      - Too few training examples per intent
      - Imbalanced classes skew predictions
      - Not handling out-of-domain inputs
      - Overfitting on small datasets
      concepts:
      - Text classification
      - TF-IDF
      - Naive Bayes
      - Confidence thresholds
      estimated_hours: 4-6
      deliverables:
      - Training data preparation organizing labeled examples by intent category
      - Intent classifier model using TF-IDF vectorization and Naive Bayes or SVM
      - Multi-intent handling detecting when input matches multiple intent categories
      - Confidence threshold filtering rejecting low-confidence classifications as unknown
    - id: 2
      name: Entity Extraction
      description: Extract entities (names, dates, numbers) from user input.
      acceptance_criteria:
      - NER extracts standard entities like person names, dates, and locations from text
      - Rule-based patterns extract structured entities like times, durations, and numbers
      - Slot filler maps extracted entities to required parameters for detected intent
      - Missing required slots are identified so system can prompt user for more information
      - Multiple entities in single message are extracted and assigned to correct slots
      hints:
        level1: Combine regex for structured data + spaCy NER for names/dates.
        level2: Create entity templates per intent. 'set alarm for {time}' -> extract time.
        level3: |-
          import re
          import spacy
          from dateutil import parser as date_parser

          class EntityExtractor:
              def __init__(self):
                  self.nlp = spacy.load("en_core_web_sm")
                  self.patterns = {
                      'time': r'\b(\d{1,2}(?::\d{2})?\s*(?:am|pm)?|\d{1,2}\s*o'?clock)\b',
                      'duration': r'\b(\d+)\s*(minutes?|hours?|seconds?)\b',
                      'number': r'\b(\d+(?:\.\d+)?)\b'
                  }

              def extract(self, text, intent=None):
                  entities = {}

                  # spaCy NER for standard entities
                  doc = self.nlp(text)
                  for ent in doc.ents:
                      if ent.label_ in ('PERSON', 'ORG', 'GPE', 'DATE', 'TIME', 'MONEY'):
                          entities[ent.label_.lower()] = ent.text

                  # Custom patterns
                  for entity_type, pattern in self.patterns.items():
                      match = re.search(pattern, text, re.IGNORECASE)
                      if match:
                          entities[entity_type] = match.group(0)

                  # Parse dates/times
                  if 'time' in entities:
                      try:
                          entities['parsed_time'] = date_parser.parse(entities['time'])
                      except:
                          pass

                  return entities

          # Slot filling
          class SlotFiller:
              def __init__(self):
                  self.intent_slots = {
                      'set_alarm': ['time'],
                      'book_restaurant': ['date', 'time', 'party_size', 'restaurant'],
                      'send_email': ['recipient', 'subject']
                  }

              def fill_slots(self, intent, entities):
                  required = self.intent_slots.get(intent, [])
                  filled = {slot: entities.get(slot) for slot in required}
                  missing = [s for s in required if not filled.get(s)]
                  return {'slots': filled, 'missing': missing}
      pitfalls:
      - Regex too greedy matches wrong text
      - spaCy model not loaded (need python -m spacy download)
      - Date parsing ambiguity (01/02 = Jan 2 or Feb 1?)
      - Overlapping entity matches
      concepts:
      - Named Entity Recognition
      - Regex patterns
      - Slot filling
      - spaCy
      estimated_hours: 4-6
      deliverables:
      - Named entity recognition extracting people, places, dates, and custom entities
      - Slot filling mapping extracted entities to intent-specific parameter slots
      - Entity normalization converting extracted text to canonical data formats
      - Custom entity type definitions for domain-specific extraction patterns
    - id: 3
      name: Dialog Management
      description: Manage multi-turn conversations with context.
      acceptance_criteria:
      - Dialog state persists across turns tracking current intent and filled slots
      - Follow-up messages add to existing context without requiring full intent restatement
      - Missing slot prompts ask user specific questions to collect required information
      - Conversation reset command clears context and returns to initial state
      - Context expires after configurable inactivity timeout to prevent stale sessions
      hints:
        level1: Store conversation state in dict keyed by session_id.
        level2: 'State machine: INIT -> COLLECTING_SLOTS -> CONFIRMING -> EXECUTING'
        level3: |-
          from dataclasses import dataclass, field
          from typing import Dict, Any, Optional
          from enum import Enum
          import time

          class DialogState(Enum):
              INIT = "init"
              COLLECTING = "collecting"
              CONFIRMING = "confirming"
              EXECUTING = "executing"
              COMPLETE = "complete"

          @dataclass
          class ConversationContext:
              session_id: str
              state: DialogState = DialogState.INIT
              current_intent: Optional[str] = None
              slots: Dict[str, Any] = field(default_factory=dict)
              history: list = field(default_factory=list)
              last_active: float = field(default_factory=time.time)

              def is_expired(self, timeout=300):
                  return time.time() - self.last_active > timeout

          class DialogManager:
              def __init__(self, intent_classifier, entity_extractor, slot_filler):
                  self.classifier = intent_classifier
                  self.extractor = entity_extractor
                  self.filler = slot_filler
                  self.contexts: Dict[str, ConversationContext] = {}

              def process(self, session_id: str, user_input: str) -> str:
                  ctx = self._get_or_create_context(session_id)
                  ctx.last_active = time.time()
                  ctx.history.append({'role': 'user', 'text': user_input})

                  # Intent classification
                  if ctx.state == DialogState.INIT:
                      result = self.classifier.predict(user_input)
                      ctx.current_intent = result['intent']
                      ctx.state = DialogState.COLLECTING

                  # Entity extraction
                  entities = self.extractor.extract(user_input, ctx.current_intent)
                  ctx.slots.update(entities)

                  # Check missing slots
                  slot_result = self.filler.fill_slots(ctx.current_intent, ctx.slots)

                  if slot_result['missing']:
                      response = self._ask_for_slot(slot_result['missing'][0])
                  else:
                      ctx.state = DialogState.CONFIRMING
                      response = self._confirm_action(ctx)

                  ctx.history.append({'role': 'assistant', 'text': response})
                  return response

              def _ask_for_slot(self, slot_name):
                  prompts = {
                      'time': "What time would you like?",
                      'date': "What date?",
                      'recipient': "Who should I send it to?"
                  }
                  return prompts.get(slot_name, f"Please provide {slot_name}")
      pitfalls:
      - Memory leak from never-expiring sessions
      - Losing context on intent change
      - Infinite loops in dialog flow
      - Not handling 'cancel' or 'start over'
      concepts:
      - State machines
      - Session management
      - Multi-turn dialog
      - Context tracking
      estimated_hours: 5-8
      deliverables:
      - Dialog state tracking maintaining conversation context across multiple turns
      - Context management preserving and updating slot values throughout conversation
      - Multi-turn conversation support handling follow-up questions and clarifications
      - Slot confirmation prompting user to verify extracted values before executing action
    - id: 4
      name: Response Generation
      description: Generate natural language responses with templates.
      acceptance_criteria:
      - Response templates include placeholders replaced with context-specific entity values
      - Variable substitution correctly inserts extracted entities into response text
      - Multiple response variations exist per intent-state combination for natural conversation
      - Fallback responses handle unrecognized input with helpful clarification prompts
      - Response tone remains consistent across all generated responses for the chatbot persona
      hints:
        level1: Use Jinja2 templates with random.choice for variations.
        level2: Group responses by intent and state. Add personality traits.
        level3: |-
          import random
          from jinja2 import Template
          from dataclasses import dataclass

          @dataclass
          class ResponseTemplate:
              templates: list

              def render(self, **kwargs):
                  template = random.choice(self.templates)
                  return Template(template).render(**kwargs)

          class ResponseGenerator:
              def __init__(self):
                  self.responses = {
                      ('weather', 'success'): ResponseTemplate([
                          "It's currently {{temp}}Â°F and {{condition}} in {{location}}.",
                          "The weather in {{location}}: {{temp}}Â°F, {{condition}}.",
                          "{{location}} is {{condition}} right now, {{temp}}Â°F."
                      ]),
                      ('alarm', 'success'): ResponseTemplate([
                          "Done! I've set an alarm for {{time}}.",
                          "Alarm set for {{time}}. I'll wake you up!",
                          "Got it - {{time}} alarm is ready."
                      ]),
                      ('alarm', 'confirm'): ResponseTemplate([
                          "Just to confirm: set an alarm for {{time}}?",
                          "You want me to wake you at {{time}}, correct?"
                      ]),
                      ('error', 'not_understood'): ResponseTemplate([
                          "I'm not sure I understand. Could you rephrase?",
                          "Sorry, I didn't catch that. Can you try again?",
                          "I'm having trouble understanding. What would you like?"
                      ]),
                      ('error', 'missing_slot'): ResponseTemplate([
                          "I need a bit more info. {{question}}",
                          "{{question}}"
                      ])
                  }

                  # Personality traits
                  self.personality = {
                      'greeting_prefix': ["Sure!", "Absolutely!", "Of course!", ""],
                      'filler_words': ["just", "actually", ""],
                  }

              def generate(self, intent, state, **context):
                  key = (intent, state)
                  if key not in self.responses:
                      key = ('error', 'not_understood')

                  response = self.responses[key].render(**context)

                  # Add personality
                  if random.random() < 0.3:
                      prefix = random.choice(self.personality['greeting_prefix'])
                      if prefix:
                          response = f"{prefix} {response}"

                  return response
      pitfalls:
      - Templates not escaping user input (XSS)
      - Missing template variables cause errors
      - Too few variations feel robotic
      - Inconsistent tone across responses
      concepts:
      - Template engines
      - NLG basics
      - Response variation
      - Personality design
      estimated_hours: 3-5
      deliverables:
      - Template-based response system with variable substitution for dynamic content
      - Dynamic content insertion filling response templates with context-specific values
      - Fallback handling providing graceful responses for unrecognized or ambiguous input
      - Response variation randomly selecting from alternative phrasings to avoid repetition
  rag-system:
    id: rag-system
    name: RAG System (Retrieval Augmented Generation)
    description: 'Build a production RAG pipeline: document ingestion, chunking, embedding, vector search, and LLM generation
      with retrieved context.'
    difficulty: intermediate
    estimated_hours: 30-50
    prerequisites:
    - Python
    - Basic ML concepts
    - REST APIs
    - Database basics
    languages:
      recommended:
      - Python
      also_possible:
      - TypeScript
      - Go
    resources:
    - name: LangChain RAG Tutorial
      url: https://python.langchain.com/docs/tutorials/rag/
      type: tutorial
    - name: OpenAI Embeddings
      url: https://platform.openai.com/docs/guides/embeddings
      type: documentation
    - name: Pinecone Vector DB
      url: https://docs.pinecone.io/
      type: documentation
    milestones:
    - id: 1
      name: Document Ingestion & Chunking
      description: Load documents and split into optimal chunks for retrieval.
      acceptance_criteria:
      - Load documents from PDF, Markdown, and HTML file formats
      - Extract text content while preserving document metadata
      - Implement multiple chunking strategies (fixed, semantic, recursive)
      - Handle chunk overlap to maintain context continuity across boundaries
      hints:
        level1: Start with fixed-size chunks (500-1000 tokens) with 100 token overlap.
        level2: 'Semantic chunking: split on paragraphs/sections, then enforce max size.'
        level3: |-
          from dataclasses import dataclass
          from typing import List, Optional
          import tiktoken
          from pypdf import PdfReader
          import markdown
          from bs4 import BeautifulSoup

          @dataclass
          class Chunk:
              text: str
              metadata: dict  # source, page, position, etc.
              token_count: int

          class DocumentLoader:
              def load(self, path: str) -> str:
                  if path.endswith('.pdf'):
                      return self._load_pdf(path)
                  elif path.endswith('.md'):
                      return self._load_markdown(path)
                  elif path.endswith('.html'):
                      return self._load_html(path)
                  else:
                      return open(path).read()

              def _load_pdf(self, path):
                  reader = PdfReader(path)
                  pages = []
                  for i, page in enumerate(reader.pages):
                      text = page.extract_text()
                      pages.append({'text': text, 'page': i + 1})
                  return pages

          class TextChunker:
              def __init__(self, chunk_size=500, overlap=100, model="gpt-4"):
                  self.chunk_size = chunk_size
                  self.overlap = overlap
                  self.encoder = tiktoken.encoding_for_model(model)

              def chunk_text(self, text: str, metadata: dict = None) -> List[Chunk]:
                  tokens = self.encoder.encode(text)
                  chunks = []
                  start = 0

                  while start < len(tokens):
                      end = start + self.chunk_size
                      chunk_tokens = tokens[start:end]
                      chunk_text = self.encoder.decode(chunk_tokens)

                      chunks.append(Chunk(
                          text=chunk_text,
                          metadata={**(metadata or {}), 'start_token': start},
                          token_count=len(chunk_tokens)
                      ))

                      start = end - self.overlap

                  return chunks

              def semantic_chunk(self, text: str, metadata: dict = None) -> List[Chunk]:
                  # Split on semantic boundaries first
                  paragraphs = text.split('\n\n')
                  chunks = []
                  current_chunk = []
                  current_tokens = 0

                  for para in paragraphs:
                      para_tokens = len(self.encoder.encode(para))

                      if current_tokens + para_tokens > self.chunk_size:
                          if current_chunk:
                              chunks.append(self._make_chunk(current_chunk, metadata))
                          current_chunk = [para]
                          current_tokens = para_tokens
                      else:
                          current_chunk.append(para)
                          current_tokens += para_tokens

                  if current_chunk:
                      chunks.append(self._make_chunk(current_chunk, metadata))

                  return chunks
      pitfalls:
      - Chunks too small lose context
      - Chunks too large exceed LLM context window
      - PDF extraction loses formatting/tables
      - Not handling encoding issues (UTF-8)
      concepts:
      - Document parsing
      - Text chunking
      - Tokenization
      - Metadata tracking
      estimated_hours: 6-10
      deliverables:
      - Document loader supporting PDF, Markdown, and plain text formats
      - Text chunking strategies including fixed-size and semantic splitting
      - Chunk overlap handling to preserve cross-boundary context
      - Metadata extraction tracking source document and position
    - id: 2
      name: Embedding Generation
      description: Convert text chunks to vector embeddings for semantic search.
      acceptance_criteria:
      - Generate vector embeddings using OpenAI or local sentence-transformer model
      - Process embeddings in batches for efficiency and rate limit compliance
      - Handle API rate limits with exponential backoff retry logic
      - Cache computed embeddings to disk to avoid redundant API calls
      hints:
        level1: Use OpenAI text-embedding-3-small for good quality/cost ratio.
        level2: Batch requests (max 2048 texts per call). Implement exponential backoff.
        level3: |-
          import openai
          from tenacity import retry, wait_exponential, stop_after_attempt
          import numpy as np
          from typing import List
          import hashlib
          import pickle
          from pathlib import Path

          class EmbeddingService:
              def __init__(self, model="text-embedding-3-small", cache_dir=".cache/embeddings"):
                  self.model = model
                  self.client = openai.OpenAI()
                  self.cache_dir = Path(cache_dir)
                  self.cache_dir.mkdir(parents=True, exist_ok=True)
                  self.dimension = 1536  # OpenAI small model

              def _cache_key(self, text: str) -> str:
                  return hashlib.md5(f"{self.model}:{text}".encode()).hexdigest()

              def _get_cached(self, text: str) -> Optional[np.ndarray]:
                  cache_path = self.cache_dir / f"{self._cache_key(text)}.pkl"
                  if cache_path.exists():
                      return pickle.load(open(cache_path, 'rb'))
                  return None

              def _set_cached(self, text: str, embedding: np.ndarray):
                  cache_path = self.cache_dir / f"{self._cache_key(text)}.pkl"
                  pickle.dump(embedding, open(cache_path, 'wb'))

              @retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))
              def _embed_batch(self, texts: List[str]) -> List[np.ndarray]:
                  response = self.client.embeddings.create(
                      model=self.model,
                      input=texts
                  )
                  return [np.array(e.embedding) for e in response.data]

              def embed(self, texts: List[str], batch_size=100) -> List[np.ndarray]:
                  results = [None] * len(texts)
                  to_embed = []  # (index, text) pairs

                  # Check cache first
                  for i, text in enumerate(texts):
                      cached = self._get_cached(text)
                      if cached is not None:
                          results[i] = cached
                      else:
                          to_embed.append((i, text))

                  # Batch embed uncached
                  for batch_start in range(0, len(to_embed), batch_size):
                      batch = to_embed[batch_start:batch_start + batch_size]
                      indices, batch_texts = zip(*batch) if batch else ([], [])

                      embeddings = self._embed_batch(list(batch_texts))

                      for idx, text, emb in zip(indices, batch_texts, embeddings):
                          self._set_cached(text, emb)
                          results[idx] = emb

                  return results

          # Local embedding alternative with sentence-transformers
          class LocalEmbedding:
              def __init__(self, model_name="all-MiniLM-L6-v2"):
                  from sentence_transformers import SentenceTransformer
                  self.model = SentenceTransformer(model_name)
                  self.dimension = self.model.get_sentence_embedding_dimension()

              def embed(self, texts: List[str]) -> List[np.ndarray]:
                  return self.model.encode(texts, convert_to_numpy=True)
      pitfalls:
      - Rate limits cause failures without retry logic
      - Large batches exceed API limits
      - Not normalizing embeddings for cosine similarity
      - Embedding dimension mismatch between models
      concepts:
      - Text embeddings
      - API rate limiting
      - Caching strategies
      - Batch processing
      estimated_hours: 4-6
      deliverables:
      - Embedding model integration with OpenAI or local transformer
      - Batch embedding processing for efficient API usage
      - Embedding cache storing computed vectors to avoid recomputation
      - Model selection supporting multiple embedding providers
    - id: 3
      name: Vector Store & Retrieval
      description: Store embeddings and perform similarity search.
      acceptance_criteria:
      - Store embedding vectors alongside chunk text and metadata
      - Perform K-nearest neighbor search returning most similar chunks
      - Support hybrid search combining vector similarity and keyword BM25
      - Filter search results by metadata attributes like source or date
      hints:
        level1: Start with Chroma (local, no setup). Graduate to Pinecone for scale.
        level2: 'Hybrid search: combine BM25 keyword score with vector similarity.'
        level3: |-
          import chromadb
          from chromadb.config import Settings
          import numpy as np
          from typing import List, Dict, Optional
          from rank_bm25 import BM25Okapi

          class VectorStore:
              def __init__(self, collection_name="documents", persist_dir=".chroma"):
                  self.client = chromadb.PersistentClient(path=persist_dir)
                  self.collection = self.client.get_or_create_collection(
                      name=collection_name,
                      metadata={"hnsw:space": "cosine"}
                  )
                  self._bm25 = None
                  self._doc_texts = []

              def add(self, chunks: List[Chunk], embeddings: List[np.ndarray]):
                  ids = [f"chunk_{i}_{hash(c.text)}" for i, c in enumerate(chunks)]

                  self.collection.add(
                      ids=ids,
                      embeddings=[e.tolist() for e in embeddings],
                      documents=[c.text for c in chunks],
                      metadatas=[c.metadata for c in chunks]
                  )

                  # Update BM25 index
                  self._doc_texts.extend([c.text for c in chunks])
                  self._rebuild_bm25()

              def _rebuild_bm25(self):
                  tokenized = [doc.lower().split() for doc in self._doc_texts]
                  self._bm25 = BM25Okapi(tokenized)

              def search(self, query_embedding: np.ndarray, query_text: str,
                         k: int = 5, filter: Dict = None, hybrid_alpha: float = 0.5) -> List[Dict]:

                  # Vector search
                  vector_results = self.collection.query(
                      query_embeddings=[query_embedding.tolist()],
                      n_results=k * 2,  # Get more for reranking
                      where=filter
                  )

                  if not self._bm25 or hybrid_alpha == 1.0:
                      return self._format_results(vector_results)

                  # BM25 search
                  tokenized_query = query_text.lower().split()
                  bm25_scores = self._bm25.get_scores(tokenized_query)

                  # Combine scores
                  combined = []
                  for i, (doc_id, doc, metadata, distance) in enumerate(zip(
                      vector_results['ids'][0],
                      vector_results['documents'][0],
                      vector_results['metadatas'][0],
                      vector_results['distances'][0]
                  )):
                      vector_score = 1 - distance  # Convert distance to similarity

                      # Find BM25 score for this doc
                      try:
                          bm25_idx = self._doc_texts.index(doc)
                          bm25_score = bm25_scores[bm25_idx] / max(bm25_scores)
                      except ValueError:
                          bm25_score = 0

                      combined_score = hybrid_alpha * vector_score + (1 - hybrid_alpha) * bm25_score
                      combined.append({
                          'id': doc_id,
                          'text': doc,
                          'metadata': metadata,
                          'score': combined_score
                      })

                  # Sort by combined score and return top k
                  combined.sort(key=lambda x: x['score'], reverse=True)
                  return combined[:k]
      pitfalls:
      - Using wrong distance metric (cosine vs L2)
      - Not normalizing scores for hybrid search
      - Memory issues with large collections
      - Stale BM25 index after updates
      concepts:
      - Vector databases
      - ANN search
      - BM25
      - Hybrid retrieval
      estimated_hours: 6-8
      deliverables:
      - Vector database integration with Chroma, Pinecone, or pgvector
      - K-nearest neighbor similarity search over stored embeddings
      - Metadata-based filtering narrowing search to relevant subsets
      - Re-ranking retrieved results using cross-encoder model
    - id: 4
      name: LLM Integration & Prompting
      description: Generate answers using retrieved context with LLM.
      acceptance_criteria:
      - Build effective RAG prompt template with retrieved context
      - Handle context window limits by truncating excess chunks
      - Stream LLM responses token by token to the client
      - Handle LLM API errors and timeouts with graceful fallback
      hints:
        level1: 'Simple template: ''Answer based on context: {context}\n\nQuestion: {question}'''
        level2: 'Add instructions: cite sources, say ''I don''t know'' if not in context.'
        level3: |-
          from openai import OpenAI
          from anthropic import Anthropic
          from typing import Generator, List
          import tiktoken

          class LLMService:
              def __init__(self, provider="openai", model="gpt-4-turbo-preview"):
                  self.provider = provider
                  self.model = model
                  if provider == "openai":
                      self.client = OpenAI()
                      self.encoder = tiktoken.encoding_for_model(model)
                  elif provider == "anthropic":
                      self.client = Anthropic()

                  self.max_context = 128000  # GPT-4 Turbo

              def _build_rag_prompt(self, question: str, contexts: List[Dict]) -> str:
                  context_text = "\n\n---\n\n".join([
                      f"[Source: {c['metadata'].get('source', 'unknown')}]\n{c['text']}"
                      for c in contexts
                  ])

                  return f'''You are a helpful assistant that answers questions based on the provided context.

          CONTEXT:
          {context_text}

          INSTRUCTIONS:
          - Answer the question based ONLY on the context above
          - If the answer is not in the context, say "I don't have enough information to answer this"
          - Cite your sources using [Source: filename] format
          - Be concise but complete

          QUESTION: {question}

          ANSWER:'''

              def _truncate_contexts(self, contexts: List[Dict], max_tokens: int) -> List[Dict]:
                  result = []
                  total_tokens = 0

                  for ctx in contexts:
                      ctx_tokens = len(self.encoder.encode(ctx['text']))
                      if total_tokens + ctx_tokens > max_tokens:
                          break
                      result.append(ctx)
                      total_tokens += ctx_tokens

                  return result

              def generate(self, question: str, contexts: List[Dict],
                           stream: bool = True) -> Generator[str, None, None]:
                  # Reserve tokens for question and response
                  max_context_tokens = self.max_context - 4000
                  contexts = self._truncate_contexts(contexts, max_context_tokens)

                  prompt = self._build_rag_prompt(question, contexts)

                  if self.provider == "openai":
                      response = self.client.chat.completions.create(
                          model=self.model,
                          messages=[{"role": "user", "content": prompt}],
                          stream=stream,
                          temperature=0.1
                      )

                      if stream:
                          for chunk in response:
                              if chunk.choices[0].delta.content:
                                  yield chunk.choices[0].delta.content
                      else:
                          yield response.choices[0].message.content

          class RAGPipeline:
              def __init__(self, embedding_service, vector_store, llm_service):
                  self.embedder = embedding_service
                  self.store = vector_store
                  self.llm = llm_service

              def query(self, question: str, k: int = 5) -> Generator[str, None, None]:
                  # Embed question
                  query_embedding = self.embedder.embed([question])[0]

                  # Retrieve relevant chunks
                  contexts = self.store.search(query_embedding, question, k=k)

                  # Generate response
                  yield from self.llm.generate(question, contexts)
      pitfalls:
      - Exceeding context window with too many chunks
      - LLM hallucinating despite 'only use context' instruction
      - Not handling streaming errors mid-response
      - Prompt injection from retrieved content
      concepts:
      - Prompt engineering
      - Context window management
      - Streaming responses
      - LLM providers
      estimated_hours: 6-8
      deliverables:
      - RAG prompt template constructing context-augmented queries
      - Prompt template with citation and grounding instructions
      - LLM API call wrapper supporting streaming responses
      - Response parsing extracting answer text and source citations
    - id: 5
      name: Evaluation & Optimization
      description: Measure and improve RAG quality.
      acceptance_criteria:
      - Implement retrieval metrics including recall at K and MRR
      - Implement generation quality metrics for faithfulness and relevance
      - Create labeled evaluation dataset with questions and expected answers
      - A/B test different configurations and measure quality differences
      hints:
        level1: 'Create golden dataset: questions + expected chunks + expected answers.'
        level2: Use LLM-as-judge for generation quality (faithfulness, relevance).
        level3: |-
          from dataclasses import dataclass
          from typing import List, Tuple
          import numpy as np

          @dataclass
          class EvalSample:
              question: str
              expected_chunks: List[str]  # Ground truth relevant chunks
              expected_answer: str

          class RAGEvaluator:
              def __init__(self, rag_pipeline, llm_judge):
                  self.rag = rag_pipeline
                  self.judge = llm_judge

              def evaluate_retrieval(self, samples: List[EvalSample], k: int = 5) -> Dict:
                  recalls = []
                  mrrs = []

                  for sample in samples:
                      query_emb = self.rag.embedder.embed([sample.question])[0]
                      retrieved = self.rag.store.search(query_emb, sample.question, k=k)
                      retrieved_texts = [r['text'] for r in retrieved]

                      # Recall@k
                      hits = sum(1 for exp in sample.expected_chunks
                                if any(exp in ret for ret in retrieved_texts))
                      recall = hits / len(sample.expected_chunks)
                      recalls.append(recall)

                      # MRR
                      for i, ret in enumerate(retrieved_texts):
                          if any(exp in ret for exp in sample.expected_chunks):
                              mrrs.append(1 / (i + 1))
                              break
                      else:
                          mrrs.append(0)

                  return {
                      'recall@k': np.mean(recalls),
                      'mrr': np.mean(mrrs)
                  }

              def evaluate_generation(self, samples: List[EvalSample]) -> Dict:
                  faithfulness_scores = []
                  relevance_scores = []

                  for sample in samples:
                      # Get RAG response
                      response = ''.join(self.rag.query(sample.question))
                      query_emb = self.rag.embedder.embed([sample.question])[0]
                      contexts = self.rag.store.search(query_emb, sample.question, k=5)

                      # Faithfulness: is response grounded in context?
                      faithfulness = self.judge.evaluate(
                          prompt=f'''Rate if the response is fully supported by the context.
          Context: {[c["text"] for c in contexts]}
          Response: {response}
          Score 1-5 (5=fully grounded):''',
                          parse_fn=lambda x: int(x.strip()) / 5
                      )
                      faithfulness_scores.append(faithfulness)

                      # Relevance: does response answer the question?
                      relevance = self.judge.evaluate(
                          prompt=f'''Rate if the response answers the question.
          Question: {sample.question}
          Response: {response}
          Score 1-5 (5=fully answers):''',
                          parse_fn=lambda x: int(x.strip()) / 5
                      )
                      relevance_scores.append(relevance)

                  return {
                      'faithfulness': np.mean(faithfulness_scores),
                      'relevance': np.mean(relevance_scores)
                  }

          # Reranking for better retrieval
          class Reranker:
              def __init__(self, model="cross-encoder/ms-marco-MiniLM-L-6-v2"):
                  from sentence_transformers import CrossEncoder
                  self.model = CrossEncoder(model)

              def rerank(self, query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
                  pairs = [(query, doc['text']) for doc in documents]
                  scores = self.model.predict(pairs)

                  for doc, score in zip(documents, scores):
                      doc['rerank_score'] = float(score)

                  documents.sort(key=lambda x: x['rerank_score'], reverse=True)
                  return documents[:top_k]
      pitfalls:
      - Evaluation set too small to be statistically significant
      - LLM judge bias toward verbose responses
      - Not versioning evaluation datasets
      - Overfitting to evaluation set
      concepts:
      - Retrieval metrics
      - Generation evaluation
      - LLM-as-judge
      - Cross-encoder reranking
      estimated_hours: 8-12
      deliverables:
      - Retrieval metrics measuring recall at K and mean reciprocal rank
      - Answer quality metrics scoring faithfulness and relevance
      - Chunk size optimization comparing different chunking configurations
      - Prompt optimization testing different template variations
  semantic-search:
    id: semantic-search
    name: Semantic Search Engine
    description: Build a semantic search engine that understands meaning, not just keywords. Core technology behind modern
      search.
    difficulty: intermediate
    estimated_hours: 25-40
    prerequisites:
    - Python
    - Embeddings basics
    - Database fundamentals
    languages:
      recommended:
      - Python
      - Go
      also_possible:
      - TypeScript
      - Rust
    resources:
    - name: Sentence Transformers
      url: https://www.sbert.net/
      type: documentation
    - name: FAISS Library
      url: https://github.com/facebookresearch/faiss
      type: documentation
    milestones:
    - id: 1
      name: Embedding Index
      description: Build efficient vector index for similarity search.
      acceptance_criteria:
      - Index millions of document vectors with efficient memory usage
      - Achieve sub-second query latency for nearest neighbor search
      - Support incremental index updates without full reconstruction
      - Implement HNSW or IVF approximate nearest neighbor index
      - Support memory-mapped access for datasets exceeding available RAM
      hints:
        level1: FAISS IVF for large scale, HNSW for accuracy. Start with Flat for small data.
        level2: 'IVF: cluster vectors, search only nearby clusters. HNSW: navigable small world graph.'
        level3: |-
          import faiss
          import numpy as np
          from typing import List, Tuple

          class VectorIndex:
              def __init__(self, dimension: int, index_type: str = "hnsw"):
                  self.dimension = dimension
                  self.index_type = index_type
                  self._build_index()
                  self.id_map = {}  # internal_id -> external_id
                  self.next_id = 0

              def _build_index(self):
                  if self.index_type == "flat":
                      # Exact search, O(n)
                      self.index = faiss.IndexFlatIP(self.dimension)

                  elif self.index_type == "ivf":
                      # Inverted file index, faster for large datasets
                      quantizer = faiss.IndexFlatIP(self.dimension)
                      self.index = faiss.IndexIVFFlat(quantizer, self.dimension, 100)  # 100 clusters
                      self.needs_training = True

                  elif self.index_type == "hnsw":
                      # Hierarchical Navigable Small World - best accuracy/speed tradeoff
                      self.index = faiss.IndexHNSWFlat(self.dimension, 32)  # 32 connections per node
                      self.index.hnsw.efConstruction = 200  # Build-time quality
                      self.index.hnsw.efSearch = 50  # Search-time quality

              def add(self, vectors: np.ndarray, ids: List[str]):
                  # Normalize for cosine similarity
                  faiss.normalize_L2(vectors)

                  # Train if needed (IVF)
                  if hasattr(self, 'needs_training') and self.needs_training:
                      self.index.train(vectors)
                      self.needs_training = False

                  # Map external IDs to internal
                  for ext_id in ids:
                      self.id_map[self.next_id] = ext_id
                      self.next_id += 1

                  self.index.add(vectors)

              def search(self, query: np.ndarray, k: int = 10) -> List[Tuple[str, float]]:
                  query = query.reshape(1, -1).astype('float32')
                  faiss.normalize_L2(query)

                  distances, indices = self.index.search(query, k)

                  results = []
                  for idx, dist in zip(indices[0], distances[0]):
                      if idx >= 0:  # -1 means not found
                          ext_id = self.id_map.get(idx, str(idx))
                          results.append((ext_id, float(dist)))

                  return results

              def save(self, path: str):
                  faiss.write_index(self.index, f"{path}.faiss")
                  np.save(f"{path}_idmap.npy", self.id_map)

              def load(self, path: str):
                  self.index = faiss.read_index(f"{path}.faiss")
                  self.id_map = np.load(f"{path}_idmap.npy", allow_pickle=True).item()
      pitfalls:
      - Not normalizing vectors for cosine similarity
      - IVF requires training before adding
      - Memory explosion with large HNSW M parameter
      - ID mapping gets out of sync
      concepts:
      - HNSW algorithm
      - IVF indexing
      - Approximate nearest neighbors
      - Index persistence
      estimated_hours: 6-10
      deliverables:
      - Document embedding pipeline converting text to vector representations
      - Vector index construction using HNSW or IVF algorithm
      - Index persistence saving and loading trained index from disk
      - Incremental update support adding new vectors without full rebuild
    - id: 2
      name: Query Processing
      description: Parse and enhance queries for better results.
      acceptance_criteria:
      - Expand queries with synonyms and related terms for broader recall
      - Understand query intent by extracting entities and semantic meaning
      - Support multi-vector queries combining multiple query aspects
      - Handle negative query terms subtracting unwanted concepts from results
      - Cache frequent query embeddings for faster repeated lookups
      hints:
        level1: Expand queries with WordNet synonyms or use LLM to generate variations.
        level2: 'Multi-vector: average embeddings of query expansion. Weight original higher.'
        level3: |-
          from functools import lru_cache
          import numpy as np
          from nltk.corpus import wordnet

          class QueryProcessor:
              def __init__(self, embedding_service):
                  self.embedder = embedding_service

              @lru_cache(maxsize=10000)
              def _get_synonyms(self, word: str) -> List[str]:
                  synonyms = set()
                  for syn in wordnet.synsets(word):
                      for lemma in syn.lemmas():
                          synonyms.add(lemma.name().replace('_', ' '))
                  return list(synonyms)[:5]

              def expand_query(self, query: str) -> List[str]:
                  words = query.lower().split()
                  expanded = [query]

                  for word in words:
                      synonyms = self._get_synonyms(word)
                      for syn in synonyms[:2]:
                          expanded.append(query.replace(word, syn))

                  return list(set(expanded))

              def process(self, query: str) -> np.ndarray:
                  # Parse negative queries
                  positive_query, negative_terms = self._parse_negatives(query)

                  # Expand query
                  expansions = self.expand_query(positive_query)

                  # Embed all expansions
                  embeddings = self.embedder.embed(expansions)

                  # Weighted average (original query weighted 2x)
                  weights = [2.0] + [1.0] * (len(embeddings) - 1)
                  weights = np.array(weights) / sum(weights)
                  query_vector = np.average(embeddings, axis=0, weights=weights)

                  # Subtract negative term vectors
                  if negative_terms:
                      neg_embeddings = self.embedder.embed(negative_terms)
                      neg_vector = np.mean(neg_embeddings, axis=0)
                      query_vector = query_vector - 0.5 * neg_vector

                  # Normalize
                  query_vector = query_vector / np.linalg.norm(query_vector)

                  return query_vector

              def _parse_negatives(self, query: str) -> Tuple[str, List[str]]:
                  # "python tutorial -video -beginner"
                  parts = query.split()
                  positive = []
                  negative = []

                  for part in parts:
                      if part.startswith('-'):
                          negative.append(part[1:])
                      else:
                          positive.append(part)

                  return ' '.join(positive), negative
      pitfalls:
      - Over-expansion dilutes original intent
      - Synonym quality varies by domain
      - Negative subtraction can flip meaning
      - Cache invalidation on embedding model change
      concepts:
      - Query expansion
      - Semantic understanding
      - Vector arithmetic
      - Query caching
      estimated_hours: 5-8
      deliverables:
      - Query text embedding converting search query to vector
      - Nearest neighbor similarity search over indexed vectors
      - Top-K result retrieval returning most similar documents
      - Query expansion generating synonym and related term variants
    - id: 3
      name: Ranking & Relevance
      description: Combine signals for optimal result ranking.
      acceptance_criteria:
      - Implement multi-stage ranking with fast retrieval then precise reranking
      - Combine semantic vector scores with lexical BM25 keyword scores
      - Apply personalization signals boosting results matching user preferences
      - Boost recent documents with freshness score decay function
      - Learn from click-through rate data to improve future ranking quality
      hints:
        level1: 'First stage: fast vector search. Second stage: cross-encoder reranking on top-100.'
        level2: 'Linear combination: score = Î±*semantic + Î²*bm25 + Î³*freshness + Î´*popularity'
        level3: |-
          from sentence_transformers import CrossEncoder
          from datetime import datetime
          import math

          class RankingService:
              def __init__(self):
                  self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

                  # Feature weights (learned or tuned)
                  self.weights = {
                      'semantic': 0.4,
                      'lexical': 0.2,
                      'rerank': 0.25,
                      'freshness': 0.1,
                      'popularity': 0.05
                  }

              def rank(self, query: str, candidates: List[Dict],
                       user_context: Dict = None) -> List[Dict]:

                  # Stage 1: Score all candidates
                  for doc in candidates:
                      doc['scores'] = {}
                      doc['scores']['semantic'] = doc.get('vector_score', 0)
                      doc['scores']['lexical'] = doc.get('bm25_score', 0)
                      doc['scores']['freshness'] = self._freshness_score(doc)
                      doc['scores']['popularity'] = self._popularity_score(doc)

                  # Stage 2: Rerank top candidates with cross-encoder
                  top_candidates = sorted(candidates,
                                          key=lambda x: x['scores']['semantic'],
                                          reverse=True)[:100]

                  pairs = [(query, doc['text'][:512]) for doc in top_candidates]
                  rerank_scores = self.cross_encoder.predict(pairs)

                  for doc, score in zip(top_candidates, rerank_scores):
                      doc['scores']['rerank'] = float(score)

                  # Combine scores
                  for doc in top_candidates:
                      doc['final_score'] = sum(
                          self.weights[k] * doc['scores'].get(k, 0)
                          for k in self.weights
                      )

                      # Personalization boost
                      if user_context:
                          doc['final_score'] *= self._personalization_boost(doc, user_context)

                  return sorted(top_candidates, key=lambda x: x['final_score'], reverse=True)

              def _freshness_score(self, doc: Dict) -> float:
                  if 'timestamp' not in doc:
                      return 0.5
                  age_days = (datetime.now() - doc['timestamp']).days
                  # Exponential decay, half-life of 30 days
                  return math.exp(-age_days / 30)

              def _popularity_score(self, doc: Dict) -> float:
                  clicks = doc.get('click_count', 0)
                  impressions = doc.get('impression_count', 1)
                  # Smoothed CTR
                  return (clicks + 1) / (impressions + 10)

              def _personalization_boost(self, doc: Dict, user: Dict) -> float:
                  boost = 1.0
                  # Boost docs from user's preferred categories
                  if doc.get('category') in user.get('preferred_categories', []):
                      boost *= 1.2
                  # Boost recent user interests
                  if any(tag in user.get('recent_tags', []) for tag in doc.get('tags', [])):
                      boost *= 1.1
                  return boost
      pitfalls:
      - Cross-encoder too slow for all results
      - Weight tuning is data-dependent
      - Popularity bias creates filter bubbles
      - Freshness can demote evergreen content
      concepts:
      - Multi-stage ranking
      - Learning to rank
      - Feature engineering
      - Personalization
      estimated_hours: 6-10
      deliverables:
      - Semantic similarity scoring measuring vector distance between query and documents
      - Hybrid search combining keyword BM25 and semantic vector scores
      - Cross-encoder re-ranking refining top candidates with pairwise model
      - Relevance tuning adjusting score weights for different signal types
    - id: 4
      name: Search API & UI
      description: Build production search API with instant results.
      acceptance_criteria:
      - Serve RESTful search API with JSON request and response format
      - Provide typeahead autocomplete suggestions with sub-100ms latency
      - Support faceted search with filter counts per category
      - Highlight matching query terms in result text snippets
      - Collect and display search analytics including zero-result queries
      hints:
        level1: 'Autocomplete: prefix trie + popular queries. Update on each keystroke.'
        level2: 'Facets: pre-compute counts per category. Update with filters.'
        level3: |-
          from fastapi import FastAPI, Query
          from pydantic import BaseModel
          from typing import List, Optional
          import asyncio

          app = FastAPI()

          class SearchRequest(BaseModel):
              query: str
              filters: Optional[Dict] = {}
              page: int = 1
              page_size: int = 10

          class SearchResult(BaseModel):
              id: str
              title: str
              snippet: str
              score: float
              highlights: List[str]

          class SearchResponse(BaseModel):
              results: List[SearchResult]
              total: int
              facets: Dict[str, List[Dict]]
              query_time_ms: float

          @app.post("/search")
          async def search(request: SearchRequest) -> SearchResponse:
              start = time.time()

              # Process query
              query_vector = query_processor.process(request.query)

              # Search with filters
              candidates = await vector_store.search_async(
                  query_vector,
                  k=request.page_size * 10,  # Over-fetch for filtering
                  filters=request.filters
              )

              # Rank
              ranked = ranking_service.rank(request.query, candidates)

              # Paginate
              start_idx = (request.page - 1) * request.page_size
              page_results = ranked[start_idx:start_idx + request.page_size]

              # Build response with highlights
              results = [
                  SearchResult(
                      id=doc['id'],
                      title=doc['title'],
                      snippet=highlight_snippet(doc['text'], request.query),
                      score=doc['final_score'],
                      highlights=find_highlights(doc['text'], request.query)
                  )
                  for doc in page_results
              ]

              # Compute facets
              facets = compute_facets(ranked, request.filters)

              return SearchResponse(
                  results=results,
                  total=len(ranked),
                  facets=facets,
                  query_time_ms=(time.time() - start) * 1000
              )

          @app.get("/autocomplete")
          async def autocomplete(q: str = Query(min_length=2)) -> List[str]:
              # Prefix search in popular queries
              suggestions = await trie.search_prefix(q.lower(), limit=10)
              return suggestions

          def highlight_snippet(text: str, query: str, context_chars: int = 150) -> str:
              query_terms = query.lower().split()
              text_lower = text.lower()

              # Find best matching window
              best_pos = 0
              best_count = 0

              for i in range(0, len(text) - context_chars, 50):
                  window = text_lower[i:i + context_chars]
                  count = sum(1 for term in query_terms if term in window)
                  if count > best_count:
                      best_count = count
                      best_pos = i

              snippet = text[best_pos:best_pos + context_chars]

              # Highlight terms
              for term in query_terms:
                  snippet = re.sub(
                      f'({re.escape(term)})',
                      r'<mark>\1</mark>',
                      snippet,
                      flags=re.IGNORECASE
                  )

              return snippet
      pitfalls:
      - Autocomplete latency > 100ms feels slow
      - Facet counts expensive to compute
      - Highlighting breaks on HTML entities
      - Not tracking null results for improvement
      concepts:
      - Search API design
      - Autocomplete
      - Faceted navigation
      - Result highlighting
      estimated_hours: 8-12
      deliverables:
      - RESTful search endpoint accepting query and filter parameters
      - Result formatting with title, snippet, and relevance score
      - Query term highlighting marking matched words in result snippets
      - Search analytics dashboard tracking query volume and result quality
  ai-agent-framework:
    id: ai-agent-framework
    name: AI Agent Framework
    description: Build a framework for autonomous AI agents that can use tools, plan, and execute multi-step tasks.
    difficulty: advanced
    estimated_hours: 50-80
    prerequisites:
    - LLM APIs
    - Python async
    - System design
    languages:
      recommended:
      - Python
      also_possible:
      - TypeScript
    resources:
    - name: LangChain Agents
      url: https://python.langchain.com/docs/modules/agents/
      type: documentation
    - name: AutoGPT
      url: https://github.com/Significant-Gravitas/AutoGPT
      type: reference
    - name: ReAct Paper
      url: https://arxiv.org/abs/2210.03629
      type: paper
    milestones:
    - id: 1
      name: Tool System
      description: Build extensible tool system for agent capabilities.
      acceptance_criteria:
      - Tool interface defines name, description, JSON-schema parameters, and an execute callback
      - Tool discovery and registration allows adding new tools at runtime without restarting the agent
      - Parameter validation rejects malformed inputs and returns descriptive error messages before execution
      - Error handling catches tool failures, retries transient errors, and surfaces permanent failures to the agent
      - Tool result formatting converts raw output into a structured text format consumable by the LLM
      hints:
        level1: Tools are functions with JSON schema for parameters. LLM chooses which to call.
        level2: Use Pydantic for parameter validation. Return structured results.
        level3: |-
          from abc import ABC, abstractmethod
          from pydantic import BaseModel, Field
          from typing import Any, Dict, Type
          import inspect

          class ToolResult(BaseModel):
              success: bool
              data: Any = None
              error: str = None

          class Tool(ABC):
              name: str
              description: str
              parameters_schema: Type[BaseModel]

              @abstractmethod
              def execute(self, **kwargs) -> ToolResult:
                  pass

              def to_openai_function(self) -> Dict:
                  return {
                      "name": self.name,
                      "description": self.description,
                      "parameters": self.parameters_schema.model_json_schema()
                  }

          class WebSearchParams(BaseModel):
              query: str = Field(description="Search query")
              num_results: int = Field(default=5, description="Number of results")

          class WebSearchTool(Tool):
              name = "web_search"
              description = "Search the web for current information"
              parameters_schema = WebSearchParams

              def __init__(self, api_key: str):
                  self.api_key = api_key

              def execute(self, query: str, num_results: int = 5) -> ToolResult:
                  try:
                      results = search_api.search(query, num_results)
                      return ToolResult(success=True, data=results)
                  except Exception as e:
                      return ToolResult(success=False, error=str(e))

          class CodeExecutionParams(BaseModel):
              code: str = Field(description="Python code to execute")
              timeout: int = Field(default=30, description="Timeout in seconds")

          class CodeExecutionTool(Tool):
              name = "execute_code"
              description = "Execute Python code in a sandboxed environment"
              parameters_schema = CodeExecutionParams

              def execute(self, code: str, timeout: int = 30) -> ToolResult:
                  try:
                      # Run in sandbox (Docker, subprocess, etc.)
                      result = sandbox.run(code, timeout=timeout)
                      return ToolResult(success=True, data={
                          'stdout': result.stdout,
                          'stderr': result.stderr,
                          'return_value': result.return_value
                      })
                  except TimeoutError:
                      return ToolResult(success=False, error="Code execution timed out")
                  except Exception as e:
                      return ToolResult(success=False, error=str(e))

          class ToolRegistry:
              def __init__(self):
                  self.tools: Dict[str, Tool] = {}

              def register(self, tool: Tool):
                  self.tools[tool.name] = tool

              def get(self, name: str) -> Tool:
                  return self.tools.get(name)

              def list_functions(self) -> List[Dict]:
                  return [t.to_openai_function() for t in self.tools.values()]

              def execute(self, name: str, params: Dict) -> ToolResult:
                  tool = self.get(name)
                  if not tool:
                      return ToolResult(success=False, error=f"Unknown tool: {name}")

                  # Validate parameters
                  try:
                      validated = tool.parameters_schema(**params)
                      return tool.execute(**validated.model_dump())
                  except Exception as e:
                      return ToolResult(success=False, error=f"Parameter error: {e}")
      pitfalls:
      - Tool descriptions too vague for LLM to choose correctly
      - Missing parameter validation causes runtime errors
      - Tool execution without timeout hangs forever
      - Sensitive tools without permission checks
      concepts:
      - Tool abstraction
      - JSON Schema
      - Sandboxed execution
      - Registry pattern
      estimated_hours: 8-12
      deliverables:
      - Tool interface definition specifying name, description, parameter schema, and execute method
      - Tool registry that allows dynamic registration, lookup, and discovery of available tools
      - Tool execution engine that validates inputs, runs the tool, and captures structured output
      - Built-in tools including web search, calculator, and sandboxed code execution with result capture
    - id: 2
      name: ReAct Loop (Reasoning + Acting)
      description: 'Implement the core agent loop: think, act, observe, repeat.'
      acceptance_criteria:
      - Thought-Action-Observation cycle repeats until the agent produces a final answer or hits a limit
      - Parser correctly extracts the tool name and JSON arguments from the LLM's action output
      - Action failures are caught and fed back as error observations so the agent can adjust its plan
      - Maximum iteration limit prevents infinite loops and triggers a graceful fallback response
      - Structured output parsing handles both JSON and freeform action formats from the LLM
      hints:
        level1: 'Prompt: ''Think step by step. Use Action: tool_name(params). Wait for Observation.'''
        level2: Use function calling API for reliable action parsing. Fallback to regex.
        level3: |-
          from openai import OpenAI
          from enum import Enum
          from dataclasses import dataclass
          from typing import Optional, List

          class AgentState(Enum):
              THINKING = "thinking"
              ACTING = "acting"
              OBSERVING = "observing"
              FINISHED = "finished"
              ERROR = "error"

          @dataclass
          class AgentStep:
              thought: Optional[str]
              action: Optional[str]
              action_input: Optional[Dict]
              observation: Optional[str]

          class ReActAgent:
              def __init__(self, tools: ToolRegistry, model: str = "gpt-4-turbo-preview"):
                  self.tools = tools
                  self.client = OpenAI()
                  self.model = model
                  self.max_iterations = 10

              def run(self, task: str) -> str:
                  messages = [
                      {"role": "system", "content": self._system_prompt()},
                      {"role": "user", "content": task}
                  ]

                  steps: List[AgentStep] = []

                  for i in range(self.max_iterations):
                      # Get LLM response with function calling
                      response = self.client.chat.completions.create(
                          model=self.model,
                          messages=messages,
                          tools=[{"type": "function", "function": f}
                                 for f in self.tools.list_functions()],
                          tool_choice="auto"
                      )

                      message = response.choices[0].message

                      # Check if finished (no tool calls)
                      if not message.tool_calls:
                          return message.content

                      # Execute tool calls
                      messages.append(message)

                      for tool_call in message.tool_calls:
                          step = AgentStep(
                              thought=message.content,
                              action=tool_call.function.name,
                              action_input=json.loads(tool_call.function.arguments),
                              observation=None
                          )

                          # Execute tool
                          result = self.tools.execute(
                              tool_call.function.name,
                              json.loads(tool_call.function.arguments)
                          )

                          step.observation = str(result.data if result.success else result.error)
                          steps.append(step)

                          # Add observation to messages
                          messages.append({
                              "role": "tool",
                              "tool_call_id": tool_call.id,
                              "content": step.observation
                          })

                  return "Max iterations reached without completing task"

              def _system_prompt(self) -> str:
                  tool_descriptions = "\n".join([
                      f"- {t.name}: {t.description}"
                      for t in self.tools.tools.values()
                  ])

                  return f'''You are a helpful AI assistant that can use tools to accomplish tasks.

          Available tools:
          {tool_descriptions}

          Think step by step about how to accomplish the task. Use tools when needed.
          When you have the final answer, respond without using any tools.'''
      pitfalls:
      - Infinite loops when agent doesn't know when to stop
      - LLM hallucinates non-existent tools
      - Action parsing fails on malformed output
      - Not handling tool timeouts
      concepts:
      - ReAct framework
      - Function calling
      - Agent loops
      - Structured outputs
      estimated_hours: 10-15
      deliverables:
      - Thought-Action-Observation cycle that alternates between LLM reasoning and tool execution
      - LLM prompt template that elicits structured reasoning before each action selection
      - Action parser that extracts tool name and arguments from the LLM's textual output
      - Observation formatter that feeds tool results back into the LLM context for the next reasoning step
    - id: 3
      name: Planning & Task Decomposition
      description: Add planning layer for complex multi-step tasks.
      acceptance_criteria:
      - Agent decomposes a complex multi-step task into a list of concrete, actionable subtasks
      - Execution plan respects the declared ordering and runs subtasks in their dependency sequence
      - Dependencies between steps are tracked so that a blocked step waits for its prerequisite to complete
      - Agent re-plans when a subtask fails, generating an alternative path toward the overall goal
      - Independent subtasks are identified and executed in parallel where possible to reduce latency
      hints:
        level1: First ask LLM to create plan, then execute each step. Simpler than full planning.
        level2: DAG of tasks with dependencies. Topological sort for execution order.
        level3: |-
          from dataclasses import dataclass, field
          from typing import List, Set
          import asyncio
          from enum import Enum

          class TaskStatus(Enum):
              PENDING = "pending"
              RUNNING = "running"
              COMPLETED = "completed"
              FAILED = "failed"

          @dataclass
          class PlanTask:
              id: str
              description: str
              dependencies: Set[str] = field(default_factory=set)
              status: TaskStatus = TaskStatus.PENDING
              result: Any = None
              error: str = None

          class TaskPlanner:
              def __init__(self, llm_client):
                  self.llm = llm_client

              def create_plan(self, objective: str) -> List[PlanTask]:
                  prompt = f'''Create a plan to accomplish this objective:
          {objective}

          Return a JSON list of tasks with:
          - id: unique task identifier
          - description: what to do
          - dependencies: list of task ids this depends on

          Example:
          [
            {{"id": "1", "description": "Search for information about X", "dependencies": []}},
            {{"id": "2", "description": "Analyze the search results", "dependencies": ["1"]}},
            {{"id": "3", "description": "Write summary based on analysis", "dependencies": ["2"]}}
          ]'''

                  response = self.llm.generate(prompt)
                  tasks_data = json.loads(response)

                  return [
                      PlanTask(
                          id=t['id'],
                          description=t['description'],
                          dependencies=set(t.get('dependencies', []))
                      )
                      for t in tasks_data
                  ]

          class PlanExecutor:
              def __init__(self, agent: ReActAgent):
                  self.agent = agent

              async def execute_plan(self, tasks: List[PlanTask]) -> Dict[str, Any]:
                  task_map = {t.id: t for t in tasks}
                  results = {}

                  while not all(t.status in (TaskStatus.COMPLETED, TaskStatus.FAILED) for t in tasks):
                      # Find ready tasks (dependencies met)
                      ready = [
                          t for t in tasks
                          if t.status == TaskStatus.PENDING and
                          all(task_map[dep].status == TaskStatus.COMPLETED
                              for dep in t.dependencies)
                      ]

                      if not ready:
                          # Check for deadlock
                          pending = [t for t in tasks if t.status == TaskStatus.PENDING]
                          if pending:
                              raise RuntimeError("Deadlock detected in task dependencies")
                          break

                      # Execute ready tasks in parallel
                      async def run_task(task: PlanTask):
                          task.status = TaskStatus.RUNNING
                          try:
                              # Include context from dependencies
                              context = {dep: results[dep] for dep in task.dependencies}
                              prompt = f"Previous results: {context}\n\nTask: {task.description}"

                              result = await asyncio.to_thread(self.agent.run, prompt)
                              task.result = result
                              task.status = TaskStatus.COMPLETED
                              results[task.id] = result
                          except Exception as e:
                              task.status = TaskStatus.FAILED
                              task.error = str(e)

                      await asyncio.gather(*[run_task(t) for t in ready])

                  return results

              async def replan_on_failure(self, tasks: List[PlanTask], objective: str) -> List[PlanTask]:
                  failed = [t for t in tasks if t.status == TaskStatus.FAILED]
                  completed = [t for t in tasks if t.status == TaskStatus.COMPLETED]

                  prompt = f'''The original plan failed. Here's what happened:

          Completed tasks: {[(t.id, t.description, t.result) for t in completed]}
          Failed tasks: {[(t.id, t.description, t.error) for t in failed]}

          Create a new plan to accomplish: {objective}
          Consider what already succeeded and avoid the failures.'''

                  return self.planner.create_plan(prompt)
      pitfalls:
      - Circular dependencies in task graph
      - Over-decomposition creates too many steps
      - Context lost between subtasks
      - Replanning loops infinitely
      concepts:
      - Task decomposition
      - DAG execution
      - Async parallel execution
      - Replanning
      estimated_hours: 12-18
      deliverables:
      - Task decomposition prompting that instructs the LLM to break complex goals into ordered subtasks
      - Sub-task tracking data structure that records status, dependencies, and results for each subtask
      - Plan execution engine that runs subtasks in order, re-planning when a step fails or context changes
      - Goal completion detection that evaluates whether all subtasks are satisfied and the overall objective is met
    - id: 4
      name: Memory & Context Management
      description: Give agents short-term and long-term memory.
      acceptance_criteria:
      - Short-term conversation history is maintained in order and accessible for the current task session
      - Working memory holds the current task's intermediate state, goals, and partial results
      - Long-term vector store indexes past interactions and retrieves the top-k most relevant entries by similarity
      - Memory retrieval injects relevant context into the LLM prompt without exceeding the token budget
      - Memory summarization compresses older conversation turns into concise summaries to free context space
      hints:
        level1: 'Short-term: sliding window of last N messages. Long-term: embed and store in vector DB.'
        level2: Summarize old messages to save context. Retrieve relevant memories for current task.
        level3: |-
          from datetime import datetime
          from typing import List, Optional
          import numpy as np

          @dataclass
          class Memory:
              content: str
              type: str  # 'conversation', 'task_result', 'fact', 'reflection'
              timestamp: datetime
              importance: float = 0.5
              embedding: Optional[np.ndarray] = None

          class AgentMemory:
              def __init__(self, embedding_service, vector_store, llm):
                  self.embedder = embedding_service
                  self.store = vector_store
                  self.llm = llm
                  self.working_memory: List[Memory] = []
                  self.max_working_memory = 10

              def add(self, content: str, memory_type: str = 'conversation', importance: float = 0.5):
                  embedding = self.embedder.embed([content])[0]

                  memory = Memory(
                      content=content,
                      type=memory_type,
                      timestamp=datetime.now(),
                      importance=importance,
                      embedding=embedding
                  )

                  # Add to working memory
                  self.working_memory.append(memory)
                  if len(self.working_memory) > self.max_working_memory:
                      # Move oldest to long-term
                      old = self.working_memory.pop(0)
                      self._store_long_term(old)

                  # High importance goes straight to long-term
                  if importance > 0.8:
                      self._store_long_term(memory)

              def _store_long_term(self, memory: Memory):
                  self.store.add(
                      ids=[f"mem_{memory.timestamp.isoformat()}"],
                      embeddings=[memory.embedding],
                      metadatas=[{
                          'content': memory.content,
                          'type': memory.type,
                          'timestamp': memory.timestamp.isoformat(),
                          'importance': memory.importance
                      }]
                  )

              def retrieve(self, query: str, k: int = 5) -> List[Memory]:
                  query_embedding = self.embedder.embed([query])[0]
                  results = self.store.search(query_embedding, query, k=k)

                  return [
                      Memory(
                          content=r['metadata']['content'],
                          type=r['metadata']['type'],
                          timestamp=datetime.fromisoformat(r['metadata']['timestamp']),
                          importance=r['metadata']['importance']
                      )
                      for r in results
                  ]

              def get_context(self, current_task: str, max_tokens: int = 2000) -> str:
                  # Recent working memory
                  recent = [m.content for m in self.working_memory[-5:]]

                  # Relevant long-term memories
                  relevant = self.retrieve(current_task, k=10)
                  relevant_content = [m.content for m in relevant]

                  context = f'''RECENT CONTEXT:
          {chr(10).join(recent)}

          RELEVANT MEMORIES:
          {chr(10).join(relevant_content)}'''

                  # Summarize if too long
                  if len(context) > max_tokens * 4:  # Rough char estimate
                      context = self._summarize(context, max_tokens)

                  return context

              def _summarize(self, content: str, max_tokens: int) -> str:
                  return self.llm.generate(
                      f"Summarize the following context in about {max_tokens} tokens, "
                      f"preserving the most important information:\n\n{content}"
                  )

              def reflect(self):
                  '''Generate insights from recent memories'''
                  recent = [m.content for m in self.working_memory]

                  reflection = self.llm.generate(
                      f"Based on these recent events, what are key insights or patterns?\n\n"
                      f"{chr(10).join(recent)}"
                  )

                  self.add(reflection, memory_type='reflection', importance=0.9)
                  return reflection
      pitfalls:
      - Memory grows unbounded without cleanup
      - Retrieved memories not relevant to task
      - Summarization loses critical details
      - Importance scoring is subjective
      concepts:
      - Working vs long-term memory
      - Memory retrieval
      - Context compression
      - Reflection
      estimated_hours: 10-15
      deliverables:
      - Conversation memory storage that persists the full dialogue history for the current session
      - Long-term memory backed by a vector store for semantic retrieval of past interactions and facts
      - Context window management that truncates or summarizes history to fit within the LLM token limit
      - Memory retrieval module that fetches relevant past context to augment the current LLM prompt
    - id: 5
      name: Multi-Agent Collaboration
      description: Build systems where multiple agents work together.
      acceptance_criteria:
      - Each agent has a defined role and advertised set of capabilities used for task routing
      - Message passing between agents supports request-response and broadcast communication patterns
      - A coordinator or orchestrator agent distributes work and monitors progress across subordinate agents
      - Shared context and state are accessible to all agents working on the same task without conflicts
      - Conflict resolution handles contradictory outputs from agents by applying a defined merge strategy
      hints:
        level1: 'Start with simple delegation: main agent calls specialist agents as tools.'
        level2: 'Pub/sub messaging: agents subscribe to topics, broadcast results.'
        level3: |-
          from abc import ABC
          from typing import Dict, List, Any
          import asyncio
          from dataclasses import dataclass

          @dataclass
          class AgentMessage:
              from_agent: str
              to_agent: str  # or 'broadcast'
              content: str
              metadata: Dict = None

          class SpecialistAgent:
              def __init__(self, name: str, role: str, tools: ToolRegistry):
                  self.name = name
                  self.role = role
                  self.tools = tools
                  self.inbox: asyncio.Queue = asyncio.Queue()

              async def process_message(self, message: AgentMessage) -> AgentMessage:
                  # Add role context
                  prompt = f"You are a {self.role} specialist.\n\nTask: {message.content}"

                  result = self.react_agent.run(prompt)

                  return AgentMessage(
                      from_agent=self.name,
                      to_agent=message.from_agent,
                      content=result
                  )

          class OrchestratorAgent:
              def __init__(self, specialists: Dict[str, SpecialistAgent]):
                  self.specialists = specialists
                  self.message_bus: asyncio.Queue = asyncio.Queue()

              async def run(self, task: str) -> str:
                  # Plan which specialists to involve
                  plan = self._create_delegation_plan(task)

                  results = {}

                  for step in plan:
                      if step['type'] == 'delegate':
                          specialist = self.specialists[step['agent']]

                          message = AgentMessage(
                              from_agent='orchestrator',
                              to_agent=step['agent'],
                              content=step['task']
                          )

                          response = await specialist.process_message(message)
                          results[step['agent']] = response.content

                      elif step['type'] == 'parallel_delegate':
                          tasks = [
                              self.specialists[agent].process_message(
                                  AgentMessage('orchestrator', agent, subtask)
                              )
                              for agent, subtask in step['assignments'].items()
                          ]
                          responses = await asyncio.gather(*tasks)
                          for resp in responses:
                              results[resp.from_agent] = resp.content

                      elif step['type'] == 'synthesize':
                          # Combine results from specialists
                          synthesis_prompt = f'''Combine these specialist results:
          {json.dumps(results, indent=2)}

          Original task: {task}'''

                          return self.llm.generate(synthesis_prompt)

                  return str(results)

              def _create_delegation_plan(self, task: str) -> List[Dict]:
                  specialist_info = {
                      name: agent.role
                      for name, agent in self.specialists.items()
                  }

                  prompt = f'''You are an orchestrator with these specialists:
          {json.dumps(specialist_info, indent=2)}

          Create a delegation plan for: {task}

          Return JSON list of steps:
          {{"type": "delegate", "agent": "agent_name", "task": "specific task"}}
          {{"type": "parallel_delegate", "assignments": {{"agent1": "task1", "agent2": "task2"}}}}
          {{"type": "synthesize"}}'''

                  return json.loads(self.llm.generate(prompt))

          # Example usage
          specialists = {
              'researcher': SpecialistAgent('researcher', 'research and information gathering', research_tools),
              'coder': SpecialistAgent('coder', 'writing and analyzing code', coding_tools),
              'writer': SpecialistAgent('writer', 'writing and editing text', writing_tools)
          }

          orchestrator = OrchestratorAgent(specialists)
          result = await orchestrator.run("Research best practices for API design and create a style guide")
      pitfalls:
      - Agents talk past each other without shared context
      - Delegation loops (A delegates to B delegates to A)
      - Orchestrator becomes bottleneck
      - No timeout for unresponsive agents
      concepts:
      - Multi-agent systems
      - Message passing
      - Orchestration patterns
      - Role specialization
      estimated_hours: 15-20
      deliverables:
      - Agent-to-agent communication protocol enabling structured message passing between agents
      - Role assignment module that assigns specialized roles and capabilities to each agent instance
      - Task delegation mechanism that routes subtasks to the most capable agent based on role matching
      - Result aggregation layer that collects outputs from multiple agents and merges them into a coherent response
  llm-eval-framework:
    id: llm-eval-framework
    name: LLM Evaluation Framework
    description: Build comprehensive evaluation system for LLM applications. Critical for production AI.
    difficulty: advanced
    estimated_hours: 40-60
    prerequisites:
    - LLM APIs
    - Statistics
    - Python
    languages:
      recommended:
      - Python
      also_possible:
      - TypeScript
    resources:
    - name: Anthropic Eval Best Practices
      url: https://docs.anthropic.com/en/docs/build-with-claude/develop-tests
      type: documentation
    - name: LangSmith
      url: https://docs.smith.langchain.com/
      type: documentation
    - name: Braintrust
      url: https://www.braintrustdata.com/docs
      type: documentation
    milestones:
    - id: 1
      name: Dataset Management
      description: Create and version evaluation datasets.
      acceptance_criteria:
      - Test case schema validates that each case has a prompt, expected output, and at least one tag
      - Datasets are versioned with git-like tracking so changes can be diffed and rolled back
      - Import from CSV and JSON files maps columns to the test case schema automatically
      - Dataset splits into train, test, and validation subsets with configurable split ratios
      - Golden examples are marked as high-quality reference cases for calibrating LLM-as-judge evaluators
      hints:
        level1: Test case = input + expected output + metadata. Store as JSON lines.
        level2: Hash dataset content for versioning. Track lineage of derived datasets.
        level3: |-
          from dataclasses import dataclass, asdict
          from typing import List, Optional, Dict, Any
          import hashlib
          import json
          from pathlib import Path
          from datetime import datetime

          @dataclass
          class TestCase:
              id: str
              input: str
              expected_output: Optional[str] = None
              context: Optional[Dict] = None
              tags: List[str] = None
              metadata: Dict = None

              def to_dict(self):
                  return asdict(self)

          @dataclass
          class Dataset:
              name: str
              version: str
              cases: List[TestCase]
              created_at: datetime
              parent_version: Optional[str] = None

              @property
              def hash(self) -> str:
                  content = json.dumps([c.to_dict() for c in self.cases], sort_keys=True)
                  return hashlib.sha256(content.encode()).hexdigest()[:12]

          class DatasetManager:
              def __init__(self, storage_path: str = ".evals/datasets"):
                  self.storage = Path(storage_path)
                  self.storage.mkdir(parents=True, exist_ok=True)

              def create(self, name: str, cases: List[TestCase]) -> Dataset:
                  dataset = Dataset(
                      name=name,
                      version=f"v1_{datetime.now().strftime('%Y%m%d')}",
                      cases=cases,
                      created_at=datetime.now()
                  )
                  self._save(dataset)
                  return dataset

              def add_cases(self, dataset: Dataset, new_cases: List[TestCase]) -> Dataset:
                  new_dataset = Dataset(
                      name=dataset.name,
                      version=f"v{len(self._get_versions(dataset.name)) + 1}_{datetime.now().strftime('%Y%m%d')}",
                      cases=dataset.cases + new_cases,
                      created_at=datetime.now(),
                      parent_version=dataset.version
                  )
                  self._save(new_dataset)
                  return new_dataset

              def _save(self, dataset: Dataset):
                  path = self.storage / dataset.name / f"{dataset.version}.jsonl"
                  path.parent.mkdir(exist_ok=True)

                  with open(path, 'w') as f:
                      for case in dataset.cases:
                          f.write(json.dumps(case.to_dict()) + '\n')

                  # Save metadata
                  meta_path = self.storage / dataset.name / f"{dataset.version}.meta.json"
                  with open(meta_path, 'w') as f:
                      json.dump({
                          'name': dataset.name,
                          'version': dataset.version,
                          'hash': dataset.hash,
                          'case_count': len(dataset.cases),
                          'created_at': dataset.created_at.isoformat(),
                          'parent_version': dataset.parent_version
                      }, f, indent=2)

              def load(self, name: str, version: str = None) -> Dataset:
                  if version is None:
                      version = self._get_latest_version(name)

                  path = self.storage / name / f"{version}.jsonl"
                  cases = []
                  with open(path) as f:
                      for line in f:
                          cases.append(TestCase(**json.loads(line)))

                  meta_path = self.storage / name / f"{version}.meta.json"
                  meta = json.loads(meta_path.read_text())

                  return Dataset(
                      name=name,
                      version=version,
                      cases=cases,
                      created_at=datetime.fromisoformat(meta['created_at']),
                      parent_version=meta.get('parent_version')
                  )

              def split(self, dataset: Dataset, train_ratio=0.8, seed=42) -> tuple:
                  import random
                  random.seed(seed)
                  cases = dataset.cases.copy()
                  random.shuffle(cases)

                  split_idx = int(len(cases) * train_ratio)
                  train_cases = cases[:split_idx]
                  test_cases = cases[split_idx:]

                  return (
                      Dataset(f"{dataset.name}_train", dataset.version, train_cases, datetime.now()),
                      Dataset(f"{dataset.name}_test", dataset.version, test_cases, datetime.now())
                  )
      pitfalls:
      - Test set leaking into training data
      - Version conflicts with concurrent edits
      - Large datasets slow to load
      - Not tracking dataset provenance
      concepts:
      - Dataset versioning
      - Train/test splits
      - Golden examples
      - Data lineage
      estimated_hours: 6-10
      deliverables:
      - Dataset loader supporting CSV, JSON, and JSONL input formats with schema validation
      - Test case structure defining input prompt, expected output, metadata tags, and difficulty level
      - Ground truth storage that associates each test case with its reference answer and scoring rubric
      - Dataset versioning system that tracks changes to test cases over time with diff history
    - id: 2
      name: Evaluation Metrics
      description: Implement metrics for different evaluation types.
      acceptance_criteria:
      - Exact match and fuzzy match modes handle whitespace normalization and case-insensitive comparison
      - Semantic similarity computes cosine similarity between embeddings of the output and reference text
      - LLM-as-judge grading sends output and reference to a judge model and parses its score
      - Custom metric functions are registered as plugins and called automatically during evaluation
      - Aggregate scoring computes mean, median, and percentile scores across the full evaluation dataset
      hints:
        level1: Start with exact match. Add fuzzy match (Levenshtein). Then semantic similarity.
        level2: 'LLM-as-judge: ask GPT-4 to rate on criteria. Parse structured score.'
        level3: |-
          from abc import ABC, abstractmethod
          from typing import Any, Dict
          import numpy as np
          from difflib import SequenceMatcher

          class Metric(ABC):
              name: str

              @abstractmethod
              def score(self, expected: Any, actual: Any, context: Dict = None) -> float:
                  '''Return score between 0 and 1'''
                  pass

          class ExactMatch(Metric):
              name = "exact_match"

              def __init__(self, case_sensitive: bool = False, strip: bool = True):
                  self.case_sensitive = case_sensitive
                  self.strip = strip

              def score(self, expected: str, actual: str, context: Dict = None) -> float:
                  if self.strip:
                      expected, actual = expected.strip(), actual.strip()
                  if not self.case_sensitive:
                      expected, actual = expected.lower(), actual.lower()
                  return 1.0 if expected == actual else 0.0

          class FuzzyMatch(Metric):
              name = "fuzzy_match"

              def __init__(self, threshold: float = 0.8):
                  self.threshold = threshold

              def score(self, expected: str, actual: str, context: Dict = None) -> float:
                  ratio = SequenceMatcher(None, expected.lower(), actual.lower()).ratio()
                  return ratio

          class SemanticSimilarity(Metric):
              name = "semantic_similarity"

              def __init__(self, embedding_service):
                  self.embedder = embedding_service

              def score(self, expected: str, actual: str, context: Dict = None) -> float:
                  embeddings = self.embedder.embed([expected, actual])
                  similarity = np.dot(embeddings[0], embeddings[1])
                  return float(similarity)

          class LLMJudge(Metric):
              name = "llm_judge"

              def __init__(self, llm_client, criteria: str, rubric: Dict[int, str] = None):
                  self.llm = llm_client
                  self.criteria = criteria
                  self.rubric = rubric or {
                      5: "Excellent - fully meets criteria",
                      4: "Good - mostly meets criteria with minor issues",
                      3: "Acceptable - meets basic criteria",
                      2: "Poor - partially meets criteria",
                      1: "Unacceptable - does not meet criteria"
                  }

              def score(self, expected: str, actual: str, context: Dict = None) -> float:
                  rubric_text = "\n".join([f"{k}: {v}" for k, v in self.rubric.items()])

                  prompt = f'''Evaluate the following response based on this criteria:
          {self.criteria}

          Expected answer: {expected}
          Actual response: {actual}

          Scoring rubric:
          {rubric_text}

          Provide your score (1-5) and brief justification.
          Format: SCORE: [number]
          REASON: [explanation]'''

                  response = self.llm.generate(prompt)

                  # Parse score
                  import re
                  match = re.search(r'SCORE:\s*(\d)', response)
                  if match:
                      score = int(match.group(1))
                      return score / 5.0
                  return 0.5  # Default if parsing fails

          class MultiCriteriaJudge(Metric):
              name = "multi_criteria"

              def __init__(self, llm_client, criteria: List[Dict]):
                  self.llm = llm_client
                  self.criteria = criteria  # [{'name': 'accuracy', 'weight': 0.5, 'description': '...'}]

              def score(self, expected: str, actual: str, context: Dict = None) -> float:
                  scores = {}

                  for criterion in self.criteria:
                      judge = LLMJudge(self.llm, criterion['description'])
                      scores[criterion['name']] = judge.score(expected, actual, context)

                  # Weighted average
                  total_weight = sum(c['weight'] for c in self.criteria)
                  weighted_sum = sum(
                      scores[c['name']] * c['weight']
                      for c in self.criteria
                  )

                  return weighted_sum / total_weight
      pitfalls:
      - Semantic similarity not calibrated (what's 'good' score?)
      - LLM judge inconsistent across runs
      - Custom metrics not between 0-1
      - Not handling edge cases (empty strings, None)
      concepts:
      - Evaluation metrics
      - Fuzzy matching
      - Semantic similarity
      - LLM-as-judge
      estimated_hours: 8-12
      deliverables:
      - Exact match scorer that compares model output character-by-character against the reference answer
      - BLEU and ROUGE score calculators that measure n-gram overlap with reference text
      - Semantic similarity scorer using embedding cosine distance to compare meaning rather than surface text
      - Custom metric plugin system that allows users to register their own scoring functions
    - id: 3
      name: Evaluation Runner
      description: Run evaluations efficiently with caching and parallelism.
      acceptance_criteria:
      - Batch evaluation processes all test cases and records each model response with its score
      - Parallel LLM calls execute up to a configurable concurrency limit to maximize throughput
      - Result caching stores responses so re-running the same prompt skips the API call and uses cached output
      - Progress tracking displays a live counter of completed, pending, and failed evaluations
      - Evaluation resumes from the last successful test case after a crash or API failure
      hints:
        level1: asyncio.gather for parallel LLM calls. Cache results by (input_hash, model, prompt_version).
        level2: Checkpoint results periodically. Resume by loading checkpoint and skipping completed.
        level3: |-
          import asyncio
          from dataclasses import dataclass, field
          from typing import Callable, List, Dict
          import hashlib
          import json
          from tqdm.asyncio import tqdm

          @dataclass
          class EvalResult:
              case_id: str
              input: str
              expected: str
              actual: str
              scores: Dict[str, float]
              latency_ms: float
              error: str = None

          @dataclass
          class EvalRun:
              id: str
              dataset_name: str
              model: str
              timestamp: datetime
              results: List[EvalResult] = field(default_factory=list)
              aggregate_scores: Dict[str, float] = None

          class EvalRunner:
              def __init__(self,
                           model_fn: Callable[[str], str],
                           metrics: List[Metric],
                           cache_dir: str = ".evals/cache",
                           max_parallel: int = 10):
                  self.model_fn = model_fn
                  self.metrics = metrics
                  self.cache_dir = Path(cache_dir)
                  self.cache_dir.mkdir(parents=True, exist_ok=True)
                  self.semaphore = asyncio.Semaphore(max_parallel)

              def _cache_key(self, input_text: str, model_id: str) -> str:
                  content = f"{model_id}:{input_text}"
                  return hashlib.sha256(content.encode()).hexdigest()

              def _get_cached(self, cache_key: str) -> Optional[str]:
                  cache_path = self.cache_dir / f"{cache_key}.json"
                  if cache_path.exists():
                      return json.loads(cache_path.read_text())['output']
                  return None

              def _set_cached(self, cache_key: str, output: str):
                  cache_path = self.cache_dir / f"{cache_key}.json"
                  cache_path.write_text(json.dumps({'output': output}))

              async def _evaluate_case(self, case: TestCase, model_id: str) -> EvalResult:
                  async with self.semaphore:
                      cache_key = self._cache_key(case.input, model_id)
                      cached = self._get_cached(cache_key)

                      start = time.time()

                      if cached:
                          actual = cached
                      else:
                          try:
                              actual = await asyncio.to_thread(self.model_fn, case.input)
                              self._set_cached(cache_key, actual)
                          except Exception as e:
                              return EvalResult(
                                  case_id=case.id,
                                  input=case.input,
                                  expected=case.expected_output,
                                  actual=None,
                                  scores={},
                                  latency_ms=(time.time() - start) * 1000,
                                  error=str(e)
                              )

                      latency = (time.time() - start) * 1000

                      # Calculate scores
                      scores = {}
                      for metric in self.metrics:
                          try:
                              scores[metric.name] = metric.score(
                                  case.expected_output, actual, case.context
                              )
                          except Exception as e:
                              scores[metric.name] = None

                      return EvalResult(
                          case_id=case.id,
                          input=case.input,
                          expected=case.expected_output,
                          actual=actual,
                          scores=scores,
                          latency_ms=latency
                      )

              async def run(self, dataset: Dataset, model_id: str,
                            checkpoint_every: int = 50) -> EvalRun:
                  run = EvalRun(
                      id=f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                      dataset_name=dataset.name,
                      model=model_id,
                      timestamp=datetime.now()
                  )

                  # Check for existing checkpoint
                  checkpoint_path = self.cache_dir / f"{run.id}_checkpoint.json"
                  completed_ids = set()
                  if checkpoint_path.exists():
                      checkpoint = json.loads(checkpoint_path.read_text())
                      run.results = [EvalResult(**r) for r in checkpoint['results']]
                      completed_ids = {r.case_id for r in run.results}

                  # Filter to incomplete cases
                  pending = [c for c in dataset.cases if c.id not in completed_ids]

                  # Run with progress bar
                  tasks = [self._evaluate_case(c, model_id) for c in pending]

                  for i, result in enumerate(await tqdm.gather(*tasks)):
                      run.results.append(result)

                      # Checkpoint periodically
                      if (i + 1) % checkpoint_every == 0:
                          self._save_checkpoint(run, checkpoint_path)

                  # Calculate aggregates
                  run.aggregate_scores = self._aggregate(run.results)

                  return run

              def _aggregate(self, results: List[EvalResult]) -> Dict[str, float]:
                  aggregates = {}

                  for metric in self.metrics:
                      scores = [r.scores.get(metric.name) for r in results
                               if r.scores.get(metric.name) is not None]
                      if scores:
                          aggregates[f"{metric.name}_mean"] = np.mean(scores)
                          aggregates[f"{metric.name}_std"] = np.std(scores)
                          aggregates[f"{metric.name}_p50"] = np.percentile(scores, 50)
                          aggregates[f"{metric.name}_p95"] = np.percentile(scores, 95)

                  return aggregates
      pitfalls:
      - Rate limiting without backoff crashes run
      - Cache invalidation when prompt changes
      - Memory issues with large result sets
      - Lost progress on crash without checkpoints
      concepts:
      - Async evaluation
      - Result caching
      - Checkpointing
      - Progress tracking
      estimated_hours: 10-15
      deliverables:
      - Model API integration layer that supports OpenAI, Anthropic, and local model inference backends
      - Batch evaluation engine that processes all test cases in configurable batch sizes
      - Progress tracker that displays completion percentage, estimated time remaining, and error counts
      - Result cache that stores model responses keyed by prompt hash to avoid redundant API calls
    - id: 4
      name: Reporting & Analysis
      description: Generate insights from evaluation results.
      acceptance_criteria:
      - Score breakdown groups results by tags and categories showing per-group mean and variance
      - Regression detection flags metrics that have degraded compared to a stored baseline evaluation
      - Failure analysis identifies the most common error patterns and groups them by similarity
      - Reports are exported as self-contained HTML or PDF files with charts and tables
      - CI/CD integration returns a pass or fail exit code based on configurable score thresholds
      hints:
        level1: Group results by tags. Calculate per-group metrics. Flag if below threshold.
        level2: 'Compare runs: significant difference = > 2 std devs. Use bootstrap for confidence.'
        level3: |-
          import pandas as pd
          from scipy import stats
          import matplotlib.pyplot as plt
          from jinja2 import Template

          class EvalAnalyzer:
              def __init__(self):
                  self.significance_threshold = 0.05

              def to_dataframe(self, run: EvalRun) -> pd.DataFrame:
                  rows = []
                  for result in run.results:
                      row = {
                          'case_id': result.case_id,
                          'latency_ms': result.latency_ms,
                          'error': result.error is not None,
                          **result.scores
                      }
                      rows.append(row)
                  return pd.DataFrame(rows)

              def breakdown_by_tag(self, run: EvalRun, dataset: Dataset) -> Dict:
                  df = self.to_dataframe(run)

                  # Add tags from dataset
                  case_tags = {c.id: c.tags or [] for c in dataset.cases}

                  breakdowns = {}
                  all_tags = set()
                  for tags in case_tags.values():
                      all_tags.update(tags)

                  for tag in all_tags:
                      tag_cases = [cid for cid, tags in case_tags.items() if tag in tags]
                      tag_df = df[df['case_id'].isin(tag_cases)]

                      breakdowns[tag] = {
                          'count': len(tag_df),
                          'metrics': {
                              col: tag_df[col].mean()
                              for col in df.columns
                              if col not in ('case_id', 'latency_ms', 'error')
                          }
                      }

                  return breakdowns

              def compare_runs(self, baseline: EvalRun, current: EvalRun) -> Dict:
                  baseline_df = self.to_dataframe(baseline)
                  current_df = self.to_dataframe(current)

                  comparisons = {}
                  metric_cols = [c for c in baseline_df.columns
                                if c not in ('case_id', 'latency_ms', 'error')]

                  for metric in metric_cols:
                      baseline_scores = baseline_df[metric].dropna()
                      current_scores = current_df[metric].dropna()

                      # Statistical test
                      t_stat, p_value = stats.ttest_ind(baseline_scores, current_scores)

                      delta = current_scores.mean() - baseline_scores.mean()
                      delta_pct = (delta / baseline_scores.mean()) * 100 if baseline_scores.mean() != 0 else 0

                      comparisons[metric] = {
                          'baseline_mean': baseline_scores.mean(),
                          'current_mean': current_scores.mean(),
                          'delta': delta,
                          'delta_pct': delta_pct,
                          'p_value': p_value,
                          'significant': p_value < self.significance_threshold,
                          'regression': delta < 0 and p_value < self.significance_threshold
                      }

                  return comparisons

              def failure_analysis(self, run: EvalRun, threshold: float = 0.5) -> Dict:
                  failures = [r for r in run.results
                             if any(s < threshold for s in r.scores.values() if s is not None)]

                  # Cluster by input patterns (simple: first 50 chars)
                  patterns = {}
                  for f in failures:
                      pattern = f.input[:50]
                      if pattern not in patterns:
                          patterns[pattern] = []
                      patterns[pattern].append(f)

                  # Sort by frequency
                  sorted_patterns = sorted(patterns.items(), key=lambda x: len(x[1]), reverse=True)

                  return {
                      'total_failures': len(failures),
                      'failure_rate': len(failures) / len(run.results),
                      'top_patterns': sorted_patterns[:10]
                  }

              def generate_report(self, run: EvalRun, baseline: EvalRun = None) -> str:
                  template = Template('''
          <!DOCTYPE html>
          <html>
          <head>
              <title>Eval Report: {{ run.id }}</title>
              <style>
                  body { font-family: sans-serif; margin: 40px; }
                  .metric { padding: 10px; margin: 10px; border: 1px solid #ccc; }
                  .regression { background: #ffcccc; }
                  .improvement { background: #ccffcc; }
              </style>
          </head>
          <body>
              <h1>Evaluation Report</h1>
              <p>Run ID: {{ run.id }}</p>
              <p>Dataset: {{ run.dataset_name }}</p>
              <p>Model: {{ run.model }}</p>

              <h2>Aggregate Scores</h2>
              {% for metric, value in run.aggregate_scores.items() %}
              <div class="metric">{{ metric }}: {{ "%.3f"|format(value) }}</div>
              {% endfor %}

              {% if comparison %}
              <h2>Comparison to Baseline</h2>
              {% for metric, comp in comparison.items() %}
              <div class="metric {{ 'regression' if comp.regression else 'improvement' if comp.delta > 0 else '' }}">
                  {{ metric }}: {{ "%.3f"|format(comp.current_mean) }}
                  ({{ "%+.1f"|format(comp.delta_pct) }}% vs baseline)
                  {% if comp.significant %}*{% endif %}
              </div>
              {% endfor %}
              {% endif %}
          </body>
          </html>
                  ''')

                  comparison = self.compare_runs(baseline, run) if baseline else None

                  return template.render(run=run, comparison=comparison)
      pitfalls:
      - Small sample sizes give unreliable comparisons
      - Multiple comparisons without correction
      - Ignoring variance in aggregate scores
      - Report generation fails silently
      concepts:
      - Statistical testing
      - Regression detection
      - Failure analysis
      - Reporting
      estimated_hours: 8-12
      deliverables:
      - Score aggregation module that computes per-tag, per-category, and overall evaluation summaries
      - Error analysis tool that clusters common failure patterns and highlights weak areas
      - Comparison report generator that shows side-by-side metric diffs between two evaluation runs
      - Visualization dashboard that renders score distributions, tag breakdowns, and trend charts
  recommendation-engine:
    id: recommendation-engine
    name: Recommendation Engine
    description: Build a production recommendation system with collaborative filtering, content-based, and hybrid approaches.
    difficulty: intermediate
    estimated_hours: 35-50
    prerequisites:
    - Python
    - Linear algebra basics
    - Database fundamentals
    languages:
      recommended:
      - Python
      also_possible:
      - Go
      - Scala
    resources:
    - name: Surprise Library
      url: https://surpriselib.com/
      type: documentation
    - name: Netflix Recommendation Paper
      url: https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-BellKor-Solution.pdf
      type: paper
    milestones:
    - id: 1
      name: Collaborative Filtering
      description: Implement user-based and item-based collaborative filtering.
      acceptance_criteria:
      - Build user-item rating matrix from interaction history data
      - Calculate user-to-user similarity using cosine or Pearson metric
      - Calculate item-to-item similarity from co-rating patterns
      - Predict ratings using K-nearest neighbors weighted average method
      - Handle cold start problem for new users and new items
      hints:
        level1: Cosine similarity between user rating vectors. Predict = weighted average of neighbors.
        level2: Item-based is more stable than user-based (items change less than users).
        level3: |-
          import numpy as np
          from scipy.sparse import csr_matrix
          from sklearn.metrics.pairwise import cosine_similarity

          class CollaborativeFilter:
              def __init__(self, k_neighbors: int = 20):
                  self.k = k_neighbors
                  self.user_matrix = None
                  self.item_matrix = None
                  self.user_similarity = None
                  self.item_similarity = None

              def fit(self, ratings: List[tuple]):  # [(user_id, item_id, rating), ...]
                  # Build sparse matrix
                  users = list(set(r[0] for r in ratings))
                  items = list(set(r[1] for r in ratings))

                  self.user_idx = {u: i for i, u in enumerate(users)}
                  self.item_idx = {it: i for i, it in enumerate(items)}
                  self.idx_user = {i: u for u, i in self.user_idx.items()}
                  self.idx_item = {i: it for it, i in self.item_idx.items()}

                  # User-item matrix
                  rows, cols, data = [], [], []
                  for user, item, rating in ratings:
                      rows.append(self.user_idx[user])
                      cols.append(self.item_idx[item])
                      data.append(rating)

                  self.user_matrix = csr_matrix(
                      (data, (rows, cols)),
                      shape=(len(users), len(items))
                  )
                  self.item_matrix = self.user_matrix.T

                  # Precompute similarities
                  self.user_similarity = cosine_similarity(self.user_matrix)
                  self.item_similarity = cosine_similarity(self.item_matrix)

              def predict_user_based(self, user_id, item_id) -> float:
                  if user_id not in self.user_idx or item_id not in self.item_idx:
                      return self._global_mean()

                  u_idx = self.user_idx[user_id]
                  i_idx = self.item_idx[item_id]

                  # Find users who rated this item
                  item_raters = self.user_matrix[:, i_idx].nonzero()[0]

                  if len(item_raters) == 0:
                      return self._user_mean(u_idx)

                  # Get similarities and ratings
                  similarities = self.user_similarity[u_idx, item_raters]
                  ratings = np.array(self.user_matrix[item_raters, i_idx].todense()).flatten()

                  # Top-k neighbors
                  top_k = np.argsort(similarities)[-self.k:]

                  sim_sum = np.sum(np.abs(similarities[top_k]))
                  if sim_sum == 0:
                      return self._user_mean(u_idx)

                  weighted_sum = np.sum(similarities[top_k] * ratings[top_k])
                  return weighted_sum / sim_sum

              def predict_item_based(self, user_id, item_id) -> float:
                  if user_id not in self.user_idx or item_id not in self.item_idx:
                      return self._global_mean()

                  u_idx = self.user_idx[user_id]
                  i_idx = self.item_idx[item_id]

                  # Find items rated by this user
                  user_items = self.user_matrix[u_idx, :].nonzero()[1]

                  if len(user_items) == 0:
                      return self._item_mean(i_idx)

                  # Get similarities and ratings
                  similarities = self.item_similarity[i_idx, user_items]
                  ratings = np.array(self.user_matrix[u_idx, user_items].todense()).flatten()

                  # Top-k similar items
                  top_k = np.argsort(similarities)[-self.k:]

                  sim_sum = np.sum(np.abs(similarities[top_k]))
                  if sim_sum == 0:
                      return self._item_mean(i_idx)

                  weighted_sum = np.sum(similarities[top_k] * ratings[top_k])
                  return weighted_sum / sim_sum

              def recommend(self, user_id, n: int = 10) -> List[tuple]:
                  if user_id not in self.user_idx:
                      return self._popular_items(n)

                  u_idx = self.user_idx[user_id]
                  rated_items = set(self.user_matrix[u_idx, :].nonzero()[1])

                  predictions = []
                  for i_idx in range(self.user_matrix.shape[1]):
                      if i_idx not in rated_items:
                          item_id = self.idx_item[i_idx]
                          pred = self.predict_item_based(user_id, item_id)
                          predictions.append((item_id, pred))

                  predictions.sort(key=lambda x: x[1], reverse=True)
                  return predictions[:n]
      pitfalls:
      - Sparse matrix operations are memory-intensive
      - Precomputing all similarities doesn't scale
      - 'Cold start: new users/items have no data'
      - Popularity bias in recommendations
      concepts:
      - Collaborative filtering
      - Similarity metrics
      - K-NN
      - Sparse matrices
      estimated_hours: 8-12
      deliverables:
      - User-item interaction matrix built from rating data
      - User similarity computation using cosine or Pearson correlation
      - Item similarity computation using co-occurrence patterns
      - Rating prediction using K-nearest neighbor weighted average
    - id: 2
      name: Matrix Factorization
      description: Implement SVD and ALS for latent factor models.
      acceptance_criteria:
      - Perform SVD decomposition of the user-item rating matrix
      - Implement ALS (Alternating Least Squares) as alternative factorization
      - Apply regularization to prevent overfitting on sparse data
      - Handle implicit feedback signals like views and clicks
      - Tune hyperparameters for latent dimension count and learning rate
      hints:
        level1: Rating â‰ˆ User_vector Â· Item_vector. Learn vectors via gradient descent.
        level2: 'ALS: fix users, optimize items. Then fix items, optimize users. Repeat.'
        level3: |-
          import numpy as np

          class MatrixFactorization:
              def __init__(self, n_factors: int = 50, learning_rate: float = 0.01,
                           regularization: float = 0.02, n_epochs: int = 20):
                  self.n_factors = n_factors
                  self.lr = learning_rate
                  self.reg = regularization
                  self.n_epochs = n_epochs

              def fit(self, ratings: List[tuple]):
                  users = list(set(r[0] for r in ratings))
                  items = list(set(r[1] for r in ratings))

                  self.user_idx = {u: i for i, u in enumerate(users)}
                  self.item_idx = {it: i for i, it in enumerate(items)}

                  n_users = len(users)
                  n_items = len(items)

                  # Initialize factors randomly
                  self.user_factors = np.random.normal(0, 0.1, (n_users, self.n_factors))
                  self.item_factors = np.random.normal(0, 0.1, (n_items, self.n_factors))

                  # Biases
                  self.global_mean = np.mean([r[2] for r in ratings])
                  self.user_bias = np.zeros(n_users)
                  self.item_bias = np.zeros(n_items)

                  # SGD training
                  for epoch in range(self.n_epochs):
                      np.random.shuffle(ratings)
                      total_error = 0

                      for user, item, rating in ratings:
                          u_idx = self.user_idx[user]
                          i_idx = self.item_idx[item]

                          # Prediction
                          pred = (self.global_mean +
                                 self.user_bias[u_idx] +
                                 self.item_bias[i_idx] +
                                 np.dot(self.user_factors[u_idx], self.item_factors[i_idx]))

                          error = rating - pred
                          total_error += error ** 2

                          # Update biases
                          self.user_bias[u_idx] += self.lr * (error - self.reg * self.user_bias[u_idx])
                          self.item_bias[i_idx] += self.lr * (error - self.reg * self.item_bias[i_idx])

                          # Update factors
                          user_factor = self.user_factors[u_idx].copy()
                          self.user_factors[u_idx] += self.lr * (
                              error * self.item_factors[i_idx] - self.reg * user_factor
                          )
                          self.item_factors[i_idx] += self.lr * (
                              error * user_factor - self.reg * self.item_factors[i_idx]
                          )

                      rmse = np.sqrt(total_error / len(ratings))
                      print(f"Epoch {epoch + 1}: RMSE = {rmse:.4f}")

              def predict(self, user_id, item_id) -> float:
                  if user_id not in self.user_idx or item_id not in self.item_idx:
                      return self.global_mean

                  u_idx = self.user_idx[user_id]
                  i_idx = self.item_idx[item_id]

                  return (self.global_mean +
                         self.user_bias[u_idx] +
                         self.item_bias[i_idx] +
                         np.dot(self.user_factors[u_idx], self.item_factors[i_idx]))

              def recommend(self, user_id, n: int = 10, exclude_rated: set = None) -> List[tuple]:
                  if user_id not in self.user_idx:
                      return []

                  u_idx = self.user_idx[user_id]
                  exclude_rated = exclude_rated or set()

                  # Score all items
                  scores = (self.global_mean +
                           self.user_bias[u_idx] +
                           self.item_bias +
                           np.dot(self.item_factors, self.user_factors[u_idx]))

                  # Filter and sort
                  items_scores = [
                      (self.idx_item[i], scores[i])
                      for i in range(len(scores))
                      if self.idx_item.get(i) not in exclude_rated
                  ]
                  items_scores.sort(key=lambda x: x[1], reverse=True)

                  return items_scores[:n]
      pitfalls:
      - Learning rate too high causes divergence
      - Overfitting without regularization
      - Implicit data needs different loss function
      - Factor initialization affects convergence
      concepts:
      - Matrix factorization
      - SVD
      - Stochastic gradient descent
      - Regularization
      estimated_hours: 8-12
      deliverables:
      - User-item matrix construction from sparse rating data
      - SVD or ALS matrix decomposition into latent factors
      - Latent factor learning with gradient descent optimization
      - Rating prediction by dot product of user and item factors
    - id: 3
      name: Content-Based Filtering
      description: Recommend based on item features and user preferences.
      acceptance_criteria:
      - Extract item features from text descriptions, categories, and tags
      - Build user preference profile from liked and interacted items
      - Compute content similarity matching user preferences to item features
      - Combine content-based scores with collaborative filtering signals
      - Generate explainable recommendations showing why items were recommended
      hints:
        level1: TF-IDF on item descriptions. User profile = weighted avg of liked item vectors.
        level2: For explainability, track which features contributed most to recommendation.
        level3: |-
          from sklearn.feature_extraction.text import TfidfVectorizer
          from sklearn.metrics.pairwise import cosine_similarity
          import numpy as np

          class ContentBasedRecommender:
              def __init__(self):
                  self.tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
                  self.item_features = None
                  self.item_ids = []

              def fit(self, items: List[Dict]):
                  # items = [{'id': '1', 'title': '...', 'description': '...', 'tags': [...]}]
                  self.item_ids = [item['id'] for item in items]
                  self.item_idx = {id: i for i, id in enumerate(self.item_ids)}

                  # Combine text features
                  texts = []
                  for item in items:
                      text = f"{item.get('title', '')} {item.get('description', '')} {' '.join(item.get('tags', []))}"
                      texts.append(text)

                  # TF-IDF matrix
                  self.item_features = self.tfidf.fit_transform(texts)

                  # Precompute item similarity
                  self.item_similarity = cosine_similarity(self.item_features)

              def build_user_profile(self, user_ratings: List[tuple]) -> np.ndarray:
                  # user_ratings = [(item_id, rating), ...]
                  if not user_ratings:
                      return np.zeros(self.item_features.shape[1])

                  # Weighted average of item vectors
                  profile = np.zeros(self.item_features.shape[1])
                  total_weight = 0

                  for item_id, rating in user_ratings:
                      if item_id in self.item_idx:
                          idx = self.item_idx[item_id]
                          weight = rating - 3  # Center around neutral rating
                          profile += weight * self.item_features[idx].toarray().flatten()
                          total_weight += abs(weight)

                  if total_weight > 0:
                      profile /= total_weight

                  return profile

              def recommend(self, user_ratings: List[tuple], n: int = 10) -> List[Dict]:
                  user_profile = self.build_user_profile(user_ratings)

                  # Similarity to user profile
                  scores = cosine_similarity([user_profile], self.item_features)[0]

                  # Exclude already rated
                  rated_items = {r[0] for r in user_ratings}

                  recommendations = []
                  for idx in np.argsort(scores)[::-1]:
                      item_id = self.item_ids[idx]
                      if item_id not in rated_items:
                          # Get explanation
                          explanation = self._explain(idx, user_profile)
                          recommendations.append({
                              'item_id': item_id,
                              'score': float(scores[idx]),
                              'explanation': explanation
                          })
                          if len(recommendations) >= n:
                              break

                  return recommendations

              def _explain(self, item_idx: int, user_profile: np.ndarray, top_k: int = 3) -> str:
                  item_vector = self.item_features[item_idx].toarray().flatten()

                  # Find overlapping important features
                  feature_names = self.tfidf.get_feature_names_out()

                  contributions = item_vector * user_profile
                  top_indices = np.argsort(contributions)[::-1][:top_k]

                  top_features = [feature_names[i] for i in top_indices if contributions[i] > 0]

                  if top_features:
                      return f"Recommended because you liked items with: {', '.join(top_features)}"
                  return "Recommended based on your preferences"

              def similar_items(self, item_id: str, n: int = 10) -> List[tuple]:
                  if item_id not in self.item_idx:
                      return []

                  idx = self.item_idx[item_id]
                  similarities = self.item_similarity[idx]

                  similar = []
                  for i in np.argsort(similarities)[::-1][1:n+1]:  # Skip self
                      similar.append((self.item_ids[i], float(similarities[i])))

                  return similar
      pitfalls:
      - Feature extraction misses important signals
      - User profile dominated by few items
      - 'Filter bubble: only recommend similar items'
      - Explanations don't match user perception
      concepts:
      - Content-based filtering
      - TF-IDF
      - User profiling
      - Explainability
      estimated_hours: 6-10
      deliverables:
      - Item feature extraction from text, categories, and tags
      - User preference profile built from interaction history
      - Content similarity computation between user profile and items
      - Hybrid scoring combining collaborative and content-based signals
    - id: 4
      name: Production System
      description: Build production-ready recommendation service.
      acceptance_criteria:
      - Serve real-time recommendation API with sub-second latency
      - Support batch candidate generation for offline precomputation
      - Implement A/B testing framework for comparing algorithm variants
      - Expose recommendation metrics and monitoring dashboards
      - Support model versioning with rollback capability
      hints:
        level1: 'Two-stage: batch generates candidates, real-time ranks them.'
        level2: Track impressions, clicks, conversions. Calculate CTR, conversion rate.
        level3: |-
          from fastapi import FastAPI, BackgroundTasks
          from pydantic import BaseModel
          from typing import List, Optional
          import asyncio
          import random

          app = FastAPI()

          class RecommendationRequest(BaseModel):
              user_id: str
              context: Optional[Dict] = {}
              n: int = 10
              experiment_id: Optional[str] = None

          class RecommendationResponse(BaseModel):
              items: List[Dict]
              model_version: str
              experiment_group: str

          class RecommendationService:
              def __init__(self):
                  self.models = {}  # version -> model
                  self.candidate_cache = {}  # user_id -> candidates
                  self.experiments = {}  # experiment_id -> config

              def load_model(self, version: str, model):
                  self.models[version] = model

              async def get_recommendations(self, request: RecommendationRequest) -> RecommendationResponse:
                  # Determine experiment group
                  experiment_group = self._assign_experiment_group(
                      request.user_id, request.experiment_id
                  )
                  model_version = self._get_model_for_group(experiment_group)

                  # Get candidates (from cache or generate)
                  candidates = await self._get_candidates(request.user_id)

                  # Rank candidates
                  model = self.models[model_version]
                  ranked = model.rank(request.user_id, candidates, request.context)

                  # Apply business rules
                  final = self._apply_business_rules(ranked, request.context)

                  return RecommendationResponse(
                      items=final[:request.n],
                      model_version=model_version,
                      experiment_group=experiment_group
                  )

              def _assign_experiment_group(self, user_id: str, experiment_id: str) -> str:
                  if not experiment_id or experiment_id not in self.experiments:
                      return "control"

                  config = self.experiments[experiment_id]
                  # Deterministic assignment based on user_id
                  hash_val = hash(f"{user_id}_{experiment_id}") % 100

                  cumulative = 0
                  for group, percentage in config['groups'].items():
                      cumulative += percentage
                      if hash_val < cumulative:
                          return group

                  return "control"

          # Metrics tracking
          class MetricsTracker:
              def __init__(self):
                  self.events = []

              def track_impression(self, user_id: str, items: List[str],
                                  experiment_group: str, model_version: str):
                  self.events.append({
                      'type': 'impression',
                      'user_id': user_id,
                      'items': items,
                      'experiment_group': experiment_group,
                      'model_version': model_version,
                      'timestamp': datetime.now()
                  })

              def track_click(self, user_id: str, item_id: str, position: int):
                  self.events.append({
                      'type': 'click',
                      'user_id': user_id,
                      'item_id': item_id,
                      'position': position,
                      'timestamp': datetime.now()
                  })

              def calculate_metrics(self, experiment_id: str) -> Dict:
                  impressions = [e for e in self.events if e['type'] == 'impression']
                  clicks = [e for e in self.events if e['type'] == 'click']

                  by_group = {}
                  for group in set(e.get('experiment_group') for e in impressions):
                      group_impressions = [e for e in impressions if e.get('experiment_group') == group]
                      group_users = set(e['user_id'] for e in group_impressions)
                      group_clicks = [e for e in clicks if e['user_id'] in group_users]

                      by_group[group] = {
                          'impressions': len(group_impressions),
                          'clicks': len(group_clicks),
                          'ctr': len(group_clicks) / max(1, len(group_impressions)),
                          'unique_users': len(group_users)
                      }

                  return by_group
      pitfalls:
      - Cold cache causes latency spikes
      - A/B test assignment not deterministic
      - Metrics delayed lose temporal correlation
      - Model drift not detected
      concepts:
      - Two-stage recommendation
      - A/B testing
      - Metrics tracking
      - Model serving
      estimated_hours: 12-16
      deliverables:
      - Recommendation API endpoint returning ranked item suggestions
      - Real-time scoring pipeline computing recommendations on demand
      - A/B testing support for comparing recommendation algorithms
      - User feedback integration loop updating models from interactions
  rest-api-design:
    id: rest-api-design
    name: Production REST API
    description: Build a production-grade REST API with authentication, validation, rate limiting, and proper error handling.
    difficulty: beginner
    estimated_hours: 20-30
    prerequisites:
    - HTTP basics
    - JSON
    - Database basics
    languages:
      recommended:
      - Go
      - Python
      - Node.js
      also_possible:
      - Rust
      - Java
    resources:
    - name: REST API Design Best Practices
      url: https://restfulapi.net/
      type: tutorial
    - name: OpenAPI Specification
      url: https://swagger.io/specification/
      type: documentation
    milestones:
    - id: 1
      name: CRUD Operations
      description: Implement Create, Read, Update, Delete operations with proper HTTP methods.
      acceptance_criteria:
      - POST /resources creates new resource and returns 201 Created
      - GET /resources returns paginated list of all resources
      - GET /resources/:id returns single resource or 404 Not Found
      - PUT/PATCH /resources/:id updates resource and returns new state
      - DELETE /resources/:id removes resource and returns 204 No Content
      hints:
        level1: 'Use proper HTTP methods: GET for read, POST for create, PUT for replace, PATCH for update, DELETE for remove.'
        level2: 'Return appropriate status codes: 200 OK, 201 Created, 204 No Content, 400 Bad Request, 404 Not Found.'
        level3: |-
          from fastapi import FastAPI, HTTPException, status
          from pydantic import BaseModel
          from typing import List, Optional
          from uuid import uuid4

          app = FastAPI()

          class Item(BaseModel):
              id: Optional[str] = None
              name: str
              description: Optional[str] = None
              price: float

          # In-memory storage
          items_db: dict[str, Item] = {}

          @app.post("/items", status_code=status.HTTP_201_CREATED)
          def create_item(item: Item) -> Item:
              item.id = str(uuid4())
              items_db[item.id] = item
              return item

          @app.get("/items")
          def list_items(skip: int = 0, limit: int = 10) -> List[Item]:
              items = list(items_db.values())
              return items[skip:skip + limit]

          @app.get("/items/{item_id}")
          def get_item(item_id: str) -> Item:
              if item_id not in items_db:
                  raise HTTPException(status_code=404, detail="Item not found")
              return items_db[item_id]

          @app.put("/items/{item_id}")
          def update_item(item_id: str, item: Item) -> Item:
              if item_id not in items_db:
                  raise HTTPException(status_code=404, detail="Item not found")
              item.id = item_id
              items_db[item_id] = item
              return item

          @app.delete("/items/{item_id}", status_code=status.HTTP_204_NO_CONTENT)
          def delete_item(item_id: str):
              if item_id not in items_db:
                  raise HTTPException(status_code=404, detail="Item not found")
              del items_db[item_id]
      pitfalls:
      - Using POST for updates or GET for mutations
      - Returning 200 for resource creation instead of 201
      - Not handling concurrent updates
      - Missing Content-Type headers
      concepts:
      - REST principles
      - HTTP methods
      - Status codes
      - Resource modeling
      estimated_hours: 4-6
      deliverables:
      - Resource endpoints following RESTful URL naming conventions
      - HTTP method mapping to create, read, update, and delete operations
      - Response formatting with consistent JSON structure and envelope
      - Appropriate HTTP status codes for success and error responses
    - id: 2
      name: Input Validation
      description: Validate all input data and return meaningful errors.
      acceptance_criteria:
      - Validate request body fields against defined JSON schema
      - Validate query parameters for type, range, and allowed values
      - Validate path parameters for correct format and existence
      - Return detailed error messages listing each validation failure
      - Sanitize all input to prevent injection and XSS attacks
      hints:
        level1: Use Pydantic (Python), Zod (TypeScript), or similar for schema validation.
        level2: Return 400 Bad Request with field-level errors. Include error codes for i18n.
        level3: |-
          from pydantic import BaseModel, Field, validator, ValidationError
          from fastapi import FastAPI, HTTPException, Query
          from fastapi.exceptions import RequestValidationError
          from fastapi.responses import JSONResponse
          from typing import List
          import re

          class CreateUserRequest(BaseModel):
              email: str = Field(..., description="User email address")
              password: str = Field(..., min_length=8, max_length=128)
              username: str = Field(..., min_length=3, max_length=50)

              @validator('email')
              def validate_email(cls, v):
                  pattern = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$'
                  if not re.match(pattern, v):
                      raise ValueError('Invalid email format')
                  return v.lower()

              @validator('password')
              def validate_password(cls, v):
                  if not any(c.isupper() for c in v):
                      raise ValueError('Password must contain uppercase letter')
                  if not any(c.isdigit() for c in v):
                      raise ValueError('Password must contain digit')
                  return v

              @validator('username')
              def validate_username(cls, v):
                  if not re.match(r'^[a-zA-Z0-9_]+$', v):
                      raise ValueError('Username can only contain letters, numbers, underscores')
                  return v

          class ErrorDetail(BaseModel):
              field: str
              message: str
              code: str

          class ErrorResponse(BaseModel):
              error: str
              details: List[ErrorDetail]

          @app.exception_handler(RequestValidationError)
          async def validation_exception_handler(request, exc):
              errors = []
              for error in exc.errors():
                  field = '.'.join(str(loc) for loc in error['loc'][1:])  # Skip 'body'
                  errors.append(ErrorDetail(
                      field=field,
                      message=error['msg'],
                      code=f"validation.{error['type']}"
                  ))

              return JSONResponse(
                  status_code=400,
                  content=ErrorResponse(
                      error="Validation failed",
                      details=errors
                  ).dict()
              )

          # Query parameter validation
          @app.get("/users")
          def list_users(
              page: int = Query(1, ge=1, description="Page number"),
              per_page: int = Query(20, ge=1, le=100, description="Items per page"),
              sort_by: str = Query("created_at", regex="^(created_at|name|email)$")
          ):
              # Validated parameters are guaranteed to be valid here
              pass
      pitfalls:
      - Only validating types, not business rules
      - Exposing internal error details
      - Not validating query/path params
      - SQL/NoSQL injection through unvalidated input
      concepts:
      - Schema validation
      - Error handling
      - Input sanitization
      - Security
      estimated_hours: 4-6
      deliverables:
      - Request body validation against JSON schema definition
      - Schema-based validation with type, format, and constraint checks
      - Structured error response listing all validation failures
      - Input sanitization removing dangerous characters and patterns
    - id: 3
      name: Authentication & Authorization
      description: Implement JWT authentication and role-based access control.
      acceptance_criteria:
      - Support user registration and login returning access tokens
      - Generate and validate JWT tokens with configurable expiration
      - Implement token refresh mechanism for long-lived sessions
      - Enforce role-based access control on protected endpoints
      - Protect routes requiring authentication with middleware guard
      hints:
        level1: JWT = header.payload.signature. Store user ID and roles in payload.
        level2: Use short-lived access tokens (15min) + long-lived refresh tokens (7d).
        level3: |-
          import jwt
          from datetime import datetime, timedelta
          from fastapi import Depends, HTTPException, status
          from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
          from passlib.context import CryptContext
          from functools import wraps

          SECRET_KEY = "your-secret-key"  # Use env var in production
          ALGORITHM = "HS256"
          ACCESS_TOKEN_EXPIRE_MINUTES = 15
          REFRESH_TOKEN_EXPIRE_DAYS = 7

          pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
          security = HTTPBearer()

          def create_access_token(user_id: str, roles: list[str]) -> str:
              expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
              payload = {
                  "sub": user_id,
                  "roles": roles,
                  "exp": expire,
                  "type": "access"
              }
              return jwt.encode(payload, SECRET_KEY, algorithm=ALGORITHM)

          def create_refresh_token(user_id: str) -> str:
              expire = datetime.utcnow() + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
              payload = {
                  "sub": user_id,
                  "exp": expire,
                  "type": "refresh"
              }
              return jwt.encode(payload, SECRET_KEY, algorithm=ALGORITHM)

          def verify_token(token: str) -> dict:
              try:
                  payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
                  return payload
              except jwt.ExpiredSignatureError:
                  raise HTTPException(status_code=401, detail="Token expired")
              except jwt.InvalidTokenError:
                  raise HTTPException(status_code=401, detail="Invalid token")

          async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
              payload = verify_token(credentials.credentials)
              if payload.get("type") != "access":
                  raise HTTPException(status_code=401, detail="Invalid token type")
              return payload

          def require_roles(*required_roles):
              def decorator(func):
                  @wraps(func)
                  async def wrapper(*args, current_user: dict = Depends(get_current_user), **kwargs):
                      user_roles = set(current_user.get("roles", []))
                      if not user_roles.intersection(required_roles):
                          raise HTTPException(status_code=403, detail="Insufficient permissions")
                      return await func(*args, current_user=current_user, **kwargs)
                  return wrapper
              return decorator

          @app.post("/auth/login")
          def login(email: str, password: str):
              user = get_user_by_email(email)
              if not user or not pwd_context.verify(password, user.hashed_password):
                  raise HTTPException(status_code=401, detail="Invalid credentials")

              return {
                  "access_token": create_access_token(user.id, user.roles),
                  "refresh_token": create_refresh_token(user.id),
                  "token_type": "bearer"
              }

          @app.post("/auth/refresh")
          def refresh(refresh_token: str):
              payload = verify_token(refresh_token)
              if payload.get("type") != "refresh":
                  raise HTTPException(status_code=401, detail="Invalid token type")

              user = get_user_by_id(payload["sub"])
              return {
                  "access_token": create_access_token(user.id, user.roles),
                  "token_type": "bearer"
              }

          @app.get("/admin/users")
          @require_roles("admin")
          async def list_all_users(current_user: dict = Depends(get_current_user)):
              return get_all_users()
      pitfalls:
      - Storing secrets in code
      - No token expiration or too long expiry
      - Not invalidating tokens on password change
      - RBAC bypass through direct object access
      concepts:
      - JWT tokens
      - Password hashing
      - RBAC
      - Token refresh
      estimated_hours: 6-8
      deliverables:
      - JWT-based authentication with token generation and validation
      - API key authentication option for service-to-service calls
      - Role-based access control restricting endpoints by user role
      - Protected route middleware rejecting unauthenticated requests
    - id: 4
      name: Rate Limiting & Throttling
      description: Protect API from abuse with rate limiting.
      acceptance_criteria:
      - Enforce per-user rate limits based on authentication identity
      - Enforce per-endpoint rate limits with different thresholds
      - Implement sliding window algorithm for smooth rate limiting
      - Include X-RateLimit headers showing limit, remaining, and reset time
      - Degrade gracefully under load by shedding lowest-priority requests
      hints:
        level1: Token bucket or sliding window. Store counts in Redis for distributed.
        level2: Return X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset headers.
        level3: |-
          import time
          import redis
          from fastapi import Request, HTTPException
          from starlette.middleware.base import BaseHTTPMiddleware

          class RateLimiter:
              def __init__(self, redis_client: redis.Redis):
                  self.redis = redis_client

              def is_allowed(self, key: str, limit: int, window_seconds: int) -> tuple[bool, dict]:
                  now = time.time()
                  window_start = now - window_seconds

                  pipe = self.redis.pipeline()

                  # Remove old entries
                  pipe.zremrangebyscore(key, 0, window_start)

                  # Count current entries
                  pipe.zcard(key)

                  # Add current request
                  pipe.zadd(key, {str(now): now})

                  # Set expiry
                  pipe.expire(key, window_seconds)

                  results = pipe.execute()
                  current_count = results[1]

                  remaining = max(0, limit - current_count - 1)
                  reset_time = int(now + window_seconds)

                  headers = {
                      "X-RateLimit-Limit": str(limit),
                      "X-RateLimit-Remaining": str(remaining),
                      "X-RateLimit-Reset": str(reset_time)
                  }

                  return current_count < limit, headers

          class RateLimitMiddleware(BaseHTTPMiddleware):
              def __init__(self, app, redis_client: redis.Redis, default_limit: int = 100):
                  super().__init__(app)
                  self.limiter = RateLimiter(redis_client)
                  self.default_limit = default_limit

                  # Per-endpoint limits
                  self.endpoint_limits = {
                      "/auth/login": (5, 60),     # 5 requests per minute
                      "/auth/register": (3, 60),  # 3 requests per minute
                      "/api/search": (30, 60),    # 30 requests per minute
                  }

              async def dispatch(self, request: Request, call_next):
                  # Get identifier (user ID or IP)
                  user_id = getattr(request.state, 'user_id', None)
                  identifier = user_id or request.client.host

                  # Get limit for endpoint
                  path = request.url.path
                  limit, window = self.endpoint_limits.get(path, (self.default_limit, 60))

                  # Check rate limit
                  key = f"ratelimit:{identifier}:{path}"
                  allowed, headers = self.limiter.is_allowed(key, limit, window)

                  if not allowed:
                      return JSONResponse(
                          status_code=429,
                          content={"error": "Rate limit exceeded", "retry_after": headers["X-RateLimit-Reset"]},
                          headers=headers
                      )

                  response = await call_next(request)

                  # Add rate limit headers
                  for header, value in headers.items():
                      response.headers[header] = value

                  return response

          # Distributed rate limiting with token bucket
          class TokenBucket:
              def __init__(self, redis_client: redis.Redis, capacity: int, refill_rate: float):
                  self.redis = redis_client
                  self.capacity = capacity
                  self.refill_rate = refill_rate  # tokens per second

              def consume(self, key: str, tokens: int = 1) -> bool:
                  now = time.time()

                  # Lua script for atomic operation
                  script = '''
                  local key = KEYS[1]
                  local capacity = tonumber(ARGV[1])
                  local refill_rate = tonumber(ARGV[2])
                  local tokens = tonumber(ARGV[3])
                  local now = tonumber(ARGV[4])

                  local bucket = redis.call('HMGET', key, 'tokens', 'last_update')
                  local current_tokens = tonumber(bucket[1]) or capacity
                  local last_update = tonumber(bucket[2]) or now

                  local elapsed = now - last_update
                  local refill = elapsed * refill_rate
                  current_tokens = math.min(capacity, current_tokens + refill)

                  if current_tokens >= tokens then
                      current_tokens = current_tokens - tokens
                      redis.call('HMSET', key, 'tokens', current_tokens, 'last_update', now)
                      redis.call('EXPIRE', key, 3600)
                      return 1
                  end
                  return 0
                  '''

                  result = self.redis.eval(script, 1, key, self.capacity, self.refill_rate, tokens, now)
                  return result == 1
      pitfalls:
      - Rate limits per IP bypass with proxies
      - Not handling Redis failures gracefully
      - Clock skew in distributed systems
      - Rate limit headers revealing too much info
      concepts:
      - Rate limiting algorithms
      - Sliding window
      - Token bucket
      - Redis
      estimated_hours: 5-7
      deliverables:
      - Rate limit middleware intercepting and counting requests
      - Standard rate limit headers included in all HTTP responses
      - HTTP 429 response body with error message and retry guidance
      - Per-client limit configuration supporting different tiers
  api-gateway:
    id: api-gateway
    name: API Gateway
    description: Build an API gateway that handles routing, authentication, rate limiting, and request transformation.
    difficulty: advanced
    estimated_hours: 50-70
    prerequisites:
    - REST APIs
    - Networking
    - Load balancing concepts
    languages:
      recommended:
      - Go
      - Rust
      also_possible:
      - Node.js
      - Java
    resources:
    - name: Kong Gateway
      url: https://docs.konghq.com/
      type: reference
    - name: NGINX as API Gateway
      url: https://www.nginx.com/blog/deploying-nginx-plus-as-an-api-gateway/
      type: article
    milestones:
    - id: 1
      name: Reverse Proxy & Routing
      description: Route requests to appropriate backend services.
      acceptance_criteria:
      - Path-based routing correctly maps URL prefixes to backend services (e.g., /api/users -> user-service)
      - Host-based routing directs requests to different backends based on the Host header (e.g., api.example.com)
      - Load balancing distributes requests across healthy backend instances using the configured strategy
      - Health checks detect unhealthy backends and remove them from the routing pool until they recover
      - Circuit breaker opens after repeated failures to a backend and stops sending traffic for a cooldown period
      hints:
        level1: Use regex patterns for path matching. Maintain pool of healthy backends.
        level2: Round-robin for basic LB. Add weights, least-connections for advanced.
        level3: |-
          package main

          import (
              "net/http"
              "net/http/httputil"
              "net/url"
              "sync"
              "time"
          )

          type Backend struct {
              URL      *url.URL
              Alive    bool
              Weight   int
              mu       sync.RWMutex
          }

          func (b *Backend) SetAlive(alive bool) {
              b.mu.Lock()
              b.Alive = alive
              b.mu.Unlock()
          }

          func (b *Backend) IsAlive() bool {
              b.mu.RLock()
              defer b.mu.RUnlock()
              return b.Alive
          }

          type Route struct {
              PathPrefix string
              Backends   []*Backend
              current    uint64
          }

          func (r *Route) NextBackend() *Backend {
              for i := 0; i < len(r.Backends); i++ {
                  idx := int(atomic.AddUint64(&r.current, 1)) % len(r.Backends)
                  if r.Backends[idx].IsAlive() {
                      return r.Backends[idx]
                  }
              }
              return nil
          }

          type Gateway struct {
              routes  map[string]*Route
              mu      sync.RWMutex
          }

          func (g *Gateway) AddRoute(pathPrefix string, backends []string) {
              route := &Route{PathPrefix: pathPrefix}

              for _, b := range backends {
                  u, _ := url.Parse(b)
                  route.Backends = append(route.Backends, &Backend{URL: u, Alive: true, Weight: 1})
              }

              g.mu.Lock()
              g.routes[pathPrefix] = route
              g.mu.Unlock()
          }

          func (g *Gateway) ServeHTTP(w http.ResponseWriter, r *http.Request) {
              // Find matching route
              g.mu.RLock()
              var matchedRoute *Route
              var longestMatch int
              for prefix, route := range g.routes {
                  if strings.HasPrefix(r.URL.Path, prefix) && len(prefix) > longestMatch {
                      matchedRoute = route
                      longestMatch = len(prefix)
                  }
              }
              g.mu.RUnlock()

              if matchedRoute == nil {
                  http.Error(w, "No route found", http.StatusNotFound)
                  return
              }

              // Get healthy backend
              backend := matchedRoute.NextBackend()
              if backend == nil {
                  http.Error(w, "No healthy backends", http.StatusServiceUnavailable)
                  return
              }

              // Reverse proxy
              proxy := httputil.NewSingleHostReverseProxy(backend.URL)

              // Modify request
              r.URL.Host = backend.URL.Host
              r.URL.Scheme = backend.URL.Scheme
              r.Header.Set("X-Forwarded-Host", r.Host)
              r.Header.Set("X-Real-IP", r.RemoteAddr)

              proxy.ServeHTTP(w, r)
          }

          // Health checker
          func (g *Gateway) HealthCheck(interval time.Duration) {
              ticker := time.NewTicker(interval)
              for range ticker.C {
                  g.mu.RLock()
                  for _, route := range g.routes {
                      for _, backend := range route.Backends {
                          go func(b *Backend) {
                              resp, err := http.Get(b.URL.String() + "/health")
                              b.SetAlive(err == nil && resp.StatusCode == 200)
                          }(backend)
                      }
                  }
                  g.mu.RUnlock()
              }
          }
      pitfalls:
      - Not forwarding original client IP
      - Health checks blocking request handling
      - Memory leaks from unclosed connections
      - Thundering herd on backend recovery
      concepts:
      - Reverse proxy
      - Load balancing
      - Health checks
      - Service discovery
      estimated_hours: 12-18
      deliverables:
      - Request routing rules engine that matches incoming requests to backend services
      - Path-based routing that forwards requests to services based on URL prefix or pattern matching
      - Header-based routing that selects backend services based on request header values like Host or X-Version
      - Load balancing across multiple backend instances using round-robin or weighted algorithms
    - id: 2
      name: Request/Response Transformation
      description: Modify requests and responses as they pass through.
      acceptance_criteria:
      - Header manipulation correctly adds, removes, and modifies headers on both requests and responses
      - Request body transformation rewrites JSON payloads according to configurable mapping rules
      - Response body transformation filters or reshapes backend response data before client delivery
      - URL rewriting modifies the request path and query parameters before forwarding to the backend
      - Request aggregation combines responses from multiple backend calls into a single client response
      hints:
        level1: Intercept request/response streams. Use middleware pattern.
        level2: For body transformation, buffer entire body, transform, forward.
        level3: |-
          import (
              "bytes"
              "encoding/json"
              "io"
          )

          type Transformer interface {
              TransformRequest(r *http.Request) error
              TransformResponse(resp *http.Response) error
          }

          type HeaderTransformer struct {
              AddHeaders    map[string]string
              RemoveHeaders []string
          }

          func (t *HeaderTransformer) TransformRequest(r *http.Request) error {
              for key, value := range t.AddHeaders {
                  r.Header.Set(key, value)
              }
              for _, key := range t.RemoveHeaders {
                  r.Header.Del(key)
              }
              return nil
          }

          type BodyTransformer struct {
              Transform func(body []byte) ([]byte, error)
          }

          func (t *BodyTransformer) TransformRequest(r *http.Request) error {
              if r.Body == nil || t.Transform == nil {
                  return nil
              }

              body, err := io.ReadAll(r.Body)
              if err != nil {
                  return err
              }
              r.Body.Close()

              transformed, err := t.Transform(body)
              if err != nil {
                  return err
              }

              r.Body = io.NopCloser(bytes.NewReader(transformed))
              r.ContentLength = int64(len(transformed))
              return nil
          }

          // URL Rewriting
          type URLRewriter struct {
              Rules []RewriteRule
          }

          type RewriteRule struct {
              Pattern *regexp.Regexp
              Replace string
          }

          func (u *URLRewriter) TransformRequest(r *http.Request) error {
              path := r.URL.Path
              for _, rule := range u.Rules {
                  if rule.Pattern.MatchString(path) {
                      path = rule.Pattern.ReplaceAllString(path, rule.Replace)
                      break
                  }
              }
              r.URL.Path = path
              return nil
          }

          // Request aggregation
          type Aggregator struct {
              Endpoints []AggregateEndpoint
          }

          type AggregateEndpoint struct {
              Name    string
              URL     string
              Extract func(response []byte) interface{}
          }

          func (a *Aggregator) Aggregate(ctx context.Context) (map[string]interface{}, error) {
              results := make(map[string]interface{})
              var mu sync.Mutex
              var wg sync.WaitGroup

              for _, endpoint := range a.Endpoints {
                  wg.Add(1)
                  go func(ep AggregateEndpoint) {
                      defer wg.Done()

                      req, _ := http.NewRequestWithContext(ctx, "GET", ep.URL, nil)
                      resp, err := http.DefaultClient.Do(req)
                      if err != nil {
                          return
                      }
                      defer resp.Body.Close()

                      body, _ := io.ReadAll(resp.Body)
                      extracted := ep.Extract(body)

                      mu.Lock()
                      results[ep.Name] = extracted
                      mu.Unlock()
                  }(endpoint)
              }

              wg.Wait()
              return results, nil
          }
      pitfalls:
      - Large body buffering causes memory issues
      - Transformation errors not handled gracefully
      - Aggregation timeout causes partial responses
      - Content-Length mismatch after transformation
      concepts:
      - Request transformation
      - Response transformation
      - URL rewriting
      - API aggregation
      estimated_hours: 10-14
      deliverables:
      - Header manipulation middleware that adds, removes, or modifies HTTP headers on requests and responses
      - Request body transformation pipeline that rewrites payloads before forwarding to backends
      - Response modification layer that alters backend responses before returning them to the client
      - Protocol translation module that converts between REST and gRPC or other protocol formats
    - id: 3
      name: Authentication & Authorization Layer
      description: Centralized auth handling at gateway level.
      acceptance_criteria:
      - JWT validation verifies RS256/HS256 signatures and rejects expired or malformed tokens at the gateway
      - API key authentication accepts valid keys and returns 401 Unauthorized for missing or invalid keys
      - OAuth2 token introspection calls the authorization server and caches active/inactive results
      - Authenticated user context (user ID, roles, scopes) is passed to backend services via headers
      - Auth caching stores validated tokens to avoid repeated verification calls for the same token
      hints:
        level1: Validate JWT signature and expiry. Cache valid tokens briefly.
        level2: Add user info to X-User-ID, X-User-Roles headers for backends.
        level3: |-
          type AuthMiddleware struct {
              JWTSecret     []byte
              APIKeyStore   APIKeyStore
              TokenCache    *lru.Cache // github.com/hashicorp/golang-lru
              CacheTTL      time.Duration
          }

          func (m *AuthMiddleware) Authenticate(next http.Handler) http.Handler {
              return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
                  var userCtx *UserContext

                  // Try Bearer token first
                  authHeader := r.Header.Get("Authorization")
                  if strings.HasPrefix(authHeader, "Bearer ") {
                      token := strings.TrimPrefix(authHeader, "Bearer ")
                      userCtx = m.validateJWT(token)
                  }

                  // Try API key
                  if userCtx == nil {
                      apiKey := r.Header.Get("X-API-Key")
                      if apiKey != "" {
                          userCtx = m.validateAPIKey(apiKey)
                      }
                  }

                  if userCtx == nil {
                      http.Error(w, "Unauthorized", http.StatusUnauthorized)
                      return
                  }

                  // Add user context to headers for backends
                  r.Header.Set("X-User-ID", userCtx.UserID)
                  r.Header.Set("X-User-Roles", strings.Join(userCtx.Roles, ","))
                  r.Header.Set("X-Auth-Method", userCtx.AuthMethod)

                  next.ServeHTTP(w, r)
              })
          }

          func (m *AuthMiddleware) validateJWT(token string) *UserContext {
              // Check cache first
              if cached, ok := m.TokenCache.Get(token); ok {
                  return cached.(*UserContext)
              }

              // Parse and validate
              claims := jwt.MapClaims{}
              parsed, err := jwt.ParseWithClaims(token, claims, func(t *jwt.Token) (interface{}, error) {
                  return m.JWTSecret, nil
              })

              if err != nil || !parsed.Valid {
                  return nil
              }

              userCtx := &UserContext{
                  UserID:     claims["sub"].(string),
                  Roles:      toStringSlice(claims["roles"]),
                  AuthMethod: "jwt",
              }

              // Cache it
              m.TokenCache.Add(token, userCtx)

              return userCtx
          }

          // OAuth2 token introspection for external tokens
          func (m *AuthMiddleware) introspectToken(token string) *UserContext {
              // Check cache
              if cached, ok := m.TokenCache.Get(token); ok {
                  return cached.(*UserContext)
              }

              // Call OAuth server
              resp, err := http.PostForm(m.IntrospectionURL, url.Values{
                  "token":           {token},
                  "client_id":       {m.ClientID},
                  "client_secret":   {m.ClientSecret},
              })
              if err != nil {
                  return nil
              }
              defer resp.Body.Close()

              var result struct {
                  Active bool   `json:"active"`
                  Sub    string `json:"sub"`
                  Scope  string `json:"scope"`
              }
              json.NewDecoder(resp.Body).Decode(&result)

              if !result.Active {
                  return nil
              }

              userCtx := &UserContext{
                  UserID:     result.Sub,
                  Roles:      strings.Split(result.Scope, " "),
                  AuthMethod: "oauth2",
              }

              m.TokenCache.Add(token, userCtx)
              return userCtx
          }
      pitfalls:
      - Caching tokens too long misses revocations
      - Not validating token audience/issuer
      - API keys in logs or error messages
      - Auth failures not rate limited (brute force)
      concepts:
      - Centralized authentication
      - Token introspection
      - Auth caching
      - Header propagation
      estimated_hours: 10-15
      deliverables:
      - JWT validation middleware that verifies token signature, expiration, and required claims
      - API key authentication that looks up and validates keys from a header or query parameter
      - OAuth2 token introspection that verifies opaque tokens against the authorization server
      - Rate limiting per client that enforces request quotas based on API key or JWT subject
    - id: 4
      name: Observability & Plugins
      description: Add logging, metrics, tracing, and plugin system.
      acceptance_criteria:
      - Structured access logs include method, path, status code, response time, and client IP for every request
      - Prometheus-compatible metrics endpoint exposes request rate, latency percentiles, and error counts
      - Distributed tracing propagates OpenTelemetry trace-id and span-id headers through the gateway to backends
      - Plugin architecture allows loading custom middleware modules without modifying the gateway core code
      - Dynamic configuration reload applies routing and plugin changes without restarting the gateway process
      hints:
        level1: Wrap handlers with logging/metrics middleware. Use OpenTelemetry SDK.
        level2: Plugin = interface with hooks. Load plugins at startup or dynamically.
        level3: |-
          import (
              "github.com/prometheus/client_golang/prometheus"
              "go.opentelemetry.io/otel"
              "go.opentelemetry.io/otel/trace"
          )

          // Metrics
          var (
              requestsTotal = prometheus.NewCounterVec(
                  prometheus.CounterOpts{
                      Name: "gateway_requests_total",
                      Help: "Total HTTP requests",
                  },
                  []string{"method", "path", "status"},
              )

              requestDuration = prometheus.NewHistogramVec(
                  prometheus.HistogramOpts{
                      Name:    "gateway_request_duration_seconds",
                      Help:    "Request duration in seconds",
                      Buckets: []float64{.001, .005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10},
                  },
                  []string{"method", "path"},
              )
          )

          func MetricsMiddleware(next http.Handler) http.Handler {
              return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
                  start := time.Now()

                  wrapped := &statusRecorder{ResponseWriter: w, status: 200}
                  next.ServeHTTP(wrapped, r)

                  duration := time.Since(start).Seconds()
                  path := sanitizePath(r.URL.Path)  // Avoid cardinality explosion

                  requestsTotal.WithLabelValues(r.Method, path, strconv.Itoa(wrapped.status)).Inc()
                  requestDuration.WithLabelValues(r.Method, path).Observe(duration)
              })
          }

          // Tracing
          func TracingMiddleware(next http.Handler) http.Handler {
              tracer := otel.Tracer("gateway")

              return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
                  ctx, span := tracer.Start(r.Context(), r.URL.Path,
                      trace.WithAttributes(
                          attribute.String("http.method", r.Method),
                          attribute.String("http.url", r.URL.String()),
                      ),
                  )
                  defer span.End()

                  // Propagate trace context to backends
                  r = r.WithContext(ctx)
                  otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(r.Header))

                  next.ServeHTTP(w, r)
              })
          }

          // Plugin system
          type Plugin interface {
              Name() string
              Init(config map[string]interface{}) error
              PreRequest(r *http.Request) error
              PostResponse(resp *http.Response) error
          }

          type PluginManager struct {
              plugins []Plugin
          }

          func (pm *PluginManager) Load(name string, config map[string]interface{}) error {
              // Dynamic loading (could use go plugins or scripting)
              var plugin Plugin

              switch name {
              case "cors":
                  plugin = &CORSPlugin{}
              case "request-id":
                  plugin = &RequestIDPlugin{}
              case "compression":
                  plugin = &CompressionPlugin{}
              default:
                  return fmt.Errorf("unknown plugin: %s", name)
              }

              if err := plugin.Init(config); err != nil {
                  return err
              }

              pm.plugins = append(pm.plugins, plugin)
              return nil
          }

          func (pm *PluginManager) RunPreRequest(r *http.Request) error {
              for _, p := range pm.plugins {
                  if err := p.PreRequest(r); err != nil {
                      return err
                  }
              }
              return nil
          }
      pitfalls:
      - High cardinality labels in metrics
      - Logging sensitive data (passwords, tokens)
      - Trace sampling too aggressive loses important traces
      - Plugin panics crash entire gateway
      concepts:
      - Observability
      - Prometheus metrics
      - OpenTelemetry tracing
      - Plugin architecture
      estimated_hours: 12-18
      deliverables:
      - Request and response logging middleware that captures method, path, status, and latency
      - Metrics collection module exporting request count, latency histograms, and error rates
      - Distributed tracing header propagation that injects and forwards trace context (W3C or B3 format)
      - Plugin system allowing custom middleware to be loaded and chained at runtime without recompilation
  grpc-service:
    id: grpc-service
    name: gRPC Microservice
    description: Build a gRPC service with streaming, error handling, and interceptors.
    difficulty: intermediate
    estimated_hours: 25-40
    prerequisites:
    - Protocol Buffers
    - RPC concepts
    - Go or similar
    languages:
      recommended:
      - Go
      - Rust
      also_possible:
      - Java
      - Python
      - C++
    resources:
    - name: gRPC Official Docs
      url: https://grpc.io/docs/
      type: documentation
    - name: Protocol Buffers
      url: https://developers.google.com/protocol-buffers
      type: documentation
    milestones:
    - id: 1
      name: Proto Definition & Code Generation
      description: Define service contract with Protocol Buffers.
      acceptance_criteria:
      - Define message types with proper field numbering and type annotations
      - Define service methods including unary, server streaming, and bidirectional streaming
      - Generate server and client code from proto definitions using protoc compiler
      - Version proto files properly with package namespacing and backward compatibility
      - Document APIs with proto comments that generate readable documentation
      hints:
        level1: proto3 syntax. Use well-known types (Timestamp, Duration). Generate with protoc.
        level2: 'Package versioning: myservice.v1.MyService. Never break backwards compatibility.'
        level3: |-
          syntax = "proto3";

          package userservice.v1;

          import "google/protobuf/timestamp.proto";
          import "google/protobuf/empty.proto";

          option go_package = "github.com/example/userservice/v1;userservicev1";

          // UserService manages user accounts
          service UserService {
            // Creates a new user account
            rpc CreateUser(CreateUserRequest) returns (User);

            // Gets a user by ID
            rpc GetUser(GetUserRequest) returns (User);

            // Lists users with pagination
            rpc ListUsers(ListUsersRequest) returns (ListUsersResponse);

            // Watches for user updates (server streaming)
            rpc WatchUsers(WatchUsersRequest) returns (stream UserEvent);

            // Bulk import users (client streaming)
            rpc ImportUsers(stream ImportUserRequest) returns (ImportUsersResponse);

            // Chat (bidirectional streaming)
            rpc Chat(stream ChatMessage) returns (stream ChatMessage);
          }

          message User {
            string id = 1;
            string email = 2;
            string name = 3;
            UserStatus status = 4;
            google.protobuf.Timestamp created_at = 5;
            google.protobuf.Timestamp updated_at = 6;

            // Nested type for profile
            Profile profile = 7;

            message Profile {
              string bio = 1;
              string avatar_url = 2;
            }
          }

          enum UserStatus {
            USER_STATUS_UNSPECIFIED = 0;
            USER_STATUS_ACTIVE = 1;
            USER_STATUS_SUSPENDED = 2;
            USER_STATUS_DELETED = 3;
          }

          message CreateUserRequest {
            string email = 1;
            string name = 2;
            string password = 3;
          }

          message GetUserRequest {
            string id = 1;
          }

          message ListUsersRequest {
            int32 page_size = 1;
            string page_token = 2;
            string filter = 3;  // e.g., "status=active"
          }

          message ListUsersResponse {
            repeated User users = 1;
            string next_page_token = 2;
            int32 total_count = 3;
          }

          message UserEvent {
            enum EventType {
              EVENT_TYPE_UNSPECIFIED = 0;
              EVENT_TYPE_CREATED = 1;
              EVENT_TYPE_UPDATED = 2;
              EVENT_TYPE_DELETED = 3;
            }

            EventType type = 1;
            User user = 2;
            google.protobuf.Timestamp occurred_at = 3;
          }

          // Generate: protoc --go_out=. --go-grpc_out=. user.proto
      pitfalls:
      - Changing field numbers breaks compatibility
      - Using required fields (proto3 doesn't have them)
      - Not setting default enum value to UNSPECIFIED
      - Large messages exceed gRPC size limits
      concepts:
      - Protocol Buffers
      - Service definition
      - Code generation
      - API versioning
      estimated_hours: 4-6
      deliverables:
      - Protocol buffer definition with message and service declarations
      - Code generation setup using protoc with language-specific plugins
      - Message types with typed fields and nested structures
      - Service definition with unary and streaming RPC method declarations
    - id: 2
      name: Server Implementation
      description: Implement gRPC server with all RPC types.
      acceptance_criteria:
      - Unary RPC implementation handles single request-response operations correctly
      - Server streaming RPC sends multiple responses for a single client request
      - Client streaming RPC receives multiple messages and returns single response
      - Bidirectional streaming handles concurrent send and receive message flows
      - Graceful shutdown completes in-flight requests before stopping the server
      hints:
        level1: Implement the generated interface. For streaming, use Send() and Recv() loops.
        level2: Handle context cancellation. Return proper gRPC status codes.
        level3: |-
          package main

          import (
              "context"
              "io"
              "sync"

              pb "github.com/example/userservice/v1"
              "google.golang.org/grpc"
              "google.golang.org/grpc/codes"
              "google.golang.org/grpc/status"
          )

          type userServer struct {
              pb.UnimplementedUserServiceServer
              users    map[string]*pb.User
              mu       sync.RWMutex
              watchers map[chan *pb.UserEvent]struct{}
          }

          func (s *userServer) CreateUser(ctx context.Context, req *pb.CreateUserRequest) (*pb.User, error) {
              // Validate
              if req.Email == "" {
                  return nil, status.Error(codes.InvalidArgument, "email is required")
              }

              s.mu.Lock()
              defer s.mu.Unlock()

              // Check duplicate
              for _, u := range s.users {
                  if u.Email == req.Email {
                      return nil, status.Error(codes.AlreadyExists, "email already registered")
                  }
              }

              user := &pb.User{
                  Id:        uuid.New().String(),
                  Email:     req.Email,
                  Name:      req.Name,
                  Status:    pb.UserStatus_USER_STATUS_ACTIVE,
                  CreatedAt: timestamppb.Now(),
              }

              s.users[user.Id] = user

              // Notify watchers
              s.notify(&pb.UserEvent{
                  Type:       pb.UserEvent_EVENT_TYPE_CREATED,
                  User:       user,
                  OccurredAt: timestamppb.Now(),
              })

              return user, nil
          }

          func (s *userServer) GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.User, error) {
              s.mu.RLock()
              defer s.mu.RUnlock()

              user, ok := s.users[req.Id]
              if !ok {
                  return nil, status.Error(codes.NotFound, "user not found")
              }

              return user, nil
          }

          // Server streaming
          func (s *userServer) WatchUsers(req *pb.WatchUsersRequest, stream pb.UserService_WatchUsersServer) error {
              ch := make(chan *pb.UserEvent, 10)

              s.mu.Lock()
              s.watchers[ch] = struct{}{}
              s.mu.Unlock()

              defer func() {
                  s.mu.Lock()
                  delete(s.watchers, ch)
                  s.mu.Unlock()
                  close(ch)
              }()

              for {
                  select {
                  case event := <-ch:
                      if err := stream.Send(event); err != nil {
                          return err
                      }
                  case <-stream.Context().Done():
                      return stream.Context().Err()
                  }
              }
          }

          // Client streaming
          func (s *userServer) ImportUsers(stream pb.UserService_ImportUsersServer) error {
              var count int32

              for {
                  req, err := stream.Recv()
                  if err == io.EOF {
                      return stream.SendAndClose(&pb.ImportUsersResponse{
                          ImportedCount: count,
                      })
                  }
                  if err != nil {
                      return err
                  }

                  // Process each user
                  _, err = s.CreateUser(stream.Context(), &pb.CreateUserRequest{
                      Email: req.Email,
                      Name:  req.Name,
                  })
                  if err == nil {
                      count++
                  }
              }
          }

          // Bidirectional streaming
          func (s *userServer) Chat(stream pb.UserService_ChatServer) error {
              for {
                  msg, err := stream.Recv()
                  if err == io.EOF {
                      return nil
                  }
                  if err != nil {
                      return err
                  }

                  // Echo back (in real app, broadcast to other participants)
                  response := &pb.ChatMessage{
                      UserId:    msg.UserId,
                      Content:   "Received: " + msg.Content,
                      Timestamp: timestamppb.Now(),
                  }

                  if err := stream.Send(response); err != nil {
                      return err
                  }
              }
          }

          func main() {
              lis, _ := net.Listen("tcp", ":50051")

              srv := grpc.NewServer()
              pb.RegisterUserServiceServer(srv, &userServer{
                  users:    make(map[string]*pb.User),
                  watchers: make(map[chan *pb.UserEvent]struct{}),
              })

              // Graceful shutdown
              go func() {
                  sigCh := make(chan os.Signal, 1)
                  signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)
                  <-sigCh
                  srv.GracefulStop()
              }()

              srv.Serve(lis)
          }
      pitfalls:
      - Blocking in streaming without timeout
      - Not handling context cancellation
      - Forgetting UnimplementedServer for forward compatibility
      - Memory leaks from unclosed streams
      concepts:
      - gRPC server
      - Streaming RPCs
      - Graceful shutdown
      - Status codes
      estimated_hours: 8-12
      deliverables:
      - gRPC server setup with listener and service registration
      - Service implementation handling all defined RPC method types
      - Request handling with input validation and business logic
      - Error responses using proper gRPC status codes and messages
    - id: 3
      name: Interceptors & Middleware
      description: Add cross-cutting concerns with interceptors.
      acceptance_criteria:
      - Logging interceptor records method name, duration, and status code for each RPC
      - Authentication interceptor validates bearer tokens from gRPC metadata headers
      - Rate limiting interceptor throttles requests exceeding per-client limits
      - Error handling and recovery interceptor catches panics and returns Internal status
      - Request and response validation interceptor checks message constraints before processing
      hints:
        level1: UnaryInterceptor and StreamInterceptor. Chain multiple with grpc.ChainUnaryInterceptor.
        level2: Extract metadata (headers) with metadata.FromIncomingContext(ctx).
        level3: |-
          import (
              "google.golang.org/grpc"
              "google.golang.org/grpc/metadata"
              "google.golang.org/grpc/codes"
              "google.golang.org/grpc/status"
          )

          // Logging interceptor
          func LoggingInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
              start := time.Now()

              // Extract metadata
              md, _ := metadata.FromIncomingContext(ctx)
              requestID := md.Get("x-request-id")

              log.Printf("[%s] %s started", requestID, info.FullMethod)

              resp, err := handler(ctx, req)

              duration := time.Since(start)
              code := codes.OK
              if err != nil {
                  code = status.Code(err)
              }

              log.Printf("[%s] %s completed in %v with code %s",
                  requestID, info.FullMethod, duration, code)

              return resp, err
          }

          // Auth interceptor
          func AuthInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
              // Skip auth for certain methods
              if info.FullMethod == "/userservice.v1.UserService/Health" {
                  return handler(ctx, req)
              }

              md, ok := metadata.FromIncomingContext(ctx)
              if !ok {
                  return nil, status.Error(codes.Unauthenticated, "missing metadata")
              }

              tokens := md.Get("authorization")
              if len(tokens) == 0 {
                  return nil, status.Error(codes.Unauthenticated, "missing token")
              }

              // Validate token
              userID, err := validateToken(tokens[0])
              if err != nil {
                  return nil, status.Error(codes.Unauthenticated, "invalid token")
              }

              // Add user to context
              ctx = context.WithValue(ctx, "user_id", userID)

              return handler(ctx, req)
          }

          // Recovery interceptor
          func RecoveryInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) {
              defer func() {
                  if r := recover(); r != nil {
                      log.Printf("panic recovered: %v\n%s", r, debug.Stack())
                      err = status.Error(codes.Internal, "internal error")
                  }
              }()

              return handler(ctx, req)
          }

          // Validation interceptor
          func ValidationInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
              // Check if request implements Validate()
              if v, ok := req.(interface{ Validate() error }); ok {
                  if err := v.Validate(); err != nil {
                      return nil, status.Error(codes.InvalidArgument, err.Error())
                  }
              }

              return handler(ctx, req)
          }

          // Stream interceptor for server streaming
          func StreamLoggingInterceptor(srv interface{}, ss grpc.ServerStream, info *grpc.StreamServerInfo, handler grpc.StreamHandler) error {
              start := time.Now()
              log.Printf("%s stream started", info.FullMethod)

              err := handler(srv, &wrappedStream{ss})

              log.Printf("%s stream ended after %v", info.FullMethod, time.Since(start))
              return err
          }

          type wrappedStream struct {
              grpc.ServerStream
          }

          func (w *wrappedStream) SendMsg(m interface{}) error {
              log.Printf("Sending message: %T", m)
              return w.ServerStream.SendMsg(m)
          }

          func (w *wrappedStream) RecvMsg(m interface{}) error {
              err := w.ServerStream.RecvMsg(m)
              log.Printf("Received message: %T", m)
              return err
          }

          // Server setup with chained interceptors
          func main() {
              srv := grpc.NewServer(
                  grpc.ChainUnaryInterceptor(
                      RecoveryInterceptor,
                      LoggingInterceptor,
                      AuthInterceptor,
                      ValidationInterceptor,
                  ),
                  grpc.ChainStreamInterceptor(
                      StreamLoggingInterceptor,
                  ),
              )
          }
      pitfalls:
      - Interceptor order matters (auth before logging?)
      - Recovery interceptor not catching all panics
      - Context values lost in interceptor chain
      - Stream interceptors more complex than unary
      concepts:
      - gRPC interceptors
      - Middleware pattern
      - Context propagation
      - Error handling
      estimated_hours: 6-10
      deliverables:
      - Server interceptors for unary and streaming RPC types
      - Logging interceptor recording method, duration, and status for each call
      - Authentication interceptor validating tokens from request metadata
      - Metrics interceptor collecting latency and error rate per RPC method
    - id: 4
      name: Client & Testing
      description: Build robust client and comprehensive tests.
      acceptance_criteria:
      - Client retries failed requests with configurable exponential backoff delays
      - Connection pooling reuses gRPC connections across multiple client calls
      - Deadline and timeout handling cancels requests exceeding configured duration
      - Unit tests use mock server to verify client behavior without real network calls
      - Integration tests verify full request-response flow with in-process gRPC server
      hints:
        level1: Use grpc.WithDefaultServiceConfig for retry policy. Set deadlines with context.
        level2: Mock server for unit tests. Bufconn for in-process testing.
        level3: |-
          import (
              "google.golang.org/grpc"
              "google.golang.org/grpc/credentials/insecure"
              "google.golang.org/grpc/backoff"
          )

          // Client with retry
          func NewUserServiceClient(target string) (pb.UserServiceClient, error) {
              retryPolicy := `{
                  "methodConfig": [{
                      "name": [{"service": "userservice.v1.UserService"}],
                      "retryPolicy": {
                          "maxAttempts": 3,
                          "initialBackoff": "0.1s",
                          "maxBackoff": "1s",
                          "backoffMultiplier": 2,
                          "retryableStatusCodes": ["UNAVAILABLE", "RESOURCE_EXHAUSTED"]
                      }
                  }]
              }`

              conn, err := grpc.Dial(target,
                  grpc.WithTransportCredentials(insecure.NewCredentials()),
                  grpc.WithDefaultServiceConfig(retryPolicy),
                  grpc.WithConnectParams(grpc.ConnectParams{
                      Backoff: backoff.Config{
                          BaseDelay:  100 * time.Millisecond,
                          Multiplier: 1.6,
                          MaxDelay:   10 * time.Second,
                      },
                      MinConnectTimeout: 5 * time.Second,
                  }),
              )
              if err != nil {
                  return nil, err
              }

              return pb.NewUserServiceClient(conn), nil
          }

          // Client wrapper with deadline
          type UserClient struct {
              client  pb.UserServiceClient
              timeout time.Duration
          }

          func (c *UserClient) GetUser(ctx context.Context, id string) (*pb.User, error) {
              ctx, cancel := context.WithTimeout(ctx, c.timeout)
              defer cancel()

              return c.client.GetUser(ctx, &pb.GetUserRequest{Id: id})
          }

          // Testing with bufconn
          import (
              "google.golang.org/grpc/test/bufconn"
              "testing"
          )

          const bufSize = 1024 * 1024

          var lis *bufconn.Listener

          func init() {
              lis = bufconn.Listen(bufSize)
              srv := grpc.NewServer()
              pb.RegisterUserServiceServer(srv, &userServer{
                  users: make(map[string]*pb.User),
              })
              go srv.Serve(lis)
          }

          func bufDialer(context.Context, string) (net.Conn, error) {
              return lis.Dial()
          }

          func TestCreateUser(t *testing.T) {
              ctx := context.Background()
              conn, err := grpc.DialContext(ctx, "bufnet",
                  grpc.WithContextDialer(bufDialer),
                  grpc.WithTransportCredentials(insecure.NewCredentials()),
              )
              if err != nil {
                  t.Fatalf("Failed to dial: %v", err)
              }
              defer conn.Close()

              client := pb.NewUserServiceClient(conn)

              resp, err := client.CreateUser(ctx, &pb.CreateUserRequest{
                  Email: "test@example.com",
                  Name:  "Test User",
              })

              if err != nil {
                  t.Fatalf("CreateUser failed: %v", err)
              }

              if resp.Email != "test@example.com" {
                  t.Errorf("Expected email test@example.com, got %s", resp.Email)
              }
          }

          // Mock for unit testing
          type mockUserServiceClient struct {
              pb.UserServiceClient
              getUser func(ctx context.Context, req *pb.GetUserRequest) (*pb.User, error)
          }

          func (m *mockUserServiceClient) GetUser(ctx context.Context, req *pb.GetUserRequest, opts ...grpc.CallOption) (*pb.User, error) {
              return m.getUser(ctx, req)
          }

          func TestBusinessLogic(t *testing.T) {
              mock := &mockUserServiceClient{
                  getUser: func(ctx context.Context, req *pb.GetUserRequest) (*pb.User, error) {
                      return &pb.User{Id: req.Id, Name: "Mocked"}, nil
                  },
              }

              // Test code that uses the client
              service := NewMyService(mock)
              result, err := service.DoSomething("user-123")
              // assertions...
          }
      pitfalls:
      - Not setting deadline causes hanging requests
      - Retry on non-idempotent methods causes duplicates
      - Connection not reused (creating new per request)
      - Tests not cleaning up connections
      concepts:
      - gRPC client
      - Retry policies
      - Connection management
      - Testing strategies
      estimated_hours: 6-10
      deliverables:
      - gRPC client setup with connection management and configuration
      - Client calls with retry policy and exponential backoff on failures
      - Integration tests verifying end-to-end RPC behavior with real server
      - Mock server for unit testing client code without network dependency
  circuit-breaker:
    id: circuit-breaker
    name: Circuit Breaker Pattern
    description: Implement circuit breaker for resilient microservices communication.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Microservices basics
    - Concurrency
    languages:
      recommended:
      - Go
      - Java
      also_possible:
      - Python
      - TypeScript
    resources:
    - name: Circuit Breaker Pattern
      url: https://martinfowler.com/bliki/CircuitBreaker.html
      type: article
    - name: Hystrix
      url: https://github.com/Netflix/Hystrix/wiki
      type: reference
    milestones:
    - id: 1
      name: Basic Circuit Breaker
      description: Implement closed, open, half-open states.
      acceptance_criteria:
      - Closed state passes requests through to downstream service normally
      - Open state fails all requests immediately without calling downstream
      - Half-open state allows limited test requests through to probe recovery
      - State transitions occur based on configurable failure threshold count
      - Thread-safe implementation handles concurrent request processing correctly
      hints:
        level1: Track consecutive failures. Open circuit after threshold. Reset after timeout.
        level2: 'Half-open: allow one request, if success -> closed, if fail -> open again.'
        level3: |-
          import (
              "sync"
              "time"
          )

          type State int

          const (
              StateClosed State = iota
              StateOpen
              StateHalfOpen
          )

          type CircuitBreaker struct {
              name          string
              maxFailures   int
              timeout       time.Duration
              halfOpenMax   int

              state         State
              failures      int
              successes     int
              lastFailure   time.Time
              halfOpenCount int
              mu            sync.RWMutex
          }

          func NewCircuitBreaker(name string, maxFailures int, timeout time.Duration) *CircuitBreaker {
              return &CircuitBreaker{
                  name:        name,
                  maxFailures: maxFailures,
                  timeout:     timeout,
                  halfOpenMax: 3,
                  state:       StateClosed,
              }
          }

          func (cb *CircuitBreaker) Execute(fn func() error) error {
              if !cb.allowRequest() {
                  return ErrCircuitOpen
              }

              err := fn()

              cb.recordResult(err)
              return err
          }

          func (cb *CircuitBreaker) allowRequest() bool {
              cb.mu.Lock()
              defer cb.mu.Unlock()

              switch cb.state {
              case StateClosed:
                  return true

              case StateOpen:
                  if time.Since(cb.lastFailure) > cb.timeout {
                      cb.state = StateHalfOpen
                      cb.halfOpenCount = 0
                      cb.successes = 0
                      return true
                  }
                  return false

              case StateHalfOpen:
                  if cb.halfOpenCount < cb.halfOpenMax {
                      cb.halfOpenCount++
                      return true
                  }
                  return false
              }

              return false
          }

          func (cb *CircuitBreaker) recordResult(err error) {
              cb.mu.Lock()
              defer cb.mu.Unlock()

              if err != nil {
                  cb.onFailure()
              } else {
                  cb.onSuccess()
              }
          }

          func (cb *CircuitBreaker) onFailure() {
              cb.failures++
              cb.lastFailure = time.Now()

              switch cb.state {
              case StateClosed:
                  if cb.failures >= cb.maxFailures {
                      cb.state = StateOpen
                  }
              case StateHalfOpen:
                  cb.state = StateOpen
              }
          }

          func (cb *CircuitBreaker) onSuccess() {
              switch cb.state {
              case StateClosed:
                  cb.failures = 0
              case StateHalfOpen:
                  cb.successes++
                  if cb.successes >= cb.halfOpenMax {
                      cb.state = StateClosed
                      cb.failures = 0
                  }
              }
          }

          var ErrCircuitOpen = errors.New("circuit breaker is open")
      pitfalls:
      - Race conditions without proper locking
      - Not resetting failure count on success
      - Half-open allows too many requests
      - Timer not being reset properly
      concepts:
      - Circuit breaker states
      - Failure detection
      - Recovery testing
      - Thread safety
      estimated_hours: 5-8
      deliverables:
      - State machine with closed, open, and half-open states
      - Failure threshold tracking with configurable limits
      - Configurable timeout duration for open state
      - Success threshold counter for recovery transitions
    - id: 2
      name: Advanced Features
      description: Add sliding window, metrics, and fallbacks.
      acceptance_criteria:
      - Sliding window calculates failure rate over configurable time period
      - Configurable error classification distinguishes transient from permanent failures
      - Fallback function executes and returns alternative response when circuit is open
      - Metrics expose current state, failure count, and success rate for observability
      - Bulkhead enforces concurrency limit per downstream service independently
      hints:
        level1: Use ring buffer for sliding window. Classify errors (timeout vs 5xx).
        level2: 'Fallback: return cached/default value. Bulkhead: limit concurrent calls.'
        level3: |-
          type SlidingWindow struct {
              size    int
              buckets []bucket
              current int
              mu      sync.Mutex
          }

          type bucket struct {
              successes int
              failures  int
              timestamp time.Time
          }

          func NewSlidingWindow(size int, bucketDuration time.Duration) *SlidingWindow {
              sw := &SlidingWindow{
                  size:    size,
                  buckets: make([]bucket, size),
              }

              // Rotate buckets periodically
              go func() {
                  ticker := time.NewTicker(bucketDuration)
                  for range ticker.C {
                      sw.rotate()
                  }
              }()

              return sw
          }

          func (sw *SlidingWindow) rotate() {
              sw.mu.Lock()
              defer sw.mu.Unlock()

              sw.current = (sw.current + 1) % sw.size
              sw.buckets[sw.current] = bucket{timestamp: time.Now()}
          }

          func (sw *SlidingWindow) RecordSuccess() {
              sw.mu.Lock()
              sw.buckets[sw.current].successes++
              sw.mu.Unlock()
          }

          func (sw *SlidingWindow) RecordFailure() {
              sw.mu.Lock()
              sw.buckets[sw.current].failures++
              sw.mu.Unlock()
          }

          func (sw *SlidingWindow) FailureRate() float64 {
              sw.mu.Lock()
              defer sw.mu.Unlock()

              var total, failures int
              for _, b := range sw.buckets {
                  total += b.successes + b.failures
                  failures += b.failures
              }

              if total == 0 {
                  return 0
              }
              return float64(failures) / float64(total)
          }

          // Enhanced circuit breaker
          type EnhancedCircuitBreaker struct {
              *CircuitBreaker
              window      *SlidingWindow
              fallback    func() (interface{}, error)
              bulkhead    chan struct{}
              classifier  func(error) bool  // true = should count as failure
              metrics     *Metrics
          }

          func (cb *EnhancedCircuitBreaker) ExecuteWithFallback(fn func() (interface{}, error)) (interface{}, error) {
              // Check bulkhead (concurrency limit)
              select {
              case cb.bulkhead <- struct{}{}:
                  defer func() { <-cb.bulkhead }()
              default:
                  cb.metrics.BulkheadRejected.Inc()
                  return cb.executeFallback(ErrBulkheadFull)
              }

              if !cb.allowRequest() {
                  cb.metrics.CircuitRejected.Inc()
                  return cb.executeFallback(ErrCircuitOpen)
              }

              start := time.Now()
              result, err := fn()
              duration := time.Since(start)

              cb.metrics.RequestDuration.Observe(duration.Seconds())

              // Classify error
              if err != nil && cb.classifier(err) {
                  cb.window.RecordFailure()
                  cb.metrics.Failures.Inc()

                  // Check if we should open circuit
                  if cb.window.FailureRate() > 0.5 {  // 50% threshold
                      cb.openCircuit()
                  }

                  return cb.executeFallback(err)
              }

              cb.window.RecordSuccess()
              cb.metrics.Successes.Inc()
              return result, nil
          }

          func (cb *EnhancedCircuitBreaker) executeFallback(originalErr error) (interface{}, error) {
              if cb.fallback != nil {
                  cb.metrics.FallbackExecuted.Inc()
                  return cb.fallback()
              }
              return nil, originalErr
          }

          // Error classifier
          func DefaultClassifier(err error) bool {
              // Ignore client errors, count server errors and timeouts
              var netErr net.Error
              if errors.As(err, &netErr) && netErr.Timeout() {
                  return true
              }

              var httpErr *HTTPError
              if errors.As(err, &httpErr) {
                  return httpErr.StatusCode >= 500
              }

              return true
          }
      pitfalls:
      - Sliding window bucket rotation timing issues
      - Fallback also failing (need fallback for fallback)
      - Bulkhead too small causes unnecessary rejections
      - Error classifier too aggressive
      concepts:
      - Sliding window
      - Fallback patterns
      - Bulkhead pattern
      - Error classification
      estimated_hours: 6-10
      deliverables:
      - Bulkhead pattern for concurrency isolation per service
      - Configurable fallback functions for graceful degradation
      - Metrics collection for circuit state and failure rates
      - Per-service configuration with independent circuit parameters
    - id: 3
      name: Integration & Testing
      description: Integrate with HTTP/gRPC clients and test chaos scenarios.
      acceptance_criteria:
      - HTTP client wrapper applies circuit breaker to outgoing requests automatically
      - gRPC interceptor applies circuit breaker to RPC calls transparently
      - Per-service circuit breakers operate independently without interference
      - Chaos testing injects failures and verifies circuit opens after threshold
      - Dashboard displays current circuit states for all registered services
      hints:
        level1: Wrap http.Client.Do(). Key circuit breaker by host or service name.
        level2: 'Chaos: randomly fail N% of requests. Test state transitions.'
        level3: |-
          // HTTP client with circuit breaker
          type ResilientHTTPClient struct {
              client   *http.Client
              breakers map[string]*EnhancedCircuitBreaker
              mu       sync.RWMutex
          }

          func (c *ResilientHTTPClient) getBreaker(host string) *EnhancedCircuitBreaker {
              c.mu.RLock()
              cb, ok := c.breakers[host]
              c.mu.RUnlock()

              if ok {
                  return cb
              }

              c.mu.Lock()
              defer c.mu.Unlock()

              // Double check
              if cb, ok = c.breakers[host]; ok {
                  return cb
              }

              cb = NewEnhancedCircuitBreaker(host, Config{
                  MaxFailures:   5,
                  Timeout:       30 * time.Second,
                  HalfOpenMax:   3,
                  BulkheadSize:  100,
              })
              c.breakers[host] = cb
              return cb
          }

          func (c *ResilientHTTPClient) Do(req *http.Request) (*http.Response, error) {
              cb := c.getBreaker(req.Host)

              result, err := cb.ExecuteWithFallback(func() (interface{}, error) {
                  return c.client.Do(req)
              })

              if err != nil {
                  return nil, err
              }
              return result.(*http.Response), nil
          }

          // gRPC interceptor
          func CircuitBreakerInterceptor(breakers map[string]*EnhancedCircuitBreaker) grpc.UnaryClientInterceptor {
              return func(ctx context.Context, method string, req, reply interface{},
                          cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {

                  // Get service name from method
                  service := strings.Split(method, "/")[1]
                  cb := breakers[service]

                  _, err := cb.ExecuteWithFallback(func() (interface{}, error) {
                      return nil, invoker(ctx, method, req, reply, cc, opts...)
                  })

                  return err
              }
          }

          // Chaos testing
          type ChaosMiddleware struct {
              failureRate float64
              latencyMs   int
              enabled     bool
          }

          func (c *ChaosMiddleware) Wrap(next http.Handler) http.Handler {
              return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
                  if !c.enabled {
                      next.ServeHTTP(w, r)
                      return
                  }

                  // Inject latency
                  if c.latencyMs > 0 {
                      time.Sleep(time.Duration(c.latencyMs) * time.Millisecond)
                  }

                  // Inject failures
                  if rand.Float64() < c.failureRate {
                      http.Error(w, "Chaos: injected failure", http.StatusInternalServerError)
                      return
                  }

                  next.ServeHTTP(w, r)
              })
          }

          // Tests
          func TestCircuitOpensOnFailures(t *testing.T) {
              cb := NewCircuitBreaker("test", 3, 1*time.Second)

              // 3 failures should open circuit
              for i := 0; i < 3; i++ {
                  cb.Execute(func() error {
                      return errors.New("fail")
                  })
              }

              // Next call should fail immediately
              err := cb.Execute(func() error {
                  t.Fatal("Should not execute")
                  return nil
              })

              if !errors.Is(err, ErrCircuitOpen) {
                  t.Errorf("Expected ErrCircuitOpen, got %v", err)
              }
          }

          func TestCircuitRecovery(t *testing.T) {
              cb := NewCircuitBreaker("test", 1, 100*time.Millisecond)

              // Open circuit
              cb.Execute(func() error { return errors.New("fail") })

              // Wait for timeout
              time.Sleep(150 * time.Millisecond)

              // Should be half-open now, success should close
              err := cb.Execute(func() error { return nil })
              if err != nil {
                  t.Errorf("Expected success, got %v", err)
              }
          }
      pitfalls:
      - Chaos testing in production without safeguards
      - Circuit breaker per-request instead of per-service
      - Not exposing circuit state for debugging
      - Test flakiness due to timing
      concepts:
      - Client integration
      - Chaos engineering
      - Per-service isolation
      - Observability
      estimated_hours: 6-10
      deliverables:
      - HTTP client integration with automatic circuit breaker wrapping
      - Decorator or wrapper pattern for transparent circuit breaker application
      - Integration tests verifying end-to-end circuit breaker behavior
      - Chaos testing framework for injecting controlled failures
  websocket-server:
    name: WebSocket Server
    description: Build a WebSocket server from scratch supporting the RFC 6455 protocol with connection management, heartbeats,
      and message framing.
    why_important: WebSockets enable real-time bidirectional communication essential for chat apps, live updates, gaming,
      and collaborative tools.
    difficulty: intermediate
    tags:
    - networking
    - real-time
    - protocols
    estimated_hours: 30
    prerequisites:
    - tcp-server
    learning_outcomes:
    - Master WebSocket handshake and frame parsing
    - Implement connection lifecycle management
    - Handle binary and text message types
    - Build heartbeat/ping-pong mechanisms
    milestones:
    - name: HTTP Upgrade Handshake
      description: Implement WebSocket handshake by parsing HTTP upgrade request and computing Sec-WebSocket-Accept using
        SHA-1 and Base64.
      hints:
        level1: Parse HTTP headers and validate Upgrade request.
        level2: Concatenate client key with magic GUID, SHA-1 hash, Base64 encode.
        level3: |-
          ```python
          import hashlib
          import base64

          MAGIC_GUID = "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"

          def compute_accept_key(client_key: str) -> str:
              combined = client_key.strip() + MAGIC_GUID
              sha1_hash = hashlib.sha1(combined.encode()).digest()
              return base64.b64encode(sha1_hash).decode()

          def parse_upgrade_request(data: bytes) -> dict:
              lines = data.decode().split('\r\n')
              headers = {}
              for line in lines[1:]:
                  if ': ' in line:
                      key, value = line.split(': ', 1)
                      headers[key.lower()] = value
              return headers

          def create_handshake_response(client_key: str) -> bytes:
              accept = compute_accept_key(client_key)
              return (
                  "HTTP/1.1 101 Switching Protocols\r\n"
                  "Upgrade: websocket\r\n"
                  "Connection: Upgrade\r\n"
                  f"Sec-WebSocket-Accept: {accept}\r\n"
                  "\r\n"
              ).encode()
          ```
      pitfalls:
      - Missing CRLF at end of headers causes handshake failure
      - Case-sensitive header parsing breaks with some clients
      - Not validating WebSocket version header
      acceptance_criteria:
      - Parse HTTP upgrade request and extract all WebSocket-specific headers
      - Validate Sec-WebSocket-Key header is present and base64-encoded correctly
      - Generate Sec-WebSocket-Accept response using SHA-1 hash of key plus magic GUID
      - Complete protocol upgrade by sending 101 response and switching to WebSocket framing
      deliverables:
      - 'Upgrade request detection identifying HTTP requests with Connection: Upgrade header'
      - Sec-WebSocket-Key processing extracting and validating the client's handshake key
      - Accept header generation computing the SHA-1 hash of key concatenated with the magic GUID
      - Protocol upgrade response sending 101 Switching Protocols with correct WebSocket headers
    - name: Frame Parsing
      description: Parse WebSocket frames including opcode, masking, and payload length handling for small and large messages.
      hints:
        level1: Read first 2 bytes for FIN, opcode, mask bit, and initial length.
        level2: Handle 7-bit, 16-bit (126), and 64-bit (127) payload lengths.
        level3: |-
          ```python
          import struct
          from dataclasses import dataclass
          from enum import IntEnum

          class Opcode(IntEnum):
              CONTINUATION = 0x0
              TEXT = 0x1
              BINARY = 0x2
              CLOSE = 0x8
              PING = 0x9
              PONG = 0xA

          @dataclass
          class Frame:
              fin: bool
              opcode: Opcode
              payload: bytes

          def parse_frame(data: bytes) -> tuple[Frame, int]:
              if len(data) < 2:
                  raise ValueError("Incomplete frame")

              byte1, byte2 = data[0], data[1]
              fin = bool(byte1 & 0x80)
              opcode = Opcode(byte1 & 0x0F)
              masked = bool(byte2 & 0x80)
              length = byte2 & 0x7F

              offset = 2
              if length == 126:
                  length = struct.unpack('>H', data[2:4])[0]
                  offset = 4
              elif length == 127:
                  length = struct.unpack('>Q', data[2:10])[0]
                  offset = 10

              if masked:
                  mask = data[offset:offset+4]
                  offset += 4
                  payload = bytes(b ^ mask[i % 4] for i, b in enumerate(data[offset:offset+length]))
              else:
                  payload = data[offset:offset+length]

              return Frame(fin, opcode, payload), offset + length
          ```
      pitfalls:
      - Forgetting to unmask client frames (clients MUST mask)
      - Integer overflow with 64-bit payload lengths
      - Not handling fragmented messages (FIN=0)
      acceptance_criteria:
      - Parse WebSocket frame header including opcode and extended payload length
      - Handle continuation frames by buffering fragments until a FIN frame arrives
      - Unmask client frames by applying the 4-byte masking key via XOR operation
      - Support text and binary frame types with correct UTF-8 validation for text
      deliverables:
      - Frame header parser extracting opcode, mask bit, and payload length fields
      - Payload masking and unmasking applying the 4-byte XOR mask to frame data
      - Frame type dispatch routing text, binary, and control frames to appropriate handlers
      - Fragmented message assembly concatenating continuation frames into a complete message
    - name: Connection Management
      description: Manage multiple WebSocket connections with proper state tracking, broadcasting, and graceful disconnection.
      hints:
        level1: Track connections in a dictionary keyed by unique ID.
        level2: Use asyncio for concurrent connection handling.
        level3: |-
          ```python
          import asyncio
          from dataclasses import dataclass, field
          from typing import Callable

          @dataclass
          class Connection:
              id: str
              writer: asyncio.StreamWriter
              reader: asyncio.StreamReader
              is_alive: bool = True
              rooms: set = field(default_factory=set)

          class ConnectionManager:
              def __init__(self):
                  self.connections: dict[str, Connection] = {}
                  self.rooms: dict[str, set[str]] = {}  # room -> connection_ids

              async def add(self, conn: Connection):
                  self.connections[conn.id] = conn

              async def remove(self, conn_id: str):
                  if conn := self.connections.pop(conn_id, None):
                      for room in conn.rooms:
                          self.rooms.get(room, set()).discard(conn_id)
                      conn.writer.close()
                      await conn.writer.wait_closed()

              async def broadcast(self, message: bytes, exclude: str = None):
                  for conn_id, conn in self.connections.items():
                      if conn_id != exclude and conn.is_alive:
                          await self.send(conn_id, message)

              async def send(self, conn_id: str, message: bytes):
                  if conn := self.connections.get(conn_id):
                      frame = create_frame(message, Opcode.TEXT)
                      conn.writer.write(frame)
                      await conn.writer.drain()

              async def join_room(self, conn_id: str, room: str):
                  if conn := self.connections.get(conn_id):
                      conn.rooms.add(room)
                      self.rooms.setdefault(room, set()).add(conn_id)

              async def broadcast_to_room(self, room: str, message: bytes, exclude: str = None):
                  for conn_id in self.rooms.get(room, set()):
                      if conn_id != exclude:
                          await self.send(conn_id, message)
          ```
      pitfalls:
      - Not handling disconnections during broadcast causes cascade failures
      - Memory leak from not cleaning up closed connections
      - Race conditions when modifying connection dict during iteration
      acceptance_criteria:
      - Track active connections with unique identifiers and metadata
      - Handle connection state (OPEN, CLOSING, CLOSED)
      - Implement clean close handshake exchanging close frames before TCP teardown
      - Support connection metadata storing client info and custom attributes per session
      deliverables:
      - Connection state tracking maintaining lifecycle state for each active WebSocket session
      - Ping and pong frames for keep-alive detecting and maintaining idle connections
      - Close handshake implementation sending and receiving close frames with status codes
      - Connection timeout logic closing sessions that remain idle beyond a configured threshold
    - name: Ping/Pong Heartbeat
      description: Implement heartbeat mechanism using ping/pong frames to detect dead connections and maintain NAT mappings.
      hints:
        level1: Send ping frames periodically, expect pong responses.
        level2: Track last pong time, disconnect if no response within timeout.
        level3: |-
          ```python
          import asyncio
          import time

          class HeartbeatManager:
              def __init__(self, conn_manager: ConnectionManager,
                           ping_interval: float = 30.0,
                           pong_timeout: float = 10.0):
                  self.conn_manager = conn_manager
                  self.ping_interval = ping_interval
                  self.pong_timeout = pong_timeout
                  self.last_pong: dict[str, float] = {}
                  self._task = None

              def start(self):
                  self._task = asyncio.create_task(self._heartbeat_loop())

              def stop(self):
                  if self._task:
                      self._task.cancel()

              def record_pong(self, conn_id: str):
                  self.last_pong[conn_id] = time.monotonic()

              async def _heartbeat_loop(self):
                  while True:
                      await asyncio.sleep(self.ping_interval)
                      now = time.monotonic()
                      dead_connections = []

                      for conn_id, conn in self.conn_manager.connections.items():
                          last = self.last_pong.get(conn_id, now)
                          if now - last > self.ping_interval + self.pong_timeout:
                              dead_connections.append(conn_id)
                          else:
                              # Send ping
                              ping_frame = create_frame(b'', Opcode.PING)
                              try:
                                  conn.writer.write(ping_frame)
                                  await conn.writer.drain()
                              except Exception:
                                  dead_connections.append(conn_id)

                      for conn_id in dead_connections:
                          await self.conn_manager.remove(conn_id)
                          self.last_pong.pop(conn_id, None)
          ```
      pitfalls:
      - Using wall clock time fails with system time changes
      - Too aggressive ping interval wastes bandwidth
      - Not initializing last_pong on connect causes immediate disconnect
      acceptance_criteria:
      - Send periodic ping frames at the configured heartbeat interval
      - Handle pong responses by resetting the connection's liveness timer
      - Detect dead connections when pong is not received within the timeout window
      - Configure heartbeat interval and timeout threshold per server or per connection
      deliverables:
      - Ping frame sender dispatching periodic ping frames to each connected client
      - Pong frame response handler processing client pong replies and updating liveness state
      - Connection timeout detection marking connections as dead when pong is not received
      - Keep-alive interval configuration setting the time between consecutive ping frames
  multiplayer-game-server:
    name: Multiplayer Game Server
    description: Build a real-time multiplayer game server with authoritative state, client prediction, lag compensation,
      and anti-cheat.
    why_important: Game servers push real-time systems to their limits with strict latency requirements, making this excellent
      practice for performance-critical code.
    difficulty: advanced
    tags:
    - real-time
    - networking
    - game-dev
    - performance
    estimated_hours: 60
    prerequisites:
    - websocket-server
    - realtime-chat
    learning_outcomes:
    - Implement authoritative server architecture
    - Build client-side prediction and reconciliation
    - Handle lag compensation for fair gameplay
    - Design tick-based game loops
    milestones:
    - name: Game Loop & Tick System
      description: Implement a fixed timestep game loop that processes input, updates state, and broadcasts at consistent
        intervals.
      hints:
        level1: Use fixed delta time (e.g., 60 ticks/second) for deterministic simulation.
        level2: Accumulate time and process multiple ticks if behind.
        level3: |-
          ```python
          import asyncio
          import time
          from dataclasses import dataclass
          from typing import Callable

          @dataclass
          class GameState:
              tick: int = 0
              entities: dict = None

              def __post_init__(self):
                  self.entities = self.entities or {}

          class GameLoop:
              def __init__(self, tick_rate: int = 60):
                  self.tick_rate = tick_rate
                  self.tick_duration = 1.0 / tick_rate
                  self.state = GameState()
                  self.input_buffer: dict[int, list] = {}  # tick -> [inputs]
                  self.running = False
                  self._update_callbacks: list[Callable] = []
                  self._broadcast_callback: Callable = None

              def on_update(self, callback: Callable):
                  self._update_callbacks.append(callback)

              def set_broadcast(self, callback: Callable):
                  self._broadcast_callback = callback

              def queue_input(self, player_id: str, input_data: dict, client_tick: int):
                  """Queue player input for processing at specific tick."""
                  target_tick = max(client_tick, self.state.tick)
                  self.input_buffer.setdefault(target_tick, []).append({
                      'player_id': player_id,
                      'input': input_data,
                      'client_tick': client_tick
                  })

              async def start(self):
                  self.running = True
                  last_time = time.perf_counter()
                  accumulator = 0.0

                  while self.running:
                      current_time = time.perf_counter()
                      frame_time = current_time - last_time
                      last_time = current_time

                      accumulator += frame_time

                      # Process fixed timestep ticks
                      while accumulator >= self.tick_duration:
                          self._process_tick()
                          accumulator -= self.tick_duration

                      # Broadcast state
                      if self._broadcast_callback:
                          await self._broadcast_callback(self.state)

                      # Sleep to maintain tick rate
                      sleep_time = self.tick_duration - (time.perf_counter() - current_time)
                      if sleep_time > 0:
                          await asyncio.sleep(sleep_time)

              def _process_tick(self):
                  # Process inputs for this tick
                  inputs = self.input_buffer.pop(self.state.tick, [])
                  for input_data in inputs:
                      self._apply_input(input_data)

                  # Run update callbacks
                  for callback in self._update_callbacks:
                      callback(self.state, self.tick_duration)

                  self.state.tick += 1

              def _apply_input(self, input_data: dict):
                  player_id = input_data['player_id']
                  if entity := self.state.entities.get(player_id):
                      entity.apply_input(input_data['input'])
          ```
      pitfalls:
      - Variable delta time causes non-deterministic simulation
      - Sleeping exact tick duration ignores processing time
      - Unbounded accumulator causes spiral of death when behind
      acceptance_criteria:
      - Game loop runs at a fixed rate such as 60 ticks per second with consistent timing between ticks
      - All pending player inputs are processed and applied to the game state within each tick
      - Game state including positions, velocities, and health is updated deterministically each tick
      - Tick timing remains consistent under varying CPU load with compensation for processing delays
      deliverables:
      - Fixed timestep game loop that advances game state at a consistent configurable rate independent of frame time
      - Game state update processor that applies all pending player inputs and advances physics each tick
      - Tick rate configuration that allows adjusting the server simulation rate between 20 and 128 ticks per second
      - State snapshot system that captures the full game state at each tick for history and replay purposes
    - name: Client Prediction & Reconciliation
      description: Implement client-side prediction for responsive controls with server reconciliation to correct mispredictions.
      hints:
        level1: Client predicts movement locally, server is authoritative.
        level2: Store input history, replay from last acknowledged tick on correction.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          import copy

          @dataclass
          class InputFrame:
              tick: int
              input_data: dict
              predicted_state: dict

          @dataclass
          class ClientPrediction:
              entity_id: str
              current_state: dict
              pending_inputs: list[InputFrame] = field(default_factory=list)
              last_server_tick: int = 0

              def apply_input(self, tick: int, input_data: dict, physics):
                  """Apply input locally and store for reconciliation."""
                  # Predict new state
                  predicted = physics.simulate(self.current_state, input_data)

                  self.pending_inputs.append(InputFrame(
                      tick=tick,
                      input_data=input_data,
                      predicted_state=copy.deepcopy(predicted)
                  ))

                  self.current_state = predicted
                  return predicted

              def reconcile(self, server_tick: int, server_state: dict, physics):
                  """Reconcile with server state, replay unacknowledged inputs."""
                  # Remove acknowledged inputs
                  self.pending_inputs = [
                      inp for inp in self.pending_inputs if inp.tick > server_tick
                  ]
                  self.last_server_tick = server_tick

                  # Check if prediction matches
                  if not self.pending_inputs:
                      self.current_state = server_state
                      return False  # No correction needed

                  # Check first pending prediction against server
                  first_pending = self.pending_inputs[0]
                  if self._states_match(first_pending.predicted_state, server_state):
                      return False  # Prediction was correct

                  # Misprediction - replay from server state
                  self.current_state = copy.deepcopy(server_state)
                  for inp in self.pending_inputs:
                      self.current_state = physics.simulate(
                          self.current_state, inp.input_data
                      )
                      inp.predicted_state = copy.deepcopy(self.current_state)

                  return True  # Correction applied

              def _states_match(self, a: dict, b: dict, tolerance: float = 0.01) -> bool:
                  for key in ['x', 'y', 'z']:
                      if abs(a.get(key, 0) - b.get(key, 0)) > tolerance:
                          return False
                  return True

          # Server-side validation
          class AuthoritativeServer:
              def __init__(self):
                  self.player_states: dict[str, dict] = {}
                  self.physics = PhysicsEngine()

              def process_input(self, player_id: str, input_data: dict, client_tick: int):
                  state = self.player_states.get(player_id, {})

                  # Validate input (anti-cheat)
                  if not self._validate_input(input_data):
                      return None  # Reject invalid input

                  # Apply authoritative simulation
                  new_state = self.physics.simulate(state, input_data)
                  self.player_states[player_id] = new_state

                  return {
                      'tick': client_tick,
                      'state': new_state
                  }
          ```
      pitfalls:
      - Not storing enough input history causes rubber-banding
      - Exact float comparison fails due to precision
      - Overcorrecting causes jittery movement
      acceptance_criteria:
      - Client-side prediction applies player inputs locally to provide responsive movement without network latency
      - Server authoritative state is compared against client predicted state on each server update received
      - State discrepancies are reconciled by replaying unacknowledged inputs on top of the corrected server state
      - Prediction errors produce smooth visual corrections instead of jarring teleportation or snapping
      deliverables:
      - Client-side prediction engine that immediately applies local player inputs without waiting for server confirmation
      - Server authoritative state manager that maintains the canonical game state that overrides client predictions
      - State reconciliation logic that corrects the client's predicted state when it diverges from the server state
      - Smoothing correction applicator that interpolates between incorrect and correct states to avoid visual snapping
    - name: Lag Compensation
      description: Implement server-side lag compensation to make hit detection fair for high-latency players.
      hints:
        level1: Store history of game states, rewind to player's perceived time.
        level2: Interpolate between stored states for accurate reconstruction.
        level3: |-
          ```python
          from dataclasses import dataclass
          from collections import deque
          import copy

          @dataclass
          class StateSnapshot:
              tick: int
              timestamp: float
              entities: dict  # entity_id -> state

          class LagCompensation:
              def __init__(self, history_duration: float = 1.0, tick_rate: int = 60):
                  self.history: deque[StateSnapshot] = deque()
                  self.history_duration = history_duration
                  self.tick_duration = 1.0 / tick_rate
                  self.max_history = int(history_duration * tick_rate)

              def record(self, tick: int, timestamp: float, entities: dict):
                  snapshot = StateSnapshot(
                      tick=tick,
                      timestamp=timestamp,
                      entities=copy.deepcopy(entities)
                  )
                  self.history.append(snapshot)

                  # Prune old history
                  while len(self.history) > self.max_history:
                      self.history.popleft()

              def get_state_at_time(self, target_time: float) -> dict:
                  """Get interpolated entity states at specific time."""
                  if not self.history:
                      return {}

                  # Find surrounding snapshots
                  before = None
                  after = None

                  for snapshot in self.history:
                      if snapshot.timestamp <= target_time:
                          before = snapshot
                      else:
                          after = snapshot
                          break

                  if before is None:
                      return self.history[0].entities
                  if after is None:
                      return self.history[-1].entities

                  # Interpolate between snapshots
                  t = (target_time - before.timestamp) / (after.timestamp - before.timestamp)
                  return self._interpolate_entities(before.entities, after.entities, t)

              def _interpolate_entities(self, before: dict, after: dict, t: float) -> dict:
                  result = {}
                  for entity_id in before:
                      if entity_id in after:
                          result[entity_id] = self._interpolate_state(
                              before[entity_id], after[entity_id], t
                          )
                  return result

              def _interpolate_state(self, a: dict, b: dict, t: float) -> dict:
                  return {
                      'x': a['x'] + (b['x'] - a['x']) * t,
                      'y': a['y'] + (b['y'] - a['y']) * t,
                      'z': a.get('z', 0) + (b.get('z', 0) - a.get('z', 0)) * t,
                  }

          class HitDetection:
              def __init__(self, lag_comp: LagCompensation):
                  self.lag_comp = lag_comp

              def check_hit(self, shooter_id: str, target_id: str,
                            shot_origin: dict, shot_direction: dict,
                            client_timestamp: float, max_rewind: float = 0.2):
                  """Check if shot hits with lag compensation."""
                  # Limit rewind to prevent abuse
                  current_time = time.time()
                  rewind_time = min(
                      current_time - client_timestamp,
                      max_rewind
                  )

                  # Get world state at shooter's perceived time
                  perceived_time = current_time - rewind_time
                  world_state = self.lag_comp.get_state_at_time(perceived_time)

                  if target_id not in world_state:
                      return False

                  target_state = world_state[target_id]
                  return self._ray_intersects_hitbox(
                      shot_origin, shot_direction, target_state
                  )
          ```
      pitfalls:
      - Unlimited rewind allows shooting around corners
      - Memory grows unbounded without history pruning
      - Interpolation fails at entity spawn/despawn boundaries
      acceptance_criteria:
      - Server rewinds game state to the client's estimated render time for accurate retroactive hit detection
      - Client network latency is measured and accounted for when processing combat actions on the server
      - Entity interpolation displays remote players at smoothly interpolated positions between server updates
      - Fairness is maintained across clients with different latencies by capping maximum rewind window
      deliverables:
      - Server-side state rewind buffer that stores recent game state history for retroactive hit detection
      - Lag-compensated hit detection that validates shots against the game state at the time the client fired
      - Time synchronization protocol that estimates the clock offset between each client and the server
      - Entity interpolation renderer that smoothly displays remote entities between received state updates
    - name: State Synchronization
      description: Efficiently synchronize game state to clients using delta compression, interest management, and prioritization.
      hints:
        level1: Send only changed values, not full state.
        level2: Prioritize nearby entities, reduce updates for distant ones.
        level3: |-
          ```python
          from dataclasses import dataclass
          from typing import Optional
          import math

          @dataclass
          class EntityPriority:
              entity_id: str
              priority: float
              last_update_tick: int

          class StateSynchronizer:
              def __init__(self, tick_rate: int = 60):
                  self.tick_rate = tick_rate
                  self.client_states: dict[str, dict] = {}  # client_id -> last sent state per entity
                  self.update_rates: dict[float, int] = {
                      1.0: 1,    # Priority 1.0: every tick
                      0.5: 2,    # Priority 0.5: every 2 ticks
                      0.25: 4,   # Priority 0.25: every 4 ticks
                      0.1: 10,   # Priority 0.1: every 10 ticks
                  }

              def calculate_priority(self, viewer_state: dict, entity_state: dict,
                                    entity_type: str) -> float:
                  # Distance-based priority
                  dx = viewer_state['x'] - entity_state['x']
                  dy = viewer_state['y'] - entity_state['y']
                  distance = math.sqrt(dx*dx + dy*dy)

                  # Base priority from distance
                  if distance < 10:
                      priority = 1.0
                  elif distance < 50:
                      priority = 0.5
                  elif distance < 100:
                      priority = 0.25
                  else:
                      priority = 0.1

                  # Boost for important entity types
                  if entity_type == 'player':
                      priority = min(1.0, priority * 1.5)
                  elif entity_type == 'projectile':
                      priority = min(1.0, priority * 2.0)

                  return priority

              def generate_update(self, client_id: str, viewer_id: str,
                                 current_tick: int, world_state: dict) -> bytes:
                  client_last = self.client_states.setdefault(client_id, {})
                  viewer_state = world_state.get(viewer_id, {'x': 0, 'y': 0})

                  updates = []

                  for entity_id, entity_state in world_state.items():
                      priority = self.calculate_priority(
                          viewer_state, entity_state, entity_state.get('type', 'generic')
                      )

                      # Check if should update this tick based on priority
                      update_interval = self._get_update_interval(priority)
                      if current_tick % update_interval != 0:
                          continue

                      # Delta compression
                      last_state = client_last.get(entity_id, {})
                      delta = self._compute_delta(last_state, entity_state)

                      if delta:
                          updates.append({
                              'id': entity_id,
                              'delta': delta,
                              'full': len(delta) > len(entity_state) // 2
                          })
                          client_last[entity_id] = entity_state.copy()

                  return self._encode_updates(updates)

              def _compute_delta(self, old: dict, new: dict) -> dict:
                  delta = {}
                  for key, value in new.items():
                      if key not in old or old[key] != value:
                          delta[key] = value
                  return delta

              def _get_update_interval(self, priority: float) -> int:
                  for p, interval in sorted(self.update_rates.items(), reverse=True):
                      if priority >= p:
                          return interval
                  return 10  # Default slow rate
          ```
      pitfalls:
      - Sending full state every tick overwhelms bandwidth
      - Delta without baseline causes desync
      - Priority calculation per entity per client is O(n*m)
      acceptance_criteria:
      - Game state is serialized efficiently using a binary protocol with minimal overhead per field
      - Delta updates transmit only changed values reducing bandwidth by at least 50% compared to full snapshots
      - Full state snapshots are sent on client reconnection to synchronize the client to the current game state
      - Client-side interpolation smoothly renders entity positions between received server state updates
      deliverables:
      - Delta compression encoder that transmits only changed fields since the last acknowledged client state
      - Priority-based update scheduler that sends nearby or important entities more frequently than distant ones
      - Reliable and unreliable channel selector that uses TCP for critical events and UDP for state updates
      - Bandwidth optimization module that limits update size and frequency to stay within configurable bitrate caps
  collaborative-editor:
    name: Collaborative Text Editor
    description: Build a real-time collaborative text editor using CRDTs or Operational Transformation for conflict-free concurrent
      editing.
    why_important: Collaborative editing is used in Google Docs, Notion, Figma. Understanding CRDTs and OT is valuable for
      any real-time collaborative application.
    difficulty: advanced
    tags:
    - distributed-systems
    - real-time
    - algorithms
    estimated_hours: 50
    prerequisites:
    - realtime-chat
    learning_outcomes:
    - Implement Operational Transformation or CRDTs
    - Handle concurrent edits without conflicts
    - Build cursor presence and selection sync
    - Design efficient document synchronization
    milestones:
    - name: Operation-based CRDT
      description: Implement a sequence CRDT (like RGA or YATA) for conflict-free text insertion and deletion.
      hints:
        level1: Assign unique IDs to each character, order by ID for consistent view.
        level2: Use Lamport timestamps + site ID for globally unique, orderable IDs.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          import bisect

          @dataclass(frozen=True, order=True)
          class CharId:
              timestamp: int
              site_id: str
              seq: int = 0

              def __str__(self):
                  return f"{self.timestamp}.{self.site_id}.{self.seq}"

          @dataclass
          class Char:
              id: CharId
              value: str
              deleted: bool = False

          @dataclass
          class RGADocument:
              """Replicated Growable Array - a sequence CRDT."""
              site_id: str
              chars: list[Char] = field(default_factory=list)
              timestamp: int = 0

              def _next_id(self) -> CharId:
                  self.timestamp += 1
                  return CharId(self.timestamp, self.site_id)

              def _find_position(self, after_id: Optional[CharId]) -> int:
                  if after_id is None:
                      return 0
                  for i, char in enumerate(self.chars):
                      if char.id == after_id:
                          return i + 1
                  return len(self.chars)

              def _visible_index_to_position(self, index: int) -> int:
                  """Convert visible text index to internal position."""
                  visible = 0
                  for i, char in enumerate(self.chars):
                      if not char.deleted:
                          if visible == index:
                              return i
                          visible += 1
                  return len(self.chars)

              def insert(self, index: int, value: str) -> dict:
                  """Local insert at visible index."""
                  pos = self._visible_index_to_position(index)
                  after_id = self.chars[pos - 1].id if pos > 0 else None

                  char_id = self._next_id()
                  char = Char(id=char_id, value=value)

                  # Find correct position respecting ID ordering
                  insert_pos = pos
                  while insert_pos < len(self.chars):
                      if self.chars[insert_pos].id < char_id:
                          break
                      insert_pos += 1

                  self.chars.insert(insert_pos, char)

                  return {
                      'type': 'insert',
                      'id': char_id,
                      'after': after_id,
                      'value': value
                  }

              def delete(self, index: int) -> dict:
                  """Local delete at visible index (tombstone)."""
                  pos = self._visible_index_to_position(index)
                  self.chars[pos].deleted = True

                  return {
                      'type': 'delete',
                      'id': self.chars[pos].id
                  }

              def apply_remote(self, op: dict):
                  """Apply operation from remote site."""
                  if op['type'] == 'insert':
                      self._apply_remote_insert(op)
                  elif op['type'] == 'delete':
                      self._apply_remote_delete(op)

                  # Update local timestamp
                  self.timestamp = max(self.timestamp, op['id'].timestamp)

              def _apply_remote_insert(self, op: dict):
                  char = Char(id=op['id'], value=op['value'])
                  pos = self._find_position(op['after'])

                  # Find correct position respecting ID ordering
                  while pos < len(self.chars):
                      if self.chars[pos].id < char.id:
                          break
                      pos += 1

                  self.chars.insert(pos, char)

              def _apply_remote_delete(self, op: dict):
                  for char in self.chars:
                      if char.id == op['id']:
                          char.deleted = True
                          break

              def get_text(self) -> str:
                  return ''.join(c.value for c in self.chars if not c.deleted)
          ```
      pitfalls:
      - Timestamp collision when sites have same clock
      - Memory grows unbounded with tombstones
      - O(n) lookup for every operation is slow on large docs
      acceptance_criteria:
      - Implement RGA (Replicated Growable Array) for ordered text
      - Handle concurrent insert operations at the same position correctly
      - Ensure convergence across all replicas after all operations are applied
      - Support delete operations that correctly remove characters from shared document
      deliverables:
      - Insert and delete operation types with positional metadata
      - Causal ordering mechanism using vector clocks or similar
      - Convergence guarantees across all connected replicas
      - Operation merging logic for concurrent edits
    - name: Cursor Presence
      description: Synchronize cursor positions and selections across all connected editors with colored indicators.
      hints:
        level1: Track cursor as (position, selection_start, selection_end) per user.
        level2: Transform cursor positions when local/remote edits occur.
        level3: |-
          ```python
          from dataclasses import dataclass
          from typing import Optional

          @dataclass
          class CursorState:
              user_id: str
              position: int
              selection_start: Optional[int] = None
              selection_end: Optional[int] = None
              color: str = "#3498db"
              name: str = "Anonymous"

          class CursorManager:
              def __init__(self, document):
                  self.document = document
                  self.cursors: dict[str, CursorState] = {}

              def update_local(self, user_id: str, position: int,
                              selection: tuple[int, int] = None) -> dict:
                  cursor = self.cursors.setdefault(user_id, CursorState(user_id, 0))
                  cursor.position = position
                  if selection:
                      cursor.selection_start, cursor.selection_end = selection
                  else:
                      cursor.selection_start = cursor.selection_end = None

                  return {
                      'type': 'cursor',
                      'user_id': user_id,
                      'position': position,
                      'selection': selection
                  }

              def apply_remote_cursor(self, op: dict):
                  user_id = op['user_id']
                  cursor = self.cursors.setdefault(user_id, CursorState(user_id, 0))
                  cursor.position = op['position']
                  if op.get('selection'):
                      cursor.selection_start, cursor.selection_end = op['selection']

              def transform_on_insert(self, insert_pos: int, length: int,
                                     author_id: str):
                  """Transform all cursors after an insert operation."""
                  for user_id, cursor in self.cursors.items():
                      # Don't transform author's cursor (they handle it locally)
                      if user_id == author_id:
                          continue

                      if cursor.position >= insert_pos:
                          cursor.position += length

                      if cursor.selection_start is not None:
                          if cursor.selection_start >= insert_pos:
                              cursor.selection_start += length
                          if cursor.selection_end >= insert_pos:
                              cursor.selection_end += length

              def transform_on_delete(self, delete_pos: int, length: int,
                                     author_id: str):
                  """Transform all cursors after a delete operation."""
                  delete_end = delete_pos + length

                  for user_id, cursor in self.cursors.items():
                      if user_id == author_id:
                          continue

                      # Cursor after deleted region
                      if cursor.position >= delete_end:
                          cursor.position -= length
                      # Cursor within deleted region
                      elif cursor.position > delete_pos:
                          cursor.position = delete_pos

                      # Transform selection
                      if cursor.selection_start is not None:
                          cursor.selection_start = self._transform_point(
                              cursor.selection_start, delete_pos, delete_end, length
                          )
                          cursor.selection_end = self._transform_point(
                              cursor.selection_end, delete_pos, delete_end, length
                          )

              def _transform_point(self, point: int, del_start: int,
                                  del_end: int, length: int) -> int:
                  if point >= del_end:
                      return point - length
                  elif point > del_start:
                      return del_start
                  return point
          ```
      pitfalls:
      - Cursor flickers when transforming on every keystroke
      - Selection can become inverted (end < start) after transform
      - Not handling cursor at document boundaries
      acceptance_criteria:
      - Track and store cursor positions per connected user in real time
      - Broadcast cursor update events to all connected peers in real-time
      - Display user name and assigned color label at each remote cursor position
      - Handle cursor position recalculation after local or remote document edits
      deliverables:
      - Cursor position sharing protocol between connected users
      - User color assignment for distinguishing remote cursors
      - Remote cursor rendering overlay in the editor view
      - Cursor position transformation after concurrent document edits
    - name: Operational Transformation
      description: Implement OT as alternative to CRDTs, with transform functions for insert/delete operations.
      hints:
        level1: Transform concurrent operations so they can be applied in any order.
        level2: Use server to serialize operations and determine canonical order.
        level3: |-
          ```python
          from dataclasses import dataclass
          from typing import Union
          from enum import Enum

          class OpType(Enum):
              INSERT = "insert"
              DELETE = "delete"

          @dataclass
          class Operation:
              type: OpType
              position: int
              text: str = ""  # For insert
              length: int = 0  # For delete
              version: int = 0

          def transform(op1: Operation, op2: Operation) -> Operation:
              """Transform op1 against op2 (op2 was applied first)."""
              if op1.type == OpType.INSERT and op2.type == OpType.INSERT:
                  return transform_insert_insert(op1, op2)
              elif op1.type == OpType.INSERT and op2.type == OpType.DELETE:
                  return transform_insert_delete(op1, op2)
              elif op1.type == OpType.DELETE and op2.type == OpType.INSERT:
                  return transform_delete_insert(op1, op2)
              else:
                  return transform_delete_delete(op1, op2)

          def transform_insert_insert(op1: Operation, op2: Operation) -> Operation:
              if op1.position <= op2.position:
                  return op1  # No transformation needed
              else:
                  return Operation(
                      type=OpType.INSERT,
                      position=op1.position + len(op2.text),
                      text=op1.text,
                      version=op1.version
                  )

          def transform_insert_delete(op1: Operation, op2: Operation) -> Operation:
              if op1.position <= op2.position:
                  return op1
              elif op1.position >= op2.position + op2.length:
                  return Operation(
                      type=OpType.INSERT,
                      position=op1.position - op2.length,
                      text=op1.text,
                      version=op1.version
                  )
              else:
                  # Insert position was in deleted region
                  return Operation(
                      type=OpType.INSERT,
                      position=op2.position,
                      text=op1.text,
                      version=op1.version
                  )

          def transform_delete_insert(op1: Operation, op2: Operation) -> Operation:
              if op2.position >= op1.position + op1.length:
                  return op1
              elif op2.position <= op1.position:
                  return Operation(
                      type=OpType.DELETE,
                      position=op1.position + len(op2.text),
                      length=op1.length,
                      version=op1.version
                  )
              else:
                  # Insert splits the delete
                  # Delete text before insert, then after
                  return Operation(
                      type=OpType.DELETE,
                      position=op1.position,
                      length=op1.length + len(op2.text),
                      version=op1.version
                  )

          def transform_delete_delete(op1: Operation, op2: Operation) -> Operation:
              # Complex case: overlapping deletes
              end1 = op1.position + op1.length
              end2 = op2.position + op2.length

              if end1 <= op2.position:
                  return op1  # op1 entirely before op2
              elif op1.position >= end2:
                  return Operation(
                      type=OpType.DELETE,
                      position=op1.position - op2.length,
                      length=op1.length,
                      version=op1.version
                  )
              else:
                  # Overlapping deletes
                  new_pos = min(op1.position, op2.position)
                  # Calculate remaining length after op2's deletion
                  overlap_start = max(op1.position, op2.position)
                  overlap_end = min(end1, end2)
                  overlap = max(0, overlap_end - overlap_start)
                  new_length = op1.length - overlap

                  if new_length <= 0:
                      return None  # op1 is completely covered by op2

                  return Operation(
                      type=OpType.DELETE,
                      position=new_pos,
                      length=new_length,
                      version=op1.version
                  )

          class OTServer:
              def __init__(self):
                  self.document = ""
                  self.version = 0
                  self.history: list[Operation] = []

              def apply(self, op: Operation) -> Operation:
                  # Transform against all operations since client's version
                  transformed = op
                  for historical in self.history[op.version:]:
                      transformed = transform(transformed, historical)
                      if transformed is None:
                          return None

                  # Apply to document
                  if transformed.type == OpType.INSERT:
                      self.document = (
                          self.document[:transformed.position] +
                          transformed.text +
                          self.document[transformed.position:]
                      )
                  else:
                      self.document = (
                          self.document[:transformed.position] +
                          self.document[transformed.position + transformed.length:]
                      )

                  transformed.version = self.version
                  self.history.append(transformed)
                  self.version += 1

                  return transformed
          ```
      pitfalls:
      - 'Transform functions must be composable: T(T(a,b),c) = T(a,T(b,c))'
      - Overlapping delete transforms are notoriously tricky
      - History grows unbounded without garbage collection
      acceptance_criteria:
      - Transform operations against concurrent operations to preserve intent
      - Maintain document consistency across all connected clients after transforms
      - Handle operation priority and ordering for conflicting concurrent edits
      - Support intention preservation so each user's edits have expected effect
      deliverables:
      - Operation representation supporting insert and delete types
      - Transform function implementation for concurrent operation pairs
      - Server-side operation ordering with total order guarantee
      - Client-side operation buffer for pending unacknowledged operations
    - name: Undo/Redo with Collaboration
      description: Implement undo/redo that works correctly with concurrent edits from multiple users.
      hints:
        level1: Each user has their own undo stack of their operations.
        level2: Undoing means inverting the operation and transforming against subsequent ops.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from typing import Optional

          @dataclass
          class UndoManager:
              user_id: str
              undo_stack: list[Operation] = field(default_factory=list)
              redo_stack: list[Operation] = field(default_factory=list)
              document: 'OTDocument' = None

              def record(self, op: Operation):
                  """Record an operation for potential undo."""
                  self.undo_stack.append(op)
                  self.redo_stack.clear()  # Clear redo on new action

              def can_undo(self) -> bool:
                  return len(self.undo_stack) > 0

              def can_redo(self) -> bool:
                  return len(self.redo_stack) > 0

              def undo(self) -> Optional[Operation]:
                  if not self.can_undo():
                      return None

                  op = self.undo_stack.pop()

                  # Create inverse operation
                  inverse = self._invert(op)

                  # Transform inverse against all operations that happened after op
                  transformed = self._transform_against_history(inverse, op.version)

                  if transformed:
                      self.redo_stack.append(op)
                      return transformed
                  return None

              def redo(self) -> Optional[Operation]:
                  if not self.can_redo():
                      return None

                  op = self.redo_stack.pop()

                  # Transform the original operation against subsequent history
                  transformed = self._transform_against_history(op, op.version)

                  if transformed:
                      self.undo_stack.append(op)
                      return transformed
                  return None

              def _invert(self, op: Operation) -> Operation:
                  """Create the inverse of an operation."""
                  if op.type == OpType.INSERT:
                      return Operation(
                          type=OpType.DELETE,
                          position=op.position,
                          length=len(op.text),
                          version=self.document.version
                      )
                  else:
                      # For delete, we need the deleted text
                      # This requires storing deleted text with operation
                      return Operation(
                          type=OpType.INSERT,
                          position=op.position,
                          text=op.deleted_text,  # Need to store this
                          version=self.document.version
                      )

              def _transform_against_history(self, op: Operation,
                                             since_version: int) -> Optional[Operation]:
                  transformed = op
                  for historical in self.document.history[since_version:]:
                      # Skip our own operations (they're in undo stack)
                      if historical.user_id == self.user_id:
                          continue
                      transformed = transform(transformed, historical)
                      if transformed is None:
                          return None
                  return transformed

              def transform_stacks_on_remote(self, remote_op: Operation):
                  """Transform all stacks when a remote operation arrives."""
                  self.undo_stack = [
                      transform(op, remote_op) for op in self.undo_stack
                      if transform(op, remote_op) is not None
                  ]
                  self.redo_stack = [
                      transform(op, remote_op) for op in self.redo_stack
                      if transform(op, remote_op) is not None
                  ]
          ```
      pitfalls:
      - Undoing a delete requires storing the deleted text
      - Undo stack becomes invalid if user disconnects/reconnects
      - Group related operations (e.g., typing) into single undo unit
      acceptance_criteria:
      - Track operation history per user for selective undo capability
      - Implement selective undo that reverses only the current user's own operations
      - Handle undo of merged operations that were transformed against concurrent edits
      - Support redo after undo to reapply previously reversed operations correctly
      deliverables:
      - Local undo stack tracking user's own operations
      - Selective undo that reverses only the current user's operations
      - Redo functionality that reapplies previously undone operations
      - Undo transformation against concurrent edits from other users
  event-sourcing:
    name: Event Sourcing System
    description: Build an event-sourced system with event store, projections, snapshots, and CQRS pattern for complex domain
      modeling.
    why_important: Event sourcing provides audit trails, temporal queries, and robust distributed systems. Used in banking,
      e-commerce, and enterprise systems.
    difficulty: advanced
    tags:
    - distributed-systems
    - architecture
    - backend
    estimated_hours: 45
    prerequisites:
    - rest-api-design
    learning_outcomes:
    - Implement append-only event store
    - Build projections from event streams
    - Handle snapshots for performance
    - Apply CQRS for read/write separation
    milestones:
    - name: Event Store
      description: Build an append-only event store with optimistic concurrency, stream versioning, and event metadata.
      hints:
        level1: Store events with stream ID, version, timestamp, and payload.
        level2: Use expected version for optimistic concurrency control.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from datetime import datetime
          from typing import Any, Optional
          import json
          import uuid

          @dataclass
          class Event:
              id: str
              stream_id: str
              version: int
              type: str
              data: dict
              metadata: dict
              timestamp: datetime = field(default_factory=datetime.utcnow)

              @classmethod
              def create(cls, stream_id: str, version: int, event_type: str,
                         data: dict, metadata: dict = None):
                  return cls(
                      id=str(uuid.uuid4()),
                      stream_id=stream_id,
                      version=version,
                      type=event_type,
                      data=data,
                      metadata=metadata or {}
                  )

          class ConcurrencyError(Exception):
              pass

          class EventStore:
              def __init__(self, storage):
                  self.storage = storage  # Could be PostgreSQL, EventStoreDB, etc.

              async def append(self, stream_id: str, events: list[Event],
                              expected_version: int = None) -> int:
                  """
                  Append events to stream with optimistic concurrency.
                  Returns new stream version.
                  """
                  async with self.storage.transaction() as tx:
                      # Get current version
                      current_version = await self._get_stream_version(tx, stream_id)

                      # Check expected version for concurrency
                      if expected_version is not None:
                          if current_version != expected_version:
                              raise ConcurrencyError(
                                  f"Expected version {expected_version}, "
                                  f"but stream is at {current_version}"
                              )

                      # Append events
                      new_version = current_version
                      for event in events:
                          new_version += 1
                          event.version = new_version
                          await self._store_event(tx, event)

                      return new_version

              async def read_stream(self, stream_id: str,
                                   from_version: int = 0,
                                   to_version: int = None) -> list[Event]:
                  query = """
                      SELECT * FROM events
                      WHERE stream_id = $1 AND version >= $2
                  """
                  params = [stream_id, from_version]

                  if to_version is not None:
                      query += " AND version <= $3"
                      params.append(to_version)

                  query += " ORDER BY version ASC"

                  rows = await self.storage.fetch(query, *params)
                  return [self._row_to_event(row) for row in rows]

              async def read_all(self, from_position: int = 0,
                                batch_size: int = 1000) -> list[Event]:
                  """Read all events across all streams (for projections)."""
                  rows = await self.storage.fetch(
                      """
                      SELECT * FROM events
                      WHERE global_position > $1
                      ORDER BY global_position ASC
                      LIMIT $2
                      """,
                      from_position, batch_size
                  )
                  return [self._row_to_event(row) for row in rows]

              async def _get_stream_version(self, tx, stream_id: str) -> int:
                  result = await tx.fetchval(
                      "SELECT COALESCE(MAX(version), 0) FROM events WHERE stream_id = $1",
                      stream_id
                  )
                  return result

              async def _store_event(self, tx, event: Event):
                  await tx.execute(
                      """
                      INSERT INTO events (id, stream_id, version, type, data, metadata, timestamp)
                      VALUES ($1, $2, $3, $4, $5, $6, $7)
                      """,
                      event.id, event.stream_id, event.version, event.type,
                      json.dumps(event.data), json.dumps(event.metadata), event.timestamp
                  )
          ```
      pitfalls:
      - Gap in versions makes stream unreadable
      - Large events bloat storage without compression
      - Missing global ordering breaks cross-stream projections
      acceptance_criteria:
      - Append events to a stream with guaranteed ordering and persistence
      - Read events for a specific stream in the correct chronological order
      - Support optimistic concurrency using expected version on append operations
      - Handle high write throughput with efficient append-only storage design
      deliverables:
      - Event append operation adding events to a stream atomically
      - Event stream per aggregate with ordered retrieval support
      - Event versioning with schema evolution for backward compatibility
      - Event serialization using JSON or binary format with type metadata
    - name: Aggregate & Event Sourcing
      description: Implement domain aggregates that are reconstituted from event history with command handlers.
      hints:
        level1: Aggregate state is built by replaying events in order.
        level2: Commands validate business rules, produce events.
        level3: |-
          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass, field
          from typing import TypeVar, Generic

          T = TypeVar('T')

          class Aggregate(ABC, Generic[T]):
              def __init__(self):
                  self.id: str = None
                  self.version: int = 0
                  self._changes: list[Event] = []

              @abstractmethod
              def apply(self, event: Event):
                  """Apply event to update aggregate state."""
                  pass

              def load(self, events: list[Event]):
                  """Reconstitute aggregate from event history."""
                  for event in events:
                      self.apply(event)
                      self.version = event.version

              def _raise_event(self, event_type: str, data: dict):
                  """Record new event (to be persisted)."""
                  event = Event.create(
                      stream_id=self.id,
                      version=self.version + len(self._changes) + 1,
                      event_type=event_type,
                      data=data
                  )
                  self._changes.append(event)
                  self.apply(event)

              def get_changes(self) -> list[Event]:
                  return self._changes

              def clear_changes(self):
                  self._changes = []

          # Example: Order aggregate
          @dataclass
          class OrderItem:
              product_id: str
              quantity: int
              price: float

          class Order(Aggregate):
              def __init__(self):
                  super().__init__()
                  self.customer_id: str = None
                  self.items: list[OrderItem] = []
                  self.status: str = "draft"
                  self.total: float = 0

              # Commands
              def create(self, order_id: str, customer_id: str):
                  if self.id is not None:
                      raise ValueError("Order already exists")
                  self._raise_event("OrderCreated", {
                      "order_id": order_id,
                      "customer_id": customer_id
                  })

              def add_item(self, product_id: str, quantity: int, price: float):
                  if self.status != "draft":
                      raise ValueError("Cannot modify non-draft order")
                  self._raise_event("ItemAdded", {
                      "product_id": product_id,
                      "quantity": quantity,
                      "price": price
                  })

              def submit(self):
                  if self.status != "draft":
                      raise ValueError("Order already submitted")
                  if not self.items:
                      raise ValueError("Cannot submit empty order")
                  self._raise_event("OrderSubmitted", {
                      "total": self.total
                  })

              # Event handlers
              def apply(self, event: Event):
                  if event.type == "OrderCreated":
                      self.id = event.data["order_id"]
                      self.customer_id = event.data["customer_id"]
                  elif event.type == "ItemAdded":
                      item = OrderItem(
                          product_id=event.data["product_id"],
                          quantity=event.data["quantity"],
                          price=event.data["price"]
                      )
                      self.items.append(item)
                      self.total += item.quantity * item.price
                  elif event.type == "OrderSubmitted":
                      self.status = "submitted"

          class AggregateRepository:
              def __init__(self, event_store: EventStore, aggregate_type: type):
                  self.event_store = event_store
                  self.aggregate_type = aggregate_type

              async def load(self, aggregate_id: str) -> Aggregate:
                  events = await self.event_store.read_stream(aggregate_id)
                  aggregate = self.aggregate_type()
                  aggregate.load(events)
                  return aggregate

              async def save(self, aggregate: Aggregate):
                  changes = aggregate.get_changes()
                  if changes:
                      await self.event_store.append(
                          aggregate.id,
                          changes,
                          expected_version=aggregate.version
                      )
                      aggregate.clear_changes()
          ```
      pitfalls:
      - Business logic in apply() instead of command handlers
      - Forgetting to clear changes after save causes duplicates
      - Mutable event data allows accidental modification
      acceptance_criteria:
      - Load aggregate state by replaying its event stream from the beginning
      - Apply events sequentially to rebuild the aggregate's current state
      - Emit new events from command handlers after validating business rules
      - Handle command validation by checking aggregate state before emitting events
      deliverables:
      - Aggregate base class with event application and command handling interface
      - Event application methods rebuilding aggregate state from event history
      - Command handling that validates and emits new domain events
      - Aggregate loading from event stream with full state reconstruction
    - name: Projections
      description: Build read models (projections) that update in response to events for efficient querying.
      hints:
        level1: Subscribe to events, update denormalized read model.
        level2: Track checkpoint to resume from last processed position.
        level3: |-
          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass
          import asyncio

          @dataclass
          class Checkpoint:
              projection_name: str
              position: int

          class Projection(ABC):
              def __init__(self, name: str):
                  self.name = name

              @abstractmethod
              async def handle(self, event: Event):
                  """Handle a single event."""
                  pass

              @abstractmethod
              def handles(self) -> list[str]:
                  """Return list of event types this projection handles."""
                  pass

          class ProjectionEngine:
              def __init__(self, event_store: EventStore, checkpoint_store):
                  self.event_store = event_store
                  self.checkpoint_store = checkpoint_store
                  self.projections: list[Projection] = []
                  self._running = False

              def register(self, projection: Projection):
                  self.projections.append(projection)

              async def start(self):
                  self._running = True
                  while self._running:
                      await self._process_batch()
                      await asyncio.sleep(0.1)  # Poll interval

              async def stop(self):
                  self._running = False

              async def _process_batch(self, batch_size: int = 100):
                  for projection in self.projections:
                      checkpoint = await self.checkpoint_store.get(projection.name)
                      position = checkpoint.position if checkpoint else 0

                      events = await self.event_store.read_all(
                          from_position=position,
                          batch_size=batch_size
                      )

                      for event in events:
                          if event.type in projection.handles():
                              await projection.handle(event)
                          position = event.global_position

                      if events:
                          await self.checkpoint_store.save(
                              Checkpoint(projection.name, position)
                          )

          # Example projection: Order summary read model
          class OrderSummaryProjection(Projection):
              def __init__(self, db):
                  super().__init__("order_summary")
                  self.db = db

              def handles(self) -> list[str]:
                  return ["OrderCreated", "ItemAdded", "OrderSubmitted"]

              async def handle(self, event: Event):
                  if event.type == "OrderCreated":
                      await self.db.execute(
                          """
                          INSERT INTO order_summaries (id, customer_id, status, total, item_count)
                          VALUES ($1, $2, 'draft', 0, 0)
                          """,
                          event.data["order_id"], event.data["customer_id"]
                      )
                  elif event.type == "ItemAdded":
                      await self.db.execute(
                          """
                          UPDATE order_summaries
                          SET total = total + $1, item_count = item_count + 1
                          WHERE id = $2
                          """,
                          event.data["quantity"] * event.data["price"],
                          event.stream_id
                      )
                  elif event.type == "OrderSubmitted":
                      await self.db.execute(
                          """
                          UPDATE order_summaries SET status = 'submitted' WHERE id = $1
                          """,
                          event.stream_id
                      )

          # Projection for customer analytics
          class CustomerAnalyticsProjection(Projection):
              def __init__(self, db):
                  super().__init__("customer_analytics")
                  self.db = db

              def handles(self) -> list[str]:
                  return ["OrderSubmitted"]

              async def handle(self, event: Event):
                  # Update customer lifetime value
                  await self.db.execute(
                      """
                      INSERT INTO customer_stats (customer_id, order_count, total_spent)
                      VALUES ($1, 1, $2)
                      ON CONFLICT (customer_id) DO UPDATE
                      SET order_count = customer_stats.order_count + 1,
                          total_spent = customer_stats.total_spent + $2
                      """,
                      event.metadata.get("customer_id"),
                      event.data["total"]
                  )
          ```
      pitfalls:
      - Processing same event twice without idempotency corrupts data
      - Checkpoint saved before event processed loses events on crash
      - Slow projection blocks all others in single-threaded engine
      acceptance_criteria:
      - Build read models from events by processing each event type into view data
      - Support multiple projections per stream for different query use cases
      - Handle projection rebuild from scratch without affecting running projections
      - Track projection position in stream to resume processing after restart
      deliverables:
      - Projection handlers subscribing to event streams for read model updates
      - Read model updates transforming events into queryable view data
      - Projection rebuilding from scratch for schema changes or corrections
      - Eventual consistency tracking between event store and read models
    - name: Snapshots
      description: Implement aggregate snapshots to avoid replaying entire event history for performance.
      hints:
        level1: Periodically save aggregate state, load snapshot + events since.
        level2: Store snapshot version to know which events to replay.
        level3: |-
          ```python
          from dataclasses import dataclass
          from typing import Optional
          import json

          @dataclass
          class Snapshot:
              aggregate_id: str
              aggregate_type: str
              version: int
              state: dict

          class SnapshotStore:
              def __init__(self, storage):
                  self.storage = storage

              async def save(self, snapshot: Snapshot):
                  await self.storage.execute(
                      """
                      INSERT INTO snapshots (aggregate_id, aggregate_type, version, state)
                      VALUES ($1, $2, $3, $4)
                      ON CONFLICT (aggregate_id) DO UPDATE
                      SET version = $3, state = $4
                      WHERE snapshots.version < $3
                      """,
                      snapshot.aggregate_id, snapshot.aggregate_type,
                      snapshot.version, json.dumps(snapshot.state)
                  )

              async def load(self, aggregate_id: str) -> Optional[Snapshot]:
                  row = await self.storage.fetchrow(
                      "SELECT * FROM snapshots WHERE aggregate_id = $1",
                      aggregate_id
                  )
                  if row:
                      return Snapshot(
                          aggregate_id=row["aggregate_id"],
                          aggregate_type=row["aggregate_type"],
                          version=row["version"],
                          state=json.loads(row["state"])
                      )
                  return None

          class SnapshottingRepository:
              def __init__(self, event_store: EventStore,
                           snapshot_store: SnapshotStore,
                           aggregate_type: type,
                           snapshot_frequency: int = 100):
                  self.event_store = event_store
                  self.snapshot_store = snapshot_store
                  self.aggregate_type = aggregate_type
                  self.snapshot_frequency = snapshot_frequency

              async def load(self, aggregate_id: str) -> Aggregate:
                  aggregate = self.aggregate_type()

                  # Try to load snapshot
                  snapshot = await self.snapshot_store.load(aggregate_id)

                  if snapshot:
                      # Restore from snapshot
                      aggregate.restore_from_snapshot(snapshot.state)
                      aggregate.version = snapshot.version

                      # Load events since snapshot
                      events = await self.event_store.read_stream(
                          aggregate_id,
                          from_version=snapshot.version + 1
                      )
                  else:
                      # Load all events
                      events = await self.event_store.read_stream(aggregate_id)

                  aggregate.load(events)
                  return aggregate

              async def save(self, aggregate: Aggregate):
                  changes = aggregate.get_changes()
                  if not changes:
                      return

                  await self.event_store.append(
                      aggregate.id,
                      changes,
                      expected_version=aggregate.version
                  )

                  new_version = aggregate.version + len(changes)
                  aggregate.clear_changes()

                  # Check if we should create a snapshot
                  if new_version % self.snapshot_frequency == 0:
                      await self._create_snapshot(aggregate, new_version)

              async def _create_snapshot(self, aggregate: Aggregate, version: int):
                  snapshot = Snapshot(
                      aggregate_id=aggregate.id,
                      aggregate_type=type(aggregate).__name__,
                      version=version,
                      state=aggregate.get_snapshot_state()
                  )
                  await self.snapshot_store.save(snapshot)

          # Add to Order aggregate
          class Order(Aggregate):
              # ... previous code ...

              def get_snapshot_state(self) -> dict:
                  return {
                      "id": self.id,
                      "customer_id": self.customer_id,
                      "items": [
                          {"product_id": i.product_id, "quantity": i.quantity, "price": i.price}
                          for i in self.items
                      ],
                      "status": self.status,
                      "total": self.total
                  }

              def restore_from_snapshot(self, state: dict):
                  self.id = state["id"]
                  self.customer_id = state["customer_id"]
                  self.items = [
                      OrderItem(**item) for item in state["items"]
                  ]
                  self.status = state["status"]
                  self.total = state["total"]
          ```
      pitfalls:
      - Snapshot schema changes break deserialization
      - Creating snapshot on every save destroys performance
      - Snapshot without version causes event replay from beginning
      acceptance_criteria:
      - Create aggregate snapshots periodically to reduce event replay time on load
      - Load aggregate from snapshot plus only events since the snapshot version
      - Handle snapshot versioning for schema changes between snapshot formats
      - Automatic snapshot creation triggers after configurable N events threshold
      deliverables:
      - Snapshot creation serializing current aggregate state at a point in time
      - Snapshot storage persisting snapshots alongside event stream metadata
      - Loading with snapshots to skip replaying early events during reconstruction
      - Snapshot scheduling based on event count or time interval thresholds
  ci-cd-pipeline:
    name: CI/CD Pipeline
    description: Build a continuous integration and deployment pipeline that handles build, test, artifact management, and
      deployment automation.
    why_important: CI/CD is the backbone of modern software delivery. Understanding pipeline internals makes you better at
      debugging and optimizing delivery workflows.
    difficulty: intermediate
    tags:
    - devops
    - automation
    - infrastructure
    estimated_hours: 35
    prerequisites:
    - shell
    learning_outcomes:
    - Design multi-stage pipeline workflows
    - Implement build and test automation
    - Handle artifact versioning and storage
    - Automate deployment with rollback support
    milestones:
    - name: Pipeline Definition Parser
      description: Parse YAML pipeline definitions with stages, jobs, steps, and dependencies between them.
      hints:
        level1: Use YAML to define stages with ordered steps.
        level2: Build a DAG from job dependencies for execution order.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          import yaml
          from enum import Enum

          class StepType(str, Enum):
              SCRIPT = "script"
              CHECKOUT = "checkout"
              ARTIFACT_UPLOAD = "artifact_upload"
              ARTIFACT_DOWNLOAD = "artifact_download"
              DEPLOY = "deploy"

          @dataclass
          class Step:
              name: str
              type: StepType
              script: Optional[str] = None
              working_dir: str = "."
              env: dict = field(default_factory=dict)
              timeout: int = 600
              continue_on_error: bool = False

          @dataclass
          class Job:
              name: str
              steps: list[Step]
              needs: list[str] = field(default_factory=list)
              runs_on: str = "default"
              env: dict = field(default_factory=dict)
              condition: Optional[str] = None

          @dataclass
          class Stage:
              name: str
              jobs: list[Job]

          @dataclass
          class Pipeline:
              name: str
              stages: list[Stage]
              triggers: dict = field(default_factory=dict)
              env: dict = field(default_factory=dict)

          class PipelineParser:
              def parse(self, yaml_content: str) -> Pipeline:
                  data = yaml.safe_load(yaml_content)

                  stages = []
                  for stage_data in data.get('stages', []):
                      jobs = []
                      for job_name, job_data in stage_data.get('jobs', {}).items():
                          steps = [
                              Step(
                                  name=s.get('name', f'step-{i}'),
                                  type=StepType(s.get('type', 'script')),
                                  script=s.get('script'),
                                  working_dir=s.get('working_dir', '.'),
                                  env=s.get('env', {}),
                                  timeout=s.get('timeout', 600),
                                  continue_on_error=s.get('continue_on_error', False)
                              )
                              for i, s in enumerate(job_data.get('steps', []))
                          ]
                          jobs.append(Job(
                              name=job_name,
                              steps=steps,
                              needs=job_data.get('needs', []),
                              runs_on=job_data.get('runs_on', 'default'),
                              env=job_data.get('env', {}),
                              condition=job_data.get('if')
                          ))
                      stages.append(Stage(name=stage_data['name'], jobs=jobs))

                  return Pipeline(
                      name=data.get('name', 'pipeline'),
                      stages=stages,
                      triggers=data.get('triggers', {}),
                      env=data.get('env', {})
                  )

              def build_execution_graph(self, pipeline: Pipeline) -> dict:
                  """Build DAG for job execution order."""
                  graph = {}
                  for stage in pipeline.stages:
                      for job in stage.jobs:
                          graph[job.name] = {
                              'job': job,
                              'dependencies': job.needs,
                              'stage': stage.name
                          }
                  return graph

              def topological_sort(self, graph: dict) -> list[str]:
                  """Return jobs in valid execution order."""
                  visited = set()
                  order = []

                  def visit(name):
                      if name in visited:
                          return
                      visited.add(name)
                      for dep in graph[name]['dependencies']:
                          visit(dep)
                      order.append(name)

                  for name in graph:
                      visit(name)
                  return order
          ```
      pitfalls:
      - Circular dependencies cause infinite loop
      - Missing dependency reference crashes at runtime
      - YAML anchors and aliases need special handling
      acceptance_criteria:
      - Parser reads YAML pipeline file and produces structured stage/job/step objects
      - Validation rejects malformed definitions with descriptive error messages
      - Dependency graph enables topological sort for correct parallel execution ordering
      - Conditional execution expressions (if/when) are parsed and evaluated per job context
      deliverables:
      - YAML pipeline definition parser extracting stages, jobs, and steps
      - Stage and job definition validation checking required fields and types
      - Dependency graph construction building DAG from job dependency declarations
      - Variable substitution resolving environment and pipeline variables in step definitions
    - name: Job Executor
      description: Execute jobs in isolated environments with proper logging, timeout handling, and exit code propagation.
      hints:
        level1: Run each step in subprocess, capture output.
        level2: Implement timeout and cancellation handling.
        level3: |-
          ```python
          import asyncio
          import subprocess
          import os
          from dataclasses import dataclass
          from datetime import datetime
          from enum import Enum

          class JobStatus(str, Enum):
              PENDING = "pending"
              RUNNING = "running"
              SUCCESS = "success"
              FAILED = "failed"
              CANCELLED = "cancelled"
              SKIPPED = "skipped"

          @dataclass
          class StepResult:
              name: str
              status: JobStatus
              exit_code: int
              stdout: str
              stderr: str
              duration: float
              started_at: datetime
              finished_at: datetime

          @dataclass
          class JobResult:
              name: str
              status: JobStatus
              step_results: list[StepResult]
              duration: float

          class JobExecutor:
              def __init__(self, workspace: str):
                  self.workspace = workspace
                  self._cancelled = False

              async def execute(self, job: Job, context: dict) -> JobResult:
                  results = []
                  job_start = datetime.utcnow()
                  status = JobStatus.SUCCESS

                  # Check condition
                  if job.condition and not self._evaluate_condition(job.condition, context):
                      return JobResult(job.name, JobStatus.SKIPPED, [], 0)

                  # Merge environment
                  env = {**os.environ, **context.get('env', {}), **job.env}

                  for step in job.steps:
                      if self._cancelled:
                          status = JobStatus.CANCELLED
                          break

                      step_env = {**env, **step.env}
                      result = await self._execute_step(step, step_env)
                      results.append(result)

                      if result.status == JobStatus.FAILED and not step.continue_on_error:
                          status = JobStatus.FAILED
                          break

                  duration = (datetime.utcnow() - job_start).total_seconds()
                  return JobResult(job.name, status, results, duration)

              async def _execute_step(self, step: Step, env: dict) -> StepResult:
                  started = datetime.utcnow()

                  try:
                      if step.type == StepType.SCRIPT:
                          result = await self._run_script(step.script, env, step.timeout, step.working_dir)
                      elif step.type == StepType.CHECKOUT:
                          result = await self._checkout(env)
                      else:
                          result = (0, "", "")

                      exit_code, stdout, stderr = result
                      status = JobStatus.SUCCESS if exit_code == 0 else JobStatus.FAILED

                  except asyncio.TimeoutError:
                      exit_code, stdout, stderr = -1, "", "Step timed out"
                      status = JobStatus.FAILED
                  except Exception as e:
                      exit_code, stdout, stderr = -1, "", str(e)
                      status = JobStatus.FAILED

                  finished = datetime.utcnow()
                  return StepResult(
                      name=step.name,
                      status=status,
                      exit_code=exit_code,
                      stdout=stdout,
                      stderr=stderr,
                      duration=(finished - started).total_seconds(),
                      started_at=started,
                      finished_at=finished
                  )

              async def _run_script(self, script: str, env: dict,
                                   timeout: int, working_dir: str) -> tuple:
                  work_path = os.path.join(self.workspace, working_dir)

                  proc = await asyncio.create_subprocess_shell(
                      script,
                      stdout=asyncio.subprocess.PIPE,
                      stderr=asyncio.subprocess.PIPE,
                      env=env,
                      cwd=work_path
                  )

                  try:
                      stdout, stderr = await asyncio.wait_for(
                          proc.communicate(), timeout=timeout
                      )
                      return proc.returncode, stdout.decode(), stderr.decode()
                  except asyncio.TimeoutError:
                      proc.kill()
                      raise

              def cancel(self):
                  self._cancelled = True
          ```
      pitfalls:
      - Zombie processes when parent killed without cleanup
      - Environment variable injection security risk
      - Large output causes memory exhaustion
      acceptance_criteria:
      - Each job step executes in isolated Docker container with specified image
      - Step logs are streamed in real-time to pipeline output for monitoring progress
      - Step timeout terminates execution and marks step as failed after configured duration
      - Failed steps trigger configurable retry with backoff before marking job as failed
      deliverables:
      - Docker container execution running each job step in isolated container environment
      - Script step runner executing shell commands with configurable working directory
      - Environment variable setup injecting pipeline and job-level variables into execution context
      - Output capture collecting stdout and stderr streams from each executed step
    - name: Artifact Management
      description: Implement artifact upload, download, and caching between pipeline stages with versioning.
      hints:
        level1: Store artifacts with run ID and path in object storage.
        level2: Implement cache keys based on file hashes for dependency caching.
        level3: |-
          ```python
          import hashlib
          import os
          import tarfile
          import io
          from dataclasses import dataclass
          from typing import Optional

          @dataclass
          class Artifact:
              name: str
              path: str
              run_id: str
              job_name: str
              size: int
              checksum: str

          class ArtifactManager:
              def __init__(self, storage, cache_storage):
                  self.storage = storage  # S3, GCS, etc.
                  self.cache = cache_storage

              async def upload(self, run_id: str, job_name: str,
                              name: str, local_path: str) -> Artifact:
                  """Upload artifact from local path."""
                  # Create tarball
                  tar_buffer = io.BytesIO()
                  with tarfile.open(fileobj=tar_buffer, mode='w:gz') as tar:
                      tar.add(local_path, arcname=os.path.basename(local_path))

                  tar_data = tar_buffer.getvalue()
                  checksum = hashlib.sha256(tar_data).hexdigest()

                  # Upload to storage
                  storage_path = f"artifacts/{run_id}/{job_name}/{name}.tar.gz"
                  await self.storage.put(storage_path, tar_data)

                  return Artifact(
                      name=name,
                      path=storage_path,
                      run_id=run_id,
                      job_name=job_name,
                      size=len(tar_data),
                      checksum=checksum
                  )

              async def download(self, artifact: Artifact, local_path: str):
                  """Download and extract artifact."""
                  data = await self.storage.get(artifact.path)

                  # Verify checksum
                  if hashlib.sha256(data).hexdigest() != artifact.checksum:
                      raise ValueError("Artifact checksum mismatch")

                  tar_buffer = io.BytesIO(data)
                  with tarfile.open(fileobj=tar_buffer, mode='r:gz') as tar:
                      tar.extractall(local_path)

              async def cache_save(self, key: str, paths: list[str]):
                  """Save paths to cache with key."""
                  tar_buffer = io.BytesIO()
                  with tarfile.open(fileobj=tar_buffer, mode='w:gz') as tar:
                      for path in paths:
                          if os.path.exists(path):
                              tar.add(path)

                  cache_path = f"cache/{key}.tar.gz"
                  await self.cache.put(cache_path, tar_buffer.getvalue())

              async def cache_restore(self, key: str, fallback_keys: list[str] = None) -> bool:
                  """Restore from cache, try fallback keys if primary misses."""
                  keys_to_try = [key] + (fallback_keys or [])

                  for k in keys_to_try:
                      cache_path = f"cache/{k}.tar.gz"
                      data = await self.cache.get(cache_path)
                      if data:
                          tar_buffer = io.BytesIO(data)
                          with tarfile.open(fileobj=tar_buffer, mode='r:gz') as tar:
                              tar.extractall('.')
                          return True
                  return False

              def compute_cache_key(self, prefix: str, files: list[str]) -> str:
                  """Compute cache key from file contents."""
                  hasher = hashlib.sha256()
                  hasher.update(prefix.encode())

                  for file_path in sorted(files):
                      if os.path.exists(file_path):
                          with open(file_path, 'rb') as f:
                              hasher.update(f.read())

                  return hasher.hexdigest()[:16]
          ```
      pitfalls:
      - Symlinks in tarball can escape extraction directory
      - Cache key collision overwrites unrelated cache
      - Large artifacts exhaust memory without streaming
      acceptance_criteria:
      - Artifacts uploaded from one job are downloadable by subsequent dependent jobs
      - Artifact download verifies checksum to detect corruption during transfer or storage
      - Retention policy deletes artifacts older than configured age to reclaim storage space
      - Large artifacts are handled efficiently without exhausting memory during transfer
      deliverables:
      - Artifact upload after job completion storing output files in versioned storage
      - Artifact download between jobs retrieving stored artifacts for dependent steps
      - Artifact storage backend persisting files with checksums for integrity verification
      - Artifact retention policy automatically cleaning expired artifacts after configured period
    - name: Deployment Strategies
      description: Implement blue-green and canary deployment strategies with health checks and automatic rollback.
      hints:
        level1: 'Blue-green: switch traffic between two identical environments.'
        level2: 'Canary: gradually shift traffic percentage to new version.'
        level3: |-
          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass
          from enum import Enum
          import asyncio

          class DeploymentStatus(str, Enum):
              PENDING = "pending"
              IN_PROGRESS = "in_progress"
              SUCCESS = "success"
              FAILED = "failed"
              ROLLED_BACK = "rolled_back"

          @dataclass
          class DeploymentConfig:
              service_name: str
              version: str
              replicas: int
              health_check_path: str = "/health"
              health_check_interval: int = 5
              health_check_timeout: int = 30

          class DeploymentStrategy(ABC):
              @abstractmethod
              async def deploy(self, config: DeploymentConfig) -> DeploymentStatus:
                  pass

              @abstractmethod
              async def rollback(self, config: DeploymentConfig):
                  pass

          class BlueGreenDeployment(DeploymentStrategy):
              def __init__(self, infrastructure, load_balancer):
                  self.infra = infrastructure
                  self.lb = load_balancer

              async def deploy(self, config: DeploymentConfig) -> DeploymentStatus:
                  service = config.service_name

                  # Determine current (blue) and new (green) environments
                  current = await self.lb.get_active_environment(service)
                  new_env = "green" if current == "blue" else "blue"

                  try:
                      # Deploy to inactive environment
                      await self.infra.deploy(
                          f"{service}-{new_env}",
                          config.version,
                          config.replicas
                      )

                      # Wait for health checks
                      healthy = await self._wait_healthy(
                          f"{service}-{new_env}",
                          config.health_check_path,
                          config.health_check_timeout
                      )

                      if not healthy:
                          await self.infra.destroy(f"{service}-{new_env}")
                          return DeploymentStatus.FAILED

                      # Switch traffic
                      await self.lb.switch_traffic(service, new_env)

                      # Keep old environment for quick rollback
                      # (destroy after confirmation period)

                      return DeploymentStatus.SUCCESS

                  except Exception as e:
                      await self.rollback(config)
                      return DeploymentStatus.FAILED

              async def rollback(self, config: DeploymentConfig):
                  service = config.service_name
                  current = await self.lb.get_active_environment(service)
                  previous = "green" if current == "blue" else "blue"
                  await self.lb.switch_traffic(service, previous)

              async def _wait_healthy(self, env: str, path: str, timeout: int) -> bool:
                  deadline = asyncio.get_event_loop().time() + timeout
                  while asyncio.get_event_loop().time() < deadline:
                      if await self.infra.health_check(env, path):
                          return True
                      await asyncio.sleep(5)
                  return False

          class CanaryDeployment(DeploymentStrategy):
              def __init__(self, infrastructure, load_balancer, metrics):
                  self.infra = infrastructure
                  self.lb = load_balancer
                  self.metrics = metrics
                  self.stages = [5, 25, 50, 75, 100]  # Traffic percentages
                  self.stage_duration = 300  # 5 minutes per stage

              async def deploy(self, config: DeploymentConfig) -> DeploymentStatus:
                  service = config.service_name
                  canary = f"{service}-canary"

                  try:
                      # Deploy canary with minimal replicas
                      await self.infra.deploy(canary, config.version, 1)

                      # Wait for initial health
                      if not await self._wait_healthy(canary, config.health_check_path, 60):
                          await self.infra.destroy(canary)
                          return DeploymentStatus.FAILED

                      # Progressive traffic shift
                      for percentage in self.stages:
                          await self.lb.set_canary_weight(service, percentage)

                          # Monitor for anomalies
                          healthy = await self._monitor_stage(
                              service, canary, self.stage_duration
                          )

                          if not healthy:
                              await self._auto_rollback(service, canary)
                              return DeploymentStatus.ROLLED_BACK

                      # Canary successful - promote to production
                      await self.infra.scale(canary, config.replicas)
                      await self.lb.promote_canary(service)
                      await self.infra.destroy(f"{service}-stable")

                      return DeploymentStatus.SUCCESS

                  except Exception:
                      await self._auto_rollback(service, canary)
                      return DeploymentStatus.FAILED

              async def _monitor_stage(self, service: str, canary: str,
                                      duration: int) -> bool:
                  """Monitor canary health during traffic stage."""
                  end_time = asyncio.get_event_loop().time() + duration

                  while asyncio.get_event_loop().time() < end_time:
                      # Check error rate
                      canary_errors = await self.metrics.get_error_rate(canary)
                      stable_errors = await self.metrics.get_error_rate(f"{service}-stable")

                      # Canary error rate significantly higher = problem
                      if canary_errors > stable_errors * 1.5:
                          return False

                      # Check latency
                      canary_p99 = await self.metrics.get_latency_p99(canary)
                      stable_p99 = await self.metrics.get_latency_p99(f"{service}-stable")

                      if canary_p99 > stable_p99 * 2:
                          return False

                      await asyncio.sleep(30)

                  return True

              async def _auto_rollback(self, service: str, canary: str):
                  await self.lb.set_canary_weight(service, 0)
                  await self.infra.destroy(canary)

              async def rollback(self, config: DeploymentConfig):
                  await self._auto_rollback(config.service_name, f"{config.service_name}-canary")
          ```
      pitfalls:
      - Database migrations incompatible between versions
      - Session affinity breaks during traffic switch
      - Metrics lag causes delayed rollback decision
      acceptance_criteria:
      - Rolling deployment updates one instance at a time verifying health before proceeding
      - Blue-green deployment switches traffic atomically after new environment passes health checks
      - Canary deployment incrementally shifts traffic and monitors error rates at each stage
      - Manual approval gates pause deployment pipeline until authorized user approves continuation
      deliverables:
      - Rolling deployment updating instances incrementally with health check validation
      - Blue-green deployment switching traffic between two identical environment versions
      - Canary deployment gradually shifting traffic percentage to new version with monitoring
      - Rollback automation reverting to previous version when deployment health checks fail
  container-runtime:
    name: Container Runtime
    description: Build a minimal container runtime using Linux namespaces, cgroups, and overlay filesystem for process isolation.
    why_important: Understanding containers at the kernel level helps debug container issues, optimize performance, and secure
      deployments.
    difficulty: advanced
    tags:
    - containers
    - linux
    - systems
    estimated_hours: 50
    prerequisites:
    - shell
    learning_outcomes:
    - Use Linux namespaces for isolation
    - Implement cgroups for resource limits
    - Build overlay filesystem for images
    - Handle container networking
    milestones:
    - name: Process Isolation with Namespaces
      description: Create isolated process environment using PID, mount, network, UTS, and user namespaces.
      hints:
        level1: Use clone() with CLONE_NEWPID, CLONE_NEWNS flags.
        level2: Set up new root filesystem with pivot_root or chroot.
        level3: |-
          ```c
          #define _GNU_SOURCE
          #include <sched.h>
          #include <stdio.h>
          #include <stdlib.h>
          #include <unistd.h>
          #include <sys/wait.h>
          #include <sys/mount.h>
          #include <sys/syscall.h>

          #define STACK_SIZE (1024 * 1024)

          static char child_stack[STACK_SIZE];

          typedef struct {
              char *rootfs;
              char *hostname;
              char **argv;
          } container_config;

          static int pivot_root(const char *new_root, const char *put_old) {
              return syscall(SYS_pivot_root, new_root, put_old);
          }

          static int setup_root(const char *rootfs) {
              // Mount rootfs as private
              if (mount(NULL, "/", NULL, MS_REC | MS_PRIVATE, NULL) < 0) {
                  perror("mount private");
                  return -1;
              }

              // Bind mount new root
              if (mount(rootfs, rootfs, NULL, MS_BIND | MS_REC, NULL) < 0) {
                  perror("mount bind rootfs");
                  return -1;
              }

              // Create put_old directory
              char put_old[256];
              snprintf(put_old, sizeof(put_old), "%s/.pivot_root", rootfs);
              mkdir(put_old, 0700);

              // Pivot root
              if (pivot_root(rootfs, put_old) < 0) {
                  perror("pivot_root");
                  return -1;
              }

              // Change to new root
              if (chdir("/") < 0) {
                  perror("chdir");
                  return -1;
              }

              // Unmount old root
              if (umount2("/.pivot_root", MNT_DETACH) < 0) {
                  perror("umount old root");
                  return -1;
              }
              rmdir("/.pivot_root");

              return 0;
          }

          static int setup_mounts(void) {
              // Mount proc
              if (mount("proc", "/proc", "proc", 0, NULL) < 0) {
                  perror("mount proc");
                  return -1;
              }

              // Mount sysfs
              if (mount("sysfs", "/sys", "sysfs", 0, NULL) < 0) {
                  perror("mount sysfs");
                  return -1;
              }

              // Mount tmpfs for /tmp
              if (mount("tmpfs", "/tmp", "tmpfs", 0, NULL) < 0) {
                  perror("mount tmpfs");
                  return -1;
              }

              return 0;
          }

          static int child_fn(void *arg) {
              container_config *config = (container_config *)arg;

              // Set hostname
              if (sethostname(config->hostname, strlen(config->hostname)) < 0) {
                  perror("sethostname");
                  return 1;
              }

              // Setup root filesystem
              if (setup_root(config->rootfs) < 0) {
                  return 1;
              }

              // Setup mounts
              if (setup_mounts() < 0) {
                  return 1;
              }

              // Execute command
              execvp(config->argv[0], config->argv);
              perror("execvp");
              return 1;
          }

          int run_container(container_config *config) {
              int flags = CLONE_NEWPID |  // New PID namespace
                          CLONE_NEWNS |   // New mount namespace
                          CLONE_NEWUTS |  // New UTS namespace (hostname)
                          CLONE_NEWNET |  // New network namespace
                          CLONE_NEWIPC |  // New IPC namespace
                          SIGCHLD;

              pid_t pid = clone(child_fn, child_stack + STACK_SIZE, flags, config);
              if (pid < 0) {
                  perror("clone");
                  return -1;
              }

              // Wait for child
              int status;
              waitpid(pid, &status, 0);
              return WEXITSTATUS(status);
          }
          ```
      pitfalls:
      - pivot_root fails if new root is not a mount point
      - Forgetting to mount /proc breaks process tools
      - User namespace UID mapping required for unprivileged containers
      acceptance_criteria:
      - Create PID namespace where container process sees itself as PID 1
      - Create mount namespace providing isolated filesystem view for container
      - Create network namespace providing isolated network interfaces and routing
      - Create UTS namespace allowing container to have its own hostname
      deliverables:
      - PID namespace setup for process ID isolation
      - Mount namespace setup for filesystem view isolation
      - Network namespace setup for network stack isolation
      - UTS and IPC namespaces for hostname and IPC isolation
    - name: Resource Limits with Cgroups
      description: Implement CPU, memory, and I/O limits using cgroups v2 for container resource control.
      hints:
        level1: Write to cgroup filesystem to set limits.
        level2: Create cgroup hierarchy per container, add process to group.
        level3: |-
          ```python
          import os
          from dataclasses import dataclass
          from pathlib import Path

          @dataclass
          class ResourceLimits:
              memory_limit: int = 0      # bytes, 0 = unlimited
              memory_swap: int = 0       # bytes
              cpu_shares: int = 1024     # relative weight
              cpu_quota: int = 0         # microseconds per period
              cpu_period: int = 100000   # microseconds
              pids_max: int = 0          # max processes

          class CgroupManager:
              def __init__(self, cgroup_root: str = "/sys/fs/cgroup"):
                  self.root = Path(cgroup_root)

              def create(self, container_id: str, limits: ResourceLimits) -> str:
                  """Create cgroup for container and apply limits."""
                  cgroup_path = self.root / "containers" / container_id
                  cgroup_path.mkdir(parents=True, exist_ok=True)

                  # Enable controllers for this cgroup
                  self._enable_controllers(cgroup_path.parent)

                  # Apply memory limits
                  if limits.memory_limit > 0:
                      (cgroup_path / "memory.max").write_text(str(limits.memory_limit))
                      if limits.memory_swap >= 0:
                          swap_max = limits.memory_limit + limits.memory_swap
                          (cgroup_path / "memory.swap.max").write_text(str(swap_max))

                  # Apply CPU limits
                  if limits.cpu_quota > 0:
                      cpu_max = f"{limits.cpu_quota} {limits.cpu_period}"
                      (cgroup_path / "cpu.max").write_text(cpu_max)

                  (cgroup_path / "cpu.weight").write_text(str(limits.cpu_shares))

                  # Apply PID limits
                  if limits.pids_max > 0:
                      (cgroup_path / "pids.max").write_text(str(limits.pids_max))

                  return str(cgroup_path)

              def add_process(self, cgroup_path: str, pid: int):
                  """Add process to cgroup."""
                  procs_file = Path(cgroup_path) / "cgroup.procs"
                  procs_file.write_text(str(pid))

              def get_stats(self, container_id: str) -> dict:
                  """Get resource usage statistics."""
                  cgroup_path = self.root / "containers" / container_id

                  stats = {}

                  # Memory stats
                  memory_current = cgroup_path / "memory.current"
                  if memory_current.exists():
                      stats['memory_usage'] = int(memory_current.read_text().strip())

                  memory_stat = cgroup_path / "memory.stat"
                  if memory_stat.exists():
                      for line in memory_stat.read_text().splitlines():
                          key, value = line.split()
                          stats[f'memory_{key}'] = int(value)

                  # CPU stats
                  cpu_stat = cgroup_path / "cpu.stat"
                  if cpu_stat.exists():
                      for line in cpu_stat.read_text().splitlines():
                          key, value = line.split()
                          stats[f'cpu_{key}'] = int(value)

                  # PID stats
                  pids_current = cgroup_path / "pids.current"
                  if pids_current.exists():
                      stats['pids_current'] = int(pids_current.read_text().strip())

                  return stats

              def destroy(self, container_id: str):
                  """Remove cgroup (processes must be terminated first)."""
                  cgroup_path = self.root / "containers" / container_id

                  # Kill any remaining processes
                  procs_file = cgroup_path / "cgroup.procs"
                  if procs_file.exists():
                      pids = procs_file.read_text().strip().split()
                      for pid in pids:
                          try:
                              os.kill(int(pid), 9)
                          except ProcessLookupError:
                              pass

                  # Remove cgroup directory
                  cgroup_path.rmdir()

              def _enable_controllers(self, parent_path: Path):
                  """Enable controllers in parent cgroup."""
                  subtree_control = parent_path / "cgroup.subtree_control"
                  if subtree_control.exists():
                      current = subtree_control.read_text()
                      needed = "+cpu +memory +pids +io"
                      for controller in needed.split():
                          if controller[1:] not in current:
                              try:
                                  subtree_control.write_text(controller)
                              except PermissionError:
                                  pass  # Controller might not be available
          ```
      pitfalls:
      - Cgroups v1 vs v2 have different interfaces
      - Can't remove cgroup with active processes
      - Memory limit without swap limit allows OOM escape
      acceptance_criteria:
      - Set memory limits using cgroups v2 memory.max controller correctly
      - Set CPU limits using cpu.max quota and period parameters
      - Monitor resource usage and report current consumption statistics
      - Handle OOM situations gracefully with configurable OOM behavior
      deliverables:
      - Cgroup controller setup for v2 unified hierarchy
      - Memory limits with hard cap and soft limit configuration
      - CPU limits using cpu.max quota and period values
      - Device access control restricting container device visibility
    - name: Overlay Filesystem
      description: Implement image layering using overlayfs for copy-on-write filesystem support.
      hints:
        level1: Stack read-only layers with one writable upper layer.
        level2: Handle whiteout files for deletions in upper layer.
        level3: |-
          ```python
          import os
          import shutil
          from dataclasses import dataclass, field
          from pathlib import Path
          import subprocess
          import json

          @dataclass
          class ImageLayer:
              id: str
              parent: str = None
              diff_path: str = ""

          @dataclass
          class Image:
              id: str
              layers: list[ImageLayer] = field(default_factory=list)
              config: dict = field(default_factory=dict)

          class OverlayManager:
              def __init__(self, storage_root: str):
                  self.root = Path(storage_root)
                  self.layers_dir = self.root / "layers"
                  self.images_dir = self.root / "images"
                  self.containers_dir = self.root / "containers"

                  for d in [self.layers_dir, self.images_dir, self.containers_dir]:
                      d.mkdir(parents=True, exist_ok=True)

              def create_layer(self, layer_id: str, parent_id: str = None) -> ImageLayer:
                  """Create a new image layer."""
                  layer_path = self.layers_dir / layer_id
                  layer_path.mkdir(exist_ok=True)
                  (layer_path / "diff").mkdir(exist_ok=True)

                  return ImageLayer(
                      id=layer_id,
                      parent=parent_id,
                      diff_path=str(layer_path / "diff")
                  )

              def prepare_container(self, container_id: str, image: Image) -> str:
                  """Prepare overlay mount for container."""
                  container_path = self.containers_dir / container_id
                  container_path.mkdir(exist_ok=True)

                  # Create container-specific directories
                  upper_dir = container_path / "upper"
                  work_dir = container_path / "work"
                  merged_dir = container_path / "merged"

                  for d in [upper_dir, work_dir, merged_dir]:
                      d.mkdir(exist_ok=True)

                  # Build lower directories (image layers in order)
                  lower_dirs = []
                  for layer in reversed(image.layers):
                      lower_dirs.append(layer.diff_path)

                  return self._mount_overlay(
                      lower_dirs=lower_dirs,
                      upper_dir=str(upper_dir),
                      work_dir=str(work_dir),
                      merged_dir=str(merged_dir)
                  )

              def _mount_overlay(self, lower_dirs: list[str], upper_dir: str,
                                work_dir: str, merged_dir: str) -> str:
                  """Mount overlayfs."""
                  lower = ":".join(lower_dirs)
                  options = f"lowerdir={lower},upperdir={upper_dir},workdir={work_dir}"

                  subprocess.run([
                      "mount", "-t", "overlay", "overlay",
                      "-o", options, merged_dir
                  ], check=True)

                  return merged_dir

              def commit_container(self, container_id: str, new_image_id: str) -> Image:
                  """Create new image from container's upper layer."""
                  container_path = self.containers_dir / container_id
                  upper_dir = container_path / "upper"

                  # Create new layer from upper
                  new_layer = self.create_layer(new_image_id)
                  shutil.copytree(upper_dir, new_layer.diff_path, dirs_exist_ok=True)

                  # Process whiteouts (files starting with .wh.)
                  self._process_whiteouts(Path(new_layer.diff_path))

                  return Image(id=new_image_id, layers=[new_layer])

              def _process_whiteouts(self, layer_path: Path):
                  """Handle overlayfs whiteout files."""
                  for root, dirs, files in os.walk(layer_path):
                      for f in files:
                          if f.startswith('.wh.'):
                              whiteout_path = Path(root) / f
                              # Convert to OCI whiteout format or mark for deletion
                              original_name = f[4:]  # Remove .wh. prefix
                              whiteout_path.rename(Path(root) / f".wh.{original_name}")

              def cleanup_container(self, container_id: str):
                  """Unmount and remove container filesystem."""
                  container_path = self.containers_dir / container_id
                  merged_dir = container_path / "merged"

                  # Unmount overlay
                  subprocess.run(["umount", str(merged_dir)], check=False)

                  # Remove container directory
                  shutil.rmtree(container_path, ignore_errors=True)
          ```
      pitfalls:
      - Overlayfs requires specific kernel version (3.18+)
      - Work directory must be empty and same filesystem as upper
      - Hardlinks across layers cause unexpected behavior
      acceptance_criteria:
      - Mount overlay filesystem for container root combining image layers
      - Handle multiple image layers stacked in correct order for overlayFS
      - Support copy-on-write semantics so file changes stay in upper layer
      - Clean up overlay layers and mount points on container removal
      deliverables:
      - OverlayFS mount combining multiple read-only layers with writable layer
      - Layer management for image layer stacking and ordering
      - Copy-on-write semantics for efficient file modifications
      - Layer caching to avoid redundant downloads and extraction
    - name: Container Networking
      description: Implement bridge networking for containers with port mapping and inter-container communication.
      hints:
        level1: Create veth pairs, attach one end to bridge, one to container.
        level2: Use iptables for NAT and port forwarding.
        level3: |-
          ```python
          import subprocess
          import ipaddress
          from dataclasses import dataclass
          from typing import Optional

          @dataclass
          class NetworkConfig:
              bridge_name: str = "container0"
              bridge_ip: str = "172.17.0.1/16"
              subnet: str = "172.17.0.0/16"

          @dataclass
          class ContainerNetwork:
              container_id: str
              ip_address: str
              mac_address: str
              veth_host: str
              veth_container: str
              port_mappings: dict = None  # {host_port: container_port}

          class NetworkManager:
              def __init__(self, config: NetworkConfig = None):
                  self.config = config or NetworkConfig()
                  self.subnet = ipaddress.ip_network(self.config.subnet)
                  self.allocated_ips = set()
                  self._setup_bridge()

              def _setup_bridge(self):
                  """Create network bridge if not exists."""
                  bridge = self.config.bridge_name

                  # Create bridge
                  subprocess.run([
                      "ip", "link", "add", bridge, "type", "bridge"
                  ], check=False)

                  # Set bridge IP
                  subprocess.run([
                      "ip", "addr", "add", self.config.bridge_ip, "dev", bridge
                  ], check=False)

                  # Bring up bridge
                  subprocess.run(["ip", "link", "set", bridge, "up"], check=True)

                  # Enable IP forwarding
                  with open("/proc/sys/net/ipv4/ip_forward", "w") as f:
                      f.write("1")

                  # Setup NAT for outbound traffic
                  subprocess.run([
                      "iptables", "-t", "nat", "-A", "POSTROUTING",
                      "-s", self.config.subnet, "-j", "MASQUERADE"
                  ], check=False)

              def connect(self, container_id: str, pid: int,
                         port_mappings: dict = None) -> ContainerNetwork:
                  """Connect container to bridge network."""
                  # Allocate IP
                  ip = self._allocate_ip()

                  # Create veth pair
                  veth_host = f"veth{container_id[:8]}"
                  veth_container = "eth0"

                  subprocess.run([
                      "ip", "link", "add", veth_host, "type", "veth",
                      "peer", "name", veth_container
                  ], check=True)

                  # Attach host end to bridge
                  subprocess.run([
                      "ip", "link", "set", veth_host, "master", self.config.bridge_name
                  ], check=True)
                  subprocess.run(["ip", "link", "set", veth_host, "up"], check=True)

                  # Move container end to container's network namespace
                  subprocess.run([
                      "ip", "link", "set", veth_container, "netns", str(pid)
                  ], check=True)

                  # Configure container interface (run in container's netns)
                  subprocess.run([
                      "nsenter", "-t", str(pid), "-n",
                      "ip", "addr", "add", f"{ip}/16", "dev", veth_container
                  ], check=True)

                  subprocess.run([
                      "nsenter", "-t", str(pid), "-n",
                      "ip", "link", "set", veth_container, "up"
                  ], check=True)

                  subprocess.run([
                      "nsenter", "-t", str(pid), "-n",
                      "ip", "link", "set", "lo", "up"
                  ], check=True)

                  # Set default route
                  gateway = self.config.bridge_ip.split('/')[0]
                  subprocess.run([
                      "nsenter", "-t", str(pid), "-n",
                      "ip", "route", "add", "default", "via", gateway
                  ], check=True)

                  # Setup port mappings
                  if port_mappings:
                      for host_port, container_port in port_mappings.items():
                          self._add_port_mapping(ip, host_port, container_port)

                  return ContainerNetwork(
                      container_id=container_id,
                      ip_address=str(ip),
                      mac_address=self._get_mac(veth_host),
                      veth_host=veth_host,
                      veth_container=veth_container,
                      port_mappings=port_mappings
                  )

              def _allocate_ip(self) -> ipaddress.IPv4Address:
                  for ip in self.subnet.hosts():
                      if ip not in self.allocated_ips and str(ip) != self.config.bridge_ip.split('/')[0]:
                          self.allocated_ips.add(ip)
                          return ip
                  raise RuntimeError("No available IP addresses")

              def _add_port_mapping(self, container_ip: str, host_port: int,
                                   container_port: int):
                  subprocess.run([
                      "iptables", "-t", "nat", "-A", "PREROUTING",
                      "-p", "tcp", "--dport", str(host_port),
                      "-j", "DNAT", "--to-destination", f"{container_ip}:{container_port}"
                  ], check=True)

              def disconnect(self, network: ContainerNetwork):
                  """Remove container from network."""
                  # Remove port mappings
                  if network.port_mappings:
                      for host_port, container_port in network.port_mappings.items():
                          subprocess.run([
                              "iptables", "-t", "nat", "-D", "PREROUTING",
                              "-p", "tcp", "--dport", str(host_port),
                              "-j", "DNAT", "--to-destination",
                              f"{network.ip_address}:{container_port}"
                          ], check=False)

                  # Delete veth pair
                  subprocess.run([
                      "ip", "link", "delete", network.veth_host
                  ], check=False)

                  # Release IP
                  ip = ipaddress.ip_address(network.ip_address)
                  self.allocated_ips.discard(ip)

              def _get_mac(self, interface: str) -> str:
                  result = subprocess.run(
                      ["cat", f"/sys/class/net/{interface}/address"],
                      capture_output=True, text=True
                  )
                  return result.stdout.strip()
          ```
      pitfalls:
      - Container loses network if host veth deleted before container end
      - iptables rules persist after container death
      - MTU mismatch causes packet fragmentation issues
      acceptance_criteria:
      - Create veth pair for container and attach one end to bridge
      - Set up bridge networking connecting multiple containers on same network
      - Implement port mapping using NAT rules for host-to-container access
      - Handle container DNS resolution using configurable nameserver settings
      deliverables:
      - Veth pair creation connecting container to host network namespace
      - Bridge setup for connecting multiple container network interfaces
      - IP address assignment from configurable subnet pool
      - Port forwarding using NAT rules for external access
  service-mesh:
    name: Service Mesh Sidecar
    description: Build a service mesh sidecar proxy handling service discovery, load balancing, circuit breaking, and mTLS.
    why_important: Service meshes like Istio and Linkerd are essential for microservices. Understanding the proxy layer helps
      with debugging and optimization.
    difficulty: advanced
    tags:
    - microservices
    - networking
    - distributed-systems
    estimated_hours: 55
    prerequisites:
    - api-gateway
    - circuit-breaker
    learning_outcomes:
    - Implement transparent traffic interception
    - Build service discovery integration
    - Handle mTLS between services
    - Implement advanced load balancing
    milestones:
    - name: Traffic Interception
      description: Intercept inbound and outbound traffic transparently using iptables redirect rules.
      hints:
        level1: Use iptables REDIRECT to send traffic to proxy port.
        level2: Preserve original destination using SO_ORIGINAL_DST.
        level3: |-
          ```python
          import socket
          import struct
          import subprocess
          from dataclasses import dataclass

          SO_ORIGINAL_DST = 80

          @dataclass
          class InterceptConfig:
              inbound_port: int = 15001
              outbound_port: int = 15006
              proxy_uid: int = 1337
              exclude_ports: list = None

          class TrafficInterceptor:
              def __init__(self, config: InterceptConfig = None):
                  self.config = config or InterceptConfig()

              def setup_iptables(self):
                  """Setup iptables rules for traffic interception."""
                  # Create PROXY_REDIRECT chain
                  subprocess.run([
                      "iptables", "-t", "nat", "-N", "PROXY_REDIRECT"
                  ], check=False)

                  # Redirect to proxy
                  subprocess.run([
                      "iptables", "-t", "nat", "-A", "PROXY_REDIRECT",
                      "-p", "tcp", "-j", "REDIRECT", "--to-port", str(self.config.outbound_port)
                  ], check=True)

                  # Create PROXY_OUTPUT chain for outbound
                  subprocess.run([
                      "iptables", "-t", "nat", "-N", "PROXY_OUTPUT"
                  ], check=False)

                  # Exclude proxy's own traffic (prevent loops)
                  subprocess.run([
                      "iptables", "-t", "nat", "-A", "PROXY_OUTPUT",
                      "-m", "owner", "--uid-owner", str(self.config.proxy_uid),
                      "-j", "RETURN"
                  ], check=True)

                  # Exclude localhost
                  subprocess.run([
                      "iptables", "-t", "nat", "-A", "PROXY_OUTPUT",
                      "-d", "127.0.0.1/32", "-j", "RETURN"
                  ], check=True)

                  # Exclude specific ports
                  for port in (self.config.exclude_ports or []):
                      subprocess.run([
                          "iptables", "-t", "nat", "-A", "PROXY_OUTPUT",
                          "-p", "tcp", "--dport", str(port), "-j", "RETURN"
                      ], check=True)

                  # Send everything else to proxy
                  subprocess.run([
                      "iptables", "-t", "nat", "-A", "PROXY_OUTPUT",
                      "-j", "PROXY_REDIRECT"
                  ], check=True)

                  # Apply to OUTPUT chain
                  subprocess.run([
                      "iptables", "-t", "nat", "-A", "OUTPUT",
                      "-p", "tcp", "-j", "PROXY_OUTPUT"
                  ], check=True)

                  # Setup inbound interception
                  subprocess.run([
                      "iptables", "-t", "nat", "-A", "PREROUTING",
                      "-p", "tcp", "-j", "REDIRECT", "--to-port", str(self.config.inbound_port)
                  ], check=True)

              def get_original_dest(self, client_socket) -> tuple[str, int]:
                  """Get original destination from redirected socket."""
                  # Get original destination using SO_ORIGINAL_DST
                  dst = client_socket.getsockopt(socket.SOL_IP, SO_ORIGINAL_DST, 16)

                  # Parse sockaddr_in structure
                  port = struct.unpack('>H', dst[2:4])[0]
                  ip = socket.inet_ntoa(dst[4:8])

                  return ip, port

              def cleanup(self):
                  """Remove iptables rules."""
                  subprocess.run([
                      "iptables", "-t", "nat", "-D", "OUTPUT", "-p", "tcp", "-j", "PROXY_OUTPUT"
                  ], check=False)
                  subprocess.run([
                      "iptables", "-t", "nat", "-F", "PROXY_OUTPUT"
                  ], check=False)
                  subprocess.run([
                      "iptables", "-t", "nat", "-X", "PROXY_OUTPUT"
                  ], check=False)
                  subprocess.run([
                      "iptables", "-t", "nat", "-F", "PROXY_REDIRECT"
                  ], check=False)
                  subprocess.run([
                      "iptables", "-t", "nat", "-X", "PROXY_REDIRECT"
                  ], check=False)
          ```
      pitfalls:
      - Redirect loop if proxy traffic not excluded
      - SO_ORIGINAL_DST not available on all systems
      - IPv6 requires separate ip6tables rules
      acceptance_criteria:
      - Intercept all inbound and outbound traffic via iptables redirect rules
      - Handle transparent proxying without requiring application code changes
      - Parse and identify HTTP and gRPC protocols from intercepted traffic
      - Support pass-through for unknown or unrecognized protocol traffic
      deliverables:
      - Iptables rules redirecting pod traffic through sidecar proxy
      - Sidecar proxy process intercepting all inbound and outbound connections
      - Transparent proxying forwarding traffic without application changes
      - Protocol detection identifying HTTP, gRPC, and TCP traffic automatically
    - name: Service Discovery Integration
      description: Integrate with service discovery (Consul, Kubernetes) to resolve service names to endpoints.
      hints:
        level1: Watch service registry for endpoint changes.
        level2: Cache endpoints locally, handle stale entries.
        level3: |-
          ```python
          import asyncio
          import aiohttp
          from dataclasses import dataclass, field
          from typing import Optional
          from abc import ABC, abstractmethod

          @dataclass
          class Endpoint:
              address: str
              port: int
              weight: int = 100
              healthy: bool = True
              metadata: dict = field(default_factory=dict)

          @dataclass
          class Service:
              name: str
              endpoints: list[Endpoint] = field(default_factory=list)
              version: str = ""

          class ServiceDiscovery(ABC):
              @abstractmethod
              async def get_service(self, name: str) -> Optional[Service]:
                  pass

              @abstractmethod
              async def watch(self, name: str, callback):
                  pass

          class KubernetesDiscovery(ServiceDiscovery):
              def __init__(self, namespace: str = "default"):
                  self.namespace = namespace
                  self.base_url = "https://kubernetes.default.svc"
                  self._cache: dict[str, Service] = {}
                  self._watchers: dict[str, asyncio.Task] = {}

              async def get_service(self, name: str) -> Optional[Service]:
                  if name in self._cache:
                      return self._cache[name]

                  async with aiohttp.ClientSession() as session:
                      url = f"{self.base_url}/api/v1/namespaces/{self.namespace}/endpoints/{name}"
                      headers = {"Authorization": f"Bearer {self._get_token()}"}

                      async with session.get(url, headers=headers, ssl=False) as resp:
                          if resp.status != 200:
                              return None

                          data = await resp.json()
                          service = self._parse_endpoints(name, data)
                          self._cache[name] = service
                          return service

              async def watch(self, name: str, callback):
                  """Watch for endpoint changes."""
                  async with aiohttp.ClientSession() as session:
                      url = f"{self.base_url}/api/v1/namespaces/{self.namespace}/endpoints"
                      params = {"watch": "true", "fieldSelector": f"metadata.name={name}"}
                      headers = {"Authorization": f"Bearer {self._get_token()}"}

                      async with session.get(url, params=params, headers=headers, ssl=False) as resp:
                          async for line in resp.content:
                              if line:
                                  import json
                                  event = json.loads(line)
                                  service = self._parse_endpoints(name, event['object'])
                                  self._cache[name] = service
                                  await callback(service)

              def _parse_endpoints(self, name: str, data: dict) -> Service:
                  endpoints = []
                  for subset in data.get('subsets', []):
                      addresses = subset.get('addresses', [])
                      ports = subset.get('ports', [])

                      for addr in addresses:
                          for port in ports:
                              endpoints.append(Endpoint(
                                  address=addr['ip'],
                                  port=port['port'],
                                  metadata={
                                      'node': addr.get('nodeName'),
                                      'pod': addr.get('targetRef', {}).get('name')
                                  }
                              ))

                  return Service(name=name, endpoints=endpoints)

              def _get_token(self) -> str:
                  with open('/var/run/secrets/kubernetes.io/serviceaccount/token') as f:
                      return f.read()

          class ServiceRegistry:
              def __init__(self, discovery: ServiceDiscovery):
                  self.discovery = discovery
                  self.services: dict[str, Service] = {}
                  self._watch_tasks: dict[str, asyncio.Task] = {}

              async def resolve(self, service_name: str) -> list[Endpoint]:
                  """Resolve service name to healthy endpoints."""
                  if service_name not in self.services:
                      service = await self.discovery.get_service(service_name)
                      if service:
                          self.services[service_name] = service
                          self._start_watch(service_name)

                  service = self.services.get(service_name)
                  if not service:
                      return []

                  return [ep for ep in service.endpoints if ep.healthy]

              def _start_watch(self, service_name: str):
                  if service_name not in self._watch_tasks:
                      self._watch_tasks[service_name] = asyncio.create_task(
                          self.discovery.watch(service_name, self._on_update)
                      )

              async def _on_update(self, service: Service):
                  self.services[service.name] = service
          ```
      pitfalls:
      - Watch connection drops require reconnection logic
      - Stale cache serves dead endpoints
      - DNS caching conflicts with dynamic discovery
      acceptance_criteria:
      - Integrate with Kubernetes or Consul for service endpoint discovery
      - Handle endpoint additions and removals as services scale
      - Support DNS-based service discovery as alternative to registry
      - Cache discovered service endpoints locally for fast lookup
      deliverables:
      - Endpoint discovery fetching available service instance addresses
      - Service registry synchronization keeping local state up to date
      - Health status tracking monitoring liveness of discovered endpoints
      - Client-side load balancing distributing requests across healthy instances
    - name: mTLS and Certificate Management
      description: Implement mutual TLS between services with automatic certificate rotation.
      hints:
        level1: Each service needs its own certificate signed by mesh CA.
        level2: Rotate certificates before expiry, handle during active connections.
        level3: |-
          ```python
          import ssl
          import asyncio
          from dataclasses import dataclass
          from datetime import datetime, timedelta
          from pathlib import Path
          import subprocess

          @dataclass
          class Certificate:
              cert_path: str
              key_path: str
              ca_path: str
              expiry: datetime
              service_name: str

          class CertificateManager:
              def __init__(self, service_name: str, cert_dir: str = "/etc/certs"):
                  self.service_name = service_name
                  self.cert_dir = Path(cert_dir)
                  self.current_cert: Certificate = None
                  self._rotation_task = None
                  self.rotation_threshold = timedelta(hours=1)

              async def initialize(self):
                  """Load or request initial certificate."""
                  cert_path = self.cert_dir / "cert.pem"
                  key_path = self.cert_dir / "key.pem"
                  ca_path = self.cert_dir / "ca.pem"

                  if not cert_path.exists():
                      await self._request_certificate()

                  self.current_cert = Certificate(
                      cert_path=str(cert_path),
                      key_path=str(key_path),
                      ca_path=str(ca_path),
                      expiry=self._get_cert_expiry(cert_path),
                      service_name=self.service_name
                  )

                  self._rotation_task = asyncio.create_task(self._rotation_loop())

              def get_ssl_context(self, server: bool = False) -> ssl.SSLContext:
                  """Get SSL context for mTLS connections."""
                  if server:
                      ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
                  else:
                      ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)

                  # Load certificate and key
                  ctx.load_cert_chain(
                      self.current_cert.cert_path,
                      self.current_cert.key_path
                  )

                  # Load CA for peer verification
                  ctx.load_verify_locations(self.current_cert.ca_path)

                  # Require client certificate (mTLS)
                  ctx.verify_mode = ssl.CERT_REQUIRED
                  ctx.check_hostname = not server

                  return ctx

              async def _rotation_loop(self):
                  """Check and rotate certificates before expiry."""
                  while True:
                      time_to_expiry = self.current_cert.expiry - datetime.utcnow()

                      if time_to_expiry < self.rotation_threshold:
                          await self._rotate_certificate()

                      # Check every minute
                      await asyncio.sleep(60)

              async def _rotate_certificate(self):
                  """Request new certificate and hot-swap."""
                  # Request new certificate
                  new_cert_path = self.cert_dir / "cert-new.pem"
                  new_key_path = self.cert_dir / "key-new.pem"

                  await self._request_certificate(
                      cert_path=new_cert_path,
                      key_path=new_key_path
                  )

                  # Atomic swap
                  import shutil
                  shutil.move(new_cert_path, self.cert_dir / "cert.pem")
                  shutil.move(new_key_path, self.cert_dir / "key.pem")

                  # Update current cert
                  self.current_cert.expiry = self._get_cert_expiry(self.cert_dir / "cert.pem")

              async def _request_certificate(self, cert_path: Path = None,
                                             key_path: Path = None):
                  """Request certificate from mesh CA (e.g., SPIFFE/SPIRE)."""
                  cert_path = cert_path or self.cert_dir / "cert.pem"
                  key_path = key_path or self.cert_dir / "key.pem"

                  # Generate CSR
                  subprocess.run([
                      "openssl", "req", "-new", "-newkey", "rsa:2048",
                      "-nodes", "-keyout", str(key_path),
                      "-out", "/tmp/csr.pem",
                      "-subj", f"/CN={self.service_name}"
                  ], check=True)

                  # In real implementation, send CSR to CA API
                  # For now, self-sign (development only)
                  subprocess.run([
                      "openssl", "x509", "-req",
                      "-in", "/tmp/csr.pem",
                      "-CA", str(self.cert_dir / "ca.pem"),
                      "-CAkey", str(self.cert_dir / "ca-key.pem"),
                      "-CAcreateserial",
                      "-out", str(cert_path),
                      "-days", "1"
                  ], check=True)

              def _get_cert_expiry(self, cert_path: Path) -> datetime:
                  result = subprocess.run([
                      "openssl", "x509", "-enddate", "-noout", "-in", str(cert_path)
                  ], capture_output=True, text=True)

                  # Parse: notAfter=Jan  1 00:00:00 2024 GMT
                  date_str = result.stdout.strip().split('=')[1]
                  return datetime.strptime(date_str, "%b %d %H:%M:%S %Y %Z")

          def verify_peer_identity(ssl_socket, expected_service: str) -> bool:
              """Verify peer certificate matches expected service identity."""
              cert = ssl_socket.getpeercert()

              # Check Common Name
              subject = dict(x[0] for x in cert['subject'])
              cn = subject.get('commonName', '')

              # Check SAN (Subject Alternative Names)
              san = cert.get('subjectAltName', [])
              dns_names = [name for type_, name in san if type_ == 'DNS']
              uri_names = [name for type_, name in san if type_ == 'URI']

              # Verify identity (SPIFFE format: spiffe://trust-domain/service-name)
              expected_spiffe = f"spiffe://mesh.local/{expected_service}"

              return (cn == expected_service or
                      expected_service in dns_names or
                      expected_spiffe in uri_names)
          ```
      pitfalls:
      - Certificate rotation during active requests causes failures
      - Clock skew makes valid certificates appear expired
      - Missing SAN in certificate breaks modern TLS verification
      acceptance_criteria:
      - Generate unique X.509 certificates for each service instance
      - Enforce mutual TLS authentication between all mesh services
      - Handle automatic certificate rotation before certificates expire
      - Verify service identity using certificate subject or SPIFFE ID
      deliverables:
      - X.509 certificate generation for each service identity
      - Automatic certificate rotation before expiration deadline
      - Mutual TLS enforcement requiring both client and server certificates
      - SPIFFE identity integration providing verifiable service identities
    - name: Load Balancing Algorithms
      description: Implement advanced load balancing including round-robin, least connections, weighted, and consistent hashing.
      hints:
        level1: Round-robin rotates through endpoints sequentially.
        level2: Least connections requires tracking active connection count.
        level3: |-
          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass, field
          import hashlib
          import bisect
          import random
          import threading

          @dataclass
          class EndpointStats:
              address: str
              active_connections: int = 0
              total_requests: int = 0
              failures: int = 0
              avg_latency_ms: float = 0
              weight: int = 100

          class LoadBalancer(ABC):
              @abstractmethod
              def select(self, endpoints: list[EndpointStats], key: str = None) -> EndpointStats:
                  pass

          class RoundRobinBalancer(LoadBalancer):
              def __init__(self):
                  self._index = 0
                  self._lock = threading.Lock()

              def select(self, endpoints: list[EndpointStats], key: str = None) -> EndpointStats:
                  if not endpoints:
                      return None

                  with self._lock:
                      endpoint = endpoints[self._index % len(endpoints)]
                      self._index += 1
                      return endpoint

          class WeightedRoundRobinBalancer(LoadBalancer):
              def __init__(self):
                  self._current_weight = 0
                  self._index = 0
                  self._lock = threading.Lock()

              def select(self, endpoints: list[EndpointStats], key: str = None) -> EndpointStats:
                  if not endpoints:
                      return None

                  with self._lock:
                      max_weight = max(ep.weight for ep in endpoints)
                      gcd_weight = self._gcd_of_weights([ep.weight for ep in endpoints])

                      while True:
                          self._index = (self._index + 1) % len(endpoints)
                          if self._index == 0:
                              self._current_weight -= gcd_weight
                              if self._current_weight <= 0:
                                  self._current_weight = max_weight

                          if endpoints[self._index].weight >= self._current_weight:
                              return endpoints[self._index]

              def _gcd_of_weights(self, weights: list[int]) -> int:
                  from math import gcd
                  from functools import reduce
                  return reduce(gcd, weights)

          class LeastConnectionsBalancer(LoadBalancer):
              def select(self, endpoints: list[EndpointStats], key: str = None) -> EndpointStats:
                  if not endpoints:
                      return None

                  # Select endpoint with fewest active connections
                  # Weighted by: connections / weight
                  return min(
                      endpoints,
                      key=lambda ep: ep.active_connections / max(ep.weight, 1)
                  )

          class ConsistentHashBalancer(LoadBalancer):
              def __init__(self, replicas: int = 150):
                  self.replicas = replicas
                  self._ring: list[tuple[int, str]] = []
                  self._endpoints: dict[str, EndpointStats] = {}

              def update_endpoints(self, endpoints: list[EndpointStats]):
                  """Rebuild hash ring when endpoints change."""
                  self._ring = []
                  self._endpoints = {ep.address: ep for ep in endpoints}

                  for ep in endpoints:
                      for i in range(self.replicas):
                          key = f"{ep.address}:{i}"
                          hash_val = self._hash(key)
                          bisect.insort(self._ring, (hash_val, ep.address))

              def select(self, endpoints: list[EndpointStats], key: str = None) -> EndpointStats:
                  if not self._ring:
                      self.update_endpoints(endpoints)

                  if not key:
                      key = str(random.random())

                  hash_val = self._hash(key)

                  # Binary search for first hash >= key hash
                  idx = bisect.bisect_left(self._ring, (hash_val, ""))
                  if idx >= len(self._ring):
                      idx = 0

                  address = self._ring[idx][1]
                  return self._endpoints.get(address)

              def _hash(self, key: str) -> int:
                  return int(hashlib.md5(key.encode()).hexdigest(), 16)

          class P2CBalancer(LoadBalancer):
              """Power of Two Choices - picks best of 2 random endpoints."""

              def select(self, endpoints: list[EndpointStats], key: str = None) -> EndpointStats:
                  if not endpoints:
                      return None
                  if len(endpoints) == 1:
                      return endpoints[0]

                  # Pick two random endpoints
                  a, b = random.sample(endpoints, 2)

                  # Return one with fewer connections (weighted)
                  score_a = a.active_connections / max(a.weight, 1)
                  score_b = b.active_connections / max(b.weight, 1)

                  return a if score_a <= score_b else b

          class AdaptiveBalancer(LoadBalancer):
              """Balancer that considers latency and error rates."""

              def select(self, endpoints: list[EndpointStats], key: str = None) -> EndpointStats:
                  if not endpoints:
                      return None

                  # Score based on multiple factors
                  def score(ep: EndpointStats) -> float:
                      # Lower is better
                      latency_factor = ep.avg_latency_ms / 100  # Normalize
                      error_rate = ep.failures / max(ep.total_requests, 1)
                      connection_factor = ep.active_connections / max(ep.weight, 1)

                      return (
                          latency_factor * 0.3 +
                          error_rate * 0.5 +
                          connection_factor * 0.2
                      )

                  return min(endpoints, key=score)
          ```
      pitfalls:
      - Consistent hashing needs ring rebuild on endpoint change
      - Least connections can thundering herd to recovered endpoint
      - Weight=0 causes division by zero
      acceptance_criteria:
      - Implement round-robin distribution across healthy service instances
      - Support least-connections routing to minimize backend load
      - Handle weighted distribution based on configured instance capacity
      - Implement health-aware routing excluding failed instances from selection
      deliverables:
      - Round-robin algorithm distributing requests evenly across backends
      - Least-connections algorithm routing to least loaded backend
      - Weighted distribution algorithm proportioning traffic by server capacity
      - Locality-aware routing preferring backends in same availability zone
  infrastructure-as-code:
    name: Infrastructure as Code Engine
    description: Build an IaC engine that parses declarative configs, manages state, computes diffs, and applies changes to
      infrastructure.
    why_important: Understanding IaC internals (like Terraform) helps debug state issues, write better modules, and build
      custom providers.
    difficulty: advanced
    tags:
    - devops
    - infrastructure
    - automation
    estimated_hours: 50
    prerequisites:
    - ci-cd-pipeline
    learning_outcomes:
    - Design declarative configuration language
    - Implement state management and locking
    - Build dependency graph for resource ordering
    - Handle provider abstraction for multi-cloud
    milestones:
    - name: Configuration Parser
      description: Parse HCL-like configuration files with resources, variables, outputs, and module references.
      hints:
        level1: Define grammar for resource blocks with attributes.
        level2: Handle variable interpolation and references.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from typing import Any, Optional
          import re

          @dataclass
          class Variable:
              name: str
              type: str = "string"
              default: Any = None
              description: str = ""

          @dataclass
          class Resource:
              type: str
              name: str
              attributes: dict = field(default_factory=dict)
              depends_on: list[str] = field(default_factory=list)
              count: int = 1
              provider: str = None

          @dataclass
          class Output:
              name: str
              value: Any
              description: str = ""

          @dataclass
          class Module:
              name: str
              source: str
              variables: dict = field(default_factory=dict)

          @dataclass
          class Configuration:
              variables: dict[str, Variable] = field(default_factory=dict)
              resources: dict[str, Resource] = field(default_factory=dict)
              outputs: dict[str, Output] = field(default_factory=dict)
              modules: dict[str, Module] = field(default_factory=dict)
              providers: dict[str, dict] = field(default_factory=dict)

          class ConfigParser:
              def __init__(self):
                  self.config = Configuration()
                  self._var_pattern = re.compile(r'\$\{([^}]+)\}')

              def parse(self, content: str) -> Configuration:
                  """Parse HCL-like configuration."""
                  import hcl2
                  import json

                  # Parse HCL to dict
                  data = hcl2.loads(content)

                  # Parse variables
                  for var_block in data.get('variable', []):
                      for name, attrs in var_block.items():
                          self.config.variables[name] = Variable(
                              name=name,
                              type=attrs.get('type', 'string'),
                              default=attrs.get('default'),
                              description=attrs.get('description', '')
                          )

                  # Parse resources
                  for res_block in data.get('resource', []):
                      for res_type, resources in res_block.items():
                          for res_name, attrs in resources.items():
                              resource_id = f"{res_type}.{res_name}"
                              self.config.resources[resource_id] = Resource(
                                  type=res_type,
                                  name=res_name,
                                  attributes=attrs,
                                  depends_on=attrs.pop('depends_on', []),
                                  count=attrs.pop('count', 1),
                                  provider=attrs.pop('provider', None)
                              )

                  # Parse outputs
                  for out_block in data.get('output', []):
                      for name, attrs in out_block.items():
                          self.config.outputs[name] = Output(
                              name=name,
                              value=attrs.get('value'),
                              description=attrs.get('description', '')
                          )

                  # Parse modules
                  for mod_block in data.get('module', []):
                      for name, attrs in mod_block.items():
                          self.config.modules[name] = Module(
                              name=name,
                              source=attrs.pop('source'),
                              variables=attrs
                          )

                  return self.config

              def resolve_references(self, value: Any, context: dict) -> Any:
                  """Resolve variable and resource references."""
                  if isinstance(value, str):
                      return self._resolve_string(value, context)
                  elif isinstance(value, list):
                      return [self.resolve_references(v, context) for v in value]
                  elif isinstance(value, dict):
                      return {k: self.resolve_references(v, context) for k, v in value.items()}
                  return value

              def _resolve_string(self, value: str, context: dict) -> Any:
                  """Resolve interpolations in string value."""
                  def replacer(match):
                      ref = match.group(1)
                      return str(self._resolve_reference(ref, context))

                  # Check if entire value is a single reference
                  if value.startswith('${') and value.endswith('}'):
                      ref = value[2:-1]
                      return self._resolve_reference(ref, context)

                  return self._var_pattern.sub(replacer, value)

              def _resolve_reference(self, ref: str, context: dict) -> Any:
                  """Resolve a single reference like var.name or aws_instance.web.id"""
                  parts = ref.split('.')

                  if parts[0] == 'var':
                      return context.get('variables', {}).get(parts[1])
                  elif parts[0] == 'local':
                      return context.get('locals', {}).get(parts[1])
                  elif parts[0] == 'module':
                      return context.get('modules', {}).get(parts[1], {}).get(parts[2])
                  else:
                      # Resource reference
                      resource_id = f"{parts[0]}.{parts[1]}"
                      resource = context.get('resources', {}).get(resource_id, {})
                      if len(parts) > 2:
                          return resource.get(parts[2])
                      return resource

                  return None
          ```
      pitfalls:
      - Circular references cause infinite resolution loop
      - Interpolation in count creates chicken-egg problem
      - Module source paths need normalization
      acceptance_criteria:
      - Parse HCL or YAML configuration files into structured resource definitions
      - Support variable interpolation resolving ${var.name} references in values
      - Handle includes and modules importing external configuration definitions
      - Validate configuration syntax and report meaningful errors for invalid input
      deliverables:
      - HCL or YAML parsing converting configuration files into structured data
      - Resource block extraction identifying resource type, name, and attributes
      - Variable interpolation resolving ${var.name} references in attribute values
      - Module support for importing and composing reusable configuration blocks
    - name: State Management
      description: Implement state file tracking of deployed resources with locking for concurrent access.
      hints:
        level1: Store resource IDs and attributes after creation.
        level2: Use file or remote locking to prevent concurrent modifications.
        level3: |-
          ```python
          import json
          import hashlib
          import fcntl
          from dataclasses import dataclass, field, asdict
          from datetime import datetime
          from pathlib import Path
          from typing import Optional
          import asyncio

          @dataclass
          class ResourceState:
              type: str
              name: str
              id: str
              attributes: dict
              dependencies: list[str] = field(default_factory=list)
              provider: str = ""

          @dataclass
          class State:
              version: int = 1
              serial: int = 0
              lineage: str = ""
              resources: dict[str, ResourceState] = field(default_factory=dict)
              outputs: dict[str, Any] = field(default_factory=dict)

          class StateManager:
              def __init__(self, state_path: str = "terraform.tfstate"):
                  self.state_path = Path(state_path)
                  self.lock_path = Path(f"{state_path}.lock")
                  self.state: State = None
                  self._lock_fd = None

              def load(self) -> State:
                  """Load state from file."""
                  if self.state_path.exists():
                      with open(self.state_path) as f:
                          data = json.load(f)
                          self.state = State(
                              version=data.get('version', 1),
                              serial=data.get('serial', 0),
                              lineage=data.get('lineage', ''),
                              resources={
                                  k: ResourceState(**v)
                                  for k, v in data.get('resources', {}).items()
                              },
                              outputs=data.get('outputs', {})
                          )
                  else:
                      import uuid
                      self.state = State(lineage=str(uuid.uuid4()))

                  return self.state

              def save(self):
                  """Save state to file."""
                  self.state.serial += 1

                  data = {
                      'version': self.state.version,
                      'serial': self.state.serial,
                      'lineage': self.state.lineage,
                      'resources': {
                          k: asdict(v) for k, v in self.state.resources.items()
                      },
                      'outputs': self.state.outputs
                  }

                  # Write to temp file first
                  temp_path = self.state_path.with_suffix('.tmp')
                  with open(temp_path, 'w') as f:
                      json.dump(data, f, indent=2)

                  # Atomic rename
                  temp_path.rename(self.state_path)

              def lock(self, timeout: float = 60) -> bool:
                  """Acquire state lock."""
                  self._lock_fd = open(self.lock_path, 'w')

                  start = datetime.now()
                  while True:
                      try:
                          fcntl.flock(self._lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                          # Write lock info
                          lock_info = {
                              'locked_at': datetime.utcnow().isoformat(),
                              'pid': os.getpid(),
                              'hostname': socket.gethostname()
                          }
                          self._lock_fd.write(json.dumps(lock_info))
                          self._lock_fd.flush()
                          return True
                      except BlockingIOError:
                          if (datetime.now() - start).total_seconds() > timeout:
                              return False
                          import time
                          time.sleep(1)

              def unlock(self):
                  """Release state lock."""
                  if self._lock_fd:
                      fcntl.flock(self._lock_fd, fcntl.LOCK_UN)
                      self._lock_fd.close()
                      self._lock_fd = None
                      self.lock_path.unlink(missing_ok=True)

              def get_resource(self, resource_id: str) -> Optional[ResourceState]:
                  return self.state.resources.get(resource_id)

              def set_resource(self, resource_id: str, resource: ResourceState):
                  self.state.resources[resource_id] = resource

              def remove_resource(self, resource_id: str):
                  self.state.resources.pop(resource_id, None)

              def compute_checksum(self) -> str:
                  """Compute state checksum for change detection."""
                  data = json.dumps(asdict(self.state), sort_keys=True)
                  return hashlib.sha256(data.encode()).hexdigest()

          class RemoteStateBackend:
              """S3 backend for remote state storage."""

              def __init__(self, bucket: str, key: str, region: str):
                  self.bucket = bucket
                  self.key = key
                  self.region = region
                  self.lock_table = "terraform-locks"  # DynamoDB table

              async def load(self) -> State:
                  import aioboto3
                  session = aioboto3.Session()

                  async with session.client('s3', region_name=self.region) as s3:
                      try:
                          response = await s3.get_object(Bucket=self.bucket, Key=self.key)
                          data = json.loads(await response['Body'].read())
                          return State(**data)
                      except s3.exceptions.NoSuchKey:
                          return State(lineage=str(uuid.uuid4()))

              async def save(self, state: State):
                  import aioboto3
                  session = aioboto3.Session()

                  async with session.client('s3', region_name=self.region) as s3:
                      await s3.put_object(
                          Bucket=self.bucket,
                          Key=self.key,
                          Body=json.dumps(asdict(state), indent=2),
                          ContentType='application/json'
                      )

              async def lock(self, lock_id: str) -> bool:
                  import aioboto3
                  session = aioboto3.Session()

                  async with session.client('dynamodb', region_name=self.region) as ddb:
                      try:
                          await ddb.put_item(
                              TableName=self.lock_table,
                              Item={
                                  'LockID': {'S': f"{self.bucket}/{self.key}"},
                                  'Info': {'S': json.dumps({'id': lock_id})}
                              },
                              ConditionExpression='attribute_not_exists(LockID)'
                          )
                          return True
                      except ddb.exceptions.ConditionalCheckFailedException:
                          return False
          ```
      pitfalls:
      - State corruption on partial write (use atomic rename)
      - Stale lock from crashed process blocks everyone
      - Remote state race condition between read and write
      acceptance_criteria:
      - Store current infrastructure state including resource IDs and attribute values
      - Lock state during apply operations to prevent concurrent modification conflicts
      - Support state backends including local filesystem and remote cloud storage
      - Handle state corruption recovery with backup and restore mechanisms
      deliverables:
      - State file format storing resource IDs, attributes, and dependency metadata
      - State locking preventing concurrent modifications during apply operations
      - State diff computation comparing current state against desired configuration
      - Remote state backend supporting S3 or similar cloud storage for team access
    - name: Dependency Graph & Planning
      description: Build resource dependency graph and generate execution plan with create, update, delete operations.
      hints:
        level1: Build DAG from explicit depends_on and implicit references.
        level2: Compare desired state vs current state to determine actions.
        level3: |-
          ```python
          from enum import Enum
          from dataclasses import dataclass, field
          from typing import Optional
          import graphlib

          class Action(str, Enum):
              CREATE = "create"
              UPDATE = "update"
              DELETE = "delete"
              REPLACE = "replace"  # delete + create
              NO_OP = "no-op"

          @dataclass
          class ResourceChange:
              resource_id: str
              action: Action
              before: dict = None
              after: dict = None
              reason: str = ""

          @dataclass
          class Plan:
              changes: list[ResourceChange] = field(default_factory=list)
              outputs: dict = field(default_factory=dict)

          class DependencyGraph:
              def __init__(self):
                  self.graph: dict[str, set[str]] = {}

              def add_resource(self, resource_id: str, depends_on: list[str] = None):
                  self.graph[resource_id] = set(depends_on or [])

              def add_dependency(self, resource_id: str, depends_on: str):
                  self.graph.setdefault(resource_id, set()).add(depends_on)

              def get_order(self) -> list[str]:
                  """Return resources in dependency order."""
                  ts = graphlib.TopologicalSorter(self.graph)
                  return list(ts.static_order())

              def get_reverse_order(self) -> list[str]:
                  """Return resources in reverse order (for deletion)."""
                  return list(reversed(self.get_order()))

              def get_dependents(self, resource_id: str) -> set[str]:
                  """Get all resources that depend on this one."""
                  dependents = set()
                  for res_id, deps in self.graph.items():
                      if resource_id in deps:
                          dependents.add(res_id)
                  return dependents

          class Planner:
              def __init__(self, config: Configuration, state: State):
                  self.config = config
                  self.state = state
                  self.graph = DependencyGraph()

              def plan(self) -> Plan:
                  """Generate execution plan."""
                  plan = Plan()

                  # Build dependency graph
                  self._build_graph()

                  # Get desired resource IDs
                  desired_ids = set(self.config.resources.keys())
                  current_ids = set(self.state.resources.keys())

                  # Resources to create
                  to_create = desired_ids - current_ids
                  # Resources to delete
                  to_delete = current_ids - desired_ids
                  # Resources to potentially update
                  to_check = desired_ids & current_ids

                  # Process deletions (reverse dependency order)
                  for resource_id in self.graph.get_reverse_order():
                      if resource_id in to_delete:
                          plan.changes.append(ResourceChange(
                              resource_id=resource_id,
                              action=Action.DELETE,
                              before=self.state.resources[resource_id].attributes
                          ))

                  # Process creates and updates (dependency order)
                  for resource_id in self.graph.get_order():
                      if resource_id in to_create:
                          plan.changes.append(ResourceChange(
                              resource_id=resource_id,
                              action=Action.CREATE,
                              after=self.config.resources[resource_id].attributes
                          ))
                      elif resource_id in to_check:
                          change = self._diff_resource(resource_id)
                          if change:
                              plan.changes.append(change)

                  return plan

              def _build_graph(self):
                  """Build dependency graph from configuration."""
                  for resource_id, resource in self.config.resources.items():
                      # Explicit dependencies
                      self.graph.add_resource(resource_id, resource.depends_on)

                      # Implicit dependencies from references
                      refs = self._find_references(resource.attributes)
                      for ref in refs:
                          if ref in self.config.resources:
                              self.graph.add_dependency(resource_id, ref)

              def _find_references(self, value: Any, refs: set = None) -> set[str]:
                  """Find resource references in attributes."""
                  refs = refs or set()

                  if isinstance(value, str):
                      import re
                      pattern = r'\$\{([a-z_]+\.[a-z_][a-z0-9_]*)'
                      for match in re.finditer(pattern, value):
                          refs.add(match.group(1))
                  elif isinstance(value, list):
                      for v in value:
                          self._find_references(v, refs)
                  elif isinstance(value, dict):
                      for v in value.values():
                          self._find_references(v, refs)

                  return refs

              def _diff_resource(self, resource_id: str) -> Optional[ResourceChange]:
                  """Compare desired vs current state for a resource."""
                  desired = self.config.resources[resource_id]
                  current = self.state.resources[resource_id]

                  # Check if attributes changed
                  if self._attributes_changed(desired.attributes, current.attributes):
                      # Check if change requires replacement
                      if self._requires_replacement(desired.type, desired.attributes, current.attributes):
                          return ResourceChange(
                              resource_id=resource_id,
                              action=Action.REPLACE,
                              before=current.attributes,
                              after=desired.attributes,
                              reason="Force replacement attribute changed"
                          )
                      else:
                          return ResourceChange(
                              resource_id=resource_id,
                              action=Action.UPDATE,
                              before=current.attributes,
                              after=desired.attributes
                          )

                  return None

              def _attributes_changed(self, desired: dict, current: dict) -> bool:
                  """Check if attributes have changed."""
                  # Simplified comparison - real implementation needs deep diff
                  return desired != current

              def _requires_replacement(self, res_type: str, desired: dict, current: dict) -> bool:
                  """Check if change requires resource replacement."""
                  # Provider defines which attributes force replacement
                  force_new_attrs = {
                      'aws_instance': ['ami', 'instance_type'],
                      'aws_vpc': ['cidr_block'],
                  }

                  attrs = force_new_attrs.get(res_type, [])
                  for attr in attrs:
                      if desired.get(attr) != current.get(attr):
                          return True
                  return False
          ```
      pitfalls:
      - Cycle in dependencies causes topological sort failure
      - Implicit dependency detection misses some patterns
      - Replace action must delete before create if unique constraint
      acceptance_criteria:
      - Build resource dependency graph from explicit depends_on and implicit attribute references
      - Generate execution plan with create, update, and delete actions for each resource
      - Show plan diff before apply so operator can review proposed infrastructure changes
      - Support targeted apply executing changes only for specified resource subset
      deliverables:
      - Resource dependency extraction from explicit and implicit references
      - DAG construction building directed acyclic graph from resource dependencies
      - Plan generation computing create, update, and delete actions from state diff
      - Plan preview displaying proposed changes before applying them to infrastructure
    - name: Provider Abstraction
      description: Build provider interface for abstracting cloud APIs with resource CRUD operations.
      hints:
        level1: Define standard interface for create, read, update, delete.
        level2: Handle resource schema validation and type coercion.
        level3: |-
          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass, field
          from typing import Any, Optional
          from enum import Enum

          class AttributeType(Enum):
              STRING = "string"
              NUMBER = "number"
              BOOL = "bool"
              LIST = "list"
              MAP = "map"

          @dataclass
          class AttributeSchema:
              name: str
              type: AttributeType
              required: bool = False
              computed: bool = False  # Set by provider, not user
              force_new: bool = False  # Change requires replacement
              default: Any = None
              description: str = ""

          @dataclass
          class ResourceSchema:
              type_name: str
              attributes: dict[str, AttributeSchema] = field(default_factory=dict)
              create_timeout: int = 300
              update_timeout: int = 300
              delete_timeout: int = 300

          @dataclass
          class ResourceData:
              id: str = ""
              attributes: dict = field(default_factory=dict)

          class Provider(ABC):
              @abstractmethod
              def get_schema(self) -> dict[str, ResourceSchema]:
                  """Return schemas for all resource types."""
                  pass

              @abstractmethod
              async def configure(self, config: dict):
                  """Configure provider with credentials and settings."""
                  pass

              @abstractmethod
              async def create(self, resource_type: str, config: dict) -> ResourceData:
                  """Create a new resource."""
                  pass

              @abstractmethod
              async def read(self, resource_type: str, id: str) -> Optional[ResourceData]:
                  """Read current state of resource."""
                  pass

              @abstractmethod
              async def update(self, resource_type: str, id: str,
                              old_config: dict, new_config: dict) -> ResourceData:
                  """Update existing resource."""
                  pass

              @abstractmethod
              async def delete(self, resource_type: str, id: str):
                  """Delete resource."""
                  pass

          class AWSProvider(Provider):
              def __init__(self):
                  self.session = None
                  self._schemas = self._define_schemas()

              def _define_schemas(self) -> dict[str, ResourceSchema]:
                  return {
                      'aws_instance': ResourceSchema(
                          type_name='aws_instance',
                          attributes={
                              'ami': AttributeSchema('ami', AttributeType.STRING, required=True, force_new=True),
                              'instance_type': AttributeSchema('instance_type', AttributeType.STRING, required=True),
                              'tags': AttributeSchema('tags', AttributeType.MAP),
                              'id': AttributeSchema('id', AttributeType.STRING, computed=True),
                              'public_ip': AttributeSchema('public_ip', AttributeType.STRING, computed=True),
                              'private_ip': AttributeSchema('private_ip', AttributeType.STRING, computed=True),
                          }
                      ),
                      'aws_vpc': ResourceSchema(
                          type_name='aws_vpc',
                          attributes={
                              'cidr_block': AttributeSchema('cidr_block', AttributeType.STRING, required=True, force_new=True),
                              'tags': AttributeSchema('tags', AttributeType.MAP),
                              'id': AttributeSchema('id', AttributeType.STRING, computed=True),
                          }
                      ),
                  }

              def get_schema(self) -> dict[str, ResourceSchema]:
                  return self._schemas

              async def configure(self, config: dict):
                  import aioboto3
                  self.session = aioboto3.Session(
                      aws_access_key_id=config.get('access_key'),
                      aws_secret_access_key=config.get('secret_key'),
                      region_name=config.get('region', 'us-east-1')
                  )

              async def create(self, resource_type: str, config: dict) -> ResourceData:
                  if resource_type == 'aws_instance':
                      return await self._create_instance(config)
                  elif resource_type == 'aws_vpc':
                      return await self._create_vpc(config)
                  raise ValueError(f"Unknown resource type: {resource_type}")

              async def _create_instance(self, config: dict) -> ResourceData:
                  async with self.session.client('ec2') as ec2:
                      response = await ec2.run_instances(
                          ImageId=config['ami'],
                          InstanceType=config['instance_type'],
                          MinCount=1,
                          MaxCount=1,
                          TagSpecifications=[{
                              'ResourceType': 'instance',
                              'Tags': [{'Key': k, 'Value': v} for k, v in config.get('tags', {}).items()]
                          }] if config.get('tags') else []
                      )

                      instance = response['Instances'][0]

                      # Wait for running state
                      waiter = ec2.get_waiter('instance_running')
                      await waiter.wait(InstanceIds=[instance['InstanceId']])

                      # Get updated info
                      desc = await ec2.describe_instances(InstanceIds=[instance['InstanceId']])
                      instance = desc['Reservations'][0]['Instances'][0]

                      return ResourceData(
                          id=instance['InstanceId'],
                          attributes={
                              'ami': instance['ImageId'],
                              'instance_type': instance['InstanceType'],
                              'public_ip': instance.get('PublicIpAddress', ''),
                              'private_ip': instance.get('PrivateIpAddress', ''),
                              'tags': {t['Key']: t['Value'] for t in instance.get('Tags', [])}
                          }
                      )

              async def read(self, resource_type: str, id: str) -> Optional[ResourceData]:
                  if resource_type == 'aws_instance':
                      async with self.session.client('ec2') as ec2:
                          try:
                              response = await ec2.describe_instances(InstanceIds=[id])
                              if not response['Reservations']:
                                  return None
                              instance = response['Reservations'][0]['Instances'][0]
                              return ResourceData(
                                  id=id,
                                  attributes={
                                      'ami': instance['ImageId'],
                                      'instance_type': instance['InstanceType'],
                                      'public_ip': instance.get('PublicIpAddress', ''),
                                      'private_ip': instance.get('PrivateIpAddress', ''),
                                  }
                              )
                          except Exception:
                              return None
                  return None

              async def delete(self, resource_type: str, id: str):
                  if resource_type == 'aws_instance':
                      async with self.session.client('ec2') as ec2:
                          await ec2.terminate_instances(InstanceIds=[id])
                          waiter = ec2.get_waiter('instance_terminated')
                          await waiter.wait(InstanceIds=[id])
          ```
      pitfalls:
      - API rate limits require retry with backoff
      - Eventual consistency means read after create may fail
      - Resource stuck in pending state needs timeout handling
      acceptance_criteria:
      - Define provider interface with standard create, read, update, and delete methods
      - Implement CRUD operations for at least one cloud resource type end-to-end
      - Handle provider authentication using configured credentials and access tokens
      - Support provider state refresh reading current resource attributes from cloud API
      deliverables:
      - Provider interface defining create, read, update, and delete operations
      - CRUD operations implementing resource lifecycle for each provider type
      - Provider configuration handling credentials and region settings
      - Provider plugins enabling extensible support for additional cloud platforms
  metrics-collector:
    name: Metrics Collection System
    description: Build a Prometheus-like metrics collection system with scraping, storage, and PromQL-style querying.
    why_important: Understanding metrics systems helps you design better instrumentation, optimize queries, and debug performance
      issues in production.
    difficulty: advanced
    tags:
    - observability
    - distributed-systems
    - databases
    estimated_hours: 50
    prerequisites:
    - time-series-db
    learning_outcomes:
    - Design pull-based metrics collection
    - Implement time-series storage efficiently
    - Build query language for aggregations
    - Handle high-cardinality metrics
    milestones:
    - name: Metrics Data Model
      description: Implement the metrics data model with counters, gauges, histograms, and summaries with labels.
      hints:
        level1: Each metric has a name, type, labels (key-value pairs), and value.
        level2: Histograms store bucketed counts, summaries track quantiles over time.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from enum import Enum
          from typing import Optional
          import time
          import threading
          import math

          class MetricType(str, Enum):
              COUNTER = "counter"
              GAUGE = "gauge"
              HISTOGRAM = "histogram"
              SUMMARY = "summary"

          @dataclass(frozen=True)
          class Labels:
              """Immutable label set for metric identification."""
              _labels: tuple[tuple[str, str], ...]

              def __init__(self, **kwargs):
                  object.__setattr__(self, '_labels', tuple(sorted(kwargs.items())))

              def __hash__(self):
                  return hash(self._labels)

              def __eq__(self, other):
                  return self._labels == other._labels

              def to_dict(self) -> dict:
                  return dict(self._labels)

          @dataclass
          class Sample:
              timestamp: float
              value: float

          class Counter:
              def __init__(self, name: str, help_text: str = ""):
                  self.name = name
                  self.help = help_text
                  self.type = MetricType.COUNTER
                  self._values: dict[Labels, float] = {}
                  self._lock = threading.Lock()

              def inc(self, labels: Labels = None, value: float = 1):
                  labels = labels or Labels()
                  with self._lock:
                      self._values[labels] = self._values.get(labels, 0) + value

              def get(self, labels: Labels = None) -> float:
                  labels = labels or Labels()
                  return self._values.get(labels, 0)

              def collect(self) -> list[tuple[Labels, float]]:
                  with self._lock:
                      return [(l, v) for l, v in self._values.items()]

          class Gauge:
              def __init__(self, name: str, help_text: str = ""):
                  self.name = name
                  self.help = help_text
                  self.type = MetricType.GAUGE
                  self._values: dict[Labels, float] = {}
                  self._lock = threading.Lock()

              def set(self, value: float, labels: Labels = None):
                  labels = labels or Labels()
                  with self._lock:
                      self._values[labels] = value

              def inc(self, labels: Labels = None, value: float = 1):
                  labels = labels or Labels()
                  with self._lock:
                      self._values[labels] = self._values.get(labels, 0) + value

              def dec(self, labels: Labels = None, value: float = 1):
                  self.inc(labels, -value)

              def collect(self) -> list[tuple[Labels, float]]:
                  with self._lock:
                      return [(l, v) for l, v in self._values.items()]

          class Histogram:
              DEFAULT_BUCKETS = (0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, float('inf'))

              def __init__(self, name: str, help_text: str = "", buckets: tuple = None):
                  self.name = name
                  self.help = help_text
                  self.type = MetricType.HISTOGRAM
                  self.buckets = buckets or self.DEFAULT_BUCKETS
                  self._counts: dict[Labels, list[int]] = {}  # bucket counts
                  self._sums: dict[Labels, float] = {}
                  self._totals: dict[Labels, int] = {}
                  self._lock = threading.Lock()

              def observe(self, value: float, labels: Labels = None):
                  labels = labels or Labels()
                  with self._lock:
                      if labels not in self._counts:
                          self._counts[labels] = [0] * len(self.buckets)
                          self._sums[labels] = 0
                          self._totals[labels] = 0

                      # Increment appropriate bucket(s)
                      for i, bound in enumerate(self.buckets):
                          if value <= bound:
                              self._counts[labels][i] += 1

                      self._sums[labels] += value
                      self._totals[labels] += 1

              def collect(self) -> list[tuple[Labels, dict]]:
                  with self._lock:
                      results = []
                      for labels in self._counts:
                          results.append((labels, {
                              'buckets': list(zip(self.buckets, self._counts[labels])),
                              'sum': self._sums[labels],
                              'count': self._totals[labels]
                          }))
                      return results

          class MetricRegistry:
              def __init__(self):
                  self._metrics: dict[str, Counter | Gauge | Histogram] = {}
                  self._lock = threading.Lock()

              def counter(self, name: str, help_text: str = "") -> Counter:
                  with self._lock:
                      if name not in self._metrics:
                          self._metrics[name] = Counter(name, help_text)
                      return self._metrics[name]

              def gauge(self, name: str, help_text: str = "") -> Gauge:
                  with self._lock:
                      if name not in self._metrics:
                          self._metrics[name] = Gauge(name, help_text)
                      return self._metrics[name]

              def histogram(self, name: str, help_text: str = "", buckets: tuple = None) -> Histogram:
                  with self._lock:
                      if name not in self._metrics:
                          self._metrics[name] = Histogram(name, help_text, buckets)
                      return self._metrics[name]

              def collect_all(self) -> dict:
                  with self._lock:
                      return {name: metric.collect() for name, metric in self._metrics.items()}
          ```
      pitfalls:
      - High cardinality labels cause memory explosion
      - Counter resets on restart need special handling
      - Histogram bucket boundaries can't change after creation
      acceptance_criteria:
      - Counter, gauge, and histogram metric types are defined with their correct semantic behaviors
      - Labels attach key-value pairs to metrics enabling multi-dimensional filtering and aggregation
      - Metric metadata including help text and unit is stored and exposed alongside metric values
      - Metric names are validated against a naming convention rejecting invalid characters or reserved prefixes
      deliverables:
      - Metric type definitions for counter, gauge, and histogram with their increment and observe methods
      - Label support system that attaches key-value dimension pairs to each metric for filtering
      - Timestamp handler that records the observation time for each metric data point in UTC
      - Metric metadata store that tracks description, unit, and type information for each registered metric
    - name: Scrape Engine
      description: Build a scrape engine that pulls metrics from configured targets with service discovery support.
      hints:
        level1: Poll targets at configured intervals, parse response format.
        level2: Handle target failures, track scrape duration and status.
        level3: |-
          ```python
          import asyncio
          import aiohttp
          import time
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum

          class ScrapeHealth(str, Enum):
              UP = "up"
              DOWN = "down"
              UNKNOWN = "unknown"

          @dataclass
          class ScrapeTarget:
              job_name: str
              address: str
              metrics_path: str = "/metrics"
              scrape_interval: float = 15.0
              scrape_timeout: float = 10.0
              labels: dict = field(default_factory=dict)

          @dataclass
          class ScrapeResult:
              target: ScrapeTarget
              timestamp: float
              samples: list[tuple[str, Labels, float]]
              health: ScrapeHealth
              scrape_duration: float
              error: Optional[str] = None

          class MetricParser:
              def parse(self, text: str) -> list[tuple[str, Labels, float]]:
                  """Parse Prometheus exposition format."""
                  samples = []
                  for line in text.strip().split('\n'):
                      if not line or line.startswith('#'):
                          continue

                      # Parse metric line: name{label="value"} value timestamp?
                      if '{' in line:
                          name_part, rest = line.split('{', 1)
                          labels_part, value_part = rest.rsplit('}', 1)
                          labels = self._parse_labels(labels_part)
                      else:
                          parts = line.split()
                          name_part = parts[0]
                          value_part = ' '.join(parts[1:])
                          labels = Labels()

                      value = float(value_part.split()[0])
                      samples.append((name_part.strip(), labels, value))

                  return samples

              def _parse_labels(self, labels_str: str) -> Labels:
                  import re
                  label_dict = {}
                  pattern = r'(\w+)="([^"]*)"'
                  for match in re.finditer(pattern, labels_str):
                      label_dict[match.group(1)] = match.group(2)
                  return Labels(**label_dict)

          class ScrapeEngine:
              def __init__(self, storage):
                  self.storage = storage
                  self.targets: list[ScrapeTarget] = []
                  self.parser = MetricParser()
                  self._tasks: dict[str, asyncio.Task] = {}
                  self._running = False

                  # Internal metrics
                  self.scrape_duration = Histogram("scrape_duration_seconds", "Scrape duration")
                  self.scrape_samples = Gauge("scrape_samples_scraped", "Samples scraped")
                  self.up = Gauge("up", "Target health")

              def add_target(self, target: ScrapeTarget):
                  self.targets.append(target)
                  if self._running:
                      self._start_scrape_loop(target)

              async def start(self):
                  self._running = True
                  for target in self.targets:
                      self._start_scrape_loop(target)

              def _start_scrape_loop(self, target: ScrapeTarget):
                  key = f"{target.job_name}:{target.address}"
                  self._tasks[key] = asyncio.create_task(self._scrape_loop(target))

              async def stop(self):
                  self._running = False
                  for task in self._tasks.values():
                      task.cancel()
                  await asyncio.gather(*self._tasks.values(), return_exceptions=True)

              async def _scrape_loop(self, target: ScrapeTarget):
                  while self._running:
                      result = await self._scrape(target)
                      await self._process_result(result)
                      await asyncio.sleep(target.scrape_interval)

              async def _scrape(self, target: ScrapeTarget) -> ScrapeResult:
                  url = f"http://{target.address}{target.metrics_path}"
                  start_time = time.time()

                  try:
                      async with aiohttp.ClientSession() as session:
                          async with session.get(url, timeout=aiohttp.ClientTimeout(total=target.scrape_timeout)) as resp:
                              if resp.status != 200:
                                  raise Exception(f"HTTP {resp.status}")

                              text = await resp.text()
                              samples = self.parser.parse(text)
                              duration = time.time() - start_time

                              return ScrapeResult(
                                  target=target,
                                  timestamp=start_time,
                                  samples=samples,
                                  health=ScrapeHealth.UP,
                                  scrape_duration=duration
                              )

                  except Exception as e:
                      return ScrapeResult(
                          target=target,
                          timestamp=start_time,
                          samples=[],
                          health=ScrapeHealth.DOWN,
                          scrape_duration=time.time() - start_time,
                          error=str(e)
                      )

              async def _process_result(self, result: ScrapeResult):
                  target = result.target
                  target_labels = Labels(job=target.job_name, instance=target.address, **target.labels)

                  # Record internal metrics
                  self.scrape_duration.observe(result.scrape_duration, target_labels)
                  self.scrape_samples.set(len(result.samples), target_labels)
                  self.up.set(1 if result.health == ScrapeHealth.UP else 0, target_labels)

                  # Store samples
                  for name, labels, value in result.samples:
                      # Merge target labels with metric labels
                      all_labels = Labels(**{**target_labels.to_dict(), **labels.to_dict()})
                      await self.storage.store(name, all_labels, result.timestamp, value)
          ```
      pitfalls:
      - Too aggressive scraping overwhelms targets
      - Network timeouts block scrape loop
      - Label collision between target and metric labels
      acceptance_criteria:
      - Scrape targets are discovered from static config files and dynamic service discovery backends
      - Metrics are pulled from HTTP endpoints by parsing the Prometheus exposition format text
      - Scrape timeouts cancel the HTTP request and mark the target as down for that scrape cycle
      - Service discovery integrations update the target list automatically when services are added or removed
      deliverables:
      - Target discovery module that finds scrape endpoints via static configuration or service discovery
      - HTTP scraper that fetches and parses Prometheus exposition format text from target endpoints
      - Scrape interval scheduler that triggers metric collection at configurable per-target intervals
      - Scrape timeout handler that aborts and reports targets that fail to respond within the deadline
    - name: Time Series Storage
      description: Implement efficient time-series storage with compression, retention, and downsampling.
      hints:
        level1: Store samples in time-ordered chunks per series.
        level2: Use delta encoding and variable-length integers for compression.
        level3: |-
          ```python
          import struct
          from dataclasses import dataclass, field
          from typing import Iterator, Optional
          import mmap
          import os
          import threading
          from bisect import bisect_left, bisect_right

          @dataclass
          class Chunk:
              """Compressed chunk of time-series samples."""
              start_time: float
              end_time: float = 0
              samples: int = 0
              data: bytearray = field(default_factory=bytearray)
              _prev_ts: float = 0
              _prev_val: float = 0

              MAX_SAMPLES = 120  # ~2 hours at 1 min interval

              def append(self, timestamp: float, value: float):
                  if self.samples == 0:
                      # First sample - store full values
                      self.data.extend(struct.pack('<d', timestamp))
                      self.data.extend(struct.pack('<d', value))
                      self.start_time = timestamp
                      self._prev_ts = timestamp
                      self._prev_val = value
                  else:
                      # Delta encode timestamp
                      ts_delta = int((timestamp - self._prev_ts) * 1000)  # ms precision
                      self._encode_varint(ts_delta)

                      # XOR encode value
                      self._encode_xor_float(value)

                      self._prev_ts = timestamp
                      self._prev_val = value

                  self.end_time = timestamp
                  self.samples += 1

              def is_full(self) -> bool:
                  return self.samples >= self.MAX_SAMPLES

              def _encode_varint(self, value: int):
                  while value > 127:
                      self.data.append((value & 0x7F) | 0x80)
                      value >>= 7
                  self.data.append(value)

              def _encode_xor_float(self, value: float):
                  # Simple XOR encoding (real implementation uses Gorilla compression)
                  prev_bits = struct.unpack('<Q', struct.pack('<d', self._prev_val))[0]
                  curr_bits = struct.unpack('<Q', struct.pack('<d', value))[0]
                  xor = prev_bits ^ curr_bits

                  if xor == 0:
                      self.data.append(0)  # Same value
                  else:
                      self.data.append(1)
                      self.data.extend(struct.pack('<Q', xor))

              def iterate(self) -> Iterator[tuple[float, float]]:
                  """Iterate over samples in chunk."""
                  if not self.data:
                      return

                  offset = 0
                  # First sample
                  ts = struct.unpack_from('<d', self.data, offset)[0]
                  offset += 8
                  val = struct.unpack_from('<d', self.data, offset)[0]
                  offset += 8
                  yield ts, val

                  prev_ts = ts
                  prev_val = val

                  for _ in range(self.samples - 1):
                      # Decode timestamp delta
                      ts_delta, bytes_read = self._decode_varint(offset)
                      offset += bytes_read
                      ts = prev_ts + ts_delta / 1000.0

                      # Decode value
                      marker = self.data[offset]
                      offset += 1
                      if marker == 0:
                          val = prev_val
                      else:
                          xor = struct.unpack_from('<Q', self.data, offset)[0]
                          offset += 8
                          prev_bits = struct.unpack('<Q', struct.pack('<d', prev_val))[0]
                          curr_bits = prev_bits ^ xor
                          val = struct.unpack('<d', struct.pack('<Q', curr_bits))[0]

                      yield ts, val
                      prev_ts = ts
                      prev_val = val

              def _decode_varint(self, offset: int) -> tuple[int, int]:
                  result = 0
                  shift = 0
                  bytes_read = 0
                  while True:
                      byte = self.data[offset + bytes_read]
                      bytes_read += 1
                      result |= (byte & 0x7F) << shift
                      if not (byte & 0x80):
                          break
                      shift += 7
                  return result, bytes_read

          @dataclass(frozen=True)
          class SeriesKey:
              name: str
              labels: Labels

              def __hash__(self):
                  return hash((self.name, self.labels))

          class TimeSeriesStorage:
              def __init__(self, data_dir: str, retention_days: int = 15):
                  self.data_dir = data_dir
                  self.retention_days = retention_days
                  self._series: dict[SeriesKey, list[Chunk]] = {}
                  self._lock = threading.RLock()
                  os.makedirs(data_dir, exist_ok=True)

              async def store(self, name: str, labels: Labels, timestamp: float, value: float):
                  key = SeriesKey(name, labels)

                  with self._lock:
                      if key not in self._series:
                          self._series[key] = [Chunk(start_time=timestamp)]

                      chunks = self._series[key]
                      current_chunk = chunks[-1]

                      if current_chunk.is_full():
                          # Start new chunk
                          chunks.append(Chunk(start_time=timestamp))
                          current_chunk = chunks[-1]

                      current_chunk.append(timestamp, value)

              def query(self, name: str, labels: Labels,
                        start_time: float, end_time: float) -> Iterator[tuple[float, float]]:
                  """Query samples in time range."""
                  key = SeriesKey(name, labels)

                  with self._lock:
                      chunks = self._series.get(key, [])

                      for chunk in chunks:
                          # Skip chunks outside time range
                          if chunk.end_time < start_time or chunk.start_time > end_time:
                              continue

                          for ts, val in chunk.iterate():
                              if start_time <= ts <= end_time:
                                  yield ts, val

              def query_by_name(self, name: str, start_time: float,
                                end_time: float) -> dict[Labels, list[tuple[float, float]]]:
                  """Query all series with given name."""
                  results = {}

                  with self._lock:
                      for key, chunks in self._series.items():
                          if key.name != name:
                              continue

                          samples = list(self.query(name, key.labels, start_time, end_time))
                          if samples:
                              results[key.labels] = samples

                  return results

              def compact(self):
                  """Compact old chunks and enforce retention."""
                  cutoff = time.time() - (self.retention_days * 86400)

                  with self._lock:
                      for key in list(self._series.keys()):
                          chunks = self._series[key]
                          # Remove chunks entirely before cutoff
                          self._series[key] = [c for c in chunks if c.end_time >= cutoff]

                          # Remove empty series
                          if not self._series[key]:
                              del self._series[key]
          ```
      pitfalls:
      - Chunk boundaries at exact times cause off-by-one
      - Concurrent writes corrupt chunk data
      - Compression ratio degrades with irregular timestamps
      acceptance_criteria:
      - Time series are stored efficiently with less than 2 bytes per sample using delta compression
      - Gorilla-style compression reduces storage size by encoding timestamp deltas and XOR-ed float values
      - Retention policies automatically delete data older than the configured retention period
      - High cardinality label combinations are handled without excessive memory usage or index bloat
      deliverables:
      - Time series data structure that stores timestamped samples indexed by metric name and label set
      - Compression engine using Gorilla-style delta-of-delta encoding for timestamps and XOR for values
      - Retention policy manager that deletes or downsamples data older than a configurable duration
      - Index structure that maps metric name and label combinations to their time series data blocks
    - name: Query Engine
      description: Build a PromQL-like query engine with instant queries, range queries, and aggregation functions.
      hints:
        level1: Parse expressions into AST, evaluate against storage.
        level2: Implement rate(), sum(), avg() and label matchers.
        level3: |-
          ```python
          from dataclasses import dataclass
          from typing import Union, Callable
          from enum import Enum
          import re

          class MatchType(Enum):
              EQUAL = "="
              NOT_EQUAL = "!="
              REGEX = "=~"
              NOT_REGEX = "!~"

          @dataclass
          class LabelMatcher:
              name: str
              value: str
              type: MatchType

              def matches(self, labels: Labels) -> bool:
                  actual = labels.to_dict().get(self.name, "")
                  if self.type == MatchType.EQUAL:
                      return actual == self.value
                  elif self.type == MatchType.NOT_EQUAL:
                      return actual != self.value
                  elif self.type == MatchType.REGEX:
                      return bool(re.match(self.value, actual))
                  elif self.type == MatchType.NOT_REGEX:
                      return not bool(re.match(self.value, actual))

          @dataclass
          class VectorSelector:
              name: str
              matchers: list[LabelMatcher]
              range_seconds: float = 0  # 0 for instant, >0 for range

          @dataclass
          class AggregateExpr:
              op: str  # sum, avg, max, min, count
              expr: 'Expr'
              by: list[str] = None  # Group by labels
              without: list[str] = None  # Group by all except

          @dataclass
          class FunctionExpr:
              name: str  # rate, irate, increase, etc.
              args: list['Expr']

          Expr = Union[VectorSelector, AggregateExpr, FunctionExpr]

          class QueryEngine:
              def __init__(self, storage: TimeSeriesStorage):
                  self.storage = storage
                  self.functions = {
                      'rate': self._rate,
                      'irate': self._irate,
                      'increase': self._increase,
                      'sum_over_time': self._sum_over_time,
                      'avg_over_time': self._avg_over_time,
                  }
                  self.aggregations = {
                      'sum': lambda vals: sum(vals),
                      'avg': lambda vals: sum(vals) / len(vals) if vals else 0,
                      'max': lambda vals: max(vals) if vals else 0,
                      'min': lambda vals: min(vals) if vals else 0,
                      'count': lambda vals: len(vals),
                  }

              def instant_query(self, query: str, timestamp: float) -> dict[Labels, float]:
                  """Execute instant query at specific time."""
                  expr = self._parse(query)
                  return self._eval(expr, timestamp, timestamp)

              def range_query(self, query: str, start: float, end: float,
                             step: float) -> dict[Labels, list[tuple[float, float]]]:
                  """Execute range query with step."""
                  expr = self._parse(query)
                  results = {}

                  for ts in self._time_range(start, end, step):
                      instant = self._eval(expr, ts - step, ts)
                      for labels, value in instant.items():
                          results.setdefault(labels, []).append((ts, value))

                  return results

              def _parse(self, query: str) -> Expr:
                  """Parse query string into expression tree."""
                  # Simplified parser - real implementation uses proper grammar
                  query = query.strip()

                  # Check for aggregation: sum(metric{...})
                  agg_match = re.match(r'(sum|avg|max|min|count)\s*\((.+)\)\s*(by|without)?\s*\(([^)]+)\)?$', query)
                  if agg_match:
                      op = agg_match.group(1)
                      inner = agg_match.group(2)
                      modifier = agg_match.group(3)
                      group_labels = [l.strip() for l in agg_match.group(4).split(',')] if agg_match.group(4) else []

                      return AggregateExpr(
                          op=op,
                          expr=self._parse(inner),
                          by=group_labels if modifier == 'by' else None,
                          without=group_labels if modifier == 'without' else None
                      )

                  # Check for function: rate(metric{...}[5m])
                  func_match = re.match(r'(\w+)\((.+)\)$', query)
                  if func_match and func_match.group(1) in self.functions:
                      return FunctionExpr(
                          name=func_match.group(1),
                          args=[self._parse(func_match.group(2))]
                      )

                  # Vector selector: metric{label="value"}[5m]
                  return self._parse_vector_selector(query)

              def _parse_vector_selector(self, query: str) -> VectorSelector:
                  # Parse: metric_name{label="value",label2=~"regex"}[5m]
                  range_match = re.search(r'\[(\d+)([smhd])\]$', query)
                  range_seconds = 0
                  if range_match:
                      query = query[:range_match.start()]
                      value = int(range_match.group(1))
                      unit = range_match.group(2)
                      range_seconds = value * {'s': 1, 'm': 60, 'h': 3600, 'd': 86400}[unit]

                  if '{' in query:
                      name, labels_str = query.split('{', 1)
                      labels_str = labels_str.rstrip('}')
                      matchers = self._parse_matchers(labels_str)
                  else:
                      name = query
                      matchers = []

                  return VectorSelector(name=name.strip(), matchers=matchers, range_seconds=range_seconds)

              def _parse_matchers(self, labels_str: str) -> list[LabelMatcher]:
                  matchers = []
                  pattern = r'(\w+)\s*(=~|!=|!~|=)\s*"([^"]*)"'
                  for match in re.finditer(pattern, labels_str):
                      matchers.append(LabelMatcher(
                          name=match.group(1),
                          value=match.group(3),
                          type=MatchType(match.group(2))
                      ))
                  return matchers

              def _eval(self, expr: Expr, start: float, end: float) -> dict[Labels, float]:
                  if isinstance(expr, VectorSelector):
                      return self._eval_selector(expr, start, end)
                  elif isinstance(expr, AggregateExpr):
                      return self._eval_aggregate(expr, start, end)
                  elif isinstance(expr, FunctionExpr):
                      return self._eval_function(expr, start, end)

              def _eval_selector(self, sel: VectorSelector, start: float, end: float) -> dict[Labels, float]:
                  # Get all series matching name
                  query_start = start - sel.range_seconds if sel.range_seconds else start
                  all_series = self.storage.query_by_name(sel.name, query_start, end)

                  results = {}
                  for labels, samples in all_series.items():
                      # Apply label matchers
                      if all(m.matches(labels) for m in sel.matchers):
                          if sel.range_seconds:
                              # Return samples in range (for rate, etc.)
                              results[labels] = samples
                          else:
                              # Return latest sample
                              if samples:
                                  results[labels] = samples[-1][1]

                  return results

              def _rate(self, samples: list[tuple[float, float]]) -> float:
                  """Calculate per-second rate of increase."""
                  if len(samples) < 2:
                      return 0

                  first_ts, first_val = samples[0]
                  last_ts, last_val = samples[-1]

                  duration = last_ts - first_ts
                  if duration <= 0:
                      return 0

                  # Handle counter resets
                  increase = last_val - first_val
                  if increase < 0:
                      increase = last_val  # Counter reset

                  return increase / duration
          ```
      pitfalls:
      - Rate calculation wrong across counter resets
      - Label matching with regex can be slow
      - Query on high-cardinality labels causes OOM
      acceptance_criteria:
      - PromQL-like queries with metric name, label filters, and aggregation functions are parsed and executed
      - Aggregation functions sum, avg, max, min, and count produce correct results grouped by specified labels
      - Range queries return all data points within the specified start and end time window at the requested step interval
      - Label matchers support exact match, not-equal, regex match, and negative regex selectors
      deliverables:
      - PromQL-style query parser that supports instant queries, range queries, and label matchers
      - Range query executor that returns time series data points within a specified time window
      - Aggregation engine implementing sum, avg, max, min, count, and quantile functions over label groups
      - Label matcher that filters time series by exact match, regex, and not-equal label selectors
  log-aggregator:
    name: Log Aggregation System
    description: Build a log aggregation system like Loki with log ingestion, indexing, and LogQL-style querying.
    why_important: Log systems are critical for debugging. Understanding log indexing helps optimize queries and reduce storage
      costs.
    difficulty: intermediate
    tags:
    - observability
    - databases
    - search
    estimated_hours: 40
    prerequisites:
    - shell
    learning_outcomes:
    - Design efficient log ingestion pipeline
    - Build inverted index for label-based queries
    - Implement log compression and chunking
    - Handle high-volume log streams
    milestones:
    - name: Log Ingestion
      description: Build log ingestion endpoint that accepts structured logs with labels and timestamps.
      hints:
        level1: Accept logs as JSON with labels, timestamp, and line.
        level2: Batch logs for efficient writes, validate and enrich data.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from datetime import datetime
          from typing import Optional
          import asyncio
          import json
          import time
          import hashlib

          @dataclass
          class LogEntry:
              timestamp: datetime
              labels: dict[str, str]
              line: str
              stream_id: str = ""

              def __post_init__(self):
                  if not self.stream_id:
                      # Generate stream ID from labels
                      label_str = json.dumps(self.labels, sort_keys=True)
                      self.stream_id = hashlib.md5(label_str.encode()).hexdigest()[:16]

          @dataclass
          class LogBatch:
              stream_id: str
              labels: dict[str, str]
              entries: list[LogEntry] = field(default_factory=list)

          class LogIngester:
              def __init__(self, storage, batch_size: int = 1000, flush_interval: float = 1.0):
                  self.storage = storage
                  self.batch_size = batch_size
                  self.flush_interval = flush_interval
                  self._batches: dict[str, LogBatch] = {}
                  self._lock = asyncio.Lock()
                  self._flush_task = None

              async def start(self):
                  self._flush_task = asyncio.create_task(self._flush_loop())

              async def stop(self):
                  if self._flush_task:
                      self._flush_task.cancel()
                  await self._flush_all()

              async def ingest(self, entries: list[LogEntry]):
                  """Ingest log entries, batching by stream."""
                  async with self._lock:
                      for entry in entries:
                          stream_id = entry.stream_id

                          if stream_id not in self._batches:
                              self._batches[stream_id] = LogBatch(
                                  stream_id=stream_id,
                                  labels=entry.labels
                              )

                          batch = self._batches[stream_id]
                          batch.entries.append(entry)

                          if len(batch.entries) >= self.batch_size:
                              await self._flush_batch(stream_id)

              async def _flush_loop(self):
                  while True:
                      await asyncio.sleep(self.flush_interval)
                      await self._flush_all()

              async def _flush_all(self):
                  async with self._lock:
                      for stream_id in list(self._batches.keys()):
                          await self._flush_batch(stream_id)

              async def _flush_batch(self, stream_id: str):
                  batch = self._batches.pop(stream_id, None)
                  if batch and batch.entries:
                      # Sort by timestamp
                      batch.entries.sort(key=lambda e: e.timestamp)
                      await self.storage.write_batch(batch)

          class LogPushHandler:
              """HTTP handler for Loki push protocol."""

              def __init__(self, ingester: LogIngester):
                  self.ingester = ingester

              async def handle_push(self, request_body: bytes) -> dict:
                  """Handle Loki push format."""
                  data = json.loads(request_body)
                  entries = []

                  for stream in data.get('streams', []):
                      labels = self._parse_labels(stream.get('stream', {}))

                      for value in stream.get('values', []):
                          # Loki format: [timestamp_ns, line]
                          ts_ns = int(value[0])
                          line = value[1]

                          entries.append(LogEntry(
                              timestamp=datetime.fromtimestamp(ts_ns / 1e9),
                              labels=labels,
                              line=line
                          ))

                  await self.ingester.ingest(entries)
                  return {'status': 'success', 'entries': len(entries)}

              def _parse_labels(self, labels: dict) -> dict[str, str]:
                  # Ensure all values are strings
                  return {k: str(v) for k, v in labels.items()}
          ```
      pitfalls:
      - Out-of-order timestamps complicate querying
      - Memory pressure from unbounded batching
      - Label value with special characters breaks parsing
      acceptance_criteria:
      - Logs are received via HTTP, TCP, and UDP endpoints with correct protocol handling for each
      - Multiple log formats including JSON, syslog, and custom regex patterns are parsed into structured fields
      - Ingestion handles burst rates of at least 10,000 messages per second without dropping entries
      - Incoming logs are buffered to disk or memory during downstream outage and replayed on recovery
      deliverables:
      - HTTP log receiver endpoint that accepts JSON-formatted log entries via POST requests
      - Syslog receiver that parses RFC 5424 and RFC 3164 formatted syslog messages over TCP and UDP
      - File tail agent that watches log files for new lines and forwards them to the ingestion pipeline
      - Log parser that extracts structured fields from unstructured log messages using configurable patterns
    - name: Log Index
      description: Build inverted index for fast label-based log queries with bloom filters for negative lookups.
      hints:
        level1: Index maps label values to stream IDs containing them.
        level2: Use bloom filter per chunk to skip chunks without matches.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          import mmh3  # MurmurHash3 for bloom filter

          class BloomFilter:
              def __init__(self, size: int = 10000, num_hashes: int = 7):
                  self.size = size
                  self.num_hashes = num_hashes
                  self.bits = bytearray((size + 7) // 8)

              def add(self, item: str):
                  for i in range(self.num_hashes):
                      idx = mmh3.hash(item, i) % self.size
                      self.bits[idx // 8] |= (1 << (idx % 8))

              def might_contain(self, item: str) -> bool:
                  for i in range(self.num_hashes):
                      idx = mmh3.hash(item, i) % self.size
                      if not (self.bits[idx // 8] & (1 << (idx % 8))):
                          return False
                  return True

          @dataclass
          class ChunkMeta:
              chunk_id: str
              stream_id: str
              start_time: float
              end_time: float
              entries: int
              bloom: BloomFilter

          @dataclass
          class StreamMeta:
              stream_id: str
              labels: dict[str, str]
              chunks: list[str] = field(default_factory=list)  # chunk IDs

          class LogIndex:
              def __init__(self):
                  # Label index: label_name -> label_value -> set of stream_ids
                  self._label_index: dict[str, dict[str, set[str]]] = {}
                  # Stream metadata
                  self._streams: dict[str, StreamMeta] = {}
                  # Chunk metadata
                  self._chunks: dict[str, ChunkMeta] = {}

              def index_stream(self, stream_id: str, labels: dict[str, str]):
                  self._streams[stream_id] = StreamMeta(stream_id=stream_id, labels=labels)

                  for name, value in labels.items():
                      if name not in self._label_index:
                          self._label_index[name] = {}
                      if value not in self._label_index[name]:
                          self._label_index[name][value] = set()
                      self._label_index[name][value].add(stream_id)

              def index_chunk(self, chunk: ChunkMeta):
                  self._chunks[chunk.chunk_id] = chunk
                  if stream := self._streams.get(chunk.stream_id):
                      stream.chunks.append(chunk.chunk_id)

              def find_streams(self, matchers: list[LabelMatcher]) -> set[str]:
                  """Find stream IDs matching all label matchers."""
                  if not matchers:
                      return set(self._streams.keys())

                  result = None
                  for matcher in matchers:
                      matching = self._match_label(matcher)
                      if result is None:
                          result = matching
                      else:
                          result &= matching

                  return result or set()

              def _match_label(self, matcher: LabelMatcher) -> set[str]:
                  label_values = self._label_index.get(matcher.name, {})

                  if matcher.type == MatchType.EQUAL:
                      return label_values.get(matcher.value, set()).copy()
                  elif matcher.type == MatchType.NOT_EQUAL:
                      result = set()
                      for value, streams in label_values.items():
                          if value != matcher.value:
                              result |= streams
                      return result
                  elif matcher.type == MatchType.REGEX:
                      result = set()
                      pattern = re.compile(matcher.value)
                      for value, streams in label_values.items():
                          if pattern.match(value):
                              result |= streams
                      return result

                  return set()

              def find_chunks(self, stream_ids: set[str], start_time: float,
                             end_time: float, text_filter: str = None) -> list[str]:
                  """Find relevant chunk IDs for query."""
                  chunk_ids = []

                  for stream_id in stream_ids:
                      stream = self._streams.get(stream_id)
                      if not stream:
                          continue

                      for chunk_id in stream.chunks:
                          chunk = self._chunks.get(chunk_id)
                          if not chunk:
                              continue

                          # Time range filter
                          if chunk.end_time < start_time or chunk.start_time > end_time:
                              continue

                          # Bloom filter for text search
                          if text_filter and not chunk.bloom.might_contain(text_filter):
                              continue

                          chunk_ids.append(chunk_id)

                  return chunk_ids

              def get_label_values(self, label_name: str) -> list[str]:
                  """Get all values for a label (for autocomplete)."""
                  return list(self._label_index.get(label_name, {}).keys())

              def get_label_names(self) -> list[str]:
                  """Get all label names."""
                  return list(self._label_index.keys())
          ```
      pitfalls:
      - Bloom filter false positives require verification
      - High cardinality labels bloat index
      - Index rebuild on corruption is expensive
      acceptance_criteria:
      - Inverted index maps each unique term to the set of log entries containing that term
      - Structured fields including level, service, and hostname are indexed for fast filtered lookups
      - Time-based partitions allow queries to scan only the relevant time window instead of all data
      - Index compaction merges small segments into larger ones to reduce disk usage and query overhead
      deliverables:
      - Inverted index mapping each unique term to the list of log entries containing that term
      - Time-based partitioning that segments the index into hourly or daily shards for efficient queries
      - Field extraction pipeline that parses structured fields like log level, service, and request ID
      - Index storage engine that persists the inverted index to disk with memory-mapped file access
    - name: Log Query Engine
      description: Build LogQL-style query engine with label filtering, text search, and log processing functions.
      hints:
        level1: Parse query into stream selector and pipeline stages.
        level2: Stream results to handle large result sets efficiently.
        level3: |-
          ```python
          from dataclasses import dataclass
          from typing import Iterator, Callable
          import re

          @dataclass
          class LogQueryResult:
              timestamp: datetime
              labels: dict[str, str]
              line: str

          class LogQueryEngine:
              def __init__(self, index: LogIndex, storage):
                  self.index = index
                  self.storage = storage

              def query(self, query: str, start_time: float,
                        end_time: float, limit: int = 1000) -> Iterator[LogQueryResult]:
                  """Execute LogQL query."""
                  parsed = self._parse_query(query)
                  stream_ids = self.index.find_streams(parsed['matchers'])

                  if not stream_ids:
                      return

                  # Find relevant chunks
                  chunk_ids = self.index.find_chunks(
                      stream_ids, start_time, end_time,
                      text_filter=parsed.get('line_filter')
                  )

                  count = 0
                  for chunk_id in chunk_ids:
                      for entry in self.storage.read_chunk(chunk_id):
                          # Apply time filter
                          ts = entry.timestamp.timestamp()
                          if ts < start_time or ts > end_time:
                              continue

                          # Apply pipeline
                          result = self._apply_pipeline(entry, parsed['pipeline'])
                          if result:
                              yield result
                              count += 1
                              if count >= limit:
                                  return

              def _parse_query(self, query: str) -> dict:
                  """Parse LogQL query."""
                  # {label="value"} |= "text" | json | line_format "{{.field}}"
                  result = {
                      'matchers': [],
                      'pipeline': [],
                      'line_filter': None
                  }

                  # Extract stream selector
                  selector_match = re.match(r'\{([^}]*)\}', query)
                  if selector_match:
                      result['matchers'] = self._parse_matchers(selector_match.group(1))
                      query = query[selector_match.end():]

                  # Parse pipeline stages
                  stages = query.split('|')
                  for stage in stages[1:]:  # Skip empty first element
                      stage = stage.strip()
                      if stage.startswith('='):
                          # Line filter: |= "text" or |~ "regex"
                          op = stage[:2] if stage[1] in '=~' else stage[0]
                          pattern = stage[len(op):].strip().strip('"')
                          result['pipeline'].append(('filter', op, pattern))
                          if op == '=':
                              result['line_filter'] = pattern
                      elif stage == 'json':
                          result['pipeline'].append(('json',))
                      elif stage == 'logfmt':
                          result['pipeline'].append(('logfmt',))
                      elif stage.startswith('line_format'):
                          template = re.search(r'"([^"]*)"', stage).group(1)
                          result['pipeline'].append(('line_format', template))
                      elif stage.startswith('label_format'):
                          # label_format new_label=value
                          assignments = self._parse_label_assignments(stage)
                          result['pipeline'].append(('label_format', assignments))

                  return result

              def _apply_pipeline(self, entry: LogEntry,
                                 pipeline: list) -> LogQueryResult:
                  """Apply pipeline stages to log entry."""
                  line = entry.line
                  labels = entry.labels.copy()

                  for stage in pipeline:
                      if stage[0] == 'filter':
                          op, pattern = stage[1], stage[2]
                          if op == '=':
                              if pattern not in line:
                                  return None
                          elif op == '!=':
                              if pattern in line:
                                  return None
                          elif op == '=~':
                              if not re.search(pattern, line):
                                  return None
                          elif op == '!~':
                              if re.search(pattern, line):
                                  return None

                      elif stage[0] == 'json':
                          try:
                              parsed = json.loads(line)
                              labels.update({k: str(v) for k, v in parsed.items()})
                          except json.JSONDecodeError:
                              pass  # Keep original line

                      elif stage[0] == 'logfmt':
                          # Parse key=value pairs
                          for match in re.finditer(r'(\w+)=("([^"]*)"|\S+)', line):
                              key = match.group(1)
                              value = match.group(3) or match.group(2)
                              labels[key] = value

                      elif stage[0] == 'line_format':
                          template = stage[1]
                          # Simple template substitution
                          for key, value in labels.items():
                              template = template.replace(f'{{{{{key}}}}}', str(value))
                          line = template

                  return LogQueryResult(
                      timestamp=entry.timestamp,
                      labels=labels,
                      line=line
                  )

              def aggregate(self, query: str, start_time: float, end_time: float,
                           step: float) -> dict[str, list[tuple[float, float]]]:
                  """Execute metric query over logs (count_over_time, rate, etc.)."""
                  # Parse aggregation: count_over_time({job="app"}[5m])
                  match = re.match(r'(\w+)\((.+)\[(\d+)([smh])\]\)', query)
                  if not match:
                      raise ValueError("Invalid aggregation query")

                  func = match.group(1)
                  inner_query = match.group(2)
                  interval = int(match.group(3)) * {'s': 1, 'm': 60, 'h': 3600}[match.group(4)]

                  results = {}
                  for ts in self._time_range(start_time, end_time, step):
                      window_start = ts - interval
                      count = 0
                      for _ in self.query(inner_query, window_start, ts, limit=10000):
                          count += 1

                      key = 'total'  # Could be grouped by labels
                      results.setdefault(key, []).append((ts, count))

                  return results
          ```
      pitfalls:
      - Unbounded query scans entire log history
      - JSON parsing failures silently drop entries
      - Pipeline order matters for filter efficiency
      acceptance_criteria:
      - Full-text search matches log entries containing any or all of the specified keywords
      - Field filters like level=ERROR and service=api narrow results to matching structured values
      - Regular expression patterns in queries match against log message text and structured fields
      - Query pagination returns results in pages with a cursor for efficient traversal of large result sets
      deliverables:
      - Query language parser that supports full-text search, field filters, and boolean operators
      - Full-text search engine that finds log entries matching a keyword or phrase across all fields
      - Field filter evaluator that restricts results to entries matching specific structured field values
      - Time range query executor that limits search results to entries within a specified time window
  alerting-system:
    name: Alerting System
    description: Build an alerting system with rule evaluation, alert grouping, silencing, and notification routing.
    why_important: Alerting is critical for on-call. Understanding alert fatigue, grouping, and routing helps design better
      alert systems.
    difficulty: intermediate
    tags:
    - observability
    - distributed-systems
    estimated_hours: 35
    prerequisites:
    - metrics-collector
    learning_outcomes:
    - Design alert rule evaluation engine
    - Implement alert state machine (pending, firing, resolved)
    - Build notification routing and grouping
    - Handle alert silencing and inhibition
    milestones:
    - name: Alert Rule Evaluation
      description: Build rule evaluation engine that periodically queries metrics and triggers alerts based on thresholds.
      hints:
        level1: Define rules with PromQL expression and threshold.
        level2: 'Track alert state transitions: inactive -> pending -> firing.'
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from datetime import datetime, timedelta
          from enum import Enum
          from typing import Optional
          import asyncio

          class AlertState(str, Enum):
              INACTIVE = "inactive"
              PENDING = "pending"
              FIRING = "firing"

          @dataclass
          class AlertRule:
              name: str
              expr: str  # PromQL expression
              for_duration: timedelta = timedelta(0)  # Duration before firing
              labels: dict[str, str] = field(default_factory=dict)
              annotations: dict[str, str] = field(default_factory=dict)
              evaluation_interval: timedelta = timedelta(seconds=60)

          @dataclass
          class Alert:
              rule: AlertRule
              labels: dict[str, str]
              state: AlertState = AlertState.INACTIVE
              active_at: Optional[datetime] = None
              fired_at: Optional[datetime] = None
              resolved_at: Optional[datetime] = None
              value: float = 0
              annotations: dict[str, str] = field(default_factory=dict)

              @property
              def fingerprint(self) -> str:
                  import hashlib
                  import json
                  label_str = json.dumps(self.labels, sort_keys=True)
                  return hashlib.md5(f"{self.rule.name}:{label_str}".encode()).hexdigest()

          class AlertEvaluator:
              def __init__(self, query_engine, notifier):
                  self.query_engine = query_engine
                  self.notifier = notifier
                  self.rules: list[AlertRule] = []
                  self.active_alerts: dict[str, Alert] = {}
                  self._running = False

              def add_rule(self, rule: AlertRule):
                  self.rules.append(rule)

              async def start(self):
                  self._running = True
                  while self._running:
                      await self._evaluate_all()
                      await asyncio.sleep(60)  # Evaluation interval

              async def stop(self):
                  self._running = False

              async def _evaluate_all(self):
                  for rule in self.rules:
                      await self._evaluate_rule(rule)

              async def _evaluate_rule(self, rule: AlertRule):
                  now = datetime.utcnow()

                  # Query metrics
                  results = self.query_engine.instant_query(rule.expr, now.timestamp())

                  # Track which alerts are still active
                  active_fingerprints = set()

                  for labels, value in results.items():
                      # Merge rule labels with metric labels
                      all_labels = {**labels.to_dict(), **rule.labels}
                      alert = Alert(rule=rule, labels=all_labels, value=value)
                      fingerprint = alert.fingerprint
                      active_fingerprints.add(fingerprint)

                      # Check for existing alert
                      existing = self.active_alerts.get(fingerprint)

                      if existing:
                          # Update existing alert
                          self._update_alert(existing, value, now)
                      else:
                          # New alert
                          alert.state = AlertState.PENDING
                          alert.active_at = now
                          alert.annotations = self._render_annotations(rule.annotations, all_labels, value)
                          self.active_alerts[fingerprint] = alert

                  # Resolve alerts no longer active
                  for fingerprint in list(self.active_alerts.keys()):
                      if fingerprint not in active_fingerprints:
                          alert = self.active_alerts[fingerprint]
                          if alert.state != AlertState.INACTIVE:
                              await self._resolve_alert(alert, now)

              def _update_alert(self, alert: Alert, value: float, now: datetime):
                  alert.value = value

                  if alert.state == AlertState.PENDING:
                      # Check if pending duration has passed
                      pending_duration = now - alert.active_at
                      if pending_duration >= alert.rule.for_duration:
                          alert.state = AlertState.FIRING
                          alert.fired_at = now
                          asyncio.create_task(self.notifier.notify(alert))

                  elif alert.state == AlertState.FIRING:
                      # Still firing, update value
                      pass

              async def _resolve_alert(self, alert: Alert, now: datetime):
                  if alert.state == AlertState.FIRING:
                      alert.resolved_at = now
                      alert.state = AlertState.INACTIVE
                      await self.notifier.notify_resolved(alert)
                  del self.active_alerts[alert.fingerprint]

              def _render_annotations(self, templates: dict, labels: dict, value: float) -> dict:
                  result = {}
                  for key, template in templates.items():
                      text = template
                      for label_name, label_value in labels.items():
                          text = text.replace(f'{{{{ $labels.{label_name} }}}}', str(label_value))
                      text = text.replace('{{ $value }}', str(value))
                      result[key] = text
                  return result
          ```
      pitfalls:
      - Flapping alerts from noisy metrics need hysteresis
      - for_duration resets if metric briefly returns normal
      - Template rendering errors crash evaluation loop
      acceptance_criteria:
      - Evaluates PromQL-like expressions against the current metric data and returns numeric results
      - Supports comparison operators (>, <, >=, <=, ==, !=) for threshold-based alerting
      - Handles for-duration by keeping alerts in pending state until the condition holds for the specified period
      - Returns correct firing or resolved status for each evaluated alert rule after each evaluation cycle
      deliverables:
      - Rule expression parser that converts PromQL-like alert expressions into evaluable rule objects
      - Periodic evaluation loop that checks all active rules at a configurable interval (e.g., every 15 seconds)
      - Threshold comparison engine supporting greater-than, less-than, and equality operators on metric values
      - Duration-based firing logic that transitions alerts from pending to firing after the configured for-duration
    - name: Alert Grouping
      description: Group related alerts together to reduce notification noise with configurable grouping keys.
      hints:
        level1: Group alerts by common labels (e.g., alertname, cluster).
        level2: Batch notifications within group_wait window.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from datetime import datetime, timedelta
          from typing import Optional
          import asyncio

          @dataclass
          class AlertGroup:
              key: str  # Hash of group labels
              labels: dict[str, str]  # Group labels
              alerts: dict[str, Alert] = field(default_factory=dict)
              first_alert_at: Optional[datetime] = None
              last_notification_at: Optional[datetime] = None

          class AlertGrouper:
              def __init__(self, group_by: list[str],
                           group_wait: timedelta = timedelta(seconds=30),
                           group_interval: timedelta = timedelta(minutes=5),
                           repeat_interval: timedelta = timedelta(hours=4)):
                  self.group_by = group_by
                  self.group_wait = group_wait
                  self.group_interval = group_interval
                  self.repeat_interval = repeat_interval
                  self._groups: dict[str, AlertGroup] = {}
                  self._pending_notifications: dict[str, asyncio.Task] = {}

              def add_alert(self, alert: Alert) -> AlertGroup:
                  """Add alert to appropriate group."""
                  group_labels = {k: alert.labels.get(k, '') for k in self.group_by}
                  group_key = self._compute_key(group_labels)

                  if group_key not in self._groups:
                      self._groups[group_key] = AlertGroup(
                          key=group_key,
                          labels=group_labels,
                          first_alert_at=datetime.utcnow()
                      )

                  group = self._groups[group_key]
                  group.alerts[alert.fingerprint] = alert

                  # Schedule notification if not already pending
                  if group_key not in self._pending_notifications:
                      self._schedule_notification(group)

                  return group

              def remove_alert(self, alert: Alert) -> Optional[AlertGroup]:
                  """Remove resolved alert from group."""
                  group_labels = {k: alert.labels.get(k, '') for k in self.group_by}
                  group_key = self._compute_key(group_labels)

                  group = self._groups.get(group_key)
                  if group:
                      group.alerts.pop(alert.fingerprint, None)
                      if not group.alerts:
                          # Group empty, remove it
                          del self._groups[group_key]
                          return None
                  return group

              def _compute_key(self, labels: dict[str, str]) -> str:
                  import hashlib
                  import json
                  return hashlib.md5(json.dumps(labels, sort_keys=True).encode()).hexdigest()

              def _schedule_notification(self, group: AlertGroup):
                  """Schedule notification after group_wait."""
                  async def notify_after_wait():
                      await asyncio.sleep(self.group_wait.total_seconds())
                      await self._send_notification(group)

                  self._pending_notifications[group.key] = asyncio.create_task(notify_after_wait())

              async def _send_notification(self, group: AlertGroup):
                  now = datetime.utcnow()

                  # Check if we should notify
                  if group.last_notification_at:
                      since_last = now - group.last_notification_at

                      # Check repeat interval for firing alerts
                      if since_last < self.repeat_interval:
                          # Check group interval for new alerts
                          if since_last < self.group_interval:
                              # Too soon, reschedule
                              self._schedule_notification(group)
                              return

                  group.last_notification_at = now
                  self._pending_notifications.pop(group.key, None)

                  # Yield notification to be sent
                  return group

              def get_groups(self) -> list[AlertGroup]:
                  return list(self._groups.values())

          class NotificationBatcher:
              """Batches notifications to reduce alert fatigue."""

              def __init__(self, grouper: AlertGrouper, sender):
                  self.grouper = grouper
                  self.sender = sender

              async def on_alert(self, alert: Alert):
                  group = self.grouper.add_alert(alert)
                  # Notification will be sent after group_wait

              async def on_resolved(self, alert: Alert):
                  group = self.grouper.remove_alert(alert)
                  if group:
                      # Send resolution notification
                      await self.sender.send_grouped(group, resolved=[alert])

              async def process_groups(self):
                  """Periodically check and send pending notifications."""
                  for group in self.grouper.get_groups():
                      notification = await self.grouper._send_notification(group)
                      if notification:
                          await self.sender.send_grouped(group)
          ```
      pitfalls:
      - Group key change orphans alert in old group
      - Long group_wait delays critical alerts
      - Memory leak from empty groups not cleaned up
      acceptance_criteria:
      - Alerts are grouped by user-configurable label sets such as alertname, cluster, or service
      - Aggregation reduces notification noise by batching multiple alerts into a single notification
      - group_wait delays the first notification to collect more alerts, group_interval controls re-send frequency
      - Group key generation produces a stable, deterministic key from the sorted set of grouping label values
      deliverables:
      - Group-by-labels logic that aggregates related alerts into a single notification group
      - Aggregated notification builder that combines grouped alerts into a single consolidated message
      - Group wait and group interval configuration controlling initial wait time and re-notification frequency
      - Group resolution handler that sends resolved notifications when all alerts in a group have cleared
    - name: Silencing & Inhibition
      description: Implement alert silencing for maintenance windows and inhibition to suppress alerts when related alerts
        fire.
      hints:
        level1: Silence matches alerts by label matchers within time window.
        level2: 'Inhibition: if source alert fires, suppress target alerts with matching labels.'
        level3: |-
          ```python
          from dataclasses import dataclass
          from datetime import datetime
          from typing import Optional
          import re

          @dataclass
          class Silence:
              id: str
              matchers: list[LabelMatcher]
              starts_at: datetime
              ends_at: datetime
              created_by: str
              comment: str

              def is_active(self, now: datetime = None) -> bool:
                  now = now or datetime.utcnow()
                  return self.starts_at <= now <= self.ends_at

              def matches(self, labels: dict[str, str]) -> bool:
                  return all(m.matches_dict(labels) for m in self.matchers)

          @dataclass
          class InhibitRule:
              source_matchers: list[LabelMatcher]  # Source alert must match
              target_matchers: list[LabelMatcher]  # Target alert must match
              equal: list[str]  # Labels that must be equal between source and target

          class SilenceManager:
              def __init__(self):
                  self.silences: dict[str, Silence] = {}

              def create(self, matchers: list[LabelMatcher], starts_at: datetime,
                         ends_at: datetime, created_by: str, comment: str) -> Silence:
                  import uuid
                  silence = Silence(
                      id=str(uuid.uuid4()),
                      matchers=matchers,
                      starts_at=starts_at,
                      ends_at=ends_at,
                      created_by=created_by,
                      comment=comment
                  )
                  self.silences[silence.id] = silence
                  return silence

              def delete(self, silence_id: str):
                  self.silences.pop(silence_id, None)

              def is_silenced(self, labels: dict[str, str], now: datetime = None) -> Optional[Silence]:
                  for silence in self.silences.values():
                      if silence.is_active(now) and silence.matches(labels):
                          return silence
                  return None

              def cleanup_expired(self):
                  now = datetime.utcnow()
                  self.silences = {
                      id: s for id, s in self.silences.items()
                      if s.ends_at > now
                  }

          class InhibitionProcessor:
              def __init__(self, rules: list[InhibitRule]):
                  self.rules = rules

              def is_inhibited(self, target: Alert, active_alerts: list[Alert]) -> bool:
                  """Check if target alert is inhibited by any active alert."""
                  for rule in self.rules:
                      # Check if target matches target_matchers
                      if not self._matches_all(target.labels, rule.target_matchers):
                          continue

                      # Find source alerts that could inhibit
                      for source in active_alerts:
                          if source.fingerprint == target.fingerprint:
                              continue

                          if source.state != AlertState.FIRING:
                              continue

                          # Check if source matches source_matchers
                          if not self._matches_all(source.labels, rule.source_matchers):
                              continue

                          # Check equal labels
                          if self._labels_equal(source.labels, target.labels, rule.equal):
                              return True

                  return False

              def _matches_all(self, labels: dict, matchers: list[LabelMatcher]) -> bool:
                  return all(m.matches_dict(labels) for m in matchers)

              def _labels_equal(self, source: dict, target: dict, keys: list[str]) -> bool:
                  return all(source.get(k) == target.get(k) for k in keys)

          class AlertProcessor:
              """Main processor combining silencing, inhibition, and grouping."""

              def __init__(self, silence_manager: SilenceManager,
                           inhibitor: InhibitionProcessor,
                           grouper: AlertGrouper,
                           notifier):
                  self.silences = silence_manager
                  self.inhibitor = inhibitor
                  self.grouper = grouper
                  self.notifier = notifier
                  self.active_alerts: list[Alert] = []

              async def process(self, alert: Alert):
                  # Check if silenced
                  silence = self.silences.is_silenced(alert.labels)
                  if silence:
                      alert.silenced_by = silence.id
                      return

                  # Check if inhibited
                  if self.inhibitor.is_inhibited(alert, self.active_alerts):
                      alert.inhibited = True
                      return

                  # Track active alert
                  self.active_alerts = [a for a in self.active_alerts
                                       if a.fingerprint != alert.fingerprint]
                  if alert.state == AlertState.FIRING:
                      self.active_alerts.append(alert)

                  # Add to group for notification
                  self.grouper.add_alert(alert)
          ```
      pitfalls:
      - Inhibition loop when alerts inhibit each other
      - Silence with wrong matchers misses alerts
      - Race between silence creation and firing alert
      acceptance_criteria:
      - Alerts matching the silence matcher labels are suppressed and do not trigger notifications
      - Time-based silence windows correctly activate at the start time and expire at the end time
      - Inhibition rules suppress target alerts when the source alert with matching labels is actively firing
      - Expired silences are automatically removed and previously silenced alerts resume normal notification flow
      deliverables:
      - Silence matcher that suppresses alerts whose labels match a user-defined set of matchers
      - Time-based silencing with configurable start time, end time, and optional recurrence schedule
      - Inhibition rules that suppress lower-severity alerts when a higher-severity related alert is firing
      - Priority-based suppression that prevents notification of inhibited alerts while preserving their state
    - name: Notification Routing
      description: Route alerts to different receivers (Slack, PagerDuty, email) based on matching rules.
      hints:
        level1: Define routes with matchers that direct to receivers.
        level2: Support nested routes with continue/stop semantics.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from abc import ABC, abstractmethod
          from typing import Optional
          import aiohttp

          @dataclass
          class Route:
              matchers: list[LabelMatcher] = field(default_factory=list)
              receiver: str = ""
              continue_matching: bool = False
              children: list['Route'] = field(default_factory=list)
              group_by: list[str] = field(default_factory=list)

          class Receiver(ABC):
              @abstractmethod
              async def send(self, group: AlertGroup):
                  pass

          class SlackReceiver(Receiver):
              def __init__(self, webhook_url: str, channel: str):
                  self.webhook_url = webhook_url
                  self.channel = channel

              async def send(self, group: AlertGroup):
                  alerts = list(group.alerts.values())
                  firing = [a for a in alerts if a.state == AlertState.FIRING]

                  blocks = [{
                      "type": "header",
                      "text": {
                          "type": "plain_text",
                          "text": f"ðŸ”¥ {len(firing)} alert(s) firing"
                      }
                  }]

                  for alert in firing[:10]:  # Limit to 10
                      blocks.append({
                          "type": "section",
                          "text": {
                              "type": "mrkdwn",
                              "text": f"*{alert.labels.get('alertname', 'Unknown')}*\n"
                                     f"{alert.annotations.get('summary', '')}\n"
                                     f"Value: {alert.value}"
                          }
                      })

                  payload = {
                      "channel": self.channel,
                      "blocks": blocks
                  }

                  async with aiohttp.ClientSession() as session:
                      await session.post(self.webhook_url, json=payload)

          class PagerDutyReceiver(Receiver):
              def __init__(self, service_key: str):
                  self.service_key = service_key
                  self.api_url = "https://events.pagerduty.com/v2/enqueue"

              async def send(self, group: AlertGroup):
                  alerts = list(group.alerts.values())

                  for alert in alerts:
                      if alert.state == AlertState.FIRING:
                          event_action = "trigger"
                      else:
                          event_action = "resolve"

                      payload = {
                          "routing_key": self.service_key,
                          "event_action": event_action,
                          "dedup_key": alert.fingerprint,
                          "payload": {
                              "summary": alert.annotations.get('summary', alert.labels.get('alertname')),
                              "source": alert.labels.get('instance', 'unknown'),
                              "severity": alert.labels.get('severity', 'warning'),
                              "custom_details": alert.labels
                          }
                      }

                      async with aiohttp.ClientSession() as session:
                          await session.post(self.api_url, json=payload)

          class NotificationRouter:
              def __init__(self, root_route: Route, receivers: dict[str, Receiver]):
                  self.root = root_route
                  self.receivers = receivers

              async def route(self, group: AlertGroup):
                  """Route alert group to matching receivers."""
                  matched_receivers = self._find_receivers(group, self.root)

                  for receiver_name in matched_receivers:
                      receiver = self.receivers.get(receiver_name)
                      if receiver:
                          try:
                              await receiver.send(group)
                          except Exception as e:
                              print(f"Failed to send to {receiver_name}: {e}")

              def _find_receivers(self, group: AlertGroup, route: Route) -> list[str]:
                  """Find all matching receivers for alert group."""
                  # Check if this route matches
                  if not self._matches_route(group, route):
                      return []

                  receivers = []

                  # Add this route's receiver if specified
                  if route.receiver:
                      receivers.append(route.receiver)

                  # Check children
                  for child in route.children:
                      child_receivers = self._find_receivers(group, child)
                      receivers.extend(child_receivers)

                      # Stop if matched and not continue
                      if child_receivers and not child.continue_matching:
                          break

                  return receivers

              def _matches_route(self, group: AlertGroup, route: Route) -> bool:
                  if not route.matchers:
                      return True  # Default route matches all

                  # Use first alert's labels for matching
                  if not group.alerts:
                      return False

                  first_alert = list(group.alerts.values())[0]
                  return all(m.matches_dict(first_alert.labels) for m in route.matchers)
          ```
      pitfalls:
      - Missing default route causes unrouted alerts
      - continue=true on all routes sends duplicate notifications
      - Rate limiting by receiver prevents important alerts
      acceptance_criteria:
      - Alerts are routed to different notification channels based on severity and label-matching rules
      - Multiple notification channels including email, Slack, and webhook are supported simultaneously
      - Notification grouping batches alerts by alert name and labels to reduce duplicate notifications
      - Rate limiting prevents notification storms by enforcing a maximum send frequency per channel
      deliverables:
      - Notification channel configuration supporting email, Slack, PagerDuty, and generic webhook targets
      - Routing rules that direct alerts to channels based on severity, team, and label-match criteria
      - Escalation policies that promote unacknowledged alerts to higher-priority channels after a timeout
      - On-call schedule integration that routes critical alerts to the currently on-call responder
  apm-system:
    name: APM Tracing System
    description: Build an Application Performance Monitoring system with distributed tracing, service maps, and performance
      analysis.
    why_important: APM helps identify performance bottlenecks across microservices. Understanding trace data helps design
      better instrumentation.
    difficulty: advanced
    tags:
    - observability
    - distributed-systems
    - performance
    estimated_hours: 45
    prerequisites:
    - distributed-tracing
    learning_outcomes:
    - Design span collection and storage
    - Build service dependency maps from traces
    - Implement latency percentile analysis
    - Handle high-volume trace sampling
    milestones:
    - name: Trace Collection
      description: Build trace collector that ingests spans from multiple services with proper parent-child linking.
      hints:
        level1: Collect spans with trace ID, span ID, parent ID, timestamps.
        level2: Handle out-of-order span arrival, reconstruct trace tree.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from datetime import datetime
          from typing import Optional
          from collections import defaultdict
          import asyncio

          @dataclass
          class Span:
              trace_id: str
              span_id: str
              parent_id: Optional[str]
              operation_name: str
              service_name: str
              start_time: datetime
              duration_us: int  # microseconds
              status: str = "OK"  # OK, ERROR
              tags: dict = field(default_factory=dict)
              logs: list = field(default_factory=list)

          @dataclass
          class Trace:
              trace_id: str
              root_span: Optional[Span] = None
              spans: dict[str, Span] = field(default_factory=dict)  # span_id -> Span
              services: set = field(default_factory=set)
              start_time: Optional[datetime] = None
              duration_us: int = 0

              def add_span(self, span: Span):
                  self.spans[span.span_id] = span
                  self.services.add(span.service_name)

                  if span.parent_id is None:
                      self.root_span = span
                      self.start_time = span.start_time
                      self.duration_us = span.duration_us

              def get_children(self, span_id: str) -> list[Span]:
                  return [s for s in self.spans.values() if s.parent_id == span_id]

              def get_depth(self) -> int:
                  if not self.root_span:
                      return 0

                  def depth(span_id: str) -> int:
                      children = self.get_children(span_id)
                      if not children:
                          return 1
                      return 1 + max(depth(c.span_id) for c in children)

                  return depth(self.root_span.span_id)

          class TraceCollector:
              def __init__(self, storage, assembler_timeout: float = 30.0):
                  self.storage = storage
                  self.assembler_timeout = assembler_timeout
                  self._pending_traces: dict[str, Trace] = {}
                  self._trace_timers: dict[str, asyncio.Task] = {}
                  self._lock = asyncio.Lock()

              async def collect(self, spans: list[Span]):
                  """Collect batch of spans."""
                  async with self._lock:
                      for span in spans:
                          await self._add_span(span)

              async def _add_span(self, span: Span):
                  trace_id = span.trace_id

                  if trace_id not in self._pending_traces:
                      self._pending_traces[trace_id] = Trace(trace_id=trace_id)
                      # Start assembly timeout
                      self._trace_timers[trace_id] = asyncio.create_task(
                          self._assembly_timeout(trace_id)
                      )

                  trace = self._pending_traces[trace_id]
                  trace.add_span(span)

                  # Check if trace is complete (has root and reasonable time passed)
                  if trace.root_span and self._is_likely_complete(trace):
                      await self._finalize_trace(trace_id)

              def _is_likely_complete(self, trace: Trace) -> bool:
                  # Heuristic: trace is complete if we have root and all spans
                  # have their parents (except root)
                  for span in trace.spans.values():
                      if span.parent_id and span.parent_id not in trace.spans:
                          return False  # Missing parent
                  return True

              async def _assembly_timeout(self, trace_id: str):
                  await asyncio.sleep(self.assembler_timeout)
                  async with self._lock:
                      if trace_id in self._pending_traces:
                          await self._finalize_trace(trace_id)

              async def _finalize_trace(self, trace_id: str):
                  trace = self._pending_traces.pop(trace_id, None)
                  timer = self._trace_timers.pop(trace_id, None)
                  if timer:
                      timer.cancel()

                  if trace:
                      # Calculate trace-level metrics
                      self._calculate_metrics(trace)
                      await self.storage.store_trace(trace)

              def _calculate_metrics(self, trace: Trace):
                  if not trace.spans:
                      return

                  # Find actual start/end times
                  start_times = [s.start_time for s in trace.spans.values()]
                  end_times = [
                      s.start_time.timestamp() * 1e6 + s.duration_us
                      for s in trace.spans.values()
                  ]

                  trace.start_time = min(start_times)
                  trace.duration_us = int(max(end_times) - min(s.timestamp() * 1e6 for s in start_times))

          class OTLPReceiver:
              """OpenTelemetry Protocol receiver."""

              def __init__(self, collector: TraceCollector):
                  self.collector = collector

              async def handle_traces(self, request_body: bytes) -> dict:
                  # Parse OTLP protobuf (simplified - real impl uses protobuf)
                  import json
                  data = json.loads(request_body)

                  spans = []
                  for resource_span in data.get('resourceSpans', []):
                      service_name = self._get_service_name(resource_span.get('resource', {}))

                      for scope_span in resource_span.get('scopeSpans', []):
                          for span_data in scope_span.get('spans', []):
                              spans.append(Span(
                                  trace_id=span_data['traceId'],
                                  span_id=span_data['spanId'],
                                  parent_id=span_data.get('parentSpanId'),
                                  operation_name=span_data['name'],
                                  service_name=service_name,
                                  start_time=datetime.fromtimestamp(span_data['startTimeUnixNano'] / 1e9),
                                  duration_us=int((span_data['endTimeUnixNano'] - span_data['startTimeUnixNano']) / 1000),
                                  status=span_data.get('status', {}).get('code', 'OK'),
                                  tags=self._parse_attributes(span_data.get('attributes', []))
                              ))

                  await self.collector.collect(spans)
                  return {'accepted': len(spans)}

              def _get_service_name(self, resource: dict) -> str:
                  for attr in resource.get('attributes', []):
                      if attr['key'] == 'service.name':
                          return attr['value'].get('stringValue', 'unknown')
                  return 'unknown'

              def _parse_attributes(self, attributes: list) -> dict:
                  result = {}
                  for attr in attributes:
                      key = attr['key']
                      value = attr['value']
                      if 'stringValue' in value:
                          result[key] = value['stringValue']
                      elif 'intValue' in value:
                          result[key] = value['intValue']
                      elif 'boolValue' in value:
                          result[key] = value['boolValue']
                  return result
          ```
      pitfalls:
      - Late-arriving spans miss assembly window
      - Memory grows unbounded with incomplete traces
      - Clock skew makes span ordering incorrect
      acceptance_criteria:
      - Ingestion endpoint receives spans via HTTP or gRPC and returns acknowledgment within 100ms
      - OpenTelemetry trace format is parsed correctly including span context, attributes, and events
      - Spans are stored with trace ID indexing so all spans in a trace can be retrieved in a single query
      - System handles high-volume span ingestion of at least 1000 spans per second without data loss
      deliverables:
      - Trace data ingestion API accepting spans via HTTP and gRPC endpoints in OpenTelemetry format
      - Trace sampling module that decides which traces to keep based on configurable sampling policies
      - Trace storage backend that persists span data with indexing for efficient retrieval by trace ID
      - Trace indexing layer that creates secondary indexes on service name, operation, and time range
    - name: Service Map
      description: Build service dependency map from trace data showing call relationships and error rates.
      hints:
        level1: Track caller-callee relationships from parent-child spans.
        level2: 'Aggregate metrics per edge: latency percentiles, error rate, throughput.'
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from collections import defaultdict
          import statistics

          @dataclass
          class ServiceEdge:
              source: str
              target: str
              request_count: int = 0
              error_count: int = 0
              latencies: list[int] = field(default_factory=list)  # microseconds

              @property
              def error_rate(self) -> float:
                  return self.error_count / self.request_count if self.request_count else 0

              @property
              def p50_latency(self) -> float:
                  if not self.latencies:
                      return 0
                  return statistics.median(self.latencies)

              @property
              def p99_latency(self) -> float:
                  if not self.latencies:
                      return 0
                  sorted_lat = sorted(self.latencies)
                  idx = int(len(sorted_lat) * 0.99)
                  return sorted_lat[min(idx, len(sorted_lat) - 1)]

          @dataclass
          class ServiceNode:
              name: str
              request_count: int = 0
              error_count: int = 0
              latencies: list[int] = field(default_factory=list)

          class ServiceMapBuilder:
              def __init__(self, window_seconds: int = 300):
                  self.window_seconds = window_seconds
                  self.nodes: dict[str, ServiceNode] = {}
                  self.edges: dict[str, ServiceEdge] = {}  # "source->target" -> edge
                  self._lock = asyncio.Lock()

              async def process_trace(self, trace: Trace):
                  """Extract service relationships from trace."""
                  async with self._lock:
                      for span in trace.spans.values():
                          # Update service node
                          service = span.service_name
                          if service not in self.nodes:
                              self.nodes[service] = ServiceNode(name=service)

                          node = self.nodes[service]
                          node.request_count += 1
                          if span.status == "ERROR":
                              node.error_count += 1
                          node.latencies.append(span.duration_us)

                          # Update edge if span has parent
                          if span.parent_id and span.parent_id in trace.spans:
                              parent = trace.spans[span.parent_id]
                              if parent.service_name != span.service_name:
                                  edge_key = f"{parent.service_name}->{span.service_name}"
                                  if edge_key not in self.edges:
                                      self.edges[edge_key] = ServiceEdge(
                                          source=parent.service_name,
                                          target=span.service_name
                                      )

                                  edge = self.edges[edge_key]
                                  edge.request_count += 1
                                  if span.status == "ERROR":
                                      edge.error_count += 1
                                  edge.latencies.append(span.duration_us)

              def get_map(self) -> dict:
                  """Return service map as graph data."""
                  return {
                      'nodes': [
                          {
                              'id': name,
                              'requests': node.request_count,
                              'error_rate': node.error_count / node.request_count if node.request_count else 0,
                              'p50_latency_ms': statistics.median(node.latencies) / 1000 if node.latencies else 0,
                              'p99_latency_ms': self._percentile(node.latencies, 0.99) / 1000 if node.latencies else 0
                          }
                          for name, node in self.nodes.items()
                      ],
                      'edges': [
                          {
                              'source': edge.source,
                              'target': edge.target,
                              'requests': edge.request_count,
                              'error_rate': edge.error_rate,
                              'p50_latency_ms': edge.p50_latency / 1000,
                              'p99_latency_ms': edge.p99_latency / 1000
                          }
                          for edge in self.edges.values()
                      ]
                  }

              def _percentile(self, values: list, p: float) -> float:
                  if not values:
                      return 0
                  sorted_vals = sorted(values)
                  idx = int(len(sorted_vals) * p)
                  return sorted_vals[min(idx, len(sorted_vals) - 1)]

              def get_dependencies(self, service: str) -> dict:
                  """Get upstream and downstream dependencies for a service."""
                  upstream = []
                  downstream = []

                  for edge in self.edges.values():
                      if edge.target == service:
                          upstream.append({
                              'service': edge.source,
                              'requests': edge.request_count,
                              'error_rate': edge.error_rate
                          })
                      elif edge.source == service:
                          downstream.append({
                              'service': edge.target,
                              'requests': edge.request_count,
                              'error_rate': edge.error_rate
                          })

                  return {
                      'service': service,
                      'upstream': upstream,
                      'downstream': downstream
                  }
          ```
      pitfalls:
      - Same-service spans inflate node metrics
      - Async calls appear as separate traces
      - High cardinality operations bloat map
      acceptance_criteria:
      - Service dependency graph is correctly built from parent-child span relationships across services
      - Inter-service call metrics (request count, latency, error rate) are computed for each edge in the graph
      - Topology changes such as new services or removed dependencies are detected within one refresh interval
      - Service map visualization clearly displays nodes for services and edges for their communication paths
      deliverables:
      - Service dependency extraction logic that derives caller-callee relationships from span parent links
      - Call graph construction that builds a directed graph of inter-service communication patterns
      - Map visualization that renders the service dependency graph as an interactive node-and-edge diagram
      - Real-time update mechanism that refreshes the service map as new trace data arrives
    - name: Trace Sampling
      description: Implement adaptive sampling to reduce storage costs while preserving interesting traces.
      hints:
        level1: Sample N% of traces randomly, but always keep errors.
        level2: 'Tail-based sampling: decide after trace completes based on characteristics.'
        level3: |-
          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass
          import random
          import hashlib

          class Sampler(ABC):
              @abstractmethod
              def should_sample(self, trace: Trace) -> bool:
                  pass

          class ProbabilisticSampler(Sampler):
              def __init__(self, sample_rate: float = 0.1):
                  self.sample_rate = sample_rate

              def should_sample(self, trace: Trace) -> bool:
                  # Use trace ID for consistent sampling
                  hash_val = int(hashlib.md5(trace.trace_id.encode()).hexdigest(), 16)
                  return (hash_val % 10000) < (self.sample_rate * 10000)

          class RateLimitingSampler(Sampler):
              def __init__(self, traces_per_second: float = 100):
                  self.traces_per_second = traces_per_second
                  self._tokens = traces_per_second
                  self._last_refill = time.time()

              def should_sample(self, trace: Trace) -> bool:
                  self._refill()
                  if self._tokens >= 1:
                      self._tokens -= 1
                      return True
                  return False

              def _refill(self):
                  now = time.time()
                  elapsed = now - self._last_refill
                  self._tokens = min(
                      self.traces_per_second,
                      self._tokens + elapsed * self.traces_per_second
                  )
                  self._last_refill = now

          @dataclass
          class SamplingPolicy:
              name: str
              matcher: callable  # (Trace) -> bool
              sampler: Sampler
              priority: int = 0

          class TailBasedSampler:
              """Decides sampling after trace is complete."""

              def __init__(self, default_rate: float = 0.01):
                  self.default_sampler = ProbabilisticSampler(default_rate)
                  self.policies: list[SamplingPolicy] = []
                  self._setup_default_policies()

              def _setup_default_policies(self):
                  # Always sample errors
                  self.policies.append(SamplingPolicy(
                      name="errors",
                      matcher=lambda t: any(s.status == "ERROR" for s in t.spans.values()),
                      sampler=ProbabilisticSampler(1.0),  # 100%
                      priority=100
                  ))

                  # Sample slow traces
                  self.policies.append(SamplingPolicy(
                      name="slow",
                      matcher=lambda t: t.duration_us > 5_000_000,  # > 5s
                      sampler=ProbabilisticSampler(0.5),  # 50%
                      priority=90
                  ))

                  # Sample traces with many spans (complex flows)
                  self.policies.append(SamplingPolicy(
                      name="complex",
                      matcher=lambda t: len(t.spans) > 50,
                      sampler=ProbabilisticSampler(0.3),
                      priority=80
                  ))

                  # Higher rate for specific services
                  self.policies.append(SamplingPolicy(
                      name="payment_service",
                      matcher=lambda t: "payment-service" in t.services,
                      sampler=ProbabilisticSampler(0.2),
                      priority=70
                  ))

              def add_policy(self, policy: SamplingPolicy):
                  self.policies.append(policy)
                  self.policies.sort(key=lambda p: p.priority, reverse=True)

              def should_sample(self, trace: Trace) -> tuple[bool, str]:
                  """Returns (should_sample, reason)."""
                  for policy in self.policies:
                      if policy.matcher(trace):
                          if policy.sampler.should_sample(trace):
                              return True, policy.name
                          # Policy matched but sampler said no
                          # Continue to check lower priority policies

                  # Fall back to default
                  if self.default_sampler.should_sample(trace):
                      return True, "default"

                  return False, "dropped"

          class AdaptiveSampler:
              """Adjusts sampling rate based on throughput."""

              def __init__(self, target_traces_per_minute: int = 1000):
                  self.target = target_traces_per_minute
                  self.current_rate = 0.1
                  self._count = 0
                  self._last_adjust = time.time()
                  self._adjust_interval = 60  # seconds

              def should_sample(self, trace: Trace) -> bool:
                  self._maybe_adjust()

                  if random.random() < self.current_rate:
                      self._count += 1
                      return True
                  return False

              def _maybe_adjust(self):
                  now = time.time()
                  if now - self._last_adjust < self._adjust_interval:
                      return

                  # Calculate actual rate
                  actual_per_minute = self._count

                  # Adjust rate
                  if actual_per_minute > self.target * 1.1:
                      # Too many, decrease rate
                      self.current_rate *= 0.9
                  elif actual_per_minute < self.target * 0.9:
                      # Too few, increase rate
                      self.current_rate *= 1.1

                  self.current_rate = max(0.001, min(1.0, self.current_rate))
                  self._count = 0
                  self._last_adjust = now
          ```
      pitfalls:
      - Head-based sampling loses interesting traces
      - Rate limiting causes bursty drops
      - Adaptive sampling oscillates under variable load
      acceptance_criteria:
      - Head-based sampling drops traces at the configured probability before any spans are stored
      - Tail-based sampling retains error traces and high-latency traces regardless of the base sampling rate
      - Sampling rates are independently configurable per service so high-traffic services can be sampled lower
      - Sampled traces include all their spans so no trace is stored with missing child span data
      deliverables:
      - Head-based sampling that makes a probabilistic keep/drop decision at trace creation time
      - Tail-based sampling that evaluates completed traces and keeps those matching interest criteria
      - Sampling rate configuration that allows per-service and per-operation sampling percentage tuning
      - Consistent sampling using trace-ID-based hashing so all spans in a trace get the same decision
  feature-flags:
    name: Feature Flag System
    description: Build a feature flag system with gradual rollouts, A/B testing support, targeting rules, and real-time updates.
    why_important: Feature flags enable safe deployments, A/B testing, and gradual rollouts. Understanding flag systems helps
      design better release strategies.
    difficulty: intermediate
    tags:
    - backend
    - architecture
    - devops
    estimated_hours: 35
    prerequisites:
    - rest-api-design
    learning_outcomes:
    - Design flag evaluation with targeting rules
    - Implement percentage-based rollouts
    - Build real-time flag updates without restart
    - Handle flag dependencies and conflicts
    milestones:
    - name: Flag Evaluation Engine
      description: Build the core flag evaluation engine with support for boolean, string, number, and JSON flags.
      hints:
        level1: Store flag configurations with default and variation values.
        level2: Evaluate targeting rules against user context to determine variation.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from typing import Any, Optional, Union
          from enum import Enum
          import hashlib

          class FlagType(str, Enum):
              BOOLEAN = "boolean"
              STRING = "string"
              NUMBER = "number"
              JSON = "json"

          @dataclass
          class Variation:
              value: Any
              name: str = ""
              description: str = ""

          @dataclass
          class TargetingRule:
              id: str
              conditions: list['Condition']
              variation_index: int
              priority: int = 0

          @dataclass
          class Condition:
              attribute: str
              operator: str  # eq, neq, contains, startswith, gt, lt, in, regex
              values: list[Any]

          @dataclass
          class FeatureFlag:
              key: str
              type: FlagType
              variations: list[Variation]
              default_variation: int  # Index of default variation
              targeting_rules: list[TargetingRule] = field(default_factory=list)
              percentage_rollout: Optional['PercentageRollout'] = None
              enabled: bool = True
              prerequisites: list[str] = field(default_factory=list)

          @dataclass
          class PercentageRollout:
              bucket_by: str = "user_id"  # Attribute to hash for bucketing
              variations: list[tuple[int, int]]  # [(variation_index, percentage), ...]

          @dataclass
          class EvaluationContext:
              user_id: str = ""
              attributes: dict = field(default_factory=dict)

              def get(self, key: str) -> Any:
                  if key == "user_id":
                      return self.user_id
                  return self.attributes.get(key)

          @dataclass
          class EvaluationResult:
              value: Any
              variation_index: int
              reason: str
              flag_key: str

          class FlagEvaluator:
              def __init__(self, flags: dict[str, FeatureFlag]):
                  self.flags = flags

              def evaluate(self, flag_key: str, context: EvaluationContext,
                          default: Any = None) -> EvaluationResult:
                  flag = self.flags.get(flag_key)

                  if not flag:
                      return EvaluationResult(
                          value=default,
                          variation_index=-1,
                          reason="FLAG_NOT_FOUND",
                          flag_key=flag_key
                      )

                  if not flag.enabled:
                      return self._result(flag, flag.default_variation, "FLAG_DISABLED")

                  # Check prerequisites
                  for prereq_key in flag.prerequisites:
                      prereq_result = self.evaluate(prereq_key, context)
                      if not prereq_result.value:
                          return self._result(flag, flag.default_variation, "PREREQUISITE_FAILED")

                  # Evaluate targeting rules (highest priority first)
                  sorted_rules = sorted(flag.targeting_rules, key=lambda r: r.priority, reverse=True)
                  for rule in sorted_rules:
                      if self._evaluate_rule(rule, context):
                          return self._result(flag, rule.variation_index, f"RULE:{rule.id}")

                  # Check percentage rollout
                  if flag.percentage_rollout:
                      variation_idx = self._evaluate_rollout(flag, context)
                      if variation_idx is not None:
                          return self._result(flag, variation_idx, "ROLLOUT")

                  return self._result(flag, flag.default_variation, "DEFAULT")

              def _evaluate_rule(self, rule: TargetingRule, context: EvaluationContext) -> bool:
                  return all(self._evaluate_condition(c, context) for c in rule.conditions)

              def _evaluate_condition(self, condition: Condition, context: EvaluationContext) -> bool:
                  attr_value = context.get(condition.attribute)

                  if condition.operator == "eq":
                      return attr_value == condition.values[0]
                  elif condition.operator == "neq":
                      return attr_value != condition.values[0]
                  elif condition.operator == "in":
                      return attr_value in condition.values
                  elif condition.operator == "contains":
                      return condition.values[0] in str(attr_value)
                  elif condition.operator == "startswith":
                      return str(attr_value).startswith(condition.values[0])
                  elif condition.operator == "gt":
                      return float(attr_value) > float(condition.values[0])
                  elif condition.operator == "lt":
                      return float(attr_value) < float(condition.values[0])
                  elif condition.operator == "regex":
                      import re
                      return bool(re.match(condition.values[0], str(attr_value)))

                  return False

              def _evaluate_rollout(self, flag: FeatureFlag, context: EvaluationContext) -> Optional[int]:
                  rollout = flag.percentage_rollout
                  bucket_value = context.get(rollout.bucket_by)
                  if bucket_value is None:
                      return None

                  # Hash to get bucket (0-100)
                  hash_input = f"{flag.key}:{bucket_value}"
                  bucket = int(hashlib.md5(hash_input.encode()).hexdigest(), 16) % 100

                  cumulative = 0
                  for variation_idx, percentage in rollout.variations:
                      cumulative += percentage
                      if bucket < cumulative:
                          return variation_idx

                  return None

              def _result(self, flag: FeatureFlag, variation_idx: int, reason: str) -> EvaluationResult:
                  return EvaluationResult(
                      value=flag.variations[variation_idx].value,
                      variation_index=variation_idx,
                      reason=reason,
                      flag_key=flag.key
                  )
          ```
      pitfalls:
      - Inconsistent hashing causes users to flip-flop variations
      - Circular prerequisites cause infinite loop
      - Rule priority ties cause non-deterministic evaluation
      acceptance_criteria:
      - Evaluate flag rules against user context and return correct variant
      - Support percentage rollouts using consistent hashing for stable assignment
      - Handle complex rule conditions with AND/OR logic and multiple attributes
      - Return variant value with evaluation reason for debugging and audit
      deliverables:
      - Flag definition storage with rules and targeting configuration
      - Evaluation logic resolving flag value from rules and context
      - Context-based targeting using user attributes and segments
      - Default values returned when no targeting rules match
    - name: Real-time Flag Updates
      description: Implement real-time flag updates using SSE/WebSocket with local caching and fallback.
      hints:
        level1: Poll or stream flag changes from server.
        level2: Cache flags locally, use stale cache if server unavailable.
        level3: |-
          ```python
          import asyncio
          import aiohttp
          import json
          from dataclasses import dataclass
          from datetime import datetime
          from typing import Callable

          @dataclass
          class FlagCache:
              flags: dict[str, FeatureFlag]
              version: int
              updated_at: datetime

          class FlagClient:
              def __init__(self, base_url: str, sdk_key: str):
                  self.base_url = base_url
                  self.sdk_key = sdk_key
                  self.cache: FlagCache = None
                  self.evaluator: FlagEvaluator = None
                  self._listeners: list[Callable] = []
                  self._sse_task = None
                  self._running = False

              async def initialize(self):
                  """Fetch initial flags and start streaming updates."""
                  await self._fetch_all_flags()
                  self._running = True
                  self._sse_task = asyncio.create_task(self._stream_updates())

              async def close(self):
                  self._running = False
                  if self._sse_task:
                      self._sse_task.cancel()

              def evaluate(self, flag_key: str, context: EvaluationContext,
                          default: Any = None) -> EvaluationResult:
                  if not self.evaluator:
                      return EvaluationResult(
                          value=default,
                          variation_index=-1,
                          reason="NOT_INITIALIZED",
                          flag_key=flag_key
                      )
                  return self.evaluator.evaluate(flag_key, context, default)

              def on_change(self, callback: Callable[[str, FeatureFlag], None]):
                  """Register listener for flag changes."""
                  self._listeners.append(callback)

              async def _fetch_all_flags(self):
                  headers = {"Authorization": f"Bearer {self.sdk_key}"}

                  async with aiohttp.ClientSession() as session:
                      async with session.get(f"{self.base_url}/flags", headers=headers) as resp:
                          if resp.status == 200:
                              data = await resp.json()
                              self._update_cache(data)

              async def _stream_updates(self):
                  """Stream flag updates using Server-Sent Events."""
                  headers = {
                      "Authorization": f"Bearer {self.sdk_key}",
                      "Accept": "text/event-stream"
                  }

                  while self._running:
                      try:
                          async with aiohttp.ClientSession() as session:
                              async with session.get(f"{self.base_url}/stream",
                                                    headers=headers,
                                                    timeout=None) as resp:
                                  async for line in resp.content:
                                      if not self._running:
                                          break

                                      line = line.decode().strip()
                                      if line.startswith("data:"):
                                          event_data = json.loads(line[5:])
                                          await self._handle_event(event_data)

                      except aiohttp.ClientError:
                          # Reconnect after delay
                          await asyncio.sleep(5)
                      except asyncio.CancelledError:
                          break

              async def _handle_event(self, event: dict):
                  event_type = event.get("type")

                  if event_type == "flag_updated":
                      flag_data = event.get("flag")
                      flag = self._parse_flag(flag_data)
                      self.cache.flags[flag.key] = flag
                      self.cache.version = event.get("version", self.cache.version + 1)
                      self._rebuild_evaluator()
                      self._notify_listeners(flag.key, flag)

                  elif event_type == "flag_deleted":
                      flag_key = event.get("key")
                      self.cache.flags.pop(flag_key, None)
                      self._rebuild_evaluator()
                      self._notify_listeners(flag_key, None)

                  elif event_type == "full_sync":
                      self._update_cache(event.get("flags"))

              def _update_cache(self, data: dict):
                  flags = {}
                  for flag_data in data.get("flags", []):
                      flag = self._parse_flag(flag_data)
                      flags[flag.key] = flag

                  self.cache = FlagCache(
                      flags=flags,
                      version=data.get("version", 0),
                      updated_at=datetime.utcnow()
                  )
                  self._rebuild_evaluator()

              def _rebuild_evaluator(self):
                  self.evaluator = FlagEvaluator(self.cache.flags)

              def _notify_listeners(self, flag_key: str, flag: FeatureFlag):
                  for listener in self._listeners:
                      try:
                          listener(flag_key, flag)
                      except Exception:
                          pass

              def _parse_flag(self, data: dict) -> FeatureFlag:
                  # Convert dict to FeatureFlag object
                  variations = [Variation(**v) for v in data.get("variations", [])]
                  rules = [
                      TargetingRule(
                          id=r["id"],
                          conditions=[Condition(**c) for c in r.get("conditions", [])],
                          variation_index=r["variation_index"],
                          priority=r.get("priority", 0)
                      )
                      for r in data.get("targeting_rules", [])
                  ]

                  rollout = None
                  if "percentage_rollout" in data:
                      rollout = PercentageRollout(**data["percentage_rollout"])

                  return FeatureFlag(
                      key=data["key"],
                      type=FlagType(data["type"]),
                      variations=variations,
                      default_variation=data["default_variation"],
                      targeting_rules=rules,
                      percentage_rollout=rollout,
                      enabled=data.get("enabled", True),
                      prerequisites=data.get("prerequisites", [])
                  )
          ```
      pitfalls:
      - SSE reconnection without backoff causes thundering herd
      - Stale cache served indefinitely when stream down
      - Large flag payloads slow down evaluation
      acceptance_criteria:
      - Push flag changes to connected SDKs within seconds of modification
      - Support streaming via SSE or WebSocket for low-latency flag updates
      - Handle SDK reconnection after network interruption with state recovery
      - Ensure consistency across all SDK instances receiving the same flag values
      deliverables:
      - Server-sent events or polling mechanism for flag change delivery
      - Flag change notification broadcasting updates to connected SDKs
      - Client SDK update handler applying new flag values without restart
      - Cache invalidation ensuring stale flag values are replaced promptly
    - name: Flag Analytics & Experiments
      description: Track flag evaluations for analytics and support A/B testing with statistical significance.
      hints:
        level1: Log each evaluation with context, variation, and timestamp.
        level2: Calculate conversion rates per variation, check statistical significance.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from datetime import datetime
          from typing import Optional
          import math
          from scipy import stats

          @dataclass
          class EvaluationEvent:
              flag_key: str
              variation_index: int
              user_id: str
              timestamp: datetime
              context_hash: str  # For deduplication

          @dataclass
          class ConversionEvent:
              experiment_key: str
              user_id: str
              metric_name: str
              metric_value: float
              timestamp: datetime

          @dataclass
          class ExperimentResults:
              control_conversions: int
              control_total: int
              treatment_conversions: int
              treatment_total: int

              @property
              def control_rate(self) -> float:
                  return self.control_conversions / self.control_total if self.control_total else 0

              @property
              def treatment_rate(self) -> float:
                  return self.treatment_conversions / self.treatment_total if self.treatment_total else 0

              @property
              def relative_lift(self) -> float:
                  if self.control_rate == 0:
                      return 0
                  return (self.treatment_rate - self.control_rate) / self.control_rate

              def is_significant(self, confidence: float = 0.95) -> bool:
                  """Check statistical significance using chi-squared test."""
                  observed = [
                      [self.control_conversions, self.control_total - self.control_conversions],
                      [self.treatment_conversions, self.treatment_total - self.treatment_conversions]
                  ]
                  chi2, p_value, _, _ = stats.chi2_contingency(observed)
                  return p_value < (1 - confidence)

              def confidence_interval(self, confidence: float = 0.95) -> tuple[float, float]:
                  """Calculate confidence interval for lift."""
                  # Use normal approximation
                  z = stats.norm.ppf(1 - (1 - confidence) / 2)

                  se_control = math.sqrt(self.control_rate * (1 - self.control_rate) / self.control_total) if self.control_total else 0
                  se_treatment = math.sqrt(self.treatment_rate * (1 - self.treatment_rate) / self.treatment_total) if self.treatment_total else 0

                  se_diff = math.sqrt(se_control**2 + se_treatment**2)
                  diff = self.treatment_rate - self.control_rate

                  return (diff - z * se_diff, diff + z * se_diff)

          class ExperimentAnalyzer:
              def __init__(self, storage):
                  self.storage = storage

              async def get_results(self, experiment_key: str, metric_name: str,
                                   start_time: datetime, end_time: datetime) -> ExperimentResults:
                  # Get users in each variation
                  control_users = await self.storage.get_users_in_variation(
                      experiment_key, 0, start_time, end_time
                  )
                  treatment_users = await self.storage.get_users_in_variation(
                      experiment_key, 1, start_time, end_time
                  )

                  # Get conversions
                  control_conversions = await self.storage.count_conversions(
                      experiment_key, metric_name, control_users, start_time, end_time
                  )
                  treatment_conversions = await self.storage.count_conversions(
                      experiment_key, metric_name, treatment_users, start_time, end_time
                  )

                  return ExperimentResults(
                      control_conversions=control_conversions,
                      control_total=len(control_users),
                      treatment_conversions=treatment_conversions,
                      treatment_total=len(treatment_users)
                  )

              def calculate_sample_size(self, baseline_rate: float, min_detectable_effect: float,
                                        power: float = 0.8, significance: float = 0.05) -> int:
                  """Calculate required sample size per variation."""
                  alpha = significance
                  beta = 1 - power

                  # Standard formula for two-proportion z-test
                  p1 = baseline_rate
                  p2 = baseline_rate * (1 + min_detectable_effect)
                  p_pooled = (p1 + p2) / 2

                  z_alpha = stats.norm.ppf(1 - alpha / 2)
                  z_beta = stats.norm.ppf(power)

                  n = (
                      (z_alpha * math.sqrt(2 * p_pooled * (1 - p_pooled)) +
                       z_beta * math.sqrt(p1 * (1 - p1) + p2 * (1 - p2)))
                      / (p2 - p1)
                  ) ** 2

                  return int(math.ceil(n))

          class FlagAnalytics:
              def __init__(self, storage):
                  self.storage = storage
                  self._buffer: list[EvaluationEvent] = []
                  self._buffer_size = 100
                  self._lock = asyncio.Lock()

              async def track_evaluation(self, result: EvaluationResult, context: EvaluationContext):
                  event = EvaluationEvent(
                      flag_key=result.flag_key,
                      variation_index=result.variation_index,
                      user_id=context.user_id,
                      timestamp=datetime.utcnow(),
                      context_hash=self._hash_context(context)
                  )

                  async with self._lock:
                      self._buffer.append(event)
                      if len(self._buffer) >= self._buffer_size:
                          await self._flush()

              async def track_conversion(self, experiment_key: str, user_id: str,
                                        metric_name: str, value: float = 1.0):
                  event = ConversionEvent(
                      experiment_key=experiment_key,
                      user_id=user_id,
                      metric_name=metric_name,
                      metric_value=value,
                      timestamp=datetime.utcnow()
                  )
                  await self.storage.store_conversion(event)

              async def _flush(self):
                  events = self._buffer
                  self._buffer = []
                  await self.storage.store_evaluations(events)

              def _hash_context(self, context: EvaluationContext) -> str:
                  import hashlib
                  import json
                  data = json.dumps({"user_id": context.user_id, **context.attributes}, sort_keys=True)
                  return hashlib.md5(data.encode()).hexdigest()
          ```
      pitfalls:
      - Peeking at results early inflates false positive rate
      - Sample ratio mismatch invalidates experiment
      - Novelty effect skews early results
      acceptance_criteria:
      - Track flag evaluations with user context for exposure analysis
      - Support A/B experiment metrics comparing variant performance over time
      - Calculate statistical significance using appropriate hypothesis testing methods
      - Generate experiment reports summarizing variant performance and confidence levels
      deliverables:
      - Flag exposure logging recording each flag evaluation with context
      - A/B test assignment linking users to experiment variants persistently
      - Metrics collection aggregating conversion and engagement by variant
      - Experiment analysis computing statistical significance of variant differences
  job-scheduler:
    name: Distributed Job Scheduler
    description: Build a distributed job scheduler with cron expressions, retries, priorities, and cluster coordination.
    why_important: Background job processing is essential for async workloads. Understanding schedulers helps design reliable
      batch processing systems.
    difficulty: advanced
    tags:
    - distributed-systems
    - backend
    - reliability
    estimated_hours: 45
    prerequisites:
    - redis-clone
    learning_outcomes:
    - Parse and evaluate cron expressions
    - Implement distributed locking for job claims
    - Design retry strategies with backoff
    - Handle worker failures and job recovery
    milestones:
    - name: Cron Expression Parser
      description: Parse cron expressions and calculate next execution times.
      hints:
        level1: 'Parse five fields: minute, hour, day, month, weekday.'
        level2: 'Handle special characters: *, /, -, , and named values.'
        level3: |-
          ```python
          from dataclasses import dataclass
          from datetime import datetime, timedelta
          from typing import Optional, Set
          import re

          @dataclass
          class CronField:
              values: Set[int]
              min_val: int
              max_val: int

              @classmethod
              def parse(cls, expr: str, min_val: int, max_val: int, names: dict = None) -> 'CronField':
                  values = set()

                  # Handle names (JAN, SUN, etc.)
                  if names:
                      for name, val in names.items():
                          expr = expr.upper().replace(name, str(val))

                  for part in expr.split(','):
                      if part == '*':
                          values.update(range(min_val, max_val + 1))
                      elif '/' in part:
                          # Step: */5 or 1-10/2
                          range_part, step = part.split('/')
                          step = int(step)
                          if range_part == '*':
                              start, end = min_val, max_val
                          elif '-' in range_part:
                              start, end = map(int, range_part.split('-'))
                          else:
                              start = int(range_part)
                              end = max_val
                          values.update(range(start, end + 1, step))
                      elif '-' in part:
                          # Range: 1-5
                          start, end = map(int, part.split('-'))
                          values.update(range(start, end + 1))
                      else:
                          values.add(int(part))

                  return cls(values=values, min_val=min_val, max_val=max_val)

          class CronExpression:
              MONTH_NAMES = {
                  'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4, 'MAY': 5, 'JUN': 6,
                  'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12
              }
              DOW_NAMES = {
                  'SUN': 0, 'MON': 1, 'TUE': 2, 'WED': 3, 'THU': 4, 'FRI': 5, 'SAT': 6
              }

              def __init__(self, expression: str):
                  self.expression = expression
                  parts = expression.split()
                  if len(parts) != 5:
                      raise ValueError(f"Invalid cron expression: {expression}")

                  self.minute = CronField.parse(parts[0], 0, 59)
                  self.hour = CronField.parse(parts[1], 0, 23)
                  self.day = CronField.parse(parts[2], 1, 31)
                  self.month = CronField.parse(parts[3], 1, 12, self.MONTH_NAMES)
                  self.dow = CronField.parse(parts[4], 0, 6, self.DOW_NAMES)

              def next_run(self, after: datetime = None) -> datetime:
                  """Calculate next execution time after given datetime."""
                  after = after or datetime.now()
                  # Start from next minute
                  current = after.replace(second=0, microsecond=0) + timedelta(minutes=1)

                  # Limit search to prevent infinite loops
                  for _ in range(366 * 24 * 60):  # Max 1 year of minutes
                      if self._matches(current):
                          return current
                      current += timedelta(minutes=1)

                  raise ValueError("No valid execution time found within 1 year")

              def _matches(self, dt: datetime) -> bool:
                  if dt.minute not in self.minute.values:
                      return False
                  if dt.hour not in self.hour.values:
                      return False
                  if dt.month not in self.month.values:
                      return False

                  # Day and DOW have OR relationship
                  day_match = dt.day in self.day.values
                  dow_match = dt.weekday() in self.dow.values

                  # If both are restricted (not *), either can match
                  day_restricted = self.day.values != set(range(1, 32))
                  dow_restricted = self.dow.values != set(range(0, 7))

                  if day_restricted and dow_restricted:
                      return day_match or dow_match
                  elif day_restricted:
                      return day_match
                  elif dow_restricted:
                      return dow_match
                  else:
                      return True

              def matches(self, dt: datetime) -> bool:
                  """Check if datetime matches this cron expression."""
                  return self._matches(dt)

              def next_n_runs(self, n: int, after: datetime = None) -> list[datetime]:
                  """Get next n execution times."""
                  runs = []
                  current = after or datetime.now()
                  for _ in range(n):
                      next_time = self.next_run(current)
                      runs.append(next_time)
                      current = next_time
                  return runs

          # Common presets
          CRON_PRESETS = {
              '@yearly': '0 0 1 1 *',
              '@monthly': '0 0 1 * *',
              '@weekly': '0 0 * * 0',
              '@daily': '0 0 * * *',
              '@hourly': '0 * * * *',
          }
          ```
      pitfalls:
      - Daylight saving time causes missed or duplicate runs
      - Day-of-month 31 skips months with fewer days
      - Infinite loop if no valid date exists
      acceptance_criteria:
      - Parser correctly interprets standard five-field cron expressions including wildcards, ranges, and step values
      - Next run time calculation returns the correct future timestamp for any valid cron expression
      - Extended cron syntax including seconds field and shorthand aliases like @daily and @hourly is supported
      - Timezone-aware scheduling converts cron times correctly across UTC and local timezones
      deliverables:
      - Cron field parser that handles minutes, hours, day-of-month, month, and day-of-week ranges
      - Next run time calculator that computes the nearest future execution from a cron expression
      - Cron expression validator that rejects malformed or out-of-range field values
      - Human-readable schedule description generator that converts cron expressions into plain English
    - name: Job Queue with Priorities
      description: Implement a job queue with priorities, delayed execution, and deduplication.
      hints:
        level1: Use sorted set for priority queue with score = priority + timestamp.
        level2: Support delayed jobs by storing with future timestamp.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from datetime import datetime, timedelta
          from enum import Enum
          from typing import Optional, Any
          import uuid
          import json
          import hashlib

          class JobStatus(str, Enum):
              PENDING = "pending"
              SCHEDULED = "scheduled"
              RUNNING = "running"
              COMPLETED = "completed"
              FAILED = "failed"
              DEAD = "dead"  # Max retries exceeded

          @dataclass
          class Job:
              id: str
              queue: str
              type: str
              payload: dict
              priority: int = 0  # Higher = more important
              status: JobStatus = JobStatus.PENDING
              created_at: datetime = field(default_factory=datetime.utcnow)
              scheduled_at: Optional[datetime] = None
              started_at: Optional[datetime] = None
              completed_at: Optional[datetime] = None
              attempts: int = 0
              max_retries: int = 3
              error: Optional[str] = None
              result: Any = None
              unique_key: Optional[str] = None  # For deduplication

              @classmethod
              def create(cls, queue: str, job_type: str, payload: dict,
                         priority: int = 0, delay: timedelta = None,
                         unique_key: str = None, max_retries: int = 3) -> 'Job':
                  job = cls(
                      id=str(uuid.uuid4()),
                      queue=queue,
                      type=job_type,
                      payload=payload,
                      priority=priority,
                      max_retries=max_retries,
                      unique_key=unique_key
                  )
                  if delay:
                      job.scheduled_at = datetime.utcnow() + delay
                      job.status = JobStatus.SCHEDULED
                  return job

              def score(self) -> float:
                  """Calculate priority score for sorted set."""
                  # Lower score = higher priority (processed first)
                  # Base: scheduled time (or created time)
                  base_time = (self.scheduled_at or self.created_at).timestamp()
                  # Subtract priority to make higher priority jobs come first
                  return base_time - (self.priority * 1000)

          class JobQueue:
              def __init__(self, redis):
                  self.redis = redis

              async def enqueue(self, job: Job) -> bool:
                  """Add job to queue. Returns False if duplicate."""
                  # Check for duplicate
                  if job.unique_key:
                      exists = await self.redis.exists(f"job:unique:{job.unique_key}")
                      if exists:
                          return False
                      # Set unique key with TTL
                      await self.redis.setex(
                          f"job:unique:{job.unique_key}",
                          86400,  # 24 hours
                          job.id
                      )

                  # Store job data
                  await self.redis.hset(f"job:{job.id}", mapping={
                      "data": json.dumps(self._serialize_job(job))
                  })

                  # Add to queue
                  if job.status == JobStatus.SCHEDULED:
                      await self.redis.zadd(f"queue:{job.queue}:scheduled", {job.id: job.score()})
                  else:
                      await self.redis.zadd(f"queue:{job.queue}:pending", {job.id: job.score()})

                  return True

              async def dequeue(self, queue: str, worker_id: str,
                               visibility_timeout: int = 300) -> Optional[Job]:
                  """Fetch next job from queue."""
                  # First, move any scheduled jobs that are due
                  await self._promote_scheduled(queue)

                  # Atomically pop job and add to processing set
                  job_id = await self._atomic_claim(queue, worker_id, visibility_timeout)
                  if not job_id:
                      return None

                  job = await self._get_job(job_id)
                  if job:
                      job.status = JobStatus.RUNNING
                      job.started_at = datetime.utcnow()
                      job.attempts += 1
                      await self._update_job(job)

                  return job

              async def complete(self, job: Job, result: Any = None):
                  """Mark job as completed."""
                  job.status = JobStatus.COMPLETED
                  job.completed_at = datetime.utcnow()
                  job.result = result

                  await self._update_job(job)
                  await self.redis.zrem(f"queue:{job.queue}:processing", job.id)

                  # Clean up unique key
                  if job.unique_key:
                      await self.redis.delete(f"job:unique:{job.unique_key}")

              async def fail(self, job: Job, error: str):
                  """Mark job as failed, possibly retry."""
                  job.error = error

                  if job.attempts < job.max_retries:
                      # Retry with exponential backoff
                      delay = timedelta(seconds=2 ** job.attempts * 60)
                      job.scheduled_at = datetime.utcnow() + delay
                      job.status = JobStatus.SCHEDULED

                      await self._update_job(job)
                      await self.redis.zrem(f"queue:{job.queue}:processing", job.id)
                      await self.redis.zadd(f"queue:{job.queue}:scheduled", {job.id: job.score()})
                  else:
                      job.status = JobStatus.DEAD
                      job.completed_at = datetime.utcnow()
                      await self._update_job(job)
                      await self.redis.zrem(f"queue:{job.queue}:processing", job.id)
                      await self.redis.zadd(f"queue:{job.queue}:dead", {job.id: job.score()})

              async def _promote_scheduled(self, queue: str):
                  """Move scheduled jobs that are due to pending."""
                  now = datetime.utcnow().timestamp()
                  due_jobs = await self.redis.zrangebyscore(
                      f"queue:{queue}:scheduled", 0, now, limit=100
                  )

                  for job_id in due_jobs:
                      job = await self._get_job(job_id)
                      if job:
                          job.status = JobStatus.PENDING
                          await self._update_job(job)
                          await self.redis.zrem(f"queue:{queue}:scheduled", job_id)
                          await self.redis.zadd(f"queue:{queue}:pending", {job_id: job.score()})

              async def _atomic_claim(self, queue: str, worker_id: str,
                                     timeout: int) -> Optional[str]:
                  # Use Lua script for atomic claim
                  script = """
                  local job_id = redis.call('ZRANGE', KEYS[1], 0, 0)[1]
                  if job_id then
                      redis.call('ZREM', KEYS[1], job_id)
                      redis.call('ZADD', KEYS[2], ARGV[1], job_id)
                      redis.call('HSET', 'job:' .. job_id, 'worker', ARGV[2])
                      return job_id
                  end
                  return nil
                  """
                  deadline = datetime.utcnow().timestamp() + timeout
                  return await self.redis.eval(
                      script,
                      2,
                      f"queue:{queue}:pending",
                      f"queue:{queue}:processing",
                      deadline,
                      worker_id
                  )
          ```
      pitfalls:
      - Race condition between claim and processing
      - Visibility timeout too short causes duplicate processing
      - Deduplication key collision across different job types
      acceptance_criteria:
      - Jobs enqueued with numeric priority levels are dequeued in strict priority order
      - Delayed jobs remain invisible until their scheduled time and then become eligible for dequeue
      - Duplicate job submissions with the same idempotency key are silently deduplicated
      - Jobs with an expired TTL are automatically removed from the queue before dequeue
      deliverables:
      - Priority queue implementation backed by a min-heap or sorted structure for job ordering
      - Job submission endpoint that accepts job payloads with priority level and scheduling metadata
      - Job dequeue mechanism that always returns the highest-priority pending job first
      - Delayed job support that holds jobs until their scheduled execution time arrives
    - name: Worker Coordination
      description: Coordinate multiple workers with leader election, heartbeats, and job recovery.
      hints:
        level1: Workers send heartbeats, leader monitors for failures.
        level2: Recover jobs from dead workers by checking processing timeout.
        level3: |-
          ```python
          import asyncio
          from dataclasses import dataclass, field
          from datetime import datetime, timedelta
          from typing import Callable, Optional
          import uuid

          @dataclass
          class Worker:
              id: str
              queues: list[str]
              last_heartbeat: datetime = field(default_factory=datetime.utcnow)
              current_job: Optional[str] = None
              processed: int = 0
              failed: int = 0

          class WorkerCoordinator:
              def __init__(self, redis, job_queue: JobQueue):
                  self.redis = redis
                  self.job_queue = job_queue
                  self.heartbeat_interval = 30
                  self.worker_timeout = 90
                  self._workers: dict[str, Worker] = {}
                  self._running = False
                  self._is_leader = False

              async def start_worker(self, queues: list[str], handlers: dict[str, Callable]):
                  """Start a worker process."""
                  worker = Worker(
                      id=str(uuid.uuid4()),
                      queues=queues
                  )
                  self._workers[worker.id] = worker

                  # Start background tasks
                  self._running = True
                  heartbeat_task = asyncio.create_task(self._heartbeat_loop(worker))
                  leader_task = asyncio.create_task(self._leader_election())
                  process_task = asyncio.create_task(self._process_loop(worker, handlers))

                  await asyncio.gather(heartbeat_task, leader_task, process_task)

              async def _heartbeat_loop(self, worker: Worker):
                  while self._running:
                      await self.redis.hset(
                          f"worker:{worker.id}",
                          mapping={
                              "last_heartbeat": datetime.utcnow().isoformat(),
                              "current_job": worker.current_job or "",
                              "processed": worker.processed,
                              "failed": worker.failed
                          }
                      )
                      await self.redis.expire(f"worker:{worker.id}", self.worker_timeout * 2)
                      await asyncio.sleep(self.heartbeat_interval)

              async def _leader_election(self):
                  """Try to become leader for cleanup tasks."""
                  leader_key = "scheduler:leader"
                  while self._running:
                      # Try to acquire leadership
                      acquired = await self.redis.set(
                          leader_key,
                          self._workers[list(self._workers.keys())[0]].id,
                          nx=True,
                          ex=self.heartbeat_interval * 2
                      )

                      if acquired:
                          self._is_leader = True
                          await self._leader_tasks()
                      else:
                          self._is_leader = False

                      await asyncio.sleep(self.heartbeat_interval)

              async def _leader_tasks(self):
                  """Tasks only leader performs."""
                  # Check for dead workers
                  await self._recover_dead_worker_jobs()
                  # Clean up old completed jobs
                  await self._cleanup_old_jobs()

              async def _recover_dead_worker_jobs(self):
                  """Recover jobs from workers that haven't sent heartbeat."""
                  # Get all workers
                  worker_keys = await self.redis.keys("worker:*")

                  for key in worker_keys:
                      worker_data = await self.redis.hgetall(key)
                      if not worker_data:
                          continue

                      last_heartbeat = datetime.fromisoformat(worker_data.get("last_heartbeat", ""))
                      if datetime.utcnow() - last_heartbeat > timedelta(seconds=self.worker_timeout):
                          # Worker is dead, recover its job
                          current_job = worker_data.get("current_job")
                          if current_job:
                              await self._requeue_job(current_job)

                          await self.redis.delete(key)

              async def _requeue_job(self, job_id: str):
                  """Put job back in pending queue."""
                  job = await self.job_queue._get_job(job_id)
                  if job and job.status == JobStatus.RUNNING:
                      job.status = JobStatus.PENDING
                      job.started_at = None
                      await self.job_queue._update_job(job)

                      # Move from processing to pending
                      for queue in job.queue:
                          await self.redis.zrem(f"queue:{queue}:processing", job_id)
                          await self.redis.zadd(f"queue:{queue}:pending", {job_id: job.score()})

              async def _process_loop(self, worker: Worker, handlers: dict[str, Callable]):
                  """Main processing loop."""
                  while self._running:
                      for queue in worker.queues:
                          job = await self.job_queue.dequeue(queue, worker.id)

                          if job:
                              worker.current_job = job.id
                              try:
                                  handler = handlers.get(job.type)
                                  if handler:
                                      result = await handler(job.payload)
                                      await self.job_queue.complete(job, result)
                                      worker.processed += 1
                                  else:
                                      await self.job_queue.fail(job, f"No handler for job type: {job.type}")
                                      worker.failed += 1
                              except Exception as e:
                                  await self.job_queue.fail(job, str(e))
                                  worker.failed += 1
                              finally:
                                  worker.current_job = None

                      await asyncio.sleep(1)  # Polling interval

              async def _cleanup_old_jobs(self, days: int = 7):
                  """Remove old completed/dead jobs."""
                  cutoff = (datetime.utcnow() - timedelta(days=days)).timestamp()
                  # Clean up each queue's completed/dead sets
                  queues = await self.redis.smembers("queues")
                  for queue in queues:
                      await self.redis.zremrangebyscore(f"queue:{queue}:completed", 0, cutoff)
                      await self.redis.zremrangebyscore(f"queue:{queue}:dead", 0, cutoff)
          ```
      pitfalls:
      - Leader election split-brain with network partition
      - Job recovered while still processing causes duplicate
      - Worker heartbeat failure during long job
      acceptance_criteria:
      - Jobs are distributed evenly to available workers based on current load and capacity
      - Job locking via lease mechanism prevents duplicate execution by multiple workers
      - Worker heartbeat failures within a configurable timeout trigger automatic job reassignment
      - Graceful worker shutdown completes in-progress jobs before deregistering the worker
      deliverables:
      - Worker registration service that tracks available workers with their capacity and capabilities
      - Job assignment logic that distributes jobs to workers based on availability and affinity
      - Worker heartbeat mechanism that periodically verifies each worker is alive and responsive
      - Failed worker handler that detects unresponsive workers and reassigns their in-progress jobs
  audit-logging:
    name: Audit Logging System
    description: Build an immutable audit log system for compliance with tamper detection, retention policies, and efficient
      querying.
    why_important: Audit logs are required for compliance (SOC2, HIPAA, GDPR). Understanding audit systems helps design secure,
      compliant applications.
    difficulty: intermediate
    tags:
    - security
    - compliance
    - backend
    estimated_hours: 30
    prerequisites:
    - log-aggregator
    learning_outcomes:
    - Design immutable append-only log storage
    - Implement hash chain for tamper detection
    - Build efficient audit log querying
    - Handle retention and archival policies
    milestones:
    - name: Audit Event Model
      description: Design audit event schema with actor, action, resource, and context information.
      hints:
        level1: Capture who did what to which resource and when.
        level2: 'Include request context: IP, user agent, session ID.'
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from datetime import datetime
          from enum import Enum
          from typing import Optional, Any
          import uuid
          import json

          class AuditAction(str, Enum):
              CREATE = "create"
              READ = "read"
              UPDATE = "update"
              DELETE = "delete"
              LOGIN = "login"
              LOGOUT = "logout"
              PERMISSION_CHANGE = "permission_change"
              EXPORT = "export"
              IMPORT = "import"

          class AuditOutcome(str, Enum):
              SUCCESS = "success"
              FAILURE = "failure"
              DENIED = "denied"

          @dataclass
          class Actor:
              id: str
              type: str  # user, service, system
              name: str
              email: Optional[str] = None
              roles: list[str] = field(default_factory=list)

          @dataclass
          class Resource:
              id: str
              type: str  # document, user, setting, etc.
              name: Optional[str] = None
              attributes: dict = field(default_factory=dict)

          @dataclass
          class RequestContext:
              ip_address: str
              user_agent: str
              session_id: Optional[str] = None
              request_id: str = ""
              correlation_id: Optional[str] = None
              geo_location: Optional[dict] = None

          @dataclass
          class AuditEvent:
              id: str
              timestamp: datetime
              actor: Actor
              action: AuditAction
              resource: Resource
              outcome: AuditOutcome
              context: RequestContext
              changes: Optional[dict] = None  # Before/after for updates
              metadata: dict = field(default_factory=dict)
              sequence: int = 0  # For ordering
              hash: str = ""  # Chain hash

              @classmethod
              def create(cls, actor: Actor, action: AuditAction, resource: Resource,
                         outcome: AuditOutcome, context: RequestContext,
                         changes: dict = None, metadata: dict = None) -> 'AuditEvent':
                  return cls(
                      id=str(uuid.uuid4()),
                      timestamp=datetime.utcnow(),
                      actor=actor,
                      action=action,
                      resource=resource,
                      outcome=outcome,
                      context=context,
                      changes=changes,
                      metadata=metadata or {}
                  )

              def to_dict(self) -> dict:
                  return {
                      "id": self.id,
                      "timestamp": self.timestamp.isoformat(),
                      "actor": {
                          "id": self.actor.id,
                          "type": self.actor.type,
                          "name": self.actor.name,
                          "email": self.actor.email,
                          "roles": self.actor.roles
                      },
                      "action": self.action.value,
                      "resource": {
                          "id": self.resource.id,
                          "type": self.resource.type,
                          "name": self.resource.name,
                          "attributes": self.resource.attributes
                      },
                      "outcome": self.outcome.value,
                      "context": {
                          "ip_address": self.context.ip_address,
                          "user_agent": self.context.user_agent,
                          "session_id": self.context.session_id,
                          "request_id": self.context.request_id
                      },
                      "changes": self.changes,
                      "metadata": self.metadata,
                      "sequence": self.sequence,
                      "hash": self.hash
                  }

              def compute_hash(self, previous_hash: str = "") -> str:
                  import hashlib
                  data = json.dumps({
                      "id": self.id,
                      "timestamp": self.timestamp.isoformat(),
                      "actor_id": self.actor.id,
                      "action": self.action.value,
                      "resource_id": self.resource.id,
                      "outcome": self.outcome.value,
                      "previous_hash": previous_hash
                  }, sort_keys=True)
                  return hashlib.sha256(data.encode()).hexdigest()

          class AuditEventBuilder:
              """Fluent builder for audit events."""

              def __init__(self):
                  self._actor = None
                  self._action = None
                  self._resource = None
                  self._outcome = AuditOutcome.SUCCESS
                  self._context = None
                  self._changes = None
                  self._metadata = {}

              def actor(self, id: str, type: str, name: str, **kwargs) -> 'AuditEventBuilder':
                  self._actor = Actor(id=id, type=type, name=name, **kwargs)
                  return self

              def action(self, action: AuditAction) -> 'AuditEventBuilder':
                  self._action = action
                  return self

              def resource(self, id: str, type: str, name: str = None, **kwargs) -> 'AuditEventBuilder':
                  self._resource = Resource(id=id, type=type, name=name, **kwargs)
                  return self

              def outcome(self, outcome: AuditOutcome) -> 'AuditEventBuilder':
                  self._outcome = outcome
                  return self

              def context(self, ip: str, user_agent: str, **kwargs) -> 'AuditEventBuilder':
                  self._context = RequestContext(ip_address=ip, user_agent=user_agent, **kwargs)
                  return self

              def changes(self, before: dict, after: dict) -> 'AuditEventBuilder':
                  self._changes = {"before": before, "after": after}
                  return self

              def meta(self, **kwargs) -> 'AuditEventBuilder':
                  self._metadata.update(kwargs)
                  return self

              def build(self) -> AuditEvent:
                  return AuditEvent.create(
                      actor=self._actor,
                      action=self._action,
                      resource=self._resource,
                      outcome=self._outcome,
                      context=self._context,
                      changes=self._changes,
                      metadata=self._metadata
                  )
          ```
      pitfalls:
      - PII in audit logs violates GDPR right to erasure
      - Missing actor context makes logs useless for investigation
      - Async logging loses context in distributed systems
      acceptance_criteria:
      - 'Schema defines required fields: actor, action, resource, timestamp, and outcome for every event'
      - Custom metadata fields can be attached to any event without modifying the core schema definition
      - Request context including client IP address and user agent string is captured in each audit event
      - Events are validated against the schema on creation and rejected if required fields are missing
      deliverables:
      - Event schema defining who (actor), what (action), when (timestamp), and where (resource) fields
      - Event serialization module that converts audit events to JSON and binary formats for storage
      - Actor and resource identification with unique IDs, types, and human-readable display names
      - Event metadata container supporting custom key-value pairs for domain-specific audit context
    - name: Immutable Storage with Hash Chain
      description: Implement append-only storage with hash chain linking for tamper detection.
      hints:
        level1: Each event hash includes hash of previous event.
        level2: Store chain anchors periodically for verification.
        level3: |-
          ```python
          import asyncio
          from dataclasses import dataclass
          from datetime import datetime
          from typing import Optional
          import hashlib

          @dataclass
          class ChainAnchor:
              sequence: int
              hash: str
              timestamp: datetime
              event_count: int

          class ImmutableAuditStore:
              def __init__(self, storage, anchor_interval: int = 1000):
                  self.storage = storage
                  self.anchor_interval = anchor_interval
                  self._last_hash = ""
                  self._sequence = 0
                  self._lock = asyncio.Lock()

              async def initialize(self):
                  """Load last hash and sequence from storage."""
                  last_event = await self.storage.get_last_event()
                  if last_event:
                      self._last_hash = last_event.hash
                      self._sequence = last_event.sequence

              async def append(self, event: AuditEvent) -> AuditEvent:
                  async with self._lock:
                      self._sequence += 1
                      event.sequence = self._sequence
                      event.hash = event.compute_hash(self._last_hash)

                      # Append to storage (immutable)
                      await self.storage.append(event)

                      self._last_hash = event.hash

                      # Create anchor periodically
                      if self._sequence % self.anchor_interval == 0:
                          await self._create_anchor()

                      return event

              async def _create_anchor(self):
                  anchor = ChainAnchor(
                      sequence=self._sequence,
                      hash=self._last_hash,
                      timestamp=datetime.utcnow(),
                      event_count=self._sequence
                  )
                  await self.storage.store_anchor(anchor)

              async def verify_chain(self, start_seq: int = 1, end_seq: int = None) -> bool:
                  """Verify hash chain integrity."""
                  end_seq = end_seq or self._sequence

                  events = await self.storage.get_range(start_seq, end_seq)

                  previous_hash = ""
                  if start_seq > 1:
                      prev_event = await self.storage.get_event(start_seq - 1)
                      previous_hash = prev_event.hash

                  for event in events:
                      expected_hash = event.compute_hash(previous_hash)
                      if event.hash != expected_hash:
                          return False
                      previous_hash = event.hash

                  return True

              async def verify_from_anchor(self, anchor: ChainAnchor) -> bool:
                  """Verify chain from anchor point."""
                  # Get events since anchor
                  events = await self.storage.get_range(anchor.sequence, self._sequence)

                  if not events:
                      return anchor.hash == self._last_hash

                  # First event should chain from anchor
                  first_event = events[0]
                  if first_event.sequence == anchor.sequence:
                      if first_event.hash != anchor.hash:
                          return False
                      events = events[1:]

                  return await self.verify_chain(anchor.sequence + 1, self._sequence)

              async def get_proof(self, event_id: str) -> dict:
                  """Generate proof of inclusion for an event."""
                  event = await self.storage.get_event_by_id(event_id)
                  if not event:
                      return None

                  # Find nearest anchor before event
                  anchor = await self.storage.get_nearest_anchor(event.sequence)

                  # Get chain from anchor to event
                  chain = await self.storage.get_range(
                      anchor.sequence if anchor else 1,
                      event.sequence
                  )

                  return {
                      "event": event.to_dict(),
                      "anchor": anchor,
                      "chain_hashes": [e.hash for e in chain],
                      "verified": await self.verify_chain(
                          anchor.sequence if anchor else 1,
                          event.sequence
                      )
                  }

          class PostgresAuditStorage:
              """PostgreSQL storage with append-only table."""

              def __init__(self, pool):
                  self.pool = pool

              async def append(self, event: AuditEvent):
                  async with self.pool.acquire() as conn:
                      await conn.execute("""
                          INSERT INTO audit_log (
                              id, sequence, timestamp, actor_id, actor_type, actor_name,
                              action, resource_id, resource_type, outcome,
                              context, changes, metadata, hash
                          ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
                      """,
                          event.id, event.sequence, event.timestamp,
                          event.actor.id, event.actor.type, event.actor.name,
                          event.action.value, event.resource.id, event.resource.type,
                          event.outcome.value, json.dumps(event.context.__dict__),
                          json.dumps(event.changes), json.dumps(event.metadata), event.hash
                      )

              async def get_range(self, start: int, end: int) -> list[AuditEvent]:
                  async with self.pool.acquire() as conn:
                      rows = await conn.fetch("""
                          SELECT * FROM audit_log
                          WHERE sequence >= $1 AND sequence <= $2
                          ORDER BY sequence
                      """, start, end)
                      return [self._row_to_event(row) for row in rows]
          ```
      pitfalls:
      - Hash chain breaks if events inserted out of order
      - Chain verification expensive on large datasets
      - Backup/restore must preserve hash chain integrity
      acceptance_criteria:
      - Events are appended to the log with a monotonically increasing sequence number for total ordering
      - Each entry's hash includes the previous entry's hash, forming a verifiable chain of integrity
      - Tampering with any entry is detected by recomputing and comparing the hash chain from that point forward
      - Log rotation creates a new segment while preserving the hash chain continuity across segment boundaries
      deliverables:
      - Append-only event storage that prevents modification or deletion of previously written records
      - Hash chain linking each event to the previous entry's hash for cryptographic tamper detection
      - Event ordering guarantees using monotonically increasing sequence numbers assigned at write time
      - Storage encryption that protects audit data at rest using AES-256 or equivalent symmetric encryption
    - name: Audit Query & Export
      description: Build efficient querying for audit logs with filtering, search, and compliance export formats.
      hints:
        level1: Index on actor, resource, action, and timestamp.
        level2: Support date range, actor, and resource filtering.
        level3: |-
          ```python
          from dataclasses import dataclass
          from datetime import datetime
          from typing import Optional
          from enum import Enum

          class ExportFormat(str, Enum):
              JSON = "json"
              CSV = "csv"
              SIEM = "siem"  # CEF/LEEF format

          @dataclass
          class AuditQuery:
              start_time: Optional[datetime] = None
              end_time: Optional[datetime] = None
              actor_ids: list[str] = None
              actor_types: list[str] = None
              actions: list[AuditAction] = None
              resource_ids: list[str] = None
              resource_types: list[str] = None
              outcomes: list[AuditOutcome] = None
              ip_addresses: list[str] = None
              search_text: Optional[str] = None
              limit: int = 100
              offset: int = 0

          class AuditQueryEngine:
              def __init__(self, storage):
                  self.storage = storage

              async def query(self, q: AuditQuery) -> list[AuditEvent]:
                  """Execute audit query."""
                  conditions = []
                  params = []
                  param_idx = 1

                  if q.start_time:
                      conditions.append(f"timestamp >= ${param_idx}")
                      params.append(q.start_time)
                      param_idx += 1

                  if q.end_time:
                      conditions.append(f"timestamp <= ${param_idx}")
                      params.append(q.end_time)
                      param_idx += 1

                  if q.actor_ids:
                      conditions.append(f"actor_id = ANY(${param_idx})")
                      params.append(q.actor_ids)
                      param_idx += 1

                  if q.actions:
                      conditions.append(f"action = ANY(${param_idx})")
                      params.append([a.value for a in q.actions])
                      param_idx += 1

                  if q.resource_types:
                      conditions.append(f"resource_type = ANY(${param_idx})")
                      params.append(q.resource_types)
                      param_idx += 1

                  if q.outcomes:
                      conditions.append(f"outcome = ANY(${param_idx})")
                      params.append([o.value for o in q.outcomes])
                      param_idx += 1

                  if q.search_text:
                      conditions.append(f"search_vector @@ plainto_tsquery(${param_idx})")
                      params.append(q.search_text)
                      param_idx += 1

                  where_clause = " AND ".join(conditions) if conditions else "TRUE"

                  query = f"""
                      SELECT * FROM audit_log
                      WHERE {where_clause}
                      ORDER BY timestamp DESC
                      LIMIT ${param_idx} OFFSET ${param_idx + 1}
                  """
                  params.extend([q.limit, q.offset])

                  rows = await self.storage.fetch(query, *params)
                  return [self._row_to_event(row) for row in rows]

              async def export(self, q: AuditQuery, format: ExportFormat) -> bytes:
                  """Export audit logs in specified format."""
                  events = await self.query(q)

                  if format == ExportFormat.JSON:
                      return self._export_json(events)
                  elif format == ExportFormat.CSV:
                      return self._export_csv(events)
                  elif format == ExportFormat.SIEM:
                      return self._export_cef(events)

              def _export_json(self, events: list[AuditEvent]) -> bytes:
                  import json
                  return json.dumps([e.to_dict() for e in events], indent=2).encode()

              def _export_csv(self, events: list[AuditEvent]) -> bytes:
                  import csv
                  import io

                  output = io.StringIO()
                  writer = csv.writer(output)

                  # Header
                  writer.writerow([
                      'timestamp', 'actor_id', 'actor_name', 'action',
                      'resource_type', 'resource_id', 'outcome', 'ip_address'
                  ])

                  for event in events:
                      writer.writerow([
                          event.timestamp.isoformat(),
                          event.actor.id,
                          event.actor.name,
                          event.action.value,
                          event.resource.type,
                          event.resource.id,
                          event.outcome.value,
                          event.context.ip_address
                      ])

                  return output.getvalue().encode()

              def _export_cef(self, events: list[AuditEvent]) -> bytes:
                  """Export in Common Event Format for SIEM integration."""
                  lines = []
                  for event in events:
                      # CEF:Version|Device Vendor|Device Product|Device Version|Signature ID|Name|Severity|Extension
                      severity = 3 if event.outcome == AuditOutcome.SUCCESS else 7
                      extension = (
                          f"src={event.context.ip_address} "
                          f"suser={event.actor.name} "
                          f"duser={event.resource.id} "
                          f"act={event.action.value} "
                          f"outcome={event.outcome.value}"
                      )
                      cef_line = (
                          f"CEF:0|MyApp|AuditLog|1.0|{event.action.value}|"
                          f"{event.action.value} {event.resource.type}|{severity}|{extension}"
                      )
                      lines.append(cef_line)

                  return "\n".join(lines).encode()

              async def get_activity_summary(self, actor_id: str,
                                             start: datetime, end: datetime) -> dict:
                  """Get activity summary for an actor."""
                  rows = await self.storage.fetch("""
                      SELECT action, outcome, COUNT(*) as count
                      FROM audit_log
                      WHERE actor_id = $1 AND timestamp BETWEEN $2 AND $3
                      GROUP BY action, outcome
                  """, actor_id, start, end)

                  return {
                      "actor_id": actor_id,
                      "period": {"start": start, "end": end},
                      "activity": [
                          {"action": r["action"], "outcome": r["outcome"], "count": r["count"]}
                          for r in rows
                      ]
                  }
          ```
      pitfalls:
      - Full text search without index causes table scan
      - Large exports exhaust memory (use streaming)
      - Time zone handling inconsistent in queries
      acceptance_criteria:
      - Queries filter events by actor, action, resource, and time range with sub-second response times
      - Pagination support returns results in configurable page sizes with stable cursor-based navigation
      - Export produces correctly formatted CSV and JSON files suitable for compliance audit submissions
      - Generated audit reports include summary statistics, event timelines, and per-actor activity breakdowns
      deliverables:
      - Time-range query engine that retrieves audit events between specified start and end timestamps
      - Actor and resource filtering that narrows results by actor ID, resource type, or action category
      - Full-text search capability that finds events matching keyword queries in action descriptions or metadata
      - Export module that produces compliance-ready reports in CSV, JSON, and PDF formats
  rate-limiter-distributed:
    name: Distributed Rate Limiter
    description: Build a distributed rate limiter supporting multiple algorithms with Redis backend for cluster-wide limiting.
    why_important: Rate limiting protects services from abuse and ensures fair resource usage. Understanding algorithms helps
      choose the right approach.
    difficulty: intermediate
    tags:
    - backend
    - distributed-systems
    - reliability
    estimated_hours: 25
    prerequisites:
    - redis-clone
    learning_outcomes:
    - Implement token bucket and sliding window algorithms
    - Design distributed rate limiting with Redis
    - Handle burst traffic gracefully
    - Build multi-tier rate limiting
    milestones:
    - name: Rate Limiting Algorithms
      description: Implement token bucket, sliding window log, and sliding window counter algorithms.
      hints:
        level1: 'Token bucket: refill tokens at fixed rate, consume on request.'
        level2: 'Sliding window: track requests in time window, count or log-based.'
        level3: |-
          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass
          from datetime import datetime
          import time
          import math

          @dataclass
          class RateLimitResult:
              allowed: bool
              remaining: int
              reset_at: float  # Unix timestamp
              retry_after: float = 0  # Seconds

          class RateLimiter(ABC):
              @abstractmethod
              async def is_allowed(self, key: str) -> RateLimitResult:
                  pass

          class TokenBucketLimiter(RateLimiter):
              def __init__(self, redis, capacity: int, refill_rate: float):
                  self.redis = redis
                  self.capacity = capacity
                  self.refill_rate = refill_rate  # tokens per second

              async def is_allowed(self, key: str) -> RateLimitResult:
                  now = time.time()
                  bucket_key = f"ratelimit:bucket:{key}"

                  # Lua script for atomic token bucket
                  script = """
                  local key = KEYS[1]
                  local capacity = tonumber(ARGV[1])
                  local refill_rate = tonumber(ARGV[2])
                  local now = tonumber(ARGV[3])

                  local bucket = redis.call('HMGET', key, 'tokens', 'last_refill')
                  local tokens = tonumber(bucket[1]) or capacity
                  local last_refill = tonumber(bucket[2]) or now

                  -- Refill tokens
                  local elapsed = now - last_refill
                  local refill = elapsed * refill_rate
                  tokens = math.min(capacity, tokens + refill)

                  -- Try to consume
                  local allowed = 0
                  if tokens >= 1 then
                      tokens = tokens - 1
                      allowed = 1
                  end

                  -- Save state
                  redis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)
                  redis.call('EXPIRE', key, math.ceil(capacity / refill_rate) + 1)

                  return {allowed, tokens, capacity}
                  """

                  result = await self.redis.eval(
                      script, 1, bucket_key,
                      self.capacity, self.refill_rate, now
                  )

                  allowed, remaining, capacity = result
                  reset_at = now + (capacity - remaining) / self.refill_rate

                  return RateLimitResult(
                      allowed=bool(allowed),
                      remaining=int(remaining),
                      reset_at=reset_at,
                      retry_after=0 if allowed else 1.0 / self.refill_rate
                  )

          class SlidingWindowLogLimiter(RateLimiter):
              """Precise but memory-intensive sliding window."""

              def __init__(self, redis, limit: int, window_seconds: int):
                  self.redis = redis
                  self.limit = limit
                  self.window = window_seconds

              async def is_allowed(self, key: str) -> RateLimitResult:
                  now = time.time()
                  window_start = now - self.window
                  log_key = f"ratelimit:log:{key}"

                  # Lua script for atomic sliding window
                  script = """
                  local key = KEYS[1]
                  local limit = tonumber(ARGV[1])
                  local window_start = tonumber(ARGV[2])
                  local now = tonumber(ARGV[3])
                  local window = tonumber(ARGV[4])

                  -- Remove old entries
                  redis.call('ZREMRANGEBYSCORE', key, 0, window_start)

                  -- Count current entries
                  local count = redis.call('ZCARD', key)

                  local allowed = 0
                  if count < limit then
                      -- Add new entry
                      redis.call('ZADD', key, now, now .. ':' .. math.random())
                      allowed = 1
                      count = count + 1
                  end

                  redis.call('EXPIRE', key, window + 1)

                  -- Get oldest entry for reset time
                  local oldest = redis.call('ZRANGE', key, 0, 0, 'WITHSCORES')
                  local reset_at = oldest[2] and (tonumber(oldest[2]) + window) or (now + window)

                  return {allowed, limit - count, reset_at}
                  """

                  result = await self.redis.eval(
                      script, 1, log_key,
                      self.limit, window_start, now, self.window
                  )

                  allowed, remaining, reset_at = result

                  return RateLimitResult(
                      allowed=bool(allowed),
                      remaining=max(0, int(remaining)),
                      reset_at=reset_at,
                      retry_after=0 if allowed else reset_at - now
                  )

          class SlidingWindowCounterLimiter(RateLimiter):
              """Memory-efficient approximate sliding window."""

              def __init__(self, redis, limit: int, window_seconds: int):
                  self.redis = redis
                  self.limit = limit
                  self.window = window_seconds

              async def is_allowed(self, key: str) -> RateLimitResult:
                  now = time.time()
                  current_window = int(now // self.window)
                  previous_window = current_window - 1

                  current_key = f"ratelimit:counter:{key}:{current_window}"
                  previous_key = f"ratelimit:counter:{key}:{previous_window}"

                  # Lua script for atomic counter
                  script = """
                  local current_key = KEYS[1]
                  local previous_key = KEYS[2]
                  local limit = tonumber(ARGV[1])
                  local window = tonumber(ARGV[2])
                  local now = tonumber(ARGV[3])
                  local current_window = tonumber(ARGV[4])

                  local previous_count = tonumber(redis.call('GET', previous_key)) or 0
                  local current_count = tonumber(redis.call('GET', current_key)) or 0

                  -- Calculate weighted count (sliding window approximation)
                  local window_start = current_window * window
                  local elapsed_ratio = (now - window_start) / window
                  local weighted_count = previous_count * (1 - elapsed_ratio) + current_count

                  local allowed = 0
                  if weighted_count < limit then
                      redis.call('INCR', current_key)
                      redis.call('EXPIRE', current_key, window * 2)
                      allowed = 1
                      weighted_count = weighted_count + 1
                  end

                  return {allowed, math.floor(limit - weighted_count), window_start + window}
                  """

                  result = await self.redis.eval(
                      script, 2, current_key, previous_key,
                      self.limit, self.window, now, current_window
                  )

                  allowed, remaining, reset_at = result

                  return RateLimitResult(
                      allowed=bool(allowed),
                      remaining=max(0, int(remaining)),
                      reset_at=reset_at,
                      retry_after=0 if allowed else reset_at - now
                  )
          ```
      pitfalls:
      - Token bucket allows burst above sustained rate
      - Sliding window log uses O(n) memory per key
      - Counter approximation can allow 2x limit at window boundary
      acceptance_criteria:
      - Implement sliding window counter handling boundary transitions
      - Support token bucket algorithm with burst and refill rate
      - Handle window boundary transitions without count loss
      - Configure rate limit threshold and time window size
      deliverables:
      - Token bucket implementation with configurable rate and capacity
      - Sliding window counter implementation for fixed-window smoothing
      - Leaky bucket option providing steady outflow rate
      - Algorithm comparison benchmark measuring accuracy and performance
    - name: Multi-tier Rate Limiting
      description: Implement hierarchical rate limiting with per-user, per-API, and global limits.
      hints:
        level1: 'Check limits in order: user -> API -> global.'
        level2: Use different algorithms for different tiers.
        level3: |-
          ```python
          from dataclasses import dataclass
          from typing import Optional
          from enum import Enum

          class LimitTier(str, Enum):
              USER = "user"
              API_KEY = "api_key"
              ENDPOINT = "endpoint"
              GLOBAL = "global"

          @dataclass
          class RateLimitConfig:
              tier: LimitTier
              limit: int
              window_seconds: int
              burst_limit: Optional[int] = None  # For token bucket

          @dataclass
          class MultiTierResult:
              allowed: bool
              tier_results: dict[LimitTier, RateLimitResult]
              limiting_tier: Optional[LimitTier] = None

          class MultiTierRateLimiter:
              def __init__(self, redis):
                  self.redis = redis
                  self.limiters: dict[LimitTier, tuple[RateLimitConfig, RateLimiter]] = {}

              def configure_tier(self, config: RateLimitConfig):
                  if config.burst_limit:
                      limiter = TokenBucketLimiter(
                          self.redis,
                          capacity=config.burst_limit,
                          refill_rate=config.limit / config.window_seconds
                      )
                  else:
                      limiter = SlidingWindowCounterLimiter(
                          self.redis,
                          limit=config.limit,
                          window_seconds=config.window_seconds
                      )
                  self.limiters[config.tier] = (config, limiter)

              async def is_allowed(self, context: dict) -> MultiTierResult:
                  """Check all configured rate limit tiers."""
                  tier_results = {}
                  limiting_tier = None

                  # Check tiers in priority order
                  tier_order = [LimitTier.USER, LimitTier.API_KEY, LimitTier.ENDPOINT, LimitTier.GLOBAL]

                  for tier in tier_order:
                      if tier not in self.limiters:
                          continue

                      config, limiter = self.limiters[tier]
                      key = self._build_key(tier, context)

                      result = await limiter.is_allowed(key)
                      tier_results[tier] = result

                      if not result.allowed:
                          limiting_tier = tier
                          break

                  return MultiTierResult(
                      allowed=limiting_tier is None,
                      tier_results=tier_results,
                      limiting_tier=limiting_tier
                  )

              def _build_key(self, tier: LimitTier, context: dict) -> str:
                  if tier == LimitTier.USER:
                      return f"user:{context.get('user_id', 'anonymous')}"
                  elif tier == LimitTier.API_KEY:
                      return f"apikey:{context.get('api_key', 'none')}"
                  elif tier == LimitTier.ENDPOINT:
                      return f"endpoint:{context.get('endpoint', 'unknown')}"
                  elif tier == LimitTier.GLOBAL:
                      return "global"
                  return "unknown"

          class AdaptiveRateLimiter:
              """Rate limiter that adjusts based on system load."""

              def __init__(self, redis, base_limit: int, window: int):
                  self.redis = redis
                  self.base_limit = base_limit
                  self.window = window
                  self.limiter = SlidingWindowCounterLimiter(redis, base_limit, window)
                  self._load_factor = 1.0

              async def update_load_factor(self, cpu_usage: float, error_rate: float):
                  """Adjust limits based on system health."""
                  # Reduce limit when system is stressed
                  if cpu_usage > 0.8 or error_rate > 0.05:
                      self._load_factor = max(0.5, self._load_factor - 0.1)
                  elif cpu_usage < 0.5 and error_rate < 0.01:
                      self._load_factor = min(1.5, self._load_factor + 0.1)

                  # Update limiter with adjusted limit
                  adjusted_limit = int(self.base_limit * self._load_factor)
                  self.limiter = SlidingWindowCounterLimiter(
                      self.redis, adjusted_limit, self.window
                  )

              async def is_allowed(self, key: str) -> RateLimitResult:
                  return await self.limiter.is_allowed(key)

          # Middleware for HTTP frameworks
          class RateLimitMiddleware:
              def __init__(self, limiter: MultiTierRateLimiter):
                  self.limiter = limiter

              async def __call__(self, request, call_next):
                  context = {
                      "user_id": request.user.id if request.user else None,
                      "api_key": request.headers.get("X-API-Key"),
                      "endpoint": f"{request.method}:{request.path}",
                      "ip": request.client.host
                  }

                  result = await self.limiter.is_allowed(context)

                  # Add rate limit headers
                  headers = {}
                  for tier, tier_result in result.tier_results.items():
                      headers[f"X-RateLimit-{tier.value}-Remaining"] = str(tier_result.remaining)
                      headers[f"X-RateLimit-{tier.value}-Reset"] = str(int(tier_result.reset_at))

                  if not result.allowed:
                      return Response(
                          status_code=429,
                          headers={
                              **headers,
                              "Retry-After": str(int(result.tier_results[result.limiting_tier].retry_after))
                          },
                          content={"error": "Rate limit exceeded", "tier": result.limiting_tier.value}
                      )

                  response = await call_next(request)
                  for key, value in headers.items():
                      response.headers[key] = value
                  return response
          ```
      pitfalls:
      - Checking all tiers even after one fails wastes resources
      - Different tier windows cause confusing UX
      - Adaptive limiting oscillates under variable load
      acceptance_criteria:
      - Support both per-user and per-IP rate limit tiers simultaneously
      - Implement global rate limits aggregating across all clients
      - Handle burst allowance permitting short spikes above steady rate
      - Support different rate limit thresholds per API endpoint
      deliverables:
      - Per-user rate limit tier with individual quotas
      - Per-IP rate limit tier for unauthenticated traffic
      - Global rate limit tier protecting overall system capacity
      - Quota management tracking usage across all tiers
  saga-orchestrator:
    name: Saga Orchestrator
    description: Build a saga orchestrator for managing distributed transactions with compensation and recovery.
    why_important: Sagas solve the distributed transaction problem in microservices. Understanding saga patterns helps design
      reliable distributed workflows.
    difficulty: advanced
    tags:
    - distributed-systems
    - microservices
    - reliability
    estimated_hours: 40
    prerequisites:
    - job-scheduler
    - event-sourcing
    learning_outcomes:
    - Design saga step definitions with compensations
    - Implement orchestration vs choreography patterns
    - Handle partial failures and rollback
    - Build saga state machine with persistence
    milestones:
    - name: Saga Definition
      description: Define saga steps with forward actions and compensation handlers.
      hints:
        level1: Each step has an action and a compensating action.
        level2: Steps can be sequential or parallel, with dependencies.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from typing import Callable, Any, Optional
          from enum import Enum
          import uuid

          class StepStatus(str, Enum):
              PENDING = "pending"
              RUNNING = "running"
              COMPLETED = "completed"
              FAILED = "failed"
              COMPENSATING = "compensating"
              COMPENSATED = "compensated"

          @dataclass
          class SagaStep:
              name: str
              action: Callable[[dict], Any]  # Forward action
              compensation: Callable[[dict, Any], None]  # Rollback action
              timeout: int = 300  # seconds
              retries: int = 3
              depends_on: list[str] = field(default_factory=list)

          @dataclass
          class StepExecution:
              step_name: str
              status: StepStatus = StepStatus.PENDING
              result: Any = None
              error: Optional[str] = None
              started_at: Optional[datetime] = None
              completed_at: Optional[datetime] = None
              attempts: int = 0

          @dataclass
          class SagaDefinition:
              name: str
              steps: list[SagaStep] = field(default_factory=list)

              def add_step(self, name: str, action: Callable, compensation: Callable,
                           depends_on: list[str] = None, **kwargs) -> 'SagaDefinition':
                  self.steps.append(SagaStep(
                      name=name,
                      action=action,
                      compensation=compensation,
                      depends_on=depends_on or [],
                      **kwargs
                  ))
                  return self

              def get_step(self, name: str) -> Optional[SagaStep]:
                  for step in self.steps:
                      if step.name == name:
                          return step
                  return None

              def get_execution_order(self) -> list[list[str]]:
                  """Return steps grouped by parallel execution levels."""
                  remaining = {s.name: set(s.depends_on) for s in self.steps}
                  levels = []

                  while remaining:
                      # Find steps with no pending dependencies
                      ready = [name for name, deps in remaining.items() if not deps]
                      if not ready:
                          raise ValueError("Circular dependency detected")

                      levels.append(ready)

                      # Remove completed steps from dependencies
                      for name in ready:
                          del remaining[name]
                      for deps in remaining.values():
                          deps -= set(ready)

                  return levels

          # Example saga definition
          def create_order_saga() -> SagaDefinition:
              saga = SagaDefinition(name="create_order")

              saga.add_step(
                  name="reserve_inventory",
                  action=lambda ctx: inventory_service.reserve(ctx["items"]),
                  compensation=lambda ctx, result: inventory_service.release(result["reservation_id"])
              )

              saga.add_step(
                  name="charge_payment",
                  action=lambda ctx: payment_service.charge(ctx["payment_info"], ctx["total"]),
                  compensation=lambda ctx, result: payment_service.refund(result["transaction_id"]),
                  depends_on=["reserve_inventory"]
              )

              saga.add_step(
                  name="create_shipment",
                  action=lambda ctx: shipping_service.create(ctx["address"], ctx["items"]),
                  compensation=lambda ctx, result: shipping_service.cancel(result["shipment_id"]),
                  depends_on=["charge_payment"]
              )

              saga.add_step(
                  name="send_confirmation",
                  action=lambda ctx: notification_service.send_order_confirmation(ctx["email"], ctx["order_id"]),
                  compensation=lambda ctx, result: None,  # No compensation needed
                  depends_on=["create_shipment"]
              )

              return saga
          ```
      pitfalls:
      - Compensation that fails leaves inconsistent state
      - Parallel steps with shared state cause race conditions
      - Timeout too short fails legitimate slow operations
      acceptance_criteria:
      - Define saga steps each with a compensating rollback action
      - Support step dependency ordering for sequential execution
      - Handle per-step timeout configuration with sensible defaults
      - Store saga definitions for reuse across multiple executions
      deliverables:
      - Step definition specifying action function and step metadata
      - Compensation definition pairing each step with its rollback action
      - Step ordering defining execution sequence and dependencies
      - Saga metadata including name, timeout, and retry configuration
    - name: Saga Orchestrator Engine
      description: Build the orchestrator that executes sagas, tracks state, and handles failures.
      hints:
        level1: Execute steps in dependency order, track results.
        level2: On failure, compensate completed steps in reverse order.
        level3: |-
          ```python
          from dataclasses import dataclass, field
          from datetime import datetime
          from typing import Optional, Any
          import asyncio

          class SagaStatus(str, Enum):
              PENDING = "pending"
              RUNNING = "running"
              COMPLETED = "completed"
              COMPENSATING = "compensating"
              COMPENSATED = "compensated"
              FAILED = "failed"  # Compensation also failed

          @dataclass
          class SagaExecution:
              id: str
              saga_name: str
              status: SagaStatus = SagaStatus.PENDING
              context: dict = field(default_factory=dict)
              step_executions: dict[str, StepExecution] = field(default_factory=dict)
              started_at: Optional[datetime] = None
              completed_at: Optional[datetime] = None
              error: Optional[str] = None

          class SagaOrchestrator:
              def __init__(self, storage, event_bus=None):
                  self.storage = storage
                  self.event_bus = event_bus
                  self.sagas: dict[str, SagaDefinition] = {}

              def register(self, saga: SagaDefinition):
                  self.sagas[saga.name] = saga

              async def execute(self, saga_name: str, context: dict) -> SagaExecution:
                  saga = self.sagas.get(saga_name)
                  if not saga:
                      raise ValueError(f"Unknown saga: {saga_name}")

                  execution = SagaExecution(
                      id=str(uuid.uuid4()),
                      saga_name=saga_name,
                      context=context,
                      started_at=datetime.utcnow()
                  )

                  # Initialize step executions
                  for step in saga.steps:
                      execution.step_executions[step.name] = StepExecution(step_name=step.name)

                  await self.storage.save(execution)

                  try:
                      await self._run_saga(saga, execution)
                      execution.status = SagaStatus.COMPLETED
                  except Exception as e:
                      execution.error = str(e)
                      await self._compensate(saga, execution)

                  execution.completed_at = datetime.utcnow()
                  await self.storage.save(execution)

                  return execution

              async def _run_saga(self, saga: SagaDefinition, execution: SagaExecution):
                  execution.status = SagaStatus.RUNNING
                  await self.storage.save(execution)

                  levels = saga.get_execution_order()

                  for level in levels:
                      # Execute parallel steps
                      tasks = [
                          self._execute_step(saga.get_step(name), execution)
                          for name in level
                      ]
                      results = await asyncio.gather(*tasks, return_exceptions=True)

                      # Check for failures
                      for name, result in zip(level, results):
                          if isinstance(result, Exception):
                              raise result

              async def _execute_step(self, step: SagaStep, execution: SagaExecution):
                  step_exec = execution.step_executions[step.name]
                  step_exec.status = StepStatus.RUNNING
                  step_exec.started_at = datetime.utcnow()
                  step_exec.attempts += 1

                  await self.storage.save(execution)

                  try:
                      # Execute with timeout
                      result = await asyncio.wait_for(
                          self._call_action(step.action, execution.context),
                          timeout=step.timeout
                      )

                      step_exec.result = result
                      step_exec.status = StepStatus.COMPLETED
                      step_exec.completed_at = datetime.utcnow()

                      # Update context with result
                      execution.context[f"{step.name}_result"] = result

                  except Exception as e:
                      step_exec.error = str(e)
                      step_exec.status = StepStatus.FAILED

                      # Retry if attempts remaining
                      if step_exec.attempts < step.retries:
                          await asyncio.sleep(2 ** step_exec.attempts)  # Exponential backoff
                          return await self._execute_step(step, execution)

                      raise

                  await self.storage.save(execution)

              async def _compensate(self, saga: SagaDefinition, execution: SagaExecution):
                  execution.status = SagaStatus.COMPENSATING
                  await self.storage.save(execution)

                  # Get completed steps in reverse order
                  levels = saga.get_execution_order()
                  completed_steps = []

                  for level in levels:
                      for name in level:
                          step_exec = execution.step_executions[name]
                          if step_exec.status == StepStatus.COMPLETED:
                              completed_steps.append(name)

                  # Compensate in reverse order
                  for step_name in reversed(completed_steps):
                      step = saga.get_step(step_name)
                      step_exec = execution.step_executions[step_name]

                      try:
                          step_exec.status = StepStatus.COMPENSATING
                          await self.storage.save(execution)

                          await self._call_action(
                              step.compensation,
                              execution.context,
                              step_exec.result
                          )

                          step_exec.status = StepStatus.COMPENSATED
                      except Exception as e:
                          step_exec.error = f"Compensation failed: {e}"
                          execution.status = SagaStatus.FAILED
                          await self.storage.save(execution)
                          raise

                  execution.status = SagaStatus.COMPENSATED
                  await self.storage.save(execution)

              async def _call_action(self, action: Callable, *args):
                  if asyncio.iscoroutinefunction(action):
                      return await action(*args)
                  return action(*args)

              async def resume(self, execution_id: str) -> SagaExecution:
                  """Resume a failed or interrupted saga."""
                  execution = await self.storage.load(execution_id)
                  saga = self.sagas.get(execution.saga_name)

                  if execution.status == SagaStatus.RUNNING:
                      # Find incomplete steps and continue
                      await self._run_saga(saga, execution)
                  elif execution.status == SagaStatus.COMPENSATING:
                      # Continue compensation
                      await self._compensate(saga, execution)

                  return execution
          ```
      pitfalls:
      - Resuming saga loses in-memory step results
      - Parallel compensation can conflict
      - Network partition causes duplicate saga execution
      acceptance_criteria:
      - Execute saga steps in defined sequence until completion or failure
      - Track completion status of each individual step
      - Trigger compensation chain on any step failure running rollbacks in reverse
      - Handle step execution timeouts killing or retrying long-running steps
      deliverables:
      - Saga execution engine running steps in defined sequence
      - Step invocation calling action function with input parameters
      - Failure handling detecting step errors and triggering compensation
      - Compensation execution running rollback actions in reverse order
    - name: Saga State Persistence
      description: Persist saga state for recovery and implement idempotent step execution.
      hints:
        level1: Store saga state after each step completion.
        level2: Use idempotency keys to prevent duplicate execution.
        level3: |-
          ```python
          from dataclasses import asdict
          import json

          class SagaStorage:
              def __init__(self, db):
                  self.db = db

              async def save(self, execution: SagaExecution):
                  await self.db.execute("""
                      INSERT INTO saga_executions (
                          id, saga_name, status, context, step_executions,
                          started_at, completed_at, error
                      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                      ON CONFLICT (id) DO UPDATE SET
                          status = $3, context = $4, step_executions = $5,
                          completed_at = $7, error = $8
                  """,
                      execution.id,
                      execution.saga_name,
                      execution.status.value,
                      json.dumps(execution.context),
                      json.dumps({k: asdict(v) for k, v in execution.step_executions.items()}),
                      execution.started_at,
                      execution.completed_at,
                      execution.error
                  )

              async def load(self, execution_id: str) -> SagaExecution:
                  row = await self.db.fetchrow(
                      "SELECT * FROM saga_executions WHERE id = $1",
                      execution_id
                  )
                  if not row:
                      raise ValueError(f"Saga execution not found: {execution_id}")

                  step_data = json.loads(row["step_executions"])
                  return SagaExecution(
                      id=row["id"],
                      saga_name=row["saga_name"],
                      status=SagaStatus(row["status"]),
                      context=json.loads(row["context"]),
                      step_executions={
                          k: StepExecution(**v) for k, v in step_data.items()
                      },
                      started_at=row["started_at"],
                      completed_at=row["completed_at"],
                      error=row["error"]
                  )

              async def list_pending(self, saga_name: str = None) -> list[SagaExecution]:
                  query = """
                      SELECT * FROM saga_executions
                      WHERE status IN ('pending', 'running', 'compensating')
                  """
                  if saga_name:
                      query += " AND saga_name = $1"
                      rows = await self.db.fetch(query, saga_name)
                  else:
                      rows = await self.db.fetch(query)

                  return [await self.load(row["id"]) for row in rows]

          class IdempotentStepExecutor:
              """Ensures steps are executed exactly once."""

              def __init__(self, storage, lock_manager):
                  self.storage = storage
                  self.lock_manager = lock_manager

              async def execute(self, saga_id: str, step_name: str,
                               action: Callable, *args) -> Any:
                  idempotency_key = f"saga:{saga_id}:step:{step_name}"

                  # Check if already executed
                  existing = await self.storage.get_step_result(idempotency_key)
                  if existing:
                      return existing

                  # Acquire lock
                  async with self.lock_manager.lock(idempotency_key, timeout=300):
                      # Double-check after acquiring lock
                      existing = await self.storage.get_step_result(idempotency_key)
                      if existing:
                          return existing

                      # Execute
                      result = await action(*args)

                      # Store result
                      await self.storage.store_step_result(idempotency_key, result)

                      return result

          class SagaRecoveryService:
              """Background service that recovers interrupted sagas."""

              def __init__(self, orchestrator: SagaOrchestrator, storage: SagaStorage):
                  self.orchestrator = orchestrator
                  self.storage = storage
                  self.check_interval = 60  # seconds

              async def start(self):
                  while True:
                      await self._recover_pending()
                      await asyncio.sleep(self.check_interval)

              async def _recover_pending(self):
                  pending = await self.storage.list_pending()

                  for execution in pending:
                      # Check if saga is actually stuck (no progress for a while)
                      last_update = self._get_last_update(execution)
                      if datetime.utcnow() - last_update > timedelta(minutes=5):
                          try:
                              await self.orchestrator.resume(execution.id)
                          except Exception as e:
                              print(f"Failed to recover saga {execution.id}: {e}")

              def _get_last_update(self, execution: SagaExecution) -> datetime:
                  times = [execution.started_at]
                  for step_exec in execution.step_executions.values():
                      if step_exec.started_at:
                          times.append(step_exec.started_at)
                      if step_exec.completed_at:
                          times.append(step_exec.completed_at)
                  return max(t for t in times if t)
          ```
      pitfalls:
      - JSON serialization loses datetime precision
      - Lock timeout shorter than step timeout causes issues
      - Recovery loop processes same saga repeatedly
      acceptance_criteria:
      - Persist saga execution state to durable storage after each step
      - Support saga recovery after orchestrator process crash or restart
      - Track compensation progress separately from forward execution state
      - Handle idempotent step execution preventing duplicate side effects
      deliverables:
      - Saga instance state record storing current execution progress
      - Step completion tracking recording success or failure per step
      - Crash recovery logic resuming saga execution from last known state
      - Idempotency mechanism preventing duplicate step execution on retry
  oauth2-provider:
    name: OAuth2/OIDC Provider
    description: Build a complete OAuth2 and OpenID Connect identity provider supporting authorization code flow, PKCE, refresh
      tokens, and JWT access tokens.
    why_expert: Identity is the foundation of security. Understanding OAuth2/OIDC internals helps debug auth issues, design
      secure systems, and integrate with any identity provider.
    difficulty: expert
    tags:
    - security
    - authentication
    - identity
    - oauth2
    - jwt
    estimated_hours: 50
    prerequisites:
    - build-http-server
    milestones:
    - name: Client Registration & Authorization Endpoint
      description: Implement client registration and the authorization endpoint with PKCE support
      skills:
      - OAuth2 flows
      - PKCE
      - Cryptographic challenges
      hints:
        level1: Start with client_id/client_secret storage and the /authorize endpoint
        level2: 'PKCE: client generates code_verifier, sends SHA256(code_verifier) as code_challenge'
        level3: |2

          ```python
          import hashlib
          import base64
          import secrets
          from dataclasses import dataclass
          from typing import Optional
          import time

          @dataclass
          class OAuthClient:
              client_id: str
              client_secret_hash: str
              redirect_uris: list[str]
              grant_types: list[str]  # authorization_code, refresh_token, client_credentials
              scopes: list[str]

          class AuthorizationServer:
              def __init__(self):
                  self.clients: dict[str, OAuthClient] = {}
                  self.authorization_codes: dict[str, dict] = {}  # code -> {client_id, user_id, scopes, code_challenge, expires_at}
                  self.refresh_tokens: dict[str, dict] = {}

              def register_client(self, redirect_uris: list[str], grant_types: list[str]) -> tuple[str, str]:
                  client_id = secrets.token_urlsafe(16)
                  client_secret = secrets.token_urlsafe(32)
                  client_secret_hash = hashlib.sha256(client_secret.encode()).hexdigest()

                  self.clients[client_id] = OAuthClient(
                      client_id=client_id,
                      client_secret_hash=client_secret_hash,
                      redirect_uris=redirect_uris,
                      grant_types=grant_types,
                      scopes=["openid", "profile", "email"]
                  )
                  return client_id, client_secret

              def authorize(self, client_id: str, redirect_uri: str, response_type: str,
                            scope: str, state: str, code_challenge: Optional[str] = None,
                            code_challenge_method: Optional[str] = None, user_id: str = None) -> str:
                  client = self.clients.get(client_id)
                  if not client:
                      raise ValueError("Invalid client_id")
                  if redirect_uri not in client.redirect_uris:
                      raise ValueError("Invalid redirect_uri")
                  if response_type != "code":
                      raise ValueError("Only authorization_code flow supported")

                  # Generate authorization code
                  code = secrets.token_urlsafe(32)
                  self.authorization_codes[code] = {
                      "client_id": client_id,
                      "user_id": user_id,
                      "redirect_uri": redirect_uri,
                      "scopes": scope.split(),
                      "code_challenge": code_challenge,
                      "code_challenge_method": code_challenge_method,
                      "expires_at": time.time() + 600  # 10 minutes
                  }

                  return f"{redirect_uri}?code={code}&state={state}"

              def verify_pkce(self, code_verifier: str, code_challenge: str, method: str) -> bool:
                  if method == "S256":
                      computed = base64.urlsafe_b64encode(
                          hashlib.sha256(code_verifier.encode()).digest()
                      ).rstrip(b'=').decode()
                      return secrets.compare_digest(computed, code_challenge)
                  elif method == "plain":
                      return secrets.compare_digest(code_verifier, code_challenge)
                  return False
          ```
      pitfalls:
      - Authorization codes must be single-use and short-lived (10 min max)
      - Always validate redirect_uri exactly - no partial matches
      - PKCE is mandatory for public clients (mobile, SPA)
      - State parameter prevents CSRF - must be unpredictable
      acceptance_criteria:
      - OAuth clients are registered with a generated client_id, hashed client_secret, and configured redirect URIs
      - Authorization consent page displays the client name, requested scopes, and allow/deny buttons to the user
      - Authorization codes are generated as cryptographically random strings bound to the client and redirect URI
      - Redirect URI validation rejects authorization requests whose redirect URI does not match the registered value
      deliverables:
      - Client registration API that creates OAuth client records with client_id, client_secret, and redirect URIs
      - Client credentials storage that hashes client secrets and stores grant types and allowed scopes per client
      - Authorization endpoint that initiates the authorization code flow by validating parameters and redirecting to consent
      - User consent handler that displays the requested scopes and captures the user's allow or deny decision
    - name: Token Endpoint & JWT Generation
      description: Implement the token endpoint with JWT access tokens and refresh token rotation
      skills:
      - JWT signing
      - Token rotation
      - Secure token storage
      hints:
        level1: Token endpoint exchanges authorization code for access_token + refresh_token
        level2: Use RS256 (asymmetric) for JWTs so resource servers can verify without shared secret
        level3: |2

          ```python
          import jwt
          from cryptography.hazmat.primitives import serialization
          from cryptography.hazmat.primitives.asymmetric import rsa
          from datetime import datetime, timedelta

          class TokenService:
              def __init__(self):
                  # Generate RSA key pair for JWT signing
                  self.private_key = rsa.generate_private_key(
                      public_exponent=65537,
                      key_size=2048
                  )
                  self.public_key = self.private_key.public_key()
                  self.issuer = "https://auth.example.com"

              def get_jwks(self) -> dict:
                  '''Return JWKS for token verification'''
                  from cryptography.hazmat.primitives.serialization import Encoding, PublicFormat
                  import json

                  public_bytes = self.public_key.public_bytes(
                      Encoding.PEM, PublicFormat.SubjectPublicKeyInfo
                  )
                  # Convert to JWK format
                  numbers = self.public_key.public_numbers()

                  def int_to_base64(n: int) -> str:
                      length = (n.bit_length() + 7) // 8
                      return base64.urlsafe_b64encode(n.to_bytes(length, 'big')).rstrip(b'=').decode()

                  return {
                      "keys": [{
                          "kty": "RSA",
                          "use": "sig",
                          "alg": "RS256",
                          "kid": "key-1",
                          "n": int_to_base64(numbers.n),
                          "e": int_to_base64(numbers.e)
                      }]
                  }

              def create_access_token(self, user_id: str, client_id: str, scopes: list[str]) -> str:
                  now = datetime.utcnow()
                  payload = {
                      "iss": self.issuer,
                      "sub": user_id,
                      "aud": client_id,
                      "exp": now + timedelta(hours=1),
                      "iat": now,
                      "scope": " ".join(scopes),
                      "jti": secrets.token_urlsafe(16)  # Unique token ID
                  }

                  private_pem = self.private_key.private_bytes(
                      encoding=serialization.Encoding.PEM,
                      format=serialization.PrivateFormat.PKCS8,
                      encryption_algorithm=serialization.NoEncryption()
                  )

                  return jwt.encode(payload, private_pem, algorithm="RS256", headers={"kid": "key-1"})

              def create_id_token(self, user_id: str, client_id: str, nonce: str, user_info: dict) -> str:
                  '''OpenID Connect ID Token'''
                  now = datetime.utcnow()
                  payload = {
                      "iss": self.issuer,
                      "sub": user_id,
                      "aud": client_id,
                      "exp": now + timedelta(hours=1),
                      "iat": now,
                      "auth_time": int(now.timestamp()),
                      "nonce": nonce,
                      **user_info  # name, email, etc.
                  }

                  private_pem = self.private_key.private_bytes(
                      encoding=serialization.Encoding.PEM,
                      format=serialization.PrivateFormat.PKCS8,
                      encryption_algorithm=serialization.NoEncryption()
                  )

                  return jwt.encode(payload, private_pem, algorithm="RS256", headers={"kid": "key-1"})

              def create_refresh_token(self) -> str:
                  return secrets.token_urlsafe(32)
          ```
      pitfalls:
      - Never include sensitive data in JWT payload - it's base64, not encrypted
      - 'Refresh token rotation: issue new refresh token on each use, invalidate old one'
      - Access tokens should be short-lived (15 min - 1 hour)
      - Always use constant-time comparison for token validation
      acceptance_criteria:
      - Authorization code is exchanged for an access token and refresh token after client authentication
      - Access tokens are signed JWTs containing sub, iss, aud, exp, iat, and scope claims
      - Client credentials grant issues an access token directly to the client without user involvement
      - Token expiration is enforced so expired access tokens are rejected by resource servers on validation
      deliverables:
      - Token endpoint that exchanges an authorization code for access and refresh tokens via HTTP POST
      - Authorization code exchanger that validates the code, client credentials, and redirect URI before issuing tokens
      - JWT token generator that creates signed access tokens with configurable claims and expiration times
      - Refresh token issuer that generates long-lived refresh tokens and stores them for later exchange
    - name: Token Introspection & Revocation
      description: Implement RFC 7662 token introspection and RFC 7009 token revocation
      skills:
      - Token lifecycle
      - Revocation strategies
      - Cache invalidation
      hints:
        level1: Introspection lets resource servers validate opaque tokens
        level2: Revocation must handle both access tokens and refresh tokens
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          import time

          @dataclass
          class TokenMetadata:
              active: bool
              client_id: str
              username: str
              scope: str
              sub: str
              aud: str
              iss: str
              exp: int
              iat: int
              token_type: str = "Bearer"

          class TokenManager:
              def __init__(self, token_service: TokenService):
                  self.token_service = token_service
                  self.revoked_tokens: set[str] = set()  # Set of revoked JTIs
                  self.refresh_tokens: dict[str, dict] = {}  # token -> metadata
                  self.token_families: dict[str, str] = {}  # refresh_token -> family_id

              def introspect(self, token: str, token_type_hint: Optional[str] = None) -> dict:
                  '''RFC 7662 - Token Introspection'''
                  # Check if it's a JWT access token
                  try:
                      # Verify JWT signature
                      public_pem = self.token_service.public_key.public_bytes(
                          encoding=serialization.Encoding.PEM,
                          format=serialization.PublicFormat.SubjectPublicKeyInfo
                      )
                      payload = jwt.decode(token, public_pem, algorithms=["RS256"],
                                         options={"verify_aud": False})

                      # Check if revoked
                      if payload.get("jti") in self.revoked_tokens:
                          return {"active": False}

                      # Check expiration
                      if payload["exp"] < time.time():
                          return {"active": False}

                      return {
                          "active": True,
                          "client_id": payload.get("aud"),
                          "username": payload.get("sub"),
                          "scope": payload.get("scope"),
                          "sub": payload.get("sub"),
                          "aud": payload.get("aud"),
                          "iss": payload.get("iss"),
                          "exp": payload.get("exp"),
                          "iat": payload.get("iat"),
                          "token_type": "Bearer"
                      }
                  except jwt.InvalidTokenError:
                      pass

                  # Check refresh tokens
                  if token in self.refresh_tokens:
                      meta = self.refresh_tokens[token]
                      if meta["expires_at"] > time.time():
                          return {"active": True, **meta}

                  return {"active": False}

              def revoke(self, token: str, token_type_hint: Optional[str] = None) -> bool:
                  '''RFC 7009 - Token Revocation'''
                  # Try to decode as JWT to get JTI
                  try:
                      # Decode without verification to get JTI
                      payload = jwt.decode(token, options={"verify_signature": False})
                      if "jti" in payload:
                          self.revoked_tokens.add(payload["jti"])
                          return True
                  except:
                      pass

                  # Check if it's a refresh token
                  if token in self.refresh_tokens:
                      # Revoke entire token family (prevents refresh token reuse attacks)
                      family_id = self.token_families.get(token)
                      if family_id:
                          tokens_to_revoke = [t for t, fid in self.token_families.items() if fid == family_id]
                          for t in tokens_to_revoke:
                              del self.refresh_tokens[t]
                              del self.token_families[t]
                      else:
                          del self.refresh_tokens[token]
                      return True

                  return False
          ```
      pitfalls:
      - Introspection endpoint must be protected (client authentication required)
      - Revoked JWTs need tracking until expiry (use jti claim)
      - 'Refresh token families: if old token reused, revoke entire family (theft detection)'
      - Consider using Redis for revocation list with TTL matching token expiry
      acceptance_criteria:
      - Introspection endpoint returns active=true with token metadata for valid tokens per RFC 7662 format
      - Revocation endpoint accepts access or refresh tokens and immediately marks them as inactive per RFC 7009
      - Resource servers can validate tokens by checking JWT signature, expiration, and required claims locally
      - Revoked tokens are immediately rejected on subsequent introspection or validation attempts
      deliverables:
      - Token introspection endpoint that returns the active status and metadata of a presented token per RFC 7662
      - Token revocation endpoint that immediately invalidates an access or refresh token per RFC 7009
      - Token validation library that verifies JWT signature, expiration, and claims for resource server use
      - Token lifecycle manager that tracks token creation, usage, expiration, and revocation events
    - name: UserInfo Endpoint & Consent Management
      description: Implement OIDC UserInfo endpoint and user consent flows for scope approval
      skills:
      - OIDC compliance
      - Consent UX
      - Scope management
      hints:
        level1: UserInfo returns claims based on granted scopes (profile, email, address, phone)
        level2: Store user consents per client - don't re-ask for already-granted scopes
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          from enum import Enum

          class Scope(Enum):
              OPENID = "openid"
              PROFILE = "profile"  # name, family_name, given_name, picture, etc.
              EMAIL = "email"      # email, email_verified
              ADDRESS = "address"  # address claim
              PHONE = "phone"      # phone_number, phone_number_verified

          @dataclass
          class UserConsent:
              user_id: str
              client_id: str
              granted_scopes: set[str]
              granted_at: float

          class UserInfoService:
              # Standard OIDC claims per scope
              SCOPE_CLAIMS = {
                  "profile": ["name", "family_name", "given_name", "middle_name", "nickname",
                             "preferred_username", "profile", "picture", "website", "gender",
                             "birthdate", "zoneinfo", "locale", "updated_at"],
                  "email": ["email", "email_verified"],
                  "address": ["address"],
                  "phone": ["phone_number", "phone_number_verified"]
              }

              def __init__(self):
                  self.user_consents: dict[tuple[str, str], UserConsent] = {}  # (user_id, client_id) -> consent
                  self.user_profiles: dict[str, dict] = {}  # user_id -> profile data

              def get_userinfo(self, access_token: str, token_manager: TokenManager) -> dict:
                  '''OIDC UserInfo Endpoint'''
                  introspection = token_manager.introspect(access_token)
                  if not introspection.get("active"):
                      raise ValueError("Invalid or expired token")

                  user_id = introspection["sub"]
                  scopes = introspection.get("scope", "").split()

                  if "openid" not in scopes:
                      raise ValueError("openid scope required")

                  profile = self.user_profiles.get(user_id, {})
                  claims = {"sub": user_id}

                  for scope in scopes:
                      if scope in self.SCOPE_CLAIMS:
                          for claim in self.SCOPE_CLAIMS[scope]:
                              if claim in profile:
                                  claims[claim] = profile[claim]

                  return claims

              def check_consent(self, user_id: str, client_id: str, requested_scopes: set[str]) -> set[str]:
                  '''Check which scopes need user consent'''
                  key = (user_id, client_id)
                  if key not in self.user_consents:
                      return requested_scopes  # All scopes need consent

                  existing = self.user_consents[key].granted_scopes
                  return requested_scopes - existing  # Return scopes needing consent

              def record_consent(self, user_id: str, client_id: str, scopes: set[str]):
                  '''Record user's consent for scopes'''
                  key = (user_id, client_id)
                  if key in self.user_consents:
                      self.user_consents[key].granted_scopes.update(scopes)
                  else:
                      self.user_consents[key] = UserConsent(
                          user_id=user_id,
                          client_id=client_id,
                          granted_scopes=scopes,
                          granted_at=time.time()
                      )

              def revoke_consent(self, user_id: str, client_id: str):
                  '''Allow user to revoke consent for a client'''
                  key = (user_id, client_id)
                  if key in self.user_consents:
                      del self.user_consents[key]
                      # Also revoke all tokens for this user/client pair
                      return True
                  return False
          ```
      pitfalls:
      - UserInfo must use access token, not ID token
      - Only return claims for scopes actually granted, not requested
      - Consent screen must clearly show what data will be shared
      - Allow users to revoke consent and see all authorized applications
      acceptance_criteria:
      - UserInfo endpoint returns user claims such as name, email, and profile based on the granted scopes
      - Previously granted consent decisions are stored so the user is not prompted again for the same client and scopes
      - Users can revoke previously granted consent causing future authorization requests to re-prompt for approval
      - Scope-based claims filtering returns only the user fields that the granted scopes authorize access to
      deliverables:
      - UserInfo endpoint that returns the authenticated user's profile claims as JSON based on granted scopes
      - Scope-to-claims mapping that determines which user profile fields are returned for each requested scope
      - Consent screen UI that displays requested scopes with descriptions and records the user's decision
      - Consent persistence store that saves the user's grant decisions so repeat authorizations skip the consent prompt
  rbac-system:
    name: RBAC/ABAC Authorization System
    description: Build a flexible authorization system supporting Role-Based Access Control (RBAC) and Attribute-Based Access
      Control (ABAC) with policy evaluation.
    why_expert: Authorization logic is often scattered and inconsistent. A centralized policy engine prevents security bugs
      and simplifies auditing.
    difficulty: expert
    tags:
    - security
    - authorization
    - rbac
    - abac
    - policy
    estimated_hours: 40
    prerequisites: []
    milestones:
    - name: Role & Permission Model
      description: Implement hierarchical roles with permission inheritance and efficient lookup
      skills:
      - Role hierarchies
      - Permission modeling
      - Graph traversal
      hints:
        level1: Roles can inherit from other roles - use DAG (no cycles)
        level2: Denormalize effective permissions for O(1) lookup at runtime
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          import time

          class Action(Enum):
              CREATE = "create"
              READ = "read"
              UPDATE = "update"
              DELETE = "delete"
              EXECUTE = "execute"
              ADMIN = "*"  # All actions

          @dataclass
          class Permission:
              resource: str      # e.g., "documents", "users", "reports"
              action: Action
              conditions: Optional[dict] = None  # For ABAC conditions

              def __hash__(self):
                  return hash((self.resource, self.action))

              def matches(self, resource: str, action: Action) -> bool:
                  if self.action == Action.ADMIN:
                      return self.resource == resource or self.resource == "*"
                  return self.resource == resource and self.action == action

          @dataclass
          class Role:
              name: str
              permissions: set[Permission] = field(default_factory=set)
              parent_roles: set[str] = field(default_factory=set)  # Inheritance

          class RBACEngine:
              def __init__(self):
                  self.roles: dict[str, Role] = {}
                  self.user_roles: dict[str, set[str]] = {}  # user_id -> role names
                  self._effective_permissions_cache: dict[str, set[Permission]] = {}

              def create_role(self, name: str, permissions: list[Permission],
                              parent_roles: list[str] = None) -> Role:
                  # Validate no cycles
                  if parent_roles:
                      for parent in parent_roles:
                          if self._would_create_cycle(name, parent):
                              raise ValueError(f"Role inheritance would create cycle: {name} -> {parent}")

                  role = Role(
                      name=name,
                      permissions=set(permissions),
                      parent_roles=set(parent_roles) if parent_roles else set()
                  )
                  self.roles[name] = role
                  self._invalidate_cache()
                  return role

              def _would_create_cycle(self, new_role: str, parent: str, visited: set = None) -> bool:
                  if visited is None:
                      visited = set()
                  if parent == new_role:
                      return True
                  if parent in visited:
                      return False
                  visited.add(parent)

                  parent_role = self.roles.get(parent)
                  if not parent_role:
                      return False
                  for grandparent in parent_role.parent_roles:
                      if self._would_create_cycle(new_role, grandparent, visited):
                          return True
                  return False

              def get_effective_permissions(self, role_name: str) -> set[Permission]:
                  if role_name in self._effective_permissions_cache:
                      return self._effective_permissions_cache[role_name]

                  role = self.roles.get(role_name)
                  if not role:
                      return set()

                  # Start with direct permissions
                  effective = set(role.permissions)

                  # Add inherited permissions (BFS)
                  visited = {role_name}
                  queue = list(role.parent_roles)

                  while queue:
                      parent_name = queue.pop(0)
                      if parent_name in visited:
                          continue
                      visited.add(parent_name)

                      parent = self.roles.get(parent_name)
                      if parent:
                          effective.update(parent.permissions)
                          queue.extend(parent.parent_roles)

                  self._effective_permissions_cache[role_name] = effective
                  return effective

              def assign_role(self, user_id: str, role_name: str):
                  if role_name not in self.roles:
                      raise ValueError(f"Role not found: {role_name}")
                  if user_id not in self.user_roles:
                      self.user_roles[user_id] = set()
                  self.user_roles[user_id].add(role_name)

              def check_permission(self, user_id: str, resource: str, action: Action) -> bool:
                  user_role_names = self.user_roles.get(user_id, set())

                  for role_name in user_role_names:
                      permissions = self.get_effective_permissions(role_name)
                      for perm in permissions:
                          if perm.matches(resource, action):
                              return True
                  return False

              def _invalidate_cache(self):
                  self._effective_permissions_cache.clear()
          ```
      pitfalls:
      - Role inheritance cycles cause infinite loops - validate DAG property
      - Cache invalidation needed when roles change - use versioning
      - Wildcard permissions (*) need careful handling to avoid over-granting
      - 'Role explosion: too many fine-grained roles become unmanageable'
      acceptance_criteria:
      - Define roles with named permission sets attached
      - Support role hierarchy with permission inheritance via DAG
      - Assign one or more roles to individual users
      - Handle role templates for creating preconfigured role bundles
      deliverables:
      - Role definition with name and permission set
      - Permission definition with resource and action pair
      - Role-to-permission mapping supporting many-to-many relationships
      - User-to-role assignment with multiple roles per user
    - name: ABAC Policy Engine
      description: Extend to attribute-based policies with conditions on user/resource/environment attributes
      skills:
      - Policy languages
      - Condition evaluation
      - Context propagation
      hints:
        level1: ABAC evaluates conditions like 'user.department == resource.owner_department'
        level2: 'Policies combine: allow if ANY policy allows, deny if ANY policy denies (deny wins)'
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Any, Callable
          from enum import Enum
          import operator

          class Effect(Enum):
              ALLOW = "allow"
              DENY = "deny"

          class Operator(Enum):
              EQUALS = "eq"
              NOT_EQUALS = "neq"
              GREATER_THAN = "gt"
              LESS_THAN = "lt"
              IN = "in"
              NOT_IN = "not_in"
              CONTAINS = "contains"
              MATCHES = "matches"  # Regex

          @dataclass
          class Condition:
              attribute: str      # e.g., "user.department", "resource.classification"
              operator: Operator
              value: Any

              def evaluate(self, context: dict) -> bool:
                  # Navigate dot notation: "user.department" -> context["user"]["department"]
                  attr_value = self._get_nested(context, self.attribute)
                  if attr_value is None:
                      return False

                  ops = {
                      Operator.EQUALS: lambda a, b: a == b,
                      Operator.NOT_EQUALS: lambda a, b: a != b,
                      Operator.GREATER_THAN: lambda a, b: a > b,
                      Operator.LESS_THAN: lambda a, b: a < b,
                      Operator.IN: lambda a, b: a in b,
                      Operator.NOT_IN: lambda a, b: a not in b,
                      Operator.CONTAINS: lambda a, b: b in a,
                  }

                  return ops[self.operator](attr_value, self.value)

              def _get_nested(self, obj: dict, path: str) -> Any:
                  parts = path.split(".")
                  current = obj
                  for part in parts:
                      if isinstance(current, dict):
                          current = current.get(part)
                      else:
                          return None
                      if current is None:
                          return None
                  return current

          @dataclass
          class Policy:
              id: str
              effect: Effect
              resources: list[str]    # Patterns like "documents/*", "users/{self}"
              actions: list[str]
              conditions: list[Condition] = field(default_factory=list)
              priority: int = 0       # Higher priority evaluated first

              def matches(self, resource: str, action: str, context: dict) -> tuple[bool, Effect]:
                  # Check resource pattern
                  if not self._matches_resource(resource, context):
                      return False, None

                  # Check action
                  if action not in self.actions and "*" not in self.actions:
                      return False, None

                  # Evaluate all conditions (AND logic)
                  for condition in self.conditions:
                      if not condition.evaluate(context):
                          return False, None

                  return True, self.effect

              def _matches_resource(self, resource: str, context: dict) -> bool:
                  for pattern in self.resources:
                      # Handle {self} placeholder
                      if "{self}" in pattern:
                          user_id = context.get("user", {}).get("id", "")
                          pattern = pattern.replace("{self}", user_id)

                      # Simple glob matching
                      if pattern == "*" or pattern == resource:
                          return True
                      if pattern.endswith("/*"):
                          prefix = pattern[:-2]
                          if resource.startswith(prefix):
                              return True
                  return False

          class ABACEngine:
              def __init__(self):
                  self.policies: list[Policy] = []

              def add_policy(self, policy: Policy):
                  self.policies.append(policy)
                  # Keep sorted by priority (descending)
                  self.policies.sort(key=lambda p: p.priority, reverse=True)

              def evaluate(self, resource: str, action: str, context: dict) -> bool:
                  '''
                  Evaluate all policies. Logic:
                  1. If any DENY policy matches -> deny
                  2. If any ALLOW policy matches -> allow
                  3. Default deny
                  '''
                  allow_matched = False

                  for policy in self.policies:
                      matches, effect = policy.matches(resource, action, context)
                      if matches:
                          if effect == Effect.DENY:
                              return False  # Deny wins immediately
                          elif effect == Effect.ALLOW:
                              allow_matched = True

                  return allow_matched  # Default deny if no allow matched

          # Example usage:
          # policy = Policy(
          #     id="owner-full-access",
          #     effect=Effect.ALLOW,
          #     resources=["documents/*"],
          #     actions=["*"],
          #     conditions=[
          #         Condition("user.id", Operator.EQUALS, "resource.owner_id")
          #     ]
          # )
          ```
      pitfalls:
      - 'Deny-by-default: no matching policy = deny'
      - Explicit deny always wins over allow (principle of least privilege)
      - Context must include all attributes needed for evaluation
      - Policy ordering matters - define clear priority rules
      acceptance_criteria:
      - Define policies using user, resource, and environment attributes
      - Evaluate attribute conditions against runtime request context
      - Support policy combining logic with AND and OR operators
      - Handle policy priority and ordering with deny-overrides strategy
      deliverables:
      - Attribute-based access rules with condition expressions
      - Policy definition language specifying effect, resources, and conditions
      - Policy evaluation engine processing rules against request context
      - Context attribute propagation passing user, resource, and environment data
    - name: Resource-Based & Multi-tenancy
      description: Implement resource-level permissions and tenant isolation for SaaS applications
      skills:
      - Multi-tenancy
      - Resource ownership
      - Tenant isolation
      hints:
        level1: Every resource belongs to a tenant; users can only access resources in their tenant
        level2: Cross-tenant access requires explicit sharing with capability-based tokens
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          import secrets

          class ShareLevel(Enum):
              VIEWER = "viewer"    # Read-only
              EDITOR = "editor"    # Read + Write
              ADMIN = "admin"      # Read + Write + Share + Delete

          @dataclass
          class Resource:
              id: str
              tenant_id: str
              owner_id: str
              resource_type: str
              shares: dict[str, ShareLevel] = field(default_factory=dict)  # user_id -> level

          @dataclass
          class ShareLink:
              token: str
              resource_id: str
              level: ShareLevel
              expires_at: Optional[float] = None
              max_uses: Optional[int] = None
              uses: int = 0

          class MultiTenantAuthz:
              def __init__(self, rbac_engine: RBACEngine, abac_engine: ABACEngine):
                  self.rbac = rbac_engine
                  self.abac = abac_engine
                  self.resources: dict[str, Resource] = {}
                  self.share_links: dict[str, ShareLink] = {}
                  self.user_tenants: dict[str, str] = {}  # user_id -> tenant_id

              def check_access(self, user_id: str, resource_id: str, action: str) -> bool:
                  resource = self.resources.get(resource_id)
                  if not resource:
                      return False

                  user_tenant = self.user_tenants.get(user_id)

                  # 1. Check tenant isolation
                  if user_tenant != resource.tenant_id:
                      # Cross-tenant - only allowed via explicit share
                      return self._check_share(user_id, resource, action)

                  # 2. Check if owner (owners have full access)
                  if resource.owner_id == user_id:
                      return True

                  # 3. Check direct share
                  if self._check_share(user_id, resource, action):
                      return True

                  # 4. Check RBAC (tenant-scoped roles)
                  context = {
                      "user": {"id": user_id, "tenant_id": user_tenant},
                      "resource": {
                          "id": resource_id,
                          "tenant_id": resource.tenant_id,
                          "owner_id": resource.owner_id,
                          "type": resource.resource_type
                      }
                  }

                  if self.abac.evaluate(f"{resource.resource_type}/{resource_id}", action, context):
                      return True

                  return False

              def _check_share(self, user_id: str, resource: Resource, action: str) -> bool:
                  share_level = resource.shares.get(user_id)
                  if not share_level:
                      return False

                  action_requirements = {
                      "read": [ShareLevel.VIEWER, ShareLevel.EDITOR, ShareLevel.ADMIN],
                      "write": [ShareLevel.EDITOR, ShareLevel.ADMIN],
                      "delete": [ShareLevel.ADMIN],
                      "share": [ShareLevel.ADMIN]
                  }

                  required = action_requirements.get(action, [ShareLevel.ADMIN])
                  return share_level in required

              def create_share_link(self, user_id: str, resource_id: str,
                                    level: ShareLevel, expires_in: int = None,
                                    max_uses: int = None) -> str:
                  # Verify user can share
                  if not self.check_access(user_id, resource_id, "share"):
                      raise PermissionError("Cannot share this resource")

                  token = secrets.token_urlsafe(32)
                  expires_at = time.time() + expires_in if expires_in else None

                  self.share_links[token] = ShareLink(
                      token=token,
                      resource_id=resource_id,
                      level=level,
                      expires_at=expires_at,
                      max_uses=max_uses
                  )

                  return token

              def redeem_share_link(self, token: str, user_id: str) -> bool:
                  link = self.share_links.get(token)
                  if not link:
                      return False

                  # Check expiry
                  if link.expires_at and time.time() > link.expires_at:
                      del self.share_links[token]
                      return False

                  # Check uses
                  if link.max_uses and link.uses >= link.max_uses:
                      return False

                  # Grant access
                  resource = self.resources.get(link.resource_id)
                  if resource:
                      resource.shares[user_id] = link.level
                      link.uses += 1
                      return True

                  return False
          ```
      pitfalls:
      - Tenant isolation must be enforced at database query level too
      - Admin roles should be tenant-scoped, not global
      - Share links need expiry and use limits to prevent abuse
      - Revoking share should be immediate - no caching of permissions
      acceptance_criteria:
      - Support fine-grained permissions on individual resources
      - Enforce tenant isolation preventing cross-tenant data access
      - Implement resource ownership granting full control to creators
      - Support controlled cross-tenant admin access with explicit grants
      deliverables:
      - Resource ownership model tracking creator and tenant
      - Tenant isolation enforcing data separation between organizations
      - Cross-tenant permission grants via explicit sharing mechanism
      - Resource hierarchy supporting nested resource access inheritance
    - name: Audit Logging & Policy Testing
      description: Implement comprehensive audit logging and policy simulation for testing
      skills:
      - Security auditing
      - Policy testing
      - Compliance
      hints:
        level1: Log every authorization decision with full context for forensics
        level2: Policy simulation lets admins test 'what if' scenarios before deploying
        level3: |2

          ```python
          from dataclasses import dataclass, asdict
          from typing import Optional
          from datetime import datetime
          import json

          @dataclass
          class AuthzDecision:
              timestamp: datetime
              user_id: str
              resource: str
              action: str
              decision: bool
              policies_evaluated: list[str]
              matching_policy: Optional[str]
              context: dict
              latency_ms: float

          class AuthzAuditLog:
              def __init__(self):
                  self.decisions: list[AuthzDecision] = []

              def log_decision(self, decision: AuthzDecision):
                  self.decisions.append(decision)
                  # In production: send to SIEM, write to immutable log
                  print(json.dumps({
                      "type": "authz_decision",
                      "timestamp": decision.timestamp.isoformat(),
                      "user": decision.user_id,
                      "resource": decision.resource,
                      "action": decision.action,
                      "allowed": decision.decision,
                      "policy": decision.matching_policy
                  }))

              def query(self, user_id: str = None, resource: str = None,
                        action: str = None, decision: bool = None,
                        start_time: datetime = None, end_time: datetime = None) -> list[AuthzDecision]:
                  results = self.decisions

                  if user_id:
                      results = [d for d in results if d.user_id == user_id]
                  if resource:
                      results = [d for d in results if resource in d.resource]
                  if action:
                      results = [d for d in results if d.action == action]
                  if decision is not None:
                      results = [d for d in results if d.decision == decision]
                  if start_time:
                      results = [d for d in results if d.timestamp >= start_time]
                  if end_time:
                      results = [d for d in results if d.timestamp <= end_time]

                  return results

          class PolicySimulator:
              def __init__(self, abac_engine: ABACEngine):
                  self.abac = abac_engine

              def simulate(self, policies: list[Policy], test_cases: list[dict]) -> list[dict]:
                  '''
                  Run policies against test cases without affecting production.

                  test_cases format:
                  [
                      {
                          "description": "Owner can read own document",
                          "resource": "documents/123",
                          "action": "read",
                          "context": {"user": {"id": "user1"}, "resource": {"owner_id": "user1"}},
                          "expected": True
                      }
                  ]
                  '''
                  # Create isolated engine for simulation
                  sim_engine = ABACEngine()
                  for policy in policies:
                      sim_engine.add_policy(policy)

                  results = []
                  for case in test_cases:
                      actual = sim_engine.evaluate(
                          case["resource"],
                          case["action"],
                          case["context"]
                      )

                      results.append({
                          "description": case["description"],
                          "passed": actual == case["expected"],
                          "expected": case["expected"],
                          "actual": actual,
                          "resource": case["resource"],
                          "action": case["action"]
                      })

                  return results

              def find_violations(self, policies: list[Policy],
                                  security_invariants: list[dict]) -> list[dict]:
                  '''
                  Check policies against security invariants.

                  invariants format:
                  [
                      {
                          "description": "No user can delete audit logs",
                          "resource_pattern": "audit-logs/*",
                          "action": "delete",
                          "should_be": "denied",
                          "for_any_context": True
                      }
                  ]
                  '''
                  violations = []

                  for invariant in security_invariants:
                      # Generate test contexts
                      test_contexts = self._generate_test_contexts(invariant)

                      for context in test_contexts:
                          result = self.simulate(policies, [{
                              "description": invariant["description"],
                              "resource": invariant["resource_pattern"].replace("*", "test"),
                              "action": invariant["action"],
                              "context": context,
                              "expected": invariant["should_be"] == "denied"
                          }])

                          if not result[0]["passed"]:
                              violations.append({
                                  "invariant": invariant["description"],
                                  "context": context,
                                  "expected": invariant["should_be"],
                                  "actual": "allowed" if result[0]["actual"] else "denied"
                              })

                  return violations

              def _generate_test_contexts(self, invariant: dict) -> list[dict]:
                  # Generate various contexts to test the invariant
                  return [
                      {"user": {"id": "admin", "role": "admin"}},
                      {"user": {"id": "user", "role": "user"}},
                      {"user": {"id": "owner", "role": "owner"}, "resource": {"owner_id": "owner"}},
                  ]
          ```
      pitfalls:
      - Audit logs must be immutable - use append-only storage
      - Include enough context to understand decision without leaking secrets
      - Simulation environment must match production exactly
      - Test negative cases (should be denied) not just positive
      acceptance_criteria:
      - Log every access control decision with user, resource, and outcome
      - Support policy simulation testing changes before production deployment
      - Track all permission and role changes with timestamp and actor
      - Generate access audit reports for compliance review
      deliverables:
      - Access decision logging capturing every authorization check result
      - Policy simulation environment for testing rule changes safely
      - Automated test suite validating policy behavior against expected outcomes
      - Compliance reporting generating access audit summaries
  secret-management:
    name: Secret Management System
    description: Build a Vault-like secret management system with encryption, access control, dynamic secrets, and audit logging.
    why_expert: Hardcoded secrets cause breaches. Understanding secret management helps design secure systems and properly
      integrate with existing solutions.
    difficulty: expert
    tags:
    - security
    - secrets
    - encryption
    - vault
    - infrastructure
    estimated_hours: 45
    prerequisites:
    - build-http-server
    milestones:
    - name: Encrypted Secret Storage
      description: Implement envelope encryption with master key and data encryption keys
      skills:
      - Envelope encryption
      - Key management
      - Secure storage
      hints:
        level1: Master Key encrypts Data Encryption Keys (DEKs). DEKs encrypt actual secrets.
        level2: Never store master key with encrypted data. Use HSM or external KMS in production.
        level3: |2

          ```python
          from cryptography.fernet import Fernet
          from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
          from cryptography.hazmat.primitives import hashes
          from cryptography.hazmat.backends import default_backend
          import base64
          import os
          import json
          from dataclasses import dataclass
          from typing import Optional
          import time

          @dataclass
          class EncryptedSecret:
              encrypted_dek: bytes      # DEK encrypted with master key
              encrypted_value: bytes    # Value encrypted with DEK
              created_at: float
              version: int
              metadata: dict

          class SecretStore:
              def __init__(self, master_key: bytes = None):
                  # In production: master key from HSM, Kubernetes secret, or cloud KMS
                  if master_key:
                      self.master_key = master_key
                  else:
                      # Derive from password for demo (use proper KMS in prod!)
                      self.master_key = self._derive_key(b"demo-password", b"salt")

                  self.master_fernet = Fernet(base64.urlsafe_b64encode(self.master_key))
                  self.secrets: dict[str, list[EncryptedSecret]] = {}  # path -> versions

              def _derive_key(self, password: bytes, salt: bytes) -> bytes:
                  kdf = PBKDF2HMAC(
                      algorithm=hashes.SHA256(),
                      length=32,
                      salt=salt,
                      iterations=480000,
                      backend=default_backend()
                  )
                  return kdf.derive(password)

              def _generate_dek(self) -> tuple[bytes, Fernet]:
                  '''Generate new Data Encryption Key'''
                  dek = Fernet.generate_key()
                  return dek, Fernet(dek)

              def put(self, path: str, value: str, metadata: dict = None) -> int:
                  '''Store a secret with envelope encryption'''
                  # Generate new DEK for this secret
                  dek, dek_fernet = self._generate_dek()

                  # Encrypt value with DEK
                  encrypted_value = dek_fernet.encrypt(value.encode())

                  # Encrypt DEK with master key
                  encrypted_dek = self.master_fernet.encrypt(dek)

                  # Determine version
                  existing = self.secrets.get(path, [])
                  version = len(existing) + 1

                  secret = EncryptedSecret(
                      encrypted_dek=encrypted_dek,
                      encrypted_value=encrypted_value,
                      created_at=time.time(),
                      version=version,
                      metadata=metadata or {}
                  )

                  if path not in self.secrets:
                      self.secrets[path] = []
                  self.secrets[path].append(secret)

                  return version

              def get(self, path: str, version: int = None) -> Optional[str]:
                  '''Retrieve and decrypt a secret'''
                  versions = self.secrets.get(path)
                  if not versions:
                      return None

                  # Get requested version or latest
                  if version:
                      if version < 1 or version > len(versions):
                          return None
                      secret = versions[version - 1]
                  else:
                      secret = versions[-1]

                  # Decrypt DEK with master key
                  dek = self.master_fernet.decrypt(secret.encrypted_dek)
                  dek_fernet = Fernet(dek)

                  # Decrypt value with DEK
                  value = dek_fernet.decrypt(secret.encrypted_value)
                  return value.decode()

              def rotate_master_key(self, new_master_key: bytes):
                  '''Re-encrypt all DEKs with new master key'''
                  new_master_fernet = Fernet(base64.urlsafe_b64encode(new_master_key))

                  for path, versions in self.secrets.items():
                      for secret in versions:
                          # Decrypt DEK with old key
                          dek = self.master_fernet.decrypt(secret.encrypted_dek)
                          # Re-encrypt with new key
                          secret.encrypted_dek = new_master_fernet.encrypt(dek)

                  self.master_key = new_master_key
                  self.master_fernet = new_master_fernet
          ```
      pitfalls:
      - Master key in memory can be extracted - consider memory encryption
      - Secret versioning needed for rotation without breaking consumers
      - Backup encrypted data AND keys separately (but both needed for recovery)
      - 'Key derivation: use high iteration count (480k+) for PBKDF2'
      acceptance_criteria:
      - Store all secrets encrypted at rest in the backend storage
      - Use AES-256-GCM authenticated encryption for secret values
      - Support secret versioning allowing retrieval of previous values
      - Handle encryption key rotation re-encrypting secrets with new key
      deliverables:
      - Encryption at rest using AES-256-GCM for all stored secrets
      - Master key handling with secure generation and storage
      - Secret versioning maintaining history of value changes
      - Key rotation mechanism replacing encryption keys without downtime
    - name: Access Policies & Authentication
      description: Implement path-based ACLs and multiple authentication methods (token, AppRole, mTLS)
      skills:
      - Path-based ACLs
      - Authentication methods
      - Token management
      hints:
        level1: Policies define what paths a token can access with what capabilities
        level2: 'AppRole: role_id (public) + secret_id (private) = token for apps'
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          import secrets
          import hashlib
          import time
          import fnmatch

          class Capability(Enum):
              CREATE = "create"
              READ = "read"
              UPDATE = "update"
              DELETE = "delete"
              LIST = "list"
              SUDO = "sudo"     # Override policies
              DENY = "deny"     # Explicit deny

          @dataclass
          class Policy:
              name: str
              path_rules: dict[str, set[Capability]]  # path pattern -> capabilities

              def check(self, path: str, capability: Capability) -> bool:
                  for pattern, caps in self.path_rules.items():
                      if fnmatch.fnmatch(path, pattern):
                          if Capability.DENY in caps:
                              return False
                          if capability in caps or Capability.SUDO in caps:
                              return True
                  return False

          @dataclass
          class Token:
              id: str
              policies: list[str]
              created_at: float
              expires_at: Optional[float]
              renewable: bool
              metadata: dict = field(default_factory=dict)

          @dataclass
          class AppRole:
              role_id: str
              secret_id_hash: str
              policies: list[str]
              token_ttl: int = 3600
              secret_id_ttl: Optional[int] = None
              secret_id_num_uses: Optional[int] = None
              bind_secret_id: bool = True

          class AuthManager:
              def __init__(self):
                  self.policies: dict[str, Policy] = {}
                  self.tokens: dict[str, Token] = {}
                  self.app_roles: dict[str, AppRole] = {}
                  self.secret_id_uses: dict[str, int] = {}  # secret_id_hash -> uses

              def create_policy(self, name: str, rules: dict[str, list[str]]) -> Policy:
                  path_rules = {}
                  for path, caps in rules.items():
                      path_rules[path] = {Capability(c) for c in caps}

                  policy = Policy(name=name, path_rules=path_rules)
                  self.policies[name] = policy
                  return policy

              def create_token(self, policies: list[str], ttl: int = 3600,
                               renewable: bool = True, metadata: dict = None) -> str:
                  token_id = "hvs." + secrets.token_urlsafe(32)

                  token = Token(
                      id=token_id,
                      policies=policies,
                      created_at=time.time(),
                      expires_at=time.time() + ttl if ttl else None,
                      renewable=renewable,
                      metadata=metadata or {}
                  )

                  self.tokens[token_id] = token
                  return token_id

              def validate_token(self, token_id: str) -> Optional[Token]:
                  token = self.tokens.get(token_id)
                  if not token:
                      return None
                  if token.expires_at and time.time() > token.expires_at:
                      del self.tokens[token_id]
                      return None
                  return token

              def check_permission(self, token_id: str, path: str, capability: Capability) -> bool:
                  token = self.validate_token(token_id)
                  if not token:
                      return False

                  for policy_name in token.policies:
                      policy = self.policies.get(policy_name)
                      if policy and policy.check(path, capability):
                          return True
                  return False

              # AppRole authentication
              def create_app_role(self, name: str, policies: list[str],
                                  token_ttl: int = 3600) -> str:
                  role_id = secrets.token_urlsafe(16)

                  self.app_roles[name] = AppRole(
                      role_id=role_id,
                      secret_id_hash="",  # No secret yet
                      policies=policies,
                      token_ttl=token_ttl
                  )

                  return role_id

              def generate_secret_id(self, role_name: str) -> str:
                  role = self.app_roles.get(role_name)
                  if not role:
                      raise ValueError("Role not found")

                  secret_id = secrets.token_urlsafe(32)
                  role.secret_id_hash = hashlib.sha256(secret_id.encode()).hexdigest()

                  return secret_id

              def login_approle(self, role_id: str, secret_id: str) -> Optional[str]:
                  '''Authenticate with AppRole, return token'''
                  # Find role by role_id
                  role = None
                  for r in self.app_roles.values():
                      if r.role_id == role_id:
                          role = r
                          break

                  if not role:
                      return None

                  # Verify secret_id
                  secret_hash = hashlib.sha256(secret_id.encode()).hexdigest()
                  if not secrets.compare_digest(secret_hash, role.secret_id_hash):
                      return None

                  # Check secret_id uses
                  if role.secret_id_num_uses:
                      uses = self.secret_id_uses.get(secret_hash, 0)
                      if uses >= role.secret_id_num_uses:
                          return None
                      self.secret_id_uses[secret_hash] = uses + 1

                  # Issue token
                  return self.create_token(
                      policies=role.policies,
                      ttl=role.token_ttl,
                      metadata={"auth_method": "approle"}
                  )
          ```
      pitfalls:
      - role_id can be embedded in code; secret_id must be delivered securely
      - Token lookup table grows unbounded - need periodic cleanup
      - 'Glob patterns in paths: ensure * doesn''t match too broadly'
      - Constant-time comparison for secret validation prevents timing attacks
      acceptance_criteria:
      - Define fine-grained access policies specifying allowed secret paths
      - Authenticate clients using tokens or TLS client certificates
      - Authorize secret access based on matching policy evaluation
      - Log all secret access attempts with identity and outcome
      deliverables:
      - Policy definition specifying which identities can access which secrets
      - Token-based authentication issuing and validating access tokens
      - Policy evaluation engine checking requests against defined rules
      - Audit logging recording every secret access and policy evaluation
    - name: Dynamic Secrets
      description: Generate short-lived credentials on-demand for databases, cloud providers, etc.
      skills:
      - Dynamic credentials
      - Lease management
      - Secret rotation
      hints:
        level1: Dynamic secrets are generated per-request with automatic expiration
        level2: 'Lease: tracks TTL and allows renewal or revocation'
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Callable, Any, Optional
          from abc import ABC, abstractmethod
          import secrets
          import string
          import time
          import threading

          @dataclass
          class Lease:
              lease_id: str
              secret_type: str
              ttl: int
              renewable: bool
              created_at: float
              expires_at: float
              data: dict  # Actual credentials
              revoke_callback: Callable[[], None]

          class SecretEngine(ABC):
              @abstractmethod
              def generate(self, role: str) -> tuple[dict, Callable[[], None]]:
                  '''Generate credentials and return (creds, revoke_fn)'''
                  pass

          class PostgresSecretEngine(SecretEngine):
              def __init__(self, conn_string: str):
                  self.conn_string = conn_string

              def generate(self, role: str) -> tuple[dict, Callable[[], None]]:
                  # Generate random username/password
                  username = f"v-{role}-{secrets.token_hex(4)}"
                  password = ''.join(secrets.choice(string.ascii_letters + string.digits)
                                    for _ in range(32))

                  # In production: actually create the user in Postgres
                  # CREATE USER {username} WITH PASSWORD '{password}';
                  # GRANT {role} TO {username};

                  def revoke():
                      # DROP USER {username};
                      print(f"Revoking Postgres user: {username}")

                  return {
                      "username": username,
                      "password": password,
                      "connection_string": f"postgresql://{username}:{password}@..."
                  }, revoke

          class AWSSecretEngine(SecretEngine):
              def __init__(self, access_key: str, secret_key: str):
                  self.access_key = access_key
                  self.secret_key = secret_key

              def generate(self, role: str) -> tuple[dict, Callable[[], None]]:
                  # In production: use AWS STS AssumeRole or IAM CreateAccessKey
                  temp_access_key = f"AKIA{secrets.token_hex(8).upper()}"
                  temp_secret_key = secrets.token_urlsafe(32)

                  def revoke():
                      # IAM DeleteAccessKey
                      print(f"Revoking AWS key: {temp_access_key}")

                  return {
                      "access_key": temp_access_key,
                      "secret_key": temp_secret_key,
                      "session_token": None  # For STS
                  }, revoke

          class LeaseManager:
              def __init__(self):
                  self.leases: dict[str, Lease] = {}
                  self.engines: dict[str, SecretEngine] = {}
                  self._start_reaper()

              def register_engine(self, name: str, engine: SecretEngine):
                  self.engines[name] = engine

              def generate(self, engine_name: str, role: str, ttl: int = 3600) -> Lease:
                  engine = self.engines.get(engine_name)
                  if not engine:
                      raise ValueError(f"Unknown engine: {engine_name}")

                  creds, revoke_fn = engine.generate(role)

                  lease_id = f"{engine_name}/{role}/{secrets.token_hex(8)}"
                  now = time.time()

                  lease = Lease(
                      lease_id=lease_id,
                      secret_type=engine_name,
                      ttl=ttl,
                      renewable=True,
                      created_at=now,
                      expires_at=now + ttl,
                      data=creds,
                      revoke_callback=revoke_fn
                  )

                  self.leases[lease_id] = lease
                  return lease

              def renew(self, lease_id: str, increment: int = None) -> Optional[Lease]:
                  lease = self.leases.get(lease_id)
                  if not lease or not lease.renewable:
                      return None

                  # Extend TTL
                  extension = increment or lease.ttl
                  lease.expires_at = time.time() + extension
                  return lease

              def revoke(self, lease_id: str) -> bool:
                  lease = self.leases.get(lease_id)
                  if not lease:
                      return False

                  # Call revoke callback (delete user, invalidate key, etc.)
                  try:
                      lease.revoke_callback()
                  except Exception as e:
                      print(f"Error revoking {lease_id}: {e}")

                  del self.leases[lease_id]
                  return True

              def _start_reaper(self):
                  '''Background thread to revoke expired leases'''
                  def reap():
                      while True:
                          time.sleep(60)
                          now = time.time()
                          expired = [lid for lid, l in self.leases.items()
                                    if l.expires_at < now]
                          for lease_id in expired:
                              self.revoke(lease_id)

                  thread = threading.Thread(target=reap, daemon=True)
                  thread.start()
          ```
      pitfalls:
      - Lease reaper must handle engine failures gracefully
      - Max TTL should be enforced even for renewals
      - Revocation can fail - need retry logic and alerting
      - 'Connection pooling: don''t create new DB user for every request'
      acceptance_criteria:
      - Generate unique database credentials on demand for each request
      - Handle automatic credential rotation at configurable intervals
      - Revoke dynamically generated credentials when lease expires
      - Support multiple secret backends including databases and cloud providers
      deliverables:
      - Dynamic secret generation creating credentials on demand
      - Database credential backend generating time-limited users
      - TTL management tracking and enforcing secret lease expiration
      - Secret revocation revoking credentials before lease expires
    - name: Unsealing & High Availability
      description: Implement Shamir's secret sharing for master key unsealing and HA replication
      skills:
      - Shamir's secret sharing
      - Consensus
      - Replication
      hints:
        level1: Split master key into N shares; require K shares to reconstruct (K-of-N)
        level2: 'Sealed state: encrypted data exists but can''t be decrypted without unsealing'
        level3: |2

          ```python
          from typing import List, Tuple
          import secrets
          from functools import reduce

          class ShamirSecretSharing:
              '''
              Shamir's Secret Sharing Scheme
              Split secret into n shares, requiring k shares to reconstruct
              '''
              PRIME = 2**127 - 1  # Mersenne prime for finite field

              @classmethod
              def split(cls, secret: int, k: int, n: int) -> List[Tuple[int, int]]:
                  '''Split secret into n shares with threshold k'''
                  if k > n:
                      raise ValueError("Threshold cannot exceed total shares")

                  # Generate random polynomial coefficients
                  # f(x) = secret + a1*x + a2*x^2 + ... + a(k-1)*x^(k-1)
                  coefficients = [secret] + [secrets.randbelow(cls.PRIME) for _ in range(k - 1)]

                  # Generate shares: (x, f(x)) for x = 1, 2, ..., n
                  shares = []
                  for x in range(1, n + 1):
                      y = cls._evaluate_polynomial(coefficients, x)
                      shares.append((x, y))

                  return shares

              @classmethod
              def reconstruct(cls, shares: List[Tuple[int, int]]) -> int:
                  '''Reconstruct secret from k shares using Lagrange interpolation'''
                  k = len(shares)
                  secret = 0

                  for i, (xi, yi) in enumerate(shares):
                      # Compute Lagrange basis polynomial at x=0
                      numerator = 1
                      denominator = 1

                      for j, (xj, _) in enumerate(shares):
                          if i != j:
                              numerator = (numerator * (-xj)) % cls.PRIME
                              denominator = (denominator * (xi - xj)) % cls.PRIME

                      # Modular multiplicative inverse
                      lagrange = (yi * numerator * pow(denominator, -1, cls.PRIME)) % cls.PRIME
                      secret = (secret + lagrange) % cls.PRIME

                  return secret

              @classmethod
              def _evaluate_polynomial(cls, coefficients: List[int], x: int) -> int:
                  result = 0
                  for i, coef in enumerate(coefficients):
                      result = (result + coef * pow(x, i, cls.PRIME)) % cls.PRIME
                  return result

          class SealableVault:
              def __init__(self, threshold: int = 3, shares: int = 5):
                  self.threshold = threshold
                  self.total_shares = shares
                  self.sealed = True
                  self.master_key: bytes = None
                  self.unseal_shares: List[Tuple[int, int]] = []
                  self.secret_store: SecretStore = None

              def initialize(self) -> List[bytes]:
                  '''Initialize vault, return key shares (distribute to operators)'''
                  # Generate master key
                  master_key_int = int.from_bytes(secrets.token_bytes(16), 'big')

                  # Split using Shamir
                  shares = ShamirSecretSharing.split(
                      master_key_int, self.threshold, self.total_shares
                  )

                  # Encode shares for distribution
                  encoded_shares = []
                  for x, y in shares:
                      # Encode as "x:y" in hex
                      share_bytes = f"{x}:{y:032x}".encode()
                      encoded_shares.append(share_bytes)

                  # Store encrypted version of master key (for verification later)
                  self._master_key_hash = hashlib.sha256(
                      master_key_int.to_bytes(16, 'big')
                  ).hexdigest()

                  return encoded_shares

              def unseal(self, share: bytes) -> bool:
                  '''Provide an unseal key. Returns True when vault is unsealed.'''
                  if not self.sealed:
                      return True

                  # Decode share
                  parts = share.decode().split(':')
                  x = int(parts[0])
                  y = int(parts[1], 16)

                  # Add to collected shares (avoid duplicates)
                  if not any(s[0] == x for s in self.unseal_shares):
                      self.unseal_shares.append((x, y))

                  # Try to reconstruct if we have enough shares
                  if len(self.unseal_shares) >= self.threshold:
                      master_key_int = ShamirSecretSharing.reconstruct(
                          self.unseal_shares[:self.threshold]
                      )
                      master_key = master_key_int.to_bytes(16, 'big')

                      # Verify
                      if hashlib.sha256(master_key).hexdigest() == self._master_key_hash:
                          self.master_key = master_key
                          self.secret_store = SecretStore(master_key)
                          self.sealed = False
                          self.unseal_shares = []  # Clear shares from memory
                          return True
                      else:
                          # Invalid reconstruction - wrong shares
                          self.unseal_shares = []
                          raise ValueError("Invalid unseal keys")

                  return False

              def seal(self):
                  '''Seal the vault - clear master key from memory'''
                  self.master_key = None
                  self.secret_store = None
                  self.sealed = True
          ```
      pitfalls:
      - Never store all shares together - defeats the purpose
      - Clear shares from memory after reconstruction
      - Sealed state must reject all secret operations
      - 'HA: only one node should be active writer to prevent split-brain'
      acceptance_criteria:
      - Implement Shamir's secret sharing to split master key into N shares
      - Require configurable threshold of key shares to unseal the vault
      - Handle high availability with leader election among cluster nodes
      - Support auto-unseal integration with cloud KMS providers
      deliverables:
      - Seal and unseal mechanism protecting master key at rest
      - Shamir's secret sharing splitting master key into key shares
      - High availability backend supporting multiple active instances
      - Standby node promotion for automatic failover on leader failure
  session-management:
    name: Distributed Session Management
    description: Build a production-grade session management system with distributed storage, secure cookies, and session
      fixation prevention.
    why_expert: Session management bugs (fixation, hijacking) are common vulnerabilities. Understanding internals helps prevent
      security issues.
    difficulty: advanced
    tags:
    - security
    - sessions
    - distributed
    - cookies
    - authentication
    estimated_hours: 30
    prerequisites:
    - build-redis
    milestones:
    - name: Secure Session Creation & Storage
      description: Implement cryptographically secure session IDs with distributed storage backend
      skills:
      - Session security
      - Distributed storage
      - Cookie handling
      hints:
        level1: Session ID must be cryptographically random (128+ bits of entropy)
        level2: Store session data server-side, only session ID in cookie
        level3: |2

          ```python
          import secrets
          import hashlib
          import time
          import json
          from dataclasses import dataclass, field
          from typing import Optional, Any
          from abc import ABC, abstractmethod

          @dataclass
          class Session:
              id: str
              user_id: Optional[str]
              data: dict
              created_at: float
              last_accessed: float
              expires_at: float
              ip_address: str
              user_agent: str

          class SessionStore(ABC):
              @abstractmethod
              def save(self, session: Session) -> None: pass

              @abstractmethod
              def load(self, session_id: str) -> Optional[Session]: pass

              @abstractmethod
              def delete(self, session_id: str) -> None: pass

              @abstractmethod
              def delete_user_sessions(self, user_id: str) -> int: pass

          class RedisSessionStore(SessionStore):
              def __init__(self, redis_client, prefix: str = "session:"):
                  self.redis = redis_client
                  self.prefix = prefix

              def save(self, session: Session) -> None:
                  key = f"{self.prefix}{session.id}"
                  ttl = int(session.expires_at - time.time())

                  data = {
                      "id": session.id,
                      "user_id": session.user_id,
                      "data": session.data,
                      "created_at": session.created_at,
                      "last_accessed": session.last_accessed,
                      "expires_at": session.expires_at,
                      "ip_address": session.ip_address,
                      "user_agent": session.user_agent
                  }

                  self.redis.setex(key, ttl, json.dumps(data))

                  # Index by user for "logout all devices"
                  if session.user_id:
                      user_key = f"{self.prefix}user:{session.user_id}"
                      self.redis.sadd(user_key, session.id)
                      self.redis.expire(user_key, ttl)

              def load(self, session_id: str) -> Optional[Session]:
                  key = f"{self.prefix}{session_id}"
                  data = self.redis.get(key)
                  if not data:
                      return None

                  d = json.loads(data)
                  return Session(**d)

              def delete(self, session_id: str) -> None:
                  key = f"{self.prefix}{session_id}"
                  self.redis.delete(key)

              def delete_user_sessions(self, user_id: str) -> int:
                  user_key = f"{self.prefix}user:{user_id}"
                  session_ids = self.redis.smembers(user_key)

                  for sid in session_ids:
                      self.delete(sid.decode() if isinstance(sid, bytes) else sid)

                  self.redis.delete(user_key)
                  return len(session_ids)

          class SessionManager:
              def __init__(self, store: SessionStore,
                           session_ttl: int = 86400,  # 24 hours
                           idle_timeout: int = 1800):  # 30 minutes
                  self.store = store
                  self.session_ttl = session_ttl
                  self.idle_timeout = idle_timeout

              def create_session(self, ip_address: str, user_agent: str,
                                 user_id: str = None) -> Session:
                  # Generate cryptographically secure session ID
                  session_id = secrets.token_urlsafe(32)  # 256 bits

                  now = time.time()
                  session = Session(
                      id=session_id,
                      user_id=user_id,
                      data={},
                      created_at=now,
                      last_accessed=now,
                      expires_at=now + self.session_ttl,
                      ip_address=ip_address,
                      user_agent=user_agent
                  )

                  self.store.save(session)
                  return session

              def get_session(self, session_id: str, ip_address: str = None) -> Optional[Session]:
                  session = self.store.load(session_id)
                  if not session:
                      return None

                  now = time.time()

                  # Check absolute expiry
                  if now > session.expires_at:
                      self.store.delete(session_id)
                      return None

                  # Check idle timeout
                  if now - session.last_accessed > self.idle_timeout:
                      self.store.delete(session_id)
                      return None

                  # Optional: IP binding for high-security applications
                  # if ip_address and session.ip_address != ip_address:
                  #     return None  # Session hijacking attempt?

                  # Update last accessed
                  session.last_accessed = now
                  self.store.save(session)

                  return session

              def regenerate_id(self, old_session_id: str) -> Optional[Session]:
                  '''Regenerate session ID (call after login to prevent fixation)'''
                  session = self.store.load(old_session_id)
                  if not session:
                      return None

                  # Delete old session
                  self.store.delete(old_session_id)

                  # Create new session with same data but new ID
                  session.id = secrets.token_urlsafe(32)
                  session.last_accessed = time.time()

                  self.store.save(session)
                  return session
          ```
      pitfalls:
      - 'Session fixation: ALWAYS regenerate session ID after login'
      - Session ID in URL is insecure (referer leaks, logs) - use cookies only
      - Idle timeout and absolute timeout are different - need both
      - 'Race condition: concurrent requests can cause session data loss'
      acceptance_criteria:
      - Generate cryptographically secure random session IDs of sufficient length
      - Store all session data on the server side keyed by session ID
      - Support multiple storage backends including Redis and database
      - Handle session expiration removing stale sessions after timeout
      deliverables:
      - Cryptographically secure session ID generation using random bytes
      - Server-side session data storage keyed by session identifier
      - Multiple storage backend support including Redis, database, and memory
      - Configurable session expiration with automatic cleanup of expired sessions
    - name: Cookie Security & Transport
      description: Implement secure cookie handling with proper flags and encryption
      skills:
      - Cookie security
      - CSRF prevention
      - Secure transport
      hints:
        level1: Set Secure, HttpOnly, SameSite flags on session cookies
        level2: Sign cookies to detect tampering; encrypt if storing data in cookie
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          import hmac
          import hashlib
          import base64
          import time
          import json
          from cryptography.fernet import Fernet

          @dataclass
          class CookieOptions:
              secure: bool = True          # HTTPS only
              http_only: bool = True       # No JavaScript access
              same_site: str = "Lax"       # CSRF protection: Strict, Lax, None
              domain: Optional[str] = None
              path: str = "/"
              max_age: Optional[int] = None  # Seconds until expiry

              def to_header_string(self, name: str, value: str) -> str:
                  parts = [f"{name}={value}"]

                  if self.max_age is not None:
                      parts.append(f"Max-Age={self.max_age}")
                  if self.domain:
                      parts.append(f"Domain={self.domain}")
                  parts.append(f"Path={self.path}")
                  if self.secure:
                      parts.append("Secure")
                  if self.http_only:
                      parts.append("HttpOnly")
                  parts.append(f"SameSite={self.same_site}")

                  return "; ".join(parts)

          class SecureCookieManager:
              def __init__(self, secret_key: bytes, encryption_key: bytes = None):
                  self.secret_key = secret_key
                  self.fernet = Fernet(encryption_key) if encryption_key else None

              def sign(self, value: str) -> str:
                  '''Sign a value for integrity verification'''
                  timestamp = str(int(time.time()))
                  message = f"{timestamp}|{value}"

                  signature = hmac.new(
                      self.secret_key,
                      message.encode(),
                      hashlib.sha256
                  ).hexdigest()

                  return f"{message}|{signature}"

              def verify_signature(self, signed_value: str, max_age: int = None) -> Optional[str]:
                  '''Verify signature and optionally check age'''
                  try:
                      parts = signed_value.rsplit("|", 2)
                      if len(parts) != 3:
                          return None

                      timestamp_str, value, signature = parts
                      timestamp = int(timestamp_str)

                      # Check age
                      if max_age and (time.time() - timestamp) > max_age:
                          return None

                      # Verify signature
                      message = f"{timestamp_str}|{value}"
                      expected = hmac.new(
                          self.secret_key,
                          message.encode(),
                          hashlib.sha256
                      ).hexdigest()

                      if hmac.compare_digest(signature, expected):
                          return value
                      return None

                  except (ValueError, TypeError):
                      return None

              def encrypt(self, data: dict) -> str:
                  '''Encrypt data for cookie storage'''
                  if not self.fernet:
                      raise ValueError("Encryption key not configured")

                  json_data = json.dumps(data)
                  encrypted = self.fernet.encrypt(json_data.encode())
                  return base64.urlsafe_b64encode(encrypted).decode()

              def decrypt(self, encrypted_value: str) -> Optional[dict]:
                  '''Decrypt cookie data'''
                  if not self.fernet:
                      return None

                  try:
                      encrypted = base64.urlsafe_b64decode(encrypted_value.encode())
                      decrypted = self.fernet.decrypt(encrypted)
                      return json.loads(decrypted.decode())
                  except Exception:
                      return None

              def create_session_cookie(self, session_id: str,
                                        options: CookieOptions = None) -> str:
                  '''Create a secure session cookie'''
                  if options is None:
                      options = CookieOptions()

                  # Sign the session ID
                  signed_id = self.sign(session_id)

                  return options.to_header_string("session", signed_id)

              def parse_session_cookie(self, cookie_value: str,
                                       max_age: int = 86400) -> Optional[str]:
                  '''Parse and verify session cookie'''
                  return self.verify_signature(cookie_value, max_age)

          # CSRF Token management
          class CSRFProtection:
              def __init__(self, secret_key: bytes):
                  self.secret_key = secret_key

              def generate_token(self, session_id: str) -> str:
                  '''Generate CSRF token bound to session'''
                  random_part = secrets.token_urlsafe(16)
                  message = f"{session_id}|{random_part}"

                  signature = hmac.new(
                      self.secret_key,
                      message.encode(),
                      hashlib.sha256
                  ).hexdigest()[:16]

                  return f"{random_part}.{signature}"

              def validate_token(self, token: str, session_id: str) -> bool:
                  '''Validate CSRF token'''
                  try:
                      random_part, signature = token.split(".")
                      message = f"{session_id}|{random_part}"

                      expected = hmac.new(
                          self.secret_key,
                          message.encode(),
                          hashlib.sha256
                      ).hexdigest()[:16]

                      return hmac.compare_digest(signature, expected)
                  except ValueError:
                      return False
          ```
      pitfalls:
      - SameSite=None requires Secure flag (HTTPS)
      - Cookie size limit is ~4KB - don't store too much data
      - CSRF tokens must be tied to session - not global
      - Double-submit cookie pattern needs both cookie AND header/form
      acceptance_criteria:
      - Set HttpOnly, Secure, and SameSite flags on all session cookies
      - Encrypt or sign cookie values to prevent client-side tampering
      - Support header-based session tokens as cookie-less alternative
      - Implement CSRF protection using per-session anti-forgery tokens
      deliverables:
      - Secure cookie configuration with HttpOnly and Secure flags enabled
      - HttpOnly flag preventing JavaScript access to session cookie
      - SameSite attribute restricting cross-origin cookie transmission
      - Cookie value encryption protecting session ID from tampering
    - name: Multi-Device & Concurrent Sessions
      description: Handle multiple sessions per user, device tracking, and forced logout
      skills:
      - Device fingerprinting
      - Session listing
      - Forced logout
      hints:
        level1: Track all active sessions per user for 'logout all devices'
        level2: Device fingerprint (IP, User-Agent, etc.) helps detect session theft
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          import hashlib
          import time
          from user_agents import parse as parse_ua  # pip install user-agents

          @dataclass
          class DeviceInfo:
              device_id: str
              device_type: str      # mobile, tablet, desktop
              os: str
              browser: str
              ip_address: str
              location: Optional[str]  # From IP geolocation
              first_seen: float
              last_seen: float
              is_current: bool = False

          @dataclass
          class UserSession:
              session_id: str
              device: DeviceInfo
              created_at: float
              last_active: float
              is_current: bool

          class MultiDeviceSessionManager:
              def __init__(self, session_manager: SessionManager, store: SessionStore):
                  self.sessions = session_manager
                  self.store = store
                  self.max_sessions_per_user = 10

              def create_session_with_device(self, user_id: str, ip_address: str,
                                             user_agent: str) -> Session:
                  # Parse user agent
                  ua = parse_ua(user_agent)

                  # Create device fingerprint
                  device_id = self._fingerprint_device(ip_address, user_agent)

                  # Check session limit
                  existing = self.get_user_sessions(user_id)
                  if len(existing) >= self.max_sessions_per_user:
                      # Remove oldest session
                      oldest = min(existing, key=lambda s: s.last_active)
                      self.store.delete(oldest.session_id)

                  # Create session
                  session = self.sessions.create_session(
                      ip_address=ip_address,
                      user_agent=user_agent,
                      user_id=user_id
                  )

                  # Store device info in session
                  session.data["device"] = {
                      "device_id": device_id,
                      "device_type": self._get_device_type(ua),
                      "os": f"{ua.os.family} {ua.os.version_string}",
                      "browser": f"{ua.browser.family} {ua.browser.version_string}",
                      "ip_address": ip_address
                  }

                  self.store.save(session)
                  return session

              def _fingerprint_device(self, ip: str, user_agent: str) -> str:
                  '''Create stable device fingerprint'''
                  # In production: add more signals (screen size, timezone, etc.)
                  data = f"{ip}|{user_agent}"
                  return hashlib.sha256(data.encode()).hexdigest()[:16]

              def _get_device_type(self, ua) -> str:
                  if ua.is_mobile:
                      return "mobile"
                  elif ua.is_tablet:
                      return "tablet"
                  return "desktop"

              def get_user_sessions(self, user_id: str) -> list[UserSession]:
                  '''List all active sessions for a user'''
                  # Get all session IDs for user from Redis set
                  user_key = f"session:user:{user_id}"
                  session_ids = self.store.redis.smembers(user_key)

                  sessions = []
                  for sid in session_ids:
                      sid_str = sid.decode() if isinstance(sid, bytes) else sid
                      session = self.store.load(sid_str)
                      if session:
                          device_data = session.data.get("device", {})
                          sessions.append(UserSession(
                              session_id=session.id,
                              device=DeviceInfo(
                                  device_id=device_data.get("device_id", "unknown"),
                                  device_type=device_data.get("device_type", "unknown"),
                                  os=device_data.get("os", "unknown"),
                                  browser=device_data.get("browser", "unknown"),
                                  ip_address=device_data.get("ip_address", "unknown"),
                                  location=None,  # Add geolocation lookup
                                  first_seen=session.created_at,
                                  last_seen=session.last_accessed,
                                  is_current=False
                              ),
                              created_at=session.created_at,
                              last_active=session.last_accessed,
                              is_current=False
                          ))

                  return sorted(sessions, key=lambda s: s.last_active, reverse=True)

              def logout_session(self, user_id: str, session_id: str,
                                 current_session_id: str) -> bool:
                  '''Logout a specific session (from settings page)'''
                  session = self.store.load(session_id)
                  if not session or session.user_id != user_id:
                      return False

                  # Don't allow logging out current session via this method
                  if session_id == current_session_id:
                      return False

                  self.store.delete(session_id)
                  return True

              def logout_all_except_current(self, user_id: str,
                                            current_session_id: str) -> int:
                  '''Logout all sessions except current'''
                  sessions = self.get_user_sessions(user_id)
                  count = 0

                  for session in sessions:
                      if session.session_id != current_session_id:
                          self.store.delete(session.session_id)
                          count += 1

                  return count

              def force_logout_user(self, user_id: str) -> int:
                  '''Admin: force logout all sessions for a user'''
                  return self.store.delete_user_sessions(user_id)

              def detect_anomaly(self, session: Session, ip_address: str,
                                 user_agent: str) -> list[str]:
                  '''Detect suspicious session activity'''
                  warnings = []
                  device_data = session.data.get("device", {})

                  # IP changed significantly
                  if device_data.get("ip_address") != ip_address:
                      # Could check if same /16 subnet or geolocation
                      warnings.append("ip_changed")

                  # User agent changed (browser update is normal, complete change is suspicious)
                  if device_data.get("browser") != user_agent:
                      new_fp = self._fingerprint_device(ip_address, user_agent)
                      if new_fp != device_data.get("device_id"):
                          warnings.append("device_changed")

                  return warnings
          ```
      pitfalls:
      - Device fingerprinting has false positives (VPN, browser updates)
      - Session limit without cleanup leads to locked-out users
      - Time-based anomaly detection needs to consider time zones
      - Don't show full session ID in UI - use hash or partial
      acceptance_criteria:
      - Track active sessions per user across multiple devices and browsers
      - Support listing and selective revocation of individual sessions
      - Enforce configurable concurrent session limits per user account
      - Track session activity timestamps for last-active monitoring
      deliverables:
      - Per-user device tracking recording session origin information
      - Session listing endpoint showing all active sessions for a user
      - Session revocation allowing users to terminate specific sessions
      - Concurrent session limit enforcing maximum active sessions per user
  payment-gateway:
    name: Payment Gateway Integration
    description: Build a payment processing system handling credit cards, idempotency, PCI compliance considerations, refunds,
      and webhook reconciliation.
    why_expert: Payment bugs cost real money and trust. Understanding payment flow internals helps prevent double-charges,
      handle edge cases, and integrate any payment provider.
    difficulty: expert
    tags:
    - payments
    - fintech
    - security
    - api-integration
    - idempotency
    estimated_hours: 45
    prerequisites:
    - build-http-server
    milestones:
    - name: Payment Intent & Idempotency
      description: Implement payment intents with idempotency keys to prevent duplicate charges
      skills:
      - Idempotency patterns
      - State machines
      - Atomic operations
      hints:
        level1: Create a payment intent before charging - it represents customer's intention to pay
        level2: 'Idempotency key: same key = same response. Store request hash with response.'
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          import secrets
          import hashlib
          import time
          import json

          class PaymentStatus(Enum):
              CREATED = "created"
              REQUIRES_PAYMENT_METHOD = "requires_payment_method"
              REQUIRES_CONFIRMATION = "requires_confirmation"
              PROCESSING = "processing"
              SUCCEEDED = "succeeded"
              FAILED = "failed"
              CANCELED = "canceled"
              REFUNDED = "refunded"
              PARTIALLY_REFUNDED = "partially_refunded"

          @dataclass
          class PaymentIntent:
              id: str
              amount: int              # In smallest currency unit (cents)
              currency: str
              status: PaymentStatus
              customer_id: Optional[str]
              payment_method_id: Optional[str]
              description: str
              metadata: dict
              created_at: float
              updated_at: float
              idempotency_key: Optional[str]
              client_secret: str       # For frontend confirmation
              error_message: Optional[str] = None
              charge_id: Optional[str] = None

          @dataclass
          class IdempotencyRecord:
              key: str
              request_hash: str
              response: dict
              status_code: int
              created_at: float
              expires_at: float

          class IdempotencyManager:
              def __init__(self, ttl: int = 86400):  # 24 hour TTL
                  self.records: dict[str, IdempotencyRecord] = {}
                  self.ttl = ttl

              def _hash_request(self, method: str, path: str, body: dict) -> str:
                  data = f"{method}|{path}|{json.dumps(body, sort_keys=True)}"
                  return hashlib.sha256(data.encode()).hexdigest()

              def check(self, idempotency_key: str, method: str,
                        path: str, body: dict) -> Optional[tuple[dict, int]]:
                  '''Check if we've seen this request before'''
                  record = self.records.get(idempotency_key)
                  if not record:
                      return None

                  # Check expiry
                  if time.time() > record.expires_at:
                      del self.records[idempotency_key]
                      return None

                  # Verify request matches (prevent key reuse with different request)
                  request_hash = self._hash_request(method, path, body)
                  if request_hash != record.request_hash:
                      raise ValueError(
                          "Idempotency key already used with different request parameters"
                      )

                  return record.response, record.status_code

              def store(self, idempotency_key: str, method: str, path: str,
                        body: dict, response: dict, status_code: int):
                  '''Store response for idempotency key'''
                  self.records[idempotency_key] = IdempotencyRecord(
                      key=idempotency_key,
                      request_hash=self._hash_request(method, path, body),
                      response=response,
                      status_code=status_code,
                      created_at=time.time(),
                      expires_at=time.time() + self.ttl
                  )

          class PaymentService:
              def __init__(self):
                  self.intents: dict[str, PaymentIntent] = {}
                  self.idempotency = IdempotencyManager()

              def create_payment_intent(self, amount: int, currency: str,
                                        customer_id: str = None,
                                        description: str = "",
                                        metadata: dict = None,
                                        idempotency_key: str = None) -> PaymentIntent:
                  # Check idempotency
                  if idempotency_key:
                      existing = self.idempotency.check(
                          idempotency_key, "POST", "/payment_intents",
                          {"amount": amount, "currency": currency}
                      )
                      if existing:
                          return self.intents[existing[0]["id"]]

                  # Create new payment intent
                  intent_id = f"pi_{secrets.token_urlsafe(16)}"
                  client_secret = f"{intent_id}_secret_{secrets.token_urlsafe(16)}"

                  now = time.time()
                  intent = PaymentIntent(
                      id=intent_id,
                      amount=amount,
                      currency=currency.lower(),
                      status=PaymentStatus.REQUIRES_PAYMENT_METHOD,
                      customer_id=customer_id,
                      payment_method_id=None,
                      description=description,
                      metadata=metadata or {},
                      created_at=now,
                      updated_at=now,
                      idempotency_key=idempotency_key,
                      client_secret=client_secret
                  )

                  self.intents[intent_id] = intent

                  # Store for idempotency
                  if idempotency_key:
                      self.idempotency.store(
                          idempotency_key, "POST", "/payment_intents",
                          {"amount": amount, "currency": currency},
                          {"id": intent_id, "client_secret": client_secret},
                          200
                      )

                  return intent

              def attach_payment_method(self, intent_id: str,
                                        payment_method_id: str) -> PaymentIntent:
                  intent = self.intents.get(intent_id)
                  if not intent:
                      raise ValueError("Payment intent not found")

                  if intent.status not in [PaymentStatus.REQUIRES_PAYMENT_METHOD,
                                           PaymentStatus.REQUIRES_CONFIRMATION]:
                      raise ValueError(f"Cannot modify intent in status: {intent.status}")

                  intent.payment_method_id = payment_method_id
                  intent.status = PaymentStatus.REQUIRES_CONFIRMATION
                  intent.updated_at = time.time()

                  return intent
          ```
      pitfalls:
      - Idempotency key reused with different params should error, not return old response
      - Client secret must be kept secret - only share with authenticated user
      - Amount in cents prevents floating point errors ($10.00 = 1000 cents)
      - Status transitions must be validated - can't go backwards
      acceptance_criteria:
      - Create payment intents with client-provided idempotency key
      - Track payment state transitions through defined lifecycle stages
      - Handle duplicate requests safely returning same result for same key
      - Support attaching arbitrary metadata to payment intent records
      deliverables:
      - Payment intent creation API returning unique intent identifier
      - Idempotency key handling preventing duplicate charge processing
      - Intent state machine tracking lifecycle from created to completed
      - Intent expiration cancelling stale uncompleted payment intents
    - name: Payment Processing & 3DS
      description: Implement payment confirmation with 3D Secure authentication flow
      skills:
      - 3DS flow
      - Payment processing
      - Error handling
      hints:
        level1: 3DS (3D Secure) is required for Strong Customer Authentication (SCA) in EU
        level2: Payment can be synchronous (immediate) or async (3DS redirect needed)
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          from enum import Enum

          class ThreeDSStatus(Enum):
              NOT_REQUIRED = "not_required"
              REQUIRED = "required"
              CHALLENGE = "challenge"      # User must complete challenge
              SUCCEEDED = "succeeded"
              FAILED = "failed"

          @dataclass
          class ThreeDSResult:
              status: ThreeDSStatus
              redirect_url: Optional[str] = None  # For challenge flow
              version: str = "2.0"

          @dataclass
          class PaymentMethodDetails:
              id: str
              card_brand: str           # visa, mastercard, amex
              last4: str
              exp_month: int
              exp_year: int
              fingerprint: str          # Unique card identifier
              three_ds_supported: bool

          class PaymentProcessor:
              def __init__(self, payment_service: PaymentService):
                  self.service = payment_service
                  self.charges: dict[str, dict] = {}

              def confirm_payment(self, intent_id: str,
                                  return_url: str = None) -> dict:
                  '''
                  Confirm payment intent. May require 3DS.
                  Returns: {status, next_action?, charge_id?}
                  '''
                  intent = self.service.intents.get(intent_id)
                  if not intent:
                      raise ValueError("Payment intent not found")

                  if intent.status != PaymentStatus.REQUIRES_CONFIRMATION:
                      raise ValueError(f"Cannot confirm intent in status: {intent.status}")

                  # Check if 3DS is required (based on card, amount, region)
                  three_ds = self._check_3ds_requirement(intent)

                  if three_ds.status == ThreeDSStatus.CHALLENGE:
                      # Need user interaction
                      intent.status = PaymentStatus.PROCESSING
                      return {
                          "status": "requires_action",
                          "next_action": {
                              "type": "redirect_to_url",
                              "redirect_to_url": {
                                  "url": three_ds.redirect_url,
                                  "return_url": return_url
                              }
                          }
                      }

                  if three_ds.status == ThreeDSStatus.FAILED:
                      intent.status = PaymentStatus.FAILED
                      intent.error_message = "3D Secure authentication failed"
                      return {"status": "failed", "error": intent.error_message}

                  # Process payment
                  return self._process_charge(intent)

              def handle_3ds_callback(self, intent_id: str,
                                      three_ds_result: str) -> dict:
                  '''Handle return from 3DS challenge'''
                  intent = self.service.intents.get(intent_id)
                  if not intent:
                      raise ValueError("Payment intent not found")

                  # Verify 3DS result (in production: verify with card network)
                  if three_ds_result == "authenticated":
                      return self._process_charge(intent)
                  else:
                      intent.status = PaymentStatus.FAILED
                      intent.error_message = "3D Secure authentication failed"
                      return {"status": "failed", "error": intent.error_message}

              def _check_3ds_requirement(self, intent: PaymentIntent) -> ThreeDSResult:
                  '''Determine if 3DS is required'''
                  # In production: check card issuer requirements, SCA rules, etc.

                  # SCA required for EU cards over certain threshold
                  if intent.amount > 3000:  # â‚¬30.00
                      return ThreeDSResult(
                          status=ThreeDSStatus.CHALLENGE,
                          redirect_url=f"https://3ds.example.com/challenge/{intent.id}"
                      )

                  return ThreeDSResult(status=ThreeDSStatus.NOT_REQUIRED)

              def _process_charge(self, intent: PaymentIntent) -> dict:
                  '''Actually charge the card'''
                  intent.status = PaymentStatus.PROCESSING

                  try:
                      # In production: call payment processor API (Stripe, Adyen, etc.)
                      charge_id = f"ch_{secrets.token_urlsafe(16)}"

                      # Simulate processing
                      success = True  # In reality: check processor response

                      if success:
                          intent.status = PaymentStatus.SUCCEEDED
                          intent.charge_id = charge_id
                          intent.updated_at = time.time()

                          self.charges[charge_id] = {
                              "id": charge_id,
                              "amount": intent.amount,
                              "currency": intent.currency,
                              "payment_intent": intent.id,
                              "status": "succeeded",
                              "created_at": time.time()
                          }

                          return {
                              "status": "succeeded",
                              "charge_id": charge_id
                          }
                      else:
                          intent.status = PaymentStatus.FAILED
                          intent.error_message = "Card declined"
                          return {"status": "failed", "error": "card_declined"}

                  except Exception as e:
                      intent.status = PaymentStatus.FAILED
                      intent.error_message = str(e)
                      return {"status": "failed", "error": str(e)}

              def capture_payment(self, intent_id: str, amount: int = None) -> dict:
                  '''
                  Capture a previously authorized payment.
                  Used for auth-then-capture flows (hotels, rentals).
                  '''
                  intent = self.service.intents.get(intent_id)
                  if not intent or intent.status != PaymentStatus.SUCCEEDED:
                      raise ValueError("Invalid intent for capture")

                  capture_amount = amount or intent.amount
                  if capture_amount > intent.amount:
                      raise ValueError("Capture amount exceeds authorized amount")

                  # In production: call processor capture API
                  return {
                      "status": "captured",
                      "amount": capture_amount,
                      "charge_id": intent.charge_id
                  }
          ```
      pitfalls:
      - 3DS redirects must include return_url for user to come back
      - Payment can succeed but webhook fail - always reconcile
      - 'Auth-capture: authorization expires (usually 7 days) - capture before expiry'
      - Currency mismatch between intent and capture causes errors
      acceptance_criteria:
      - Process card payments by submitting charges to payment provider
      - Implement 3D Secure authentication flow for supported card issuers
      - Handle authentication redirect returning user to merchant after 3DS
      - Support multiple payment methods including cards and bank transfers
      deliverables:
      - Payment method tokenization securing card details as opaque token
      - Charge creation submitting payment for processing by provider
      - 3D Secure authentication flow handling issuer verification step
      - Payment confirmation completing charge after successful authentication
    - name: Refunds & Disputes
      description: Implement refund processing, partial refunds, and dispute handling
      skills:
      - Refund workflows
      - Dispute handling
      - Financial reconciliation
      hints:
        level1: Refunds can be full or partial; total refunds can't exceed original charge
        level2: Disputes (chargebacks) require evidence submission within deadline
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          from enum import Enum
          import time

          class RefundStatus(Enum):
              PENDING = "pending"
              SUCCEEDED = "succeeded"
              FAILED = "failed"
              CANCELED = "canceled"

          class DisputeStatus(Enum):
              WARNING_NEEDS_RESPONSE = "warning_needs_response"
              WARNING_UNDER_REVIEW = "warning_under_review"
              WARNING_CLOSED = "warning_closed"
              NEEDS_RESPONSE = "needs_response"
              UNDER_REVIEW = "under_review"
              CHARGE_REFUNDED = "charge_refunded"
              WON = "won"
              LOST = "lost"

          class DisputeReason(Enum):
              DUPLICATE = "duplicate"
              FRAUDULENT = "fraudulent"
              SUBSCRIPTION_CANCELED = "subscription_canceled"
              PRODUCT_NOT_RECEIVED = "product_not_received"
              PRODUCT_UNACCEPTABLE = "product_unacceptable"
              UNRECOGNIZED = "unrecognized"
              CREDIT_NOT_PROCESSED = "credit_not_processed"
              GENERAL = "general"

          @dataclass
          class Refund:
              id: str
              charge_id: str
              payment_intent_id: str
              amount: int
              currency: str
              status: RefundStatus
              reason: Optional[str]
              created_at: float
              metadata: dict

          @dataclass
          class Dispute:
              id: str
              charge_id: str
              amount: int
              currency: str
              status: DisputeStatus
              reason: DisputeReason
              evidence_due_by: float
              evidence: Optional[dict]
              created_at: float

          class RefundService:
              def __init__(self, payment_service: PaymentService,
                           processor: PaymentProcessor):
                  self.payments = payment_service
                  self.processor = processor
                  self.refunds: dict[str, Refund] = {}
                  self.charge_refunds: dict[str, list[str]] = {}  # charge_id -> refund_ids

              def create_refund(self, charge_id: str = None,
                                payment_intent_id: str = None,
                                amount: int = None,
                                reason: str = None) -> Refund:
                  # Find the charge
                  if payment_intent_id:
                      intent = self.payments.intents.get(payment_intent_id)
                      if not intent or not intent.charge_id:
                          raise ValueError("Payment intent has no charge")
                      charge_id = intent.charge_id

                  charge = self.processor.charges.get(charge_id)
                  if not charge:
                      raise ValueError("Charge not found")

                  # Calculate refundable amount
                  existing_refunds = self.charge_refunds.get(charge_id, [])
                  total_refunded = sum(
                      self.refunds[rid].amount for rid in existing_refunds
                      if self.refunds[rid].status == RefundStatus.SUCCEEDED
                  )

                  refundable = charge["amount"] - total_refunded
                  refund_amount = amount or refundable

                  if refund_amount > refundable:
                      raise ValueError(
                          f"Refund amount ({refund_amount}) exceeds "
                          f"refundable amount ({refundable})"
                      )

                  if refund_amount <= 0:
                      raise ValueError("Charge already fully refunded")

                  # Create refund
                  refund_id = f"re_{secrets.token_urlsafe(16)}"
                  refund = Refund(
                      id=refund_id,
                      charge_id=charge_id,
                      payment_intent_id=charge.get("payment_intent"),
                      amount=refund_amount,
                      currency=charge["currency"],
                      status=RefundStatus.PENDING,
                      reason=reason,
                      created_at=time.time(),
                      metadata={}
                  )

                  self.refunds[refund_id] = refund

                  # Track refunds per charge
                  if charge_id not in self.charge_refunds:
                      self.charge_refunds[charge_id] = []
                  self.charge_refunds[charge_id].append(refund_id)

                  # Process refund (in production: async with webhook confirmation)
                  self._process_refund(refund)

                  # Update payment intent status
                  intent = self.payments.intents.get(charge.get("payment_intent"))
                  if intent:
                      if total_refunded + refund_amount >= charge["amount"]:
                          intent.status = PaymentStatus.REFUNDED
                      else:
                          intent.status = PaymentStatus.PARTIALLY_REFUNDED

                  return refund

              def _process_refund(self, refund: Refund):
                  # In production: call processor refund API
                  try:
                      # Simulate processing
                      refund.status = RefundStatus.SUCCEEDED
                  except Exception as e:
                      refund.status = RefundStatus.FAILED

          class DisputeService:
              def __init__(self, processor: PaymentProcessor):
                  self.processor = processor
                  self.disputes: dict[str, Dispute] = {}

              def handle_dispute_webhook(self, event_type: str, data: dict):
                  '''Handle dispute webhook from payment processor'''
                  if event_type == "charge.dispute.created":
                      dispute = Dispute(
                          id=data["id"],
                          charge_id=data["charge"],
                          amount=data["amount"],
                          currency=data["currency"],
                          status=DisputeStatus.NEEDS_RESPONSE,
                          reason=DisputeReason(data["reason"]),
                          evidence_due_by=data["evidence_due_by"],
                          evidence=None,
                          created_at=time.time()
                      )
                      self.disputes[dispute.id] = dispute

                      # Alert team immediately
                      self._alert_dispute(dispute)

                  elif event_type == "charge.dispute.closed":
                      dispute = self.disputes.get(data["id"])
                      if dispute:
                          dispute.status = DisputeStatus(data["status"])

              def submit_evidence(self, dispute_id: str, evidence: dict) -> Dispute:
                  '''
                  Submit evidence for a dispute.
                  Evidence includes: receipts, shipping proof, correspondence, etc.
                  '''
                  dispute = self.disputes.get(dispute_id)
                  if not dispute:
                      raise ValueError("Dispute not found")

                  if time.time() > dispute.evidence_due_by:
                      raise ValueError("Evidence submission deadline passed")

                  if dispute.status not in [DisputeStatus.NEEDS_RESPONSE,
                                             DisputeStatus.WARNING_NEEDS_RESPONSE]:
                      raise ValueError("Dispute not accepting evidence")

                  # Validate required evidence based on reason
                  required_fields = self._get_required_evidence(dispute.reason)
                  missing = [f for f in required_fields if f not in evidence]
                  if missing:
                      raise ValueError(f"Missing required evidence: {missing}")

                  dispute.evidence = evidence
                  dispute.status = DisputeStatus.UNDER_REVIEW

                  # In production: submit to processor API
                  return dispute

              def _get_required_evidence(self, reason: DisputeReason) -> list[str]:
                  evidence_requirements = {
                      DisputeReason.PRODUCT_NOT_RECEIVED: [
                          "shipping_carrier", "shipping_tracking_number",
                          "shipping_date"
                      ],
                      DisputeReason.FRAUDULENT: [
                          "customer_email_address", "customer_ip_address",
                          "billing_address"
                      ],
                      DisputeReason.DUPLICATE: [
                          "duplicate_charge_explanation",
                          "original_transaction"
                      ],
                  }
                  return evidence_requirements.get(reason, [])

              def _alert_dispute(self, dispute: Dispute):
                  print(f"ALERT: New dispute {dispute.id} for ${dispute.amount/100:.2f}")
                  print(f"Reason: {dispute.reason.value}")
                  print(f"Evidence due: {dispute.evidence_due_by}")
          ```
      pitfalls:
      - Refund can take 5-10 business days to appear on statement
      - Dispute evidence deadline is strict - automate alerts
      - Dispute fee charged even if you win - prevention is key
      - 'Partial refunds: track total refunded vs original amount carefully'
      acceptance_criteria:
      - Process both full and partial refunds against original payment
      - Track refund status through pending, completed, and failed stages
      - Handle chargeback notifications received from payment network
      - Support dispute evidence submission to contest chargebacks
      deliverables:
      - Full and partial refund processing against completed charges
      - Partial refund support returning portion of original charge amount
      - Dispute handling tracking chargeback status and deadlines
      - Chargeback notification integration receiving alerts from payment network
    - name: Webhook Reconciliation
      description: Implement reliable webhook processing with signature verification and reconciliation
      skills:
      - Webhook security
      - Event processing
      - Reconciliation
      hints:
        level1: Always verify webhook signature before processing
        level2: Webhooks can arrive out of order or duplicated - handle idempotently
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Callable
          import hmac
          import hashlib
          import time
          import json

          @dataclass
          class WebhookEvent:
              id: str
              type: str
              data: dict
              created_at: float
              processed_at: float = None
              attempts: int = 0

          class PaymentWebhookHandler:
              def __init__(self, webhook_secret: str,
                           payment_service: PaymentService,
                           refund_service: RefundService,
                           dispute_service: DisputeService):
                  self.secret = webhook_secret
                  self.payments = payment_service
                  self.refunds = refund_service
                  self.disputes = dispute_service
                  self.processed_events: set[str] = set()
                  self.handlers: dict[str, Callable] = {
                      "payment_intent.succeeded": self._handle_payment_succeeded,
                      "payment_intent.payment_failed": self._handle_payment_failed,
                      "charge.refunded": self._handle_refund,
                      "charge.dispute.created": self._handle_dispute_created,
                      "charge.dispute.closed": self._handle_dispute_closed,
                  }

              def verify_signature(self, payload: bytes, signature: str,
                                   timestamp: str) -> bool:
                  '''Verify Stripe-style webhook signature'''
                  # Signature format: t=timestamp,v1=signature
                  expected = hmac.new(
                      self.secret.encode(),
                      f"{timestamp}.{payload.decode()}".encode(),
                      hashlib.sha256
                  ).hexdigest()

                  # Constant-time comparison
                  return hmac.compare_digest(f"v1={expected}", signature.split(",")[1])

              def handle_webhook(self, payload: bytes, signature: str) -> dict:
                  '''Process incoming webhook'''
                  # Parse signature header
                  parts = dict(p.split("=") for p in signature.split(","))
                  timestamp = parts.get("t", "")

                  # Verify signature
                  if not self.verify_signature(payload, signature, timestamp):
                      raise ValueError("Invalid webhook signature")

                  # Check timestamp (prevent replay attacks)
                  if abs(time.time() - int(timestamp)) > 300:  # 5 minute tolerance
                      raise ValueError("Webhook timestamp too old")

                  # Parse event
                  event_data = json.loads(payload)
                  event_id = event_data["id"]
                  event_type = event_data["type"]

                  # Idempotency check
                  if event_id in self.processed_events:
                      return {"status": "already_processed"}

                  event = WebhookEvent(
                      id=event_id,
                      type=event_type,
                      data=event_data["data"]["object"],
                      created_at=event_data["created"]
                  )

                  # Route to handler
                  handler = self.handlers.get(event_type)
                  if handler:
                      try:
                          handler(event)
                          event.processed_at = time.time()
                          self.processed_events.add(event_id)
                          return {"status": "processed"}
                      except Exception as e:
                          event.attempts += 1
                          # Log error, will be retried by processor
                          return {"status": "error", "error": str(e)}
                  else:
                      # Unknown event type - acknowledge to prevent retries
                      return {"status": "ignored", "reason": "unknown_event_type"}

              def _handle_payment_succeeded(self, event: WebhookEvent):
                  '''Reconcile successful payment'''
                  intent_id = event.data["id"]
                  intent = self.payments.intents.get(intent_id)

                  if intent:
                      # Update our records to match processor state
                      intent.status = PaymentStatus.SUCCEEDED
                      intent.charge_id = event.data.get("latest_charge")
                      intent.updated_at = time.time()

                      # Trigger fulfillment (send email, provision service, etc.)
                      self._trigger_fulfillment(intent)
                  else:
                      # Payment succeeded but we don't have the intent
                      # This can happen if webhook arrives before API response
                      # Log for investigation
                      print(f"WARNING: Unknown payment intent succeeded: {intent_id}")

              def _handle_payment_failed(self, event: WebhookEvent):
                  '''Handle failed payment'''
                  intent_id = event.data["id"]
                  intent = self.payments.intents.get(intent_id)

                  if intent:
                      intent.status = PaymentStatus.FAILED
                      intent.error_message = event.data.get("last_payment_error", {}).get("message")
                      intent.updated_at = time.time()

              def _handle_refund(self, event: WebhookEvent):
                  '''Reconcile refund'''
                  charge_id = event.data["id"]
                  # Check if refund already recorded
                  existing = self.refunds.charge_refunds.get(charge_id, [])

                  for refund_data in event.data.get("refunds", {}).get("data", []):
                      if refund_data["id"] not in [self.refunds.refunds[r].id for r in existing]:
                          # Refund happened outside our system (admin dashboard, etc.)
                          # Record it
                          print(f"Recording external refund: {refund_data['id']}")

              def _handle_dispute_created(self, event: WebhookEvent):
                  self.disputes.handle_dispute_webhook("charge.dispute.created", event.data)

              def _handle_dispute_closed(self, event: WebhookEvent):
                  self.disputes.handle_dispute_webhook("charge.dispute.closed", event.data)

              def _trigger_fulfillment(self, intent: PaymentIntent):
                  '''Trigger order fulfillment after successful payment'''
                  # In production: queue job to send confirmation email,
                  # update order status, provision service, etc.
                  print(f"Fulfilling order for payment: {intent.id}")

              def reconcile(self):
                  '''
                  Periodic reconciliation to catch missed webhooks.
                  Compare our records with processor's records.
                  '''
                  # In production: fetch recent payments from processor API
                  # Compare status with local records
                  # Update any mismatches
                  pass
          ```
      pitfalls:
      - Webhook signature verification is critical - never skip
      - Events can arrive out of order - check timestamps and states
      - Processor may retry webhooks - always be idempotent
      - Run periodic reconciliation - webhooks can fail silently
      acceptance_criteria:
      - Receive and process payment webhooks from external provider
      - Update local payment status records based on webhook event data
      - Verify webhook payload signature to prevent spoofed notifications
      - Reconcile local payment records against expected provider state
      deliverables:
      - Webhook endpoint receiving payment event notifications from provider
      - Signature verification authenticating webhook payloads cryptographically
      - Event processing handler updating local records from webhook data
      - State reconciliation comparing local payment state against provider state
  subscription-billing:
    name: Subscription & Billing System
    description: Build a complete subscription management system with plans, billing cycles, proration, trials, and usage-based
      billing.
    why_expert: Subscription logic is complex with edge cases (upgrades, downgrades, trials, cancellations). Building one
      teaches financial calculations and state management.
    difficulty: expert
    tags:
    - billing
    - subscriptions
    - saas
    - fintech
    - recurring-payments
    estimated_hours: 50
    prerequisites:
    - payment-gateway
    milestones:
    - name: Plans & Pricing
      description: Implement flexible pricing plans with tiers, features, and currencies
      skills:
      - Pricing models
      - Feature flags
      - Multi-currency
      hints:
        level1: Plans define pricing; subscriptions track who's on what plan
        level2: Support multiple billing intervals (monthly, yearly) with different prices
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          from decimal import Decimal
          import time

          class BillingInterval(Enum):
              DAILY = "daily"
              WEEKLY = "weekly"
              MONTHLY = "monthly"
              YEARLY = "yearly"

          class PricingModel(Enum):
              FLAT_RATE = "flat_rate"           # Fixed price
              PER_SEAT = "per_seat"             # Price Ã— number of users
              TIERED = "tiered"                 # Different rates at volume tiers
              VOLUME = "volume"                 # Single rate based on total volume
              USAGE_BASED = "usage_based"       # Pay for what you use

          @dataclass
          class PriceTier:
              up_to: Optional[int]  # None = unlimited
              unit_amount: int      # Price per unit in cents
              flat_amount: int = 0  # Optional flat fee for this tier

          @dataclass
          class Price:
              id: str
              plan_id: str
              currency: str
              unit_amount: int                          # Base price in cents
              billing_interval: BillingInterval
              interval_count: int = 1                   # Every N intervals
              pricing_model: PricingModel = PricingModel.FLAT_RATE
              tiers: list[PriceTier] = field(default_factory=list)
              trial_period_days: int = 0
              metadata: dict = field(default_factory=dict)

          @dataclass
          class Feature:
              id: str
              name: str
              description: str
              value_type: str = "boolean"  # boolean, number, unlimited
              default_value: any = False

          @dataclass
          class Plan:
              id: str
              name: str
              description: str
              prices: dict[str, Price]   # currency -> Price
              features: dict[str, any]   # feature_id -> value
              active: bool = True
              metadata: dict = field(default_factory=dict)

          class PricingEngine:
              def __init__(self):
                  self.plans: dict[str, Plan] = {}
                  self.features: dict[str, Feature] = {}

              def create_plan(self, plan_id: str, name: str, description: str,
                              base_price: int, currency: str = "usd",
                              interval: BillingInterval = BillingInterval.MONTHLY,
                              features: dict = None,
                              trial_days: int = 0) -> Plan:
                  price = Price(
                      id=f"price_{plan_id}_{currency}",
                      plan_id=plan_id,
                      currency=currency,
                      unit_amount=base_price,
                      billing_interval=interval,
                      trial_period_days=trial_days
                  )

                  plan = Plan(
                      id=plan_id,
                      name=name,
                      description=description,
                      prices={currency: price},
                      features=features or {}
                  )

                  self.plans[plan_id] = plan
                  return plan

              def add_tiered_pricing(self, plan_id: str, currency: str,
                                     tiers: list[dict], model: PricingModel):
                  '''Add tiered or volume pricing to a plan'''
                  plan = self.plans.get(plan_id)
                  if not plan:
                      raise ValueError("Plan not found")

                  price = plan.prices.get(currency)
                  if not price:
                      raise ValueError("Price not found for currency")

                  price.pricing_model = model
                  price.tiers = [
                      PriceTier(
                          up_to=t.get("up_to"),
                          unit_amount=t["unit_amount"],
                          flat_amount=t.get("flat_amount", 0)
                      )
                      for t in tiers
                  ]

              def calculate_price(self, plan_id: str, currency: str,
                                  quantity: int = 1) -> dict:
                  '''Calculate price for given quantity'''
                  plan = self.plans.get(plan_id)
                  if not plan:
                      raise ValueError("Plan not found")

                  price = plan.prices.get(currency)
                  if not price:
                      raise ValueError("Price not found for currency")

                  if price.pricing_model == PricingModel.FLAT_RATE:
                      return {
                          "subtotal": price.unit_amount,
                          "quantity": 1,
                          "unit_amount": price.unit_amount
                      }

                  elif price.pricing_model == PricingModel.PER_SEAT:
                      return {
                          "subtotal": price.unit_amount * quantity,
                          "quantity": quantity,
                          "unit_amount": price.unit_amount
                      }

                  elif price.pricing_model == PricingModel.TIERED:
                      # Each tier applies to units within that tier
                      total = 0
                      remaining = quantity
                      breakdown = []

                      for tier in sorted(price.tiers, key=lambda t: t.up_to or float('inf')):
                          if remaining <= 0:
                              break

                          tier_max = tier.up_to or float('inf')
                          prev_max = breakdown[-1]["up_to"] if breakdown else 0
                          tier_quantity = min(remaining, tier_max - prev_max)

                          tier_amount = tier.flat_amount + (tier.unit_amount * tier_quantity)
                          total += tier_amount
                          remaining -= tier_quantity

                          breakdown.append({
                              "up_to": tier.up_to,
                              "quantity": tier_quantity,
                              "unit_amount": tier.unit_amount,
                              "amount": tier_amount
                          })

                      return {
                          "subtotal": total,
                          "quantity": quantity,
                          "breakdown": breakdown
                      }

                  elif price.pricing_model == PricingModel.VOLUME:
                      # Single rate based on total quantity (find applicable tier)
                      applicable_tier = None
                      for tier in sorted(price.tiers, key=lambda t: t.up_to or float('inf')):
                          if tier.up_to is None or quantity <= tier.up_to:
                              applicable_tier = tier
                              break

                      if not applicable_tier:
                          applicable_tier = price.tiers[-1]

                      total = applicable_tier.flat_amount + (applicable_tier.unit_amount * quantity)
                      return {
                          "subtotal": total,
                          "quantity": quantity,
                          "unit_amount": applicable_tier.unit_amount,
                          "tier_up_to": applicable_tier.up_to
                      }

                  raise ValueError(f"Unknown pricing model: {price.pricing_model}")

              def compare_plans(self, plan_id_1: str, plan_id_2: str) -> dict:
                  '''Compare features between two plans'''
                  plan1 = self.plans.get(plan_id_1)
                  plan2 = self.plans.get(plan_id_2)

                  if not plan1 or not plan2:
                      raise ValueError("Plan not found")

                  comparison = {}
                  all_features = set(plan1.features.keys()) | set(plan2.features.keys())

                  for feature_id in all_features:
                      comparison[feature_id] = {
                          plan_id_1: plan1.features.get(feature_id),
                          plan_id_2: plan2.features.get(feature_id)
                      }

                  return comparison
          ```
      pitfalls:
      - 'Tiered vs Volume pricing: tiered charges each tier separately; volume uses single rate'
      - 'Currency precision: always use smallest unit (cents) to avoid float errors'
      - 'Plan changes: archive old plans rather than deleting (existing subscribers)'
      - 'Feature access: check plan features, not subscription status alone'
      acceptance_criteria:
      - Define subscription plans with tiers, billing intervals, and pricing amounts
      - Support multiple pricing models (flat, tiered)
      - Handle plan versioning so existing subscribers keep their original plan terms
      - Support trial periods with automatic conversion to paid at expiry
      deliverables:
      - Plan definition schema supporting name, interval, currency, and feature list
      - Pricing tier configuration supporting flat-rate, per-unit, and tiered models
      - Feature entitlement matrix mapping plan tiers to accessible feature flags
      - Plan management API for creating, updating, and deprecating subscription plans
    - name: Subscription Lifecycle
      description: Implement subscription creation, activation, renewal, and cancellation
      skills:
      - State machines
      - Billing cycles
      - Grace periods
      hints:
        level1: 'Subscription has lifecycle: trialing -> active -> past_due -> canceled'
        level2: Track current_period_start/end for billing; anchor date for consistent billing
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          from datetime import datetime, timedelta
          import calendar

          class SubscriptionStatus(Enum):
              TRIALING = "trialing"
              ACTIVE = "active"
              PAST_DUE = "past_due"      # Payment failed, retrying
              UNPAID = "unpaid"          # Payment failed, no more retries
              CANCELED = "canceled"
              INCOMPLETE = "incomplete"   # Initial payment failed
              INCOMPLETE_EXPIRED = "incomplete_expired"

          class CancellationReason(Enum):
              CUSTOMER_REQUEST = "customer_request"
              PAYMENT_FAILURE = "payment_failure"
              FRAUD = "fraud"

          @dataclass
          class Subscription:
              id: str
              customer_id: str
              plan_id: str
              price_id: str
              status: SubscriptionStatus
              quantity: int
              current_period_start: float
              current_period_end: float
              trial_start: Optional[float]
              trial_end: Optional[float]
              canceled_at: Optional[float]
              cancel_at_period_end: bool
              ended_at: Optional[float]
              billing_cycle_anchor: float
              created_at: float
              metadata: dict = field(default_factory=dict)

          class SubscriptionManager:
              def __init__(self, pricing: PricingEngine,
                           payment_service):  # PaymentService from payment project
                  self.pricing = pricing
                  self.payments = payment_service
                  self.subscriptions: dict[str, Subscription] = {}

              def create_subscription(self, customer_id: str, plan_id: str,
                                      currency: str = "usd",
                                      quantity: int = 1,
                                      trial_from_plan: bool = True,
                                      billing_cycle_anchor: int = None) -> Subscription:
                  plan = self.pricing.plans.get(plan_id)
                  if not plan:
                      raise ValueError("Plan not found")

                  price = plan.prices.get(currency)
                  if not price:
                      raise ValueError("Price not found")

                  now = time.time()
                  sub_id = f"sub_{secrets.token_urlsafe(16)}"

                  # Determine trial period
                  trial_end = None
                  if trial_from_plan and price.trial_period_days > 0:
                      trial_end = now + (price.trial_period_days * 86400)

                  # Set billing anchor (day of month for monthly, etc.)
                  anchor = billing_cycle_anchor or now

                  # Calculate initial period
                  if trial_end:
                      period_start = now
                      period_end = trial_end
                      status = SubscriptionStatus.TRIALING
                  else:
                      period_start = now
                      period_end = self._calculate_period_end(
                          now, price.billing_interval, price.interval_count
                      )
                      status = SubscriptionStatus.INCOMPLETE

                  subscription = Subscription(
                      id=sub_id,
                      customer_id=customer_id,
                      plan_id=plan_id,
                      price_id=price.id,
                      status=status,
                      quantity=quantity,
                      current_period_start=period_start,
                      current_period_end=period_end,
                      trial_start=now if trial_end else None,
                      trial_end=trial_end,
                      canceled_at=None,
                      cancel_at_period_end=False,
                      ended_at=None,
                      billing_cycle_anchor=anchor,
                      created_at=now
                  )

                  self.subscriptions[sub_id] = subscription

                  # If no trial, create initial payment
                  if not trial_end:
                      self._create_initial_invoice(subscription)

                  return subscription

              def _calculate_period_end(self, start: float,
                                        interval: BillingInterval,
                                        interval_count: int) -> float:
                  dt = datetime.fromtimestamp(start)

                  if interval == BillingInterval.DAILY:
                      end = dt + timedelta(days=interval_count)
                  elif interval == BillingInterval.WEEKLY:
                      end = dt + timedelta(weeks=interval_count)
                  elif interval == BillingInterval.MONTHLY:
                      # Add months, preserving day of month (or last day if overflow)
                      month = dt.month + interval_count
                      year = dt.year + (month - 1) // 12
                      month = (month - 1) % 12 + 1
                      day = min(dt.day, calendar.monthrange(year, month)[1])
                      end = dt.replace(year=year, month=month, day=day)
                  elif interval == BillingInterval.YEARLY:
                      try:
                          end = dt.replace(year=dt.year + interval_count)
                      except ValueError:
                          # Feb 29 -> Feb 28
                          end = dt.replace(year=dt.year + interval_count, day=28)

                  return end.timestamp()

              def cancel_subscription(self, subscription_id: str,
                                      at_period_end: bool = True,
                                      reason: CancellationReason = CancellationReason.CUSTOMER_REQUEST) -> Subscription:
                  sub = self.subscriptions.get(subscription_id)
                  if not sub:
                      raise ValueError("Subscription not found")

                  if sub.status in [SubscriptionStatus.CANCELED,
                                    SubscriptionStatus.INCOMPLETE_EXPIRED]:
                      raise ValueError("Subscription already canceled")

                  sub.canceled_at = time.time()

                  if at_period_end:
                      # Cancel at end of billing period (most common)
                      sub.cancel_at_period_end = True
                  else:
                      # Immediate cancellation
                      sub.status = SubscriptionStatus.CANCELED
                      sub.ended_at = time.time()

                  return sub

              def reactivate_subscription(self, subscription_id: str) -> Subscription:
                  '''Reactivate a subscription scheduled for cancellation'''
                  sub = self.subscriptions.get(subscription_id)
                  if not sub:
                      raise ValueError("Subscription not found")

                  if not sub.cancel_at_period_end:
                      raise ValueError("Subscription not scheduled for cancellation")

                  if sub.status == SubscriptionStatus.CANCELED:
                      raise ValueError("Cannot reactivate fully canceled subscription")

                  sub.cancel_at_period_end = False
                  sub.canceled_at = None

                  return sub

              def renew_subscription(self, subscription_id: str) -> Subscription:
                  '''Process subscription renewal (called by billing job)'''
                  sub = self.subscriptions.get(subscription_id)
                  if not sub:
                      raise ValueError("Subscription not found")

                  # Check if should be canceled
                  if sub.cancel_at_period_end:
                      sub.status = SubscriptionStatus.CANCELED
                      sub.ended_at = time.time()
                      return sub

                  # Create invoice for new period
                  plan = self.pricing.plans[sub.plan_id]
                  price = plan.prices[sub.price_id.split("_")[1]]  # Extract currency

                  # Update period
                  sub.current_period_start = sub.current_period_end
                  sub.current_period_end = self._calculate_period_end(
                      sub.current_period_start,
                      price.billing_interval,
                      price.interval_count
                  )

                  # Create payment
                  self._create_renewal_invoice(sub)

                  return sub

              def _create_initial_invoice(self, sub: Subscription):
                  plan = self.pricing.plans[sub.plan_id]
                  price_info = self.pricing.calculate_price(
                      sub.plan_id, "usd", sub.quantity
                  )

                  # In production: create invoice, attempt payment
                  # If payment succeeds: sub.status = ACTIVE
                  # If payment fails: sub.status = INCOMPLETE

              def _create_renewal_invoice(self, sub: Subscription):
                  # Similar to initial, but with retry logic for failures
                  pass
          ```
      pitfalls:
      - 'Billing anchor: ensures consistent billing date each month'
      - 'Past due vs unpaid: past_due = still retrying; unpaid = gave up'
      - 'Cancel at period end: most user-friendly, they paid for the period'
      - 'Month overflow: Jan 31 + 1 month = Feb 28, not Mar 3'
      acceptance_criteria:
      - Create subscriptions for customers with selected plan and payment method
      - Handle upgrades and downgrades with appropriate effective date logic
      - Process cancellations recording reason and scheduling termination date
      - Handle subscription pausing and resuming with prorated billing adjustment
      deliverables:
      - Subscription creation service provisioning new subscriptions for customers
      - Renewal processing engine generating invoices at each billing cycle boundary
      - Cancellation handler supporting immediate and end-of-period termination options
      - Reactivation service restoring cancelled subscriptions within a grace period
    - name: Proration & Plan Changes
      description: Implement upgrade/downgrade with prorated charges and credits
      skills:
      - Proration calculation
      - Credits
      - Mid-cycle changes
      hints:
        level1: 'Proration: charge/credit proportionally for unused time'
        level2: Upgrade immediately charges difference; downgrade creates credit for next invoice
        level3: |2

          ```python
          from dataclasses import dataclass
          from enum import Enum
          from decimal import Decimal, ROUND_HALF_UP

          class ProrationBehavior(Enum):
              CREATE_PRORATIONS = "create_prorations"   # Immediate adjustment
              NONE = "none"                              # No proration
              ALWAYS_INVOICE = "always_invoice"          # Invoice immediately

          @dataclass
          class ProrationItem:
              description: str
              amount: int           # Positive = charge, negative = credit
              quantity: int
              period_start: float
              period_end: float

          class ProrationCalculator:
              def calculate_proration(self, old_price: int, new_price: int,
                                      remaining_days: int, total_days: int,
                                      old_quantity: int = 1,
                                      new_quantity: int = 1) -> list[ProrationItem]:
                  '''
                  Calculate proration items for plan change.

                  Returns list of line items:
                  - Credit for unused time on old plan
                  - Charge for remaining time on new plan
                  '''
                  items = []

                  # Credit for unused time on old plan
                  daily_old = Decimal(old_price * old_quantity) / total_days
                  credit = int((daily_old * remaining_days).quantize(
                      Decimal('1'), rounding=ROUND_HALF_UP
                  ))

                  if credit > 0:
                      items.append(ProrationItem(
                          description="Unused time on previous plan",
                          amount=-credit,  # Negative = credit
                          quantity=old_quantity,
                          period_start=0,  # Will be filled in
                          period_end=0
                      ))

                  # Charge for remaining time on new plan
                  daily_new = Decimal(new_price * new_quantity) / total_days
                  charge = int((daily_new * remaining_days).quantize(
                      Decimal('1'), rounding=ROUND_HALF_UP
                  ))

                  if charge > 0:
                      items.append(ProrationItem(
                          description="Remaining time on new plan",
                          amount=charge,  # Positive = charge
                          quantity=new_quantity,
                          period_start=0,
                          period_end=0
                      ))

                  return items

          class PlanChangeManager:
              def __init__(self, subscription_manager: SubscriptionManager,
                           pricing: PricingEngine):
                  self.subscriptions = subscription_manager
                  self.pricing = pricing
                  self.proration_calc = ProrationCalculator()
                  self.customer_credits: dict[str, int] = {}  # customer_id -> credit balance

              def preview_change(self, subscription_id: str, new_plan_id: str,
                                 new_quantity: int = None) -> dict:
                  '''Preview what charges/credits would result from plan change'''
                  sub = self.subscriptions.subscriptions.get(subscription_id)
                  if not sub:
                      raise ValueError("Subscription not found")

                  old_plan = self.pricing.plans[sub.plan_id]
                  new_plan = self.pricing.plans.get(new_plan_id)
                  if not new_plan:
                      raise ValueError("New plan not found")

                  currency = "usd"  # Simplified
                  old_price_info = self.pricing.calculate_price(
                      sub.plan_id, currency, sub.quantity
                  )
                  new_quantity = new_quantity or sub.quantity
                  new_price_info = self.pricing.calculate_price(
                      new_plan_id, currency, new_quantity
                  )

                  # Calculate remaining time in period
                  now = time.time()
                  total_seconds = sub.current_period_end - sub.current_period_start
                  remaining_seconds = sub.current_period_end - now
                  total_days = int(total_seconds / 86400)
                  remaining_days = int(remaining_seconds / 86400)

                  items = self.proration_calc.calculate_proration(
                      old_price_info["subtotal"],
                      new_price_info["subtotal"],
                      remaining_days,
                      total_days,
                      sub.quantity,
                      new_quantity
                  )

                  total_due = sum(item.amount for item in items)

                  return {
                      "items": [
                          {
                              "description": item.description,
                              "amount": item.amount
                          }
                          for item in items
                      ],
                      "total_due": total_due,
                      "is_upgrade": new_price_info["subtotal"] > old_price_info["subtotal"],
                      "immediate_charge": total_due if total_due > 0 else 0,
                      "credit_applied": abs(total_due) if total_due < 0 else 0
                  }

              def change_plan(self, subscription_id: str, new_plan_id: str,
                              new_quantity: int = None,
                              proration_behavior: ProrationBehavior = ProrationBehavior.CREATE_PRORATIONS) -> Subscription:
                  '''Execute plan change with proration'''
                  sub = self.subscriptions.subscriptions.get(subscription_id)
                  if not sub:
                      raise ValueError("Subscription not found")

                  preview = self.preview_change(subscription_id, new_plan_id, new_quantity)

                  if proration_behavior == ProrationBehavior.CREATE_PRORATIONS:
                      if preview["total_due"] > 0:
                          # Upgrade: charge immediately
                          # In production: create payment intent and process
                          print(f"Charging ${preview['total_due']/100:.2f} for upgrade")
                      elif preview["total_due"] < 0:
                          # Downgrade: apply credit
                          credit = abs(preview["total_due"])
                          self.customer_credits[sub.customer_id] = (
                              self.customer_credits.get(sub.customer_id, 0) + credit
                          )
                          print(f"Applied ${credit/100:.2f} credit for downgrade")

                  # Update subscription
                  new_plan = self.pricing.plans[new_plan_id]
                  new_price = new_plan.prices.get("usd")

                  sub.plan_id = new_plan_id
                  sub.price_id = new_price.id
                  sub.quantity = new_quantity or sub.quantity

                  return sub

              def change_quantity(self, subscription_id: str,
                                  new_quantity: int) -> Subscription:
                  '''Change subscription quantity (seats)'''
                  sub = self.subscriptions.subscriptions.get(subscription_id)
                  if not sub:
                      raise ValueError("Subscription not found")

                  return self.change_plan(subscription_id, sub.plan_id, new_quantity)

              def apply_credit_to_invoice(self, customer_id: str,
                                          invoice_amount: int) -> tuple[int, int]:
                  '''Apply customer credit balance to invoice'''
                  credit = self.customer_credits.get(customer_id, 0)
                  if credit <= 0:
                      return invoice_amount, 0

                  credit_applied = min(credit, invoice_amount)
                  remaining_due = invoice_amount - credit_applied
                  self.customer_credits[customer_id] = credit - credit_applied

                  return remaining_due, credit_applied
          ```
      pitfalls:
      - 'Proration direction: upgrade charges extra; downgrade credits'
      - Round proration carefully - small errors accumulate over many customers
      - Credits must be applied automatically on next invoice
      - Quantity changes are proration too - not just plan changes
      acceptance_criteria:
      - Calculate prorated charges on upgrade based on remaining days in cycle
      - Calculate credits on downgrade for unused portion of higher-tier plan
      - Handle mid-cycle plan changes with correct effective date and amount
      - Support immediate versus end-of-cycle plan change scheduling options
      deliverables:
      - Proration calculation engine computing partial charges for mid-cycle changes
      - Upgrade and downgrade handler adjusting entitlements and billing amounts
      - Credit application service applying unused-time credits to the customer balance
      - Mid-cycle change processor handling plan switches at any point in the billing period
    - name: Usage-Based Billing
      description: Implement metered billing with usage tracking, aggregation, and reporting
      skills:
      - Usage metering
      - Aggregation
      - Rate limiting
      hints:
        level1: Track usage events in real-time; aggregate for billing at period end
        level2: 'Usage can be: sum, max, last, unique count over billing period'
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          from collections import defaultdict
          import time

          class AggregationType(Enum):
              SUM = "sum"              # Total usage (API calls, storage GB)
              MAX = "max"              # Peak usage (concurrent users)
              LAST = "last"            # Final value (current storage)
              UNIQUE_COUNT = "unique"  # Unique values (active users)

          @dataclass
          class Meter:
              id: str
              name: str
              event_name: str                    # e.g., "api_call", "storage_byte"
              aggregation: AggregationType
              unit_label: str                    # e.g., "API calls", "GB"
              filter_expression: Optional[str]   # Filter which events count

          @dataclass
          class UsageRecord:
              meter_id: str
              subscription_id: str
              timestamp: float
              quantity: int
              properties: dict                   # Additional metadata

          @dataclass
          class UsageSummary:
              meter_id: str
              subscription_id: str
              period_start: float
              period_end: float
              total_quantity: int
              billable_quantity: int
              unit_amount: int
              total_amount: int

          class UsageTracker:
              def __init__(self):
                  self.meters: dict[str, Meter] = {}
                  self.usage_records: list[UsageRecord] = []
                  # Indexed for fast lookup
                  self.usage_by_sub: dict[str, dict[str, list[UsageRecord]]] = defaultdict(
                      lambda: defaultdict(list)
                  )  # subscription_id -> meter_id -> records

              def create_meter(self, meter_id: str, name: str, event_name: str,
                               aggregation: AggregationType,
                               unit_label: str = "units") -> Meter:
                  meter = Meter(
                      id=meter_id,
                      name=name,
                      event_name=event_name,
                      aggregation=aggregation,
                      unit_label=unit_label,
                      filter_expression=None
                  )
                  self.meters[meter_id] = meter
                  return meter

              def record_usage(self, subscription_id: str, meter_id: str,
                               quantity: int = 1,
                               timestamp: float = None,
                               properties: dict = None,
                               idempotency_key: str = None):
                  '''Record a usage event'''
                  meter = self.meters.get(meter_id)
                  if not meter:
                      raise ValueError("Meter not found")

                  record = UsageRecord(
                      meter_id=meter_id,
                      subscription_id=subscription_id,
                      timestamp=timestamp or time.time(),
                      quantity=quantity,
                      properties=properties or {}
                  )

                  # In production: check idempotency key to prevent duplicates
                  self.usage_records.append(record)
                  self.usage_by_sub[subscription_id][meter_id].append(record)

              def get_usage_summary(self, subscription_id: str, meter_id: str,
                                    period_start: float,
                                    period_end: float) -> UsageSummary:
                  '''Aggregate usage for a billing period'''
                  meter = self.meters.get(meter_id)
                  if not meter:
                      raise ValueError("Meter not found")

                  records = [
                      r for r in self.usage_by_sub[subscription_id][meter_id]
                      if period_start <= r.timestamp < period_end
                  ]

                  if not records:
                      total = 0
                  elif meter.aggregation == AggregationType.SUM:
                      total = sum(r.quantity for r in records)
                  elif meter.aggregation == AggregationType.MAX:
                      total = max(r.quantity for r in records)
                  elif meter.aggregation == AggregationType.LAST:
                      latest = max(records, key=lambda r: r.timestamp)
                      total = latest.quantity
                  elif meter.aggregation == AggregationType.UNIQUE_COUNT:
                      # Count unique values of a property (e.g., user_id)
                      unique_values = set()
                      for r in records:
                          unique_values.add(r.properties.get("unique_key", r.quantity))
                      total = len(unique_values)

                  return UsageSummary(
                      meter_id=meter_id,
                      subscription_id=subscription_id,
                      period_start=period_start,
                      period_end=period_end,
                      total_quantity=total,
                      billable_quantity=total,  # May differ with included amounts
                      unit_amount=0,  # Set by billing
                      total_amount=0
                  )

          class UsageBilling:
              def __init__(self, usage_tracker: UsageTracker, pricing: PricingEngine):
                  self.usage = usage_tracker
                  self.pricing = pricing

              def calculate_usage_charges(self, subscription_id: str,
                                          period_start: float,
                                          period_end: float) -> list[dict]:
                  '''Calculate usage charges for all meters'''
                  charges = []

                  for meter_id in self.usage.meters:
                      summary = self.usage.get_usage_summary(
                          subscription_id, meter_id, period_start, period_end
                      )

                      if summary.total_quantity == 0:
                          continue

                      # In production: get meter-specific pricing from plan
                      # This is simplified
                      unit_price = 10  # $0.10 per unit

                      charge = {
                          "meter_id": meter_id,
                          "meter_name": self.usage.meters[meter_id].name,
                          "quantity": summary.total_quantity,
                          "unit_amount": unit_price,
                          "total_amount": summary.total_quantity * unit_price,
                          "unit_label": self.usage.meters[meter_id].unit_label
                      }
                      charges.append(charge)

                  return charges

              def get_current_usage(self, subscription_id: str) -> dict:
                  '''Get real-time usage for customer dashboard'''
                  # In production: might need current period from subscription
                  now = time.time()
                  period_start = now - (30 * 86400)  # Last 30 days approximation

                  usage = {}
                  for meter_id, meter in self.usage.meters.items():
                      summary = self.usage.get_usage_summary(
                          subscription_id, meter_id, period_start, now
                      )
                      usage[meter_id] = {
                          "name": meter.name,
                          "quantity": summary.total_quantity,
                          "unit_label": meter.unit_label
                      }

                  return usage

          # Example usage tracking middleware
          class UsageMiddleware:
              def __init__(self, tracker: UsageTracker):
                  self.tracker = tracker

              def track_api_call(self, subscription_id: str, endpoint: str):
                  '''Track an API call'''
                  self.tracker.record_usage(
                      subscription_id=subscription_id,
                      meter_id="api_calls",
                      quantity=1,
                      properties={"endpoint": endpoint}
                  )

              def track_storage(self, subscription_id: str, bytes_used: int):
                  '''Track storage usage (last value aggregation)'''
                  self.tracker.record_usage(
                      subscription_id=subscription_id,
                      meter_id="storage",
                      quantity=bytes_used,  # Total storage, not delta
                  )

              def track_active_user(self, subscription_id: str, user_id: str):
                  '''Track unique active user'''
                  self.tracker.record_usage(
                      subscription_id=subscription_id,
                      meter_id="active_users",
                      quantity=1,
                      properties={"unique_key": user_id}
                  )
          ```
      pitfalls:
      - 'Idempotency: same event reported twice shouldn''t be double-counted'
      - 'Clock skew: use server timestamps, not client timestamps'
      - 'Aggregation type matters: sum vs max give very different results'
      - 'Usage limits: alert before hitting limit, not after'
      acceptance_criteria:
      - Track usage events per subscription with idempotent event ingestion
      - Aggregate usage for billing period with correct start and end boundaries
      - Calculate charges based on usage tiers
      - Handle usage overage by applying the correct per-unit overage rate
      deliverables:
      - Usage tracking service recording metered events with timestamps and quantities
      - Metering API accepting usage reports and deduplicating repeated submissions
      - Usage aggregation engine totaling consumption per subscription per billing period
      - Overage charge calculator applying tiered rates when usage exceeds plan allowance
  notification-service:
    name: Multi-Channel Notification Service
    description: Build a unified notification system supporting email, SMS, push notifications, and in-app messages with templates,
      preferences, and delivery tracking.
    why_expert: Every app needs notifications. Understanding delivery patterns, rate limiting, and preference management helps
      build user-friendly communication systems.
    difficulty: expert
    tags:
    - notifications
    - messaging
    - email
    - push
    - sms
    estimated_hours: 45
    prerequisites:
    - build-message-queue
    milestones:
    - name: Channel Abstraction & Routing
      description: Implement pluggable channels (email, SMS, push) with intelligent routing
      skills:
      - Channel abstraction
      - Provider fallback
      - Routing logic
      hints:
        level1: Each channel (email, SMS, push) is a plugin implementing common interface
        level2: Router decides which channel(s) based on notification type and user preferences
        level3: |2

          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          import time

          class Channel(Enum):
              EMAIL = "email"
              SMS = "sms"
              PUSH = "push"
              IN_APP = "in_app"
              WEBHOOK = "webhook"

          class Priority(Enum):
              LOW = "low"
              NORMAL = "normal"
              HIGH = "high"
              URGENT = "urgent"

          @dataclass
          class Notification:
              id: str
              user_id: str
              type: str                     # e.g., "order_shipped", "password_reset"
              title: str
              body: str
              data: dict = field(default_factory=dict)
              channels: list[Channel] = field(default_factory=list)
              priority: Priority = Priority.NORMAL
              created_at: float = field(default_factory=time.time)
              scheduled_for: Optional[float] = None
              expires_at: Optional[float] = None

          @dataclass
          class DeliveryResult:
              channel: Channel
              success: bool
              provider: str
              message_id: Optional[str] = None
              error: Optional[str] = None
              delivered_at: Optional[float] = None

          class NotificationChannel(ABC):
              @abstractmethod
              def send(self, notification: Notification, recipient: dict) -> DeliveryResult:
                  pass

              @abstractmethod
              def supports_batch(self) -> bool:
                  pass

          class EmailChannel(NotificationChannel):
              def __init__(self, providers: list):
                  self.providers = providers  # Ordered by preference
                  self.current_provider_idx = 0

              def send(self, notification: Notification, recipient: dict) -> DeliveryResult:
                  email = recipient.get("email")
                  if not email:
                      return DeliveryResult(
                          channel=Channel.EMAIL,
                          success=False,
                          provider="none",
                          error="No email address"
                      )

                  # Try providers with fallback
                  for i, provider in enumerate(self.providers[self.current_provider_idx:]):
                      try:
                          result = provider.send_email(
                              to=email,
                              subject=notification.title,
                              body=notification.body,
                              html=notification.data.get("html_body")
                          )
                          return DeliveryResult(
                              channel=Channel.EMAIL,
                              success=True,
                              provider=provider.name,
                              message_id=result.message_id,
                              delivered_at=time.time()
                          )
                      except Exception as e:
                          if i < len(self.providers) - 1:
                              continue  # Try next provider
                          return DeliveryResult(
                              channel=Channel.EMAIL,
                              success=False,
                              provider=provider.name,
                              error=str(e)
                          )

              def supports_batch(self) -> bool:
                  return True

          class SMSChannel(NotificationChannel):
              def __init__(self, provider):
                  self.provider = provider

              def send(self, notification: Notification, recipient: dict) -> DeliveryResult:
                  phone = recipient.get("phone")
                  if not phone:
                      return DeliveryResult(
                          channel=Channel.SMS,
                          success=False,
                          provider="none",
                          error="No phone number"
                      )

                  try:
                      # SMS is expensive - keep messages short
                      message = notification.body[:160]  # SMS character limit

                      result = self.provider.send_sms(to=phone, body=message)
                      return DeliveryResult(
                          channel=Channel.SMS,
                          success=True,
                          provider=self.provider.name,
                          message_id=result.sid
                      )
                  except Exception as e:
                      return DeliveryResult(
                          channel=Channel.SMS,
                          success=False,
                          provider=self.provider.name,
                          error=str(e)
                      )

              def supports_batch(self) -> bool:
                  return False

          class PushChannel(NotificationChannel):
              def __init__(self, fcm_client, apns_client):
                  self.fcm = fcm_client      # Firebase for Android
                  self.apns = apns_client    # Apple Push for iOS

              def send(self, notification: Notification, recipient: dict) -> DeliveryResult:
                  device_tokens = recipient.get("device_tokens", [])
                  if not device_tokens:
                      return DeliveryResult(
                          channel=Channel.PUSH,
                          success=False,
                          provider="none",
                          error="No device tokens"
                      )

                  errors = []
                  for token in device_tokens:
                      try:
                          platform = token.get("platform", "android")
                          push_token = token.get("token")

                          if platform == "ios":
                              self.apns.send(
                                  token=push_token,
                                  title=notification.title,
                                  body=notification.body,
                                  data=notification.data
                              )
                          else:
                              self.fcm.send(
                                  token=push_token,
                                  title=notification.title,
                                  body=notification.body,
                                  data=notification.data
                              )
                      except Exception as e:
                          errors.append(str(e))

                  if len(errors) == len(device_tokens):
                      return DeliveryResult(
                          channel=Channel.PUSH,
                          success=False,
                          provider="fcm/apns",
                          error="; ".join(errors)
                      )

                  return DeliveryResult(
                      channel=Channel.PUSH,
                      success=True,
                      provider="fcm/apns",
                      delivered_at=time.time()
                  )

              def supports_batch(self) -> bool:
                  return True

          class NotificationRouter:
              def __init__(self):
                  self.channels: dict[Channel, NotificationChannel] = {}
                  self.type_channel_map: dict[str, list[Channel]] = {}
                  self.priority_channel_map: dict[Priority, list[Channel]] = {
                      Priority.URGENT: [Channel.SMS, Channel.PUSH, Channel.EMAIL],
                      Priority.HIGH: [Channel.PUSH, Channel.EMAIL],
                      Priority.NORMAL: [Channel.EMAIL, Channel.IN_APP],
                      Priority.LOW: [Channel.IN_APP]
                  }

              def register_channel(self, channel: Channel, implementation: NotificationChannel):
                  self.channels[channel] = implementation

              def configure_notification_type(self, notification_type: str, channels: list[Channel]):
                  self.type_channel_map[notification_type] = channels

              def route(self, notification: Notification, user_preferences: dict) -> list[Channel]:
                  '''Determine which channels to use for a notification'''

                  # 1. Check if channels explicitly specified
                  if notification.channels:
                      return notification.channels

                  # 2. Check notification type configuration
                  if notification.type in self.type_channel_map:
                      channels = self.type_channel_map[notification.type]
                  else:
                      # 3. Fall back to priority-based routing
                      channels = self.priority_channel_map.get(
                          notification.priority,
                          [Channel.EMAIL]
                      )

                  # 4. Filter by user preferences
                  filtered = []
                  for channel in channels:
                      pref_key = f"{channel.value}_enabled"
                      if user_preferences.get(pref_key, True):  # Default enabled
                          filtered.append(channel)

                  # 5. Check quiet hours (don't SMS/Push during sleep)
                  if self._is_quiet_hours(user_preferences):
                      filtered = [c for c in filtered if c not in [Channel.SMS, Channel.PUSH]]

                  return filtered or [Channel.IN_APP]  # Always fallback to in-app

              def _is_quiet_hours(self, preferences: dict) -> bool:
                  quiet_start = preferences.get("quiet_hours_start")  # e.g., 22 (10 PM)
                  quiet_end = preferences.get("quiet_hours_end")      # e.g., 8 (8 AM)

                  if not quiet_start or not quiet_end:
                      return False

                  from datetime import datetime
                  hour = datetime.now().hour

                  if quiet_start < quiet_end:
                      return quiet_start <= hour < quiet_end
                  else:  # Crosses midnight
                      return hour >= quiet_start or hour < quiet_end
          ```
      pitfalls:
      - Provider fallback must track failures to avoid cascading errors
      - SMS costs money - only use for truly urgent notifications
      - Push token can become invalid - handle token refresh errors
      - Quiet hours must consider user's timezone, not server's
      acceptance_criteria:
      - Channel interface defines a common contract for send, validate, and format across all notification backends
      - Notifications are routed to the appropriate channel based on notification type and user preferences
      - Channel-specific formatting transforms the notification payload into the format required by each backend
      - Fallback channels are attempted in order when the primary channel fails to deliver the notification
      deliverables:
      - Channel interface defining send, validate, and format methods for email, SMS, and push notification backends
      - Concrete channel implementations for email via SMTP, SMS via Twilio, and push via FCM or APNs
      - Routing rules engine that selects the appropriate channel based on notification type and user preference
      - Fallback channel configuration that tries alternative delivery channels when the primary channel fails
    - name: Template System
      description: Implement notification templates with localization and personalization
      skills:
      - Template engines
      - i18n
      - Content personalization
      hints:
        level1: Templates separate content from delivery logic - easier to update
        level2: Support different content per channel (email HTML vs SMS text)
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from jinja2 import Environment, BaseLoader, TemplateNotFound
          import json

          @dataclass
          class NotificationTemplate:
              id: str
              name: str
              type: str                    # Notification type this template is for
              channel_content: dict        # channel -> {subject, body, html_body}
              variables: list[str]         # Required variables
              default_locale: str = "en"
              localized_content: dict = field(default_factory=dict)  # locale -> channel_content

          class TemplateStore:
              def __init__(self):
                  self.templates: dict[str, NotificationTemplate] = {}

              def create_template(self, template_id: str, name: str, notification_type: str,
                                  channel_content: dict, variables: list[str]) -> NotificationTemplate:
                  template = NotificationTemplate(
                      id=template_id,
                      name=name,
                      type=notification_type,
                      channel_content=channel_content,
                      variables=variables
                  )
                  self.templates[template_id] = template
                  return template

              def add_localization(self, template_id: str, locale: str, channel_content: dict):
                  template = self.templates.get(template_id)
                  if not template:
                      raise ValueError("Template not found")
                  template.localized_content[locale] = channel_content

              def get_template(self, template_id: str, locale: str = None) -> dict:
                  template = self.templates.get(template_id)
                  if not template:
                      raise ValueError("Template not found")

                  if locale and locale in template.localized_content:
                      return template.localized_content[locale]
                  return template.channel_content

          class TemplateRenderer:
              def __init__(self, template_store: TemplateStore):
                  self.store = template_store
                  self.jinja_env = Environment(loader=BaseLoader())

                  # Add custom filters
                  self.jinja_env.filters['currency'] = self._currency_filter
                  self.jinja_env.filters['date'] = self._date_filter
                  self.jinja_env.filters['truncate_sms'] = lambda s: s[:160] if s else ''

              def render(self, template_id: str, channel: Channel,
                         variables: dict, locale: str = "en") -> dict:
                  '''Render template for a specific channel'''
                  template_content = self.store.get_template(template_id, locale)
                  channel_content = template_content.get(channel.value, {})

                  if not channel_content:
                      # Fallback to email content
                      channel_content = template_content.get("email", {})

                  rendered = {}
                  for key in ['subject', 'body', 'html_body']:
                      if key in channel_content:
                          template = self.jinja_env.from_string(channel_content[key])
                          rendered[key] = template.render(**variables)

                  return rendered

              def validate_variables(self, template_id: str, variables: dict) -> list[str]:
                  '''Check if all required variables are provided'''
                  template = self.store.templates.get(template_id)
                  if not template:
                      raise ValueError("Template not found")

                  missing = [v for v in template.variables if v not in variables]
                  return missing

              def _currency_filter(self, amount: int, currency: str = "USD") -> str:
                  '''Format amount (in cents) as currency'''
                  symbols = {"USD": "$", "EUR": "â‚¬", "GBP": "Â£"}
                  symbol = symbols.get(currency, currency)
                  return f"{symbol}{amount/100:.2f}"

              def _date_filter(self, timestamp: float, format: str = "%B %d, %Y") -> str:
                  '''Format timestamp as date'''
                  from datetime import datetime
                  return datetime.fromtimestamp(timestamp).strftime(format)

          # Example template setup
          def setup_templates(store: TemplateStore):
              # Order shipped notification
              store.create_template(
                  template_id="order_shipped",
                  name="Order Shipped",
                  notification_type="order_shipped",
                  channel_content={
                      "email": {
                          "subject": "Your order #{{ order_id }} has shipped!",
                          "body": "Hi {{ user_name }},\n\nGreat news! Your order has shipped.\n\nTracking: {{ tracking_number }}\nCarrier: {{ carrier }}\n\nEstimated delivery: {{ delivery_date | date }}",
                          "html_body": '''
                              <h1>Your order is on its way!</h1>
                              <p>Hi {{ user_name }},</p>
                              <p>Great news! Your order <strong>#{{ order_id }}</strong> has shipped.</p>
                              <div class="tracking-info">
                                  <p><strong>Tracking Number:</strong> {{ tracking_number }}</p>
                                  <p><strong>Carrier:</strong> {{ carrier }}</p>
                                  <p><strong>Estimated Delivery:</strong> {{ delivery_date | date }}</p>
                              </div>
                              <a href="{{ tracking_url }}" class="button">Track Package</a>
                          '''
                      },
                      "sms": {
                          "body": "Order #{{ order_id }} shipped! Track: {{ tracking_url | truncate_sms }}"
                      },
                      "push": {
                          "subject": "Order Shipped",
                          "body": "Your order #{{ order_id }} is on the way!"
                      }
                  },
                  variables=["user_name", "order_id", "tracking_number", "carrier",
                             "delivery_date", "tracking_url"]
              )

              # Add Vietnamese localization
              store.add_localization("order_shipped", "vi", {
                  "email": {
                      "subject": "ÄÆ¡n hÃ ng #{{ order_id }} Ä‘Ã£ Ä‘Æ°á»£c gá»­i!",
                      "body": "Xin chÃ o {{ user_name }},\n\nÄÆ¡n hÃ ng cá»§a báº¡n Ä‘Ã£ Ä‘Æ°á»£c gá»­i.\n\nMÃ£ váº­n Ä‘Æ¡n: {{ tracking_number }}"
                  },
                  "sms": {
                      "body": "ÄÆ¡n hÃ ng #{{ order_id }} Ä‘Ã£ gá»­i! Theo dÃµi: {{ tracking_url | truncate_sms }}"
                  }
              })
          ```
      pitfalls:
      - SMS templates must fit in 160 chars (or pay for multiple segments)
      - HTML email needs inline styles - CSS classes often stripped
      - Template variables need escaping for XSS prevention in HTML
      - Locale detection should fall back gracefully (vi -> vi-VN -> en)
      acceptance_criteria:
      - Notification templates are defined with named placeholders for dynamic content substitution
      - Variable substitution replaces all placeholders with their corresponding data values at send time
      - Localization selects the correct language variant of a template based on the user's locale preference
      - Template preview renders a template with sample data for visual review before enabling it in production
      deliverables:
      - Template storage system that manages notification templates identified by name and version
      - Variable substitution engine that replaces placeholder tokens in templates with dynamic data values
      - Localization support that selects the correct template translation based on the recipient's locale
      - Template versioning system that tracks template changes and allows rollback to previous versions
    - name: User Preferences & Unsubscribe
      description: Implement user notification preferences with granular controls and one-click unsubscribe
      skills:
      - Preference management
      - GDPR compliance
      - Unsubscribe handling
      hints:
        level1: 'Users need control: what notifications, which channels, when'
        level2: 'CAN-SPAM/GDPR: must include unsubscribe link in marketing emails'
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          import secrets
          import time
          import hashlib
          import hmac

          class NotificationCategory(Enum):
              TRANSACTIONAL = "transactional"  # Order confirmations, receipts
              SECURITY = "security"             # Password reset, login alerts
              MARKETING = "marketing"           # Promotions, newsletters
              PRODUCT = "product"               # Feature updates, tips
              SOCIAL = "social"                 # Comments, mentions

          @dataclass
          class UserPreferences:
              user_id: str
              email_enabled: bool = True
              sms_enabled: bool = False
              push_enabled: bool = True
              in_app_enabled: bool = True

              # Category-specific settings
              category_settings: dict[str, dict] = field(default_factory=dict)
              # e.g., {"marketing": {"email": False, "push": True}}

              # Quiet hours
              quiet_hours_enabled: bool = False
              quiet_hours_start: int = 22  # 10 PM
              quiet_hours_end: int = 8     # 8 AM
              timezone: str = "UTC"

              # Frequency limits
              max_emails_per_day: Optional[int] = None
              max_sms_per_week: Optional[int] = None

              # Unsubscribe tracking
              unsubscribed_types: set[str] = field(default_factory=set)
              global_unsubscribe: bool = False

              updated_at: float = field(default_factory=time.time)

          class PreferenceManager:
              def __init__(self, secret_key: bytes):
                  self.preferences: dict[str, UserPreferences] = {}
                  self.secret_key = secret_key
                  # Track sends for frequency limiting
                  self.send_counts: dict[str, dict] = {}  # user_id -> {channel: count}

              def get_preferences(self, user_id: str) -> UserPreferences:
                  if user_id not in self.preferences:
                      self.preferences[user_id] = UserPreferences(user_id=user_id)
                  return self.preferences[user_id]

              def update_preferences(self, user_id: str, updates: dict) -> UserPreferences:
                  prefs = self.get_preferences(user_id)

                  for key, value in updates.items():
                      if hasattr(prefs, key):
                          setattr(prefs, key, value)

                  prefs.updated_at = time.time()
                  return prefs

              def set_category_preference(self, user_id: str, category: NotificationCategory,
                                           channel: Channel, enabled: bool):
                  prefs = self.get_preferences(user_id)

                  if category.value not in prefs.category_settings:
                      prefs.category_settings[category.value] = {}

                  prefs.category_settings[category.value][channel.value] = enabled
                  prefs.updated_at = time.time()

              def can_send(self, user_id: str, notification_type: str,
                           channel: Channel, category: NotificationCategory) -> tuple[bool, str]:
                  '''Check if notification can be sent to user'''
                  prefs = self.get_preferences(user_id)

                  # Global unsubscribe
                  if prefs.global_unsubscribe and category != NotificationCategory.TRANSACTIONAL:
                      return False, "global_unsubscribe"

                  # Type-specific unsubscribe
                  if notification_type in prefs.unsubscribed_types:
                      return False, f"unsubscribed_from_{notification_type}"

                  # Channel disabled globally
                  channel_enabled = getattr(prefs, f"{channel.value}_enabled", True)
                  if not channel_enabled:
                      return False, f"{channel.value}_disabled"

                  # Category-specific setting
                  cat_settings = prefs.category_settings.get(category.value, {})
                  if channel.value in cat_settings and not cat_settings[channel.value]:
                      return False, f"category_{category.value}_disabled_for_{channel.value}"

                  # Frequency limits
                  if not self._check_frequency_limit(user_id, channel, prefs):
                      return False, "frequency_limit_exceeded"

                  # Transactional and security always allowed (after unsubscribe checks)
                  if category in [NotificationCategory.TRANSACTIONAL, NotificationCategory.SECURITY]:
                      return True, "allowed_transactional"

                  return True, "allowed"

              def _check_frequency_limit(self, user_id: str, channel: Channel,
                                          prefs: UserPreferences) -> bool:
                  counts = self.send_counts.get(user_id, {})

                  if channel == Channel.EMAIL and prefs.max_emails_per_day:
                      today_count = counts.get("email_today", 0)
                      return today_count < prefs.max_emails_per_day

                  if channel == Channel.SMS and prefs.max_sms_per_week:
                      week_count = counts.get("sms_week", 0)
                      return week_count < prefs.max_sms_per_week

                  return True

              def record_send(self, user_id: str, channel: Channel):
                  if user_id not in self.send_counts:
                      self.send_counts[user_id] = {}

                  if channel == Channel.EMAIL:
                      self.send_counts[user_id]["email_today"] = (
                          self.send_counts[user_id].get("email_today", 0) + 1
                      )
                  elif channel == Channel.SMS:
                      self.send_counts[user_id]["sms_week"] = (
                          self.send_counts[user_id].get("sms_week", 0) + 1
                      )

              # Unsubscribe link generation
              def generate_unsubscribe_token(self, user_id: str,
                                              notification_type: str = None) -> str:
                  '''Generate signed unsubscribe token'''
                  timestamp = str(int(time.time()))
                  data = f"{user_id}|{notification_type or 'all'}|{timestamp}"

                  signature = hmac.new(
                      self.secret_key,
                      data.encode(),
                      hashlib.sha256
                  ).hexdigest()[:16]

                  token = f"{data}|{signature}"
                  # Base64 encode for URL safety
                  import base64
                  return base64.urlsafe_b64encode(token.encode()).decode()

              def process_unsubscribe(self, token: str) -> dict:
                  '''Process unsubscribe from token'''
                  import base64
                  try:
                      decoded = base64.urlsafe_b64decode(token.encode()).decode()
                      parts = decoded.split("|")
                      if len(parts) != 4:
                          return {"success": False, "error": "invalid_token"}

                      user_id, notification_type, timestamp, signature = parts

                      # Verify signature
                      data = f"{user_id}|{notification_type}|{timestamp}"
                      expected = hmac.new(
                          self.secret_key,
                          data.encode(),
                          hashlib.sha256
                      ).hexdigest()[:16]

                      if not hmac.compare_digest(signature, expected):
                          return {"success": False, "error": "invalid_signature"}

                      # Check token age (30 days max)
                      if time.time() - int(timestamp) > 30 * 86400:
                          return {"success": False, "error": "token_expired"}

                      # Process unsubscribe
                      prefs = self.get_preferences(user_id)
                      if notification_type == "all":
                          prefs.global_unsubscribe = True
                      else:
                          prefs.unsubscribed_types.add(notification_type)

                      return {
                          "success": True,
                          "user_id": user_id,
                          "unsubscribed_from": notification_type
                      }

                  except Exception as e:
                      return {"success": False, "error": str(e)}

              def generate_unsubscribe_url(self, user_id: str, notification_type: str,
                                            base_url: str) -> str:
                  token = self.generate_unsubscribe_token(user_id, notification_type)
                  return f"{base_url}/unsubscribe?token={token}"
          ```
      pitfalls:
      - Transactional emails (receipts, password reset) can't be unsubscribed per CAN-SPAM
      - One-click unsubscribe required for marketing - RFC 8058
      - Unsubscribe token must be signed to prevent enumeration attacks
      - 'GDPR: must honor unsubscribe within 10 days (do it immediately)'
      acceptance_criteria:
      - Notification preferences are stored per user and per category controlling which channels receive which notifications
      - Unsubscribe requests immediately suppress future notifications for the specified channel or category
      - Notification categories like marketing, transactional, and alerts can be individually toggled by the user
      - Quiet hours settings suppress non-urgent notifications during the user's configured do-not-disturb window
      deliverables:
      - Preference storage that persists per-user notification preferences for each channel and category
      - Channel opt-out mechanism that allows users to disable specific notification channels entirely
      - Unsubscribe link generator that creates tokenized one-click unsubscribe URLs for email notifications
      - Preference management API with endpoints to read, update, and reset notification preferences
    - name: Delivery Tracking & Analytics
      description: Implement delivery status tracking, open/click tracking, and analytics
      skills:
      - Event tracking
      - Pixel tracking
      - Analytics
      hints:
        level1: 'Track: sent, delivered, bounced, opened, clicked'
        level2: Email opens tracked via tracking pixel; clicks via redirect
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          from collections import defaultdict
          import time
          import secrets
          import hashlib

          class DeliveryStatus(Enum):
              PENDING = "pending"
              SENT = "sent"
              DELIVERED = "delivered"
              OPENED = "opened"
              CLICKED = "clicked"
              BOUNCED = "bounced"
              COMPLAINED = "complained"  # Spam report
              FAILED = "failed"
              UNSUBSCRIBED = "unsubscribed"

          @dataclass
          class DeliveryEvent:
              notification_id: str
              channel: Channel
              status: DeliveryStatus
              timestamp: float
              metadata: dict = field(default_factory=dict)

          @dataclass
          class NotificationDelivery:
              notification_id: str
              user_id: str
              channel: Channel
              status: DeliveryStatus
              provider_message_id: Optional[str]
              events: list[DeliveryEvent] = field(default_factory=list)
              created_at: float = field(default_factory=time.time)

          class DeliveryTracker:
              def __init__(self, tracking_domain: str, secret_key: bytes):
                  self.deliveries: dict[str, NotificationDelivery] = {}
                  self.tracking_domain = tracking_domain
                  self.secret_key = secret_key
                  # Tracking ID -> notification mapping
                  self.tracking_ids: dict[str, str] = {}

              def create_delivery(self, notification_id: str, user_id: str,
                                  channel: Channel) -> NotificationDelivery:
                  delivery = NotificationDelivery(
                      notification_id=notification_id,
                      user_id=user_id,
                      channel=channel,
                      status=DeliveryStatus.PENDING,
                      provider_message_id=None
                  )
                  self.deliveries[notification_id] = delivery
                  return delivery

              def update_status(self, notification_id: str, status: DeliveryStatus,
                                metadata: dict = None):
                  delivery = self.deliveries.get(notification_id)
                  if not delivery:
                      return

                  event = DeliveryEvent(
                      notification_id=notification_id,
                      channel=delivery.channel,
                      status=status,
                      timestamp=time.time(),
                      metadata=metadata or {}
                  )

                  delivery.events.append(event)
                  delivery.status = status

              def handle_provider_webhook(self, provider: str, event_type: str,
                                           data: dict):
                  '''Handle delivery webhooks from email/SMS providers'''
                  # Map provider event types to our statuses
                  event_mapping = {
                      "sendgrid": {
                          "delivered": DeliveryStatus.DELIVERED,
                          "open": DeliveryStatus.OPENED,
                          "click": DeliveryStatus.CLICKED,
                          "bounce": DeliveryStatus.BOUNCED,
                          "spamreport": DeliveryStatus.COMPLAINED
                      },
                      "twilio": {
                          "delivered": DeliveryStatus.DELIVERED,
                          "failed": DeliveryStatus.FAILED,
                          "undelivered": DeliveryStatus.BOUNCED
                      }
                  }

                  mapping = event_mapping.get(provider, {})
                  status = mapping.get(event_type)

                  if status:
                      message_id = data.get("message_id") or data.get("MessageSid")
                      # Find notification by provider message ID
                      for nid, delivery in self.deliveries.items():
                          if delivery.provider_message_id == message_id:
                              self.update_status(nid, status, {"provider_event": event_type})
                              break

              # Email tracking pixel and link tracking
              def generate_tracking_pixel_url(self, notification_id: str) -> str:
                  '''Generate unique tracking pixel URL for email opens'''
                  tracking_id = secrets.token_urlsafe(16)
                  self.tracking_ids[tracking_id] = notification_id

                  # Sign to prevent forgery
                  signature = hmac.new(
                      self.secret_key,
                      tracking_id.encode(),
                      hashlib.sha256
                  ).hexdigest()[:8]

                  return f"https://{self.tracking_domain}/track/open/{tracking_id}/{signature}.gif"

              def generate_tracking_link(self, notification_id: str,
                                          original_url: str, link_id: str = None) -> str:
                  '''Generate tracking redirect URL for link clicks'''
                  tracking_id = secrets.token_urlsafe(16)
                  link_id = link_id or hashlib.md5(original_url.encode()).hexdigest()[:8]

                  # Store mapping
                  self.tracking_ids[tracking_id] = {
                      "notification_id": notification_id,
                      "original_url": original_url,
                      "link_id": link_id
                  }

                  return f"https://{self.tracking_domain}/track/click/{tracking_id}"

              def handle_tracking_pixel(self, tracking_id: str, signature: str,
                                         ip: str, user_agent: str) -> bytes:
                  '''Handle tracking pixel request - return 1x1 transparent GIF'''
                  # Verify signature
                  expected = hmac.new(
                      self.secret_key,
                      tracking_id.encode(),
                      hashlib.sha256
                  ).hexdigest()[:8]

                  if hmac.compare_digest(signature, expected):
                      notification_id = self.tracking_ids.get(tracking_id)
                      if notification_id:
                          self.update_status(notification_id, DeliveryStatus.OPENED, {
                              "ip": ip,
                              "user_agent": user_agent
                          })

                  # Return 1x1 transparent GIF
                  return bytes([
                      0x47, 0x49, 0x46, 0x38, 0x39, 0x61, 0x01, 0x00,
                      0x01, 0x00, 0x80, 0x00, 0x00, 0xff, 0xff, 0xff,
                      0x00, 0x00, 0x00, 0x21, 0xf9, 0x04, 0x01, 0x00,
                      0x00, 0x00, 0x00, 0x2c, 0x00, 0x00, 0x00, 0x00,
                      0x01, 0x00, 0x01, 0x00, 0x00, 0x02, 0x02, 0x44,
                      0x01, 0x00, 0x3b
                  ])

              def handle_tracking_click(self, tracking_id: str,
                                         ip: str, user_agent: str) -> Optional[str]:
                  '''Handle click tracking - return redirect URL'''
                  data = self.tracking_ids.get(tracking_id)
                  if not data or not isinstance(data, dict):
                      return None

                  self.update_status(data["notification_id"], DeliveryStatus.CLICKED, {
                      "ip": ip,
                      "user_agent": user_agent,
                      "link_id": data["link_id"]
                  })

                  return data["original_url"]

          class NotificationAnalytics:
              def __init__(self, tracker: DeliveryTracker):
                  self.tracker = tracker

              def get_delivery_stats(self, start_time: float, end_time: float) -> dict:
                  '''Get delivery statistics for time period'''
                  stats = defaultdict(lambda: defaultdict(int))

                  for delivery in self.tracker.deliveries.values():
                      if start_time <= delivery.created_at <= end_time:
                          channel = delivery.channel.value
                          stats[channel]["total"] += 1
                          stats[channel][delivery.status.value] += 1

                  # Calculate rates
                  result = {}
                  for channel, counts in stats.items():
                      total = counts["total"]
                      result[channel] = {
                          "total": total,
                          "delivered": counts.get("delivered", 0),
                          "delivery_rate": counts.get("delivered", 0) / total if total else 0,
                          "opened": counts.get("opened", 0),
                          "open_rate": counts.get("opened", 0) / counts.get("delivered", 1),
                          "clicked": counts.get("clicked", 0),
                          "click_rate": counts.get("clicked", 0) / counts.get("opened", 1),
                          "bounced": counts.get("bounced", 0),
                          "bounce_rate": counts.get("bounced", 0) / total if total else 0,
                          "complained": counts.get("complained", 0)
                      }

                  return result

              def get_notification_type_performance(self, notification_type: str) -> dict:
                  '''Analyze performance of specific notification type'''
                  # Would join with notification data to get type
                  pass
          ```
      pitfalls:
      - Email opens tracked via pixel are unreliable (image blocking)
      - Apple Mail Privacy Protection prefetches pixels - inflates open rates
      - Don't track transactional emails for privacy (password resets)
      - Spam complaints must trigger immediate unsubscribe
      acceptance_criteria:
      - Delivery status is tracked through sent, delivered, opened, clicked, bounced, and failed states for each notification
      - Open and click events are recorded with timestamps and linked back to the originating notification
      - Delivery metrics including delivery rate, bounce rate, and open rate are calculated per channel and campaign
      - Alerts fire when delivery failure rate exceeds a configurable threshold indicating a channel issue
      deliverables:
      - Delivery status tracker that records sent, delivered, bounced, and failed states for each notification
      - Open and click tracker that detects when email recipients open a message or click embedded links
      - Bounce handler that processes hard and soft bounces and updates recipient reachability status
      - Analytics dashboard that visualizes delivery rates, open rates, and click rates across channels and time
  webhook-delivery:
    name: Webhook Delivery System
    description: Build a reliable webhook delivery system with signature verification, retry logic, circuit breakers, and
      delivery guarantees.
    why_expert: Webhooks are the backbone of modern integrations. Building one teaches async delivery patterns, failure handling,
      and at-least-once guarantees.
    difficulty: advanced
    tags:
    - webhooks
    - async
    - reliability
    - integration
    - events
    estimated_hours: 35
    prerequisites:
    - build-message-queue
    milestones:
    - name: Webhook Registration & Security
      description: Implement webhook endpoint registration with signature verification
      skills:
      - HMAC signatures
      - Endpoint validation
      - Secret management
      hints:
        level1: Each webhook endpoint gets a unique signing secret
        level2: Verify endpoint ownership via challenge before activating
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from enum import Enum
          import secrets
          import hmac
          import hashlib
          import time
          import httpx

          class WebhookStatus(Enum):
              PENDING_VERIFICATION = "pending_verification"
              ACTIVE = "active"
              DISABLED = "disabled"
              FAILED = "failed"  # Too many delivery failures

          @dataclass
          class WebhookEndpoint:
              id: str
              url: str
              secret: str
              events: list[str]               # Event types to subscribe to
              status: WebhookStatus
              created_at: float
              verified_at: Optional[float] = None
              description: str = ""
              metadata: dict = field(default_factory=dict)

              # Health tracking
              consecutive_failures: int = 0
              last_failure_at: Optional[float] = None
              total_deliveries: int = 0
              successful_deliveries: int = 0

          class WebhookRegistry:
              def __init__(self, verify_timeout: int = 30):
                  self.endpoints: dict[str, WebhookEndpoint] = {}
                  self.verify_timeout = verify_timeout

              def register_endpoint(self, url: str, events: list[str],
                                    description: str = "") -> WebhookEndpoint:
                  '''Register new webhook endpoint'''
                  # Validate URL
                  if not url.startswith("https://"):
                      raise ValueError("Webhook URLs must use HTTPS")

                  endpoint_id = secrets.token_urlsafe(16)
                  signing_secret = f"whsec_{secrets.token_urlsafe(32)}"

                  endpoint = WebhookEndpoint(
                      id=endpoint_id,
                      url=url,
                      secret=signing_secret,
                      events=events,
                      status=WebhookStatus.PENDING_VERIFICATION,
                      created_at=time.time(),
                      description=description
                  )

                  self.endpoints[endpoint_id] = endpoint

                  # Initiate verification
                  self._send_verification_challenge(endpoint)

                  return endpoint

              def _send_verification_challenge(self, endpoint: WebhookEndpoint):
                  '''Send challenge to verify endpoint ownership'''
                  challenge = secrets.token_urlsafe(32)

                  # Store challenge for later verification
                  endpoint.metadata["verification_challenge"] = challenge
                  endpoint.metadata["verification_expires"] = time.time() + 3600  # 1 hour

                  payload = {
                      "type": "webhook.verification",
                      "challenge": challenge
                  }

                  try:
                      # Endpoint must respond with challenge value
                      response = httpx.post(
                          endpoint.url,
                          json=payload,
                          headers=self._build_headers(endpoint, payload),
                          timeout=self.verify_timeout
                      )

                      if response.status_code == 200:
                          response_data = response.json()
                          if response_data.get("challenge") == challenge:
                              endpoint.status = WebhookStatus.ACTIVE
                              endpoint.verified_at = time.time()
                              del endpoint.metadata["verification_challenge"]
                  except Exception as e:
                      endpoint.metadata["verification_error"] = str(e)

              def _build_headers(self, endpoint: WebhookEndpoint, payload: dict) -> dict:
                  '''Build headers with signature'''
                  timestamp = str(int(time.time()))
                  payload_str = json.dumps(payload, separators=(',', ':'), sort_keys=True)

                  # Create signature
                  signature_payload = f"{timestamp}.{payload_str}"
                  signature = hmac.new(
                      endpoint.secret.encode(),
                      signature_payload.encode(),
                      hashlib.sha256
                  ).hexdigest()

                  return {
                      "Content-Type": "application/json",
                      "X-Webhook-ID": endpoint.id,
                      "X-Webhook-Timestamp": timestamp,
                      "X-Webhook-Signature": f"v1={signature}",
                      "User-Agent": "WebhookDelivery/1.0"
                  }

              def update_events(self, endpoint_id: str, events: list[str]) -> WebhookEndpoint:
                  endpoint = self.endpoints.get(endpoint_id)
                  if not endpoint:
                      raise ValueError("Endpoint not found")

                  endpoint.events = events
                  return endpoint

              def disable_endpoint(self, endpoint_id: str):
                  endpoint = self.endpoints.get(endpoint_id)
                  if endpoint:
                      endpoint.status = WebhookStatus.DISABLED

              def get_endpoints_for_event(self, event_type: str) -> list[WebhookEndpoint]:
                  '''Get all active endpoints subscribed to event type'''
                  return [
                      e for e in self.endpoints.values()
                      if e.status == WebhookStatus.ACTIVE
                      and (event_type in e.events or "*" in e.events)
                  ]

          # Signature verification helper for receivers
          class WebhookSignatureVerifier:
              def __init__(self, secret: str, tolerance: int = 300):
                  self.secret = secret
                  self.tolerance = tolerance  # Timestamp tolerance in seconds

              def verify(self, payload: bytes, timestamp: str, signature: str) -> bool:
                  '''Verify webhook signature'''
                  # Check timestamp to prevent replay attacks
                  try:
                      ts = int(timestamp)
                      if abs(time.time() - ts) > self.tolerance:
                          return False
                  except ValueError:
                      return False

                  # Verify signature
                  expected_payload = f"{timestamp}.{payload.decode()}"
                  expected_sig = hmac.new(
                      self.secret.encode(),
                      expected_payload.encode(),
                      hashlib.sha256
                  ).hexdigest()

                  # Extract signature value
                  if signature.startswith("v1="):
                      signature = signature[3:]

                  return hmac.compare_digest(expected_sig, signature)
          ```
      pitfalls:
      - HTTPS only - never deliver webhooks over HTTP
      - Timestamp tolerance prevents replay but allows clock skew
      - 'Signing secret rotation: support multiple active secrets temporarily'
      - 'URL validation: block private IPs to prevent SSRF'
      acceptance_criteria:
      - Store webhook endpoints with associated secrets and event type subscriptions
      - Generate HMAC signatures for payloads and include them in delivery headers
      - Validate endpoint ownership by sending a challenge request and verifying the response
      - Handle secret rotation allowing new secrets without invalidating in-flight deliveries
      deliverables:
      - Endpoint registration API accepting callback URL and event type subscriptions
      - Secret key generation producing a cryptographically random signing key per webhook
      - Signature generation computing HMAC-SHA256 over the payload with the webhook secret
      - URL validation verifying the callback endpoint is reachable and returns a confirmation
    - name: Delivery Queue & Retry Logic
      description: Implement reliable delivery with exponential backoff and dead letter queue
      skills:
      - Retry strategies
      - Exponential backoff
      - Dead letter queues
      hints:
        level1: Use message queue for delivery; retry with exponential backoff on failure
        level2: After N retries, move to dead letter queue for manual review
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional, Callable
          from enum import Enum
          import time
          import random
          import httpx
          import asyncio

          class DeliveryStatus(Enum):
              PENDING = "pending"
              IN_FLIGHT = "in_flight"
              DELIVERED = "delivered"
              FAILED = "failed"
              DEAD_LETTER = "dead_letter"

          @dataclass
          class WebhookDelivery:
              id: str
              endpoint_id: str
              event_type: str
              payload: dict
              status: DeliveryStatus
              created_at: float
              scheduled_for: float         # When to attempt delivery
              attempts: int = 0
              max_attempts: int = 5
              last_attempt_at: Optional[float] = None
              last_error: Optional[str] = None
              last_response_code: Optional[int] = None
              delivered_at: Optional[float] = None

          class RetryPolicy:
              def __init__(self, base_delay: int = 60,
                           max_delay: int = 3600,
                           jitter: float = 0.1):
                  self.base_delay = base_delay   # 1 minute
                  self.max_delay = max_delay     # 1 hour
                  self.jitter = jitter           # 10% random jitter

              def get_next_delay(self, attempt: int) -> int:
                  '''Calculate delay with exponential backoff and jitter'''
                  # Exponential: 1m, 2m, 4m, 8m, 16m...
                  delay = min(self.base_delay * (2 ** attempt), self.max_delay)

                  # Add jitter to prevent thundering herd
                  jitter_range = delay * self.jitter
                  delay += random.uniform(-jitter_range, jitter_range)

                  return int(delay)

          class WebhookDeliveryQueue:
              def __init__(self, registry: WebhookRegistry,
                           retry_policy: RetryPolicy = None):
                  self.registry = registry
                  self.retry_policy = retry_policy or RetryPolicy()
                  self.pending: list[WebhookDelivery] = []
                  self.dead_letter: list[WebhookDelivery] = []
                  self.delivered: list[WebhookDelivery] = []

              def enqueue(self, endpoint_id: str, event_type: str,
                          payload: dict) -> WebhookDelivery:
                  '''Add webhook delivery to queue'''
                  delivery = WebhookDelivery(
                      id=secrets.token_urlsafe(16),
                      endpoint_id=endpoint_id,
                      event_type=event_type,
                      payload=payload,
                      status=DeliveryStatus.PENDING,
                      created_at=time.time(),
                      scheduled_for=time.time()  # Immediate
                  )

                  self.pending.append(delivery)
                  return delivery

              def enqueue_for_event(self, event_type: str, payload: dict) -> list[WebhookDelivery]:
                  '''Fan out event to all subscribed endpoints'''
                  endpoints = self.registry.get_endpoints_for_event(event_type)
                  deliveries = []

                  for endpoint in endpoints:
                      delivery = self.enqueue(endpoint.id, event_type, payload)
                      deliveries.append(delivery)

                  return deliveries

              async def process_queue(self):
                  '''Process pending deliveries'''
                  now = time.time()
                  ready = [d for d in self.pending if d.scheduled_for <= now]

                  for delivery in ready:
                      self.pending.remove(delivery)
                      await self._attempt_delivery(delivery)

              async def _attempt_delivery(self, delivery: WebhookDelivery):
                  '''Attempt single delivery'''
                  endpoint = self.registry.endpoints.get(delivery.endpoint_id)
                  if not endpoint or endpoint.status != WebhookStatus.ACTIVE:
                      delivery.status = DeliveryStatus.DEAD_LETTER
                      delivery.last_error = "Endpoint not active"
                      self.dead_letter.append(delivery)
                      return

                  delivery.status = DeliveryStatus.IN_FLIGHT
                  delivery.attempts += 1
                  delivery.last_attempt_at = time.time()

                  try:
                      # Build signed request
                      headers = self.registry._build_headers(endpoint, delivery.payload)

                      async with httpx.AsyncClient() as client:
                          response = await client.post(
                              endpoint.url,
                              json=delivery.payload,
                              headers=headers,
                              timeout=30.0
                          )

                      delivery.last_response_code = response.status_code

                      # 2xx = success
                      if 200 <= response.status_code < 300:
                          delivery.status = DeliveryStatus.DELIVERED
                          delivery.delivered_at = time.time()
                          self.delivered.append(delivery)

                          # Update endpoint health
                          endpoint.total_deliveries += 1
                          endpoint.successful_deliveries += 1
                          endpoint.consecutive_failures = 0
                          return

                      # 4xx (except 429) = don't retry, bad request
                      if 400 <= response.status_code < 500 and response.status_code != 429:
                          delivery.status = DeliveryStatus.DEAD_LETTER
                          delivery.last_error = f"HTTP {response.status_code}"
                          self.dead_letter.append(delivery)
                          return

                      # 5xx or 429 = retry
                      delivery.last_error = f"HTTP {response.status_code}"

                  except httpx.TimeoutException:
                      delivery.last_error = "Timeout"
                  except httpx.RequestError as e:
                      delivery.last_error = str(e)

                  # Failed - check retry
                  endpoint.total_deliveries += 1
                  endpoint.consecutive_failures += 1
                  endpoint.last_failure_at = time.time()

                  if delivery.attempts >= delivery.max_attempts:
                      delivery.status = DeliveryStatus.DEAD_LETTER
                      self.dead_letter.append(delivery)

                      # Disable endpoint if too many failures
                      if endpoint.consecutive_failures >= 10:
                          endpoint.status = WebhookStatus.FAILED
                  else:
                      # Schedule retry
                      delay = self.retry_policy.get_next_delay(delivery.attempts)
                      delivery.status = DeliveryStatus.PENDING
                      delivery.scheduled_for = time.time() + delay
                      self.pending.append(delivery)

              def retry_dead_letter(self, delivery_id: str) -> Optional[WebhookDelivery]:
                  '''Manually retry a dead letter delivery'''
                  for delivery in self.dead_letter:
                      if delivery.id == delivery_id:
                          self.dead_letter.remove(delivery)
                          delivery.status = DeliveryStatus.PENDING
                          delivery.scheduled_for = time.time()
                          delivery.attempts = 0  # Reset attempt count
                          self.pending.append(delivery)
                          return delivery
                  return None

              def get_delivery_status(self, delivery_id: str) -> Optional[WebhookDelivery]:
                  for queue in [self.pending, self.delivered, self.dead_letter]:
                      for delivery in queue:
                          if delivery.id == delivery_id:
                              return delivery
                  return None
          ```
      pitfalls:
      - Jitter prevents thundering herd when many webhooks fail simultaneously
      - 4xx errors (except 429) shouldn't retry - request is malformed
      - Dead letter queue needs alerting and manual review process
      - 'Circuit breaker: disable endpoint after consecutive failures'
      acceptance_criteria:
      - Queue webhook events for delivery with ordering guarantees per endpoint
      - Implement exponential backoff retries with configurable base delay and max attempts
      - Handle delivery timeouts by marking attempts as failed after a configured duration
      - Track delivery attempts recording status code, response time, and error for each try
      deliverables:
      - Delivery queue persisting pending webhook events for reliable ordered processing
      - HTTP delivery client sending POST requests with signed payload to registered endpoints
      - Retry logic with exponential backoff increasing delay between successive failed attempts
      - Dead letter queue capturing events that exhaust all retry attempts for manual inspection
    - name: Circuit Breaker & Rate Limiting
      description: Implement circuit breaker to protect failing endpoints and rate limiting
      skills:
      - Circuit breaker pattern
      - Rate limiting
      - Health checks
      hints:
        level1: Circuit breaker prevents hammering failing endpoints
        level2: 'States: closed (normal), open (skip), half-open (test)'
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          from enum import Enum
          import time
          import threading

          class CircuitState(Enum):
              CLOSED = "closed"      # Normal operation
              OPEN = "open"          # Failing, skip requests
              HALF_OPEN = "half_open"  # Testing recovery

          @dataclass
          class CircuitBreaker:
              endpoint_id: str
              state: CircuitState = CircuitState.CLOSED
              failure_count: int = 0
              success_count: int = 0
              last_failure_time: Optional[float] = None
              last_state_change: float = 0

              # Thresholds
              failure_threshold: int = 5     # Failures to open
              success_threshold: int = 3     # Successes to close
              timeout: int = 60              # Seconds before half-open

          class CircuitBreakerManager:
              def __init__(self):
                  self.breakers: dict[str, CircuitBreaker] = {}
                  self.lock = threading.Lock()

              def get_breaker(self, endpoint_id: str) -> CircuitBreaker:
                  with self.lock:
                      if endpoint_id not in self.breakers:
                          self.breakers[endpoint_id] = CircuitBreaker(
                              endpoint_id=endpoint_id,
                              last_state_change=time.time()
                          )
                      return self.breakers[endpoint_id]

              def can_execute(self, endpoint_id: str) -> bool:
                  '''Check if request should proceed'''
                  breaker = self.get_breaker(endpoint_id)

                  if breaker.state == CircuitState.CLOSED:
                      return True

                  if breaker.state == CircuitState.OPEN:
                      # Check if timeout has passed
                      if time.time() - breaker.last_state_change >= breaker.timeout:
                          self._transition(breaker, CircuitState.HALF_OPEN)
                          return True  # Allow test request
                      return False

                  if breaker.state == CircuitState.HALF_OPEN:
                      return True  # Allow test requests

                  return False

              def record_success(self, endpoint_id: str):
                  '''Record successful delivery'''
                  breaker = self.get_breaker(endpoint_id)

                  with self.lock:
                      if breaker.state == CircuitState.HALF_OPEN:
                          breaker.success_count += 1
                          if breaker.success_count >= breaker.success_threshold:
                              self._transition(breaker, CircuitState.CLOSED)
                      elif breaker.state == CircuitState.CLOSED:
                          breaker.failure_count = 0  # Reset on success

              def record_failure(self, endpoint_id: str):
                  '''Record failed delivery'''
                  breaker = self.get_breaker(endpoint_id)

                  with self.lock:
                      breaker.failure_count += 1
                      breaker.last_failure_time = time.time()

                      if breaker.state == CircuitState.HALF_OPEN:
                          # Any failure in half-open goes back to open
                          self._transition(breaker, CircuitState.OPEN)
                      elif breaker.state == CircuitState.CLOSED:
                          if breaker.failure_count >= breaker.failure_threshold:
                              self._transition(breaker, CircuitState.OPEN)

              def _transition(self, breaker: CircuitBreaker, new_state: CircuitState):
                  breaker.state = new_state
                  breaker.last_state_change = time.time()

                  if new_state == CircuitState.CLOSED:
                      breaker.failure_count = 0
                      breaker.success_count = 0
                  elif new_state == CircuitState.HALF_OPEN:
                      breaker.success_count = 0

          class RateLimiter:
              '''Per-endpoint rate limiting using sliding window'''

              def __init__(self, requests_per_second: int = 10,
                           requests_per_minute: int = 100):
                  self.rps = requests_per_second
                  self.rpm = requests_per_minute
                  self.second_windows: dict[str, list[float]] = {}
                  self.minute_windows: dict[str, list[float]] = {}
                  self.lock = threading.Lock()

              def can_send(self, endpoint_id: str) -> tuple[bool, Optional[float]]:
                  '''Check if can send, returns (allowed, retry_after)'''
                  now = time.time()

                  with self.lock:
                      # Check per-second limit
                      second_window = self.second_windows.get(endpoint_id, [])
                      second_window = [t for t in second_window if now - t < 1]
                      self.second_windows[endpoint_id] = second_window

                      if len(second_window) >= self.rps:
                          retry_after = 1 - (now - second_window[0])
                          return False, retry_after

                      # Check per-minute limit
                      minute_window = self.minute_windows.get(endpoint_id, [])
                      minute_window = [t for t in minute_window if now - t < 60]
                      self.minute_windows[endpoint_id] = minute_window

                      if len(minute_window) >= self.rpm:
                          retry_after = 60 - (now - minute_window[0])
                          return False, retry_after

                      return True, None

              def record_send(self, endpoint_id: str):
                  '''Record a send'''
                  now = time.time()
                  with self.lock:
                      if endpoint_id not in self.second_windows:
                          self.second_windows[endpoint_id] = []
                      if endpoint_id not in self.minute_windows:
                          self.minute_windows[endpoint_id] = []

                      self.second_windows[endpoint_id].append(now)
                      self.minute_windows[endpoint_id].append(now)

          class WebhookDeliveryWithProtection:
              '''Delivery queue with circuit breaker and rate limiting'''

              def __init__(self, queue: WebhookDeliveryQueue):
                  self.queue = queue
                  self.circuit_breaker = CircuitBreakerManager()
                  self.rate_limiter = RateLimiter()

              async def attempt_delivery(self, delivery: WebhookDelivery) -> bool:
                  endpoint_id = delivery.endpoint_id

                  # Check circuit breaker
                  if not self.circuit_breaker.can_execute(endpoint_id):
                      # Reschedule for later
                      delivery.scheduled_for = time.time() + 60
                      return False

                  # Check rate limit
                  allowed, retry_after = self.rate_limiter.can_send(endpoint_id)
                  if not allowed:
                      delivery.scheduled_for = time.time() + (retry_after or 1)
                      return False

                  # Attempt delivery
                  self.rate_limiter.record_send(endpoint_id)
                  success = await self.queue._attempt_delivery(delivery)

                  if success:
                      self.circuit_breaker.record_success(endpoint_id)
                  else:
                      self.circuit_breaker.record_failure(endpoint_id)

                  return success
          ```
      pitfalls:
      - 'Half-open: only allow limited test requests, not full traffic'
      - Circuit breaker timeout should increase on repeated failures
      - Rate limiting should respect Retry-After headers from receiver
      - Alert when circuit opens - endpoint owner needs to know
      acceptance_criteria:
      - Implement circuit breaker per endpoint with configurable failure threshold
      - Disable endpoints after repeated failures and alert the webhook owner
      - Rate limit delivery attempts to avoid overwhelming downstream endpoints
      - Support endpoint recovery transitioning from open to half-open to closed circuit state
      deliverables:
      - Per-endpoint circuit breaker halting delivery after consecutive failure threshold is reached
      - Rate limiting throttle capping the delivery rate per endpoint per time window
      - Backpressure handling slowing queue consumption when downstream endpoints are overloaded
      - Health tracking monitor recording success rates and latency per registered endpoint
    - name: Event Log & Replay
      description: Implement event logging for debugging and replay capability
      skills:
      - Event sourcing
      - Log retention
      - Event replay
      hints:
        level1: Log all webhook events for debugging and audit
        level2: Allow replaying failed webhooks from a specific time
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional, Iterator
          import time
          import json

          @dataclass
          class WebhookEventLog:
              id: str
              event_type: str
              payload: dict
              timestamp: float
              deliveries: list[dict] = field(default_factory=list)
              # Each delivery: {endpoint_id, delivery_id, status, attempts, delivered_at}

          class EventLogStore:
              def __init__(self, retention_days: int = 30):
                  self.events: list[WebhookEventLog] = []
                  self.retention_days = retention_days
                  self.index_by_type: dict[str, list[int]] = {}  # event_type -> indices

              def log_event(self, event_type: str, payload: dict) -> WebhookEventLog:
                  '''Log a new event'''
                  event = WebhookEventLog(
                      id=secrets.token_urlsafe(16),
                      event_type=event_type,
                      payload=payload,
                      timestamp=time.time()
                  )

                  idx = len(self.events)
                  self.events.append(event)

                  # Index by type
                  if event_type not in self.index_by_type:
                      self.index_by_type[event_type] = []
                  self.index_by_type[event_type].append(idx)

                  return event

              def record_delivery(self, event_id: str, endpoint_id: str,
                                  delivery_id: str, status: str,
                                  attempts: int = 0, delivered_at: float = None):
                  '''Record delivery attempt for an event'''
                  for event in self.events:
                      if event.id == event_id:
                          event.deliveries.append({
                              "endpoint_id": endpoint_id,
                              "delivery_id": delivery_id,
                              "status": status,
                              "attempts": attempts,
                              "delivered_at": delivered_at
                          })
                          return

              def query_events(self, event_type: str = None,
                               start_time: float = None,
                               end_time: float = None,
                               limit: int = 100,
                               offset: int = 0) -> list[WebhookEventLog]:
                  '''Query events with filters'''
                  results = self.events

                  if event_type:
                      indices = self.index_by_type.get(event_type, [])
                      results = [self.events[i] for i in indices]

                  if start_time:
                      results = [e for e in results if e.timestamp >= start_time]
                  if end_time:
                      results = [e for e in results if e.timestamp <= end_time]

                  # Sort by timestamp descending (newest first)
                  results = sorted(results, key=lambda e: e.timestamp, reverse=True)

                  return results[offset:offset + limit]

              def get_failed_deliveries(self, endpoint_id: str,
                                        start_time: float = None) -> list[WebhookEventLog]:
                  '''Get events with failed deliveries to specific endpoint'''
                  results = []

                  for event in self.events:
                      if start_time and event.timestamp < start_time:
                          continue

                      for delivery in event.deliveries:
                          if (delivery["endpoint_id"] == endpoint_id and
                              delivery["status"] in ["failed", "dead_letter"]):
                              results.append(event)
                              break

                  return results

              def cleanup_old_events(self):
                  '''Remove events older than retention period'''
                  cutoff = time.time() - (self.retention_days * 86400)
                  self.events = [e for e in self.events if e.timestamp >= cutoff]
                  # Rebuild indices
                  self._rebuild_indices()

              def _rebuild_indices(self):
                  self.index_by_type = {}
                  for i, event in enumerate(self.events):
                      if event.event_type not in self.index_by_type:
                          self.index_by_type[event.event_type] = []
                      self.index_by_type[event.event_type].append(i)

          class WebhookReplayService:
              def __init__(self, event_store: EventLogStore,
                           delivery_queue: WebhookDeliveryQueue):
                  self.store = event_store
                  self.queue = delivery_queue

              def replay_event(self, event_id: str,
                               endpoint_ids: list[str] = None) -> list[WebhookDelivery]:
                  '''Replay a specific event'''
                  event = None
                  for e in self.store.events:
                      if e.id == event_id:
                          event = e
                          break

                  if not event:
                      raise ValueError("Event not found")

                  deliveries = []

                  if endpoint_ids:
                      # Replay to specific endpoints
                      for endpoint_id in endpoint_ids:
                          delivery = self.queue.enqueue(
                              endpoint_id, event.event_type, event.payload
                          )
                          deliveries.append(delivery)
                  else:
                      # Replay to all original endpoints that failed
                      for d in event.deliveries:
                          if d["status"] in ["failed", "dead_letter"]:
                              delivery = self.queue.enqueue(
                                  d["endpoint_id"], event.event_type, event.payload
                              )
                              deliveries.append(delivery)

                  return deliveries

              def replay_range(self, endpoint_id: str,
                               start_time: float, end_time: float = None) -> list[WebhookDelivery]:
                  '''Replay all events in a time range to an endpoint'''
                  end_time = end_time or time.time()
                  events = self.store.query_events(
                      start_time=start_time,
                      end_time=end_time,
                      limit=10000  # Safety limit
                  )

                  deliveries = []
                  for event in events:
                      # Check if endpoint is subscribed to this event type
                      endpoint = self.queue.registry.endpoints.get(endpoint_id)
                      if endpoint and (event.event_type in endpoint.events or "*" in endpoint.events):
                          delivery = self.queue.enqueue(
                              endpoint_id, event.event_type, event.payload
                          )
                          deliveries.append(delivery)

                  return deliveries

              def export_events(self, start_time: float, end_time: float,
                                event_types: list[str] = None) -> Iterator[dict]:
                  '''Export events as JSON for external processing'''
                  events = self.store.query_events(
                      start_time=start_time,
                      end_time=end_time,
                      limit=100000
                  )

                  for event in events:
                      if event_types and event.event_type not in event_types:
                          continue

                      yield {
                          "id": event.id,
                          "type": event.event_type,
                          "timestamp": event.timestamp,
                          "payload": event.payload
                      }
          ```
      pitfalls:
      - Event log can grow huge - implement retention and archival
      - Replay should use new delivery IDs to track separately
      - Bulk replay can overwhelm endpoints - respect rate limits
      - Payload might be stale on replay - warn users
      acceptance_criteria:
      - Store event delivery history including payload, attempts, and response codes
      - Support manual event replay by re-queuing a specific event for delivery
      - Provide delivery logs per webhook showing chronological attempt history
      - Handle replay with deduplication so endpoints can detect and ignore duplicate deliveries
      deliverables:
      - Delivery log persisting every event payload, attempt, and response for auditing
      - Event replay API allowing re-delivery of specific events to their registered endpoint
      - Delivery status tracking dashboard showing pending, delivered, and failed event counts
      - Debugging tools displaying request and response headers and bodies for failed deliveries
  search-engine:
    name: Full-Text Search Engine
    description: Build a search engine with inverted indexes, TF-IDF ranking, fuzzy matching, and query parsing like Elasticsearch/Meilisearch.
    why_expert: Search is everywhere. Understanding inverted indexes, ranking algorithms, and query optimization helps debug
      search issues and build better search UX.
    difficulty: expert
    tags:
    - search
    - indexing
    - information-retrieval
    - ranking
    - nlp
    estimated_hours: 55
    prerequisites: []
    milestones:
    - name: Inverted Index
      description: Implement an inverted index with tokenization and normalization
      skills:
      - Inverted indexes
      - Tokenization
      - Text normalization
      hints:
        level1: 'Inverted index: term -> [doc_id, positions] for fast lookup'
        level2: Tokenization splits text; normalization lowercases, removes accents, stems
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          from collections import defaultdict
          import re
          import unicodedata

          @dataclass
          class Posting:
              doc_id: str
              positions: list[int]      # Word positions in document
              term_frequency: int       # Count of term in doc

          @dataclass
          class Document:
              id: str
              content: str
              fields: dict = field(default_factory=dict)  # title, body, tags, etc.
              metadata: dict = field(default_factory=dict)

          class Tokenizer:
              def __init__(self, min_length: int = 2, stopwords: set = None):
                  self.min_length = min_length
                  self.stopwords = stopwords or {
                      'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',
                      'to', 'for', 'of', 'is', 'it', 'this', 'that', 'with'
                  }

              def tokenize(self, text: str) -> list[tuple[str, int]]:
                  '''Tokenize text, return (token, position) pairs'''
                  # Normalize unicode
                  text = unicodedata.normalize('NFKD', text)
                  text = ''.join(c for c in text if not unicodedata.combining(c))

                  # Lowercase
                  text = text.lower()

                  # Split on non-alphanumeric
                  tokens = []
                  position = 0

                  for match in re.finditer(r'[a-z0-9]+', text):
                      word = match.group()
                      if len(word) >= self.min_length and word not in self.stopwords:
                          tokens.append((word, position))
                      position += 1

                  return tokens

              def stem(self, word: str) -> str:
                  '''Simple Porter-like stemming'''
                  # Very simplified - real implementation uses Porter/Snowball
                  suffixes = ['ing', 'ed', 'ly', 'es', 's']
                  for suffix in suffixes:
                      if word.endswith(suffix) and len(word) > len(suffix) + 2:
                          return word[:-len(suffix)]
                  return word

          class InvertedIndex:
              def __init__(self, tokenizer: Tokenizer = None):
                  self.tokenizer = tokenizer or Tokenizer()
                  self.index: dict[str, list[Posting]] = defaultdict(list)
                  self.documents: dict[str, Document] = {}
                  self.doc_lengths: dict[str, int] = {}  # For BM25

              def add_document(self, doc: Document):
                  '''Index a document'''
                  self.documents[doc.id] = doc

                  # Combine all fields for indexing
                  full_text = doc.content
                  for field_name, field_value in doc.fields.items():
                      if isinstance(field_value, str):
                          full_text += " " + field_value

                  tokens = self.tokenizer.tokenize(full_text)
                  self.doc_lengths[doc.id] = len(tokens)

                  # Build term -> positions mapping for this doc
                  term_positions: dict[str, list[int]] = defaultdict(list)
                  for token, position in tokens:
                      stemmed = self.tokenizer.stem(token)
                      term_positions[stemmed].append(position)

                  # Add to inverted index
                  for term, positions in term_positions.items():
                      posting = Posting(
                          doc_id=doc.id,
                          positions=positions,
                          term_frequency=len(positions)
                      )

                      # Insert in sorted order by doc_id for efficient merging
                      postings = self.index[term]
                      # Binary search insert
                      lo, hi = 0, len(postings)
                      while lo < hi:
                          mid = (lo + hi) // 2
                          if postings[mid].doc_id < doc.id:
                              lo = mid + 1
                          else:
                              hi = mid
                      postings.insert(lo, posting)

              def remove_document(self, doc_id: str):
                  '''Remove document from index'''
                  if doc_id not in self.documents:
                      return

                  del self.documents[doc_id]
                  del self.doc_lengths[doc_id]

                  # Remove from all posting lists
                  for term in list(self.index.keys()):
                      self.index[term] = [p for p in self.index[term] if p.doc_id != doc_id]
                      if not self.index[term]:
                          del self.index[term]

              def get_postings(self, term: str) -> list[Posting]:
                  '''Get postings for a term'''
                  stemmed = self.tokenizer.stem(term.lower())
                  return self.index.get(stemmed, [])

              def search(self, query: str) -> list[tuple[str, list[Posting]]]:
                  '''Basic AND search - returns docs containing all terms'''
                  tokens = self.tokenizer.tokenize(query)
                  if not tokens:
                      return []

                  # Get postings for each term
                  term_postings = []
                  for token, _ in tokens:
                      stemmed = self.tokenizer.stem(token)
                      postings = self.index.get(stemmed, [])
                      if not postings:
                          return []  # AND semantics: no matches
                      term_postings.append((stemmed, postings))

                  # Intersect posting lists (find docs containing all terms)
                  # Start with smallest list for efficiency
                  term_postings.sort(key=lambda x: len(x[1]))

                  result_docs = {p.doc_id for p in term_postings[0][1]}
                  for _, postings in term_postings[1:]:
                      result_docs &= {p.doc_id for p in postings}

                  # Collect matching postings
                  results = []
                  for doc_id in result_docs:
                      doc_postings = []
                      for term, postings in term_postings:
                          for p in postings:
                              if p.doc_id == doc_id:
                                  doc_postings.append(p)
                                  break
                      results.append((doc_id, doc_postings))

                  return results
          ```
      pitfalls:
      - Stemming can over-stem (running -> run -> r) - use proven algorithms
      - Stopwords list should be configurable per language
      - Unicode normalization crucial for international text
      - Index updates are expensive - use batch updates when possible
      acceptance_criteria:
      - Build inverted index from a corpus of text documents
      - Support index updates including document addition and deletion
      - Handle large vocabularies with efficient memory usage
      - Implement index compression reducing storage size on disk
      deliverables:
      - Term-to-document mapping built from tokenized document content
      - Posting list structure storing document IDs and term positions
      - Index construction pipeline processing documents into inverted index
      - Index persistence saving and loading index from disk storage
    - name: TF-IDF & BM25 Ranking
      description: Implement relevance ranking with TF-IDF and BM25 algorithms
      skills:
      - TF-IDF
      - BM25
      - Relevance scoring
      hints:
        level1: 'TF-IDF: terms appearing often in doc but rarely overall are important'
        level2: BM25 improves on TF-IDF with document length normalization
        level3: |2

          ```python
          import math
          from dataclasses import dataclass

          @dataclass
          class SearchResult:
              doc_id: str
              score: float
              highlights: dict = None  # term -> [positions]

          class Scorer:
              def __init__(self, index: InvertedIndex):
                  self.index = index

              def tf(self, term_freq: int) -> float:
                  '''Term frequency component'''
                  return 1 + math.log(term_freq) if term_freq > 0 else 0

              def idf(self, term: str) -> float:
                  '''Inverse document frequency'''
                  N = len(self.index.documents)
                  df = len(self.index.get_postings(term))
                  if df == 0:
                      return 0
                  return math.log(N / df)

              def tf_idf_score(self, doc_id: str, terms: list[str]) -> float:
                  '''Calculate TF-IDF score for document'''
                  score = 0.0
                  for term in terms:
                      postings = self.index.get_postings(term)
                      for posting in postings:
                          if posting.doc_id == doc_id:
                              score += self.tf(posting.term_frequency) * self.idf(term)
                              break
                  return score

          class BM25Scorer(Scorer):
              def __init__(self, index: InvertedIndex, k1: float = 1.2, b: float = 0.75):
                  super().__init__(index)
                  self.k1 = k1  # Term frequency saturation
                  self.b = b    # Length normalization

                  # Precompute average document length
                  total_length = sum(index.doc_lengths.values())
                  self.avg_doc_length = total_length / len(index.documents) if index.documents else 0

              def bm25_idf(self, term: str) -> float:
                  '''BM25 IDF variant'''
                  N = len(self.index.documents)
                  df = len(self.index.get_postings(term))
                  if df == 0:
                      return 0
                  # Robertson-Sparck Jones IDF
                  return math.log((N - df + 0.5) / (df + 0.5) + 1)

              def score(self, doc_id: str, terms: list[str]) -> float:
                  '''Calculate BM25 score'''
                  doc_length = self.index.doc_lengths.get(doc_id, 0)
                  if doc_length == 0:
                      return 0

                  score = 0.0
                  for term in terms:
                      idf = self.bm25_idf(term)

                      # Find term frequency in this doc
                      tf = 0
                      for posting in self.index.get_postings(term):
                          if posting.doc_id == doc_id:
                              tf = posting.term_frequency
                              break

                      if tf == 0:
                          continue

                      # BM25 formula
                      numerator = tf * (self.k1 + 1)
                      denominator = tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)
                      score += idf * (numerator / denominator)

                  return score

              def search(self, query: str, limit: int = 10) -> list[SearchResult]:
                  '''Search with BM25 ranking'''
                  tokens = self.index.tokenizer.tokenize(query)
                  terms = [self.index.tokenizer.stem(t[0]) for t in tokens]

                  # Get candidate documents (OR semantics for ranking)
                  candidate_docs = set()
                  for term in terms:
                      for posting in self.index.get_postings(term):
                          candidate_docs.add(posting.doc_id)

                  # Score all candidates
                  results = []
                  for doc_id in candidate_docs:
                      score = self.score(doc_id, terms)
                      if score > 0:
                          # Build highlights
                          highlights = {}
                          for term in terms:
                              for posting in self.index.get_postings(term):
                                  if posting.doc_id == doc_id:
                                      highlights[term] = posting.positions[:5]  # First 5 positions
                                      break

                          results.append(SearchResult(
                              doc_id=doc_id,
                              score=score,
                              highlights=highlights
                          ))

                  # Sort by score descending
                  results.sort(key=lambda r: r.score, reverse=True)
                  return results[:limit]

          class FieldWeightedScorer(BM25Scorer):
              '''BM25 with field boosting (title matches worth more than body)'''

              def __init__(self, index: InvertedIndex, field_weights: dict = None):
                  super().__init__(index)
                  self.field_weights = field_weights or {
                      'title': 3.0,
                      'body': 1.0,
                      'tags': 2.0
                  }

              def score(self, doc_id: str, terms: list[str]) -> float:
                  '''Score with field weighting'''
                  base_score = super().score(doc_id, terms)

                  # Boost based on field matches
                  doc = self.index.documents.get(doc_id)
                  if not doc:
                      return base_score

                  boost = 1.0
                  for field_name, weight in self.field_weights.items():
                      field_value = doc.fields.get(field_name, '')
                      if isinstance(field_value, str):
                          field_tokens = set(
                              self.index.tokenizer.stem(t[0])
                              for t in self.index.tokenizer.tokenize(field_value)
                          )
                          # Check how many query terms match this field
                          matches = sum(1 for term in terms if term in field_tokens)
                          if matches > 0:
                              boost += (weight - 1.0) * (matches / len(terms))

                  return base_score * boost
          ```
      pitfalls:
      - BM25 k1 and b parameters need tuning for your data
      - Very short documents can get artificially high scores
      - Precompute IDF values for performance
      - Field boosting can be gamed - normalize carefully
      acceptance_criteria:
      - Calculate term frequency for each term in each document
      - Calculate inverse document frequency across the full corpus
      - Implement BM25 scoring formula with saturation and length normalization
      - Rank search results in descending order by relevance score
      deliverables:
      - Term frequency calculation counting occurrences per document
      - Document frequency tracking how many documents contain each term
      - TF-IDF scoring combining term frequency and inverse document frequency
      - BM25 scoring implementation with tunable k1 and b parameters
    - name: Fuzzy Matching & Autocomplete
      description: Implement typo tolerance with Levenshtein distance and prefix-based autocomplete
      skills:
      - Edit distance
      - Prefix trees
      - Fuzzy search
      hints:
        level1: Levenshtein distance counts edits needed to transform one string to another
        level2: For autocomplete, use trie (prefix tree) for O(prefix_length) lookup
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional

          class TrieNode:
              def __init__(self):
                  self.children: dict[str, 'TrieNode'] = {}
                  self.is_end: bool = False
                  self.frequency: int = 0  # For ranking suggestions
                  self.doc_ids: set[str] = set()

          class Trie:
              def __init__(self):
                  self.root = TrieNode()

              def insert(self, word: str, doc_id: str = None, frequency: int = 1):
                  node = self.root
                  for char in word.lower():
                      if char not in node.children:
                          node.children[char] = TrieNode()
                      node = node.children[char]
                  node.is_end = True
                  node.frequency += frequency
                  if doc_id:
                      node.doc_ids.add(doc_id)

              def search_prefix(self, prefix: str, limit: int = 10) -> list[tuple[str, int, set]]:
                  '''Find all words with given prefix'''
                  node = self.root
                  for char in prefix.lower():
                      if char not in node.children:
                          return []
                      node = node.children[char]

                  # DFS to find all words
                  results = []
                  self._collect_words(node, prefix, results, limit * 3)

                  # Sort by frequency and return top results
                  results.sort(key=lambda x: x[1], reverse=True)
                  return results[:limit]

              def _collect_words(self, node: TrieNode, prefix: str,
                                 results: list, limit: int):
                  if len(results) >= limit:
                      return

                  if node.is_end:
                      results.append((prefix, node.frequency, node.doc_ids))

                  for char, child in node.children.items():
                      self._collect_words(child, prefix + char, results, limit)

          class FuzzyMatcher:
              def __init__(self, max_distance: int = 2):
                  self.max_distance = max_distance

              def levenshtein_distance(self, s1: str, s2: str) -> int:
                  '''Calculate Levenshtein edit distance'''
                  if len(s1) < len(s2):
                      s1, s2 = s2, s1

                  if len(s2) == 0:
                      return len(s1)

                  previous_row = list(range(len(s2) + 1))
                  for i, c1 in enumerate(s1):
                      current_row = [i + 1]
                      for j, c2 in enumerate(s2):
                          insertions = previous_row[j + 1] + 1
                          deletions = current_row[j] + 1
                          substitutions = previous_row[j] + (c1 != c2)
                          current_row.append(min(insertions, deletions, substitutions))
                      previous_row = current_row

                  return previous_row[-1]

              def damerau_levenshtein(self, s1: str, s2: str) -> int:
                  '''Damerau-Levenshtein: includes transpositions'''
                  len1, len2 = len(s1), len(s2)

                  # Create distance matrix
                  d = [[0] * (len2 + 1) for _ in range(len1 + 1)]

                  for i in range(len1 + 1):
                      d[i][0] = i
                  for j in range(len2 + 1):
                      d[0][j] = j

                  for i in range(1, len1 + 1):
                      for j in range(1, len2 + 1):
                          cost = 0 if s1[i-1] == s2[j-1] else 1

                          d[i][j] = min(
                              d[i-1][j] + 1,      # Deletion
                              d[i][j-1] + 1,      # Insertion
                              d[i-1][j-1] + cost  # Substitution
                          )

                          # Transposition
                          if i > 1 and j > 1 and s1[i-1] == s2[j-2] and s1[i-2] == s2[j-1]:
                              d[i][j] = min(d[i][j], d[i-2][j-2] + cost)

                  return d[len1][len2]

              def find_fuzzy_matches(self, query: str, vocabulary: list[str],
                                     max_distance: int = None) -> list[tuple[str, int]]:
                  '''Find words within edit distance'''
                  max_dist = max_distance or self.max_distance
                  matches = []

                  for word in vocabulary:
                      distance = self.damerau_levenshtein(query.lower(), word.lower())
                      if distance <= max_dist:
                          matches.append((word, distance))

                  # Sort by distance then alphabetically
                  matches.sort(key=lambda x: (x[1], x[0]))
                  return matches

          class FuzzySearcher:
              def __init__(self, index: InvertedIndex, max_typos: int = 2):
                  self.index = index
                  self.trie = Trie()
                  self.fuzzy = FuzzyMatcher(max_typos)
                  self.vocabulary = set()

                  # Build trie from index vocabulary
                  for term in index.index.keys():
                      self.trie.insert(term)
                      self.vocabulary.add(term)

              def autocomplete(self, prefix: str, limit: int = 10) -> list[dict]:
                  '''Autocomplete suggestions'''
                  suggestions = self.trie.search_prefix(prefix, limit)
                  return [
                      {'term': term, 'frequency': freq, 'doc_count': len(docs)}
                      for term, freq, docs in suggestions
                  ]

              def expand_query(self, query: str) -> list[str]:
                  '''Expand query with fuzzy matches'''
                  tokens = self.index.tokenizer.tokenize(query)
                  expanded_terms = []

                  for token, _ in tokens:
                      stemmed = self.index.tokenizer.stem(token)

                      # Exact match first
                      if stemmed in self.vocabulary:
                          expanded_terms.append(stemmed)
                          continue

                      # Fuzzy match
                      matches = self.fuzzy.find_fuzzy_matches(
                          stemmed, list(self.vocabulary), max_distance=2
                      )

                      if matches:
                          # Add top fuzzy match
                          expanded_terms.append(matches[0][0])
                      else:
                          expanded_terms.append(stemmed)

                  return expanded_terms

              def search(self, query: str, limit: int = 10) -> list[SearchResult]:
                  '''Search with typo tolerance'''
                  expanded_terms = self.expand_query(query)

                  # Use BM25 scorer with expanded terms
                  scorer = BM25Scorer(self.index)

                  # Get candidates
                  candidate_docs = set()
                  for term in expanded_terms:
                      for posting in self.index.get_postings(term):
                          candidate_docs.add(posting.doc_id)

                  results = []
                  for doc_id in candidate_docs:
                      score = scorer.score(doc_id, expanded_terms)
                      if score > 0:
                          results.append(SearchResult(doc_id=doc_id, score=score))

                  results.sort(key=lambda r: r.score, reverse=True)
                  return results[:limit]
          ```
      pitfalls:
      - Edit distance is O(n*m) - prefilter candidates first
      - Max 2 typos is usually enough; more causes too many false matches
      - Transpositions are common typos - Damerau-Levenshtein handles them
      - Short words need fewer allowed edits (1 typo in 'cat' is too much)
      acceptance_criteria:
      - Implement Levenshtein distance matching for approximate string comparison
      - Support prefix-based autocomplete returning matching terms
      - Handle common typos with configurable tolerance threshold
      - Rank autocomplete suggestions by combination of frequency and relevance
      deliverables:
      - Edit distance computation using Levenshtein algorithm
      - Fuzzy search returning results within configurable edit distance
      - Prefix matching for typeahead query completion
      - Autocomplete suggestions ranked by frequency and relevance
    - name: Query Parser & Filters
      description: Implement query parsing with boolean operators, phrases, and field filters
      skills:
      - Query parsing
      - Boolean logic
      - Filter expressions
      hints:
        level1: 'Parse: ''title:python AND (error OR exception) -deprecated'''
        level2: Build AST from query, then evaluate against documents
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Union
          from enum import Enum
          import re

          class QueryOperator(Enum):
              AND = "AND"
              OR = "OR"
              NOT = "NOT"

          @dataclass
          class TermQuery:
              term: str
              field: str = None        # None = search all fields
              is_phrase: bool = False  # "exact phrase"
              is_prefix: bool = False  # python*

          @dataclass
          class BooleanQuery:
              operator: QueryOperator
              operands: list  # List of Query objects

          @dataclass
          class FilterQuery:
              field: str
              operator: str  # =, !=, >, <, >=, <=, in
              value: any

          Query = Union[TermQuery, BooleanQuery, FilterQuery]

          class QueryParser:
              def __init__(self):
                  self.default_operator = QueryOperator.AND

              def parse(self, query_string: str) -> Query:
                  '''Parse query string into AST'''
                  tokens = self._tokenize(query_string)
                  return self._parse_or(tokens, 0)[0]

              def _tokenize(self, query: str) -> list[str]:
                  '''Tokenize query into terms and operators'''
                  tokens = []
                  i = 0

                  while i < len(query):
                      # Skip whitespace
                      if query[i].isspace():
                          i += 1
                          continue

                      # Quoted phrase
                      if query[i] == '"':
                          j = query.find('"', i + 1)
                          if j == -1:
                              j = len(query)
                          tokens.append(('PHRASE', query[i+1:j]))
                          i = j + 1
                          continue

                      # Parentheses
                      if query[i] in '()':
                          tokens.append((query[i], query[i]))
                          i += 1
                          continue

                      # Operators and terms
                      word_match = re.match(r'[\w:*\-\.]+', query[i:])
                      if word_match:
                          word = word_match.group()
                          if word.upper() in ('AND', 'OR', 'NOT'):
                              tokens.append((word.upper(), word.upper()))
                          elif ':' in word:
                              field, value = word.split(':', 1)
                              tokens.append(('FIELD', (field, value)))
                          elif word.startswith('-'):
                              tokens.append(('NOT_TERM', word[1:]))
                          elif word.endswith('*'):
                              tokens.append(('PREFIX', word[:-1]))
                          else:
                              tokens.append(('TERM', word))
                          i += len(word)
                          continue

                      i += 1

                  return tokens

              def _parse_or(self, tokens: list, pos: int) -> tuple[Query, int]:
                  '''Parse OR expressions'''
                  left, pos = self._parse_and(tokens, pos)

                  while pos < len(tokens) and tokens[pos][0] == 'OR':
                      pos += 1
                      right, pos = self._parse_and(tokens, pos)
                      left = BooleanQuery(QueryOperator.OR, [left, right])

                  return left, pos

              def _parse_and(self, tokens: list, pos: int) -> tuple[Query, int]:
                  '''Parse AND expressions'''
                  left, pos = self._parse_not(tokens, pos)

                  while pos < len(tokens):
                      if tokens[pos][0] == 'AND':
                          pos += 1
                          right, pos = self._parse_not(tokens, pos)
                          left = BooleanQuery(QueryOperator.AND, [left, right])
                      elif tokens[pos][0] in ('TERM', 'PHRASE', 'FIELD', 'PREFIX', '('):
                          # Implicit AND
                          right, pos = self._parse_not(tokens, pos)
                          left = BooleanQuery(QueryOperator.AND, [left, right])
                      else:
                          break

                  return left, pos

              def _parse_not(self, tokens: list, pos: int) -> tuple[Query, int]:
                  '''Parse NOT expressions'''
                  if pos < len(tokens):
                      if tokens[pos][0] == 'NOT':
                          pos += 1
                          operand, pos = self._parse_primary(tokens, pos)
                          return BooleanQuery(QueryOperator.NOT, [operand]), pos
                      elif tokens[pos][0] == 'NOT_TERM':
                          term = tokens[pos][1]
                          return BooleanQuery(QueryOperator.NOT, [TermQuery(term)]), pos + 1

                  return self._parse_primary(tokens, pos)

              def _parse_primary(self, tokens: list, pos: int) -> tuple[Query, int]:
                  '''Parse primary expressions'''
                  if pos >= len(tokens):
                      return TermQuery(''), pos

                  token_type, token_value = tokens[pos]

                  if token_type == '(':
                      result, pos = self._parse_or(tokens, pos + 1)
                      if pos < len(tokens) and tokens[pos][0] == ')':
                          pos += 1
                      return result, pos

                  if token_type == 'PHRASE':
                      return TermQuery(token_value, is_phrase=True), pos + 1

                  if token_type == 'FIELD':
                      field, value = token_value
                      return TermQuery(value, field=field), pos + 1

                  if token_type == 'PREFIX':
                      return TermQuery(token_value, is_prefix=True), pos + 1

                  if token_type == 'TERM':
                      return TermQuery(token_value), pos + 1

                  return TermQuery(''), pos + 1

          class QueryExecutor:
              def __init__(self, index: InvertedIndex, scorer: BM25Scorer):
                  self.index = index
                  self.scorer = scorer

              def execute(self, query: Query) -> set[str]:
                  '''Execute query, return matching doc IDs'''
                  if isinstance(query, TermQuery):
                      return self._execute_term(query)
                  elif isinstance(query, BooleanQuery):
                      return self._execute_boolean(query)
                  return set()

              def _execute_term(self, query: TermQuery) -> set[str]:
                  if query.is_phrase:
                      return self._phrase_search(query.term, query.field)
                  elif query.is_prefix:
                      return self._prefix_search(query.term, query.field)
                  else:
                      postings = self.index.get_postings(query.term)
                      return {p.doc_id for p in postings}

              def _execute_boolean(self, query: BooleanQuery) -> set[str]:
                  if query.operator == QueryOperator.AND:
                      result = None
                      for operand in query.operands:
                          matches = self.execute(operand)
                          if result is None:
                              result = matches
                          else:
                              result &= matches
                      return result or set()

                  elif query.operator == QueryOperator.OR:
                      result = set()
                      for operand in query.operands:
                          result |= self.execute(operand)
                      return result

                  elif query.operator == QueryOperator.NOT:
                      all_docs = set(self.index.documents.keys())
                      excluded = self.execute(query.operands[0])
                      return all_docs - excluded

                  return set()

              def _phrase_search(self, phrase: str, field: str = None) -> set[str]:
                  '''Search for exact phrase'''
                  tokens = self.index.tokenizer.tokenize(phrase)
                  if not tokens:
                      return set()

                  terms = [self.index.tokenizer.stem(t[0]) for t in tokens]

                  # Get docs containing all terms
                  candidate_docs = None
                  term_postings = {}
                  for term in terms:
                      postings = self.index.get_postings(term)
                      doc_ids = {p.doc_id for p in postings}
                      if candidate_docs is None:
                          candidate_docs = doc_ids
                      else:
                          candidate_docs &= doc_ids
                      term_postings[term] = {p.doc_id: p.positions for p in postings}

                  if not candidate_docs:
                      return set()

                  # Check phrase positions
                  results = set()
                  for doc_id in candidate_docs:
                      positions_list = [term_postings[t].get(doc_id, []) for t in terms]
                      if self._check_phrase_positions(positions_list):
                          results.add(doc_id)

                  return results

              def _check_phrase_positions(self, positions_list: list[list[int]]) -> bool:
                  '''Check if positions form consecutive phrase'''
                  if not positions_list or not positions_list[0]:
                      return False

                  # For each starting position of first term
                  for start_pos in positions_list[0]:
                      match = True
                      for i, positions in enumerate(positions_list[1:], 1):
                          if start_pos + i not in positions:
                              match = False
                              break
                      if match:
                          return True
                  return False

              def _prefix_search(self, prefix: str, field: str = None) -> set[str]:
                  '''Search for terms starting with prefix'''
                  results = set()
                  prefix = prefix.lower()

                  for term in self.index.index.keys():
                      if term.startswith(prefix):
                          for posting in self.index.index[term]:
                              results.add(posting.doc_id)

                  return results
          ```
      pitfalls:
      - Phrase search requires position tracking - expensive
      - NOT without other terms returns entire index
      - Deeply nested queries can stack overflow - limit depth
      - Wildcard at start is very expensive - requires full scan
      acceptance_criteria:
      - Parse boolean queries with AND, OR, and NOT operators
      - Support phrase queries matching exact word sequences in order
      - Handle field-specific filters like author:name or date:range
      - Support numeric range queries filtering by min and max values
      deliverables:
      - Query tokenization splitting input into individual terms
      - Boolean operator support for AND, OR, and NOT logic
      - Field-specific filtering restricting search to named fields
      - Phrase query support matching exact multi-word sequences
  etl-pipeline:
    name: Data Pipeline / ETL System
    description: Build an ETL pipeline system with job scheduling, transformation DAGs, data validation, and monitoring.
    why_expert: Data pipelines are core infrastructure. Understanding ETL patterns helps build reliable data systems and debug
      data issues.
    difficulty: expert
    tags:
    - etl
    - data-engineering
    - pipelines
    - scheduling
    - batch
    estimated_hours: 45
    prerequisites:
    - job-scheduler
    milestones:
    - name: Pipeline DAG Definition
      description: Implement pipeline definition with dependencies as directed acyclic graph
      skills:
      - DAG modeling
      - Task dependencies
      - Configuration
      hints:
        level1: Tasks form a DAG - each task can depend on others
        level2: Topological sort determines execution order; detect cycles
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Callable, Optional, Any
          from enum import Enum
          from collections import defaultdict
          import time

          class TaskStatus(Enum):
              PENDING = "pending"
              RUNNING = "running"
              SUCCESS = "success"
              FAILED = "failed"
              SKIPPED = "skipped"
              UPSTREAM_FAILED = "upstream_failed"

          @dataclass
          class Task:
              id: str
              name: str
              callable: Callable
              dependencies: list[str] = field(default_factory=list)
              retries: int = 3
              retry_delay: int = 60
              timeout: int = 3600
              params: dict = field(default_factory=dict)

          @dataclass
          class TaskRun:
              task_id: str
              status: TaskStatus
              started_at: Optional[float] = None
              finished_at: Optional[float] = None
              attempt: int = 1
              error: Optional[str] = None
              output: Any = None

          @dataclass
          class Pipeline:
              id: str
              name: str
              tasks: dict[str, Task] = field(default_factory=dict)
              schedule: Optional[str] = None  # Cron expression
              description: str = ""

          class PipelineBuilder:
              def __init__(self, pipeline_id: str, name: str):
                  self.pipeline = Pipeline(id=pipeline_id, name=name)

              def add_task(self, task_id: str, name: str, callable: Callable,
                           dependencies: list[str] = None, **kwargs) -> 'PipelineBuilder':
                  task = Task(
                      id=task_id,
                      name=name,
                      callable=callable,
                      dependencies=dependencies or [],
                      **kwargs
                  )
                  self.pipeline.tasks[task_id] = task
                  return self

              def validate(self) -> list[str]:
                  '''Validate pipeline, return errors'''
                  errors = []

                  # Check for missing dependencies
                  for task_id, task in self.pipeline.tasks.items():
                      for dep in task.dependencies:
                          if dep not in self.pipeline.tasks:
                              errors.append(f"Task '{task_id}' depends on unknown task '{dep}'")

                  # Check for cycles
                  if self._has_cycle():
                      errors.append("Pipeline contains a cycle")

                  return errors

              def _has_cycle(self) -> bool:
                  '''Detect cycles using DFS'''
                  WHITE, GRAY, BLACK = 0, 1, 2
                  color = {t: WHITE for t in self.pipeline.tasks}

                  def dfs(task_id: str) -> bool:
                      color[task_id] = GRAY
                      for dep in self.pipeline.tasks[task_id].dependencies:
                          if color[dep] == GRAY:
                              return True  # Back edge = cycle
                          if color[dep] == WHITE and dfs(dep):
                              return True
                      color[task_id] = BLACK
                      return False

                  for task_id in self.pipeline.tasks:
                      if color[task_id] == WHITE:
                          if dfs(task_id):
                              return True
                  return False

              def build(self) -> Pipeline:
                  errors = self.validate()
                  if errors:
                      raise ValueError(f"Invalid pipeline: {errors}")
                  return self.pipeline

          class DAGExecutor:
              def __init__(self):
                  self.runs: dict[str, dict[str, TaskRun]] = {}  # pipeline_run_id -> task_id -> run

              def topological_sort(self, pipeline: Pipeline) -> list[str]:
                  '''Get tasks in execution order'''
                  in_degree = defaultdict(int)
                  for task_id, task in pipeline.tasks.items():
                      for dep in task.dependencies:
                          in_degree[task_id] += 1

                  # Start with tasks that have no dependencies
                  queue = [t for t in pipeline.tasks if in_degree[t] == 0]
                  result = []

                  while queue:
                      task_id = queue.pop(0)
                      result.append(task_id)

                      # Reduce in-degree of dependent tasks
                      for other_id, other_task in pipeline.tasks.items():
                          if task_id in other_task.dependencies:
                              in_degree[other_id] -= 1
                              if in_degree[other_id] == 0:
                                  queue.append(other_id)

                  return result

              def get_ready_tasks(self, pipeline: Pipeline,
                                  task_runs: dict[str, TaskRun]) -> list[str]:
                  '''Get tasks ready to execute (all deps satisfied)'''
                  ready = []

                  for task_id, task in pipeline.tasks.items():
                      run = task_runs.get(task_id)

                      # Already running or finished
                      if run and run.status != TaskStatus.PENDING:
                          continue

                      # Check all dependencies succeeded
                      deps_satisfied = True
                      for dep in task.dependencies:
                          dep_run = task_runs.get(dep)
                          if not dep_run or dep_run.status != TaskStatus.SUCCESS:
                              deps_satisfied = False
                              break

                      if deps_satisfied:
                          ready.append(task_id)

                  return ready

              def should_skip_task(self, pipeline: Pipeline, task_id: str,
                                   task_runs: dict[str, TaskRun]) -> bool:
                  '''Check if task should be skipped due to upstream failure'''
                  task = pipeline.tasks[task_id]

                  for dep in task.dependencies:
                      dep_run = task_runs.get(dep)
                      if dep_run and dep_run.status in [TaskStatus.FAILED, TaskStatus.UPSTREAM_FAILED]:
                          return True
                  return False
          ```
      pitfalls:
      - Cycle detection must run before execution starts
      - Topological sort doesn't handle failures - need separate tracking
      - Parallel execution needs thread-safe state management
      - Long chains of dependencies slow down pipeline
      acceptance_criteria:
      - Define tasks with dependencies in YAML or Python configuration format
      - Parse DAG definition from YAML or Python configuration files correctly
      - Validate no cycles exist in the DAG and report errors for invalid graphs
      - Support parameterized tasks with runtime variable substitution in configs
      deliverables:
      - Task definition with name, type, and configuration parameters
      - Dependency specification declaring upstream task requirements
      - DAG validation ensuring no cycles exist in task dependencies
      - DAG visualization rendering pipeline structure as a graph
    - name: Data Extraction & Loading
      description: Implement extractors and loaders for common data sources
      skills:
      - Data connectors
      - Batch processing
      - Incremental loads
      hints:
        level1: Extractors read from sources; loaders write to destinations
        level2: Support full and incremental loads using watermarks
        level3: |2

          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass
          from typing import Iterator, Optional, Any
          import json

          @dataclass
          class DataBatch:
              records: list[dict]
              schema: Optional[dict] = None
              metadata: dict = field(default_factory=dict)

          class Extractor(ABC):
              @abstractmethod
              def extract(self, **kwargs) -> Iterator[DataBatch]:
                  '''Extract data in batches'''
                  pass

              @abstractmethod
              def get_watermark(self) -> Any:
                  '''Get current watermark for incremental loads'''
                  pass

          class Loader(ABC):
              @abstractmethod
              def load(self, batch: DataBatch) -> int:
                  '''Load batch, return records loaded'''
                  pass

              @abstractmethod
              def begin_transaction(self):
                  pass

              @abstractmethod
              def commit(self):
                  pass

              @abstractmethod
              def rollback(self):
                  pass

          class DatabaseExtractor(Extractor):
              def __init__(self, connection, table: str, batch_size: int = 1000,
                           watermark_column: str = None):
                  self.conn = connection
                  self.table = table
                  self.batch_size = batch_size
                  self.watermark_column = watermark_column
                  self._watermark = None

              def extract(self, since: Any = None, **kwargs) -> Iterator[DataBatch]:
                  query = f"SELECT * FROM {self.table}"
                  params = []

                  if since and self.watermark_column:
                      query += f" WHERE {self.watermark_column} > ?"
                      params.append(since)

                  if self.watermark_column:
                      query += f" ORDER BY {self.watermark_column}"

                  cursor = self.conn.execute(query, params)
                  columns = [desc[0] for desc in cursor.description]

                  batch = []
                  for row in cursor:
                      record = dict(zip(columns, row))
                      batch.append(record)

                      if self.watermark_column:
                          self._watermark = record[self.watermark_column]

                      if len(batch) >= self.batch_size:
                          yield DataBatch(records=batch, schema={"columns": columns})
                          batch = []

                  if batch:
                      yield DataBatch(records=batch, schema={"columns": columns})

              def get_watermark(self) -> Any:
                  return self._watermark

          class APIExtractor(Extractor):
              def __init__(self, base_url: str, endpoint: str,
                           auth: dict = None, batch_size: int = 100):
                  self.base_url = base_url
                  self.endpoint = endpoint
                  self.auth = auth
                  self.batch_size = batch_size
                  self._watermark = None

              def extract(self, since: Any = None, **kwargs) -> Iterator[DataBatch]:
                  import httpx

                  page = 1
                  while True:
                      params = {"page": page, "per_page": self.batch_size}
                      if since:
                          params["since"] = since

                      response = httpx.get(
                          f"{self.base_url}/{self.endpoint}",
                          params=params,
                          headers=self._build_headers()
                      )
                      response.raise_for_status()

                      data = response.json()
                      records = data.get("data", data)  # Handle nested or flat response

                      if not records:
                          break

                      # Track watermark
                      if records and "updated_at" in records[-1]:
                          self._watermark = records[-1]["updated_at"]

                      yield DataBatch(records=records)
                      page += 1

                      # Check if last page
                      if len(records) < self.batch_size:
                          break

              def _build_headers(self) -> dict:
                  headers = {"Accept": "application/json"}
                  if self.auth:
                      if "token" in self.auth:
                          headers["Authorization"] = f"Bearer {self.auth['token']}"
                  return headers

              def get_watermark(self) -> Any:
                  return self._watermark

          class FileExtractor(Extractor):
              def __init__(self, path: str, format: str = "json", batch_size: int = 1000):
                  self.path = path
                  self.format = format
                  self.batch_size = batch_size

              def extract(self, **kwargs) -> Iterator[DataBatch]:
                  import csv

                  if self.format == "json":
                      with open(self.path) as f:
                          data = json.load(f)
                          for i in range(0, len(data), self.batch_size):
                              yield DataBatch(records=data[i:i + self.batch_size])

                  elif self.format == "csv":
                      with open(self.path) as f:
                          reader = csv.DictReader(f)
                          batch = []
                          for row in reader:
                              batch.append(row)
                              if len(batch) >= self.batch_size:
                                  yield DataBatch(records=batch)
                                  batch = []
                          if batch:
                              yield DataBatch(records=batch)

                  elif self.format == "jsonl":
                      with open(self.path) as f:
                          batch = []
                          for line in f:
                              batch.append(json.loads(line))
                              if len(batch) >= self.batch_size:
                                  yield DataBatch(records=batch)
                                  batch = []
                          if batch:
                              yield DataBatch(records=batch)

              def get_watermark(self) -> Any:
                  import os
                  return os.path.getmtime(self.path)

          class DatabaseLoader(Loader):
              def __init__(self, connection, table: str, mode: str = "append"):
                  self.conn = connection
                  self.table = table
                  self.mode = mode  # append, replace, upsert

              def begin_transaction(self):
                  pass  # Connection handles this

              def load(self, batch: DataBatch) -> int:
                  if not batch.records:
                      return 0

                  columns = list(batch.records[0].keys())
                  placeholders = ", ".join(["?" for _ in columns])
                  column_names = ", ".join(columns)

                  if self.mode == "upsert":
                      # SQLite upsert syntax
                      query = f'''
                          INSERT INTO {self.table} ({column_names})
                          VALUES ({placeholders})
                          ON CONFLICT DO UPDATE SET
                          {", ".join(f"{c} = excluded.{c}" for c in columns)}
                      '''
                  else:
                      query = f"INSERT INTO {self.table} ({column_names}) VALUES ({placeholders})"

                  for record in batch.records:
                      values = [record.get(c) for c in columns]
                      self.conn.execute(query, values)

                  return len(batch.records)

              def commit(self):
                  self.conn.commit()

              def rollback(self):
                  self.conn.rollback()
          ```
      pitfalls:
      - API pagination can change during extraction - use stable cursors
      - Watermark must be saved atomically with loaded data
      - Upsert needs proper conflict detection keys
      - Large batches can exhaust memory - stream when possible
      acceptance_criteria:
      - Extract data from databases using configurable SQL query connectors
      - Extract data from REST APIs using configurable HTTP client connectors
      - Load data to target destination with configurable batch size for efficiency
      - Handle schema mapping between source and destination column definitions
      deliverables:
      - Source connectors for databases, APIs, and file systems
      - Destination connectors for target storage and database systems
      - Incremental extraction tracking changed records since last run
      - Bulk loading with batched inserts for efficient data transfer
    - name: Data Transformations
      description: Implement transformation operations with schema validation
      skills:
      - Data mapping
      - Schema validation
      - Type conversion
      hints:
        level1: 'Transformations: map, filter, aggregate, join'
        level2: Validate schema before and after transformation
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Callable, Any, Optional
          from abc import ABC, abstractmethod
          import json
          import re

          class Transform(ABC):
              @abstractmethod
              def apply(self, batch: DataBatch) -> DataBatch:
                  pass

          class MapTransform(Transform):
              def __init__(self, mapper: Callable[[dict], dict]):
                  self.mapper = mapper

              def apply(self, batch: DataBatch) -> DataBatch:
                  transformed = [self.mapper(record) for record in batch.records]
                  return DataBatch(records=transformed, metadata=batch.metadata)

          class FilterTransform(Transform):
              def __init__(self, predicate: Callable[[dict], bool]):
                  self.predicate = predicate

              def apply(self, batch: DataBatch) -> DataBatch:
                  filtered = [r for r in batch.records if self.predicate(r)]
                  return DataBatch(records=filtered, metadata=batch.metadata)

          class RenameColumnsTransform(Transform):
              def __init__(self, mapping: dict[str, str]):
                  self.mapping = mapping

              def apply(self, batch: DataBatch) -> DataBatch:
                  transformed = []
                  for record in batch.records:
                      new_record = {}
                      for key, value in record.items():
                          new_key = self.mapping.get(key, key)
                          new_record[new_key] = value
                      transformed.append(new_record)
                  return DataBatch(records=transformed, metadata=batch.metadata)

          class TypeCastTransform(Transform):
              def __init__(self, type_mapping: dict[str, type]):
                  self.type_mapping = type_mapping

              def apply(self, batch: DataBatch) -> DataBatch:
                  transformed = []
                  for record in batch.records:
                      new_record = dict(record)
                      for field, target_type in self.type_mapping.items():
                          if field in new_record:
                              new_record[field] = self._cast(new_record[field], target_type)
                      transformed.append(new_record)
                  return DataBatch(records=transformed, metadata=batch.metadata)

              def _cast(self, value: Any, target_type: type) -> Any:
                  if value is None:
                      return None
                  if target_type == int:
                      return int(float(value))
                  elif target_type == float:
                      return float(value)
                  elif target_type == bool:
                      return value in (True, 'true', 'True', '1', 1)
                  elif target_type == str:
                      return str(value)
                  return value

          class DeriveColumnTransform(Transform):
              def __init__(self, column: str, expression: Callable[[dict], Any]):
                  self.column = column
                  self.expression = expression

              def apply(self, batch: DataBatch) -> DataBatch:
                  transformed = []
                  for record in batch.records:
                      new_record = dict(record)
                      new_record[self.column] = self.expression(record)
                      transformed.append(new_record)
                  return DataBatch(records=transformed, metadata=batch.metadata)

          # Schema validation
          @dataclass
          class FieldSchema:
              name: str
              type: str  # string, int, float, bool, datetime, json
              required: bool = True
              nullable: bool = False
              pattern: Optional[str] = None
              min_value: Optional[float] = None
              max_value: Optional[float] = None
              enum_values: Optional[list] = None

          @dataclass
          class DataSchema:
              fields: list[FieldSchema]
              strict: bool = False  # Reject unknown fields

          class SchemaValidator:
              def __init__(self, schema: DataSchema):
                  self.schema = schema
                  self.field_map = {f.name: f for f in schema.fields}

              def validate(self, batch: DataBatch) -> list[dict]:
                  '''Validate batch, return list of errors'''
                  errors = []

                  for i, record in enumerate(batch.records):
                      record_errors = self._validate_record(record, i)
                      errors.extend(record_errors)

                  return errors

              def _validate_record(self, record: dict, row_num: int) -> list[dict]:
                  errors = []

                  # Check required fields
                  for field in self.schema.fields:
                      if field.required and field.name not in record:
                          errors.append({
                              "row": row_num,
                              "field": field.name,
                              "error": "required_field_missing"
                          })

                  # Check each field
                  for field_name, value in record.items():
                      if field_name not in self.field_map:
                          if self.schema.strict:
                              errors.append({
                                  "row": row_num,
                                  "field": field_name,
                                  "error": "unknown_field"
                              })
                          continue

                      field = self.field_map[field_name]
                      field_errors = self._validate_field(value, field, row_num)
                      errors.extend(field_errors)

                  return errors

              def _validate_field(self, value: Any, field: FieldSchema,
                                  row_num: int) -> list[dict]:
                  errors = []

                  # Null check
                  if value is None:
                      if not field.nullable:
                          errors.append({
                              "row": row_num,
                              "field": field.name,
                              "error": "null_not_allowed"
                          })
                      return errors

                  # Type check
                  type_valid = self._check_type(value, field.type)
                  if not type_valid:
                      errors.append({
                          "row": row_num,
                          "field": field.name,
                          "error": f"invalid_type_expected_{field.type}",
                          "value": str(value)[:100]
                      })
                      return errors

                  # Pattern check
                  if field.pattern and field.type == "string":
                      if not re.match(field.pattern, str(value)):
                          errors.append({
                              "row": row_num,
                              "field": field.name,
                              "error": "pattern_mismatch"
                          })

                  # Range check
                  if field.min_value is not None and value < field.min_value:
                      errors.append({
                          "row": row_num,
                          "field": field.name,
                          "error": f"below_minimum_{field.min_value}"
                      })
                  if field.max_value is not None and value > field.max_value:
                      errors.append({
                          "row": row_num,
                          "field": field.name,
                          "error": f"above_maximum_{field.max_value}"
                      })

                  # Enum check
                  if field.enum_values and value not in field.enum_values:
                      errors.append({
                          "row": row_num,
                          "field": field.name,
                          "error": "invalid_enum_value"
                      })

                  return errors

              def _check_type(self, value: Any, expected: str) -> bool:
                  if expected == "string":
                      return isinstance(value, str)
                  elif expected == "int":
                      return isinstance(value, int) and not isinstance(value, bool)
                  elif expected == "float":
                      return isinstance(value, (int, float)) and not isinstance(value, bool)
                  elif expected == "bool":
                      return isinstance(value, bool)
                  elif expected == "json":
                      return isinstance(value, (dict, list))
                  return True
          ```
      pitfalls:
      - Type coercion can lose precision (float -> int)
      - Schema validation on every record is slow - sample or skip
      - Derived columns can fail - handle exceptions per record
      - Null handling differs between databases
      acceptance_criteria:
      - Support SQL-based transformations for declarative data manipulation
      - Support Python UDF transformations for custom processing logic
      - Handle null values and type conversions without data loss or errors
      - Implement data validation rules that reject or flag invalid records
      deliverables:
      - Transform functions for data manipulation and enrichment
      - Data cleaning operations for null handling and deduplication
      - Schema mapping for column renaming and type conversion
      - Aggregation operations for grouping and summarization
    - name: Pipeline Orchestration & Monitoring
      description: Implement pipeline execution with monitoring, alerting, and lineage tracking
      skills:
      - Orchestration
      - Monitoring
      - Data lineage
      hints:
        level1: 'Track execution metrics: duration, records, errors'
        level2: 'Data lineage: track where data came from and where it went'
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional, Any
          from enum import Enum
          import time
          import traceback
          from concurrent.futures import ThreadPoolExecutor

          class PipelineRunStatus(Enum):
              PENDING = "pending"
              RUNNING = "running"
              SUCCESS = "success"
              FAILED = "failed"
              CANCELLED = "cancelled"

          @dataclass
          class PipelineRun:
              id: str
              pipeline_id: str
              status: PipelineRunStatus
              started_at: Optional[float] = None
              finished_at: Optional[float] = None
              task_runs: dict[str, TaskRun] = field(default_factory=dict)
              error: Optional[str] = None
              metrics: dict = field(default_factory=dict)

          @dataclass
          class LineageRecord:
              run_id: str
              task_id: str
              source: str          # Table, API, file path
              destination: str
              records_read: int
              records_written: int
              timestamp: float

          class PipelineOrchestrator:
              def __init__(self, max_parallel: int = 4):
                  self.executor = ThreadPoolExecutor(max_workers=max_parallel)
                  self.dag_executor = DAGExecutor()
                  self.runs: dict[str, PipelineRun] = {}
                  self.lineage: list[LineageRecord] = []

              def run_pipeline(self, pipeline: Pipeline,
                               params: dict = None) -> PipelineRun:
                  '''Execute pipeline'''
                  run_id = f"{pipeline.id}_{int(time.time())}"
                  run = PipelineRun(
                      id=run_id,
                      pipeline_id=pipeline.id,
                      status=PipelineRunStatus.RUNNING,
                      started_at=time.time()
                  )
                  self.runs[run_id] = run

                  # Initialize task runs
                  for task_id in pipeline.tasks:
                      run.task_runs[task_id] = TaskRun(
                          task_id=task_id,
                          status=TaskStatus.PENDING
                      )

                  try:
                      self._execute_pipeline(pipeline, run, params or {})
                      run.status = PipelineRunStatus.SUCCESS
                  except Exception as e:
                      run.status = PipelineRunStatus.FAILED
                      run.error = str(e)
                  finally:
                      run.finished_at = time.time()
                      self._collect_metrics(run)

                  return run

              def _execute_pipeline(self, pipeline: Pipeline, run: PipelineRun,
                                    params: dict):
                  '''Execute tasks in dependency order'''
                  completed = set()

                  while len(completed) < len(pipeline.tasks):
                      # Find tasks ready to run
                      ready = self.dag_executor.get_ready_tasks(pipeline, run.task_runs)

                      if not ready:
                          # Check for failures
                          failed = [t for t, r in run.task_runs.items()
                                   if r.status == TaskStatus.FAILED]
                          if failed:
                              # Mark downstream as upstream_failed
                              for task_id, task_run in run.task_runs.items():
                                  if task_run.status == TaskStatus.PENDING:
                                      if self.dag_executor.should_skip_task(
                                          pipeline, task_id, run.task_runs
                                      ):
                                          task_run.status = TaskStatus.UPSTREAM_FAILED
                                          completed.add(task_id)
                              if len(completed) >= len(pipeline.tasks):
                                  break
                              raise Exception(f"Pipeline failed: tasks {failed}")
                          continue

                      # Execute ready tasks (could parallelize here)
                      for task_id in ready:
                          self._execute_task(pipeline.tasks[task_id], run, params)
                          completed.add(task_id)

              def _execute_task(self, task: Task, run: PipelineRun, params: dict):
                  '''Execute single task with retries'''
                  task_run = run.task_runs[task.id]
                  task_run.started_at = time.time()
                  task_run.status = TaskStatus.RUNNING

                  # Merge pipeline params with task params
                  task_params = {**task.params, **params}

                  for attempt in range(1, task.retries + 1):
                      task_run.attempt = attempt
                      try:
                          result = task.callable(**task_params)
                          task_run.status = TaskStatus.SUCCESS
                          task_run.output = result
                          task_run.finished_at = time.time()

                          # Record lineage if result contains it
                          if isinstance(result, dict) and 'lineage' in result:
                              self._record_lineage(run.id, task.id, result['lineage'])

                          return

                      except Exception as e:
                          task_run.error = f"Attempt {attempt}: {str(e)}"
                          if attempt < task.retries:
                              time.sleep(task.retry_delay)
                          else:
                              task_run.status = TaskStatus.FAILED
                              task_run.finished_at = time.time()
                              raise

              def _record_lineage(self, run_id: str, task_id: str, lineage: dict):
                  record = LineageRecord(
                      run_id=run_id,
                      task_id=task_id,
                      source=lineage.get('source', 'unknown'),
                      destination=lineage.get('destination', 'unknown'),
                      records_read=lineage.get('records_read', 0),
                      records_written=lineage.get('records_written', 0),
                      timestamp=time.time()
                  )
                  self.lineage.append(record)

              def _collect_metrics(self, run: PipelineRun):
                  '''Collect run metrics'''
                  run.metrics = {
                      'duration_seconds': (run.finished_at or time.time()) - run.started_at,
                      'tasks_total': len(run.task_runs),
                      'tasks_succeeded': sum(1 for r in run.task_runs.values()
                                             if r.status == TaskStatus.SUCCESS),
                      'tasks_failed': sum(1 for r in run.task_runs.values()
                                          if r.status == TaskStatus.FAILED),
                      'total_records': sum(
                          r.output.get('records_processed', 0)
                          for r in run.task_runs.values()
                          if r.output and isinstance(r.output, dict)
                      )
                  }

              def get_lineage(self, destination: str) -> list[LineageRecord]:
                  '''Get lineage for a destination'''
                  return [l for l in self.lineage if l.destination == destination]

          class PipelineMonitor:
              def __init__(self, orchestrator: PipelineOrchestrator):
                  self.orchestrator = orchestrator
                  self.alerts: list[dict] = []

              def check_health(self) -> dict:
                  '''Check overall pipeline health'''
                  recent_runs = [
                      r for r in self.orchestrator.runs.values()
                      if r.finished_at and time.time() - r.finished_at < 86400
                  ]

                  success_rate = (
                      sum(1 for r in recent_runs if r.status == PipelineRunStatus.SUCCESS) /
                      len(recent_runs) if recent_runs else 1.0
                  )

                  return {
                      'total_runs_24h': len(recent_runs),
                      'success_rate': success_rate,
                      'healthy': success_rate > 0.9
                  }

              def alert_on_failure(self, run: PipelineRun):
                  '''Create alert for failed run'''
                  if run.status == PipelineRunStatus.FAILED:
                      self.alerts.append({
                          'type': 'pipeline_failure',
                          'pipeline_id': run.pipeline_id,
                          'run_id': run.id,
                          'error': run.error,
                          'timestamp': time.time()
                      })
          ```
      pitfalls:
      - Retry delay should use exponential backoff
      - Parallel task execution needs careful state management
      - Lineage tracking adds overhead - make it optional
      - Cancelled pipelines need cleanup of partial data
      acceptance_criteria:
      - Schedule pipeline runs using cron expressions or event-based triggers
      - Track run status and timing for each task with per-task state reporting
      - Handle task failures with configurable alerts and automatic retry logic
      - Provide run history and log access for debugging and audit purposes
      deliverables:
      - Scheduler integration for cron-based and event-driven pipeline triggers
      - Task execution tracking with status, timing, and dependency resolution
      - Failure handling with configurable retry and alerting policies
      - Monitoring dashboard displaying pipeline run history and performance
  cdc-system:
    name: Change Data Capture (CDC) System
    description: Build a CDC system that captures database changes in real-time using transaction logs and streams them to
      consumers.
    why_expert: CDC is essential for real-time data sync. Understanding WAL parsing, log-based replication, and event streaming
      enables building reactive data systems.
    difficulty: expert
    tags:
    - cdc
    - replication
    - streaming
    - database
    - events
    estimated_hours: 50
    prerequisites:
    - build-sqlite
    milestones:
    - name: Log Parsing & Change Events
      description: Parse database transaction logs to extract change events
      skills:
      - WAL parsing
      - Binary protocols
      - Event modeling
      hints:
        level1: 'WAL/binlog contains: operation type, table, old values, new values'
        level2: 'Events: INSERT, UPDATE (with before/after), DELETE'
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional, Any
          from enum import Enum
          import json
          import time

          class OperationType(Enum):
              INSERT = "insert"
              UPDATE = "update"
              DELETE = "delete"
              TRUNCATE = "truncate"
              DDL = "ddl"

          @dataclass
          class ChangeEvent:
              id: str
              timestamp: float
              transaction_id: str
              operation: OperationType
              database: str
              schema: str
              table: str
              primary_key: dict
              before: Optional[dict] = None   # For UPDATE/DELETE
              after: Optional[dict] = None    # For INSERT/UPDATE
              metadata: dict = field(default_factory=dict)

              def to_json(self) -> str:
                  return json.dumps({
                      'id': self.id,
                      'timestamp': self.timestamp,
                      'transaction_id': self.transaction_id,
                      'operation': self.operation.value,
                      'database': self.database,
                      'schema': self.schema,
                      'table': self.table,
                      'primary_key': self.primary_key,
                      'before': self.before,
                      'after': self.after
                  })

          @dataclass
          class Position:
              '''Track position in transaction log'''
              log_file: str
              log_position: int
              timestamp: float

          class LogParser:
              '''Base class for log parsers'''

              def __init__(self, position: Position = None):
                  self.position = position
                  self.event_count = 0

              def parse(self) -> Iterator[ChangeEvent]:
                  raise NotImplementedError

              def get_position(self) -> Position:
                  return self.position

          # Simulated PostgreSQL logical replication
          class PostgresLogicalDecoder:
              '''Parse PostgreSQL logical replication stream'''

              def __init__(self, connection, slot_name: str, publication: str):
                  self.conn = connection
                  self.slot_name = slot_name
                  self.publication = publication
                  self.position = None

              def create_slot(self):
                  '''Create replication slot'''
                  self.conn.execute(f'''
                      SELECT pg_create_logical_replication_slot(
                          '{self.slot_name}', 'pgoutput'
                      )
                  ''')

              def read_changes(self, start_lsn: str = None) -> Iterator[ChangeEvent]:
                  '''Read changes from replication slot'''
                  # In production: use psycopg2 replication protocol
                  # This is a simulation using polling

                  cursor = self.conn.execute(f'''
                      SELECT lsn, data FROM pg_logical_slot_get_changes(
                          '{self.slot_name}', NULL, NULL,
                          'publication_names', '{self.publication}'
                      )
                  ''')

                  for lsn, data in cursor:
                      event = self._parse_message(lsn, data)
                      if event:
                          yield event

              def _parse_message(self, lsn: str, data: bytes) -> Optional[ChangeEvent]:
                  '''Parse logical replication message'''
                  # pgoutput format: message_type + payload
                  if not data:
                      return None

                  msg_type = data[0:1]

                  if msg_type == b'I':  # Insert
                      return self._parse_insert(lsn, data)
                  elif msg_type == b'U':  # Update
                      return self._parse_update(lsn, data)
                  elif msg_type == b'D':  # Delete
                      return self._parse_delete(lsn, data)

                  return None

              def _parse_insert(self, lsn: str, data: bytes) -> ChangeEvent:
                  # Simplified - real implementation parses binary format
                  table_info = self._extract_table_info(data)
                  new_tuple = self._extract_tuple(data)

                  return ChangeEvent(
                      id=f"{lsn}_{time.time()}",
                      timestamp=time.time(),
                      transaction_id=lsn,
                      operation=OperationType.INSERT,
                      database=table_info['database'],
                      schema=table_info['schema'],
                      table=table_info['table'],
                      primary_key=self._extract_pk(new_tuple),
                      after=new_tuple
                  )

          # MySQL binlog simulation
          class MySQLBinlogReader:
              '''Parse MySQL binary log'''

              def __init__(self, connection, server_id: int):
                  self.conn = connection
                  self.server_id = server_id
                  self.position = None

              def read_events(self, binlog_file: str = None,
                              position: int = 0) -> Iterator[ChangeEvent]:
                  '''Read events from binlog'''
                  # In production: use mysql-replication package
                  # This simulates by querying

                  cursor = self.conn.execute('''
                      SHOW BINLOG EVENTS
                  ''')

                  for row in cursor:
                      event = self._parse_binlog_event(row)
                      if event:
                          yield event

              def _parse_binlog_event(self, row) -> Optional[ChangeEvent]:
                  event_type = row['Event_type']

                  if event_type == 'Write_rows':
                      return self._create_event(OperationType.INSERT, row)
                  elif event_type == 'Update_rows':
                      return self._create_event(OperationType.UPDATE, row)
                  elif event_type == 'Delete_rows':
                      return self._create_event(OperationType.DELETE, row)

                  return None

          # Generic trigger-based CDC (works with any database)
          class TriggerBasedCDC:
              '''CDC using database triggers - fallback for when log access is unavailable'''

              def __init__(self, connection, tables: list[str]):
                  self.conn = connection
                  self.tables = tables
                  self.change_table = '_cdc_changes'

              def setup(self):
                  '''Create change capture infrastructure'''
                  # Create change log table
                  self.conn.execute(f'''
                      CREATE TABLE IF NOT EXISTS {self.change_table} (
                          id INTEGER PRIMARY KEY AUTOINCREMENT,
                          table_name TEXT,
                          operation TEXT,
                          primary_key TEXT,
                          old_data TEXT,
                          new_data TEXT,
                          timestamp REAL,
                          processed INTEGER DEFAULT 0
                      )
                  ''')

                  # Create triggers for each table
                  for table in self.tables:
                      self._create_triggers(table)

              def _create_triggers(self, table: str):
                  # Insert trigger
                  self.conn.execute(f'''
                      CREATE TRIGGER IF NOT EXISTS {table}_insert_cdc
                      AFTER INSERT ON {table}
                      BEGIN
                          INSERT INTO {self.change_table}
                          (table_name, operation, primary_key, new_data, timestamp)
                          VALUES ('{table}', 'INSERT',
                                  NEW.id, json_object('id', NEW.id), unixepoch());
                      END
                  ''')

                  # Update trigger
                  self.conn.execute(f'''
                      CREATE TRIGGER IF NOT EXISTS {table}_update_cdc
                      AFTER UPDATE ON {table}
                      BEGIN
                          INSERT INTO {self.change_table}
                          (table_name, operation, primary_key, old_data, new_data, timestamp)
                          VALUES ('{table}', 'UPDATE',
                                  NEW.id,
                                  json_object('id', OLD.id),
                                  json_object('id', NEW.id),
                                  unixepoch());
                      END
                  ''')

                  # Delete trigger
                  self.conn.execute(f'''
                      CREATE TRIGGER IF NOT EXISTS {table}_delete_cdc
                      AFTER DELETE ON {table}
                      BEGIN
                          INSERT INTO {self.change_table}
                          (table_name, operation, primary_key, old_data, timestamp)
                          VALUES ('{table}', 'DELETE',
                                  OLD.id, json_object('id', OLD.id), unixepoch());
                      END
                  ''')

              def read_changes(self, batch_size: int = 100) -> list[ChangeEvent]:
                  '''Read unprocessed changes'''
                  cursor = self.conn.execute(f'''
                      SELECT * FROM {self.change_table}
                      WHERE processed = 0
                      ORDER BY id
                      LIMIT ?
                  ''', (batch_size,))

                  events = []
                  for row in cursor:
                      event = ChangeEvent(
                          id=str(row['id']),
                          timestamp=row['timestamp'],
                          transaction_id=str(row['id']),
                          operation=OperationType(row['operation'].lower()),
                          database='main',
                          schema='public',
                          table=row['table_name'],
                          primary_key=json.loads(row['primary_key']) if row['primary_key'] else {},
                          before=json.loads(row['old_data']) if row['old_data'] else None,
                          after=json.loads(row['new_data']) if row['new_data'] else None
                      )
                      events.append(event)

                  return events

              def mark_processed(self, event_ids: list[str]):
                  '''Mark events as processed'''
                  placeholders = ','.join(['?' for _ in event_ids])
                  self.conn.execute(f'''
                      UPDATE {self.change_table}
                      SET processed = 1
                      WHERE id IN ({placeholders})
                  ''', event_ids)
                  self.conn.commit()
          ```
      pitfalls:
      - WAL parsing is database-specific - need adapter per DB
      - Trigger-based CDC adds write overhead to main tables
      - Binary log formats change between versions
      - Large transactions can create huge change events
      acceptance_criteria:
      - Parser connects to PostgreSQL WAL or MySQL binlog and reads transaction entries
      - INSERT, UPDATE, and DELETE operations are correctly extracted from log entries
      - Change events include before-image and after-image row values for UPDATE operations
      - Position tracking enables resuming log reading from last processed offset after restart
      deliverables:
      - Database log connection reading WAL or binlog transaction entries
      - Log event parser extracting operation type, table, and row data from entries
      - Change event construction producing structured events with before and after values
      - Position and offset tracking recording current read position for resumption
    - name: Event Streaming & Delivery
      description: Stream change events to consumers with ordering and delivery guarantees
      skills:
      - Event streaming
      - Ordering guarantees
      - Consumer groups
      hints:
        level1: Events for same row must be delivered in order
        level2: Partition by table/pk for parallel processing with ordering
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Callable, Optional
          from collections import defaultdict
          import threading
          import queue
          import time

          @dataclass
          class EventPartition:
              key: str  # table:pk combination
              events: queue.Queue = field(default_factory=queue.Queue)
              last_event_time: float = 0
              consumer_offset: int = 0

          @dataclass
          class Consumer:
              id: str
              group_id: str
              assigned_partitions: set[str] = field(default_factory=set)
              last_heartbeat: float = 0

          class CDCEventStream:
              '''Stream CDC events with ordering guarantees'''

              def __init__(self, num_partitions: int = 16):
                  self.num_partitions = num_partitions
                  self.partitions: dict[int, list[ChangeEvent]] = defaultdict(list)
                  self.partition_locks: dict[int, threading.Lock] = {
                      i: threading.Lock() for i in range(num_partitions)
                  }

                  # Consumer group management
                  self.consumer_groups: dict[str, dict[str, Consumer]] = defaultdict(dict)
                  self.consumer_offsets: dict[str, dict[int, int]] = defaultdict(
                      lambda: defaultdict(int)
                  )  # group_id -> partition -> offset

              def _get_partition(self, event: ChangeEvent) -> int:
                  '''Determine partition for event (consistent hashing by table:pk)'''
                  partition_key = f"{event.table}:{json.dumps(event.primary_key, sort_keys=True)}"
                  return hash(partition_key) % self.num_partitions

              def publish(self, event: ChangeEvent):
                  '''Publish event to stream'''
                  partition_id = self._get_partition(event)

                  with self.partition_locks[partition_id]:
                      self.partitions[partition_id].append(event)

              def publish_batch(self, events: list[ChangeEvent]):
                  '''Publish batch of events'''
                  # Group by partition
                  by_partition = defaultdict(list)
                  for event in events:
                      partition_id = self._get_partition(event)
                      by_partition[partition_id].append(event)

                  # Publish to each partition
                  for partition_id, partition_events in by_partition.items():
                      with self.partition_locks[partition_id]:
                          self.partitions[partition_id].extend(partition_events)

              def subscribe(self, group_id: str, consumer_id: str) -> Consumer:
                  '''Register consumer in group'''
                  consumer = Consumer(
                      id=consumer_id,
                      group_id=group_id,
                      last_heartbeat=time.time()
                  )

                  self.consumer_groups[group_id][consumer_id] = consumer
                  self._rebalance(group_id)

                  return consumer

              def _rebalance(self, group_id: str):
                  '''Rebalance partitions among consumers in group'''
                  consumers = list(self.consumer_groups[group_id].values())
                  if not consumers:
                      return

                  # Clear current assignments
                  for consumer in consumers:
                      consumer.assigned_partitions.clear()

                  # Assign partitions round-robin
                  for i in range(self.num_partitions):
                      consumer = consumers[i % len(consumers)]
                      consumer.assigned_partitions.add(i)

              def poll(self, consumer: Consumer, max_events: int = 100,
                       timeout: float = 1.0) -> list[ChangeEvent]:
                  '''Poll for events assigned to consumer'''
                  consumer.last_heartbeat = time.time()
                  events = []

                  for partition_id in consumer.assigned_partitions:
                      offset = self.consumer_offsets[consumer.group_id][partition_id]

                      with self.partition_locks[partition_id]:
                          partition_events = self.partitions[partition_id]
                          new_events = partition_events[offset:offset + max_events]
                          events.extend(new_events)

                  return events

              def commit_offset(self, consumer: Consumer, partition_id: int, offset: int):
                  '''Commit consumer offset'''
                  self.consumer_offsets[consumer.group_id][partition_id] = offset

              def get_lag(self, group_id: str) -> dict[int, int]:
                  '''Get consumer group lag per partition'''
                  lag = {}
                  for partition_id in range(self.num_partitions):
                      with self.partition_locks[partition_id]:
                          current_offset = len(self.partitions[partition_id])
                      committed_offset = self.consumer_offsets[group_id][partition_id]
                      lag[partition_id] = current_offset - committed_offset
                  return lag

          class CDCEventProcessor:
              '''Process CDC events with handlers'''

              def __init__(self, stream: CDCEventStream, group_id: str):
                  self.stream = stream
                  self.group_id = group_id
                  self.handlers: dict[str, list[Callable]] = defaultdict(list)
                  self.running = False

              def register_handler(self, table: str, handler: Callable[[ChangeEvent], None]):
                  '''Register handler for table changes'''
                  self.handlers[table].append(handler)

              def register_global_handler(self, handler: Callable[[ChangeEvent], None]):
                  '''Register handler for all changes'''
                  self.handlers['*'].append(handler)

              def start(self, consumer_id: str):
                  '''Start processing events'''
                  consumer = self.stream.subscribe(self.group_id, consumer_id)
                  self.running = True

                  while self.running:
                      events = self.stream.poll(consumer)

                      for event in events:
                          self._process_event(event)

                      # Commit offsets after processing
                      for partition_id in consumer.assigned_partitions:
                          current_offset = len(self.stream.partitions[partition_id])
                          self.stream.commit_offset(consumer, partition_id, current_offset)

                      if not events:
                          time.sleep(0.1)  # Back off when no events

              def _process_event(self, event: ChangeEvent):
                  '''Process single event'''
                  # Table-specific handlers
                  for handler in self.handlers.get(event.table, []):
                      try:
                          handler(event)
                      except Exception as e:
                          print(f"Handler error for {event.table}: {e}")

                  # Global handlers
                  for handler in self.handlers.get('*', []):
                      try:
                          handler(event)
                      except Exception as e:
                          print(f"Global handler error: {e}")

              def stop(self):
                  self.running = False
          ```
      pitfalls:
      - Consumer rebalancing causes brief processing pause
      - Partition by pk, not just table, for better parallelism
      - Offset commit after processing prevents duplicates
      - Consumer heartbeat failure needs automatic reassignment
      acceptance_criteria:
      - Change events are published to Kafka topic partitioned by table and primary key
      - Consumer receives every change event at least once even after transient failures
      - Events for the same primary key are delivered in the same order they occurred
      - Consumer lag is monitored and alerts trigger when backlog exceeds configured threshold
      deliverables:
      - Kafka or message queue integration publishing change events to topic partitions
      - At-least-once delivery guaranteeing every change event reaches consumers
      - Event ordering per table maintaining causal order for same-row changes
      - Backpressure handling slowing event production when consumers fall behind
    - name: Schema Evolution & Compatibility
      description: Handle schema changes without breaking consumers
      skills:
      - Schema evolution
      - Compatibility checks
      - Migration
      hints:
        level1: Track schema versions; include schema in events or registry
        level2: 'Compatibility: backward (new reader, old data), forward (old reader, new data)'
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          from enum import Enum
          import json

          class CompatibilityType(Enum):
              BACKWARD = "backward"      # New reader can read old data
              FORWARD = "forward"        # Old reader can read new data
              FULL = "full"              # Both directions
              NONE = "none"              # No compatibility check

          @dataclass
          class SchemaVersion:
              id: int
              table: str
              columns: list[dict]  # [{name, type, nullable, default}]
              primary_key: list[str]
              created_at: float

          class SchemaRegistry:
              def __init__(self):
                  self.schemas: dict[str, list[SchemaVersion]] = defaultdict(list)
                  self.compatibility_mode: dict[str, CompatibilityType] = {}

              def register_schema(self, table: str, columns: list[dict],
                                  primary_key: list[str]) -> SchemaVersion:
                  '''Register new schema version'''
                  versions = self.schemas[table]
                  version_id = len(versions) + 1

                  schema = SchemaVersion(
                      id=version_id,
                      table=table,
                      columns=columns,
                      primary_key=primary_key,
                      created_at=time.time()
                  )

                  # Check compatibility with previous version
                  if versions:
                      mode = self.compatibility_mode.get(table, CompatibilityType.BACKWARD)
                      if not self._check_compatibility(versions[-1], schema, mode):
                          raise ValueError(f"Schema change not compatible with {mode.value}")

                  versions.append(schema)
                  return schema

              def get_schema(self, table: str, version: int = None) -> Optional[SchemaVersion]:
                  '''Get schema version'''
                  versions = self.schemas.get(table, [])
                  if not versions:
                      return None
                  if version:
                      return versions[version - 1] if version <= len(versions) else None
                  return versions[-1]

              def _check_compatibility(self, old_schema: SchemaVersion,
                                        new_schema: SchemaVersion,
                                        mode: CompatibilityType) -> bool:
                  '''Check if schema change is compatible'''
                  if mode == CompatibilityType.NONE:
                      return True

                  old_cols = {c['name']: c for c in old_schema.columns}
                  new_cols = {c['name']: c for c in new_schema.columns}

                  # Backward: new reader can read old data
                  # - New columns must be nullable or have default
                  # - Column types must be compatible (can widen, not narrow)
                  if mode in [CompatibilityType.BACKWARD, CompatibilityType.FULL]:
                      for name, col in new_cols.items():
                          if name not in old_cols:
                              # New column
                              if not col.get('nullable') and col.get('default') is None:
                                  return False

                      for name, old_col in old_cols.items():
                          if name in new_cols:
                              # Type change must be compatible
                              if not self._types_compatible(old_col['type'],
                                                             new_cols[name]['type']):
                                  return False

                  # Forward: old reader can read new data
                  # - Cannot remove required columns
                  # - Cannot add required columns
                  if mode in [CompatibilityType.FORWARD, CompatibilityType.FULL]:
                      for name, old_col in old_cols.items():
                          if name not in new_cols:
                              return False  # Column removed

                  return True

              def _types_compatible(self, old_type: str, new_type: str) -> bool:
                  '''Check if type change is compatible'''
                  # Widening is OK: int -> bigint, varchar(10) -> varchar(100)
                  compatible_widenings = {
                      ('int', 'bigint'): True,
                      ('float', 'double'): True,
                      ('varchar', 'text'): True,
                  }

                  if old_type == new_type:
                      return True

                  return compatible_widenings.get((old_type, new_type), False)

          class SchemaEvolutionHandler:
              '''Handle schema changes in CDC stream'''

              def __init__(self, registry: SchemaRegistry):
                  self.registry = registry

              def detect_schema_change(self, event: ChangeEvent,
                                        expected_schema: SchemaVersion) -> Optional[list[dict]]:
                  '''Detect if event has different schema'''
                  event_columns = set()

                  if event.after:
                      event_columns.update(event.after.keys())
                  if event.before:
                      event_columns.update(event.before.keys())

                  expected_columns = {c['name'] for c in expected_schema.columns}

                  # Check for differences
                  new_columns = event_columns - expected_columns
                  removed_columns = expected_columns - event_columns

                  if new_columns or removed_columns:
                      return {
                          'new_columns': list(new_columns),
                          'removed_columns': list(removed_columns)
                      }

                  return None

              def transform_event(self, event: ChangeEvent,
                                   source_schema: SchemaVersion,
                                   target_schema: SchemaVersion) -> ChangeEvent:
                  '''Transform event from source schema to target schema'''
                  source_cols = {c['name']: c for c in source_schema.columns}
                  target_cols = {c['name']: c for c in target_schema.columns}

                  def transform_record(record: dict) -> dict:
                      if not record:
                          return record

                      transformed = {}

                      for col_name, col_def in target_cols.items():
                          if col_name in record:
                              # Column exists - may need type conversion
                              transformed[col_name] = self._convert_type(
                                  record[col_name],
                                  source_cols.get(col_name, {}).get('type'),
                                  col_def['type']
                              )
                          elif col_def.get('default') is not None:
                              # Use default value
                              transformed[col_name] = col_def['default']
                          elif col_def.get('nullable'):
                              # Nullable - use None
                              transformed[col_name] = None
                          # else: required column missing - error

                      return transformed

                  return ChangeEvent(
                      id=event.id,
                      timestamp=event.timestamp,
                      transaction_id=event.transaction_id,
                      operation=event.operation,
                      database=event.database,
                      schema=event.schema,
                      table=event.table,
                      primary_key=event.primary_key,
                      before=transform_record(event.before),
                      after=transform_record(event.after),
                      metadata={**event.metadata, 'schema_version': target_schema.id}
                  )

              def _convert_type(self, value: Any, source_type: str,
                                 target_type: str) -> Any:
                  '''Convert value between types'''
                  if value is None:
                      return None

                  if source_type == target_type:
                      return value

                  # Type conversions
                  if target_type in ('bigint', 'int'):
                      return int(value)
                  elif target_type in ('float', 'double'):
                      return float(value)
                  elif target_type in ('varchar', 'text', 'string'):
                      return str(value)
                  elif target_type == 'boolean':
                      return bool(value)

                  return value
          ```
      pitfalls:
      - DDL events need special handling - may require resync
      - Schema registry is single point of failure - replicate it
      - Backward compatibility is usually most important
      - Type narrowing (bigint->int) can cause data loss
      acceptance_criteria:
      - Schema registry stores versioned schema for each table and returns schema by version ID
      - Column additions with nullable or default values pass backward compatibility check
      - Column removal or type narrowing fails forward compatibility validation check
      - Schema change events notify consumers to update their deserialization logic
      deliverables:
      - Schema registry integration storing and versioning table schema definitions
      - Schema versioning tracking column additions, removals, and type changes over time
      - Compatibility checking validating schema changes against backward compatibility rules
      - Schema migration event generation notifying consumers of schema version changes
  file-upload-service:
    name: Resumable File Upload Service
    description: Build a file upload service supporting chunked uploads, resumable transfers, virus scanning, and storage
      backends (S3, local).
    why_expert: File uploads are tricky - large files, network failures, security. Understanding chunked protocols and storage
      abstractions enables reliable upload handling.
    difficulty: advanced
    tags:
    - files
    - uploads
    - storage
    - s3
    - resumable
    estimated_hours: 35
    prerequisites:
    - build-http-server
    milestones:
    - name: Chunked Upload Protocol
      description: Implement tus.io-style resumable upload protocol
      skills:
      - Chunked uploads
      - Resume logic
      - HTTP headers
      hints:
        level1: Client sends chunks; server tracks offset; resume from last chunk
        level2: Use Upload-Offset header to track progress
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional, BinaryIO
          from enum import Enum
          import os
          import secrets
          import time
          import hashlib

          class UploadStatus(Enum):
              CREATED = "created"
              IN_PROGRESS = "in_progress"
              COMPLETE = "complete"
              EXPIRED = "expired"

          @dataclass
          class Upload:
              id: str
              size: int                    # Total file size
              offset: int                  # Current offset
              status: UploadStatus
              filename: Optional[str]
              content_type: Optional[str]
              metadata: dict
              created_at: float
              expires_at: float
              checksum: Optional[str] = None

          class ChunkedUploadService:
              def __init__(self, storage_path: str, max_size: int = 5 * 1024**3,
                           chunk_size: int = 5 * 1024**2, expiry_hours: int = 24):
                  self.storage_path = storage_path
                  self.max_size = max_size
                  self.chunk_size = chunk_size
                  self.expiry_hours = expiry_hours
                  self.uploads: dict[str, Upload] = {}

                  os.makedirs(storage_path, exist_ok=True)
                  os.makedirs(os.path.join(storage_path, 'chunks'), exist_ok=True)
                  os.makedirs(os.path.join(storage_path, 'complete'), exist_ok=True)

              def create_upload(self, size: int, filename: str = None,
                                content_type: str = None, metadata: dict = None) -> Upload:
                  '''Create new upload session (POST /uploads)'''
                  if size > self.max_size:
                      raise ValueError(f"File size exceeds maximum ({self.max_size})")

                  upload_id = secrets.token_urlsafe(16)
                  now = time.time()

                  upload = Upload(
                      id=upload_id,
                      size=size,
                      offset=0,
                      status=UploadStatus.CREATED,
                      filename=filename,
                      content_type=content_type,
                      metadata=metadata or {},
                      created_at=now,
                      expires_at=now + (self.expiry_hours * 3600)
                  )

                  self.uploads[upload_id] = upload

                  # Create chunk file
                  chunk_path = self._get_chunk_path(upload_id)
                  with open(chunk_path, 'wb') as f:
                      f.truncate(size)  # Pre-allocate

                  return upload

              def upload_chunk(self, upload_id: str, data: bytes,
                               offset: int, checksum: str = None) -> Upload:
                  '''Upload a chunk (PATCH /uploads/{id})'''
                  upload = self.uploads.get(upload_id)
                  if not upload:
                      raise ValueError("Upload not found")

                  if upload.status == UploadStatus.EXPIRED:
                      raise ValueError("Upload expired")

                  if upload.status == UploadStatus.COMPLETE:
                      raise ValueError("Upload already complete")

                  if offset != upload.offset:
                      raise ValueError(f"Offset mismatch. Expected {upload.offset}, got {offset}")

                  # Verify checksum if provided
                  if checksum:
                      computed = hashlib.sha256(data).hexdigest()
                      if computed != checksum:
                          raise ValueError("Checksum mismatch")

                  # Write chunk
                  chunk_path = self._get_chunk_path(upload_id)
                  with open(chunk_path, 'r+b') as f:
                      f.seek(offset)
                      f.write(data)

                  upload.offset += len(data)
                  upload.status = UploadStatus.IN_PROGRESS

                  # Check if complete
                  if upload.offset >= upload.size:
                      self._finalize_upload(upload)

                  return upload

              def get_upload(self, upload_id: str) -> Optional[Upload]:
                  '''Get upload status (HEAD /uploads/{id})'''
                  upload = self.uploads.get(upload_id)
                  if upload and time.time() > upload.expires_at:
                      upload.status = UploadStatus.EXPIRED
                  return upload

              def _finalize_upload(self, upload: Upload):
                  '''Move completed upload to final location'''
                  chunk_path = self._get_chunk_path(upload.id)
                  final_path = self._get_final_path(upload.id, upload.filename)

                  os.rename(chunk_path, final_path)
                  upload.status = UploadStatus.COMPLETE

                  # Calculate final checksum
                  with open(final_path, 'rb') as f:
                      upload.checksum = hashlib.sha256(f.read()).hexdigest()

              def _get_chunk_path(self, upload_id: str) -> str:
                  return os.path.join(self.storage_path, 'chunks', upload_id)

              def _get_final_path(self, upload_id: str, filename: str = None) -> str:
                  name = filename or upload_id
                  return os.path.join(self.storage_path, 'complete', f"{upload_id}_{name}")

              def cleanup_expired(self):
                  '''Remove expired uploads'''
                  now = time.time()
                  expired = [uid for uid, u in self.uploads.items()
                             if now > u.expires_at and u.status != UploadStatus.COMPLETE]

                  for upload_id in expired:
                      chunk_path = self._get_chunk_path(upload_id)
                      if os.path.exists(chunk_path):
                          os.remove(chunk_path)
                      del self.uploads[upload_id]
          ```
      pitfalls:
      - Pre-allocate file to avoid fragmentation
      - 'Offset mismatch: client must track and resume from server''s offset'
      - Chunk checksum prevents corruption from network errors
      - Expiry cleanup must not delete in-progress uploads
      acceptance_criteria:
      - Initialize multipart upload returning upload ID for subsequent chunk uploads
      - Accept chunk uploads with part numbers and associate with upload session
      - Track chunk completion status showing which parts are received and pending
      - Complete upload by assembling all received chunks into the final file
      deliverables:
      - Chunk upload endpoint accepting individual file parts with metadata
      - Chunk ordering with part number tracking for correct reassembly
      - Resume interrupted uploads by tracking which chunks are received
      - Chunk assembly combining all parts into the final complete file
    - name: Storage Abstraction
      description: Implement storage backends (local, S3, GCS) with common interface
      skills:
      - Storage abstraction
      - S3 API
      - Multipart uploads
      hints:
        level1: 'Abstract storage: put, get, delete, list operations'
        level2: S3 multipart upload for large files
        level3: |2

          ```python
          from abc import ABC, abstractmethod
          from dataclasses import dataclass
          from typing import BinaryIO, Iterator, Optional
          import os
          import shutil

          @dataclass
          class StorageObject:
              key: str
              size: int
              content_type: Optional[str]
              last_modified: float
              metadata: dict
              etag: Optional[str] = None

          class StorageBackend(ABC):
              @abstractmethod
              def put(self, key: str, data: BinaryIO, size: int,
                      content_type: str = None, metadata: dict = None) -> StorageObject:
                  pass

              @abstractmethod
              def get(self, key: str) -> tuple[BinaryIO, StorageObject]:
                  pass

              @abstractmethod
              def delete(self, key: str) -> bool:
                  pass

              @abstractmethod
              def list(self, prefix: str = "", limit: int = 1000) -> Iterator[StorageObject]:
                  pass

              @abstractmethod
              def exists(self, key: str) -> bool:
                  pass

          class LocalStorageBackend(StorageBackend):
              def __init__(self, base_path: str):
                  self.base_path = base_path
                  os.makedirs(base_path, exist_ok=True)

              def _get_path(self, key: str) -> str:
                  # Sanitize key to prevent path traversal
                  safe_key = key.replace('..', '').lstrip('/')
                  return os.path.join(self.base_path, safe_key)

              def put(self, key: str, data: BinaryIO, size: int,
                      content_type: str = None, metadata: dict = None) -> StorageObject:
                  path = self._get_path(key)
                  os.makedirs(os.path.dirname(path), exist_ok=True)

                  with open(path, 'wb') as f:
                      shutil.copyfileobj(data, f)

                  # Store metadata in sidecar file
                  meta_path = path + '.meta'
                  import json
                  with open(meta_path, 'w') as f:
                      json.dump({
                          'content_type': content_type,
                          'metadata': metadata or {}
                      }, f)

                  stat = os.stat(path)
                  return StorageObject(
                      key=key,
                      size=stat.st_size,
                      content_type=content_type,
                      last_modified=stat.st_mtime,
                      metadata=metadata or {}
                  )

              def get(self, key: str) -> tuple[BinaryIO, StorageObject]:
                  path = self._get_path(key)
                  if not os.path.exists(path):
                      raise FileNotFoundError(key)

                  stat = os.stat(path)

                  # Load metadata
                  meta_path = path + '.meta'
                  content_type = None
                  metadata = {}
                  if os.path.exists(meta_path):
                      import json
                      with open(meta_path) as f:
                          meta = json.load(f)
                          content_type = meta.get('content_type')
                          metadata = meta.get('metadata', {})

                  obj = StorageObject(
                      key=key,
                      size=stat.st_size,
                      content_type=content_type,
                      last_modified=stat.st_mtime,
                      metadata=metadata
                  )

                  return open(path, 'rb'), obj

              def delete(self, key: str) -> bool:
                  path = self._get_path(key)
                  meta_path = path + '.meta'

                  if os.path.exists(path):
                      os.remove(path)
                      if os.path.exists(meta_path):
                          os.remove(meta_path)
                      return True
                  return False

              def list(self, prefix: str = "", limit: int = 1000) -> Iterator[StorageObject]:
                  base = self._get_path(prefix)
                  count = 0

                  for root, dirs, files in os.walk(self.base_path):
                      for name in files:
                          if name.endswith('.meta'):
                              continue

                          path = os.path.join(root, name)
                          key = os.path.relpath(path, self.base_path)

                          if prefix and not key.startswith(prefix):
                              continue

                          stat = os.stat(path)
                          yield StorageObject(
                              key=key,
                              size=stat.st_size,
                              content_type=None,
                              last_modified=stat.st_mtime,
                              metadata={}
                          )

                          count += 1
                          if count >= limit:
                              return

              def exists(self, key: str) -> bool:
                  return os.path.exists(self._get_path(key))

          class S3StorageBackend(StorageBackend):
              '''S3-compatible storage backend'''

              def __init__(self, bucket: str, region: str = 'us-east-1',
                           endpoint_url: str = None):
                  import boto3
                  self.bucket = bucket
                  self.client = boto3.client(
                      's3',
                      region_name=region,
                      endpoint_url=endpoint_url  # For MinIO, LocalStack
                  )

              def put(self, key: str, data: BinaryIO, size: int,
                      content_type: str = None, metadata: dict = None) -> StorageObject:

                  extra_args = {}
                  if content_type:
                      extra_args['ContentType'] = content_type
                  if metadata:
                      extra_args['Metadata'] = {k: str(v) for k, v in metadata.items()}

                  # Use multipart for large files
                  if size > 100 * 1024 * 1024:  # 100MB
                      return self._multipart_upload(key, data, size, extra_args)

                  self.client.upload_fileobj(data, self.bucket, key, ExtraArgs=extra_args)

                  # Get object info
                  response = self.client.head_object(Bucket=self.bucket, Key=key)

                  return StorageObject(
                      key=key,
                      size=response['ContentLength'],
                      content_type=response.get('ContentType'),
                      last_modified=response['LastModified'].timestamp(),
                      metadata=response.get('Metadata', {}),
                      etag=response['ETag'].strip('"')
                  )

              def _multipart_upload(self, key: str, data: BinaryIO, size: int,
                                    extra_args: dict) -> StorageObject:
                  '''Multipart upload for large files'''
                  part_size = 100 * 1024 * 1024  # 100MB parts

                  # Initiate multipart upload
                  response = self.client.create_multipart_upload(
                      Bucket=self.bucket,
                      Key=key,
                      **extra_args
                  )
                  upload_id = response['UploadId']

                  parts = []
                  part_number = 1

                  try:
                      while True:
                          chunk = data.read(part_size)
                          if not chunk:
                              break

                          response = self.client.upload_part(
                              Bucket=self.bucket,
                              Key=key,
                              UploadId=upload_id,
                              PartNumber=part_number,
                              Body=chunk
                          )

                          parts.append({
                              'PartNumber': part_number,
                              'ETag': response['ETag']
                          })
                          part_number += 1

                      # Complete upload
                      self.client.complete_multipart_upload(
                          Bucket=self.bucket,
                          Key=key,
                          UploadId=upload_id,
                          MultipartUpload={'Parts': parts}
                      )

                  except Exception:
                      # Abort on failure
                      self.client.abort_multipart_upload(
                          Bucket=self.bucket,
                          Key=key,
                          UploadId=upload_id
                      )
                      raise

                  return self.get(key)[1]

              def get(self, key: str) -> tuple[BinaryIO, StorageObject]:
                  response = self.client.get_object(Bucket=self.bucket, Key=key)

                  obj = StorageObject(
                      key=key,
                      size=response['ContentLength'],
                      content_type=response.get('ContentType'),
                      last_modified=response['LastModified'].timestamp(),
                      metadata=response.get('Metadata', {}),
                      etag=response['ETag'].strip('"')
                  )

                  return response['Body'], obj

              def delete(self, key: str) -> bool:
                  try:
                      self.client.delete_object(Bucket=self.bucket, Key=key)
                      return True
                  except:
                      return False

              def list(self, prefix: str = "", limit: int = 1000) -> Iterator[StorageObject]:
                  paginator = self.client.get_paginator('list_objects_v2')
                  count = 0

                  for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
                      for obj in page.get('Contents', []):
                          yield StorageObject(
                              key=obj['Key'],
                              size=obj['Size'],
                              content_type=None,
                              last_modified=obj['LastModified'].timestamp(),
                              metadata={},
                              etag=obj['ETag'].strip('"')
                          )
                          count += 1
                          if count >= limit:
                              return

              def exists(self, key: str) -> bool:
                  try:
                      self.client.head_object(Bucket=self.bucket, Key=key)
                      return True
                  except:
                      return False
          ```
      pitfalls:
      - 'S3 multipart: parts must be at least 5MB (except last)'
      - 'Local storage: sanitize keys to prevent path traversal'
      - S3 eventually consistent for overwrite - use versioning
      - Streaming large files - don't load entire file in memory
      acceptance_criteria:
      - Abstract storage interface supports local, S3, and GCS backends uniformly
      - Handle storage credentials and authentication for cloud backends securely
      - Generate signed download URLs with configurable expiration for secure access
      - Support storage migration moving files between backends without data loss
      deliverables:
      - Storage interface defining read, write, and delete operations
      - Local filesystem backend implementing the storage interface
      - S3-compatible backend for cloud object storage integration
      - Storage selection based on configuration for deployment flexibility
    - name: Virus Scanning & Validation
      description: Implement file validation with type checking and virus scanning
      skills:
      - File validation
      - MIME detection
      - Antivirus integration
      hints:
        level1: Check magic bytes, not just extension
        level2: ClamAV for virus scanning via clamd socket
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional, BinaryIO
          from enum import Enum
          import mimetypes

          class ScanResult(Enum):
              CLEAN = "clean"
              INFECTED = "infected"
              ERROR = "error"
              SKIPPED = "skipped"

          @dataclass
          class ValidationResult:
              valid: bool
              mime_type: Optional[str]
              detected_extension: Optional[str]
              scan_result: ScanResult
              scan_details: Optional[str] = None
              errors: list[str] = None

          class FileValidator:
              # Magic bytes for common file types
              MAGIC_BYTES = {
                  b'\x89PNG\r\n\x1a\n': ('image/png', '.png'),
                  b'\xff\xd8\xff': ('image/jpeg', '.jpg'),
                  b'GIF87a': ('image/gif', '.gif'),
                  b'GIF89a': ('image/gif', '.gif'),
                  b'%PDF': ('application/pdf', '.pdf'),
                  b'PK\x03\x04': ('application/zip', '.zip'),
                  b'\x1f\x8b': ('application/gzip', '.gz'),
              }

              def __init__(self, allowed_types: list[str] = None,
                           max_size: int = 100 * 1024 * 1024,
                           scan_viruses: bool = True):
                  self.allowed_types = set(allowed_types) if allowed_types else None
                  self.max_size = max_size
                  self.scan_viruses = scan_viruses
                  self.scanner = ClamAVScanner() if scan_viruses else None

              def validate(self, data: BinaryIO, filename: str = None,
                           claimed_type: str = None) -> ValidationResult:
                  errors = []

                  # Read header for magic byte detection
                  header = data.read(16)
                  data.seek(0)

                  # Detect MIME type from magic bytes
                  detected_type, detected_ext = self._detect_type(header)

                  # Check size
                  data.seek(0, 2)  # Seek to end
                  size = data.tell()
                  data.seek(0)

                  if size > self.max_size:
                      errors.append(f"File too large: {size} > {self.max_size}")

                  # Check allowed types
                  if self.allowed_types and detected_type:
                      if detected_type not in self.allowed_types:
                          errors.append(f"File type not allowed: {detected_type}")

                  # Check extension matches content
                  if filename and detected_ext:
                      actual_ext = '.' + filename.rsplit('.', 1)[-1].lower() if '.' in filename else ''
                      if actual_ext and actual_ext != detected_ext:
                          errors.append(f"Extension mismatch: {actual_ext} vs detected {detected_ext}")

                  # Virus scan
                  scan_result = ScanResult.SKIPPED
                  scan_details = None

                  if self.scan_viruses and self.scanner:
                      scan_result, scan_details = self.scanner.scan(data)
                      data.seek(0)

                      if scan_result == ScanResult.INFECTED:
                          errors.append(f"Virus detected: {scan_details}")

                  return ValidationResult(
                      valid=len(errors) == 0,
                      mime_type=detected_type,
                      detected_extension=detected_ext,
                      scan_result=scan_result,
                      scan_details=scan_details,
                      errors=errors if errors else None
                  )

              def _detect_type(self, header: bytes) -> tuple[Optional[str], Optional[str]]:
                  for magic, (mime_type, ext) in self.MAGIC_BYTES.items():
                      if header.startswith(magic):
                          return mime_type, ext
                  return None, None

          class ClamAVScanner:
              '''Virus scanner using ClamAV daemon'''

              def __init__(self, socket_path: str = '/var/run/clamav/clamd.ctl',
                           host: str = None, port: int = 3310):
                  self.socket_path = socket_path
                  self.host = host
                  self.port = port

              def scan(self, data: BinaryIO) -> tuple[ScanResult, Optional[str]]:
                  import socket

                  try:
                      if self.host:
                          sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                          sock.connect((self.host, self.port))
                      else:
                          sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
                          sock.connect(self.socket_path)

                      # Send INSTREAM command
                      sock.send(b'nINSTREAM\n')

                      # Send file in chunks
                      while True:
                          chunk = data.read(8192)
                          if not chunk:
                              break
                          # Send chunk size (4 bytes big endian) + data
                          sock.send(len(chunk).to_bytes(4, 'big') + chunk)

                      # Send zero-length chunk to end
                      sock.send(b'\x00\x00\x00\x00')

                      # Read response
                      response = sock.recv(4096).decode().strip()
                      sock.close()

                      if 'OK' in response:
                          return ScanResult.CLEAN, None
                      elif 'FOUND' in response:
                          virus_name = response.split('FOUND')[0].strip()
                          return ScanResult.INFECTED, virus_name
                      else:
                          return ScanResult.ERROR, response

                  except Exception as e:
                      return ScanResult.ERROR, str(e)
          ```
      pitfalls:
      - Never trust file extension alone - always check magic bytes
      - Virus scan before storing, not after
      - ClamAV can timeout on large files - set appropriate limits
      - Quarantine infected files, don't delete immediately (for forensics)
      acceptance_criteria:
      - Scan uploaded files with ClamAV or similar antivirus engine before storage
      - Validate file types using magic bytes rather than relying on file extension
      - Enforce configurable size limits and reject uploads exceeding the maximum
      - Quarantine suspicious files in isolated storage for review rather than deleting
      deliverables:
      - File type validation using magic bytes for content verification
      - Size limit enforcement rejecting uploads exceeding maximum threshold
      - Virus scan integration with ClamAV or equivalent scanning engine
      - Quarantine handling isolating suspicious files from normal storage
  media-processing:
    name: Media Processing Pipeline
    description: Build a media processing service for image resizing, video transcoding, and thumbnail generation.
    why_expert: Media processing is CPU-intensive and complex. Understanding codecs, formats, and async processing enables
      building efficient media services.
    difficulty: advanced
    tags:
    - media
    - images
    - video
    - transcoding
    - thumbnails
    estimated_hours: 40
    prerequisites:
    - build-message-queue
    milestones:
    - name: Image Processing
      description: Implement image resizing, format conversion, and optimization
      skills:
      - Image formats
      - Resizing algorithms
      - Optimization
      hints:
        level1: Generate multiple sizes on upload for responsive images
        level2: Preserve aspect ratio; use Lanczos for quality downscaling
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional, BinaryIO
          from enum import Enum
          from PIL import Image
          import io

          class ImageFormat(Enum):
              JPEG = "jpeg"
              PNG = "png"
              WEBP = "webp"
              AVIF = "avif"

          @dataclass
          class ImageVariant:
              name: str
              width: int
              height: Optional[int]  # None = maintain aspect ratio
              format: ImageFormat
              quality: int = 85

          @dataclass
          class ProcessedImage:
              variant: str
              width: int
              height: int
              format: ImageFormat
              size: int
              data: bytes

          class ImageProcessor:
              DEFAULT_VARIANTS = [
                  ImageVariant("thumbnail", 150, 150, ImageFormat.WEBP, 80),
                  ImageVariant("small", 320, None, ImageFormat.WEBP, 85),
                  ImageVariant("medium", 640, None, ImageFormat.WEBP, 85),
                  ImageVariant("large", 1280, None, ImageFormat.WEBP, 90),
                  ImageVariant("original", 0, None, ImageFormat.WEBP, 95),  # 0 = keep size
              ]

              def __init__(self, variants: list[ImageVariant] = None):
                  self.variants = variants or self.DEFAULT_VARIANTS

              def process(self, image_data: BinaryIO) -> list[ProcessedImage]:
                  '''Process image into all variants'''
                  original = Image.open(image_data)

                  # Convert to RGB if necessary (for JPEG output)
                  if original.mode in ('RGBA', 'P'):
                      background = Image.new('RGB', original.size, (255, 255, 255))
                      if original.mode == 'P':
                          original = original.convert('RGBA')
                      background.paste(original, mask=original.split()[3])
                      original = background
                  elif original.mode != 'RGB':
                      original = original.convert('RGB')

                  results = []
                  for variant in self.variants:
                      processed = self._process_variant(original, variant)
                      results.append(processed)

                  return results

              def _process_variant(self, image: Image.Image,
                                   variant: ImageVariant) -> ProcessedImage:
                  # Calculate target size
                  if variant.width == 0:
                      # Keep original size
                      target_size = image.size
                  elif variant.height:
                      # Fixed dimensions (crop to fit)
                      target_size = (variant.width, variant.height)
                      image = self._crop_to_aspect(image, target_size)
                  else:
                      # Width only - maintain aspect ratio
                      ratio = variant.width / image.width
                      target_size = (variant.width, int(image.height * ratio))

                  # Resize if needed
                  if target_size != image.size:
                      image = image.resize(target_size, Image.Resampling.LANCZOS)

                  # Convert to output format
                  output = io.BytesIO()
                  format_str = variant.format.value.upper()

                  if variant.format == ImageFormat.WEBP:
                      image.save(output, 'WEBP', quality=variant.quality, method=6)
                  elif variant.format == ImageFormat.JPEG:
                      image.save(output, 'JPEG', quality=variant.quality, optimize=True)
                  elif variant.format == ImageFormat.PNG:
                      image.save(output, 'PNG', optimize=True)
                  elif variant.format == ImageFormat.AVIF:
                      # Requires pillow-avif-plugin
                      image.save(output, 'AVIF', quality=variant.quality)

                  data = output.getvalue()

                  return ProcessedImage(
                      variant=variant.name,
                      width=image.width,
                      height=image.height,
                      format=variant.format,
                      size=len(data),
                      data=data
                  )

              def _crop_to_aspect(self, image: Image.Image,
                                  target_size: tuple[int, int]) -> Image.Image:
                  '''Crop image to target aspect ratio (center crop)'''
                  target_ratio = target_size[0] / target_size[1]
                  image_ratio = image.width / image.height

                  if image_ratio > target_ratio:
                      # Image is wider - crop sides
                      new_width = int(image.height * target_ratio)
                      left = (image.width - new_width) // 2
                      image = image.crop((left, 0, left + new_width, image.height))
                  elif image_ratio < target_ratio:
                      # Image is taller - crop top/bottom
                      new_height = int(image.width / target_ratio)
                      top = (image.height - new_height) // 2
                      image = image.crop((0, top, image.width, top + new_height))

                  return image

              def extract_metadata(self, image_data: BinaryIO) -> dict:
                  '''Extract EXIF and other metadata'''
                  image = Image.open(image_data)
                  metadata = {
                      'width': image.width,
                      'height': image.height,
                      'format': image.format,
                      'mode': image.mode
                  }

                  # Extract EXIF
                  if hasattr(image, '_getexif') and image._getexif():
                      exif = image._getexif()
                      # Map common EXIF tags
                      exif_tags = {
                          271: 'camera_make',
                          272: 'camera_model',
                          306: 'datetime',
                          274: 'orientation'
                      }
                      for tag_id, name in exif_tags.items():
                          if tag_id in exif:
                              metadata[name] = exif[tag_id]

                  return metadata
          ```
      pitfalls:
      - 'EXIF orientation: rotate image according to tag before processing'
      - WebP/AVIF save significant bandwidth but check browser support
      - Lanczos is best for downscaling; use different for upscaling
      - Strip EXIF from output for privacy (location data)
      acceptance_criteria:
      - Images are resized to multiple target dimensions while preserving aspect ratio or using center-crop
      - Format conversion between JPEG, PNG, and WebP produces valid output with configurable quality levels
      - Thumbnails are generated at standard sizes with smart cropping that focuses on the image center of interest
      - EXIF metadata including orientation, camera, and GPS data is read, preserved, or stripped as configured
      deliverables:
      - Image loader that reads JPEG, PNG, WebP, and GIF files into an in-memory pixel buffer
      - Resize and crop engine that scales images to target dimensions with configurable aspect ratio handling
      - Format converter that transcodes images between JPEG, PNG, WebP, and AVIF with quality settings
      - Thumbnail generator that produces small preview images at standard sizes with smart cropping
    - name: Video Transcoding
      description: Implement video transcoding with FFmpeg for web playback
      skills:
      - FFmpeg
      - Video codecs
      - HLS streaming
      hints:
        level1: Transcode to H.264/AAC for broad compatibility
        level2: Generate HLS segments for adaptive streaming
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          import subprocess
          import os
          import json

          @dataclass
          class VideoProfile:
              name: str
              width: int
              height: int
              video_bitrate: str
              audio_bitrate: str
              preset: str = "medium"

          @dataclass
          class TranscodeResult:
              profile: str
              path: str
              width: int
              height: int
              duration: float
              size: int

          class VideoTranscoder:
              PROFILES = [
                  VideoProfile("360p", 640, 360, "800k", "96k"),
                  VideoProfile("480p", 854, 480, "1400k", "128k"),
                  VideoProfile("720p", 1280, 720, "2800k", "128k"),
                  VideoProfile("1080p", 1920, 1080, "5000k", "192k"),
              ]

              def __init__(self, output_dir: str, ffmpeg_path: str = "ffmpeg"):
                  self.output_dir = output_dir
                  self.ffmpeg = ffmpeg_path
                  os.makedirs(output_dir, exist_ok=True)

              def get_video_info(self, input_path: str) -> dict:
                  '''Get video metadata using ffprobe'''
                  cmd = [
                      'ffprobe', '-v', 'quiet',
                      '-print_format', 'json',
                      '-show_format', '-show_streams',
                      input_path
                  ]

                  result = subprocess.run(cmd, capture_output=True, text=True)
                  data = json.loads(result.stdout)

                  video_stream = next(
                      (s for s in data['streams'] if s['codec_type'] == 'video'),
                      None
                  )

                  return {
                      'duration': float(data['format'].get('duration', 0)),
                      'width': video_stream['width'] if video_stream else 0,
                      'height': video_stream['height'] if video_stream else 0,
                      'codec': video_stream['codec_name'] if video_stream else None,
                      'bitrate': int(data['format'].get('bit_rate', 0))
                  }

              def transcode_mp4(self, input_path: str, job_id: str,
                                profile: VideoProfile) -> TranscodeResult:
                  '''Transcode to MP4 with H.264'''
                  output_path = os.path.join(
                      self.output_dir, f"{job_id}_{profile.name}.mp4"
                  )

                  cmd = [
                      self.ffmpeg, '-y',
                      '-i', input_path,
                      '-c:v', 'libx264',
                      '-preset', profile.preset,
                      '-b:v', profile.video_bitrate,
                      '-maxrate', profile.video_bitrate,
                      '-bufsize', str(int(profile.video_bitrate.rstrip('k')) * 2) + 'k',
                      '-vf', f'scale={profile.width}:{profile.height}:force_original_aspect_ratio=decrease,pad={profile.width}:{profile.height}:(ow-iw)/2:(oh-ih)/2',
                      '-c:a', 'aac',
                      '-b:a', profile.audio_bitrate,
                      '-movflags', '+faststart',  # Enable streaming
                      output_path
                  ]

                  subprocess.run(cmd, check=True, capture_output=True)

                  info = self.get_video_info(output_path)
                  return TranscodeResult(
                      profile=profile.name,
                      path=output_path,
                      width=info['width'],
                      height=info['height'],
                      duration=info['duration'],
                      size=os.path.getsize(output_path)
                  )

              def generate_hls(self, input_path: str, job_id: str) -> dict:
                  '''Generate HLS playlist with multiple qualities'''
                  hls_dir = os.path.join(self.output_dir, job_id, 'hls')
                  os.makedirs(hls_dir, exist_ok=True)

                  info = self.get_video_info(input_path)
                  source_height = info['height']

                  # Select appropriate profiles
                  profiles = [p for p in self.PROFILES if p.height <= source_height]
                  if not profiles:
                      profiles = [self.PROFILES[0]]

                  variants = []
                  for profile in profiles:
                      variant_dir = os.path.join(hls_dir, profile.name)
                      os.makedirs(variant_dir, exist_ok=True)

                      output_playlist = os.path.join(variant_dir, 'playlist.m3u8')

                      cmd = [
                          self.ffmpeg, '-y',
                          '-i', input_path,
                          '-c:v', 'libx264',
                          '-preset', 'fast',
                          '-b:v', profile.video_bitrate,
                          '-vf', f'scale={profile.width}:-2',
                          '-c:a', 'aac',
                          '-b:a', profile.audio_bitrate,
                          '-hls_time', '6',
                          '-hls_playlist_type', 'vod',
                          '-hls_segment_filename', os.path.join(variant_dir, 'segment_%03d.ts'),
                          output_playlist
                      ]

                      subprocess.run(cmd, check=True, capture_output=True)

                      variants.append({
                          'name': profile.name,
                          'bandwidth': int(profile.video_bitrate.rstrip('k')) * 1000,
                          'resolution': f'{profile.width}x{profile.height}',
                          'playlist': f'{profile.name}/playlist.m3u8'
                      })

                  # Generate master playlist
                  master_path = os.path.join(hls_dir, 'master.m3u8')
                  self._write_master_playlist(master_path, variants)

                  return {
                      'master_playlist': master_path,
                      'variants': variants,
                      'duration': info['duration']
                  }

              def _write_master_playlist(self, path: str, variants: list[dict]):
                  with open(path, 'w') as f:
                      f.write('#EXTM3U\n')
                      for v in sorted(variants, key=lambda x: x['bandwidth']):
                          f.write(f'#EXT-X-STREAM-INF:BANDWIDTH={v["bandwidth"]},RESOLUTION={v["resolution"]}\n')
                          f.write(f'{v["playlist"]}\n')

              def generate_thumbnail(self, input_path: str, job_id: str,
                                     time_offset: float = 1.0) -> str:
                  '''Generate thumbnail at specified time'''
                  output_path = os.path.join(self.output_dir, f"{job_id}_thumb.jpg")

                  cmd = [
                      self.ffmpeg, '-y',
                      '-i', input_path,
                      '-ss', str(time_offset),
                      '-vframes', '1',
                      '-vf', 'scale=640:-1',
                      '-q:v', '3',
                      output_path
                  ]

                  subprocess.run(cmd, check=True, capture_output=True)
                  return output_path
          ```
      pitfalls:
      - H.264 baseline profile for maximum compatibility
      - -movflags +faststart enables progressive download
      - HLS segment size affects startup time vs seeking
      - Video processing is CPU-intensive - use job queue
      acceptance_criteria:
      - Videos are transcoded to multiple format and codec combinations including H.264, H.265, and VP9
      - Adaptive bitrate variants are generated at multiple resolutions and bitrates for streaming playback
      - Video thumbnail frames are extracted at configurable time offsets as still image files
      - HLS and DASH output generates properly segmented manifest files and media segments for adaptive playback
      deliverables:
      - FFmpeg integration layer that wraps command-line FFmpeg calls with progress parsing and error handling
      - Codec selection module that maps input formats to optimal output codecs based on target platform
      - Bitrate controller that manages constant and variable bitrate encoding with target file size constraints
      - Resolution scaler that transcodes video to multiple resolution variants for adaptive streaming
    - name: Processing Queue & Progress
      description: Implement async processing queue with progress tracking
      skills:
      - Job queues
      - Progress tracking
      - Webhooks
      hints:
        level1: Queue processing jobs; track progress with callbacks
        level2: Report progress percentage; notify on completion
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Callable, Optional
          from enum import Enum
          import threading
          import queue
          import time

          class JobStatus(Enum):
              QUEUED = "queued"
              PROCESSING = "processing"
              COMPLETED = "completed"
              FAILED = "failed"

          @dataclass
          class ProcessingJob:
              id: str
              type: str  # image, video
              input_path: str
              status: JobStatus
              progress: float = 0.0
              created_at: float = field(default_factory=time.time)
              started_at: Optional[float] = None
              completed_at: Optional[float] = None
              result: Optional[dict] = None
              error: Optional[str] = None
              webhook_url: Optional[str] = None

          class MediaProcessingQueue:
              def __init__(self, image_processor: ImageProcessor,
                           video_transcoder: VideoTranscoder,
                           num_workers: int = 4):
                  self.image_processor = image_processor
                  self.video_transcoder = video_transcoder
                  self.queue = queue.PriorityQueue()
                  self.jobs: dict[str, ProcessingJob] = {}
                  self.callbacks: dict[str, list[Callable]] = {}
                  self.workers = []

                  for i in range(num_workers):
                      worker = threading.Thread(target=self._worker, daemon=True)
                      worker.start()
                      self.workers.append(worker)

              def submit(self, job_id: str, job_type: str, input_path: str,
                         priority: int = 5, webhook_url: str = None) -> ProcessingJob:
                  job = ProcessingJob(
                      id=job_id,
                      type=job_type,
                      input_path=input_path,
                      status=JobStatus.QUEUED,
                      webhook_url=webhook_url
                  )

                  self.jobs[job_id] = job
                  self.queue.put((priority, time.time(), job_id))

                  return job

              def get_status(self, job_id: str) -> Optional[ProcessingJob]:
                  return self.jobs.get(job_id)

              def on_progress(self, job_id: str, callback: Callable[[float], None]):
                  if job_id not in self.callbacks:
                      self.callbacks[job_id] = []
                  self.callbacks[job_id].append(callback)

              def _worker(self):
                  while True:
                      try:
                          priority, timestamp, job_id = self.queue.get()
                          job = self.jobs.get(job_id)

                          if not job:
                              continue

                          self._process_job(job)
                          self.queue.task_done()

                      except Exception as e:
                          print(f"Worker error: {e}")

              def _process_job(self, job: ProcessingJob):
                  job.status = JobStatus.PROCESSING
                  job.started_at = time.time()

                  try:
                      if job.type == "image":
                          result = self._process_image(job)
                      elif job.type == "video":
                          result = self._process_video(job)
                      else:
                          raise ValueError(f"Unknown job type: {job.type}")

                      job.status = JobStatus.COMPLETED
                      job.result = result
                      job.progress = 100.0

                  except Exception as e:
                      job.status = JobStatus.FAILED
                      job.error = str(e)

                  finally:
                      job.completed_at = time.time()
                      self._notify_completion(job)

              def _process_image(self, job: ProcessingJob) -> dict:
                  with open(job.input_path, 'rb') as f:
                      results = self.image_processor.process(f)

                  self._update_progress(job, 100)

                  return {
                      'variants': [
                          {
                              'name': r.variant,
                              'width': r.width,
                              'height': r.height,
                              'format': r.format.value,
                              'size': r.size
                          }
                          for r in results
                      ]
                  }

              def _process_video(self, job: ProcessingJob) -> dict:
                  # Generate HLS
                  self._update_progress(job, 10)

                  hls_result = self.video_transcoder.generate_hls(
                      job.input_path, job.id
                  )
                  self._update_progress(job, 80)

                  # Generate thumbnail
                  thumbnail = self.video_transcoder.generate_thumbnail(
                      job.input_path, job.id
                  )
                  self._update_progress(job, 100)

                  return {
                      'hls': hls_result,
                      'thumbnail': thumbnail
                  }

              def _update_progress(self, job: ProcessingJob, progress: float):
                  job.progress = progress

                  for callback in self.callbacks.get(job.id, []):
                      try:
                          callback(progress)
                      except:
                          pass

              def _notify_completion(self, job: ProcessingJob):
                  if job.webhook_url:
                      import httpx
                      try:
                          httpx.post(job.webhook_url, json={
                              'job_id': job.id,
                              'status': job.status.value,
                              'result': job.result,
                              'error': job.error
                          })
                      except:
                          pass
          ```
      pitfalls:
      - Video progress is hard to estimate - use stages not percentage
      - Webhook delivery can fail - implement retry
      - Clean up temp files after processing
      - 'Memory limits: process one large video at a time'
      acceptance_criteria:
      - Media processing jobs are queued with priority ordering and executed by available worker processes
      - Processing progress is tracked and queryable showing percentage complete and estimated time remaining
      - Failed jobs are retried with exponential backoff up to a configurable maximum retry count
      - Webhook notifications are sent on job completion, failure, and progress milestone events
      deliverables:
      - Job queue that accepts media processing requests and schedules them for worker execution
      - Progress tracker that reports percentage completion for each active processing job in real time
      - Webhook notification sender that posts job status updates to configured callback URLs
      - Error handler that retries failed jobs with exponential backoff and reports permanent failures
  cdn-implementation:
    name: Content Delivery Network (CDN)
    description: Build a CDN with edge caching, cache invalidation, and origin shielding.
    why_expert: CDNs are critical for performance. Understanding caching strategies, invalidation, and edge logic helps optimize
      content delivery.
    difficulty: expert
    tags:
    - cdn
    - caching
    - edge
    - performance
    - distributed
    estimated_hours: 45
    prerequisites:
    - build-http-server
    - build-redis
    milestones:
    - name: Edge Cache Implementation
      description: Implement edge caching with TTL and cache control headers
      skills:
      - HTTP caching
      - Cache-Control
      - Vary headers
      hints:
        level1: Cache responses based on Cache-Control headers from origin
        level2: Vary header determines cache key variations (Accept-Encoding, etc)
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          import hashlib
          import time

          @dataclass
          class CacheEntry:
              key: str
              body: bytes
              headers: dict
              status_code: int
              created_at: float
              expires_at: float
              etag: Optional[str]
              last_modified: Optional[str]
              vary_headers: list[str]

          class CacheControl:
              def __init__(self, header: str):
                  self.max_age: Optional[int] = None
                  self.s_maxage: Optional[int] = None
                  self.no_cache = False
                  self.no_store = False
                  self.private = False
                  self.public = False
                  self.must_revalidate = False
                  self.stale_while_revalidate: Optional[int] = None
                  self.stale_if_error: Optional[int] = None

                  self._parse(header)

              def _parse(self, header: str):
                  if not header:
                      return

                  for directive in header.split(','):
                      directive = directive.strip().lower()

                      if directive == 'no-cache':
                          self.no_cache = True
                      elif directive == 'no-store':
                          self.no_store = True
                      elif directive == 'private':
                          self.private = True
                      elif directive == 'public':
                          self.public = True
                      elif directive == 'must-revalidate':
                          self.must_revalidate = True
                      elif directive.startswith('max-age='):
                          self.max_age = int(directive.split('=')[1])
                      elif directive.startswith('s-maxage='):
                          self.s_maxage = int(directive.split('=')[1])
                      elif directive.startswith('stale-while-revalidate='):
                          self.stale_while_revalidate = int(directive.split('=')[1])
                      elif directive.startswith('stale-if-error='):
                          self.stale_if_error = int(directive.split('=')[1])

              def is_cacheable(self) -> bool:
                  if self.no_store:
                      return False
                  if self.private:
                      return False  # CDN can't cache private
                  return True

              def get_ttl(self) -> int:
                  # s-maxage takes precedence for shared caches (CDN)
                  if self.s_maxage is not None:
                      return self.s_maxage
                  if self.max_age is not None:
                      return self.max_age
                  return 0

          class EdgeCache:
              def __init__(self, max_size_bytes: int = 1024 * 1024 * 1024):  # 1GB
                  self.cache: dict[str, CacheEntry] = {}
                  self.max_size = max_size_bytes
                  self.current_size = 0

              def generate_cache_key(self, url: str, vary_headers: dict) -> str:
                  '''Generate cache key from URL and Vary headers'''
                  key_parts = [url]

                  for header_name in sorted(vary_headers.keys()):
                      key_parts.append(f"{header_name}:{vary_headers[header_name]}")

                  key_string = '|'.join(key_parts)
                  return hashlib.sha256(key_string.encode()).hexdigest()

              def get(self, key: str) -> Optional[CacheEntry]:
                  entry = self.cache.get(key)
                  if not entry:
                      return None

                  now = time.time()

                  # Check if expired
                  if now > entry.expires_at:
                      # Check stale-while-revalidate
                      # (would need to trigger background revalidation)
                      return None

                  return entry

              def put(self, key: str, response: 'Response') -> Optional[CacheEntry]:
                  '''Cache response if cacheable'''
                  cache_control = CacheControl(response.headers.get('Cache-Control', ''))

                  if not cache_control.is_cacheable():
                      return None

                  ttl = cache_control.get_ttl()
                  if ttl <= 0:
                      return None

                  # Parse Vary header
                  vary_headers = []
                  vary = response.headers.get('Vary', '')
                  if vary and vary != '*':
                      vary_headers = [h.strip() for h in vary.split(',')]

                  now = time.time()
                  entry = CacheEntry(
                      key=key,
                      body=response.body,
                      headers=dict(response.headers),
                      status_code=response.status_code,
                      created_at=now,
                      expires_at=now + ttl,
                      etag=response.headers.get('ETag'),
                      last_modified=response.headers.get('Last-Modified'),
                      vary_headers=vary_headers
                  )

                  # Evict if needed
                  entry_size = len(entry.body)
                  while self.current_size + entry_size > self.max_size:
                      self._evict_one()

                  self.cache[key] = entry
                  self.current_size += entry_size

                  return entry

              def _evict_one(self):
                  '''Evict oldest entry (simple LRU would be better)'''
                  if not self.cache:
                      return

                  oldest_key = min(self.cache.keys(),
                                   key=lambda k: self.cache[k].created_at)
                  entry = self.cache.pop(oldest_key)
                  self.current_size -= len(entry.body)

              def invalidate(self, pattern: str):
                  '''Invalidate cache entries matching pattern'''
                  import fnmatch
                  keys_to_remove = [
                      k for k in self.cache.keys()
                      if fnmatch.fnmatch(k, pattern)
                  ]
                  for key in keys_to_remove:
                      entry = self.cache.pop(key)
                      self.current_size -= len(entry.body)

              def conditional_get(self, entry: CacheEntry, request_headers: dict) -> bool:
                  '''Check if conditional GET can return 304'''
                  # If-None-Match (ETag)
                  if_none_match = request_headers.get('If-None-Match')
                  if if_none_match and entry.etag:
                      if if_none_match == entry.etag or if_none_match == '*':
                          return True

                  # If-Modified-Since
                  if_modified = request_headers.get('If-Modified-Since')
                  if if_modified and entry.last_modified:
                      # Parse and compare dates
                      return if_modified == entry.last_modified

                  return False
          ```
      pitfalls:
      - 'Vary: * means never cache - handle this case'
      - 's-maxage vs max-age: CDN should use s-maxage'
      - 'ETag weak vs strong: weak allows semantic equivalence'
      - Cache key must include all Vary dimensions
      acceptance_criteria:
      - Cached responses are served based on URL and relevant Vary header combinations
      - LRU or LFU eviction removes least valuable entries when cache capacity is exceeded
      - Cache respects Accept-Encoding variants serving correct compressed or plain responses
      - Conditional requests with ETag return 304 Not Modified for unchanged resources
      deliverables:
      - Cache storage layer holding response bodies keyed by URL and vary headers
      - Cache key generation combining URL and Vary header values for unique identification
      - TTL management expiring cached entries based on Cache-Control header directives
      - Cache hit and miss handling serving cached responses or forwarding to origin
    - name: Cache Invalidation
      description: Implement purge, ban, and tag-based invalidation
      skills:
      - Invalidation strategies
      - Purge propagation
      - Surrogate keys
      hints:
        level1: 'Purge: remove specific URL; Ban: remove by pattern'
        level2: 'Surrogate keys: tag content for group invalidation'
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          import fnmatch
          import time
          import threading

          @dataclass
          class SurrogateKey:
              key: str
              cache_keys: set[str]

          class CacheInvalidator:
              def __init__(self, cache: EdgeCache):
                  self.cache = cache
                  self.surrogate_keys: dict[str, set[str]] = {}  # surrogate -> cache_keys
                  self.bans: list[tuple[str, float]] = []  # (pattern, timestamp)
                  self.ban_lock = threading.Lock()

              def purge(self, url: str) -> bool:
                  '''Purge specific URL from cache'''
                  # Generate all possible cache keys for this URL
                  # (simplified - would need to check all Vary combinations)
                  key = self.cache.generate_cache_key(url, {})

                  if key in self.cache.cache:
                      entry = self.cache.cache.pop(key)
                      self.cache.current_size -= len(entry.body)
                      return True
                  return False

              def purge_surrogate(self, surrogate_key: str) -> int:
                  '''Purge all objects tagged with surrogate key'''
                  cache_keys = self.surrogate_keys.get(surrogate_key, set())
                  count = 0

                  for cache_key in list(cache_keys):
                      if cache_key in self.cache.cache:
                          entry = self.cache.cache.pop(cache_key)
                          self.cache.current_size -= len(entry.body)
                          count += 1

                  # Clear surrogate mapping
                  if surrogate_key in self.surrogate_keys:
                      del self.surrogate_keys[surrogate_key]

                  return count

              def ban(self, pattern: str, ttl: int = 3600):
                  '''Ban pattern - matching requests bypass cache'''
                  with self.ban_lock:
                      expires = time.time() + ttl
                      self.bans.append((pattern, expires))

                      # Also immediately remove matching entries
                      self.cache.invalidate(pattern)

              def is_banned(self, url: str) -> bool:
                  '''Check if URL matches any active ban'''
                  now = time.time()

                  with self.ban_lock:
                      # Clean expired bans
                      self.bans = [(p, e) for p, e in self.bans if e > now]

                      for pattern, expires in self.bans:
                          if fnmatch.fnmatch(url, pattern):
                              return True

                  return False

              def register_surrogate(self, cache_key: str, surrogate_keys: list[str]):
                  '''Register cache entry with surrogate keys'''
                  for sk in surrogate_keys:
                      if sk not in self.surrogate_keys:
                          self.surrogate_keys[sk] = set()
                      self.surrogate_keys[sk].add(cache_key)

              def parse_surrogate_header(self, header: str) -> list[str]:
                  '''Parse Surrogate-Key header'''
                  if not header:
                      return []
                  return [k.strip() for k in header.split()]

          class CDNNode:
              def __init__(self, node_id: str, cache: EdgeCache,
                           invalidator: CacheInvalidator):
                  self.node_id = node_id
                  self.cache = cache
                  self.invalidator = invalidator
                  self.peers: list['CDNNode'] = []

              def propagate_purge(self, url: str):
                  '''Propagate purge to all peers'''
                  self.invalidator.purge(url)

                  for peer in self.peers:
                      # In production: async HTTP call to peer
                      peer.invalidator.purge(url)

              def propagate_ban(self, pattern: str):
                  '''Propagate ban to all peers'''
                  self.invalidator.ban(pattern)

                  for peer in self.peers:
                      peer.invalidator.ban(pattern)
          ```
      pitfalls:
      - Surrogate keys enable efficient invalidation of related content
      - 'Propagation delay: clients may get stale content briefly'
      - Soft purge (stale-while-revalidate) better than hard purge
      - Bans can grow unbounded - implement TTL and cleanup
      acceptance_criteria:
      - Purge API removes specific URL from cache and next request fetches fresh from origin
      - Surrogate key purge invalidates all resources tagged with specified key in single operation
      - Soft purge continues serving stale content while asynchronously revalidating from origin
      - Invalidation event propagates to all edge nodes within configurable time window
      deliverables:
      - Purge by URL removing specific cached resource from all edge nodes
      - Purge by tag using surrogate keys to invalidate groups of related resources
      - Soft purge marking content as stale while serving it during background revalidation
      - Invalidation propagation distributing purge commands to all edge cache nodes
    - name: Origin Shield & Request Collapsing
      description: Implement origin shielding and request collapsing to reduce origin load
      skills:
      - Origin protection
      - Request deduplication
      - Thundering herd
      hints:
        level1: 'Origin shield: single edge fetches from origin, others fetch from it'
        level2: 'Request collapsing: one request to origin, all waiters get result'
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional, Callable
          import threading
          import time
          from concurrent.futures import Future

          @dataclass
          class PendingRequest:
              key: str
              future: Future
              created_at: float

          class RequestCollapser:
              '''Collapse multiple requests for same resource into one'''

              def __init__(self, timeout: float = 30.0):
                  self.pending: dict[str, PendingRequest] = {}
                  self.lock = threading.Lock()
                  self.timeout = timeout

              def get_or_fetch(self, key: str,
                               fetch_fn: Callable[[], 'Response']) -> 'Response':
                  '''Get from pending request or create new one'''

                  with self.lock:
                      # Check if request already pending
                      if key in self.pending:
                          pending = self.pending[key]
                          # Wait for result
                          return pending.future.result(timeout=self.timeout)

                      # Create new pending request
                      future = Future()
                      self.pending[key] = PendingRequest(
                          key=key,
                          future=future,
                          created_at=time.time()
                      )

                  # Fetch outside lock
                  try:
                      response = fetch_fn()
                      future.set_result(response)
                      return response

                  except Exception as e:
                      future.set_exception(e)
                      raise

                  finally:
                      with self.lock:
                          if key in self.pending:
                              del self.pending[key]

          class OriginShield:
              '''Shield origin from direct edge requests'''

              def __init__(self, shield_cache: EdgeCache, origin_url: str):
                  self.cache = shield_cache
                  self.origin_url = origin_url
                  self.collapser = RequestCollapser()
                  self.request_count = 0
                  self.origin_requests = 0

              def fetch(self, path: str, headers: dict) -> 'Response':
                  '''Fetch from shield cache or origin'''
                  self.request_count += 1

                  cache_key = self.cache.generate_cache_key(path, {})

                  # Check shield cache
                  entry = self.cache.get(cache_key)
                  if entry:
                      return Response(
                          status_code=entry.status_code,
                          headers=entry.headers,
                          body=entry.body
                      )

                  # Fetch from origin with request collapsing
                  def fetch_origin():
                      self.origin_requests += 1
                      import httpx
                      response = httpx.get(
                          f"{self.origin_url}{path}",
                          headers=headers,
                          timeout=30.0
                      )
                      return Response(
                          status_code=response.status_code,
                          headers=dict(response.headers),
                          body=response.content
                      )

                  response = self.collapser.get_or_fetch(cache_key, fetch_origin)

                  # Cache response
                  self.cache.put(cache_key, response)

                  return response

              def get_stats(self) -> dict:
                  return {
                      'total_requests': self.request_count,
                      'origin_requests': self.origin_requests,
                      'shield_hit_rate': 1 - (self.origin_requests / max(1, self.request_count))
                  }

          class CDNEdge:
              '''Edge node with shield support'''

              def __init__(self, edge_id: str, local_cache: EdgeCache,
                           shield: OriginShield, invalidator: CacheInvalidator):
                  self.edge_id = edge_id
                  self.cache = local_cache
                  self.shield = shield
                  self.invalidator = invalidator
                  self.collapser = RequestCollapser()

              def handle_request(self, request: 'Request') -> 'Response':
                  '''Handle incoming request'''
                  url = request.url

                  # Check bans
                  if self.invalidator.is_banned(url):
                      return self._fetch_from_shield(request)

                  # Generate cache key
                  cache_key = self.cache.generate_cache_key(url, {})

                  # Check local cache
                  entry = self.cache.get(cache_key)
                  if entry:
                      # Check conditional request
                      if self.cache.conditional_get(entry, request.headers):
                          return Response(
                              status_code=304,
                              headers={'ETag': entry.etag},
                              body=b''
                          )

                      return Response(
                          status_code=entry.status_code,
                          headers=entry.headers,
                          body=entry.body
                      )

                  # Fetch from shield (with collapsing)
                  def fetch():
                      return self.shield.fetch(url, request.headers)

                  response = self.collapser.get_or_fetch(cache_key, fetch)

                  # Cache locally
                  entry = self.cache.put(cache_key, response)
                  if entry:
                      # Register surrogate keys
                      surrogate_header = response.headers.get('Surrogate-Key')
                      if surrogate_header:
                          keys = self.invalidator.parse_surrogate_header(surrogate_header)
                          self.invalidator.register_surrogate(cache_key, keys)

                  return response

              def _fetch_from_shield(self, request: 'Request') -> 'Response':
                  return self.shield.fetch(request.url, request.headers)
          ```
      pitfalls:
      - Request collapsing timeout must be shorter than client timeout
      - Shield adds latency but dramatically reduces origin load
      - 'Negative caching: cache 404s briefly to prevent origin storms'
      - 'Health checks: bypass shield when origin is unhealthy'
      acceptance_criteria:
      - Origin shield reduces origin requests by serving as intermediate cache for edge nodes
      - Concurrent requests for same resource result in single origin fetch with shared response
      - Cache miss storms do not overwhelm origin due to request collapsing and queuing
      - Stale-while-revalidate serves cached content while background request refreshes cache
      deliverables:
      - Origin shield layer caching responses between edge nodes and origin server
      - Request collapsing for concurrent requests deduplicating identical origin fetches
      - Cache fill optimization reducing origin load during cache miss storms
      - Origin protection limiting concurrent requests to prevent origin overload
  load-testing-framework:
    name: Distributed Load Testing Framework
    description: Build a load testing tool like k6/Locust with distributed workers, realistic user simulation, and real-time
      metrics.
    why_expert: Performance testing prevents outages. Understanding load generation, metrics collection, and bottleneck analysis
      is crucial for production systems.
    difficulty: expert
    tags:
    - testing
    - performance
    - load-testing
    - distributed
    - metrics
    estimated_hours: 45
    prerequisites:
    - build-http-server
    milestones:
    - name: Virtual User Simulation
      description: Implement virtual users with realistic think times and behavior
      skills:
      - User simulation
      - HTTP client
      - Think times
      hints:
        level1: Virtual user executes scenario repeatedly with configurable think times
        level2: 'Simulate realistic patterns: login, browse, action, logout'
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Callable, Optional
          from enum import Enum
          import asyncio
          import aiohttp
          import random
          import time

          @dataclass
          class RequestMetrics:
              method: str
              url: str
              status_code: int
              response_time_ms: float
              response_size: int
              error: Optional[str] = None
              timestamp: float = field(default_factory=time.time)

          class LoadProfile(Enum):
              CONSTANT = "constant"      # Fixed number of VUs
              RAMP_UP = "ramp_up"        # Gradually increase
              STEP = "step"              # Step increases
              SPIKE = "spike"            # Sudden spike

          @dataclass
          class StageConfig:
              duration: int      # Seconds
              target_vus: int    # Target virtual users
              think_time: tuple[float, float] = (1.0, 3.0)  # Min, max think time

          class VirtualUser:
              def __init__(self, user_id: int, scenario: 'Scenario',
                           metrics_callback: Callable[[RequestMetrics], None]):
                  self.user_id = user_id
                  self.scenario = scenario
                  self.metrics_callback = metrics_callback
                  self.session: Optional[aiohttp.ClientSession] = None
                  self.cookies = {}
                  self.running = False

              async def start(self):
                  self.running = True
                  self.session = aiohttp.ClientSession()

                  try:
                      while self.running:
                          await self._run_iteration()
                  finally:
                      await self.session.close()

              async def stop(self):
                  self.running = False

              async def _run_iteration(self):
                  '''Run one complete scenario iteration'''
                  for step in self.scenario.steps:
                      if not self.running:
                          break

                      # Execute step
                      metrics = await self._execute_step(step)
                      self.metrics_callback(metrics)

                      # Think time
                      if step.think_time:
                          await asyncio.sleep(random.uniform(*step.think_time))

              async def _execute_step(self, step: 'ScenarioStep') -> RequestMetrics:
                  start_time = time.time()

                  try:
                      async with self.session.request(
                          method=step.method,
                          url=step.url,
                          headers=step.headers,
                          json=step.body if step.body else None,
                          timeout=aiohttp.ClientTimeout(total=30)
                      ) as response:
                          body = await response.read()

                          return RequestMetrics(
                              method=step.method,
                              url=step.url,
                              status_code=response.status,
                              response_time_ms=(time.time() - start_time) * 1000,
                              response_size=len(body)
                          )

                  except Exception as e:
                      return RequestMetrics(
                          method=step.method,
                          url=step.url,
                          status_code=0,
                          response_time_ms=(time.time() - start_time) * 1000,
                          response_size=0,
                          error=str(e)
                      )

          @dataclass
          class ScenarioStep:
              name: str
              method: str
              url: str
              headers: dict = field(default_factory=dict)
              body: Optional[dict] = None
              think_time: Optional[tuple[float, float]] = None
              check: Optional[Callable] = None  # Response validation

          @dataclass
          class Scenario:
              name: str
              steps: list[ScenarioStep]
              weight: int = 1  # For weighted random selection

          class LoadRunner:
              def __init__(self, base_url: str):
                  self.base_url = base_url
                  self.scenarios: list[Scenario] = []
                  self.stages: list[StageConfig] = []
                  self.virtual_users: list[VirtualUser] = []
                  self.metrics: list[RequestMetrics] = []
                  self.running = False

              def add_scenario(self, scenario: Scenario):
                  self.scenarios.append(scenario)

              def add_stage(self, duration: int, target_vus: int, **kwargs):
                  self.stages.append(StageConfig(
                      duration=duration,
                      target_vus=target_vus,
                      **kwargs
                  ))

              def collect_metric(self, metric: RequestMetrics):
                  self.metrics.append(metric)

              async def run(self):
                  self.running = True
                  start_time = time.time()

                  for stage in self.stages:
                      stage_start = time.time()

                      # Adjust VU count
                      await self._scale_to(stage.target_vus)

                      # Wait for stage duration
                      while time.time() - stage_start < stage.duration:
                          if not self.running:
                              break
                          await asyncio.sleep(0.1)

                  # Cleanup
                  await self._scale_to(0)
                  self.running = False

              async def _scale_to(self, target: int):
                  current = len(self.virtual_users)

                  if target > current:
                      # Add VUs
                      for i in range(current, target):
                          scenario = random.choice(self.scenarios)  # Could weight
                          vu = VirtualUser(i, scenario, self.collect_metric)
                          self.virtual_users.append(vu)
                          asyncio.create_task(vu.start())

                  elif target < current:
                      # Remove VUs
                      for vu in self.virtual_users[target:]:
                          await vu.stop()
                      self.virtual_users = self.virtual_users[:target]
          ```
      pitfalls:
      - Think time prevents unrealistic load - real users pause between actions
      - Connection pooling affects results - configure properly
      - 'Coordinated omission: measure time from request creation, not send'
      - 'Virtual user state: some tests need session/cookie persistence'
      acceptance_criteria:
      - Concurrent virtual users are spawned up to a configurable maximum with a ramp-up period
      - Each virtual user executes the defined scenario as a repeating sequence of HTTP requests
      - Configurable think time delays between requests simulate realistic user pacing and behavior
      - Session and cookie management persists authentication tokens across requests within a virtual user
      deliverables:
      - User scenario definition DSL that describes a sequence of HTTP requests with parameters
      - Request executor that sends HTTP requests with configured method, headers, body, and assertions
      - Think time simulator that pauses between requests for a configurable random or fixed duration
      - Session handler that maintains cookies, tokens, and state across sequential requests in a scenario
    - name: Distributed Workers
      description: Implement distributed load generation with coordinator and workers
      skills:
      - Distributed coordination
      - Worker management
      - Aggregation
      hints:
        level1: Coordinator divides work; workers generate load and report metrics
        level2: Workers heartbeat to coordinator; auto-rebalance on failure
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional
          import asyncio
          import json
          import time
          from enum import Enum

          class WorkerStatus(Enum):
              IDLE = "idle"
              RUNNING = "running"
              STOPPING = "stopping"
              ERROR = "error"

          @dataclass
          class Worker:
              id: str
              host: str
              port: int
              status: WorkerStatus
              current_vus: int = 0
              last_heartbeat: float = 0
              metrics_count: int = 0

          @dataclass
          class TestConfig:
              scenarios: list[dict]
              stages: list[dict]
              base_url: str

          class Coordinator:
              def __init__(self, port: int = 5000):
                  self.port = port
                  self.workers: dict[str, Worker] = {}
                  self.test_config: Optional[TestConfig] = None
                  self.running = False
                  self.aggregated_metrics: list[RequestMetrics] = []

              async def register_worker(self, worker_id: str, host: str, port: int) -> Worker:
                  worker = Worker(
                      id=worker_id,
                      host=host,
                      port=port,
                      status=WorkerStatus.IDLE,
                      last_heartbeat=time.time()
                  )
                  self.workers[worker_id] = worker
                  return worker

              async def start_test(self, config: TestConfig):
                  self.test_config = config
                  self.running = True

                  # Divide stages among workers
                  worker_count = len(self.workers)
                  if worker_count == 0:
                      raise ValueError("No workers available")

                  for stage in config.stages:
                      # Distribute VUs evenly
                      vus_per_worker = stage['target_vus'] // worker_count
                      remainder = stage['target_vus'] % worker_count

                      for i, worker in enumerate(self.workers.values()):
                          worker_vus = vus_per_worker + (1 if i < remainder else 0)

                          await self._send_to_worker(worker, {
                              'command': 'run_stage',
                              'duration': stage['duration'],
                              'target_vus': worker_vus,
                              'scenarios': config.scenarios,
                              'base_url': config.base_url
                          })

                      # Wait for stage
                      await asyncio.sleep(stage['duration'])

                  await self.stop_test()

              async def stop_test(self):
                  self.running = False
                  for worker in self.workers.values():
                      await self._send_to_worker(worker, {'command': 'stop'})

              async def _send_to_worker(self, worker: Worker, message: dict):
                  import aiohttp
                  async with aiohttp.ClientSession() as session:
                      await session.post(
                          f"http://{worker.host}:{worker.port}/command",
                          json=message
                      )

              async def handle_heartbeat(self, worker_id: str, metrics: list[dict]):
                  if worker_id in self.workers:
                      self.workers[worker_id].last_heartbeat = time.time()
                      self.workers[worker_id].metrics_count += len(metrics)

                      # Aggregate metrics
                      for m in metrics:
                          self.aggregated_metrics.append(RequestMetrics(**m))

              async def check_worker_health(self):
                  now = time.time()
                  for worker in list(self.workers.values()):
                      if now - worker.last_heartbeat > 30:
                          worker.status = WorkerStatus.ERROR
                          # Redistribute load
                          await self._rebalance()

              async def _rebalance(self):
                  healthy = [w for w in self.workers.values()
                             if w.status != WorkerStatus.ERROR]
                  if not healthy or not self.running:
                      return

                  # Recalculate VU distribution
                  # ... (similar to start_test)

          class WorkerNode:
              def __init__(self, worker_id: str, coordinator_url: str, port: int = 5001):
                  self.worker_id = worker_id
                  self.coordinator_url = coordinator_url
                  self.port = port
                  self.load_runner: Optional[LoadRunner] = None
                  self.pending_metrics: list[RequestMetrics] = []

              async def run(self):
                  # Start web server for commands
                  from aiohttp import web

                  app = web.Application()
                  app.router.add_post('/command', self.handle_command)

                  # Start heartbeat task
                  asyncio.create_task(self._heartbeat_loop())

                  runner = web.AppRunner(app)
                  await runner.setup()
                  site = web.TCPSite(runner, '0.0.0.0', self.port)
                  await site.start()

              async def handle_command(self, request):
                  data = await request.json()
                  command = data.get('command')

                  if command == 'run_stage':
                      await self._run_stage(data)
                  elif command == 'stop':
                      if self.load_runner:
                          self.load_runner.running = False

                  return web.Response(text='ok')

              async def _run_stage(self, config: dict):
                  self.load_runner = LoadRunner(config['base_url'])

                  # Add scenarios
                  for s in config['scenarios']:
                      steps = [ScenarioStep(**step) for step in s['steps']]
                      self.load_runner.add_scenario(Scenario(
                          name=s['name'],
                          steps=steps
                      ))

                  # Collect metrics
                  self.load_runner.metrics_callback = lambda m: self.pending_metrics.append(m)

                  # Add stage
                  self.load_runner.add_stage(
                      duration=config['duration'],
                      target_vus=config['target_vus']
                  )

                  await self.load_runner.run()

              async def _heartbeat_loop(self):
                  import aiohttp

                  while True:
                      await asyncio.sleep(5)

                      # Send pending metrics
                      metrics_to_send = self.pending_metrics[:100]
                      self.pending_metrics = self.pending_metrics[100:]

                      try:
                          async with aiohttp.ClientSession() as session:
                              await session.post(
                                  f"{self.coordinator_url}/heartbeat",
                                  json={
                                      'worker_id': self.worker_id,
                                      'metrics': [m.__dict__ for m in metrics_to_send]
                                  }
                              )
                      except:
                          pass  # Coordinator unavailable
          ```
      pitfalls:
      - Network latency between workers adds to reported response time
      - 'Time synchronization: use relative times or sync clocks'
      - 'Worker failure mid-test: decide whether to continue or abort'
      - Metric aggregation can create memory pressure
      acceptance_criteria:
      - Virtual user load is distributed evenly across all connected worker nodes in the cluster
      - Coordinator signals synchronized test start and stop across all worker nodes simultaneously
      - Metrics from all workers are aggregated into a unified result set with consistent timestamps
      - Worker failures are detected and their load is redistributed to remaining healthy workers
      deliverables:
      - Worker node agent that connects to the coordinator and awaits test distribution instructions
      - Work distribution logic that partitions virtual users evenly across connected worker nodes
      - Result collector that aggregates raw response data from all workers into a central data store
      - Coordinator node that orchestrates test lifecycle including start, stop, and worker health monitoring
    - name: Real-time Metrics & Reporting
      description: Implement live metrics dashboard with percentiles and analysis
      skills:
      - Percentile calculation
      - Time series
      - Streaming aggregation
      hints:
        level1: Track p50, p95, p99 latency, not just average
        level2: Use t-digest or HDR histogram for accurate percentiles
        level3: |2

          ```python
          from dataclasses import dataclass
          from typing import Optional
          import math
          import time
          from collections import defaultdict

          class HDRHistogram:
              '''High Dynamic Range Histogram for accurate percentiles'''

              def __init__(self, lowest: int = 1, highest: int = 3600000,
                           significant_figures: int = 3):
                  self.lowest = lowest
                  self.highest = highest
                  self.significant_figures = significant_figures

                  # Calculate bucket count
                  self.bucket_count = self._calculate_bucket_count()
                  self.counts = [0] * self.bucket_count
                  self.total_count = 0

              def _calculate_bucket_count(self) -> int:
                  # Simplified - real implementation is more complex
                  return int(math.log2(self.highest / self.lowest) * 1000)

              def _get_bucket(self, value: float) -> int:
                  if value < self.lowest:
                      return 0
                  if value > self.highest:
                      return self.bucket_count - 1
                  return int(math.log2(value / self.lowest) * 100)

              def record(self, value: float):
                  bucket = self._get_bucket(value)
                  self.counts[bucket] += 1
                  self.total_count += 1

              def percentile(self, p: float) -> float:
                  if self.total_count == 0:
                      return 0

                  target_count = int(self.total_count * p / 100)
                  count_so_far = 0

                  for bucket, count in enumerate(self.counts):
                      count_so_far += count
                      if count_so_far >= target_count:
                          # Convert bucket back to value
                          return self.lowest * (2 ** (bucket / 100))

                  return self.highest

          @dataclass
          class MetricsSummary:
              count: int
              error_count: int
              error_rate: float
              min_ms: float
              max_ms: float
              mean_ms: float
              p50_ms: float
              p95_ms: float
              p99_ms: float
              requests_per_second: float

          class MetricsAggregator:
              def __init__(self, window_size: int = 10):
                  self.window_size = window_size  # Seconds
                  self.histograms: dict[str, HDRHistogram] = {}  # Per endpoint
                  self.global_histogram = HDRHistogram()

                  self.window_metrics: list[RequestMetrics] = []
                  self.window_start: float = time.time()

                  # Counters
                  self.total_requests = 0
                  self.total_errors = 0
                  self.endpoint_counts: dict[str, int] = defaultdict(int)

              def record(self, metric: RequestMetrics):
                  self.total_requests += 1

                  if metric.error or metric.status_code >= 400:
                      self.total_errors += 1

                  # Record to histograms
                  self.global_histogram.record(metric.response_time_ms)

                  endpoint = f"{metric.method} {metric.url}"
                  if endpoint not in self.histograms:
                      self.histograms[endpoint] = HDRHistogram()
                  self.histograms[endpoint].record(metric.response_time_ms)
                  self.endpoint_counts[endpoint] += 1

                  # Window metrics
                  self.window_metrics.append(metric)
                  self._cleanup_window()

              def _cleanup_window(self):
                  now = time.time()
                  cutoff = now - self.window_size
                  self.window_metrics = [
                      m for m in self.window_metrics
                      if m.timestamp > cutoff
                  ]

              def get_summary(self) -> MetricsSummary:
                  if self.total_requests == 0:
                      return MetricsSummary(
                          count=0, error_count=0, error_rate=0,
                          min_ms=0, max_ms=0, mean_ms=0,
                          p50_ms=0, p95_ms=0, p99_ms=0,
                          requests_per_second=0
                      )

                  response_times = [m.response_time_ms for m in self.window_metrics]

                  return MetricsSummary(
                      count=self.total_requests,
                      error_count=self.total_errors,
                      error_rate=self.total_errors / self.total_requests,
                      min_ms=min(response_times) if response_times else 0,
                      max_ms=max(response_times) if response_times else 0,
                      mean_ms=sum(response_times) / len(response_times) if response_times else 0,
                      p50_ms=self.global_histogram.percentile(50),
                      p95_ms=self.global_histogram.percentile(95),
                      p99_ms=self.global_histogram.percentile(99),
                      requests_per_second=len(self.window_metrics) / self.window_size
                  )

              def get_endpoint_summary(self, endpoint: str) -> Optional[MetricsSummary]:
                  if endpoint not in self.histograms:
                      return None

                  hist = self.histograms[endpoint]
                  count = self.endpoint_counts[endpoint]

                  return MetricsSummary(
                      count=count,
                      error_count=0,  # Would need separate tracking
                      error_rate=0,
                      min_ms=0,
                      max_ms=0,
                      mean_ms=0,
                      p50_ms=hist.percentile(50),
                      p95_ms=hist.percentile(95),
                      p99_ms=hist.percentile(99),
                      requests_per_second=0
                  )

          class ReportGenerator:
              def __init__(self, aggregator: MetricsAggregator):
                  self.aggregator = aggregator

              def generate_text_report(self) -> str:
                  summary = self.aggregator.get_summary()

                  lines = [
                      "Load Test Results",
                      "=" * 50,
                      f"Total Requests:    {summary.count:,}",
                      f"Failed Requests:   {summary.error_count:,} ({summary.error_rate:.1%})",
                      f"Requests/sec:      {summary.requests_per_second:.1f}",
                      "",
                      "Response Times (ms):",
                      f"  Min:    {summary.min_ms:.1f}",
                      f"  Mean:   {summary.mean_ms:.1f}",
                      f"  P50:    {summary.p50_ms:.1f}",
                      f"  P95:    {summary.p95_ms:.1f}",
                      f"  P99:    {summary.p99_ms:.1f}",
                      f"  Max:    {summary.max_ms:.1f}",
                  ]

                  return "\n".join(lines)

              def generate_json_report(self) -> dict:
                  summary = self.aggregator.get_summary()
                  return {
                      'summary': {
                          'total_requests': summary.count,
                          'error_count': summary.error_count,
                          'error_rate': summary.error_rate,
                          'requests_per_second': summary.requests_per_second
                      },
                      'latency': {
                          'min': summary.min_ms,
                          'mean': summary.mean_ms,
                          'p50': summary.p50_ms,
                          'p95': summary.p95_ms,
                          'p99': summary.p99_ms,
                          'max': summary.max_ms
                      },
                      'endpoints': {
                          endpoint: {
                              'count': self.aggregator.endpoint_counts[endpoint],
                              'p50': self.aggregator.histograms[endpoint].percentile(50),
                              'p99': self.aggregator.histograms[endpoint].percentile(99)
                          }
                          for endpoint in self.aggregator.histograms
                      }
                  }
          ```
      pitfalls:
      - Average hides outliers - always report percentiles
      - HDR histogram more accurate than naive quantile calculation
      - 'Streaming percentiles: use t-digest for memory efficiency'
      - Report generator should handle empty metrics gracefully
      acceptance_criteria:
      - Latency percentiles including p50, p90, p95, and p99 are calculated and updated in real time
      - Throughput in requests per second and error rates are tracked and displayed continuously
      - Live dashboard updates at least once per second showing current test metrics and progress
      - Final report is exported in HTML and JSON formats with summary statistics and time-series charts
      deliverables:
      - Response time tracker that records latency for every request with nanosecond precision
      - Throughput calculator that computes requests per second over sliding time windows
      - Error rate tracker that counts and categorizes HTTP errors, timeouts, and connection failures
      - Live dashboard that displays real-time charts of latency, throughput, and error rate during tests
  chaos-engineering:
    name: Chaos Engineering Platform
    description: Build a chaos engineering tool to test system resilience through controlled failure injection.
    why_expert: Systems fail in production. Chaos engineering proactively finds weaknesses. Understanding failure modes helps
      build resilient systems.
    difficulty: expert
    tags:
    - chaos
    - reliability
    - testing
    - resilience
    - fault-injection
    estimated_hours: 50
    prerequisites:
    - build-http-server
    - container-runtime
    milestones:
    - name: Fault Injection Framework
      description: Implement fault injection primitives (latency, errors, resource exhaustion)
      skills:
      - Fault injection
      - Proxy interception
      - Resource limits
      hints:
        level1: Inject faults at network, process, or application level
        level2: Use proxy to inject latency/errors; cgroups for resource limits
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Optional, Callable
          from abc import ABC, abstractmethod
          from enum import Enum
          import random
          import time
          import asyncio

          class FaultType(Enum):
              LATENCY = "latency"
              ERROR = "error"
              PACKET_LOSS = "packet_loss"
              CPU_STRESS = "cpu_stress"
              MEMORY_STRESS = "memory_stress"
              DISK_STRESS = "disk_stress"
              PROCESS_KILL = "process_kill"

          @dataclass
          class FaultConfig:
              type: FaultType
              target: str           # Service, pod, container
              duration: int         # Seconds
              probability: float = 1.0  # 0-1, for partial faults
              parameters: dict = field(default_factory=dict)

          class Fault(ABC):
              def __init__(self, config: FaultConfig):
                  self.config = config
                  self.active = False

              @abstractmethod
              async def inject(self):
                  pass

              @abstractmethod
              async def rollback(self):
                  pass

          class LatencyFault(Fault):
              '''Inject network latency'''

              async def inject(self):
                  latency_ms = self.config.parameters.get('latency_ms', 500)
                  jitter_ms = self.config.parameters.get('jitter_ms', 100)

                  # Use tc (traffic control) to add latency
                  import subprocess
                  subprocess.run([
                      'tc', 'qdisc', 'add', 'dev', 'eth0', 'root', 'netem',
                      'delay', f'{latency_ms}ms', f'{jitter_ms}ms'
                  ], check=True)

                  self.active = True

              async def rollback(self):
                  import subprocess
                  subprocess.run([
                      'tc', 'qdisc', 'del', 'dev', 'eth0', 'root'
                  ], check=False)
                  self.active = False

          class ErrorFault(Fault):
              '''Inject HTTP errors via proxy'''

              def __init__(self, config: FaultConfig, proxy: 'FaultProxy'):
                  super().__init__(config)
                  self.proxy = proxy

              async def inject(self):
                  status_code = self.config.parameters.get('status_code', 500)
                  self.proxy.add_rule(FaultRule(
                      target=self.config.target,
                      probability=self.config.probability,
                      action='error',
                      status_code=status_code
                  ))
                  self.active = True

              async def rollback(self):
                  self.proxy.remove_rule(self.config.target)
                  self.active = False

          class CPUStressFault(Fault):
              '''Stress CPU to simulate resource contention'''

              async def inject(self):
                  cores = self.config.parameters.get('cores', 1)
                  load_percent = self.config.parameters.get('load', 80)

                  # Start stress workers
                  self._workers = []
                  for _ in range(cores):
                      task = asyncio.create_task(self._stress_worker(load_percent))
                      self._workers.append(task)

                  self.active = True

              async def rollback(self):
                  for worker in self._workers:
                      worker.cancel()
                  self._workers = []
                  self.active = False

              async def _stress_worker(self, load_percent: int):
                  while True:
                      # Busy loop for load_percent of time
                      busy_time = load_percent / 100
                      start = time.time()
                      while time.time() - start < busy_time:
                          _ = sum(i*i for i in range(1000))

                      # Sleep for remainder
                      await asyncio.sleep(1 - busy_time)

          class ProcessKillFault(Fault):
              '''Kill process to test recovery'''

              async def inject(self):
                  process_name = self.config.parameters.get('process')
                  signal = self.config.parameters.get('signal', 'SIGKILL')

                  import subprocess
                  subprocess.run(['pkill', f'-{signal}', process_name])
                  self.active = True

              async def rollback(self):
                  # Process kill is permanent - can't rollback
                  # Could restart process if we have that capability
                  self.active = False

          @dataclass
          class FaultRule:
              target: str
              probability: float
              action: str
              status_code: int = 500
              latency_ms: int = 0

          class FaultProxy:
              '''HTTP proxy that injects faults'''

              def __init__(self, port: int = 8080):
                  self.port = port
                  self.rules: dict[str, FaultRule] = {}

              def add_rule(self, rule: FaultRule):
                  self.rules[rule.target] = rule

              def remove_rule(self, target: str):
                  if target in self.rules:
                      del self.rules[target]

              async def handle_request(self, request) -> 'Response':
                  # Check if any rule matches
                  for target, rule in self.rules.items():
                      if target in request.url:
                          if random.random() < rule.probability:
                              return self._apply_fault(request, rule)

                  # Forward to upstream
                  return await self._forward(request)

              def _apply_fault(self, request, rule: FaultRule) -> 'Response':
                  if rule.action == 'error':
                      return Response(
                          status_code=rule.status_code,
                          body=b'Injected fault'
                      )
                  elif rule.action == 'latency':
                      time.sleep(rule.latency_ms / 1000)
                      return self._forward(request)

              async def _forward(self, request) -> 'Response':
                  import aiohttp
                  async with aiohttp.ClientSession() as session:
                      async with session.request(
                          method=request.method,
                          url=request.url.replace(f':{self.port}', ''),
                          headers=request.headers,
                          data=request.body
                      ) as resp:
                          return Response(
                              status_code=resp.status,
                              body=await resp.read()
                          )
          ```
      pitfalls:
      - tc commands require root/CAP_NET_ADMIN
      - CPU stress can affect chaos tool itself - isolate
      - Process kill needs restart mechanism or test fails
      - Probability <1.0 creates intermittent failures (realistic)
      acceptance_criteria:
      - Network latency injection adds configurable delay to targeted service traffic
      - Packet loss simulation drops configurable percentage of network packets
      - Process kill fault terminates specified process and observes recovery behavior
      - CPU and memory stress faults exhaust resources to configurable utilization levels
      deliverables:
      - Fault type definitions for latency, error, packet loss, and resource exhaustion
      - Target selection mechanism specifying affected pods, services, or network paths
      - Fault scheduling controlling when and how long faults are active
      - Fault cleanup and rollback restoring normal operation after experiment ends
    - name: Experiment Orchestration
      description: Implement experiment definition, scheduling, and safety controls
      skills:
      - Experiment design
      - Safety controls
      - Rollback
      hints:
        level1: 'Experiment: hypothesis, faults to inject, metrics to observe'
        level2: 'Safety: abort conditions, blast radius limits, automatic rollback'
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Callable, Optional
          from enum import Enum
          import asyncio
          import time

          class ExperimentStatus(Enum):
              PENDING = "pending"
              RUNNING = "running"
              COMPLETED = "completed"
              ABORTED = "aborted"
              FAILED = "failed"

          @dataclass
          class SteadyStateHypothesis:
              name: str
              probe: Callable[[], bool]  # Returns True if steady state
              tolerance: float = 0.95   # Acceptable success rate

          @dataclass
          class AbortCondition:
              name: str
              check: Callable[[], bool]  # Returns True if should abort
              message: str

          @dataclass
          class Experiment:
              id: str
              name: str
              description: str
              hypothesis: SteadyStateHypothesis
              faults: list[FaultConfig]
              abort_conditions: list[AbortCondition]
              duration: int  # Seconds
              blast_radius: float = 0.1  # Max % of instances affected

          @dataclass
          class ExperimentResult:
              experiment_id: str
              status: ExperimentStatus
              started_at: float
              ended_at: float
              steady_state_before: bool
              steady_state_after: bool
              abort_reason: Optional[str] = None
              observations: list[dict] = field(default_factory=list)

          class ExperimentRunner:
              def __init__(self):
                  self.active_faults: list[Fault] = []
                  self.running = False

              async def run_experiment(self, experiment: Experiment) -> ExperimentResult:
                  result = ExperimentResult(
                      experiment_id=experiment.id,
                      status=ExperimentStatus.RUNNING,
                      started_at=time.time(),
                      ended_at=0,
                      steady_state_before=False,
                      steady_state_after=False
                  )

                  try:
                      # 1. Verify steady state before
                      result.steady_state_before = await self._check_steady_state(
                          experiment.hypothesis
                      )
                      if not result.steady_state_before:
                          result.status = ExperimentStatus.FAILED
                          result.abort_reason = "System not in steady state before experiment"
                          return result

                      # 2. Inject faults
                      self.running = True
                      for fault_config in experiment.faults:
                          fault = self._create_fault(fault_config)
                          await fault.inject()
                          self.active_faults.append(fault)

                      # 3. Run for duration while monitoring abort conditions
                      abort_reason = await self._monitor_experiment(
                          experiment.duration,
                          experiment.abort_conditions
                      )

                      if abort_reason:
                          result.status = ExperimentStatus.ABORTED
                          result.abort_reason = abort_reason
                      else:
                          # 4. Verify steady state after
                          result.steady_state_after = await self._check_steady_state(
                              experiment.hypothesis
                          )
                          result.status = ExperimentStatus.COMPLETED

                  except Exception as e:
                      result.status = ExperimentStatus.FAILED
                      result.abort_reason = str(e)

                  finally:
                      # 5. Rollback all faults
                      await self._rollback_all()
                      result.ended_at = time.time()
                      self.running = False

                  return result

              async def _check_steady_state(self, hypothesis: SteadyStateHypothesis) -> bool:
                  # Run probe multiple times to check stability
                  successes = 0
                  attempts = 10

                  for _ in range(attempts):
                      try:
                          if hypothesis.probe():
                              successes += 1
                      except:
                          pass
                      await asyncio.sleep(0.5)

                  return (successes / attempts) >= hypothesis.tolerance

              async def _monitor_experiment(self, duration: int,
                                             abort_conditions: list[AbortCondition]) -> Optional[str]:
                  start = time.time()

                  while time.time() - start < duration:
                      # Check abort conditions
                      for condition in abort_conditions:
                          try:
                              if condition.check():
                                  return condition.message
                          except:
                              pass

                      await asyncio.sleep(1)

                  return None

              async def _rollback_all(self):
                  for fault in self.active_faults:
                      try:
                          await fault.rollback()
                      except Exception as e:
                          print(f"Rollback failed: {e}")

                  self.active_faults = []

              def _create_fault(self, config: FaultConfig) -> Fault:
                  fault_classes = {
                      FaultType.LATENCY: LatencyFault,
                      FaultType.CPU_STRESS: CPUStressFault,
                      FaultType.PROCESS_KILL: ProcessKillFault,
                  }
                  return fault_classes[config.type](config)

          class SafetyController:
              '''Global safety controls for chaos experiments'''

              def __init__(self):
                  self.global_abort = False
                  self.active_experiments: list[str] = []
                  self.max_concurrent = 1
                  self.allowed_hours: tuple[int, int] = (9, 17)  # 9 AM - 5 PM

              def can_start_experiment(self, experiment: Experiment) -> tuple[bool, str]:
                  if self.global_abort:
                      return False, "Global abort is active"

                  if len(self.active_experiments) >= self.max_concurrent:
                      return False, "Too many concurrent experiments"

                  # Check time window
                  hour = time.localtime().tm_hour
                  if not (self.allowed_hours[0] <= hour < self.allowed_hours[1]):
                      return False, f"Outside allowed hours {self.allowed_hours}"

                  return True, ""

              def emergency_stop(self):
                  '''Stop all experiments immediately'''
                  self.global_abort = True
                  # In production: send signal to all experiment runners
          ```
      pitfalls:
      - Always verify steady state BEFORE injecting faults
      - Automatic rollback is critical - manual cleanup is error-prone
      - Abort conditions should check error rates, not just availability
      - Run during business hours initially - easier to respond to issues
      acceptance_criteria:
      - Experiments define clear hypothesis about expected system behavior under fault conditions
      - Steady state is verified before fault injection to establish valid baseline
      - Safety abort conditions automatically stop experiment when critical thresholds are breached
      - Experiment results report whether steady state was maintained during fault injection
      deliverables:
      - Experiment definition specifying hypothesis, faults, duration, and blast radius
      - Steady state hypothesis verification checking system health before and after faults
      - Experiment execution engine injecting faults and monitoring system behavior
      - Result analysis comparing pre-fault and post-fault steady state measurements
    - name: GameDay Automation
      description: Implement scheduled chaos experiments with runbooks and incident response
      skills:
      - GameDay planning
      - Runbooks
      - Incident response
      hints:
        level1: 'GameDay: scheduled chaos with observers and runbooks'
        level2: 'Automated runbooks: if X happens, do Y to recover'
        level3: |2

          ```python
          from dataclasses import dataclass, field
          from typing import Callable, Optional
          from enum import Enum
          import asyncio
          import time
          from datetime import datetime, timedelta

          @dataclass
          class RunbookStep:
              name: str
              action: Callable
              rollback: Optional[Callable] = None
              timeout: int = 60

          @dataclass
          class Runbook:
              id: str
              name: str
              trigger_condition: Callable[[], bool]
              steps: list[RunbookStep]
              auto_execute: bool = False

          class RunbookExecutor:
              async def execute(self, runbook: Runbook) -> dict:
                  results = {'steps': [], 'success': True}
                  executed_steps = []

                  for step in runbook.steps:
                      try:
                          await asyncio.wait_for(
                              asyncio.create_task(step.action()),
                              timeout=step.timeout
                          )
                          results['steps'].append({
                              'name': step.name,
                              'status': 'success'
                          })
                          executed_steps.append(step)

                      except Exception as e:
                          results['steps'].append({
                              'name': step.name,
                              'status': 'failed',
                              'error': str(e)
                          })
                          results['success'] = False

                          # Rollback executed steps
                          await self._rollback(executed_steps)
                          break

                  return results

              async def _rollback(self, steps: list[RunbookStep]):
                  for step in reversed(steps):
                      if step.rollback:
                          try:
                              await step.rollback()
                          except:
                              pass

          @dataclass
          class GameDay:
              id: str
              name: str
              description: str
              scheduled_start: datetime
              experiments: list[Experiment]
              runbooks: list[Runbook]
              observers: list[str]  # Email/Slack of observers
              duration_hours: int = 2

          class GameDayCoordinator:
              def __init__(self, experiment_runner: ExperimentRunner,
                           runbook_executor: RunbookExecutor,
                           notifier: 'Notifier'):
                  self.runner = experiment_runner
                  self.runbook_exec = runbook_executor
                  self.notifier = notifier
                  self.scheduled_gamedays: list[GameDay] = []

              async def schedule_gameday(self, gameday: GameDay):
                  self.scheduled_gamedays.append(gameday)

                  # Notify observers
                  await self.notifier.notify(
                      gameday.observers,
                      f"GameDay '{gameday.name}' scheduled for {gameday.scheduled_start}"
                  )

              async def run_gameday(self, gameday: GameDay):
                  await self.notifier.notify(
                      gameday.observers,
                      f"GameDay '{gameday.name}' starting now!"
                  )

                  results = {
                      'gameday_id': gameday.id,
                      'experiments': [],
                      'runbooks_triggered': []
                  }

                  for experiment in gameday.experiments:
                      # Run experiment
                      exp_result = await self.runner.run_experiment(experiment)
                      results['experiments'].append({
                          'id': experiment.id,
                          'status': exp_result.status.value,
                          'steady_state_maintained': exp_result.steady_state_after
                      })

                      # Check if any runbooks should trigger
                      for runbook in gameday.runbooks:
                          if runbook.trigger_condition():
                              if runbook.auto_execute:
                                  rb_result = await self.runbook_exec.execute(runbook)
                                  results['runbooks_triggered'].append({
                                      'runbook': runbook.id,
                                      'result': rb_result
                                  })
                              else:
                                  await self.notifier.notify(
                                      gameday.observers,
                                      f"Runbook '{runbook.name}' should be executed manually"
                                  )

                      # Brief pause between experiments
                      await asyncio.sleep(60)

                  await self.notifier.notify(
                      gameday.observers,
                      f"GameDay '{gameday.name}' completed. Results: {results}"
                  )

                  return results

          @dataclass
          class Observation:
              timestamp: float
              observer: str
              type: str  # 'issue', 'note', 'recovery'
              description: str
              metrics: dict = field(default_factory=dict)

          class GameDayRecorder:
              '''Record observations during GameDay'''

              def __init__(self, gameday_id: str):
                  self.gameday_id = gameday_id
                  self.observations: list[Observation] = []
                  self.timeline: list[dict] = []

              def record_observation(self, observer: str, obs_type: str,
                                     description: str, metrics: dict = None):
                  obs = Observation(
                      timestamp=time.time(),
                      observer=observer,
                      type=obs_type,
                      description=description,
                      metrics=metrics or {}
                  )
                  self.observations.append(obs)

              def record_event(self, event_type: str, details: dict):
                  self.timeline.append({
                      'timestamp': time.time(),
                      'event': event_type,
                      'details': details
                  })

              def generate_report(self) -> dict:
                  return {
                      'gameday_id': self.gameday_id,
                      'timeline': self.timeline,
                      'observations': [
                          {
                              'time': obs.timestamp,
                              'observer': obs.observer,
                              'type': obs.type,
                              'description': obs.description
                          }
                          for obs in self.observations
                      ],
                      'issues_found': [
                          obs for obs in self.observations
                          if obs.type == 'issue'
                      ],
                      'recommendations': self._generate_recommendations()
                  }

              def _generate_recommendations(self) -> list[str]:
                  recommendations = []

                  # Analyze observations for patterns
                  issues = [o for o in self.observations if o.type == 'issue']
                  for issue in issues:
                      if 'timeout' in issue.description.lower():
                          recommendations.append("Consider implementing circuit breakers")
                      if 'memory' in issue.description.lower():
                          recommendations.append("Review memory limits and implement backpressure")

                  return recommendations
          ```
      pitfalls:
      - GameDays need preparation - brief observers on experiments
      - Manual runbook steps may be needed for complex recovery
      - Record everything - observations are valuable learning
      - Schedule GameDays when team is available to respond
      acceptance_criteria:
      - GameDay runs multiple experiments in sequence with pauses between each
      - System metrics are monitored continuously during experiment and compared to baseline
      - Auto-rollback triggers when safety threshold breach is detected during any experiment
      - Final report summarizes each experiment outcome with pass/fail and observations
      deliverables:
      - Scenario scripting defining multi-experiment sequences for comprehensive testing
      - Automated health checks monitoring system metrics throughout experiment duration
      - Incident simulation replicating real-world failure scenarios in controlled environment
      - Recovery validation confirming system returns to normal after faults are removed
  time-series-db:
    name: Time-Series Database
    description: Build a specialized database optimized for time-stamped data with compression, downsampling, and efficient
      range queries - essential for metrics, IoT, and financial data
    category: Data Storage
    difficulty: advanced
    estimated_hours: 180
    skills:
    - Time-series data modeling
    - Columnar storage
    - Delta encoding and compression
    - Write-ahead logging
    - Retention policies
    - Continuous queries
    prerequisites:
    - database-engine
    - distributed-cache
    learning_outcomes:
    - Understand time-series data characteristics and access patterns
    - Implement columnar storage with compression algorithms
    - Build efficient time-range query execution
    - Design downsampling and aggregation pipelines
    - Create retention and compaction policies
    milestones:
    - name: Storage Engine
      description: Time-series optimized storage with compression
      skills:
      - Columnar storage
      - Delta encoding
      - Run-length encoding
      deliverables:
      - Time-structured merge tree (TSM) implementation
      - Delta-of-delta timestamp compression reducing storage for sequential timestamps
      - Gorilla float compression algorithm encoding similar float values efficiently
      - Dictionary encoding for tags and labels mapping strings to integer IDs
      - Block-based storage with an index mapping time ranges to data blocks
      - Memory-mapped file access for efficient random reads without user-space buffering
      acceptance_criteria:
      - Design time series storage format with columnar layout for timestamps and values
      - Implement column-oriented storage separating timestamps, values, and tags
      - Handle high write throughput sustaining thousands of points per second
      - Support efficient range queries scanning only relevant time blocks
      hints:
        level1: Time-series data is append-heavy and read-sequential. Store timestamps and values in separate columns for
          better compression. Consider delta encoding for timestamps.
        level2: |-
          Implement Gorilla compression for floats - XOR consecutive values and encode leading/trailing zeros. For timestamps, use delta-of-delta encoding:
          ```python
          def encode_timestamp_delta(timestamps):
              deltas = [timestamps[i] - timestamps[i-1] for i in range(1, len(timestamps))]
              delta_of_deltas = [deltas[i] - deltas[i-1] for i in range(1, len(deltas))]
              return delta_of_deltas  # Most will be 0 for regular intervals
          ```
        level3: "Full columnar storage with compression:\n```python\nimport struct\nfrom bitarray import bitarray\n\nclass\
          \ TSMBlock:\n     def __init__(self, timestamps, values):\n        self.ts_compressed = self._compress_timestamps(timestamps)\n\
          \        self.val_compressed = self._compress_floats(values)\n        self.min_ts, self.max_ts = min(timestamps),\
          \ max(timestamps)\n        self.count = len(timestamps)\n    \n    def _compress_timestamps(self, ts):\n       \
          \ # Delta-of-delta encoding\n        if len(ts) < 2: return ts\n        first = ts[0]\n        deltas = [ts[i] -\
          \ ts[i-1] for i in range(1, len(ts))]\n        return (first, deltas[0], [deltas[i] - deltas[i-1] for i in range(1,\
          \ len(deltas))])\n    \n    def _compress_floats(self, values):\n        # Gorilla XOR compression\n        bits\
          \ = bitarray()\n        prev = 0\n        for v in values:\n            curr = struct.unpack('Q', struct.pack('d',\
          \ v))[0]\n            xor = prev ^ curr\n            # Encode XOR with leading/trailing zero optimization\n    \
          \        bits.extend(self._encode_xor(xor))\n            prev = curr\n        return bits\n```"
    - name: Write Path
      description: High-throughput ingestion with buffering
      skills:
      - Write batching
      - WAL
      - Memory management
      deliverables:
      - Write-ahead log ensuring durability before acknowledging writes
      - In-memory buffer (memtable) for writes
      - Batch point ingestion API accepting multiple data points in one call
      - Out-of-order write handling inserting late-arriving points into correct positions
      - Series cardinality tracking monitoring the number of unique time series
      - Backpressure mechanism throttling writes when the memtable is near capacity
      acceptance_criteria:
      - Buffer writes in memory and persist via WAL before acknowledging
      - Batch writes flush to storage when memtable reaches size threshold
      - Handle out-of-order data by merging late points during compaction
      - Implement write acknowledgment confirming durability to the client
      hints:
        level1: 'Use a two-stage write path: WAL for durability, then memtable for fast writes. Batch points by series key
          before flushing to disk. Handle backpressure when memtable fills up.'
        level2: "Implement write batching with configurable thresholds:\n```python\nclass WriteBuffer:\n    def __init__(self,\
          \ max_size=10000, max_age_ms=1000):\n        self.points = defaultdict(list)  # series_key -> [(ts, value)]\n  \
          \      self.count = 0\n        self.last_flush = time.time()\n    \n    def add(self, series_key, timestamp, value):\n\
          \        self.points[series_key].append((timestamp, value))\n        self.count += 1\n        if self.should_flush():\n\
          \            return self.flush()\n```"
        level3: "Full write path with WAL and out-of-order handling:\n```python\nclass WriteAheadLog:\n    def __init__(self,\
          \ path):\n        self.file = open(path, 'ab')\n        self.segment_size = 64 * 1024 * 1024\n    \n    def append(self,\
          \ points):\n        # Format: [len][crc32][data]\n        data = msgpack.packb(points)\n        header = struct.pack('<IH',\
          \ len(data), crc32(data) & 0xFFFF)\n        self.file.write(header + data)\n        self.file.flush()\n        os.fsync(self.file.fileno())\n\
          \nclass TSDBWriter:\n    def __init__(self, data_dir):\n        self.wal = WriteAheadLog(f'{data_dir}/wal')\n  \
          \      self.memtable = WriteBuffer()\n        self.ooo_buffer = {}  # Out-of-order points\n    \n    def write(self,\
          \ series, timestamp, value):\n        self.wal.append([(series, timestamp, value)])\n        if timestamp < self.memtable.min_ts.get(series,\
          \ float('inf')) - 3600:\n            # More than 1 hour old, goes to OOO buffer\n            self.ooo_buffer.setdefault(series,\
          \ []).append((timestamp, value))\n        else:\n            self.memtable.add(series, timestamp, value)\n```"
    - name: Query Engine
      description: Efficient time-range queries and aggregations
      skills:
      - Query planning
      - Aggregations
      - Downsampling
      deliverables:
      - Time-range predicate pushdown skipping blocks outside the query window
      - Tag-based filtering and indexing using inverted index for fast label lookups
      - Built-in aggregation functions (sum, avg, min, max, count)
      - Windowed aggregations supporting tumbling and sliding time intervals
      - GROUP BY time bucket queries partitioning results into fixed-width intervals
      - Last and first value query optimization using block-level metadata shortcuts
      acceptance_criteria:
      - Support time range queries returning points within start and end bounds
      - Implement aggregation functions producing correct results over time windows
      - Support downsampling returning lower-resolution summaries of raw data
      - Handle complex filter expressions combining tag predicates with boolean logic
      hints:
        level1: 'Push time range predicates down to storage layer to skip irrelevant blocks. Use block min/max timestamps
          for pruning. Implement common aggregations: sum, avg, min, max, count, first, last.'
        level2: |-
          Implement windowed aggregations with tumbling/sliding windows:
          ```python
          def aggregate_by_window(points, window_size, agg_func):
              buckets = defaultdict(list)
              for ts, value in points:
                  bucket = ts // window_size * window_size
                  buckets[bucket].append(value)
              return [(ts, agg_func(vals)) for ts, vals in sorted(buckets.items())]

          def downsample(points, factor, agg='avg'):
              funcs = {'avg': statistics.mean, 'sum': sum, 'min': min, 'max': max}
              return aggregate_by_window(points, factor, funcs[agg])
          ```
        level3: "Full query engine with predicate pushdown:\n```python\nclass QueryEngine:\n    def __init__(self, storage):\n\
          \        self.storage = storage\n    \n    def execute(self, query):\n        # Parse: SELECT avg(value) FROM cpu\
          \ WHERE time > now()-1h GROUP BY time(5m)\n        time_range = query.get_time_range()\n        series_filter =\
          \ query.get_series_filter()\n        \n        # Predicate pushdown - only scan relevant blocks\n        blocks\
          \ = self.storage.find_blocks(\n            series=series_filter,\n            min_ts=time_range.start,\n       \
          \     max_ts=time_range.end\n        )\n        \n        # Stream aggregation to minimize memory\n        aggregator\
          \ = WindowedAggregator(\n            window=query.group_by_interval,\n            func=query.aggregation\n     \
          \   )\n        \n        for block in blocks:\n            for ts, value in block.scan(time_range):\n          \
          \      if query.matches(ts, value):\n                    aggregator.add(ts, value)\n        \n        return aggregator.results()\n\
          ```"
    - name: Retention & Compaction
      description: Automatic data lifecycle management
      skills:
      - Retention policies
      - Compaction
      - Downsampling
      deliverables:
      - TTL-based retention policies specifying maximum data age per measurement
      - Automatic data expiration and deletion
      - Background compaction process merging small TSM files into larger optimized ones
      - Level-based compaction strategy promoting data through compaction tiers
      - Continuous downsampling queries pre-aggregating old data into lower resolution
      - Rollup aggregation storage persisting pre-computed summaries for fast historical queries
      acceptance_criteria:
      - Enforce data TTL by automatically deleting expired data blocks
      - Implement storage compaction merging overlapping blocks and reclaiming space
      - Support downsampling for old data reducing storage while preserving trends
      - Handle storage reclamation freeing disk space after compaction and expiry
      hints:
        level1: Implement TTL at the block level - mark blocks for deletion when all data exceeds retention. Run compaction
          in background to merge small blocks and remove tombstones.
        level2: "Implement level-based compaction similar to LSM trees:\n```python\nclass CompactionManager:\n    def __init__(self,\
          \ data_dir):\n        self.levels = [[], [], [], []]  # L0-L3\n        self.level_max_size = [4, 10, 100, 1000]\
          \  # MB\n    \n    def maybe_compact(self, level):\n        if self.level_size(level) > self.level_max_size[level]:\n\
          \            self.compact_level(level)\n    \n    def compact_level(self, level):\n        # Pick overlapping blocks\
          \ from level and level+1\n        blocks = self.select_blocks(level)\n        merged = self.merge_blocks(blocks)\n\
          \        self.write_to_level(merged, level + 1)\n        self.delete_blocks(blocks)\n```"
        level3: "Full retention and downsampling system:\n```python\nclass RetentionManager:\n    def __init__(self, storage):\n\
          \        self.storage = storage\n        self.policies = {}  # measurement -> RetentionPolicy\n    \n    def apply_retention(self):\n\
          \        now = time.time()\n        for measurement, policy in self.policies.items():\n            # Delete raw\
          \ data older than retention\n            cutoff = now - policy.retention_seconds\n            self.storage.delete_before(measurement,\
          \ cutoff)\n            \n            # Downsample to rollup table\n            if policy.rollup_interval:\n    \
          \            self.create_rollups(measurement, policy)\n    \n    def create_rollups(self, measurement, policy):\n\
          \        # Find data not yet rolled up\n        raw_data = self.storage.scan(\n            measurement,\n      \
          \      start=self.last_rollup_ts,\n            end=time.time() - policy.rollup_delay\n        )\n        # Aggregate\
          \ by rollup interval\n        rollups = aggregate_by_window(\n            raw_data,\n            policy.rollup_interval,\n\
          \            policy.rollup_aggregations  # e.g., ['min', 'max', 'mean', 'count']\n        )\n        self.storage.write_rollup(f'{measurement}_rollup',\
          \ rollups)\n```"
    - name: Query Language & API
      description: Expressive query interface for time-series
      skills:
      - Query parsing
      - API design
      - PromQL/InfluxQL
      deliverables:
      - SQL-like query language with time extensions
      - Flux-style functional query pipeline chaining filter, map, and aggregate operations
      - HTTP write API (Line Protocol compatible)
      - Query API with multiple output formats
      - Prometheus remote read and write API for integration with Prometheus ecosystem
      - Grafana data source compatibility endpoint serving queries in Grafana format
      acceptance_criteria:
      - Implement time series query language parsing SELECT with time-range predicates
      - Support aggregation functions within query language expressions
      - Handle GROUP BY time intervals with configurable bucket widths
      - Expose REST and gRPC API endpoints for both writes and queries
      hints:
        level1: Design a SQL-like query language with time-specific extensions. Support both pull queries (SELECT) and push
          queries (continuous queries). Implement Line Protocol for high-throughput writes.
        level2: |-
          Implement InfluxQL-style syntax with time functions:
          ```
          SELECT mean("cpu") FROM "metrics" WHERE time > now() - 1h GROUP BY time(5m), host
          ```

          Parse with a simple recursive descent parser or use a parser generator like PLY/ANTLR.
        level3: |-
          Full query language and API implementation:
          ```python
          from flask import Flask, request
          import re

          app = Flask(__name__)

          # Line Protocol parser: measurement,tag=value field=value timestamp
          def parse_line_protocol(line):
              match = re.match(
                  r'([^,\s]+),?([^\s]*)\s+([^\s]+)\s*(\d+)?',
                  line
              )
              measurement = match.group(1)
              tags = dict(t.split('=') for t in match.group(2).split(',') if t)
              fields = {}
              for f in match.group(3).split(','):
                  k, v = f.split('=')
                  fields[k] = float(v.rstrip('i'))
              timestamp = int(match.group(4)) if match.group(4) else time.time_ns()
              return measurement, tags, fields, timestamp

          @app.route('/write', methods=['POST'])
          def write():
              for line in request.data.decode().strip().split('\n'):
                  measurement, tags, fields, ts = parse_line_protocol(line)
                  db.write(measurement, tags, fields, ts)
              return '', 204

          @app.route('/query', methods=['GET', 'POST'])
          def query():
              q = request.args.get('q') or request.form.get('q')
              ast = parse_query(q)
              results = execute_query(ast)
              return jsonify({'results': [{'series': results}]})
          ```
  graphql-server:
    name: GraphQL Server
    description: Build a production GraphQL API with schema-first design, resolvers, dataloaders for N+1 prevention, and real-time
      subscriptions
    category: Backend
    difficulty: intermediate
    estimated_hours: 40
    skills:
    - GraphQL schema design
    - Resolver patterns
    - N+1 query prevention
    - Real-time subscriptions
    - Authentication in GraphQL
    - Error handling
    prerequisites:
    - REST API basics
    - Database queries
    learning_outcomes:
    - Design type-safe GraphQL schemas
    - Implement efficient data fetching with dataloaders
    - Handle authentication and authorization in GraphQL
    - Build real-time features with subscriptions
    milestones:
    - name: Schema & Type System
      description: Define GraphQL schema with types, queries, mutations
      skills:
      - SDL
      - Type definitions
      - Schema design
      deliverables:
      - GraphQL schema with Object types defining entity structure
      - Query type with field resolvers for data retrieval operations
      - Mutation type for CRUD operations modifying server state
      - Input types for mutations with validated field definitions
      - Custom scalar types such as DateTime and JSON with serialization
      - Enum types for fixed-value fields with explicit allowed values
      hints:
        level1: |-
          Start with SDL (Schema Definition Language). Define your domain types first:
          ```graphql
          type User {
            id: ID!
            email: String!
            posts: [Post!]!
          }
          ```
        level2: |-
          Use input types for mutations to keep them clean:
          ```graphql
          input CreateUserInput {
            email: String!
            name: String!
          }

          type Mutation {
            createUser(input: CreateUserInput!): User!
          }
          ```
        level3: |-
          For custom scalars, you need both schema definition and resolver:
          ```javascript
          const DateTimeScalar = new GraphQLScalarType({
            name: 'DateTime',
            serialize(value) { return value.toISOString(); },
            parseValue(value) { return new Date(value); }
          });
          ```
      acceptance_criteria:
      - Define types, queries, and mutations using GraphQL SDL syntax
      - Support custom scalar types with serialize, parseValue, and parseLiteral methods
      - Implement interfaces and unions for polymorphic type relationships
      - Validate schema at startup and report errors for invalid definitions
    - name: Resolvers & Data Fetching
      description: Implement resolvers that fetch data from database
      skills:
      - Resolver functions
      - Context
      - Database queries
      deliverables:
      - Root query resolvers handling top-level data fetching operations
      - Field resolvers for nested types loading related entity data
      - Mutation resolvers with input validation and error handling
      - Context setup providing database connection and auth info to resolvers
      - Error handling in resolvers with structured GraphQL error responses
      - Resolver composition patterns for reusable middleware and authorization
      hints:
        level1: |-
          Resolvers receive (parent, args, context, info). Use context for shared resources:
          ```javascript
          const resolvers = {
            Query: {
              user: (_, { id }, { db }) => db.users.findById(id)
            }
          };
          ```
        level2: |-
          Field resolvers handle nested data. They receive parent object:
          ```javascript
          User: {
            posts: (user, _, { db }) => db.posts.findByUserId(user.id)
          }
          ```
        level3: |-
          Use resolver middleware for cross-cutting concerns:
          ```javascript
          const authenticated = (resolver) => (parent, args, ctx, info) => {
            if (!ctx.user) throw new AuthenticationError('Not logged in');
            return resolver(parent, args, ctx, info);
          };
          ```
      acceptance_criteria:
      - Implement resolver functions per field that fetch correct data from database
      - Pass context object containing database connection and user info to all resolvers
      - Handle resolver errors gracefully and return structured GraphQL error responses
      - Support async resolvers for non-blocking database and external service calls
    - name: DataLoader & N+1 Prevention
      description: Batch and cache database queries to prevent N+1 problem
      skills:
      - DataLoader
      - Batching
      - Caching
      deliverables:
      - DataLoader instance for each entity type in the schema
      - Batch function implementation loading multiple records in single query
      - Per-request caching avoiding duplicate loads within one GraphQL operation
      - DataLoader integration in resolver context for transparent batching
      - Handling batch errors mapping failures to individual requested keys
      - Cache invalidation strategy preventing stale data across requests
      hints:
        level1: |-
          N+1 happens when fetching users[].posts makes N separate queries. DataLoader batches them:
          ```javascript
          const userLoader = new DataLoader(async (ids) => {
            const users = await db.users.findByIds(ids);
            return ids.map(id => users.find(u => u.id === id));
          });
          ```
        level2: |-
          Create loaders per-request to avoid cache leaks:
          ```javascript
          const createLoaders = (db) => ({
            userLoader: new DataLoader(ids => batchUsers(db, ids)),
            postLoader: new DataLoader(ids => batchPosts(db, ids))
          });
          // In context: { loaders: createLoaders(db) }
          ```
        level3: |-
          For has-many relations, batch by foreign key:
          ```javascript
          const postsByUserLoader = new DataLoader(async (userIds) => {
            const posts = await db.posts.where('userId').in(userIds);
            return userIds.map(id => posts.filter(p => p.userId === id));
          });
          ```
      acceptance_criteria:
      - Batch database requests with DataLoader reducing queries from N+1 to 2
      - Cache loaded results within a single request to avoid duplicate fetches
      - Clear DataLoader cache between requests to prevent cross-request data leaks
      - Show N+1 queries eliminated by comparing query counts in request logs
    - name: Subscriptions
      description: Real-time updates via WebSocket subscriptions
      skills:
      - WebSocket
      - PubSub
      - Event-driven
      deliverables:
      - WebSocket transport setup for persistent subscription connections
      - PubSub implementation for event broadcasting to subscriber channels
      - Subscription resolvers yielding events via async iterator pattern
      - Filtered subscriptions delivering only events matching subscriber criteria
      - Authentication for subscriptions validating credentials on WebSocket connect
      - Connection lifecycle handling for connect, disconnect, and keep-alive events
      hints:
        level1: |-
          Subscriptions use AsyncIterator pattern:
          ```javascript
          Subscription: {
            postCreated: {
              subscribe: () => pubsub.asyncIterator(['POST_CREATED'])
            }
          }
          // Publish: pubsub.publish('POST_CREATED', { postCreated: post })
          ```
        level2: |-
          Filter subscriptions by arguments:
          ```javascript
          postCreated: {
            subscribe: withFilter(
              () => pubsub.asyncIterator(['POST_CREATED']),
              (payload, variables) => payload.postCreated.authorId === variables.authorId
            )
          }
          ```
        level3: |-
          Handle connection auth in onConnect:
          ```javascript
          new WebSocketServer({
            onConnect: async (connectionParams) => {
              const token = connectionParams.authToken;
              const user = await verifyToken(token);
              return { user };
            }
          });
          ```
      acceptance_criteria:
      - WebSocket connection established for GraphQL subscription transport layer
      - Publish events to all active subscribers on matching subscription channels
      - Handle subscription filtering so subscribers receive only relevant event data
      - Clean up subscription resources on client disconnect or connection timeout
  background-job-processor:
    name: Background Job Processor
    description: Build an async task queue system like Sidekiq/Celery with job scheduling, retries, priorities, and monitoring
    category: Backend Infrastructure
    difficulty: intermediate
    estimated_hours: 50
    skills:
    - Message queues
    - Worker processes
    - Job scheduling
    - Retry strategies
    - Concurrency control
    - Job persistence
    prerequisites:
    - Redis basics
    - Process management
    learning_outcomes:
    - Design reliable async job processing systems
    - Implement exponential backoff and retry logic
    - Handle job failures gracefully
    - Build monitoring and observability for background jobs
    milestones:
    - name: Job Queue Core
      description: Basic job enqueueing and storage in Redis
      skills:
      - Redis lists
      - Job serialization
      - Queue operations
      deliverables:
      - Job class with JSON serialization for storing job type, arguments, and metadata
      - Enqueue operation using Redis LPUSH to atomically add jobs to the queue
      - Multiple named queues with independent priority levels for different job categories
      - Job ID generation using UUIDs or ULID to uniquely identify each enqueued job
      - Job payload validation that rejects malformed or oversized payloads before enqueuing
      - Queue inspection APIs that return queue length, peek at pending jobs, and list queue names
      hints:
        level1: |-
          Use Redis lists as queues. LPUSH to enqueue, BRPOP to dequeue:
          ```python
          class JobQueue:
              def enqueue(self, job_class, *args):
                  job = {'id': uuid4(), 'class': job_class, 'args': args}
                  redis.lpush('queue:default', json.dumps(job))
          ```
        level2: |-
          Support multiple queues with priorities:
          ```python
          QUEUES = ['critical', 'default', 'low']
          def dequeue(self):
              # BRPOP blocks until job available, checks queues in order
              queue, job = redis.brpop([f'queue:{q}' for q in QUEUES])
              return json.loads(job)
          ```
        level3: |-
          Add job metadata for tracking:
          ```python
          job = {
              'id': str(uuid4()),
              'class': job_class.__name__,
              'args': args,
              'created_at': time.time(),
              'retry_count': 0,
              'max_retries': 3
          }
          ```
      acceptance_criteria:
      - Jobs are enqueued with a JSON-serialized payload to a Redis list and are retrievable in FIFO order
      - Multiple named queues with configurable priority weights control the order workers poll them
      - Job serialization and deserialization round-trips correctly preserving all payload fields and types
      - Payloads larger than 1MB are rejected at enqueue time with a descriptive validation error
    - name: Worker Process
      description: Worker that processes jobs from queue
      skills:
      - Process management
      - Job execution
      - Error handling
      deliverables:
      - Worker main loop that polls queues using BRPOPLPUSH for reliable dequeue with backup
      - Job class registry that maps job type strings to their handler classes for dispatch
      - Job execution wrapper with configurable per-job timeout that kills long-running jobs
      - Graceful shutdown handler that catches SIGTERM and finishes the current job before exiting
      - Worker heartbeat that periodically reports liveness to Redis so dead workers can be detected
      - Concurrent job processing using a thread or process pool for parallel execution
      hints:
        level1: "Basic worker loop:\n```python\nclass Worker:\n    def run(self):\n        while self.running:\n         \
          \   job = self.queue.dequeue(timeout=5)\n            if job:\n                self.process(job)\n    \n    def process(self,\
          \ job):\n        klass = self.registry[job['class']]\n        klass().perform(*job['args'])\n```"
        level2: |-
          Handle signals for graceful shutdown:
          ```python
          def run(self):
              signal.signal(signal.SIGTERM, self.shutdown)
              while self.running:
                  job = self.queue.dequeue(timeout=5)
                  if job:
                      self.current_job = job
                      self.process(job)
                      self.current_job = None
          ```
        level3: "Use thread/process pool for concurrency:\n```python\nfrom concurrent.futures import ThreadPoolExecutor\n\n\
          class Worker:\n    def __init__(self, concurrency=5):\n        self.executor = ThreadPoolExecutor(max_workers=concurrency)\n\
          \    \n    def run(self):\n        while self.running:\n            job = self.queue.dequeue()\n            self.executor.submit(self.process,\
          \ job)\n```"
      acceptance_criteria:
      - Workers poll configured queues and dispatch jobs to the correct handler based on job type
      - Configurable concurrency allows a single worker process to execute multiple jobs in parallel
      - Graceful shutdown on SIGTERM completes the currently running job before the worker process exits
      - Worker heartbeat is updated at a regular interval and stale workers are detected by the monitoring system
    - name: Retry & Error Handling
      description: Automatic retries with exponential backoff
      skills:
      - Retry strategies
      - Dead letter queue
      - Error tracking
      deliverables:
      - Exponential backoff calculator that computes retry delays as base * 2^attempt with optional jitter
      - Retry queue implemented as a Redis sorted set with scheduled execution times as scores
      - Dead letter queue that stores permanently failed jobs after exceeding the maximum retry count
      - Error serialization that captures exception class, message, and backtrace for failed job records
      - Maximum retry limit that is configurable per job type with a global default fallback
      - Custom retry strategy hooks allowing per-job-type override of the default backoff algorithm
      hints:
        level1: |-
          Calculate exponential backoff delay:
          ```python
          def backoff_delay(retry_count):
              # 15s, 1m, 4m, 15m, 1h...
              return min(15 * (4 ** retry_count), 86400)
          ```
        level2: |-
          Use Redis sorted sets for scheduled retries:
          ```python
          def retry_later(self, job, error):
              job['retry_count'] += 1
              job['error'] = str(error)
              execute_at = time.time() + backoff_delay(job['retry_count'])
              redis.zadd('queue:retry', {json.dumps(job): execute_at})
          ```
        level3: |-
          Move to dead letter queue after max retries:
          ```python
          def handle_failure(self, job, error):
              if job['retry_count'] >= job['max_retries']:
                  redis.lpush('queue:dead', json.dumps(job))
              else:
                  self.retry_later(job, error)
          ```
      acceptance_criteria:
      - Exponential backoff retries jobs at increasing intervals (1s, 2s, 4s, 8s, ...) with optional jitter
      - Jobs that exhaust their maximum retry count are moved to the dead letter queue with full error history
      - Error details including exception class, message, and stack trace are stored with each failed job record
      - Dead letter queue jobs can be manually retried or deleted through the management API
    - name: Scheduling & Cron
      description: Schedule jobs for future execution or recurring
      skills:
      - Cron parsing
      - Scheduled jobs
      - Time handling
      deliverables:
      - Delayed job scheduling that enqueues a job for execution at a specific future timestamp
      - Recurring job definitions that specify a job class and arguments to run on a repeated schedule
      - Cron expression parser supporting standard five-field (minute, hour, day, month, weekday) expressions
      - Scheduler process that polls for due scheduled jobs and enqueues them into the appropriate queue
      - Timezone handling that evaluates cron expressions in the configured timezone rather than UTC only
      - Unique job constraints that prevent duplicate enqueue of the same scheduled job within one period
      hints:
        level1: |-
          Use sorted set for scheduled jobs:
          ```python
          def schedule(self, job_class, args, run_at):
              job = self.create_job(job_class, args)
              redis.zadd('queue:scheduled', {json.dumps(job): run_at.timestamp()})
          ```
        level2: |-
          Scheduler moves due jobs to work queue:
          ```python
          def poll_scheduled(self):
              now = time.time()
              jobs = redis.zrangebyscore('queue:scheduled', 0, now)
              for job in jobs:
                  redis.lpush('queue:default', job)
                  redis.zrem('queue:scheduled', job)
          ```
        level3: |-
          For cron jobs, calculate next run time:
          ```python
          from croniter import croniter

          def schedule_cron(self, job_class, cron_expr):
              cron = croniter(cron_expr)
              next_run = cron.get_next(datetime)
              self.schedule(job_class, [], next_run)
              # Re-schedule after execution
          ```
      acceptance_criteria:
      - Delayed jobs are enqueued for future execution and only become available to workers at the scheduled time
      - Recurring jobs with cron syntax are automatically re-enqueued according to their schedule expression
      - Unique job constraints prevent the same recurring job from being enqueued more than once per schedule period
      - Scheduler crash recovery detects missed schedule windows on restart and enqueues overdue jobs immediately
    - name: Monitoring & Dashboard
      description: Real-time monitoring and web dashboard
      skills:
      - Metrics
      - Web UI
      - Real-time updates
      deliverables:
      - Job count metrics per queue reporting pending, active, completed, and failed job totals
      - Worker status tracking showing active workers, their current job, and last heartbeat time
      - Job history log recording execution time, status, and error details for completed and failed jobs
      - Failure rate metrics calculating rolling error rates per queue and per job type
      - Web dashboard UI displaying real-time queue depths, worker status, and job history
      - Manual retry and delete controls for managing jobs in the dead letter queue from the dashboard
      hints:
        level1: |-
          Track metrics in Redis:
          ```python
          def record_processed(self, job, duration):
              redis.incr('stats:processed')
              redis.incr(f'stats:processed:{date.today()}')
              redis.lpush('stats:recent', json.dumps({
                  'job_id': job['id'], 'duration': duration
              }))
          ```
        level2: |-
          Worker heartbeat for status:
          ```python
          def heartbeat(self):
              redis.hset('workers', self.id, json.dumps({
                  'pid': os.getpid(),
                  'queues': self.queues,
                  'current_job': self.current_job,
                  'last_seen': time.time()
              }))
          ```
        level3: |-
          Web dashboard endpoint:
          ```python
          @app.get('/dashboard')
          def dashboard():
              return {
                  'queues': {q: redis.llen(f'queue:{q}') for q in QUEUES},
                  'workers': redis.hgetall('workers'),
                  'processed_today': redis.get(f'stats:processed:{date.today()}'),
                  'failed': redis.llen('queue:dead')
              }
          ```
      acceptance_criteria:
      - Dashboard tracks queue depth, processing rate, and error rate updated in near real-time
      - Web dashboard displays job counts, worker status, and recent failures in an accessible interface
      - Alerting triggers when queue backlog exceeds a threshold or error rate spikes above the configured limit
      - Job search allows filtering by job ID, status, queue name, and enqueue time range
  multi-tenant-saas:
    name: Multi-tenant SaaS Backend
    description: Build a multi-tenant architecture with tenant isolation, row-level security, and per-tenant customization
    category: Backend Architecture
    difficulty: advanced
    estimated_hours: 60
    skills:
    - Multi-tenancy patterns
    - Row-level security
    - Tenant isolation
    - Database design
    - Request context
    - Billing integration
    prerequisites:
    - REST API
    - Database design
    - Authentication
    learning_outcomes:
    - Design scalable multi-tenant architectures
    - Implement secure tenant data isolation
    - Handle tenant-specific customizations
    - Build usage-based billing systems
    milestones:
    - name: Tenant Data Model
      description: Design database schema for multi-tenancy
      skills:
      - Schema design
      - Foreign keys
      - Indexes
      deliverables:
      - Tenant table with columns for ID, name, subdomain, plan, settings JSON, and creation timestamp
      - Tenant ID foreign key column added to all application data tables for ownership tracking
      - Composite database indexes that include tenant_id as a prefix for efficient tenant-scoped queries
      - Tenant provisioning workflow that creates the tenant record and initializes default settings
      - Tenant subdomain and slug mapping table that resolves custom subdomains to tenant IDs
      - Soft delete flag on tenant data that marks records as deleted without physically removing them
      hints:
        level1: |-
          Add tenant_id to every table:
          ```sql
          CREATE TABLE users (
              id UUID PRIMARY KEY,
              tenant_id UUID NOT NULL REFERENCES tenants(id),
              email VARCHAR(255) NOT NULL,
              UNIQUE(tenant_id, email)
          );
          CREATE INDEX idx_users_tenant ON users(tenant_id);
          ```
        level2: |-
          Tenant table stores configuration:
          ```sql
          CREATE TABLE tenants (
              id UUID PRIMARY KEY,
              slug VARCHAR(63) UNIQUE NOT NULL,
              name VARCHAR(255),
              settings JSONB DEFAULT '{}',
              plan VARCHAR(50) DEFAULT 'free',
              created_at TIMESTAMP DEFAULT NOW()
          );
          ```
        level3: |-
          Use composite primary keys for better isolation:
          ```sql
          CREATE TABLE projects (
              tenant_id UUID NOT NULL,
              id UUID NOT NULL,
              name VARCHAR(255),
              PRIMARY KEY (tenant_id, id),
              FOREIGN KEY (tenant_id) REFERENCES tenants(id)
          );
          ```
      acceptance_criteria:
      - All application data tables include a tenant_id column with a foreign key to the tenant table
      - Tenant context is extracted from each request and made available throughout the request lifecycle
      - Per-tenant configuration settings are stored and retrieved for customizing tenant behavior
      - Tenant creation provisions a new record with default settings and tenant deletion soft-deletes all data
    - name: Request Context & Isolation
      description: Automatic tenant context injection in requests
      skills:
      - Middleware
      - Context management
      - ORM hooks
      deliverables:
      - Tenant resolver that identifies the tenant from request subdomain, header, or JWT claim
      - Request-scoped tenant context that stores the current tenant ID for the duration of each request
      - Automatic tenant ID injection middleware that appends tenant_id filters to all database queries
      - Tenant validation middleware that rejects requests with missing or invalid tenant identification
      - Cross-tenant access prevention guard that blocks any query attempting to access another tenant's data
      - Admin superuser bypass that allows platform operators to access data across all tenants
      hints:
        level1: |-
          Resolve tenant from subdomain:
          ```python
          @app.middleware('http')
          async def tenant_middleware(request, call_next):
              host = request.headers.get('host', '')
              subdomain = host.split('.')[0]
              tenant = await get_tenant_by_slug(subdomain)
              request.state.tenant = tenant
              return await call_next(request)
          ```
        level2: |-
          Use context variables for tenant:
          ```python
          from contextvars import ContextVar
          current_tenant: ContextVar[Tenant] = ContextVar('tenant')

          # In middleware
          current_tenant.set(tenant)

          # In any code
          tenant = current_tenant.get()
          ```
        level3: |-
          SQLAlchemy event to auto-filter:
          ```python
          @event.listens_for(Session, 'do_orm_execute')
          def add_tenant_filter(orm_execute_state):
              if orm_execute_state.is_select:
                  tenant = current_tenant.get(None)
                  if tenant:
                      orm_execute_state.statement = orm_execute_state.statement.filter_by(
                          tenant_id=tenant.id
                      )
          ```
      acceptance_criteria:
      - Tenant is extracted from the request subdomain, X-Tenant-ID header, or JWT tenant claim
      - Database queries are automatically filtered by the current tenant context without manual WHERE clauses
      - Cross-tenant data access attempts are blocked and logged as security audit events
      - Tenant context is included in all log entries for request tracing and security auditing
    - name: Row-Level Security
      description: Database-level tenant isolation with RLS
      skills:
      - PostgreSQL RLS
      - Policies
      - Session variables
      deliverables:
      - Row-level security enablement on all tenant data tables in the database
      - Tenant isolation RLS policies that restrict SELECT, INSERT, UPDATE, and DELETE to the current tenant
      - Session variable setter that configures the current tenant ID in the database connection context
      - Per-operation RLS policies for SELECT, INSERT, UPDATE, and DELETE with appropriate tenant checks
      - Migration bypass mechanism that disables RLS for schema migration scripts running as superuser
      - RLS policy test suite that verifies isolation between multiple test tenants
      hints:
        level1: |-
          Enable RLS and create policy:
          ```sql
          ALTER TABLE users ENABLE ROW LEVEL SECURITY;

          CREATE POLICY tenant_isolation ON users
              USING (tenant_id = current_setting('app.tenant_id')::uuid);
          ```
        level2: |-
          Set tenant in connection:
          ```python
          async def set_tenant_context(conn, tenant_id):
              await conn.execute(
                  f"SET app.tenant_id = '{tenant_id}'"
              )
          ```
        level3: |-
          Separate policies for operations:
          ```sql
          CREATE POLICY tenant_select ON users FOR SELECT
              USING (tenant_id = current_setting('app.tenant_id')::uuid);

          CREATE POLICY tenant_insert ON users FOR INSERT
              WITH CHECK (tenant_id = current_setting('app.tenant_id')::uuid);
          ```
      acceptance_criteria:
      - RLS policies restrict all data access to rows matching the current session's tenant ID
      - Isolation is verified by attempting cross-tenant queries which return zero rows instead of other tenant data
      - Admin and superuser roles can bypass RLS policies for platform-level operations and migrations
      - No data leakage occurs when multiple tenants query the same tables simultaneously in parallel requests
    - name: Tenant Customization
      description: Per-tenant features, branding, and configuration
      skills:
      - Feature flags
      - Configuration
      - Theming
      deliverables:
      - Tenant settings schema that defines configurable options with types, defaults, and validation rules
      - Feature flag system that enables or disables features per tenant based on their subscription plan
      - Plan-based feature access control that gates premium features behind specific subscription tiers
      - Custom branding module that applies tenant-specific logo, colors, and styling to the application
      - Tenant-specific webhook configuration that triggers callbacks for events relevant to each tenant
      - Settings API and management UI that allows tenants to view and update their configuration options
      hints:
        level1: "Store settings as JSONB:\n```python\nclass Tenant:\n    settings = Column(JSONB, default={})\n    \n    def\
          \ get_setting(self, key, default=None):\n        return self.settings.get(key, default)\n    \n    def has_feature(self,\
          \ feature):\n        return feature in self.plan_features\n```"
        level2: |-
          Plan-based feature access:
          ```python
          PLAN_FEATURES = {
              'free': ['basic_reports'],
              'pro': ['basic_reports', 'api_access', 'custom_domain'],
              'enterprise': ['basic_reports', 'api_access', 'custom_domain', 'sso', 'audit_logs']
          }

          def require_feature(feature):
              def decorator(f):
                  def wrapper(*args, **kwargs):
                      if feature not in PLAN_FEATURES[current_tenant.get().plan]:
                          raise UpgradeRequired(feature)
                      return f(*args, **kwargs)
                  return wrapper
              return decorator
          ```
        level3: |-
          Tenant-specific configuration override:
          ```python
          def get_config(key):
              tenant = current_tenant.get(None)
              if tenant and key in tenant.settings:
                  return tenant.settings[key]
              return app.config[key]  # Fall back to default
          ```
      acceptance_criteria:
      - Per-tenant feature flags enable or disable specific features based on tenant plan and configuration
      - Tenant branding applies custom logo, primary color, and accent color throughout the application UI
      - Tenant-specific integrations like webhooks and API keys are configured and stored per tenant
      - Settings management UI allows tenant administrators to view and update their configuration options
    - name: Usage Tracking & Billing
      description: Track usage metrics and integrate with billing
      skills:
      - Metering
      - Billing APIs
      - Usage limits
      deliverables:
      - Usage event tracker that records metered actions like API calls, storage bytes, and compute minutes per tenant
      - Usage aggregation pipeline that rolls up raw events into daily and monthly totals per tenant
      - Plan limits enforcer that blocks or throttles tenants exceeding their subscription plan quotas
      - Stripe billing integration that creates subscriptions, invoices, and processes payments for each tenant
      - Usage-based pricing calculator that computes charges based on metered usage beyond plan allowances
      - Overage handler that applies additional charges or restricts access when plan limits are exceeded
      hints:
        level1: |-
          Track usage events:
          ```python
          async def track_usage(tenant_id, metric, quantity=1):
              key = f'usage:{tenant_id}:{metric}:{date.today()}'
              await redis.incrby(key, quantity)
              await redis.expire(key, 86400 * 90)  # Keep 90 days
          ```
        level2: |-
          Enforce limits before operations:
          ```python
          async def check_limit(tenant, metric, requested=1):
              current = await get_usage(tenant.id, metric)
              limit = PLAN_LIMITS[tenant.plan][metric]
              if current + requested > limit:
                  raise LimitExceeded(metric, current, limit)
          ```
        level3: |-
          Sync usage to Stripe for billing:
          ```python
          async def sync_usage_to_stripe(tenant):
              usage = await get_monthly_usage(tenant.id)
              stripe.SubscriptionItem.create_usage_record(
                  tenant.stripe_subscription_item_id,
                  quantity=usage['api_calls'],
                  timestamp=int(time.time()),
                  action='set'
              )
          ```
      acceptance_criteria:
      - Usage metrics including API calls, storage, and compute are tracked and aggregated per tenant per period
      - Usage-based billing computes charges from metered events and generates accurate invoice line items
      - Invoices are generated automatically at the end of each billing period with itemized usage details
      - Quota enforcement blocks or throttles API requests when the tenant exceeds their plan limits
  vector-database:
    name: Vector Database
    description: Build a vector similarity search database with HNSW indexing for AI/ML embeddings
    category: Data Storage
    difficulty: advanced
    estimated_hours: 80
    skills:
    - Vector similarity
    - HNSW algorithm
    - Distance metrics
    - Approximate nearest neighbor
    - Memory-mapped storage
    - Index persistence
    prerequisites:
    - Data structures
    - Linear algebra basics
    learning_outcomes:
    - Understand vector similarity search algorithms
    - Implement HNSW for efficient ANN search
    - Design memory-efficient vector storage
    - Build production-ready vector search APIs
    milestones:
    - name: Vector Storage
      description: Efficient storage and retrieval of vectors
      skills:
      - Memory layout
      - Serialization
      - Memory mapping
      deliverables:
      - Fixed-dimension vector storage engine persisting float arrays with uniform dimensionality
      - Vector ID mapping index associating unique identifiers with stored vector offsets
      - Memory-mapped file storage enabling efficient access to large vector datasets on disk
      - Batch insert operations adding multiple vectors in a single transactional write
      - Vector retrieval by ID returning the full vector and associated metadata
      - Storage compaction process reclaiming space from deleted vector entries
      hints:
        level1: "Use numpy for efficient vector storage:\n```python\nclass VectorStore:\n    def __init__(self, dim):\n  \
          \      self.dim = dim\n        self.vectors = np.zeros((0, dim), dtype=np.float32)\n        self.ids = []\n    \n\
          \    def add(self, id, vector):\n        self.ids.append(id)\n        self.vectors = np.vstack([self.vectors, vector])\n\
          ```"
        level2: |-
          Memory-map for large datasets:
          ```python
          def create_mmap_storage(path, dim, capacity):
              # 4 bytes per float32
              size = capacity * dim * 4
              fp = np.memmap(path, dtype=np.float32, mode='w+', shape=(capacity, dim))
              return fp
          ```
        level3: |-
          Use struct for metadata header:
          ```python
          import struct
          HEADER_FORMAT = 'IIQ'  # dim, count, capacity

          def write_header(f, dim, count, capacity):
              f.write(struct.pack(HEADER_FORMAT, dim, count, capacity))
          ```
      acceptance_criteria:
      - Store vectors with associated metadata fields like labels or tags
      - Support multiple vector dimensions configurable at collection creation time
      - Handle vector normalization converting raw vectors to unit length on insert
      - Implement efficient storage layout with contiguous memory for fast sequential access
    - name: Distance Metrics
      description: Implement various similarity measures
      skills:
      - Cosine similarity
      - Euclidean distance
      - Dot product
      deliverables:
      - Cosine similarity computation measuring angular closeness between two vectors
      - Euclidean L2 distance computation measuring geometric distance between vectors
      - Dot product similarity computation measuring projection magnitude between vectors
      - SIMD-optimized implementations using hardware vector instructions for batch distance math
      - Batch distance computation evaluating one query against many stored vectors efficiently
      - Normalized vector handling skipping normalization when vectors are already unit-length
      hints:
        level1: |-
          Basic distance functions:
          ```python
          def cosine_similarity(a, b):
              return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

          def euclidean_distance(a, b):
              return np.linalg.norm(a - b)
          ```
        level2: |-
          Batch computation is much faster:
          ```python
          def batch_cosine(query, vectors):
              # Normalize
              query_norm = query / np.linalg.norm(query)
              vec_norms = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
              return np.dot(vec_norms, query_norm)
          ```
        level3: |-
          Pre-normalize for faster cosine:
          ```python
          class VectorStore:
              def add(self, id, vector, normalize=True):
                  if normalize:
                      vector = vector / np.linalg.norm(vector)
                  # Now cosine = dot product
          ```
      acceptance_criteria:
      - Implement cosine similarity returning values in the range of -1 to 1
      - Support Euclidean distance returning non-negative distance values
      - Support dot product similarity returning correct projection values
      - Handle metric-specific optimizations such as skipping normalization for pre-normalized vectors
    - name: Brute Force Search
      description: Exact nearest neighbor search baseline
      skills:
      - Linear scan
      - Top-K selection
      - Filtering
      deliverables:
      - K-nearest neighbors search returning the top-k closest vectors to a query
      - Threshold-based search returning all vectors within a similarity score cutoff
      - Metadata filtering restricting search results to vectors matching field predicates
      - Batch query support executing multiple search queries in a single request
      - Search with exclusions omitting specified vector IDs from result candidates
      - Performance benchmarking suite measuring query latency and throughput baselines
      hints:
        level1: |-
          Simple brute force search:
          ```python
          def search(self, query, k=10):
              distances = np.dot(self.vectors, query)  # Assuming normalized
              indices = np.argsort(distances)[-k:][::-1]
              return [(self.ids[i], distances[i]) for i in indices]
          ```
        level2: |-
          Use argpartition for efficiency:
          ```python
          def search(self, query, k=10):
              distances = np.dot(self.vectors, query)
              # argpartition is O(n) vs O(n log n) for argsort
              indices = np.argpartition(distances, -k)[-k:]
              indices = indices[np.argsort(distances[indices])[::-1]]
              return [(self.ids[i], distances[i]) for i in indices]
          ```
        level3: |-
          Add metadata filtering:
          ```python
          def search(self, query, k=10, filter_fn=None):
              distances = np.dot(self.vectors, query)
              if filter_fn:
                  mask = np.array([filter_fn(self.metadata[i]) for i in range(len(self.ids))])
                  distances = np.where(mask, distances, -np.inf)
              # ... continue with top-k
          ```
      acceptance_criteria:
      - Implement exact nearest neighbor search computing distance to every stored vector
      - Support top-k queries returning results ranked by distance
      - Handle large datasets efficiently using batched distance computation
      - Use as baseline for ANN accuracy
    - name: HNSW Index
      description: Hierarchical Navigable Small World graph for ANN
      skills:
      - Graph construction
      - Layer navigation
      - Greedy search
      deliverables:
      - Multi-layer graph structure organizing vectors in a hierarchical navigable small world
      - Node insertion with level assignment
      - Greedy search within each layer traversing edges toward nearest neighbors
      - Layer traversal strategy descending from top layer to base for query routing
      - ef_construction parameter controlling index build quality and neighbor exploration count
      - Index serialization persisting the graph structure to disk for later reload
      hints:
        level1: |-
          HNSW node structure:
          ```python
          class HNSWNode:
              def __init__(self, id, vector, level):
                  self.id = id
                  self.vector = vector
                  self.level = level
                  self.neighbors = {l: [] for l in range(level + 1)}  # neighbors per layer
          ```
        level2: |-
          Level assignment (exponential decay):
          ```python
          def random_level(self, ml=0.5):
              level = 0
              while random.random() < ml and level < self.max_level:
                  level += 1
              return level
          ```
        level3: "Greedy search in layer:\n```python\ndef search_layer(self, query, entry_point, ef, layer):\n    visited =\
          \ {entry_point.id}\n    candidates = [(-distance(query, entry_point.vector), entry_point)]\n    results = [(-distance(query,\
          \ entry_point.vector), entry_point)]\n    \n    while candidates:\n        _, current = heapq.heappop(candidates)\n\
          \        if -candidates[0][0] > -results[0][0]:\n            break\n        for neighbor in current.neighbors[layer]:\n\
          \            if neighbor.id not in visited:\n                visited.add(neighbor.id)\n                d = distance(query,\
          \ neighbor.vector)\n                if d < -results[0][0] or len(results) < ef:\n                    heapq.heappush(candidates,\
          \ (-d, neighbor))\n                    heapq.heappush(results, (-d, neighbor))\n                    if len(results)\
          \ > ef:\n                        heapq.heappop(results)\n    return results\n```"
      acceptance_criteria:
      - Implement HNSW graph construction with configurable layer count and connectivity
      - Support configurable M and ef parameters
      - Handle incremental inserts adding new vectors without full index rebuild
      - Achieve sub-linear search complexity demonstrated by benchmark comparison with brute force
    - name: Query API & Server
      description: REST/gRPC API for vector operations
      skills:
      - API design
      - Batch operations
      - Concurrent access
      deliverables:
      - Insert and upsert vectors API accepting vector data with optional metadata
      - Search API with filter support combining vector similarity and metadata predicates
      - Delete vectors endpoint removing entries by ID and reclaiming index space
      - Collection management API supporting create, list, and delete collection operations
      - Concurrent read and write handling using locking or lock-free structures for thread safety
      - Batch operations endpoint processing multiple inserts or queries in one request
      hints:
        level1: |-
          REST API endpoints:
          ```python
          @app.post('/collections/{name}/vectors')
          async def upsert(name: str, vectors: List[VectorInput]):
              collection = get_collection(name)
              for v in vectors:
                  collection.upsert(v.id, v.values, v.metadata)
              return {'upserted': len(vectors)}

          @app.post('/collections/{name}/search')
          async def search(name: str, query: SearchQuery):
              collection = get_collection(name)
              results = collection.search(query.vector, k=query.top_k)
              return {'results': results}
          ```
        level2: "Handle concurrent writes with locks:\n```python\nfrom asyncio import Lock\n\nclass Collection:\n    def __init__(self):\n\
          \        self.write_lock = Lock()\n    \n    async def upsert(self, id, vector, metadata):\n        async with self.write_lock:\n\
          \            # Rebuild HNSW node connections\n            self._insert(id, vector, metadata)\n```"
        level3: |-
          Batch for efficiency:
          ```python
          @app.post('/collections/{name}/vectors/batch')
          async def batch_upsert(name: str, vectors: List[VectorInput]):
              collection = get_collection(name)
              async with collection.write_lock:
                  for v in vectors:
                      collection._insert(v.id, v.values, v.metadata)
                  collection._rebuild_index()  # Batch rebuild
              return {'upserted': len(vectors)}
          ```
      acceptance_criteria:
      - REST or gRPC API exposes vector insert, search, and delete operations
      - Support batch queries executing multiple searches in a single round trip
      - Handle metadata filtering in search queries restricting results by field predicates
      - Implement hybrid search (vector + keyword)
  workflow-orchestrator:
    name: Workflow Orchestrator
    description: Build a DAG-based workflow orchestration system like Airflow with scheduling, dependencies, and monitoring
    category: Data Engineering
    difficulty: advanced
    estimated_hours: 70
    skills:
    - DAG scheduling
    - Task dependencies
    - State management
    - Distributed execution
    - Failure handling
    - Monitoring
    prerequisites:
    - Background jobs
    - Database
    - Process management
    learning_outcomes:
    - Design DAG-based workflow systems
    - Implement task dependency resolution
    - Handle failures and retries in pipelines
    - Build workflow monitoring and alerting
    milestones:
    - name: DAG Definition
      description: Define workflows as directed acyclic graphs
      skills:
      - Graph structures
      - DSL design
      - Validation
      deliverables:
      - Task class definition encapsulating a callable unit of work with metadata
      - DAG class with dependency tracking
      - Operator types (Python, Bash, SQL)
      - DAG validation with cycle detection rejecting invalid dependency graphs
      - Task parameters and templating supporting runtime variable substitution in task configs
      - DAG file discovery scanning a configured directory for DAG definition files
      hints:
        level1: |-
          Basic task and DAG definition:
          ```python
          class Task:
              def __init__(self, task_id, callable, dag=None):
                  self.task_id = task_id
                  self.callable = callable
                  self.upstream = []
                  self.downstream = []
                  if dag:
                      dag.add_task(self)

          class DAG:
              def __init__(self, dag_id, schedule=None):
                  self.dag_id = dag_id
                  self.schedule = schedule
                  self.tasks = {}
          ```
        level2: "Dependency operator overloading:\n```python\nclass Task:\n    def __rshift__(self, other):  # task1 >> task2\n\
          \        self.downstream.append(other)\n        other.upstream.append(self)\n        return other\n    \n    def\
          \ __lshift__(self, other):  # task1 << task2\n        self.upstream.append(other)\n        other.downstream.append(self)\n\
          \        return self\n\n# Usage: extract >> transform >> load\n```"
        level3: "Cycle detection with DFS:\n```python\ndef validate_dag(self):\n    visited = set()\n    rec_stack = set()\n\
          \    \n    def has_cycle(task_id):\n        visited.add(task_id)\n        rec_stack.add(task_id)\n        for downstream\
          \ in self.tasks[task_id].downstream:\n            if downstream.task_id not in visited:\n                if has_cycle(downstream.task_id):\n\
          \                    return True\n            elif downstream.task_id in rec_stack:\n                return True\n\
          \        rec_stack.remove(task_id)\n        return False\n    \n    for task_id in self.tasks:\n        if task_id\
          \ not in visited:\n            if has_cycle(task_id):\n                raise CycleDetected()\n```"
      acceptance_criteria:
      - Define tasks with upstream and downstream dependency declarations
      - Parse DAG from config file and construct the internal dependency graph
      - Validate DAG has no cycles using topological sort or depth-first search
      - Support task parameters passed at trigger time and accessible in task execution
    - name: Scheduler
      description: Schedule DAG runs based on cron or triggers
      skills:
      - Cron parsing
      - Run scheduling
      - Backfill
      deliverables:
      - Cron-based scheduling engine triggering DAG runs at configured cron intervals
      - Manual trigger support allowing ad-hoc DAG runs via API or CLI command
      - DAG run creation instantiating a new execution record with status tracking
      - Execution date handling assigning logical dates to each scheduled DAG run
      - Backfill support creating runs for missed intervals within a historical range
      - Catchup behavior configuration controlling whether past missed runs are auto-triggered
      hints:
        level1: |-
          Scheduler main loop:
          ```python
          class Scheduler:
              def run(self):
                  while True:
                      for dag in self.discover_dags():
                          if self.should_run(dag):
                              self.create_dag_run(dag)
                      self.dispatch_ready_tasks()
                      time.sleep(self.heartbeat_interval)
          ```
        level2: |-
          Calculate next run time:
          ```python
          from croniter import croniter

          def get_next_run(self, dag):
              last_run = self.get_last_run(dag.dag_id)
              if last_run:
                  cron = croniter(dag.schedule, last_run.execution_date)
              else:
                  cron = croniter(dag.schedule, dag.start_date)
              return cron.get_next(datetime)
          ```
        level3: |-
          Backfill past runs:
          ```python
          def backfill(self, dag, start_date, end_date):
              cron = croniter(dag.schedule, start_date)
              while True:
                  execution_date = cron.get_next(datetime)
                  if execution_date > end_date:
                      break
                  if not self.run_exists(dag.dag_id, execution_date):
                      self.create_dag_run(dag, execution_date)
          ```
      acceptance_criteria:
      - Trigger DAG runs based on schedule
      - Support cron schedules with standard five-field cron expression syntax
      - Handle manual triggers creating a DAG run with the current time as execution date
      - Track scheduled vs actual run time
    - name: Task Execution
      description: Execute tasks with dependency resolution
      skills:
      - Topological sort
      - Parallel execution
      - State management
      deliverables:
      - Task instance state machine tracking transitions from queued to running to success or failure
      - Dependency checking gate ensuring upstream tasks completed before downstream execution
      - Parallel task execution running independent tasks concurrently up to a configured limit
      - Task queuing dispatching ready task instances to the executor for processing
      - XCom cross-task communication passing small data payloads between task instances
      - Task timeout handling killing tasks that exceed their configured maximum runtime
      hints:
        level1: |-
          Task states:
          ```python
          class TaskState(Enum):
              PENDING = 'pending'
              QUEUED = 'queued'
              RUNNING = 'running'
              SUCCESS = 'success'
              FAILED = 'failed'
              SKIPPED = 'skipped'
              UP_FOR_RETRY = 'up_for_retry'
          ```
        level2: "Check if task is ready to run:\n```python\ndef is_ready(self, task_instance):\n    for upstream in task_instance.task.upstream:\n\
          \        upstream_ti = self.get_task_instance(\n            upstream.task_id, \n            task_instance.dag_run_id\n\
          \        )\n        if upstream_ti.state != TaskState.SUCCESS:\n            return False\n    return True\n```"
        level3: "XCom for inter-task communication:\n```python\nclass XCom:\n    @staticmethod\n    def push(task_id, dag_run_id,\
          \ key, value):\n        db.xcom.insert({\n            'task_id': task_id,\n            'dag_run_id': dag_run_id,\n\
          \            'key': key,\n            'value': pickle.dumps(value)\n        })\n    \n    @staticmethod\n    def\
          \ pull(task_id, dag_run_id, key):\n        row = db.xcom.find_one(task_id=task_id, dag_run_id=dag_run_id, key=key)\n\
          \        return pickle.loads(row['value'])\n```"
      acceptance_criteria:
      - Execute ready tasks only after all upstream dependencies have succeeded
      - Handle task success and failure by updating state and triggering downstream logic
      - Support task retries with configurable retry count and delay between attempts
      - Implement task timeout killing execution that exceeds the configured duration limit
    - name: Worker & Executor
      description: Distributed task execution across workers
      skills:
      - Worker processes
      - Task distribution
      - Resource management
      deliverables:
      - Local executor running tasks sequentially in the scheduler process
      - Celery or Redis executor distributing tasks to remote worker processes via message queue
      - Worker heartbeat mechanism reporting liveness at regular intervals to the scheduler
      - Task result collection gathering output and status from completed worker tasks
      - Worker pool management maintaining a set of concurrent worker processes or threads
      - Resource slot management limiting concurrent task execution to available capacity
      hints:
        level1: "Base executor interface:\n```python\nclass BaseExecutor:\n    def execute(self, task_instance):\n       \
          \ raise NotImplementedError\n    \n    def get_result(self, task_instance):\n        raise NotImplementedError\n\
          \nclass LocalExecutor(BaseExecutor):\n    def execute(self, ti):\n        try:\n            result = ti.task.callable()\n\
          \            ti.state = TaskState.SUCCESS\n            return result\n        except Exception as e:\n         \
          \   ti.state = TaskState.FAILED\n            raise\n```"
        level2: |-
          Celery executor:
          ```python
          from celery import Celery

          app = Celery('workflow', broker='redis://localhost')

          @app.task
          def execute_task(task_id, dag_run_id):
              ti = TaskInstance.get(task_id, dag_run_id)
              return ti.task.callable()

          class CeleryExecutor(BaseExecutor):
              def execute(self, ti):
                  result = execute_task.delay(ti.task_id, ti.dag_run_id)
                  ti.celery_task_id = result.id
          ```
        level3: "Parallelism control:\n```python\nclass Executor:\n    def __init__(self, parallelism=16):\n        self.parallelism\
          \ = parallelism\n        self.running_tasks = {}\n    \n    def has_slot(self):\n        return len(self.running_tasks)\
          \ < self.parallelism\n    \n    def dispatch(self, ti):\n        if not self.has_slot():\n            return False\n\
          \        self.running_tasks[ti.key] = self.execute_async(ti)\n        return True\n```"
      acceptance_criteria:
      - Distribute tasks to workers via the configured executor backend
      - Support different executors including local sequential and distributed container modes
      - Handle worker failures by detecting missing heartbeats and requeueing affected tasks
      - Track task assignments mapping each running task to its assigned worker node
    - name: Web UI & Monitoring
      description: Dashboard for workflow monitoring and management
      skills:
      - Web UI
      - Real-time updates
      - Logging
      deliverables:
      - DAG list view displaying all registered DAGs with schedule and status summary
      - DAG graph visualization rendering task dependencies as a directed acyclic graph diagram
      - Task logs viewing interface displaying stdout and stderr output for each task instance
      - Manual trigger and clear controls allowing users to start runs or reset task states
      - Run history timeline showing past DAG runs with duration and outcome status
      - Alerting on failures sending notifications via email or webhook when tasks fail
      hints:
        level1: |-
          API endpoints:
          ```python
          @app.get('/api/dags')
          def list_dags():
              return [{'dag_id': d.dag_id, 'schedule': d.schedule} for d in discover_dags()]

          @app.get('/api/dags/{dag_id}/runs')
          def get_runs(dag_id: str):
              return db.dag_runs.find(dag_id=dag_id).order_by('-execution_date')
          ```
        level2: |-
          Graph data for visualization:
          ```python
          @app.get('/api/dags/{dag_id}/graph')
          def get_graph(dag_id: str):
              dag = get_dag(dag_id)
              return {
                  'nodes': [{'id': t.task_id} for t in dag.tasks.values()],
                  'edges': [
                      {'source': t.task_id, 'target': d.task_id}
                      for t in dag.tasks.values()
                      for d in t.downstream
                  ]
              }
          ```
        level3: |-
          Stream logs:
          ```python
          @app.get('/api/tasks/{task_id}/logs')
          async def stream_logs(task_id: str, dag_run_id: str):
              async def generate():
                  log_file = get_log_path(task_id, dag_run_id)
                  async with aiofiles.open(log_file) as f:
                      while True:
                          line = await f.readline()
                          if line:
                              yield f'data: {line}\n\n'
                          else:
                              await asyncio.sleep(0.5)
              return StreamingResponse(generate(), media_type='text/event-stream')
          ```
      acceptance_criteria:
      - Display DAG graph with nodes colored by task execution status
      - Show run history listing past executions with start time, duration, and result
      - View task logs displaying captured output for debugging failed task instances
      - Trigger manual DAG runs from the web interface with optional parameter overrides
  kubernetes-operator:
    name: Kubernetes Operator
    description: Build a Kubernetes operator with custom resources (CRDs) for automated application management
    category: Cloud Native
    difficulty: advanced
    estimated_hours: 60
    skills:
    - Kubernetes API
    - Custom Resources
    - Controller pattern
    - Reconciliation loop
    - Leader election
    - Webhook validation
    prerequisites:
    - Kubernetes basics
    - Go or Python
    learning_outcomes:
    - Understand Kubernetes controller patterns
    - Implement CRDs and controllers
    - Handle reconciliation and state management
    - Deploy and operate custom operators
    milestones:
    - name: Custom Resource Definition
      description: Define custom resources for your domain
      skills:
      - CRD schema
      - OpenAPI validation
      - Versioning
      deliverables:
      - CRD YAML manifest with OpenAPI v3 schema validation for spec fields
      - Status subresource definition enabling independent spec and status updates
      - Printer columns configuration that displays key fields in kubectl get output
      - Validation rules using CEL expressions to enforce field constraints beyond schema
      - Default values specification that auto-populates omitted optional fields
      - Multiple CRD versions with conversion webhooks for version migration
      hints:
        level1: |-
          Basic CRD structure:
          ```yaml
          apiVersion: apiextensions.k8s.io/v1
          kind: CustomResourceDefinition
          metadata:
            name: myapps.example.com
          spec:
            group: example.com
            names:
              kind: MyApp
              plural: myapps
            scope: Namespaced
            versions:
            - name: v1
              served: true
              storage: true
              schema:
                openAPIV3Schema:
                  type: object
                  properties:
                    spec:
                      type: object
                      properties:
                        replicas:
                          type: integer
          ```
        level2: |-
          Add status subresource:
          ```yaml
          versions:
          - name: v1
            subresources:
              status: {}
            schema:
              openAPIV3Schema:
                properties:
                  status:
                    type: object
                    properties:
                      availableReplicas:
                        type: integer
                      conditions:
                        type: array
                        items:
                          type: object
          ```
        level3: |-
          Printer columns for kubectl:
          ```yaml
          additionalPrinterColumns:
          - name: Replicas
            type: integer
            jsonPath: .spec.replicas
          - name: Available
            type: integer
            jsonPath: .status.availableReplicas
          - name: Age
            type: date
            jsonPath: .metadata.creationTimestamp
          ```
      acceptance_criteria:
      - CRD schema with OpenAPI validation rejects resources with missing required fields or wrong types
      - Spec and status subresources are independently updatable without overwriting each other
      - Custom printer columns display relevant fields like status and age in kubectl get output
      - CRD versioning supports multiple API versions with a storage version and conversion strategy
    - name: Controller Setup
      description: Set up controller with client and informers
      skills:
      - Client-go
      - Informers
      - Work queue
      deliverables:
      - Kubernetes client configuration that authenticates via in-cluster or kubeconfig credentials
      - Shared informer for the custom resource that caches and watches resource events efficiently
      - Rate-limited work queue that buffers reconcile requests with exponential backoff on failure
      - Event handler functions that enqueue add, update, and delete events into the work queue
      - Controller struct that holds references to the client, informer, queue, and recorder
      - Run loop that starts informer sync and launches worker goroutines to process the queue
      hints:
        level1: |-
          Controller structure (Go):
          ```go
          type Controller struct {
              clientset    kubernetes.Interface
              myappLister  listers.MyAppLister
              myappsSynced cache.InformerSynced
              workqueue    workqueue.RateLimitingInterface
          }
          ```
        level2: |-
          Add event handlers:
          ```go
          myappInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
              AddFunc: func(obj interface{}) {
                  key, _ := cache.MetaNamespaceKeyFunc(obj)
                  c.workqueue.Add(key)
              },
              UpdateFunc: func(old, new interface{}) {
                  key, _ := cache.MetaNamespaceKeyFunc(new)
                  c.workqueue.Add(key)
              },
              DeleteFunc: func(obj interface{}) {
                  key, _ := cache.DeletionHandlingMetaNamespaceKeyFunc(obj)
                  c.workqueue.Add(key)
              },
          })
          ```
        level3: |-
          Run workers:
          ```go
          func (c *Controller) Run(workers int, stopCh <-chan struct{}) error {
              if !cache.WaitForCacheSync(stopCh, c.myappsSynced) {
                  return fmt.Errorf("cache sync failed")
              }
              for i := 0; i < workers; i++ {
                  go wait.Until(c.runWorker, time.Second, stopCh)
              }
              <-stopCh
              return nil
          }
          ```
      acceptance_criteria:
      - Controller watches custom resource events and enqueues reconcile requests for each change
      - Informers provide a local cache that reduces API server load for list and watch operations
      - Connection errors and API server restarts trigger automatic reconnection with backoff
      - Leader election ensures only one controller replica actively reconciles at any time
    - name: Reconciliation Loop
      description: Implement the core reconciliation logic
      skills:
      - Desired vs actual state
      - Idempotency
      - Error handling
      deliverables:
      - Current state fetcher that reads the live state of owned resources from the API server
      - Desired state comparator that diffs the spec against the actual state of owned resources
      - Resource mutator that creates, updates, or deletes owned resources to match the desired state
      - Status updater that writes the current reconciliation state back to the custom resource status
      - Requeue-on-error handler that requeues the resource with exponential backoff after failures
      - Idempotent operation design ensuring repeated reconciliation produces the same result
      hints:
        level1: "Basic reconcile function:\n```go\nfunc (c *Controller) reconcile(key string) error {\n    namespace, name,\
          \ _ := cache.SplitMetaNamespaceKey(key)\n    \n    myapp, err := c.myappLister.MyApps(namespace).Get(name)\n   \
          \ if errors.IsNotFound(err) {\n        return nil  // Deleted\n    }\n    \n    // Reconcile deployment\n    deployment\
          \ := newDeployment(myapp)\n    _, err = c.clientset.AppsV1().Deployments(namespace).Create(ctx, deployment, metav1.CreateOptions{})\n\
          \    if errors.IsAlreadyExists(err) {\n        _, err = c.clientset.AppsV1().Deployments(namespace).Update(ctx,\
          \ deployment, metav1.UpdateOptions{})\n    }\n    return err\n}\n```"
        level2: |-
          Update status:
          ```go
          func (c *Controller) updateStatus(myapp *v1.MyApp, deployment *appsv1.Deployment) error {
              myappCopy := myapp.DeepCopy()
              myappCopy.Status.AvailableReplicas = deployment.Status.AvailableReplicas
              myappCopy.Status.Conditions = append(myappCopy.Status.Conditions, v1.Condition{
                  Type:               "Available",
                  Status:             "True",
                  LastTransitionTime: metav1.Now(),
              })
              _, err := c.myappclient.MyApps(myapp.Namespace).UpdateStatus(ctx, myappCopy, metav1.UpdateOptions{})
              return err
          }
          ```
        level3: |-
          Owner references for garbage collection:
          ```go
          func newDeployment(myapp *v1.MyApp) *appsv1.Deployment {
              return &appsv1.Deployment{
                  ObjectMeta: metav1.ObjectMeta{
                      Name:      myapp.Name,
                      Namespace: myapp.Namespace,
                      OwnerReferences: []metav1.OwnerReference{
                          *metav1.NewControllerRef(myapp, v1.SchemeGroupVersion.WithKind("MyApp")),
                      },
                  },
                  // ...
              }
          }
          ```
      acceptance_criteria:
      - Reconciler compares the desired spec state with the actual state of all owned resources
      - Owned resources are created, updated, or deleted to converge toward the desired state
      - Status subresource is updated with current conditions, ready replicas, and last reconciliation time
      - Reconciliation errors trigger automatic requeue with exponential backoff and jitter
    - name: Webhooks
      description: Admission webhooks for validation and mutation
      skills:
      - Validating webhook
      - Mutating webhook
      - TLS
      deliverables:
      - HTTPS webhook server that listens for admission review requests from the API server
      - Validating admission webhook that rejects resources failing custom business rule checks
      - Mutating admission webhook that injects default values into newly created resources
      - TLS certificate setup using cert-manager or self-signed certificates for webhook HTTPS
      - Webhook configuration manifests that register the webhooks with the API server
      - Error response handler that returns structured admission review rejection messages
      hints:
        level1: "Webhook handler:\n```go\nfunc (h *Handler) validate(ar *admissionv1.AdmissionReview) *admissionv1.AdmissionResponse\
          \ {\n    myapp := &v1.MyApp{}\n    json.Unmarshal(ar.Request.Object.Raw, myapp)\n    \n    if myapp.Spec.Replicas\
          \ < 1 {\n        return &admissionv1.AdmissionResponse{\n            Allowed: false,\n            Result: &metav1.Status{\n\
          \                Message: \"replicas must be >= 1\",\n            },\n        }\n    }\n    return &admissionv1.AdmissionResponse{Allowed:\
          \ true}\n}\n```"
        level2: "Mutating webhook for defaults:\n```go\nfunc (h *Handler) mutate(ar *admissionv1.AdmissionReview) *admissionv1.AdmissionResponse\
          \ {\n    myapp := &v1.MyApp{}\n    json.Unmarshal(ar.Request.Object.Raw, myapp)\n    \n    patches := []jsonPatch{}\n\
          \    if myapp.Spec.Replicas == 0 {\n        patches = append(patches, jsonPatch{\n            Op:    \"add\",\n\
          \            Path:  \"/spec/replicas\",\n            Value: 1,\n        })\n    }\n    \n    patchBytes, _ := json.Marshal(patches)\n\
          \    return &admissionv1.AdmissionResponse{\n        Allowed: true,\n        Patch:   patchBytes,\n        PatchType:\
          \ func() *admissionv1.PatchType {\n            pt := admissionv1.PatchTypeJSONPatch\n            return &pt\n  \
          \      }(),\n    }\n}\n```"
        level3: |-
          Webhook configuration:
          ```yaml
          apiVersion: admissionregistration.k8s.io/v1
          kind: ValidatingWebhookConfiguration
          metadata:
            name: myapp-validation
          webhooks:
          - name: validate.myapp.example.com
            rules:
            - apiGroups: ["example.com"]
              resources: ["myapps"]
              operations: ["CREATE", "UPDATE"]
            clientConfig:
              service:
                name: myapp-operator
                namespace: default
                path: /validate
              caBundle: ${CA_BUNDLE}
          ```
      acceptance_criteria:
      - Validating webhook rejects resources with invalid field combinations and returns clear error messages
      - Mutating webhook injects default values into resource spec when optional fields are omitted
      - Webhook TLS certificates are provisioned and rotated automatically by cert-manager
      - Admission review responses include human-readable reason strings for validation failures
    - name: Testing & Deployment
      description: Test and deploy the operator
      skills:
      - Envtest
      - Helm charts
      - RBAC
      deliverables:
      - Unit tests using a fake Kubernetes client that verify reconciliation logic without a cluster
      - Integration tests using envtest that run the controller against a real API server in-process
      - RBAC role and role-binding manifests scoped to the minimum required permissions
      - Deployment manifests including the controller Deployment, ServiceAccount, and Namespace
      - Helm chart with configurable values for image, replicas, resources, and RBAC settings
      - Leader election configuration that prevents multiple controller replicas from reconciling simultaneously
      hints:
        level1: "Test with fake client:\n```go\nfunc TestReconcile(t *testing.T) {\n    myapp := &v1.MyApp{\n        ObjectMeta:\
          \ metav1.ObjectMeta{Name: \"test\", Namespace: \"default\"},\n        Spec:       v1.MyAppSpec{Replicas: 3},\n \
          \   }\n    \n    client := fake.NewSimpleClientset(myapp)\n    controller := NewController(client)\n    \n    err\
          \ := controller.reconcile(\"default/test\")\n    assert.NoError(t, err)\n}\n```"
        level2: |-
          RBAC for operator:
          ```yaml
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: myapp-operator
          rules:
          - apiGroups: ["example.com"]
            resources: ["myapps", "myapps/status"]
            verbs: ["get", "list", "watch", "update", "patch"]
          - apiGroups: ["apps"]
            resources: ["deployments"]
            verbs: ["get", "list", "watch", "create", "update", "delete"]
          ```
        level3: |-
          Leader election:
          ```go
          lock := &resourcelock.LeaseLock{
              LeaseMeta: metav1.ObjectMeta{Name: "myapp-operator", Namespace: "default"},
              Client:    clientset.CoordinationV1(),
              LockConfig: resourcelock.ResourceLockConfig{Identity: hostname},
          }

          leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
              Lock:          lock,
              LeaseDuration: 15 * time.Second,
              RenewDeadline: 10 * time.Second,
              RetryPeriod:   2 * time.Second,
              Callbacks: leaderelection.LeaderCallbacks{
                  OnStartedLeading: func(ctx context.Context) {
                      controller.Run(2, ctx.Done())
                  },
              },
          })
          ```
      acceptance_criteria:
      - Unit tests with fake client cover create, update, delete, and error reconciliation paths
      - Integration tests with envtest verify end-to-end reconciliation against a real API server
      - Container image is built and tagged with the operator binary and all required dependencies
      - Deployment manifests apply RBAC permissions following the principle of least privilege
  ml-model-serving:
    name: ML Model Serving API
    description: Build a production ML model serving system with batching, versioning, A/B testing, and monitoring
    category: AI/ML Infrastructure
    difficulty: intermediate
    estimated_hours: 45
    skills:
    - Model loading
    - Request batching
    - Model versioning
    - A/B testing
    - Latency optimization
    - Model monitoring
    prerequisites:
    - REST API
    - Basic ML
    - Docker
    learning_outcomes:
    - Design scalable model serving architectures
    - Implement efficient batching for throughput
    - Handle model versioning and rollback
    - Monitor model performance in production
    milestones:
    - name: Model Loading & Inference
      description: Load models and serve predictions
      skills:
      - Model formats
      - Memory management
      - Inference
      deliverables:
      - Model loader that imports models from PyTorch, TensorFlow SavedModel, and ONNX file formats
      - Prediction HTTP endpoint that accepts input data and returns model inference results as JSON
      - Input validator that checks request payloads against the model's expected input schema and dimensions
      - Output formatter that converts raw model tensors into structured JSON response payloads
      - GPU and CPU device handler that places models on the appropriate compute device based on availability
      - Warm-up request runner that sends initial inference requests at startup to pre-compile model graphs
      hints:
        level1: |-
          Basic model server:
          ```python
          from fastapi import FastAPI
          import torch

          app = FastAPI()
          model = None

          @app.on_event('startup')
          async def load_model():
              global model
              model = torch.load('model.pt')
              model.eval()

          @app.post('/predict')
          async def predict(data: PredictRequest):
              with torch.no_grad():
                  input_tensor = preprocess(data.inputs)
                  output = model(input_tensor)
                  return {'predictions': output.tolist()}
          ```
        level2: |-
          Support multiple model formats:
          ```python
          class ModelLoader:
              @staticmethod
              def load(path: str):
                  if path.endswith('.pt'):
                      return torch.load(path)
                  elif path.endswith('.onnx'):
                      import onnxruntime
                      return onnxruntime.InferenceSession(path)
                  elif path.endswith('.pkl'):
                      import joblib
                      return joblib.load(path)
          ```
        level3: |-
          Warm-up to avoid cold start:
          ```python
          @app.on_event('startup')
          async def warmup():
              global model
              model = load_model(MODEL_PATH)
              # Run dummy inference to warm up
              dummy_input = torch.zeros((1, *INPUT_SHAPE))
              for _ in range(3):
                  model(dummy_input)
              logger.info('Model warmed up')
          ```
      acceptance_criteria:
      - Models from PyTorch, TensorFlow, and ONNX frameworks are loaded and ready for inference at startup
      - Inference endpoint validates input shape and dtype before running the model and returns structured results
      - Model warmup sends configurable dummy requests at startup to eliminate cold-start inference latency
      - GPU inference is used when a CUDA device is available and falls back to CPU when it is not
    - name: Request Batching
      description: Batch requests for better throughput
      skills:
      - Dynamic batching
      - Async processing
      - Timeout handling
      deliverables:
      - Request queue that buffers incoming inference requests for batch formation before model execution
      - Dynamic batch former that groups queued requests into batches up to a configurable maximum size
      - Batch size limiter that caps the number of requests per batch to avoid GPU out-of-memory errors
      - Timeout handler that flushes a partial batch for execution when the maximum wait time is reached
      - Response router that splits batch inference results and returns each result to its original requester
      - Throughput metrics collector that tracks requests per second, batch sizes, and queue wait times
      hints:
        level1: "Simple batching with queue:\n```python\nimport asyncio\nfrom collections import deque\n\nclass Batcher:\n\
          \    def __init__(self, max_batch=32, max_wait=0.01):\n        self.queue = deque()\n        self.max_batch = max_batch\n\
          \        self.max_wait = max_wait\n    \n    async def add(self, request):\n        future = asyncio.Future()\n\
          \        self.queue.append((request, future))\n        return await future\n```"
        level2: "Batch processor:\n```python\nasync def process_batches(self):\n    while True:\n        batch = []\n    \
          \    deadline = time.time() + self.max_wait\n        \n        while len(batch) < self.max_batch and time.time()\
          \ < deadline:\n            if self.queue:\n                batch.append(self.queue.popleft())\n            else:\n\
          \                await asyncio.sleep(0.001)\n        \n        if batch:\n            requests = [r for r, _ in\
          \ batch]\n            futures = [f for _, f in batch]\n            results = await self.inference(requests)\n  \
          \          for future, result in zip(futures, results):\n                future.set_result(result)\n```"
        level3: "Adaptive batching based on load:\n```python\nclass AdaptiveBatcher:\n    def __init__(self):\n        self.current_qps\
          \ = 0\n        self.target_latency = 0.1\n    \n    def get_optimal_batch_size(self):\n        # Larger batches\
          \ for higher load\n        if self.current_qps > 1000:\n            return 64\n        elif self.current_qps > 100:\n\
          \            return 32\n        return 8\n```"
      acceptance_criteria:
      - Incoming requests are batched together to improve GPU utilization and inference throughput
      - Maximum batch size and maximum wait timeout are configurable to balance latency and throughput
      - Partial batches are flushed and executed when the timeout expires even if the batch is not full
      - Batching metrics including average batch size, queue depth, and wait time are exposed for monitoring
    - name: Model Versioning
      description: Manage multiple model versions
      skills:
      - Version management
      - Hot reload
      - Rollback
      deliverables:
      - Model registry that stores and catalogs multiple models with their version numbers and metadata
      - Version metadata store that records model accuracy, training date, and framework for each version
      - Hot model reload mechanism that swaps a running model to a new version without server restart
      - Rollback mechanism that reverts to the previous model version if the new version performs poorly
      - Default version selector that routes requests to the latest stable version when no version is specified
      - Version-specific endpoint router that directs requests to a particular model version by URL path or header
      hints:
        level1: "Model registry:\n```python\nclass ModelRegistry:\n    def __init__(self, storage_path):\n        self.storage_path\
          \ = storage_path\n        self.models = {}  # {model_name: {version: model}}\n        self.default_versions = {}\n\
          \    \n    def load_version(self, name, version):\n        path = f'{self.storage_path}/{name}/{version}/model.pt'\n\
          \        self.models.setdefault(name, {})[version] = load_model(path)\n    \n    def get(self, name, version=None):\n\
          \        version = version or self.default_versions.get(name)\n        return self.models[name][version]\n```"
        level2: "Hot reload without downtime:\n```python\nasync def reload_model(self, name, version):\n    # Load new model\
          \ in background\n    new_model = await asyncio.to_thread(load_model, path)\n    \n    # Atomic swap\n    old_model\
          \ = self.models[name].get(version)\n    self.models[name][version] = new_model\n    \n    # Cleanup old model\n\
          \    if old_model:\n        del old_model\n        torch.cuda.empty_cache()\n```"
        level3: |-
          Version in request:
          ```python
          @app.post('/v1/models/{model_name}/predict')
          async def predict(
              model_name: str,
              request: PredictRequest,
              version: Optional[str] = Query(None)
          ):
              model = registry.get(model_name, version)
              return model.predict(request.inputs)
          ```
      acceptance_criteria:
      - Multiple versions of the same model are stored in the registry and available for inference simultaneously
      - Version-specific inference routes requests to the exact model version specified in the request
      - Hot model swapping loads a new version into memory and switches traffic without dropping requests
      - Version metadata including accuracy, training date, and data hash is tracked and queryable
    - name: A/B Testing & Canary
      description: Traffic splitting for model comparison
      skills:
      - Traffic routing
      - Experiment tracking
      - Statistical analysis
      deliverables:
      - Traffic split configuration that defines percentage weights for routing requests between model versions
      - Consistent user routing that hashes a user identifier to always send the same user to the same model version
      - Experiment metrics tracker that records per-version prediction outcomes and business metrics
      - Statistical significance calculator that determines when enough data has been collected to declare a winner
      - Gradual rollout controller that incrementally increases traffic to a new version over a configured schedule
      - Automatic rollback trigger that reverts traffic to the previous version if error rates exceed a threshold
      hints:
        level1: "Simple traffic split:\n```python\nimport random\n\nclass ABRouter:\n    def __init__(self):\n        self.experiments\
          \ = {}  # {name: {version: weight}}\n    \n    def route(self, model_name, user_id=None):\n        weights = self.experiments.get(model_name,\
          \ {'default': 1.0})\n        if user_id:\n            # Consistent routing based on user\n            random.seed(hash(f'{model_name}:{user_id}'))\n\
          \        return random.choices(\n            list(weights.keys()),\n            weights=list(weights.values())\n\
          \        )[0]\n```"
        level2: |-
          Track experiment metrics:
          ```python
          class ExperimentTracker:
              async def record(self, experiment, version, metrics):
                  await redis.hincrby(f'exp:{experiment}:{version}', 'count', 1)
                  await redis.hincrbyfloat(f'exp:{experiment}:{version}', 'latency_sum', metrics['latency'])
                  if metrics.get('feedback'):
                      await redis.hincrby(f'exp:{experiment}:{version}', 'positive', 1 if metrics['feedback'] > 0 else 0)
          ```
        level3: |-
          Auto-rollback on degradation:
          ```python
          async def check_experiment_health(self, experiment):
              for version, stats in self.get_stats(experiment).items():
                  error_rate = stats['errors'] / stats['count']
                  if error_rate > 0.05:  # 5% error threshold
                      await self.rollback(experiment, version)
                      await self.alert(f'Rolled back {experiment}:{version}')
          ```
      acceptance_criteria:
      - Traffic is split between model versions according to configured percentage weights with minimal skew
      - Traffic percentage weights are configurable per experiment and can be adjusted without server restart
      - Per-version metrics including latency, error rate, and prediction distribution are tracked and compared
      - Gradual rollout increases traffic to the new version in configurable increments over time
    - name: Monitoring & Observability
      description: Monitor model performance and drift
      skills:
      - Metrics
      - Logging
      - Drift detection
      deliverables:
      - Latency metrics tracker recording p50, p90, p95, and p99 inference times per model and version
      - Throughput metrics tracker recording requests per second and successful versus failed inference counts
      - Input and output logger that samples and stores request payloads and model predictions for debugging
      - Data drift detector that monitors input feature distributions for significant shifts from the training baseline
      - Model accuracy monitor that compares predictions against delayed ground truth labels when available
      - Alerting rule engine that fires notifications when latency, error rate, or drift metrics exceed thresholds
      hints:
        level1: |-
          Prometheus metrics:
          ```python
          from prometheus_client import Histogram, Counter

          REQUEST_LATENCY = Histogram(
              'model_request_latency_seconds',
              'Model inference latency',
              ['model', 'version']
          )
          REQUEST_COUNT = Counter(
              'model_request_total',
              'Total requests',
              ['model', 'version', 'status']
          )

          @app.middleware('http')
          async def metrics_middleware(request, call_next):
              start = time.time()
              response = await call_next(request)
              REQUEST_LATENCY.labels(model=model_name, version=version).observe(time.time() - start)
              return response
          ```
        level2: |-
          Log predictions for analysis:
          ```python
          async def log_prediction(self, request_id, model, version, input_data, output, latency):
              await kafka.send('predictions', {
                  'request_id': request_id,
                  'model': model,
                  'version': version,
                  'input_hash': hash_input(input_data),
                  'output': output,
                  'latency': latency,
                  'timestamp': datetime.utcnow().isoformat()
              })
          ```
        level3: "Simple drift detection:\n```python\nclass DriftDetector:\n    def __init__(self, reference_stats):\n    \
          \    self.reference = reference_stats\n    \n    def check(self, recent_inputs):\n        current_mean = np.mean(recent_inputs,\
          \ axis=0)\n        current_std = np.std(recent_inputs, axis=0)\n        \n        # PSI (Population Stability Index)\n\
          \        psi = np.sum(\n            (current_mean - self.reference['mean']) * \n            np.log(current_mean\
          \ / self.reference['mean'])\n        )\n        \n        if psi > 0.2:\n            return DriftAlert(psi=psi)\n\
          ```"
      acceptance_criteria:
      - Inference latency percentiles p50, p90, p95, and p99 are tracked and exposed as metrics per model version
      - Prediction output distribution is monitored to detect shifts in model behavior over time
      - Data drift is detected by comparing live input feature distributions against the training data baseline
      - Alerts fire when monitored metrics exceed configured thresholds and are delivered via configured channels
  llm-finetuning-pipeline:
    name: LLM Fine-tuning Pipeline
    description: Build an end-to-end LLM fine-tuning system with LoRA/QLoRA, dataset preparation, and evaluation
    category: AI/ML
    difficulty: advanced
    estimated_hours: 55
    skills:
    - LoRA/QLoRA
    - Dataset preparation
    - Training loop
    - Evaluation metrics
    - Model merging
    - Quantization
    prerequisites:
    - PyTorch basics
    - Transformers library
    - GPU training
    learning_outcomes:
    - Understand parameter-efficient fine-tuning methods
    - Prepare and format datasets for instruction tuning
    - Implement training with memory optimization
    - Evaluate and deploy fine-tuned models
    milestones:
    - name: Dataset Preparation
      description: Prepare and format training data
      skills:
      - Data formatting
      - Tokenization
      - Chat templates
      deliverables:
      - Data loader that reads training data from JSON, JSONL, CSV, and Parquet formats
      - Instruction-response formatter that converts raw data into the model's expected prompt template
      - Chat template applicator that wraps instruction-response pairs in the model's chat format tokens
      - Tokenizer pipeline that encodes text with proper padding, truncation, and attention masks
      - Train-validation splitter that partitions data into training and validation sets with stratification
      - Data quality filter that removes duplicates, short entries, and samples exceeding max token length
      hints:
        level1: |-
          Format for instruction tuning:
          ```python
          def format_instruction(example):
              return f"""### Instruction:
          {example['instruction']}

          ### Input:
          {example.get('input', '')}

          ### Response:
          {example['output']}"""

          dataset = dataset.map(lambda x: {'text': format_instruction(x)})
          ```
        level2: |-
          Use chat template:
          ```python
          from transformers import AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')

          def format_chat(example):
              messages = [
                  {'role': 'user', 'content': example['instruction']},
                  {'role': 'assistant', 'content': example['output']}
              ]
              return tokenizer.apply_chat_template(messages, tokenize=False)
          ```
        level3: |-
          Tokenize with labels:
          ```python
          def tokenize(example):
              result = tokenizer(
                  example['text'],
                  truncation=True,
                  max_length=2048,
                  padding='max_length'
              )
              result['labels'] = result['input_ids'].copy()
              # Mask instruction part in labels
              response_start = example['text'].find('### Response:')
              response_token_start = len(tokenizer(example['text'][:response_start])['input_ids'])
              result['labels'][:response_token_start] = [-100] * response_token_start
              return result
          ```
      acceptance_criteria:
      - Training data is loaded and validated for required fields like instruction and response
      - Raw data is converted to the instruction-response format expected by the fine-tuning trainer
      - Data is split into training and validation sets with configurable ratios and optional stratification
      - Data augmentation techniques like paraphrasing or back-translation are supported as optional steps
    - name: LoRA Configuration
      description: Set up LoRA adapters for efficient fine-tuning
      skills:
      - PEFT library
      - LoRA math
      - Target modules
      deliverables:
      - LoRA configuration object specifying rank, alpha, dropout, and target module names
      - Target module identifier that auto-detects attention and MLP layers eligible for LoRA adapters
      - Rank and alpha tuning utilities that help select appropriate hyperparameters for the task
      - Adapter initializer that injects low-rank matrices into the selected model layers
      - Trainable parameter counter that reports the total and percentage of parameters being fine-tuned
      - Memory estimator that predicts GPU VRAM usage based on model size and LoRA configuration
      hints:
        level1: |-
          Basic LoRA setup:
          ```python
          from peft import LoraConfig, get_peft_model

          lora_config = LoraConfig(
              r=16,  # Rank
              lora_alpha=32,  # Scaling
              target_modules=['q_proj', 'v_proj'],  # Attention layers
              lora_dropout=0.05,
              bias='none',
              task_type='CAUSAL_LM'
          )

          model = get_peft_model(model, lora_config)
          model.print_trainable_parameters()
          ```
        level2: |-
          Target all linear layers:
          ```python
          import re

          def find_linear_modules(model):
              modules = []
              for name, module in model.named_modules():
                  if isinstance(module, torch.nn.Linear):
                      # Skip lm_head and embeddings
                      if 'lm_head' not in name and 'embed' not in name:
                          modules.append(name.split('.')[-1])
              return list(set(modules))

          # Usually: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
          ```
        level3: "Calculate memory savings:\n```python\ndef estimate_memory(model, lora_config):\n    total_params = sum(p.numel()\
          \ for p in model.parameters())\n    \n    # LoRA params: 2 * r * d for each target module\n    lora_params = 0\n\
          \    for name, module in model.named_modules():\n        if any(t in name for t in lora_config.target_modules):\n\
          \            if hasattr(module, 'weight'):\n                d_in, d_out = module.weight.shape\n                lora_params\
          \ += 2 * lora_config.r * (d_in + d_out)\n    \n    print(f'Total: {total_params:,}, LoRA: {lora_params:,} ({100*lora_params/total_params:.2f}%)')\n\
          ```"
      acceptance_criteria:
      - Target modules for LoRA injection are identified automatically based on model architecture
      - Rank and alpha parameters are configurable and affect the capacity of the low-rank adapters
      - Trainable LoRA adapters are initialized and injected into the frozen base model layers
      - Parameter count reduction is verified to be less than 1% of the full model parameter count
    - name: QLoRA & Quantization
      description: 4-bit quantization for memory efficiency
      skills:
      - BitsAndBytes
      - 4-bit quantization
      - NF4
      deliverables:
      - 4-bit model loader that loads the base model quantized to 4-bit NormalFloat precision
      - NF4 quantization configuration specifying quantization type, compute dtype, and nested quantization
      - Compute dtype selector that sets the dtype for forward pass calculations during training
      - Double quantization option that quantizes the quantization constants for additional memory savings
      - Memory benchmarking tool that measures actual VRAM usage before and after quantization
      - Quality-versus-memory tradeoff analysis comparing perplexity at different quantization levels
      hints:
        level1: |-
          Load model in 4-bit:
          ```python
          from transformers import BitsAndBytesConfig

          bnb_config = BitsAndBytesConfig(
              load_in_4bit=True,
              bnb_4bit_quant_type='nf4',
              bnb_4bit_compute_dtype=torch.bfloat16,
              bnb_4bit_use_double_quant=True
          )

          model = AutoModelForCausalLM.from_pretrained(
              model_name,
              quantization_config=bnb_config,
              device_map='auto'
          )
          ```
        level2: |-
          Prepare for k-bit training:
          ```python
          from peft import prepare_model_for_kbit_training

          model = prepare_model_for_kbit_training(model)
          # This:
          # - Casts layernorm to fp32
          # - Enables gradient checkpointing
          # - Enables input gradient requirement
          ```
        level3: |-
          Memory comparison:
          ```python
          def print_gpu_memory():
              for i in range(torch.cuda.device_count()):
                  mem = torch.cuda.memory_allocated(i) / 1e9
                  print(f'GPU {i}: {mem:.2f} GB')

          # 7B model memory:
          # FP32: ~28 GB
          # FP16: ~14 GB
          # INT8: ~7 GB
          # INT4 (QLoRA): ~3.5 GB
          ```
      acceptance_criteria:
      - Base model loads in 4-bit precision using NF4 quantization with acceptable quality loss
      - Quantization parameters including quant type and compute dtype are configurable via a config object
      - Mixed precision training runs forward pass in float16 or bfloat16 while keeping weights in 4-bit
      - VRAM usage is monitored and reported before training, during training, and after model loading
    - name: Training Loop
      description: Implement training with optimization
      skills:
      - Trainer API
      - Gradient accumulation
      - Checkpointing
      deliverables:
      - Training arguments configuration covering learning rate, batch size, warmup steps, and max epochs
      - Gradient accumulation handler that simulates larger batch sizes across multiple forward passes
      - Learning rate scheduler supporting linear warmup with cosine or linear decay over training steps
      - Checkpoint manager that saves model weights and optimizer state at configurable step intervals
      - Logging integration that streams training metrics to WandB or TensorBoard in real time
      - Early stopping callback that halts training when validation loss stops improving for N evaluations
      hints:
        level1: |-
          Basic training setup:
          ```python
          from transformers import TrainingArguments, Trainer

          training_args = TrainingArguments(
              output_dir='./results',
              num_train_epochs=3,
              per_device_train_batch_size=4,
              gradient_accumulation_steps=4,  # Effective batch = 16
              learning_rate=2e-4,
              warmup_ratio=0.03,
              logging_steps=10,
              save_strategy='epoch',
              fp16=True,
          )

          trainer = Trainer(
              model=model,
              args=training_args,
              train_dataset=train_dataset,
              data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
          )
          ```
        level2: "Custom training loop with gradient checkpointing:\n```python\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\
          \noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\nscheduler = get_cosine_schedule_with_warmup(\n   \
          \ optimizer,\n    num_warmup_steps=100,\n    num_training_steps=len(dataloader) * epochs\n)\n\nfor epoch in range(epochs):\n\
          \    for step, batch in enumerate(dataloader):\n        outputs = model(**batch)\n        loss = outputs.loss /\
          \ gradient_accumulation_steps\n        loss.backward()\n        \n        if (step + 1) % gradient_accumulation_steps\
          \ == 0:\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n```"
        level3: |-
          WandB logging:
          ```python
          import wandb

          wandb.init(project='llm-finetune', config={
              'model': model_name,
              'lora_r': lora_config.r,
              'learning_rate': training_args.learning_rate
          })

          training_args = TrainingArguments(
              ...,
              report_to='wandb',
              run_name=f'{model_name}-lora-r{lora_config.r}'
          )
          ```
      acceptance_criteria:
      - Training loop runs with gradient accumulation across the configured number of micro-batches
      - Training loss is tracked and logged at every step to monitor convergence progress
      - Checkpoints are saved at regular intervals and the best checkpoint is tracked by validation loss
      - Early stopping halts training when validation loss fails to improve for a configurable number of evaluations
    - name: Evaluation & Merging
      description: Evaluate model and merge adapters
      skills:
      - Evaluation metrics
      - Model merging
      - GGUF export
      deliverables:
      - Perplexity calculator that evaluates the fine-tuned model on the held-out validation set
      - Task-specific evaluation runner that measures accuracy on domain-relevant benchmark tasks
      - Adapter merger that fuses LoRA weights back into the base model for standalone inference
      - Model exporter that converts the merged model to GGUF format for local inference engines
      - Inference comparison tool that runs the same prompts on base and fine-tuned models side by side
      - Quality benchmark suite that scores the fine-tuned model against standard LLM evaluation benchmarks
      hints:
        level1: "Calculate perplexity:\n```python\nimport math\n\ndef evaluate_perplexity(model, dataloader):\n    model.eval()\n\
          \    total_loss = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n \
          \           outputs = model(**batch)\n            total_loss += outputs.loss.item() * batch['input_ids'].numel()\n\
          \            total_tokens += batch['input_ids'].numel()\n    \n    return math.exp(total_loss / total_tokens)\n\
          ```"
        level2: |-
          Merge LoRA weights:
          ```python
          from peft import PeftModel

          # Load base model
          base_model = AutoModelForCausalLM.from_pretrained(base_model_name)

          # Load adapter
          model = PeftModel.from_pretrained(base_model, adapter_path)

          # Merge and unload
          merged_model = model.merge_and_unload()

          # Save merged model
          merged_model.save_pretrained('merged_model')
          tokenizer.save_pretrained('merged_model')
          ```
        level3: |-
          Export to GGUF for llama.cpp:
          ```bash
          # Install llama.cpp
          git clone https://github.com/ggerganov/llama.cpp
          cd llama.cpp && make

          # Convert to GGUF
          python convert.py ../merged_model --outtype f16 --outfile model.gguf

          # Quantize
          ./quantize model.gguf model-q4_k_m.gguf q4_k_m
          ```
      acceptance_criteria:
      - Perplexity is evaluated on the held-out validation set and compared against the base model
      - Task-specific metrics show measurable improvement comparing before and after fine-tuning scores
      - LoRA adapter weights are merged into the base model producing a single standalone model file
      - Merged model is exported in GGUF format and loads correctly in llama.cpp or similar inference runtime
  order-matching-engine:
    name: Order Matching Engine
    description: Build a low-latency trading order matching engine with order book management and price-time priority
    category: Fintech
    difficulty: advanced
    estimated_hours: 70
    skills:
    - Order book data structures
    - Matching algorithms
    - Low-latency design
    - Concurrency
    - Market data feeds
    - Risk checks
    prerequisites:
    - Data structures
    - Concurrency
    - Networking
    learning_outcomes:
    - Design high-performance order matching systems
    - Implement price-time priority matching
    - Handle concurrent order operations safely
    - Build market data dissemination
    milestones:
    - name: Order Book Data Structure
      description: Efficient order book with price levels
      skills:
      - Tree structures
      - Price levels
      - Order queues
      deliverables:
      - Order class with price, quantity, side, and timestamp fields
      - Price level structure with FIFO order queue
      - Separate bid and ask side order books
      - O(1) lookup for best bid and best ask prices
      - O(log n) insertion and removal at price levels
      - Hash map for O(1) order ID to order lookup
      hints:
        level1: |-
          Order and price level structures:
          ```python
          from dataclasses import dataclass
          from collections import deque
          from sortedcontainers import SortedDict

          @dataclass
          class Order:
              order_id: str
              side: str  # 'buy' or 'sell'
              price: Decimal
              quantity: int
              timestamp: int

          class PriceLevel:
              def __init__(self, price):
                  self.price = price
                  self.orders = deque()  # FIFO queue
                  self.total_quantity = 0
          ```
        level2: "Order book with sorted price levels:\n```python\nclass OrderBook:\n    def __init__(self, symbol):\n    \
          \    self.symbol = symbol\n        self.bids = SortedDict()  # price -> PriceLevel, descending\n        self.asks\
          \ = SortedDict()  # price -> PriceLevel, ascending\n        self.orders = {}  # order_id -> Order\n    \n    def\
          \ best_bid(self):\n        return self.bids.peekitem(-1)[0] if self.bids else None\n    \n    def best_ask(self):\n\
          \        return self.asks.peekitem(0)[0] if self.asks else None\n```"
        level3: "Custom tree for O(1) best price:\n```python\nclass OrderBookSide:\n    def __init__(self, is_bid):\n    \
          \    self.is_bid = is_bid\n        self.levels = SortedDict()\n        self._best = None\n    \n    def add_level(self,\
          \ price):\n        level = PriceLevel(price)\n        self.levels[price] = level\n        if self._best is None\
          \ or (self.is_bid and price > self._best) or (not self.is_bid and price < self._best):\n            self._best =\
          \ price\n        return level\n    \n    def remove_level(self, price):\n        del self.levels[price]\n      \
          \  if price == self._best:\n            self._best = self.levels.peekitem(-1 if self.is_bid else 0)[0] if self.levels\
          \ else None\n```"
      acceptance_criteria:
      - Implement price-level ordered structure supporting bid and ask sides
      - Support both bid and ask sides with correct ordering
      - Enforce price-time priority for orders at same price level
      - Achieve efficient insertion and removal with O(log n) complexity
    - name: Order Operations
      description: Add, cancel, and modify orders
      skills:
      - Order lifecycle
      - Validation
      - State management
      deliverables:
      - Add limit order to correct side and price level
      - Cancel existing order by order ID
      - Modify order quantity while preserving time priority
      - Input validation for price, quantity, and side fields
      - Order acknowledgment message sent to submitter
      - Execution report generated for fills and cancellations
      hints:
        level1: "Add order to book:\n```python\ndef add_order(self, order: Order):\n    side = self.bids if order.side ==\
          \ 'buy' else self.asks\n    \n    if order.price not in side:\n        side[order.price] = PriceLevel(order.price)\n\
          \    \n    level = side[order.price]\n    level.orders.append(order)\n    level.total_quantity += order.quantity\n\
          \    self.orders[order.order_id] = order\n    \n    return OrderAck(order.order_id, 'ACCEPTED')\n```"
        level2: "Cancel order:\n```python\ndef cancel_order(self, order_id: str):\n    if order_id not in self.orders:\n \
          \       return OrderAck(order_id, 'REJECTED', 'Order not found')\n    \n    order = self.orders[order_id]\n    side\
          \ = self.bids if order.side == 'buy' else self.asks\n    level = side[order.price]\n    \n    level.orders.remove(order)\n\
          \    level.total_quantity -= order.quantity\n    \n    if level.total_quantity == 0:\n        del side[order.price]\n\
          \    \n    del self.orders[order_id]\n    return OrderAck(order_id, 'CANCELLED')\n```"
        level3: "Modify preserves time priority only if quantity decreases:\n```python\ndef modify_order(self, order_id: str,\
          \ new_quantity: int):\n    order = self.orders.get(order_id)\n    if not order:\n        return OrderAck(order_id,\
          \ 'REJECTED')\n    \n    if new_quantity > order.quantity:\n        # Lose time priority - cancel and re-add\n \
          \       self.cancel_order(order_id)\n        order.quantity = new_quantity\n        order.timestamp = time.time_ns()\n\
          \        return self.add_order(order)\n    else:\n        # Keep time priority\n        level = self.get_level(order)\n\
          \        level.total_quantity -= (order.quantity - new_quantity)\n        order.quantity = new_quantity\n      \
          \  return OrderAck(order_id, 'MODIFIED')\n```"
      acceptance_criteria:
      - Handle limit order placement at specified price level
      - Support market orders that match immediately at best price
      - Implement order cancellation that removes from book correctly
      - Handle order modification preserving or resetting time priority
    - name: Matching Engine
      description: Price-time priority matching algorithm
      skills:
      - Matching algorithms
      - Trade execution
      - Partial fills
      deliverables:
      - Price-time priority matching algorithm implementation
      - Full and partial fill handling with residual order tracking
      - Trade execution record with price, quantity, and timestamps
      - Aggressive versus passive order classification logic
      - Self-trade prevention check before generating executions
      - Match statistics tracking fill rate and execution counts
      hints:
        level1: "Basic matching:\n```python\ndef match(self, incoming: Order):\n    trades = []\n    opposite = self.asks\
          \ if incoming.side == 'buy' else self.bids\n    \n    while incoming.quantity > 0 and opposite:\n        best_price\
          \ = opposite.peekitem(0 if incoming.side == 'buy' else -1)[0]\n        \n        # Check if prices cross\n     \
          \   if incoming.side == 'buy' and incoming.price < best_price:\n            break\n        if incoming.side == 'sell'\
          \ and incoming.price > best_price:\n            break\n        \n        level = opposite[best_price]\n        trades.extend(self.match_at_level(incoming,\
          \ level))\n        \n        if level.total_quantity == 0:\n            del opposite[best_price]\n    \n    # Add\
          \ remainder to book\n    if incoming.quantity > 0:\n        self.add_order(incoming)\n    \n    return trades\n\
          ```"
        level2: "Match at price level (FIFO):\n```python\ndef match_at_level(self, aggressor, level):\n    trades = []\n \
          \   \n    while aggressor.quantity > 0 and level.orders:\n        resting = level.orders[0]\n        \n        fill_qty\
          \ = min(aggressor.quantity, resting.quantity)\n        \n        trade = Trade(\n            symbol=self.symbol,\n\
          \            price=level.price,  # Passive order's price\n            quantity=fill_qty,\n            aggressor_id=aggressor.order_id,\n\
          \            passive_id=resting.order_id\n        )\n        trades.append(trade)\n        \n        aggressor.quantity\
          \ -= fill_qty\n        resting.quantity -= fill_qty\n        level.total_quantity -= fill_qty\n        \n      \
          \  if resting.quantity == 0:\n            level.orders.popleft()\n            del self.orders[resting.order_id]\n\
          \    \n    return trades\n```"
        level3: "Self-trade prevention:\n```python\ndef match_at_level(self, aggressor, level):\n    trades = []\n    \n \
          \   for resting in list(level.orders):\n        if aggressor.quantity == 0:\n            break\n        \n     \
          \   # Self-trade prevention\n        if aggressor.trader_id == resting.trader_id:\n            # Options: cancel\
          \ newest, cancel oldest, cancel both\n            if self.stp_mode == 'CANCEL_NEWEST':\n                aggressor.quantity\
          \ = 0\n                return trades\n            continue\n        \n        # ... rest of matching\n```"
      acceptance_criteria:
      - Match incoming orders against resting orders by price-time priority
      - Generate trade execution records with both sides identified
      - Handle partial fills leaving residual quantity on the book
      - Update order book state correctly after each match cycle
    - name: Concurrency & Performance
      description: Thread-safe operations and low latency
      skills:
      - Lock-free structures
      - Batching
      - Latency optimization
      deliverables:
      - Lock-free order book update mechanism using atomic operations
      - Order batching to reduce per-order overhead
      - Memory pre-allocation pool for order and trade objects
      - Cache-friendly data layout for hot path optimization
      - Latency measurement harness tracking p50, p99, and p999
      - Throughput benchmarking suite measuring orders per second
      hints:
        level1: "Single-threaded with event loop:\n```python\nimport asyncio\n\nclass MatchingEngine:\n    def __init__(self):\n\
          \        self.order_queue = asyncio.Queue()\n        self.books = {}  # symbol -> OrderBook\n    \n    async def\
          \ run(self):\n        while True:\n            order = await self.order_queue.get()\n            book = self.books[order.symbol]\n\
          \            trades = book.match(order)\n            await self.publish_trades(trades)\n```"
        level2: "Measure latency:\n```python\nimport time\n\nclass LatencyTracker:\n    def __init__(self):\n        self.latencies\
          \ = []\n    \n    def record(self, start_ns):\n        latency = time.time_ns() - start_ns\n        self.latencies.append(latency)\n\
          \    \n    def report(self):\n        arr = sorted(self.latencies)\n        return {\n            'p50': arr[len(arr)//2]\
          \ / 1000,  # microseconds\n            'p99': arr[int(len(arr)*0.99)] / 1000,\n            'p999': arr[int(len(arr)*0.999)]\
          \ / 1000\n        }\n```"
        level3: "Object pooling:\n```python\nclass OrderPool:\n    def __init__(self, size=10000):\n        self.pool = [Order()\
          \ for _ in range(size)]\n        self.available = list(range(size))\n    \n    def acquire(self):\n        if self.available:\n\
          \            idx = self.available.pop()\n            return self.pool[idx]\n        return Order()  # Fallback to\
          \ allocation\n    \n    def release(self, order):\n        order.reset()\n        idx = self.pool.index(order)\n\
          \        self.available.append(idx)\n```"
      acceptance_criteria:
      - Handle concurrent order submissions from multiple threads safely
      - Achieve sub-millisecond latency per order on average
      - Support throughput of at least 10K orders per second
      - Implement lock-free data structures where possible for hot paths
    - name: Market Data & API
      description: Market data feeds and trading API
      skills:
      - Market data
      - WebSocket
      - FIX protocol
      deliverables:
      - Level 2 market data feed with full order book depth
      - Real-time trade feed broadcasting execution details
      - WebSocket streaming endpoint for live market data
      - REST API for order submission, cancellation, and status
      - Basic FIX protocol message parsing and generation
      - Rate limiting middleware to protect API endpoints
      hints:
        level1: |-
          Market data snapshot:
          ```python
          def get_depth(self, symbol, levels=10):
              book = self.books[symbol]
              return {
                  'symbol': symbol,
                  'bids': [
                      {'price': str(p), 'quantity': l.total_quantity}
                      for p, l in list(book.bids.items())[-levels:]
                  ][::-1],
                  'asks': [
                      {'price': str(p), 'quantity': l.total_quantity}
                      for p, l in list(book.asks.items())[:levels]
                  ]
              }
          ```
        level2: "WebSocket market data:\n```python\nfrom fastapi import WebSocket\n\nclass MarketDataFeed:\n    def __init__(self):\n\
          \        self.subscribers = defaultdict(set)\n    \n    async def subscribe(self, websocket: WebSocket, symbol:\
          \ str):\n        self.subscribers[symbol].add(websocket)\n        # Send snapshot\n        await websocket.send_json(self.get_depth(symbol))\n\
          \    \n    async def publish_update(self, symbol: str, update: dict):\n        for ws in self.subscribers[symbol]:\n\
          \            try:\n                await ws.send_json(update)\n            except:\n                self.subscribers[symbol].remove(ws)\n\
          ```"
        level3: "Incremental updates:\n```python\ndef generate_update(self, order, trades):\n    updates = []\n    \n    #\
          \ Price level changes\n    level = self.get_level(order)\n    updates.append({\n        'type': 'level',\n     \
          \   'side': order.side,\n        'price': str(order.price),\n        'quantity': level.total_quantity if level else\
          \ 0\n    })\n    \n    # Trades\n    for trade in trades:\n        updates.append({\n            'type': 'trade',\n\
          \            'price': str(trade.price),\n            'quantity': trade.quantity,\n            'timestamp': trade.timestamp\n\
          \        })\n    \n    return updates\n```"
      acceptance_criteria:
      - Publish order book snapshots with all price levels and quantities
      - Stream trade executions to subscribers in real time
      - Provide REST and WebSocket API for order management
      - Handle market data subscriptions with topic-based filtering
  ledger-system:
    name: Double-entry Ledger System
    description: Build a double-entry accounting system with journal entries, account balances, and audit trail
    category: Fintech
    difficulty: advanced
    estimated_hours: 50
    skills:
    - Double-entry accounting
    - Transaction integrity
    - Balance calculation
    - Audit logging
    - Financial reporting
    - Idempotency
    prerequisites:
    - Database design
    - Transactions
    - API design
    learning_outcomes:
    - Understand double-entry accounting principles
    - Design immutable financial transaction systems
    - Implement balance calculations and reconciliation
    - Build audit-compliant financial systems
    milestones:
    - name: Account & Entry Model
      description: Design core ledger data model
      skills:
      - Schema design
      - Account types
      - Entry structure
      deliverables:
      - Account table with columns for ID, name, type, currency, and parent account reference
      - Journal entry table with columns for entry ID, date, description, and posting status
      - Entry line items table linking journal entries to accounts with debit or credit amounts
      - Currency handling module that stores amounts as fixed-point integers to avoid floating-point errors
      - Account hierarchy structure supporting parent-child relationships for chart of accounts nesting
      - Normal balance rules engine that enforces debit-normal for assets and credit-normal for liabilities
      hints:
        level1: |-
          Core tables:
          ```sql
          CREATE TABLE accounts (
              id UUID PRIMARY KEY,
              code VARCHAR(50) UNIQUE NOT NULL,
              name VARCHAR(255) NOT NULL,
              type VARCHAR(20) NOT NULL,  -- asset, liability, equity, revenue, expense
              currency CHAR(3) DEFAULT 'USD',
              parent_id UUID REFERENCES accounts(id),
              created_at TIMESTAMP DEFAULT NOW()
          );

          CREATE TABLE journal_entries (
              id UUID PRIMARY KEY,
              entry_date DATE NOT NULL,
              description TEXT,
              reference VARCHAR(100),
              created_at TIMESTAMP DEFAULT NOW()
          );

          CREATE TABLE entry_lines (
              id UUID PRIMARY KEY,
              journal_entry_id UUID REFERENCES journal_entries(id),
              account_id UUID REFERENCES accounts(id),
              debit DECIMAL(20,4) DEFAULT 0,
              credit DECIMAL(20,4) DEFAULT 0,
              CHECK (debit >= 0 AND credit >= 0),
              CHECK (debit = 0 OR credit = 0)  -- Can't have both
          );
          ```
        level2: |-
          Account types and normal balances:
          ```python
          class AccountType(Enum):
              ASSET = 'asset'          # Debit normal
              LIABILITY = 'liability'  # Credit normal
              EQUITY = 'equity'        # Credit normal
              REVENUE = 'revenue'      # Credit normal
              EXPENSE = 'expense'      # Debit normal

          def normal_balance(account_type):
              return 'debit' if account_type in [AccountType.ASSET, AccountType.EXPENSE] else 'credit'
          ```
        level3: "Ensure debits = credits:\n```sql\nCREATE OR REPLACE FUNCTION check_balanced_entry()\nRETURNS TRIGGER AS $$\n\
          BEGIN\n    IF (SELECT SUM(debit) - SUM(credit) FROM entry_lines \n        WHERE journal_entry_id = NEW.journal_entry_id)\
          \ != 0 THEN\n        RAISE EXCEPTION 'Entry must be balanced';\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE\
          \ plpgsql;\n```"
      acceptance_criteria:
      - Account types include asset, liability, equity, income, and expense with correct normal balances
      - Journal entries store multiple debit and credit line items referencing specific accounts
      - Every journal entry enforces that total debits equal total credits before it can be posted
      - Multi-currency accounts store amounts in their native currency with exchange rate metadata
    - name: Transaction Recording
      description: Record transactions with double-entry
      skills:
      - Transaction atomicity
      - Validation
      - Idempotency
      deliverables:
      - Journal entry creation API that accepts a list of debit and credit lines atomically
      - Balance validation logic that rejects entries where total debits do not equal total credits
      - Idempotency key system that prevents duplicate entries from repeated API submissions
      - Transaction template library providing predefined entry patterns for common operations
      - Batch entry processor that creates multiple journal entries in a single atomic transaction
      - Entry reversal mechanism that creates a counter-entry to void a previously posted entry
      hints:
        level1: "Create balanced entry:\n```python\nclass Ledger:\n    def create_entry(self, date, description, lines, reference=None):\n\
          \        # Validate balance\n        total_debit = sum(l['debit'] for l in lines)\n        total_credit = sum(l['credit']\
          \ for l in lines)\n        if total_debit != total_credit:\n            raise ValueError(f'Entry not balanced: {total_debit}\
          \ != {total_credit}')\n        \n        with db.transaction():\n            entry = JournalEntry.create(\n    \
          \            entry_date=date,\n                description=description,\n                reference=reference\n \
          \           )\n            for line in lines:\n                EntryLine.create(\n                    journal_entry_id=entry.id,\n\
          \                    account_id=line['account_id'],\n                    debit=line.get('debit', 0),\n         \
          \           credit=line.get('credit', 0)\n                )\n        return entry\n```"
        level2: "Idempotency:\n```python\ndef create_entry(self, idempotency_key, ...):\n    # Check if already processed\n\
          \    existing = db.query(\n        'SELECT id FROM journal_entries WHERE idempotency_key = %s',\n        [idempotency_key]\n\
          \    )\n    if existing:\n        return existing[0]  # Return existing entry\n    \n    with db.transaction():\n\
          \        entry = JournalEntry.create(\n            idempotency_key=idempotency_key,\n            ...\n        )\n\
          \        # ... create lines\n    return entry\n```"
        level3: "Common transaction templates:\n```python\nclass TransactionTemplates:\n    @staticmethod\n    def payment_received(customer_account,\
          \ revenue_account, amount):\n        return [\n            {'account_id': customer_account, 'debit': amount, 'credit':\
          \ 0},\n            {'account_id': revenue_account, 'debit': 0, 'credit': amount}\n        ]\n    \n    @staticmethod\n\
          \    def transfer(from_account, to_account, amount):\n        return [\n            {'account_id': from_account,\
          \ 'debit': 0, 'credit': amount},\n            {'account_id': to_account, 'debit': amount, 'credit': 0}\n       \
          \ ]\n```"
      acceptance_criteria:
      - Journal entries are created atomically so either all lines post or none do on failure
      - Account type validation ensures debit and credit lines reference appropriate account types
      - Each transaction receives a unique auto-generated ID for traceability and audit reference
      - Transaction metadata fields store arbitrary key-value pairs like invoice number or memo
    - name: Balance Calculation
      description: Calculate account balances efficiently
      skills:
      - Running balances
      - Point-in-time
      - Aggregation
      deliverables:
      - Current balance calculator that sums all posted debits and credits for a given account
      - Point-in-time balance query that computes an account balance as of a specified date
      - Running balance table that caches account balances after each posted journal entry
      - Balance cache invalidation logic that updates cached balances when new entries are posted
      - Trial balance report generator that lists all accounts with their debit and credit totals
      - Account reconciliation tool that compares ledger balances against external bank statements
      hints:
        level1: "Calculate balance from entries:\n```python\ndef get_balance(self, account_id, as_of=None):\n    query = '''\n\
          \        SELECT \n            COALESCE(SUM(debit), 0) as total_debit,\n            COALESCE(SUM(credit), 0) as total_credit\n\
          \        FROM entry_lines el\n        JOIN journal_entries je ON el.journal_entry_id = je.id\n        WHERE el.account_id\
          \ = %s\n    '''\n    params = [account_id]\n    \n    if as_of:\n        query += ' AND je.entry_date <= %s'\n \
          \       params.append(as_of)\n    \n    result = db.query(query, params)[0]\n    \n    account = Account.get(account_id)\n\
          \    if normal_balance(account.type) == 'debit':\n        return result.total_debit - result.total_credit\n    else:\n\
          \        return result.total_credit - result.total_debit\n```"
        level2: |-
          Materialized running balance:
          ```sql
          CREATE TABLE account_balances (
              account_id UUID REFERENCES accounts(id),
              balance_date DATE,
              debit_total DECIMAL(20,4),
              credit_total DECIMAL(20,4),
              balance DECIMAL(20,4),
              PRIMARY KEY (account_id, balance_date)
          );

          -- Update on entry creation
          CREATE TRIGGER update_balance
          AFTER INSERT ON entry_lines
          FOR EACH ROW
          EXECUTE FUNCTION update_account_balance();
          ```
        level3: "Trial balance:\n```python\ndef trial_balance(self, as_of=None):\n    balances = db.query('''\n        SELECT\
          \ \n            a.code, a.name, a.type,\n            SUM(el.debit) as debits,\n            SUM(el.credit) as credits\n\
          \        FROM accounts a\n        LEFT JOIN entry_lines el ON a.id = el.account_id\n        LEFT JOIN journal_entries\
          \ je ON el.journal_entry_id = je.id\n        WHERE je.entry_date <= %s OR je.entry_date IS NULL\n        GROUP BY\
          \ a.id\n        ORDER BY a.code\n    ''', [as_of or date.today()])\n    \n    total_debit = sum(b.debits or 0 for\
          \ b in balances)\n    total_credit = sum(b.credits or 0 for b in balances)\n    \n    return {\n        'accounts':\
          \ balances,\n        'total_debit': total_debit,\n        'total_credit': total_credit,\n        'balanced': total_debit\
          \ == total_credit\n    }\n```"
      acceptance_criteria:
      - Account balance is calculated efficiently using running totals rather than scanning all entries
      - Balance-at-date queries return the correct balance considering only entries posted on or before that date
      - Running balance updates are triggered automatically when new journal entries are posted
      - System handles large transaction volumes with sub-second balance query response times
    - name: Audit Trail
      description: Immutable audit logging
      skills:
      - Immutability
      - Audit logs
      - Change tracking
      deliverables:
      - Immutable entry storage that prevents UPDATE and DELETE operations on posted journal entries
      - Correction entry mechanism that creates reversal entries instead of modifying the original
      - Change history table that logs all account and entry modifications with timestamps and actors
      - User action audit log that records who performed each operation and when it occurred
      - Entry approval workflow that requires manager sign-off before journal entries are posted
      - Audit export module that generates downloadable reports of all entries for a given period
      hints:
        level1: |-
          Prevent modifications:
          ```sql
          CREATE OR REPLACE FUNCTION prevent_entry_modification()
          RETURNS TRIGGER AS $$
          BEGIN
              RAISE EXCEPTION 'Journal entries cannot be modified. Create a reversal instead.';
          END;
          $$ LANGUAGE plpgsql;

          CREATE TRIGGER no_update_entries
          BEFORE UPDATE OR DELETE ON journal_entries
          FOR EACH ROW EXECUTE FUNCTION prevent_entry_modification();

          CREATE TRIGGER no_update_lines
          BEFORE UPDATE OR DELETE ON entry_lines
          FOR EACH ROW EXECUTE FUNCTION prevent_entry_modification();
          ```
        level2: "Reversal entry:\n```python\ndef reverse_entry(self, entry_id, reason):\n    original = JournalEntry.get(entry_id)\n\
          \    original_lines = EntryLine.where(journal_entry_id=entry_id)\n    \n    # Create reversal with swapped debits/credits\n\
          \    reversal_lines = [\n        {\n            'account_id': line.account_id,\n            'debit': line.credit,\
          \  # Swap\n            'credit': line.debit   # Swap\n        }\n        for line in original_lines\n    ]\n   \
          \ \n    return self.create_entry(\n        date=date.today(),\n        description=f'REVERSAL: {reason} (original:\
          \ {entry_id})',\n        lines=reversal_lines,\n        reference=f'REV-{entry_id}'\n    )\n```"
        level3: |-
          Audit log:
          ```sql
          CREATE TABLE audit_log (
              id BIGSERIAL PRIMARY KEY,
              timestamp TIMESTAMP DEFAULT NOW(),
              user_id UUID,
              action VARCHAR(50),
              entity_type VARCHAR(50),
              entity_id UUID,
              details JSONB,
              ip_address INET
          );

          -- Log all entry creations
          CREATE TRIGGER log_entry_creation
          AFTER INSERT ON journal_entries
          FOR EACH ROW
          EXECUTE FUNCTION log_audit_event('CREATE', 'journal_entry');
          ```
      acceptance_criteria:
      - All changes are logged with timestamp, actor identity, and before/after values of modified fields
      - Posted journal entries cannot be modified or deleted; only reversals are allowed
      - Entry reversal creates a new offsetting entry that zeroes out the original without deleting it
      - Audit reports can be generated for any date range showing all entries and modifications
    - name: Financial Reports
      description: Generate standard financial reports
      skills:
      - Report generation
      - Period closing
      - Multi-currency
      deliverables:
      - Income statement generator that computes revenue minus expenses for a given period
      - Balance sheet generator that displays assets, liabilities, and equity at a point in time
      - Cash flow statement generator that categorizes cash movements into operating, investing, and financing
      - Period closing entries processor that transfers income and expense balances to retained earnings
      - Multi-currency reporting that converts foreign currency balances at period-end exchange rates
      - Report export module that outputs financial reports in PDF and CSV formats
      hints:
        level1: "Income statement:\n```python\ndef income_statement(self, start_date, end_date):\n    revenue = self.sum_by_type(AccountType.REVENUE,\
          \ start_date, end_date)\n    expenses = self.sum_by_type(AccountType.EXPENSE, start_date, end_date)\n    \n    return\
          \ {\n        'period': {'start': start_date, 'end': end_date},\n        'revenue': revenue,\n        'expenses':\
          \ expenses,\n        'net_income': revenue['total'] - expenses['total']\n    }\n\ndef sum_by_type(self, account_type,\
          \ start, end):\n    accounts = db.query('''\n        SELECT a.name, SUM(el.credit) - SUM(el.debit) as amount\n \
          \       FROM accounts a\n        JOIN entry_lines el ON a.id = el.account_id\n        JOIN journal_entries je ON\
          \ el.journal_entry_id = je.id\n        WHERE a.type = %s AND je.entry_date BETWEEN %s AND %s\n        GROUP BY a.id\n\
          \    ''', [account_type, start, end])\n    return {'accounts': accounts, 'total': sum(a.amount for a in accounts)}\n\
          ```"
        level2: "Balance sheet:\n```python\ndef balance_sheet(self, as_of):\n    assets = self.sum_by_type(AccountType.ASSET,\
          \ None, as_of)\n    liabilities = self.sum_by_type(AccountType.LIABILITY, None, as_of)\n    equity = self.sum_by_type(AccountType.EQUITY,\
          \ None, as_of)\n    \n    # Retained earnings = prior net income\n    retained = self.calculate_retained_earnings(as_of)\n\
          \    \n    return {\n        'as_of': as_of,\n        'assets': assets,\n        'liabilities': liabilities,\n \
          \       'equity': {\n            'accounts': equity['accounts'],\n            'retained_earnings': retained,\n \
          \           'total': equity['total'] + retained\n        },\n        'balanced': assets['total'] == liabilities['total']\
          \ + equity['total'] + retained\n    }\n```"
        level3: "Period closing:\n```python\ndef close_period(self, period_end):\n    # Calculate net income\n    revenue\
          \ = self.sum_by_type(AccountType.REVENUE, period_start, period_end)\n    expenses = self.sum_by_type(AccountType.EXPENSE,\
          \ period_start, period_end)\n    net_income = revenue['total'] - expenses['total']\n    \n    # Close revenue and\
          \ expense to retained earnings\n    lines = []\n    for acc in revenue['accounts']:\n        lines.append({'account_id':\
          \ acc.id, 'debit': acc.amount, 'credit': 0})\n    for acc in expenses['accounts']:\n        lines.append({'account_id':\
          \ acc.id, 'debit': 0, 'credit': acc.amount})\n    lines.append({'account_id': retained_earnings_id, 'debit': 0 if\
          \ net_income > 0 else -net_income, 'credit': net_income if net_income > 0 else 0})\n    \n    return self.create_entry(period_end,\
          \ f'Period close {period_end}', lines)\n```"
      acceptance_criteria:
      - Balance sheet displays assets equal to liabilities plus equity at the selected date
      - Income statement shows net income as total revenue minus total expenses for the period
      - Trial balance report lists all accounts and verifies total debits equal total credits
      - Reports are exported in PDF and CSV formats with proper formatting and headers
  build-graphql-engine:
    name: Build Your Own GraphQL Engine
    description: Build a GraphQL execution engine with query parsing, schema stitching, and automatic API generation like
      Hasura/PostGraphile
    category: Backend Infrastructure
    difficulty: expert
    estimated_hours: 120
    skills:
    - GraphQL spec
    - Query parsing
    - Schema introspection
    - Database reflection
    - Query optimization
    - Real-time subscriptions
    prerequisites:
    - GraphQL server
    - Database internals
    - Compiler basics
    learning_outcomes:
    - Deep understanding of GraphQL specification
    - Implement query parsing and validation
    - Build automatic schema generation from database
    - Optimize GraphQL queries to SQL
    milestones:
    - name: GraphQL Parser
      description: Parse GraphQL queries into AST
      skills:
      - Lexer
      - Parser
      - AST
      deliverables:
      - GraphQL lexer and tokenizer that breaks a query string into tokens (names, strings, numbers, punctuation)
      - Recursive descent parser that builds an abstract syntax tree from the token stream
      - Query document AST representing operations, selection sets, fields, arguments, and directives
      - Fragment definition and spread handling that expands named fragments into their selection sets
      - Variable definition parsing that captures variable names, types, and default values from the operation header
      - Directive parsing that reads @-prefixed directives with their argument lists on fields and fragments
      hints:
        level1: |-
          Token types:
          ```python
          class TokenType(Enum):
              NAME = 'NAME'
              INT = 'INT'
              FLOAT = 'FLOAT'
              STRING = 'STRING'
              LBRACE = '{'
              RBRACE = '}'
              LPAREN = '('
              RPAREN = ')'
              COLON = ':'
              BANG = '!'
              DOLLAR = '$'
              AT = '@'
              SPREAD = '...'
          ```
        level2: |-
          Parse selection set:
          ```python
          def parse_selection_set(self):
              self.expect(TokenType.LBRACE)
              selections = []
              while not self.check(TokenType.RBRACE):
                  if self.check(TokenType.SPREAD):
                      selections.append(self.parse_fragment())
                  else:
                      selections.append(self.parse_field())
              self.expect(TokenType.RBRACE)
              return SelectionSet(selections)
          ```
        level3: "Field with arguments:\n```python\ndef parse_field(self):\n    alias = None\n    name = self.expect(TokenType.NAME)\n\
          \    if self.check(TokenType.COLON):\n        self.advance()\n        alias = name\n        name = self.expect(TokenType.NAME)\n\
          \    \n    arguments = []\n    if self.check(TokenType.LPAREN):\n        arguments = self.parse_arguments()\n  \
          \  \n    selection_set = None\n    if self.check(TokenType.LBRACE):\n        selection_set = self.parse_selection_set()\n\
          \    \n    return Field(alias, name, arguments, selection_set)\n```"
      acceptance_criteria:
      - Parser converts a valid GraphQL query string into a complete AST representing all operations and fields
      - Queries, mutations, and subscription operations are all recognized and represented as distinct AST operation types
      - Named fragments and inline fragments are parsed and their selection sets are correctly attached in the AST
      - Parse errors include the line number, column number, and a description of the expected vs. actual token
    - name: Schema & Type System
      description: Build type system and schema representation
      skills:
      - Type system
      - Introspection
      - Validation
      deliverables:
      - Scalar type definitions for built-in types (Int, Float, String, Boolean, ID) and custom scalars
      - Object type definitions with field lists, argument definitions, and interface implementations
      - Input type definitions for mutation arguments with field validation and default values
      - Enum type definitions mapping symbolic names to their allowed value sets
      - Interface and union type definitions supporting polymorphic field resolution and type conditions
      - Introspection query support implementing __schema, __type, and __typename meta-fields per the spec
      hints:
        level1: |-
          Type classes:
          ```python
          class GraphQLType:
              pass

          class GraphQLScalar(GraphQLType):
              def __init__(self, name, serialize, parse_value, parse_literal):
                  self.name = name
                  self.serialize = serialize
                  self.parse_value = parse_value

          class GraphQLObject(GraphQLType):
              def __init__(self, name, fields):
                  self.name = name
                  self.fields = fields  # {name: GraphQLField}
          ```
        level2: "Schema with root types:\n```python\nclass GraphQLSchema:\n    def __init__(self, query, mutation=None, subscription=None):\n\
          \        self.query_type = query\n        self.mutation_type = mutation\n        self.subscription_type = subscription\n\
          \        self.type_map = self._build_type_map()\n    \n    def get_type(self, name):\n        return self.type_map.get(name)\n\
          ```"
        level3: |-
          Introspection:
          ```python
          def add_introspection(schema):
              schema.query_type.fields['__schema'] = GraphQLField(
                  type=__Schema,
                  resolve=lambda *_: schema
              )
              schema.query_type.fields['__type'] = GraphQLField(
                  type=__Type,
                  args={'name': GraphQLArgument(GraphQLString)},
                  resolve=lambda _, args, *__: schema.get_type(args['name'])
              )
          ```
      acceptance_criteria:
      - All GraphQL type kinds (Object, Scalar, Enum, Interface, Union, InputObject, List, NonNull) are supported
      - Schema is constructed from type definitions and validated for consistency (e.g., no circular input types)
      - Schema validation detects errors such as missing required fields, undefined types, and duplicate type names
      - Custom scalar types can be defined with serialize, parseValue, and parseLiteral functions
    - name: Query Execution
      description: Execute queries against schema
      skills:
      - Execution
      - Resolvers
      - Error handling
      deliverables:
      - Field resolver dispatch that calls the correct resolver function for each field in the selection set
      - Argument coercion that converts raw input values to the expected types defined in the schema
      - List and non-null type handling that enforces nullability rules and wraps results in arrays as specified
      - Error collection mechanism that gathers field-level errors without aborting the entire query execution
      - Parallel field execution that resolves sibling fields concurrently when their resolvers are independent
      - Execution context object that carries authentication, data loaders, and request-scoped state to resolvers
      hints:
        level1: |-
          Execute selection set:
          ```python
          async def execute_selection_set(self, selection_set, object_type, root_value, context):
              results = {}
              for selection in selection_set.selections:
                  if isinstance(selection, Field):
                      field_name = selection.alias or selection.name
                      results[field_name] = await self.execute_field(
                          selection, object_type, root_value, context
                      )
              return results
          ```
        level2: "Execute field with resolver:\n```python\nasync def execute_field(self, field, parent_type, source, context):\n\
          \    field_def = parent_type.fields[field.name]\n    \n    # Coerce arguments\n    args = self.coerce_arguments(field.arguments,\
          \ field_def.args)\n    \n    # Call resolver\n    resolver = field_def.resolve or default_resolver\n    try:\n \
          \       result = await maybe_await(resolver(source, args, context, field))\n    except Exception as e:\n       \
          \ self.errors.append(GraphQLError(str(e), field))\n        return None\n    \n    return await self.complete_value(field_def.type,\
          \ result, field, context)\n```"
        level3: "Complete value for type:\n```python\nasync def complete_value(self, return_type, result, field, context):\n\
          \    if isinstance(return_type, GraphQLNonNull):\n        completed = await self.complete_value(return_type.of_type,\
          \ result, field, context)\n        if completed is None:\n            raise GraphQLError('Cannot return null for\
          \ non-null field')\n        return completed\n    \n    if result is None:\n        return None\n    \n    if isinstance(return_type,\
          \ GraphQLList):\n        return [await self.complete_value(return_type.of_type, item, field, context) for item in\
          \ result]\n    \n    if isinstance(return_type, GraphQLScalar):\n        return return_type.serialize(result)\n\
          \    \n    if isinstance(return_type, GraphQLObject):\n        return await self.execute_selection_set(field.selection_set,\
          \ return_type, result, context)\n```"
      acceptance_criteria:
      - Queries are executed against the schema and return a data object matching the requested selection set shape
      - Each field is resolved by calling its resolver function with parent value, arguments, and context
      - Nested fields are resolved recursively with each level receiving the parent field's resolved value
      - Non-null fields that resolve to null propagate the null to the nearest nullable parent field per the GraphQL spec
    - name: Database Schema Reflection
      description: Auto-generate GraphQL schema from database
      skills:
      - Database introspection
      - Schema generation
      - Relationships
      deliverables:
      - Table-to-type mapper that converts database tables into corresponding GraphQL object types
      - Column-to-field mapper that converts database columns into typed GraphQL fields with appropriate scalar types
      - Foreign key relationship detector that creates GraphQL fields for related objects based on FK constraints
      - Primary key query generator that creates by-ID lookup query fields for each reflected table type
      - Filter argument generator that creates where-clause input types for each table's queryable columns
      - Pagination argument support that adds first, last, after, and before arguments to collection fields
      hints:
        level1: "Reflect PostgreSQL tables:\n```python\nasync def reflect_tables(self, conn):\n    tables = await conn.fetch('''\n\
          \        SELECT table_name FROM information_schema.tables\n        WHERE table_schema = 'public'\n    ''')\n   \
          \ \n    for table in tables:\n        columns = await conn.fetch('''\n            SELECT column_name, data_type,\
          \ is_nullable\n            FROM information_schema.columns\n            WHERE table_name = $1\n        ''', table['table_name'])\n\
          \        \n        yield Table(table['table_name'], columns)\n```"
        level2: "Generate GraphQL type from table:\n```python\ndef table_to_graphql_type(table):\n    fields = {}\n    for\
          \ col in table.columns:\n        gql_type = SQL_TO_GRAPHQL[col.data_type]\n        if col.is_nullable == 'NO':\n\
          \            gql_type = GraphQLNonNull(gql_type)\n        fields[col.column_name] = GraphQLField(gql_type)\n   \
          \ \n    return GraphQLObject(pascal_case(table.name), fields)\n\nSQL_TO_GRAPHQL = {\n    'integer': GraphQLInt,\n\
          \    'bigint': GraphQLInt,\n    'text': GraphQLString,\n    'varchar': GraphQLString,\n    'boolean': GraphQLBoolean,\n\
          \    'timestamp': GraphQLDateTime,\n}\n```"
        level3: "Generate relationship fields:\n```python\nasync def add_relationships(self, conn, types):\n    fks = await\
          \ conn.fetch('''\n        SELECT tc.table_name, kcu.column_name, \n               ccu.table_name AS foreign_table,\n\
          \               ccu.column_name AS foreign_column\n        FROM information_schema.table_constraints tc\n      \
          \  JOIN information_schema.key_column_usage kcu USING (constraint_name)\n        JOIN information_schema.constraint_column_usage\
          \ ccu USING (constraint_name)\n        WHERE tc.constraint_type = 'FOREIGN KEY'\n    ''')\n    \n    for fk in fks:\n\
          \        # Add object relationship (many-to-one)\n        types[fk['table_name']].fields[fk['foreign_table']] =\
          \ GraphQLField(\n            types[fk['foreign_table']],\n            resolve=lambda obj, *_: fetch_by_id(fk['foreign_table'],\
          \ obj[fk['column_name']])\n        )\n        # Add array relationship (one-to-many)\n        types[fk['foreign_table']].fields[fk['table_name']\
          \ + 's'] = GraphQLField(\n            GraphQLList(types[fk['table_name']]),\n            resolve=lambda obj, *_:\
          \ fetch_by_fk(fk['table_name'], fk['column_name'], obj['id'])\n        )\n```"
      acceptance_criteria:
      - Database tables and columns are introspected and their names and types are read from the information schema
      - GraphQL object types are auto-generated from tables with one field per column using appropriate scalar types
      - Database column types (integer, varchar, boolean, timestamp) are correctly mapped to GraphQL scalar types
      - Foreign key relationships produce nested object fields that resolve to the related table's GraphQL type
    - name: Query to SQL Compilation
      description: Compile GraphQL queries to efficient SQL
      skills:
      - Query planning
      - JOIN optimization
      - Batching
      deliverables:
      - Selection-to-SELECT mapper that converts GraphQL field selections into SQL SELECT column lists
      - Nested selection-to-JOIN compiler that translates nested object fields into SQL JOIN clauses on foreign keys
      - Filter-to-WHERE translator that converts GraphQL filter arguments into parameterized SQL WHERE conditions
      - Order and limit handler that applies sorting and pagination to SQL queries from GraphQL arguments
      - Aggregate query support that compiles count, sum, avg, min, and max GraphQL fields into SQL aggregations
      - Query batching with DataLoader pattern that combines multiple resolver calls into a single SQL query
      hints:
        level1: "Basic query compilation:\n```python\ndef compile_query(self, field, table):\n    columns = []\n    joins\
          \ = []\n    \n    for selection in field.selection_set.selections:\n        if selection.name in table.columns:\n\
          \            columns.append(f'{table.name}.{selection.name}')\n        elif selection.name in table.relationships:\n\
          \            rel = table.relationships[selection.name]\n            joins.append(f'LEFT JOIN {rel.target_table}\
          \ ON ...')\n            columns.extend(self.compile_nested(selection, rel.target_table))\n    \n    return f'SELECT\
          \ {', '.join(columns)} FROM {table.name} {' '.join(joins)}'\n```"
        level2: "Compile filters:\n```python\ndef compile_where(self, where_arg, table):\n    clauses = []\n    params = []\n\
          \    \n    for field, condition in where_arg.items():\n        if field == '_and':\n            sub = [self.compile_where(c,\
          \ table) for c in condition]\n            clauses.append(f\"({' AND '.join(s[0] for s in sub)})\")\n           \
          \ params.extend(p for s in sub for p in s[1])\n        elif field == '_or':\n            sub = [self.compile_where(c,\
          \ table) for c in condition]\n            clauses.append(f\"({' OR '.join(s[0] for s in sub)})\")\n        else:\n\
          \            for op, value in condition.items():\n                sql_op = {'_eq': '=', '_neq': '!=', '_gt': '>',\
          \ '_lt': '<', '_like': 'LIKE'}[op]\n                clauses.append(f'{table}.{field} {sql_op} ${len(params)+1}')\n\
          \                params.append(value)\n    \n    return ' AND '.join(clauses), params\n```"
        level3: "Avoid N+1 with lateral join:\n```python\ndef compile_nested_lateral(self, parent, child_field):\n    return\
          \ f'''\n        SELECT {parent.name}.*, \n               COALESCE(json_agg({child_field.name}.*), '[]') as {child_field.name}\n\
          \        FROM {parent.name}\n        LEFT JOIN LATERAL (\n            SELECT * FROM {child_field.table}\n      \
          \      WHERE {child_field.fk} = {parent.name}.id\n        ) {child_field.name} ON true\n        GROUP BY {parent.name}.id\n\
          \    '''\n```"
      acceptance_criteria:
      - GraphQL queries compile to efficient SQL statements that select only the columns requested in the query
      - Nested object fields generate SQL JOINs on foreign key relationships instead of issuing separate queries
      - N+1 query problem is prevented by batching related-object lookups into a single SQL query with IN clause
      - Filtering, pagination (limit/offset or cursor), and ordering are correctly translated to SQL WHERE, LIMIT, and ORDER
        BY clauses
  data-quality-framework:
    name: Data Quality Framework
    description: Build a data quality system with schema validation, anomaly detection, and data profiling like Great Expectations
    category: Data Engineering
    difficulty: intermediate
    estimated_hours: 45
    skills:
    - Schema validation
    - Statistical profiling
    - Anomaly detection
    - Rule engine
    - Data contracts
    - Alerting
    prerequisites:
    - SQL
    - Statistics basics
    - Python
    learning_outcomes:
    - Design data validation frameworks
    - Implement statistical data profiling
    - Build rule-based quality checks
    - Create data quality dashboards
    milestones:
    - name: Expectation Engine
      description: Define and run data quality expectations
      skills:
      - Rule definition
      - Validation logic
      - Result collection
      deliverables:
      - Expectation base class with validation and serialization interface
      - Column expectations including not-null, unique, and type checks
      - Value expectations including range, regex, and allowed-values checks
      - Result objects with pass/fail status and detailed metrics
      - Expectation suite grouping related expectations together
      - JSON serialization for expectation definitions and results
      hints:
        level1: "Expectation base class:\n```python\nclass Expectation:\n    def __init__(self, column=None, **kwargs):\n\
          \        self.column = column\n        self.kwargs = kwargs\n    \n    def validate(self, df) -> ExpectationResult:\n\
          \        raise NotImplementedError\n\nclass ExpectationResult:\n    def __init__(self, success, expectation, observed_value,\
          \ details=None):\n        self.success = success\n        self.expectation = expectation\n        self.observed_value\
          \ = observed_value\n        self.details = details or {}\n```"
        level2: |-
          Common expectations:
          ```python
          class ExpectColumnNotNull(Expectation):
              def validate(self, df):
                  null_count = df[self.column].isnull().sum()
                  total = len(df)
                  return ExpectationResult(
                      success=null_count == 0,
                      expectation=self,
                      observed_value={'null_count': null_count, 'total': total}
                  )

          class ExpectColumnValuesInRange(Expectation):
              def validate(self, df):
                  min_val, max_val = self.kwargs['min'], self.kwargs['max']
                  out_of_range = ((df[self.column] < min_val) | (df[self.column] > max_val)).sum()
                  return ExpectationResult(
                      success=out_of_range == 0,
                      expectation=self,
                      observed_value={'out_of_range': out_of_range}
                  )
          ```
        level3: "Expectation suite:\n```python\nclass ExpectationSuite:\n    def __init__(self, name):\n        self.name\
          \ = name\n        self.expectations = []\n    \n    def add(self, expectation):\n        self.expectations.append(expectation)\n\
          \        return self\n    \n    def validate(self, df):\n        results = [exp.validate(df) for exp in self.expectations]\n\
          \        return ValidationResult(\n            success=all(r.success for r in results),\n            results=results,\n\
          \            statistics={'passed': sum(r.success for r in results), 'total': len(results)}\n        )\n    \n  \
          \  def to_json(self):\n        return {'name': self.name, 'expectations': [e.to_dict() for e in self.expectations]}\n\
          ```"
      acceptance_criteria:
      - Define data quality expectations such as not_null, unique, and range checks
      - Evaluate expectations against datasets and report pass/fail per expectation
      - Support custom expectation definitions via user-provided validation functions
      - Return detailed validation results with row counts, failure counts, and percentages
    - name: Data Profiling
      description: Automatic statistical profiling of datasets
      skills:
      - Descriptive statistics
      - Distribution analysis
      - Cardinality
      deliverables:
      - Column statistics computation for mean, std, min, and max values
      - Null percentage calculation per column for data completeness assessment
      - Cardinality and uniqueness ratio analysis per column
      - Value distribution histogram generation for data understanding
      - Data type inference engine for automatic schema detection
      - Profile report generation in human-readable and machine-readable formats
      hints:
        level1: |-
          Basic column profiler:
          ```python
          class ColumnProfiler:
              def profile(self, series):
                  return {
                      'dtype': str(series.dtype),
                      'count': len(series),
                      'null_count': series.isnull().sum(),
                      'null_pct': series.isnull().mean() * 100,
                      'unique_count': series.nunique(),
                      'unique_pct': series.nunique() / len(series) * 100
                  }

          class NumericProfiler(ColumnProfiler):
              def profile(self, series):
                  base = super().profile(series)
                  return {**base,
                      'mean': series.mean(),
                      'std': series.std(),
                      'min': series.min(),
                      'max': series.max(),
                      'percentiles': series.quantile([0.25, 0.5, 0.75]).to_dict()
                  }
          ```
        level2: |-
          Value distribution:
          ```python
          def get_distribution(series, bins=20):
              if series.dtype in ['int64', 'float64']:
                  hist, edges = np.histogram(series.dropna(), bins=bins)
                  return {'type': 'histogram', 'counts': hist.tolist(), 'edges': edges.tolist()}
              else:
                  counts = series.value_counts().head(20)
                  return {'type': 'categorical', 'values': counts.to_dict()}
          ```
        level3: "Full dataset profile:\n```python\nclass DatasetProfiler:\n    def profile(self, df):\n        profiles =\
          \ {}\n        for col in df.columns:\n            if pd.api.types.is_numeric_dtype(df[col]):\n                profiler\
          \ = NumericProfiler()\n            elif pd.api.types.is_datetime64_dtype(df[col]):\n                profiler = DatetimeProfiler()\n\
          \            else:\n                profiler = CategoricalProfiler()\n            profiles[col] = profiler.profile(df[col])\n\
          \        \n        return DatasetProfile(\n            row_count=len(df),\n            column_count=len(df.columns),\n\
          \            columns=profiles\n        )\n```"
      acceptance_criteria:
      - Calculate column statistics including min, max, mean, and null counts accurately
      - Detect data types automatically from column values using heuristic inference
      - Generate distribution histograms showing value frequency for each column
      - Identify potential data quality issues such as outliers and inconsistent types
    - name: Anomaly Detection
      description: Detect data anomalies and drift
      skills:
      - Statistical tests
      - Drift detection
      - Outliers
      deliverables:
      - Z-score outlier detection for numerical columns
      - IQR method for robust outlier identification
      - Distribution drift detection using Kolmogorov-Smirnov test
      - Schema drift detection comparing current schema against baseline
      - Volume anomaly detection for unexpected row count changes
      - Freshness checks verifying data recency against expected schedule
      hints:
        level1: "Outlier detection:\n```python\nclass OutlierDetector:\n    def zscore(self, series, threshold=3):\n     \
          \   z = np.abs((series - series.mean()) / series.std())\n        return series[z > threshold]\n    \n    def iqr(self,\
          \ series, multiplier=1.5):\n        q1, q3 = series.quantile([0.25, 0.75])\n        iqr = q3 - q1\n        lower\
          \ = q1 - multiplier * iqr\n        upper = q3 + multiplier * iqr\n        return series[(series < lower) | (series\
          \ > upper)]\n```"
        level2: "Distribution drift:\n```python\nfrom scipy import stats\n\nclass DriftDetector:\n    def __init__(self, reference_profile):\n\
          \        self.reference = reference_profile\n    \n    def detect_drift(self, current_df):\n        drift_results\
          \ = {}\n        for col in current_df.columns:\n            if col in self.reference.columns:\n                ks_stat,\
          \ p_value = stats.ks_2samp(\n                    self.reference[col].dropna(),\n                    current_df[col].dropna()\n\
          \                )\n                drift_results[col] = {\n                    'ks_statistic': ks_stat,\n     \
          \               'p_value': p_value,\n                    'drift_detected': p_value < 0.05\n                }\n \
          \       return drift_results\n```"
        level3: "Volume anomaly:\n```python\nclass VolumeMonitor:\n    def __init__(self, history_window=30):\n        self.history\
          \ = []\n        self.window = history_window\n    \n    def check(self, row_count):\n        if len(self.history)\
          \ < 7:\n            self.history.append(row_count)\n            return {'status': 'collecting_baseline'}\n     \
          \   \n        mean = np.mean(self.history[-self.window:])\n        std = np.std(self.history[-self.window:])\n \
          \       z_score = (row_count - mean) / std if std > 0 else 0\n        \n        self.history.append(row_count)\n\
          \        return {\n            'row_count': row_count,\n            'expected': mean,\n            'z_score': z_score,\n\
          \            'anomaly': abs(z_score) > 3\n        }\n```"
      acceptance_criteria:
      - Detect statistical anomalies using Z-score and IQR methods on numerical data
      - Track metric trends over time and alert on significant deviations
      - Alert on sudden distribution changes that exceed configurable thresholds
      - Handle seasonality in time series data to reduce false positive alerts
    - name: Data Contracts
      description: Schema contracts and versioning
      skills:
      - Schema definition
      - Contract validation
      - Versioning
      deliverables:
      - Schema contract definition format in YAML with typed fields
      - Contract validation engine comparing data against contract specifications
      - Breaking change detection between contract versions automatically
      - Contract versioning with semantic versioning for compatibility tracking
      - Producer and consumer registration linking teams to their contracts
      - Contract testing framework for CI/CD pipeline integration
      hints:
        level1: |-
          Contract definition:
          ```yaml
          # data_contract.yaml
          name: user_events
          version: 1.0.0
          owner: data-team

          schema:
            columns:
              - name: user_id
                type: string
                required: true
              - name: event_type
                type: string
                required: true
                allowed_values: [click, view, purchase]
              - name: timestamp
                type: timestamp
                required: true
              - name: amount
                type: decimal
                required: false

          quality:
            - expect_column_values_not_null:
                column: user_id
            - expect_column_values_in_set:
                column: event_type
                values: [click, view, purchase]
          ```
        level2: "Contract validator:\n```python\nclass ContractValidator:\n    def __init__(self, contract_path):\n      \
          \  with open(contract_path) as f:\n            self.contract = yaml.safe_load(f)\n    \n    def validate_schema(self,\
          \ df):\n        errors = []\n        for col_spec in self.contract['schema']['columns']:\n            if col_spec['name']\
          \ not in df.columns:\n                if col_spec.get('required', False):\n                    errors.append(f\"\
          Missing required column: {col_spec['name']}\")\n            else:\n                actual_type = str(df[col_spec['name']].dtype)\n\
          \                if not self.types_compatible(actual_type, col_spec['type']):\n                    errors.append(f\"\
          Type mismatch for {col_spec['name']}\")\n        return errors\n```"
        level3: "Breaking change detection:\n```python\ndef detect_breaking_changes(old_contract, new_contract):\n    breaking\
          \ = []\n    \n    old_cols = {c['name']: c for c in old_contract['schema']['columns']}\n    new_cols = {c['name']:\
          \ c for c in new_contract['schema']['columns']}\n    \n    for name, spec in old_cols.items():\n        if name\
          \ not in new_cols:\n            breaking.append(f'Column removed: {name}')\n        elif new_cols[name]['type']\
          \ != spec['type']:\n            breaking.append(f'Type changed: {name}')\n        elif not spec.get('required')\
          \ and new_cols[name].get('required'):\n            breaking.append(f'Column became required: {name}')\n    \n  \
          \  return breaking\n```"
      acceptance_criteria:
      - Define schema contracts in YAML with field names, types, and constraints
      - Version contracts with semantic versioning for compatibility management
      - Validate incoming data against the active contract and report violations
      - Track contract violations over time with historical metrics and trends
  stream-processing-engine:
    name: Stream Processing Engine
    description: Build a real-time stream processing engine with windowing, state management, and exactly-once semantics like
      Flink
    category: Data Engineering
    difficulty: expert
    estimated_hours: 100
    skills:
    - Event time processing
    - Windowing
    - State management
    - Checkpointing
    - Watermarks
    - Exactly-once semantics
    prerequisites:
    - Distributed systems
    - Message queues
    - State machines
    learning_outcomes:
    - Understand stream processing fundamentals
    - Implement windowing and watermarks
    - Build fault-tolerant stateful processing
    - Design exactly-once delivery systems
    milestones:
    - name: Stream Abstraction
      description: Core stream data structures and operators
      skills:
      - Stream API
      - Operators
      - Transformations
      deliverables:
      - DataStream class representing an unbounded sequence of typed records
      - Map, filter, and flatMap operators transforming stream elements
      - KeyBy partitioning operator grouping stream elements by extracted key
      - Operator chaining mechanism composing transformations into a processing pipeline
      - Source and sink interfaces for pluggable data ingestion and output
      - Execution graph builder that compiles operator chain into a runnable DAG
      hints:
        level1: "DataStream API:\n```python\nclass DataStream:\n    def __init__(self, source, operators=None):\n        self.source\
          \ = source\n        self.operators = operators or []\n    \n    def map(self, fn):\n        return DataStream(self.source,\
          \ self.operators + [MapOperator(fn)])\n    \n    def filter(self, fn):\n        return DataStream(self.source, self.operators\
          \ + [FilterOperator(fn)])\n    \n    def key_by(self, key_fn):\n        return KeyedStream(self.source, self.operators,\
          \ key_fn)\n```"
        level2: "Operators:\n```python\nclass Operator:\n    def process(self, element):\n        raise NotImplementedError\n\
          \nclass MapOperator(Operator):\n    def __init__(self, fn):\n        self.fn = fn\n    \n    def process(self, element):\n\
          \        yield self.fn(element)\n\nclass FilterOperator(Operator):\n    def __init__(self, predicate):\n       \
          \ self.predicate = predicate\n    \n    def process(self, element):\n        if self.predicate(element):\n     \
          \       yield element\n```"
        level3: "Execution graph:\n```python\nclass StreamExecutionEnvironment:\n    def __init__(self):\n        self.sources\
          \ = []\n    \n    def add_source(self, source):\n        stream = DataStream(source)\n        self.sources.append(stream)\n\
          \        return stream\n    \n    def execute(self):\n        graph = self.build_graph()\n        for task in graph.tasks:\n\
          \            self.submit_task(task)\n```"
      acceptance_criteria:
      - Define stream data structure with typed record schema and metadata
      - Support operators map, filter, and flatMap with correct element-wise semantics
      - Handle operator chaining producing a composed pipeline from sequential calls
      - Implement parallel processing across partitions for keyed streams
    - name: Windowing
      description: Time-based windowing for aggregations
      skills:
      - Window types
      - Triggers
      - Evictors
      deliverables:
      - Tumbling window assigner grouping elements into fixed non-overlapping intervals
      - Sliding window assigner grouping elements into overlapping intervals with configurable slide
      - Session window assigner merging elements within an inactivity gap threshold
      - Window assigner interface abstracting window assignment from processing logic
      - Triggers for count-based and time-based window evaluation firing
      - Late data handler routing out-of-window elements to a side output
      hints:
        level1: "Window types:\n```python\nclass TumblingWindow:\n    def __init__(self, size_ms):\n        self.size = size_ms\n\
          \    \n    def assign(self, timestamp):\n        start = timestamp - (timestamp % self.size)\n        return Window(start,\
          \ start + self.size)\n\nclass SlidingWindow:\n    def __init__(self, size_ms, slide_ms):\n        self.size = size_ms\n\
          \        self.slide = slide_ms\n    \n    def assign(self, timestamp):\n        windows = []\n        start = timestamp\
          \ - (timestamp % self.slide)\n        while start > timestamp - self.size:\n            windows.append(Window(start,\
          \ start + self.size))\n            start -= self.slide\n        return windows\n```"
        level2: "Windowed stream:\n```python\nclass WindowedStream:\n    def __init__(self, keyed_stream, window_assigner):\n\
          \        self.keyed_stream = keyed_stream\n        self.window_assigner = window_assigner\n        self.window_buffers\
          \ = defaultdict(lambda: defaultdict(list))\n    \n    def process(self, element):\n        key = self.keyed_stream.key_fn(element)\n\
          \        windows = self.window_assigner.assign(element.timestamp)\n        for window in windows:\n            self.window_buffers[key][window].append(element)\n\
          \    \n    def reduce(self, fn):\n        return WindowReduceStream(self, fn)\n```"
        level3: "Triggers:\n```python\nclass Trigger:\n    def on_element(self, element, window, ctx):\n        raise NotImplementedError\n\
          \    \n    def on_processing_time(self, time, window, ctx):\n        raise NotImplementedError\n\nclass EventTimeTrigger(Trigger):\n\
          \    def on_element(self, element, window, ctx):\n        if ctx.current_watermark >= window.end:\n            return\
          \ TriggerResult.FIRE_AND_PURGE\n        ctx.register_event_time_timer(window.end)\n        return TriggerResult.CONTINUE\n\
          ```"
      acceptance_criteria:
      - Support tumbling windows with fixed time duration and correct boundary alignment
      - Support sliding windows with configurable window size and slide interval
      - Support session windows that merge when gap between events is below threshold
      - Handle window triggers firing computation at correct count or time thresholds
    - name: Event Time & Watermarks
      description: Handle out-of-order events with watermarks
      skills:
      - Event time
      - Watermark generation
      - Lateness
      deliverables:
      - Event time extractor pulling timestamps from record payload fields
      - Watermark generation strategies including periodic and punctuated approaches
      - Bounded out-of-orderness watermark allowing configurable maximum event delay
      - Watermark propagation logic forwarding minimum watermark across parallel operators
      - Idle source detection advancing watermarks when a partition has no new data
      - Allowed lateness configuration specifying how long late events are accepted
      hints:
        level1: "Watermark generator:\n```python\nclass WatermarkGenerator:\n    def __init__(self, max_out_of_orderness):\n\
          \        self.max_ooo = max_out_of_orderness\n        self.max_timestamp = 0\n    \n    def on_event(self, event,\
          \ timestamp):\n        self.max_timestamp = max(self.max_timestamp, timestamp)\n    \n    def get_watermark(self):\n\
          \        return Watermark(self.max_timestamp - self.max_ooo)\n```"
        level2: "Watermark propagation:\n```python\nclass WatermarkCoordinator:\n    def __init__(self, sources):\n      \
          \  self.source_watermarks = {s: 0 for s in sources}\n    \n    def update(self, source, watermark):\n        self.source_watermarks[source]\
          \ = watermark\n        # Min watermark across all sources\n        return min(self.source_watermarks.values())\n\
          \    \n    def handle_idle_source(self, source, timeout_ms):\n        # Mark source as idle, exclude from min calculation\n\
          \        if time.time() - self.last_event[source] > timeout_ms:\n            self.source_watermarks[source] = float('inf')\n\
          ```"
        level3: "Late data handling:\n```python\nclass WindowOperator:\n    def __init__(self, allowed_lateness=0):\n    \
          \    self.allowed_lateness = allowed_lateness\n        self.late_output = SideOutput('late')\n    \n    def process_watermark(self,\
          \ watermark):\n        # Fire windows that are complete\n        for window in self.pending_windows:\n         \
          \   if watermark.timestamp >= window.end:\n                self.fire_window(window)\n        \n        # Clean up\
          \ windows past allowed lateness\n        cleanup_time = watermark.timestamp - self.allowed_lateness\n        self.cleanup_windows_before(cleanup_time)\n\
          \    \n    def process_element(self, element):\n        windows = self.assigner.assign(element.timestamp)\n    \
          \    for window in windows:\n            if self.is_late(element, window):\n                self.late_output.emit(element)\n\
          \            else:\n                self.add_to_window(element, window)\n```"
      acceptance_criteria:
      - Track event time vs processing time
      - Generate watermarks that monotonically advance based on observed event timestamps
      - Handle late events by routing them to a side output or late-data handler
      - Support allowed lateness so windows accept events arriving after watermark passes
    - name: Stateful Processing
      description: Manage operator state with checkpointing
      skills:
      - State backends
      - Keyed state
      - Checkpointing
      deliverables:
      - ValueState, ListState, and MapState abstractions for keyed operator state
      - State backend interface abstracting storage behind memory or persistent backends
      - RocksDB state backend storing large keyed state on local disk with caching
      - Checkpoint barrier injection coordinating consistent snapshots across operators
      - Async checkpointing writing state snapshots without blocking record processing
      - State recovery loader restoring operator state from the latest checkpoint on restart
      hints:
        level1: "State abstractions:\n```python\nclass ValueState:\n    def __init__(self, backend, key, name):\n        self.backend\
          \ = backend\n        self.key = key\n        self.name = name\n    \n    def value(self):\n        return self.backend.get(self.key,\
          \ self.name)\n    \n    def update(self, value):\n        self.backend.put(self.key, self.name, value)\n\nclass\
          \ KeyedStateBackend:\n    def __init__(self):\n        self.state = defaultdict(dict)  # key -> {name -> value}\n\
          \    \n    def get(self, key, name):\n        return self.state[key].get(name)\n    \n    def put(self, key, name,\
          \ value):\n        self.state[key][name] = value\n```"
        level2: "Checkpoint barriers:\n```python\nclass CheckpointBarrier:\n    def __init__(self, checkpoint_id, timestamp):\n\
          \        self.checkpoint_id = checkpoint_id\n        self.timestamp = timestamp\n\nclass BarrierHandler:\n    def\
          \ __init__(self, num_inputs):\n        self.pending_barriers = {}\n        self.num_inputs = num_inputs\n    \n\
          \    def process_barrier(self, barrier, input_channel):\n        cp_id = barrier.checkpoint_id\n        if cp_id\
          \ not in self.pending_barriers:\n            self.pending_barriers[cp_id] = set()\n        \n        self.pending_barriers[cp_id].add(input_channel)\n\
          \        \n        if len(self.pending_barriers[cp_id]) == self.num_inputs:\n            # All barriers received\
          \ - trigger checkpoint\n            self.trigger_checkpoint(cp_id)\n            del self.pending_barriers[cp_id]\n\
          ```"
        level3: "RocksDB state backend:\n```python\nimport rocksdb\n\nclass RocksDBStateBackend:\n    def __init__(self, path):\n\
          \        self.db = rocksdb.DB(path, rocksdb.Options(create_if_missing=True))\n    \n    def get(self, key, name):\n\
          \        full_key = f'{key}:{name}'.encode()\n        value = self.db.get(full_key)\n        return pickle.loads(value)\
          \ if value else None\n    \n    def put(self, key, name, value):\n        full_key = f'{key}:{name}'.encode()\n\
          \        self.db.put(full_key, pickle.dumps(value))\n    \n    def snapshot(self, checkpoint_path):\n        checkpoint\
          \ = rocksdb.Checkpoint(self.db)\n        checkpoint.create_checkpoint(checkpoint_path)\n```"
      acceptance_criteria:
      - Maintain operator state across records with correct per-key isolation
      - Implement checkpointing that captures globally consistent state snapshot
      - Support state recovery restoring exact pre-failure state from checkpoint
      - Handle state migration when operator parallelism changes during rescale
    - name: Exactly-Once Semantics
      description: Guarantee exactly-once processing
      skills:
      - Two-phase commit
      - Idempotent sinks
      - Transaction coordination
      deliverables:
      - Transactional source connectors supporting offset commit on checkpoint completion
      - Two-phase commit sink connectors pre-committing output and finalizing on checkpoint
      - Checkpoint completion callbacks notifying sources and sinks of successful snapshots
      - Transaction coordinator managing distributed commit across all sink operators
      - Abort and recovery logic rolling back uncommitted transactions after failure
      - End-to-end exactly-once delivery guarantee from source through sink
      hints:
        level1: "Two-phase commit sink:\n```python\nclass TwoPhaseCommitSink:\n    def __init__(self):\n        self.pending_transactions\
          \ = {}\n    \n    def invoke(self, value, context):\n        # Write to pending transaction\n        txn = self.current_transaction()\n\
          \        txn.write(value)\n    \n    def snapshot_state(self, checkpoint_id):\n        # Pre-commit: flush but don't\
          \ commit\n        txn = self.current_transaction()\n        txn.pre_commit()\n        self.pending_transactions[checkpoint_id]\
          \ = txn\n        self.begin_new_transaction()\n    \n    def notify_checkpoint_complete(self, checkpoint_id):\n\
          \        # Commit on checkpoint success\n        txn = self.pending_transactions.pop(checkpoint_id)\n        txn.commit()\n\
          ```"
        level2: "Kafka exactly-once:\n```python\nclass KafkaExactlyOnceSink(TwoPhaseCommitSink):\n    def __init__(self, bootstrap_servers):\n\
          \        self.producer = KafkaProducer(\n            bootstrap_servers=bootstrap_servers,\n            transactional_id='my-transactional-id'\n\
          \        )\n        self.producer.init_transactions()\n    \n    def begin_transaction(self):\n        self.producer.begin_transaction()\n\
          \    \n    def pre_commit(self):\n        self.producer.flush()\n    \n    def commit(self):\n        self.producer.commit_transaction()\n\
          \    \n    def abort(self):\n        self.producer.abort_transaction()\n```"
        level3: "Transaction coordinator:\n```python\nclass TransactionCoordinator:\n    def __init__(self, sinks):\n    \
          \    self.sinks = sinks\n    \n    def on_checkpoint_complete(self, checkpoint_id):\n        # Phase 2: Commit all\
          \ sinks\n        for sink in self.sinks:\n            try:\n                sink.commit(checkpoint_id)\n       \
          \     except Exception as e:\n                # If any commit fails, need to handle recovery\n                self.handle_commit_failure(checkpoint_id,\
          \ sink, e)\n    \n    def on_checkpoint_abort(self, checkpoint_id):\n        for sink in self.sinks:\n         \
          \   sink.abort(checkpoint_id)\n```"
      acceptance_criteria:
      - Implement idempotent sinks that produce the same output on replay
      - Support transactional writes that commit atomically with checkpoint completion
      - Handle checkpoint barriers aligning across parallel operator instances
      - Verify no duplicate output records appear after crash recovery and replay
  gitops-deployment:
    name: GitOps Deployment System
    description: Build a GitOps deployment system with Git as source of truth, like ArgoCD
    category: Cloud Native
    difficulty: advanced
    estimated_hours: 55
    skills:
    - Git operations
    - Kubernetes API
    - Reconciliation
    - Diff detection
    - Rollback
    - Health checks
    prerequisites:
    - Kubernetes basics
    - Git
    - YAML/Helm
    learning_outcomes:
    - Understand GitOps principles
    - Implement Git-driven deployments
    - Build sync and health monitoring
    - Handle rollback and recovery
    milestones:
    - name: Git Repository Sync
      description: Clone and sync Git repositories
      skills:
      - Git operations
      - Polling/webhooks
      - Credential management
      deliverables:
      - Repository cloning from remote URL with branch selection
      - Branch and tag tracking for monitoring multiple ref targets
      - Polling for changes with configurable interval and commit comparison
      - Webhook receiver for immediate sync on push event notification
      - SSH and HTTPS credential management for private repository access
      - Repository caching to avoid redundant full clones on each sync
      hints:
        level1: "Git sync:\n```python\nimport git\n\nclass GitRepoSync:\n    def __init__(self, url, branch, path, credentials=None):\n\
          \        self.url = url\n        self.branch = branch\n        self.path = path\n        self.credentials = credentials\n\
          \    \n    def sync(self):\n        if os.path.exists(self.path):\n            repo = git.Repo(self.path)\n    \
          \        repo.remotes.origin.pull()\n        else:\n            repo = git.Repo.clone_from(self.url, self.path,\
          \ branch=self.branch)\n        return repo.head.commit.hexsha\n```"
        level2: "Webhook handler:\n```python\n@app.post('/webhook/github')\nasync def github_webhook(request: Request):\n\
          \    payload = await request.json()\n    \n    if request.headers.get('X-GitHub-Event') == 'push':\n        repo_url\
          \ = payload['repository']['clone_url']\n        branch = payload['ref'].split('/')[-1]\n        commit = payload['after']\n\
          \        \n        # Trigger sync for matching applications\n        apps = await get_apps_for_repo(repo_url, branch)\n\
          \        for app in apps:\n            await trigger_sync(app, commit)\n    \n    return {'status': 'ok'}\n```"
        level3: "Credential management:\n```python\nclass CredentialStore:\n    def __init__(self, k8s_client):\n        self.k8s\
          \ = k8s_client\n    \n    async def get_credentials(self, secret_name, namespace):\n        secret = await self.k8s.read_namespaced_secret(secret_name,\
          \ namespace)\n        \n        if 'sshPrivateKey' in secret.data:\n            return SSHCredentials(\n       \
          \         private_key=base64.b64decode(secret.data['sshPrivateKey'])\n            )\n        elif 'username' in\
          \ secret.data:\n            return HTTPSCredentials(\n                username=base64.b64decode(secret.data['username']).decode(),\n\
          \                password=base64.b64decode(secret.data['password']).decode()\n            )\n```"
      acceptance_criteria:
      - Clone Git repository from remote URL with correct branch checked out
      - Poll for changes periodically and detect new commits since last sync
      - Support webhook triggers for immediate sync without polling delay
      - Track current synced commit SHA for comparison with remote HEAD
    - name: Manifest Generation
      description: Generate Kubernetes manifests from source
      skills:
      - YAML processing
      - Helm
      - Kustomize
      deliverables:
      - Plain YAML reading and parsing for raw Kubernetes manifests
      - Helm template rendering from chart with values file overrides
      - Kustomize build processing overlays and patches into final manifests
      - Parameter overrides for environment-specific value customization
      - Environment-specific values supporting dev, staging, and production configs
      - Manifest validation checking resource schema before deployment
      hints:
        level1: |-
          Manifest generator interface:
          ```python
          class ManifestGenerator:
              def generate(self, source_path, params) -> List[dict]:
                  raise NotImplementedError

          class PlainYAMLGenerator(ManifestGenerator):
              def generate(self, source_path, params):
                  manifests = []
                  for file in glob.glob(f'{source_path}/**/*.yaml', recursive=True):
                      with open(file) as f:
                          for doc in yaml.safe_load_all(f):
                              if doc:
                                  manifests.append(doc)
                  return manifests
          ```
        level2: "Helm template:\n```python\nclass HelmGenerator(ManifestGenerator):\n    def generate(self, chart_path, params):\n\
          \        values_file = self.write_values(params.get('values', {}))\n        \n        cmd = [\n            'helm',\
          \ 'template',\n            params.get('release_name', 'release'),\n            chart_path,\n            '-f', values_file,\n\
          \            '--namespace', params.get('namespace', 'default')\n        ]\n        \n        result = subprocess.run(cmd,\
          \ capture_output=True, text=True)\n        if result.returncode != 0:\n            raise HelmError(result.stderr)\n\
          \        \n        return list(yaml.safe_load_all(result.stdout))\n```"
        level3: "Kustomize:\n```python\nclass KustomizeGenerator(ManifestGenerator):\n    def generate(self, kustomize_path,\
          \ params):\n        # Apply parameter overrides\n        if params.get('images'):\n            for image in params['images']:\n\
          \                subprocess.run([\n                    'kustomize', 'edit', 'set', 'image',\n                  \
          \  f\"{image['name']}={image['newName']}:{image['newTag']}\"\n                ], cwd=kustomize_path)\n        \n\
          \        result = subprocess.run(\n            ['kustomize', 'build', kustomize_path],\n            capture_output=True,\
          \ text=True\n        )\n        return list(yaml.safe_load_all(result.stdout))\n```"
      acceptance_criteria:
      - Support plain Kubernetes YAML manifest files read from repository
      - Support Kustomize overlays generating manifests from base and patches
      - Support Helm chart rendering with configurable values and release names
      - Validate generated manifests against Kubernetes schema before applying
    - name: Sync & Reconciliation
      description: Apply manifests and reconcile state
      skills:
      - K8s apply
      - Diff detection
      - Pruning
      deliverables:
      - Manifest diff calculation comparing desired state against live cluster
      - Apply with server-side apply for declarative resource management
      - Resource pruning removing orphaned resources not in desired state
      - Sync waves and hooks for ordered multi-phase deployment execution
      - Selective sync allowing targeted resource-level sync operations
      - Dry-run mode previewing changes without modifying cluster state
      hints:
        level1: "Sync application:\n```python\nclass Syncer:\n    def __init__(self, k8s_client):\n        self.k8s = k8s_client\n\
          \    \n    async def sync(self, app, manifests):\n        results = []\n        for manifest in manifests:\n   \
          \         result = await self.apply_manifest(manifest)\n            results.append(result)\n        \n        if\
          \ app.spec.sync_policy.prune:\n            await self.prune_orphans(app, manifests)\n        \n        return SyncResult(results)\n\
          \    \n    async def apply_manifest(self, manifest):\n        api = self.get_api_for_kind(manifest['kind'])\n  \
          \      try:\n            await api.patch(\n                name=manifest['metadata']['name'],\n                namespace=manifest['metadata'].get('namespace'),\n\
          \                body=manifest,\n                field_manager='gitops-controller'\n            )\n            return\
          \ ApplyResult(manifest, 'synced')\n        except Exception as e:\n            return ApplyResult(manifest, 'failed',\
          \ str(e))\n```"
        level2: "Diff calculation:\n```python\ndef calculate_diff(self, desired, live):\n    diffs = []\n    \n    desired_map\
          \ = {self.resource_key(m): m for m in desired}\n    live_map = {self.resource_key(m): m for m in live}\n    \n \
          \   for key, manifest in desired_map.items():\n        if key not in live_map:\n            diffs.append(Diff(manifest,\
          \ None, 'add'))\n        else:\n            live_manifest = live_map[key]\n            if not self.equal(manifest,\
          \ live_manifest):\n                diffs.append(Diff(manifest, live_manifest, 'modify'))\n    \n    for key, manifest\
          \ in live_map.items():\n        if key not in desired_map:\n            diffs.append(Diff(None, manifest, 'delete'))\n\
          \    \n    return diffs\n```"
        level3: "Sync waves:\n```python\ndef group_by_wave(self, manifests):\n    waves = defaultdict(list)\n    for m in\
          \ manifests:\n        wave = int(m.get('metadata', {}).get('annotations', {}).get('argocd.io/sync-wave', '0'))\n\
          \        waves[wave].append(m)\n    return sorted(waves.items())\n\nasync def sync_with_waves(self, app, manifests):\n\
          \    for wave_num, wave_manifests in self.group_by_wave(manifests):\n        # Apply all in wave\n        results\
          \ = await asyncio.gather(*[\n            self.apply_manifest(m) for m in wave_manifests\n        ])\n        \n\
          \        # Wait for health before next wave\n        await self.wait_for_health(wave_manifests)\n```"
      acceptance_criteria:
      - Compare desired state from git with actual state from cluster accurately
      - Apply diffs to reach desired state using server-side apply mechanism
      - Handle resource creation, update, and deletion during reconciliation cycle
      - Support sync hooks for pre-sync and post-sync custom resource execution
    - name: Health Assessment
      description: Monitor application and resource health
      skills:
      - Health checks
      - Status aggregation
      - Custom health
      deliverables:
      - Built-in health checks per Kubernetes resource kind (Deployment, Service, etc.)
      - Deployment rollout status monitoring tracking replica readiness
      - Custom health scripts for application-specific health evaluation
      - Application health aggregation combining individual resource health status
      - Degraded vs healthy vs progressing state classification for resources
      - Health history tracking status changes over time for trend analysis
      hints:
        level1: "Health check per resource type:\n```python\nclass HealthChecker:\n    def check(self, resource) -> HealthStatus:\n\
          \        kind = resource['kind']\n        checker = getattr(self, f'check_{kind.lower()}', self.check_default)\n\
          \        return checker(resource)\n    \n    def check_deployment(self, deployment):\n        status = deployment.get('status',\
          \ {})\n        replicas = status.get('replicas', 0)\n        ready = status.get('readyReplicas', 0)\n        updated\
          \ = status.get('updatedReplicas', 0)\n        \n        if ready == replicas == updated:\n            return HealthStatus.HEALTHY\n\
          \        elif updated < replicas:\n            return HealthStatus.PROGRESSING\n        else:\n            return\
          \ HealthStatus.DEGRADED\n```"
        level2: "Aggregate application health:\n```python\ndef aggregate_health(self, resources):\n    statuses = [self.check(r)\
          \ for r in resources]\n    \n    if all(s == HealthStatus.HEALTHY for s in statuses):\n        return HealthStatus.HEALTHY\n\
          \    elif any(s == HealthStatus.DEGRADED for s in statuses):\n        return HealthStatus.DEGRADED\n    elif any(s\
          \ == HealthStatus.PROGRESSING for s in statuses):\n        return HealthStatus.PROGRESSING\n    else:\n        return\
          \ HealthStatus.UNKNOWN\n```"
        level3: "Custom health check (Lua script):\n```python\nimport lupa\n\nclass CustomHealthChecker:\n    def __init__(self):\n\
          \        self.lua = lupa.LuaRuntime()\n    \n    def check_with_script(self, resource, script):\n        func =\
          \ self.lua.eval(f'''\n            function(obj)\n                {script}\n            end\n        ''')\n     \
          \   result = func(resource)\n        return HealthStatus(result['status']), result.get('message')\n\n# Example Lua\
          \ script:\n# if obj.status.phase == \"Running\" then\n#     return {status = \"Healthy\"}\n# else\n#     return\
          \ {status = \"Progressing\", message = \"Waiting for pod\"}\n# end\n```"
      acceptance_criteria:
      - Monitor deployed resource health using kind-specific status field inspection
      - Detect degraded, progressing, and healthy states based on resource status
      - Track sync errors and warnings with detailed messages for troubleshooting
      - Support custom health checks using user-defined evaluation scripts or rules
    - name: Rollback & History
      description: Track history and enable rollback
      skills:
      - Version tracking
      - Rollback
      - Audit
      deliverables:
      - Revision history storage recording deployed commits and manifest hashes
      - Rollback to specific revision re-deploying previous manifest set
      - Auto-rollback on failure reverting to last known good deployment
      - Deployment annotations recording sync metadata on deployed resources
      - Audit logging tracking who deployed what and when for compliance
      - Sync status tracking showing current and historical sync outcomes
      hints:
        level1: "Revision history:\n```python\n@dataclass\nclass Revision:\n    id: int\n    commit_sha: str\n    deployed_at:\
          \ datetime\n    manifests_hash: str\n    sync_status: str\n\nclass RevisionHistory:\n    def __init__(self, app_name,\
          \ storage):\n        self.app_name = app_name\n        self.storage = storage\n    \n    def record(self, commit_sha,\
          \ manifests):\n        revision = Revision(\n            id=self.next_id(),\n            commit_sha=commit_sha,\n\
          \            deployed_at=datetime.utcnow(),\n            manifests_hash=hash_manifests(manifests),\n           \
          \ sync_status='synced'\n        )\n        self.storage.save(self.app_name, revision)\n        return revision\n\
          ```"
        level2: "Rollback:\n```python\nasync def rollback(self, app_name, revision_id):\n    revision = self.history.get(app_name,\
          \ revision_id)\n    \n    # Get manifests from that revision\n    manifests = await self.get_manifests_at_commit(app_name,\
          \ revision.commit_sha)\n    \n    # Sync to those manifests\n    result = await self.syncer.sync(app_name, manifests)\n\
          \    \n    # Record as new revision\n    new_revision = self.history.record(\n        revision.commit_sha,\n   \
          \     manifests,\n        metadata={'rollback_from': self.history.current(app_name).id}\n    )\n    \n    return\
          \ new_revision\n```"
        level3: "Auto-rollback on degraded:\n```python\nclass AutoRollbackPolicy:\n    def __init__(self, health_threshold_seconds=300):\n\
          \        self.threshold = health_threshold_seconds\n        self.degraded_since = {}\n    \n    async def check(self,\
          \ app):\n        health = await self.health_checker.check_app(app)\n        \n        if health == HealthStatus.DEGRADED:\n\
          \            if app.name not in self.degraded_since:\n                self.degraded_since[app.name] = time.time()\n\
          \            elif time.time() - self.degraded_since[app.name] > self.threshold:\n                # Auto-rollback\n\
          \                previous = self.history.previous(app.name)\n                if previous:\n                    await\
          \ self.rollback(app.name, previous.id)\n                    await self.notify(f'Auto-rolled back {app.name}')\n\
          \        else:\n            self.degraded_since.pop(app.name, None)\n```"
      acceptance_criteria:
      - Store deployment history with commit SHA, timestamp, and sync result
      - Support rollback to any previous revision by re-syncing its manifest set
      - Show diff between any two revisions for change review and comparison
      - Track who deployed what and when for audit and compliance requirements
  serverless-runtime:
    name: Serverless Function Runtime
    description: Build a serverless function runtime with cold start optimization, auto-scaling, and isolation
    category: Cloud Native
    difficulty: expert
    estimated_hours: 80
    skills:
    - Container isolation
    - Cold start optimization
    - Auto-scaling
    - Request routing
    - Resource limits
    - Function lifecycle
    prerequisites:
    - Containers
    - HTTP servers
    - Process management
    learning_outcomes:
    - Understand serverless architecture
    - Implement function isolation and sandboxing
    - Build efficient cold start mechanisms
    - Design auto-scaling systems
    milestones:
    - name: Function Packaging
      description: Package and store function code
      skills:
      - Code packaging
      - Dependency resolution
      - Storage
      deliverables:
      - Function definition format specifying handler, runtime, and configuration
      - Code upload API accepting function source archive
      - Dependency bundling packaging function code with required libraries
      - Runtime selection supporting Python, Node.js, and Go environments
      - Version management tracking deployed function code revisions
      - Code storage backend persisting function packages to S3 or local disk
      hints:
        level1: "Function definition:\n```python\n@dataclass\nclass FunctionDefinition:\n    name: str\n    runtime: str \
          \ # python3.9, nodejs18, etc.\n    handler: str  # module.function\n    memory_mb: int = 128\n    timeout_seconds:\
          \ int = 30\n    environment: dict = None\n    code_uri: str = None  # s3://bucket/code.zip\n\nclass FunctionRegistry:\n\
          \    def create(self, definition: FunctionDefinition, code_bytes: bytes):\n        # Store code\n        code_uri\
          \ = self.storage.upload(f'functions/{definition.name}/{uuid4()}.zip', code_bytes)\n        definition.code_uri =\
          \ code_uri\n        \n        # Save definition\n        self.db.functions.insert(asdict(definition))\n        return\
          \ definition\n```"
        level2: "Build function container image:\n```python\nclass FunctionBuilder:\n    RUNTIME_IMAGES = {\n        'python3.9':\
          \ 'python:3.9-slim',\n        'python3.11': 'python:3.11-slim',\n        'nodejs18': 'node:18-slim',\n    }\n  \
          \  \n    def build(self, function):\n        dockerfile = f'''\n            FROM {self.RUNTIME_IMAGES[function.runtime]}\n\
          \            COPY bootstrap /bootstrap\n            COPY code /var/task\n            WORKDIR /var/task\n       \
          \     RUN pip install -r requirements.txt || true\n            CMD [\"/bootstrap\"]\n        '''\n        \n   \
          \     # Build image\n        image_tag = f'functions/{function.name}:{function.version}'\n        self.docker.build(dockerfile,\
          \ tag=image_tag)\n        return image_tag\n```"
        level3: "Runtime bootstrap:\n```python\n# bootstrap.py - Runs inside container\nimport importlib\nimport json\nimport\
          \ sys\n\ndef main():\n    handler_path = os.environ['_HANDLER']  # module.function\n    module_name, func_name =\
          \ handler_path.rsplit('.', 1)\n    \n    module = importlib.import_module(module_name)\n    handler = getattr(module,\
          \ func_name)\n    \n    # Wait for invocations via HTTP\n    while True:\n        event = get_next_invocation()\
          \  # HTTP call to runtime API\n        try:\n            result = handler(event['body'], event['context'])\n   \
          \         send_response(event['request_id'], result)\n        except Exception as e:\n            send_error(event['request_id'],\
          \ str(e))\n```"
      acceptance_criteria:
      - Package function source code together with declared dependencies
      - Store packaged function artifacts in durable backend storage
      - Support multiple language runtimes including Python, Node, and Go
      - Handle function versioning with immutable version identifiers
    - name: Execution Environment
      description: Isolated function execution with resource limits
      skills:
      - Container runtime
      - Resource limits
      - Isolation
      deliverables:
      - Container-based isolation running each function in separate environment
      - Memory limit enforcement capping function RAM consumption
      - CPU limit enforcement throttling function processor usage
      - Network isolation restricting function outbound connectivity
      - Filesystem isolation providing clean writable /tmp per invocation
      - Execution timeout killing function after configurable deadline
      hints:
        level1: "Create execution sandbox:\n```python\nimport docker\n\nclass Sandbox:\n    def __init__(self, function):\n\
          \        self.function = function\n        self.client = docker.from_env()\n        self.container = None\n    \n\
          \    def start(self):\n        self.container = self.client.containers.run(\n            image=self.function.image,\n\
          \            detach=True,\n            mem_limit=f'{self.function.memory_mb}m',\n            cpu_period=100000,\n\
          \            cpu_quota=int(self.function.memory_mb / 1769 * 100000),  # Proportional CPU\n            network_mode='none',\
          \  # No network by default\n            read_only=True,\n            environment=self.function.environment\n   \
          \     )\n```"
        level2: "Execution with timeout:\n```python\nimport asyncio\n\nclass FunctionExecutor:\n    async def invoke(self,\
          \ sandbox, event):\n        try:\n            result = await asyncio.wait_for(\n                self.send_invocation(sandbox,\
          \ event),\n                timeout=sandbox.function.timeout_seconds\n            )\n            return InvocationResult(status='success',\
          \ body=result)\n        except asyncio.TimeoutError:\n            sandbox.kill()\n            return InvocationResult(status='timeout')\n\
          \        except Exception as e:\n            return InvocationResult(status='error', error=str(e))\n    \n    async\
          \ def send_invocation(self, sandbox, event):\n        # Send event to container's runtime API\n        async with\
          \ aiohttp.ClientSession() as session:\n            resp = await session.post(\n                f'http://{sandbox.ip}:8080/invoke',\n\
          \                json=event\n            )\n            return await resp.json()\n```"
        level3: |-
          gVisor for stronger isolation:
          ```python
          class GVisorSandbox(Sandbox):
              def start(self):
                  # Use gVisor runsc runtime
                  self.container = self.client.containers.run(
                      image=self.function.image,
                      detach=True,
                      runtime='runsc',  # gVisor
                      mem_limit=f'{self.function.memory_mb}m',
                      security_opt=['no-new-privileges'],
                      cap_drop=['ALL'],
                  )
          ```
      acceptance_criteria:
      - Create isolated execution environment for each function invocation
      - Inject function handler code into prepared runtime container
      - Enforce memory and CPU resource limits on running functions
      - Terminate function execution after configurable timeout period
    - name: Cold Start Optimization
      description: Minimize function startup latency
      skills:
      - Warm pools
      - Snapshotting
      - Pre-warming
      deliverables:
      - Warm container pool maintaining pre-initialized environments ready for use
      - Container reuse assigning returning requests to previously used environments
      - Pre-initialization loading runtime and dependencies before first request
      - Snapshot and restore using CRIU for instant environment restoration
      - Predictive warming pre-creating containers based on traffic patterns
      - Shared layer caching reusing common runtime and dependency layers
      hints:
        level1: "Warm pool:\n```python\nclass WarmPool:\n    def __init__(self, max_idle_per_function=3):\n        self.pools\
          \ = defaultdict(list)  # function_name -> [Sandbox]\n        self.max_idle = max_idle_per_function\n    \n    def\
          \ acquire(self, function_name):\n        pool = self.pools[function_name]\n        if pool:\n            return\
          \ pool.pop()\n        return None  # Need cold start\n    \n    def release(self, sandbox):\n        pool = self.pools[sandbox.function.name]\n\
          \        if len(pool) < self.max_idle:\n            sandbox.reset()  # Clear state\n            pool.append(sandbox)\n\
          \        else:\n            sandbox.destroy()\n```"
        level2: "Pre-warming based on schedule:\n```python\nclass PreWarmer:\n    def __init__(self, warm_pool, metrics):\n\
          \        self.pool = warm_pool\n        self.metrics = metrics\n    \n    async def run(self):\n        while True:\n\
          \            # Analyze invocation patterns\n            patterns = self.metrics.get_invocation_patterns()\n    \
          \        \n            for function_name, predicted_count in patterns.items():\n                current = len(self.pool.pools[function_name])\n\
          \                needed = min(predicted_count, self.pool.max_idle) - current\n                \n               \
          \ if needed > 0:\n                    function = await self.get_function(function_name)\n                    for\
          \ _ in range(needed):\n                        sandbox = await self.create_warm_sandbox(function)\n            \
          \            self.pool.release(sandbox)\n            \n            await asyncio.sleep(60)\n```"
        level3: "Snapshot with CRIU:\n```python\nclass SnapshotManager:\n    def create_snapshot(self, sandbox, snapshot_path):\n\
          \        # Checkpoint container state with CRIU\n        subprocess.run([\n            'docker', 'checkpoint', 'create',\n\
          \            '--checkpoint-dir', snapshot_path,\n            sandbox.container.id,\n            'init-snapshot'\n\
          \        ])\n    \n    def restore_from_snapshot(self, function, snapshot_path):\n        container = subprocess.run([\n\
          \            'docker', 'start',\n            '--checkpoint', 'init-snapshot',\n            '--checkpoint-dir', snapshot_path,\n\
          \            function.container_id\n        ])\n        return Sandbox(function, container)\n\n# Create snapshot\
          \ after init, restore for each invocation\n```"
      acceptance_criteria:
      - Pre-warm execution environments to reduce cold start latency
      - Implement container snapshot and restore for sub-100ms cold starts
      - Cache warm container instances for reuse by subsequent invocations
      - Track cold start frequency and duration in metrics dashboard
    - name: Request Routing
      description: Route requests to function instances
      skills:
      - Load balancing
      - Request queuing
      - Concurrency
      deliverables:
      - HTTP gateway accepting inbound function invocation requests
      - Per-function request queue buffering incoming invocations
      - Concurrency limit enforcement capping parallel executions per function
      - Load balancing distributing requests across available instances
      - Request timeout returning error if function does not respond in time
      - Asynchronous invocation mode queuing request for background execution
      hints:
        level1: "Gateway routing:\n```python\n@app.post('/functions/{function_name}/invoke')\nasync def invoke(function_name:\
          \ str, request: Request):\n    function = await get_function(function_name)\n    if not function:\n        raise\
          \ HTTPException(404, 'Function not found')\n    \n    # Get or create sandbox\n    sandbox = warm_pool.acquire(function_name)\n\
          \    if not sandbox:\n        sandbox = await create_sandbox(function)\n    \n    try:\n        event = {\n    \
          \        'body': await request.body(),\n            'headers': dict(request.headers),\n            'method': request.method\n\
          \        }\n        result = await executor.invoke(sandbox, event)\n        return Response(content=result.body,\
          \ status_code=200)\n    finally:\n        warm_pool.release(sandbox)\n```"
        level2: "Concurrency control:\n```python\nclass ConcurrencyLimiter:\n    def __init__(self):\n        self.semaphores\
          \ = {}  # function -> Semaphore\n        self.queues = {}  # function -> Queue\n    \n    async def acquire(self,\
          \ function_name, max_concurrent):\n        if function_name not in self.semaphores:\n            self.semaphores[function_name]\
          \ = asyncio.Semaphore(max_concurrent)\n        \n        await self.semaphores[function_name].acquire()\n    \n\
          \    def release(self, function_name):\n        self.semaphores[function_name].release()\n\n# Usage\nasync def invoke_with_limit(function,\
          \ event):\n    await limiter.acquire(function.name, function.max_concurrency)\n    try:\n        return await executor.invoke(function,\
          \ event)\n    finally:\n        limiter.release(function.name)\n```"
        level3: "Async invocation:\n```python\n@app.post('/functions/{function_name}/invoke-async')\nasync def invoke_async(function_name:\
          \ str, request: Request):\n    invocation_id = str(uuid4())\n    \n    # Queue for async processing\n    await invocation_queue.put({\n\
          \        'id': invocation_id,\n        'function': function_name,\n        'event': await request.json(),\n    \
          \    'callback_url': request.headers.get('X-Callback-URL')\n    })\n    \n    return {'invocation_id': invocation_id,\
          \ 'status': 'queued'}\n\n# Background worker\nasync def process_async_invocations():\n    while True:\n        invocation\
          \ = await invocation_queue.get()\n        result = await invoke(invocation['function'], invocation['event'])\n \
          \       \n        # Store result\n        await store_result(invocation['id'], result)\n        \n        # Callback\
          \ if specified\n        if invocation.get('callback_url'):\n            await send_callback(invocation['callback_url'],\
          \ result)\n```"
      acceptance_criteria:
      - Route incoming HTTP requests to correct function instance
      - Handle multiple concurrent requests with per-function limits
      - Implement request queuing when all instances are busy
      - Support both synchronous and asynchronous invocation modes
    - name: Auto-Scaling
      description: Scale function instances based on demand
      skills:
      - Metrics collection
      - Scaling algorithms
      - Scale to zero
      deliverables:
      - Request rate metrics tracking invocations per second per function
      - Concurrent execution tracking counting active running instances
      - Scale-up trigger adding instances when utilization exceeds threshold
      - Scale-down with cooldown removing idle instances after quiet period
      - Scale to zero terminating all instances after prolonged inactivity
      - Provisioned concurrency keeping minimum instances always warm
      hints:
        level1: "Metrics collection:\n```python\nclass FunctionMetrics:\n    def __init__(self):\n        self.invocations\
          \ = defaultdict(list)  # function -> [timestamp]\n        self.concurrent = defaultdict(int)  # function -> count\n\
          \    \n    def record_invocation(self, function_name):\n        self.invocations[function_name].append(time.time())\n\
          \        # Keep last 5 minutes\n        cutoff = time.time() - 300\n        self.invocations[function_name] = [\n\
          \            t for t in self.invocations[function_name] if t > cutoff\n        ]\n    \n    def get_request_rate(self,\
          \ function_name, window_seconds=60):\n        cutoff = time.time() - window_seconds\n        count = sum(1 for t\
          \ in self.invocations[function_name] if t > cutoff)\n        return count / window_seconds\n```"
        level2: "Auto-scaler:\n```python\nclass AutoScaler:\n    def __init__(self, metrics, warm_pool):\n        self.metrics\
          \ = metrics\n        self.pool = warm_pool\n        self.target_concurrency = 10  # requests per instance\n    \n\
          \    async def scale(self, function):\n        rate = self.metrics.get_request_rate(function.name)\n        current_instances\
          \ = len(self.pool.pools[function.name])\n        \n        desired = max(1, int(rate / self.target_concurrency))\n\
          \        desired = min(desired, function.max_instances)\n        \n        if desired > current_instances:\n   \
          \         # Scale up\n            for _ in range(desired - current_instances):\n                sandbox = await\
          \ self.create_sandbox(function)\n                self.pool.release(sandbox)\n        elif desired < current_instances\
          \ and self.can_scale_down(function):\n            # Scale down\n            excess = current_instances - desired\n\
          \            for _ in range(excess):\n                sandbox = self.pool.acquire(function.name)\n             \
          \   if sandbox:\n                    sandbox.destroy()\n```"
        level3: "Scale to zero with cold start tracking:\n```python\nclass ScaleToZeroPolicy:\n    def __init__(self, idle_timeout=300):\n\
          \        self.idle_timeout = idle_timeout\n        self.last_invocation = {}\n    \n    def should_scale_to_zero(self,\
          \ function_name):\n        last = self.last_invocation.get(function_name, 0)\n        return time.time() - last\
          \ > self.idle_timeout\n    \n    async def run(self):\n        while True:\n            for function_name, instances\
          \ in self.pool.pools.items():\n                if instances and self.should_scale_to_zero(function_name):\n    \
          \                # Destroy all instances\n                    while instances:\n                        sandbox\
          \ = instances.pop()\n                        sandbox.destroy()\n                    logger.info(f'Scaled {function_name}\
          \ to zero')\n            \n            await asyncio.sleep(60)\n```"
      acceptance_criteria:
      - Scale function instances up based on incoming request rate
      - Implement scale-to-zero removing all instances after idle timeout
      - Enforce minimum and maximum scaling limits per function
      - Track scaling events and instance utilization in metrics
  mlops-platform:
    name: MLOps Platform
    description: Build an end-to-end MLOps platform for training, versioning, deploying, and monitoring ML models
    category: AI/ML Infrastructure
    difficulty: advanced
    estimated_hours: 70
    skills:
    - Experiment tracking
    - Model versioning
    - Pipeline orchestration
    - Model deployment
    - Feature store
    - Model monitoring
    prerequisites:
    - ML basics
    - Docker
    - REST APIs
    learning_outcomes:
    - Design ML lifecycle management systems
    - Implement experiment tracking and versioning
    - Build automated training pipelines
    - Deploy and monitor models in production
    milestones:
    - name: Experiment Tracking
      description: Track experiments, parameters, and metrics
      skills:
      - Logging
      - Metadata storage
      - Visualization
      deliverables:
      - Experiment and run abstraction layer that organizes training runs into named experiment groups
      - Parameter logging API that records hyperparameter key-value pairs for each training run
      - Metric logging API that records named metrics at each training step with step number and timestamp
      - Artifact storage system that saves models, plots, and data files associated with each run
      - Run comparison view that displays side-by-side metrics and parameters for selected runs
      - Experiment dashboard UI that lists runs with sortable columns for key metrics and parameters
      hints:
        level1: "Experiment tracking API:\n```python\nclass Experiment:\n    def __init__(self, name, tracking_uri):\n   \
          \     self.name = name\n        self.tracking_uri = tracking_uri\n        self.run_id = None\n    \n    def start_run(self,\
          \ run_name=None):\n        self.run_id = str(uuid4())\n        self.run = Run(\n            id=self.run_id,\n  \
          \          experiment=self.name,\n            name=run_name,\n            start_time=datetime.utcnow()\n       \
          \ )\n        return self\n    \n    def log_param(self, key, value):\n        self.run.params[key] = value\n   \
          \ \n    def log_metric(self, key, value, step=None):\n        self.run.metrics.append(Metric(key, value, step, time.time()))\n\
          \    \n    def log_artifact(self, local_path, artifact_path=None):\n        dest = f'{self.tracking_uri}/artifacts/{self.run_id}/{artifact_path\
          \ or os.path.basename(local_path)}'\n        shutil.copy(local_path, dest)\n```"
        level2: "Auto-logging decorator:\n```python\ndef autolog(framework='sklearn'):\n    def decorator(train_func):\n \
          \       def wrapper(*args, **kwargs):\n            with mlops.start_run():\n                # Log function params\n\
          \                mlops.log_params(kwargs)\n                \n                result = train_func(*args, **kwargs)\n\
          \                \n                # Framework-specific logging\n                if framework == 'sklearn' and hasattr(result,\
          \ 'get_params'):\n                    mlops.log_params(result.get_params())\n                \n                return\
          \ result\n        return wrapper\n    return decorator\n\n@autolog('sklearn')\ndef train_model(X, y, n_estimators=100):\n\
          \    model = RandomForestClassifier(n_estimators=n_estimators)\n    model.fit(X, y)\n    return model\n```"
        level3: "Run comparison:\n```python\nclass ExperimentAnalyzer:\n    def compare_runs(self, run_ids):\n        runs\
          \ = [self.get_run(rid) for rid in run_ids]\n        \n        # Parameter diff\n        all_params = set()\n   \
          \     for run in runs:\n            all_params.update(run.params.keys())\n        \n        param_diff = {}\n  \
          \      for param in all_params:\n            values = [run.params.get(param) for run in runs]\n            if len(set(values))\
          \ > 1:\n                param_diff[param] = values\n        \n        # Metric comparison\n        metric_comparison\
          \ = {}\n        for run in runs:\n            for metric in run.metrics:\n                if metric.key not in metric_comparison:\n\
          \                    metric_comparison[metric.key] = []\n                metric_comparison[metric.key].append({\n\
          \                    'run_id': run.id,\n                    'value': metric.value\n                })\n        \n\
          \        return {'param_diff': param_diff, 'metrics': metric_comparison}\n```"
      acceptance_criteria:
      - Experiment parameters are logged as key-value pairs and associated with the specific run that produced them
      - Training metrics are tracked at each step creating a time series that shows learning progress
      - Artifacts including model files, plots, and configuration files are stored and downloadable per run
      - Multiple experiment runs can be compared side-by-side on metrics and parameters in a table or chart view
    - name: Model Registry
      description: Version and manage trained models
      skills:
      - Model serialization
      - Versioning
      - Stage management
      deliverables:
      - Model registration API that creates a named model entry with version and link to the source run
      - Version management system that auto-increments version numbers and tracks version history
      - Stage transition workflow that moves model versions through staging, production, and archived stages
      - Model metadata store that records framework, accuracy, data hash, and custom tags for each version
      - Model lineage tracker that links each model version back to its training data, code commit, and run
      - Model download API that serves model artifacts for a specific version from the registry storage
      hints:
        level1: |-
          Model registry:
          ```python
          @dataclass
          class RegisteredModel:
              name: str
              versions: List['ModelVersion']
              description: str = None
              tags: dict = None

          @dataclass
          class ModelVersion:
              version: int
              model_uri: str
              run_id: str
              stage: str = 'None'  # None, Staging, Production, Archived
              created_at: datetime = None

          class ModelRegistry:
              def register_model(self, name, model_uri, run_id=None):
                  model = self.get_or_create_model(name)
                  version = ModelVersion(
                      version=len(model.versions) + 1,
                      model_uri=model_uri,
                      run_id=run_id,
                      created_at=datetime.utcnow()
                  )
                  model.versions.append(version)
                  self.save(model)
                  return version
          ```
        level2: "Stage transitions:\n```python\ndef transition_stage(self, name, version, stage):\n    model = self.get_model(name)\n\
          \    version_obj = model.versions[version - 1]\n    \n    # If transitioning to Production, archive current production\n\
          \    if stage == 'Production':\n        for v in model.versions:\n            if v.stage == 'Production':\n    \
          \            v.stage = 'Archived'\n    \n    version_obj.stage = stage\n    self.save(model)\n    \n    # Trigger\
          \ webhooks\n    self.notify_stage_change(name, version, stage)\n    \n    return version_obj\n```"
        level3: "Model lineage:\n```python\nclass ModelLineage:\n    def get_lineage(self, model_name, version):\n       \
          \ model_version = self.registry.get_version(model_name, version)\n        run = self.tracking.get_run(model_version.run_id)\n\
          \        \n        return {\n            'model': {'name': model_name, 'version': version},\n            'training_run':\
          \ {\n                'id': run.id,\n                'experiment': run.experiment,\n                'params': run.params,\n\
          \                'metrics': run.final_metrics()\n            },\n            'data': {\n                'dataset':\
          \ run.params.get('dataset'),\n                'dataset_version': run.params.get('dataset_version')\n           \
          \ },\n            'code': {\n                'git_commit': run.tags.get('git_commit'),\n                'git_repo':\
          \ run.tags.get('git_repo')\n            }\n        }\n```"
      acceptance_criteria:
      - Trained models are registered with metadata including name, version, accuracy, and source run ID
      - Model versions transition through defined stages with approval gates between staging and production
      - Model lineage traces back to the training data version, code commit, and experiment run that produced it
      - Model search and discovery queries filter and sort models by name, stage, metrics, and tags
    - name: Training Pipeline
      description: Orchestrate training workflows
      skills:
      - DAG orchestration
      - Containerized training
      - Distributed training
      deliverables:
      - Pipeline definition DSL that describes training steps as a directed acyclic graph of operations
      - Step executor that runs each pipeline step with configured compute resources and container image
      - Data passing mechanism that transfers outputs from upstream steps to downstream step inputs
      - Conditional execution logic that skips or includes pipeline steps based on runtime conditions
      - Parallel step scheduler that executes independent steps concurrently to reduce total pipeline time
      - Pipeline versioning system that tracks changes to pipeline definitions for reproducibility
      hints:
        level1: "Pipeline definition:\n```python\nclass Pipeline:\n    def __init__(self, name):\n        self.name = name\n\
          \        self.steps = []\n    \n    def add_step(self, step):\n        self.steps.append(step)\n        return self\n\
          \n@dataclass\nclass Step:\n    name: str\n    command: str\n    image: str = 'python:3.9'\n    inputs: List[str]\
          \ = None\n    outputs: List[str] = None\n    resources: dict = None\n\n# Usage\npipeline = Pipeline('training')\n\
          pipeline.add_step(Step(\n    name='preprocess',\n    command='python preprocess.py --input {input_data} --output\
          \ {processed_data}',\n    outputs=['processed_data']\n))\npipeline.add_step(Step(\n    name='train',\n    command='python\
          \ train.py --data {processed_data} --model {model_output}',\n    inputs=['processed_data'],\n    outputs=['model_output'],\n\
          \    resources={'gpu': 1}\n))\n```"
        level2: "Pipeline executor:\n```python\nclass PipelineExecutor:\n    async def run(self, pipeline, params):\n    \
          \    context = {'params': params, 'outputs': {}}\n        \n        for step in self.topological_sort(pipeline.steps):\n\
          \            # Wait for dependencies\n            for input_name in step.inputs or []:\n                if input_name\
          \ not in context['outputs']:\n                    raise ValueError(f'Missing input: {input_name}')\n           \
          \ \n            # Execute step\n            result = await self.run_step(step, context)\n            context['outputs'].update(result)\n\
          \        \n        return context['outputs']\n    \n    async def run_step(self, step, context):\n        # Render\
          \ command with context\n        command = step.command.format(**context['params'], **context['outputs'])\n     \
          \   \n        # Run in container\n        container = await self.docker.run(\n            image=step.image,\n  \
          \          command=command,\n            resources=step.resources\n        )\n        \n        return {out: container.get_output(out)\
          \ for out in step.outputs or []}\n```"
        level3: "Pipeline versioning and triggers:\n```python\nclass PipelineRegistry:\n    def register(self, pipeline):\n\
          \        version = self.next_version(pipeline.name)\n        pipeline_def = {\n            'name': pipeline.name,\n\
          \            'version': version,\n            'steps': [asdict(s) for s in pipeline.steps],\n            'created_at':\
          \ datetime.utcnow()\n        }\n        self.store.save(pipeline.name, version, pipeline_def)\n        return version\n\
          \    \n    def trigger_on_data_change(self, pipeline_name, data_path):\n        # Register webhook for data changes\n\
          \        self.webhooks.register(\n            event='data.updated',\n            filter={'path': data_path},\n \
          \           action={'pipeline': pipeline_name, 'trigger': 'run'}\n        )\n```"
      acceptance_criteria:
      - Training pipeline steps are defined in a DAG with explicit data dependencies between steps
      - Steps execute on configured compute infrastructure with specified CPU, memory, and GPU resources
      - Resource usage for each step including CPU time, memory peak, and GPU utilization is tracked and logged
      - Distributed training across multiple GPUs or nodes is supported for steps that require it
    - name: Model Deployment
      description: Deploy models to production
      skills:
      - Serving infrastructure
      - Canary deployment
      - A/B testing
      deliverables:
      - Model serving endpoint generator that wraps a registered model version behind an HTTP inference API
      - Blue-green deployment controller that runs two versions simultaneously and switches traffic atomically
      - Canary rollout manager that gradually shifts traffic from the current version to the new version
      - A/B testing framework that splits traffic between model versions and tracks per-version metrics
      - Auto-scaler that adjusts the number of serving replicas based on request load and latency targets
      - Deployment rollback mechanism that reverts to the previous model version on error rate spike
      hints:
        level1: "Deploy model:\n```python\nclass ModelDeployer:\n    def deploy(self, model_name, version, endpoint_name):\n\
          \        model_version = self.registry.get_version(model_name, version)\n        \n        # Create serving container\n\
          \        deployment = Deployment(\n            name=endpoint_name,\n            image='model-server:latest',\n \
          \           env={\n                'MODEL_URI': model_version.model_uri,\n                'MODEL_NAME': model_name\n\
          \            },\n            replicas=2,\n            resources={'cpu': '500m', 'memory': '1Gi'}\n        )\n  \
          \      \n        self.k8s.create_deployment(deployment)\n        self.k8s.create_service(endpoint_name)\n      \
          \  \n        return Endpoint(name=endpoint_name, url=f'http://{endpoint_name}/predict')\n```"
        level2: "Canary deployment:\n```python\ndef deploy_canary(self, model_name, new_version, endpoint_name, canary_percent=10):\n\
          \    current = self.get_deployment(endpoint_name)\n    \n    # Create canary deployment\n    canary = self.deploy(\n\
          \        model_name, new_version,\n        endpoint_name=f'{endpoint_name}-canary'\n    )\n    \n    # Update traffic\
          \ split\n    self.update_traffic_split(endpoint_name, {\n        current.name: 100 - canary_percent,\n        canary.name:\
          \ canary_percent\n    })\n    \n    return CanaryDeployment(primary=current, canary=canary, percent=canary_percent)\n\
          \ndef promote_canary(self, endpoint_name):\n    # Shift all traffic to canary\n    self.update_traffic_split(endpoint_name,\
          \ {\n        f'{endpoint_name}-canary': 100\n    })\n    # Rename canary to primary\n    self.rename_deployment(f'{endpoint_name}-canary',\
          \ endpoint_name)\n```"
        level3: "A/B test with metrics:\n```python\nclass ABTest:\n    def __init__(self, name, variants, metric):\n     \
          \   self.name = name\n        self.variants = variants  # {name: model_version}\n        self.metric = metric\n\
          \        self.results = defaultdict(list)\n    \n    def assign_variant(self, user_id):\n        # Consistent hashing\
          \ for user\n        hash_val = hash(f'{self.name}:{user_id}') % 100\n        cumulative = 0\n        for variant,\
          \ percent in self.variants.items():\n            cumulative += percent\n            if hash_val < cumulative:\n\
          \                return variant\n    \n    def record_outcome(self, user_id, outcome):\n        variant = self.assign_variant(user_id)\n\
          \        self.results[variant].append(outcome)\n    \n    def get_results(self):\n        from scipy import stats\n\
          \        control = self.results['control']\n        treatment = self.results['treatment']\n        \n        t_stat,\
          \ p_value = stats.ttest_ind(control, treatment)\n        return {\n            'control_mean': np.mean(control),\n\
          \            'treatment_mean': np.mean(treatment),\n            'p_value': p_value,\n            'significant':\
          \ p_value < 0.05\n        }\n```"
      acceptance_criteria:
      - A registered model version is deployed as an HTTP API endpoint accepting inference requests
      - Canary deployments route a configurable percentage of traffic to the new version for validation
      - Auto-scaling adjusts replica count based on request rate and p99 latency within configured bounds
      - Deployment integrates with inference servers like Triton, TorchServe, or TensorFlow Serving
    - name: Model Monitoring
      description: Monitor model performance in production
      skills:
      - Metrics collection
      - Drift detection
      - Alerting
      deliverables:
      - Prediction logger that records model inputs, outputs, and timestamps for every inference request
      - Performance metrics tracker that computes accuracy, latency, and throughput for the deployed model
      - Data drift detector that compares incoming feature distributions against the training data baseline
      - Model drift detector that monitors prediction distribution changes indicating concept drift
      - Alerting rule engine that fires notifications when monitored metrics cross configured thresholds
      - Monitoring dashboard that visualizes prediction metrics, drift scores, and alert history over time
      hints:
        level1: "Prediction logging:\n```python\nclass PredictionLogger:\n    def __init__(self, kafka_producer):\n      \
          \  self.producer = kafka_producer\n    \n    def log(self, model_name, version, input_data, prediction, latency):\n\
          \        record = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'model': model_name,\n\
          \            'version': version,\n            'input_hash': hash(str(input_data)),\n            'input_sample':\
          \ self.sample_input(input_data),\n            'prediction': prediction,\n            'latency_ms': latency\n   \
          \     }\n        self.producer.send('predictions', record)\n    \n    def log_feedback(self, prediction_id, ground_truth):\n\
          \        self.producer.send('feedback', {\n            'prediction_id': prediction_id,\n            'ground_truth':\
          \ ground_truth\n        })\n```"
        level2: "Data drift detection:\n```python\nclass DriftMonitor:\n    def __init__(self, reference_data):\n        self.reference_stats\
          \ = self.compute_stats(reference_data)\n    \n    def check_drift(self, current_data):\n        current_stats =\
          \ self.compute_stats(current_data)\n        drift_scores = {}\n        \n        for feature in self.reference_stats:\n\
          \            # PSI (Population Stability Index)\n            ref = self.reference_stats[feature]['distribution']\n\
          \            curr = current_stats[feature]['distribution']\n            psi = self.compute_psi(ref, curr)\n    \
          \        drift_scores[feature] = psi\n        \n        return {\n            'drift_scores': drift_scores,\n  \
          \          'has_drift': any(s > 0.2 for s in drift_scores.values())\n        }\n    \n    def compute_psi(self,\
          \ expected, actual):\n        psi = 0\n        for i in range(len(expected)):\n            if expected[i] == 0 or\
          \ actual[i] == 0:\n                continue\n            psi += (actual[i] - expected[i]) * np.log(actual[i] / expected[i])\n\
          \        return psi\n```"
        level3: "Alerting:\n```python\nclass AlertManager:\n    def __init__(self):\n        self.rules = []\n    \n    def\
          \ add_rule(self, name, condition, action):\n        self.rules.append(AlertRule(name, condition, action))\n    \n\
          \    async def evaluate(self, metrics):\n        for rule in self.rules:\n            if rule.condition(metrics):\n\
          \                await rule.action.execute({\n                    'rule': rule.name,\n                    'metrics':\
          \ metrics\n                })\n\n# Example rules\nalert_manager.add_rule(\n    'high_latency',\n    condition=lambda\
          \ m: m['p99_latency'] > 500,\n    action=SlackAlert(channel='#ml-alerts')\n)\nalert_manager.add_rule(\n    'accuracy_drop',\n\
          \    condition=lambda m: m['accuracy'] < 0.9,\n    action=PagerDutyAlert(severity='high')\n)\nalert_manager.add_rule(\n\
          \    'data_drift',\n    condition=lambda m: m['drift_score'] > 0.2,\n    action=RetrainTrigger(pipeline='training')\n\
          )\n```"
      acceptance_criteria:
      - Prediction metrics including accuracy, latency percentiles, and throughput are tracked in production
      - Data drift is detected by computing statistical distance between live and training feature distributions
      - Alerts fire when model accuracy drops or prediction distribution shifts beyond configured thresholds
      - A/B testing analysis computes statistical significance between model versions and recommends a winner
  filesystem:
    name: Filesystem Implementation
    description: Build a simple filesystem with inodes, directories, and journaling - understand how data is organized on
      disk
    category: Systems
    difficulty: advanced
    estimated_hours: 60
    skills:
    - Block devices
    - Inode structures
    - Directory entries
    - Journaling
    - FUSE interface
    - Caching
    prerequisites:
    - File I/O
    - Data structures
    - C programming
    learning_outcomes:
    - Understand filesystem internals and on-disk layouts
    - Implement inode-based file management
    - Build directory tree operations
    - Handle crash recovery with journaling
    milestones:
    - name: Block Layer
      description: Raw block device read/write operations
      skills:
      - Block I/O
      - Superblock
      - Bitmap allocation
      deliverables:
      - Block device abstraction providing fixed-size read and write operations
      - Superblock with filesystem metadata including size and block count
      - Block bitmap for tracking free and allocated block status
      - Block allocation and deallocation with bitmap updates
      - Disk image file backend for testing without physical device
      - Block caching layer reducing disk I/O for frequently accessed blocks
      hints:
        level1: |-
          Basic block device:
          ```c
          #define BLOCK_SIZE 4096

          struct block_device {
              int fd;
              size_t num_blocks;
          };

          int block_read(struct block_device *dev, uint64_t block_num, void *buf) {
              off_t offset = block_num * BLOCK_SIZE;
              return pread(dev->fd, buf, BLOCK_SIZE, offset);
          }

          int block_write(struct block_device *dev, uint64_t block_num, const void *buf) {
              off_t offset = block_num * BLOCK_SIZE;
              return pwrite(dev->fd, buf, BLOCK_SIZE, offset);
          }
          ```
        level2: |-
          Superblock structure:
          ```c
          struct superblock {
              uint32_t magic;           // Filesystem magic number
              uint32_t block_size;      // Block size in bytes
              uint64_t total_blocks;    // Total blocks in filesystem
              uint64_t inode_count;     // Total inodes
              uint64_t free_blocks;     // Free block count
              uint64_t free_inodes;     // Free inode count
              uint64_t block_bitmap;    // Block bitmap start
              uint64_t inode_bitmap;    // Inode bitmap start
              uint64_t inode_table;     // Inode table start
              uint64_t data_blocks;     // Data blocks start
          };
          ```
        level3: |-
          Bitmap operations:
          ```c
          int bitmap_alloc(uint8_t *bitmap, size_t size) {
              for (size_t i = 0; i < size; i++) {
                  if (bitmap[i] != 0xFF) {
                      for (int bit = 0; bit < 8; bit++) {
                          if (!(bitmap[i] & (1 << bit))) {
                              bitmap[i] |= (1 << bit);
                              return i * 8 + bit;
                          }
                      }
                  }
              }
              return -1;  // No free blocks
          }

          void bitmap_free(uint8_t *bitmap, int index) {
              bitmap[index / 8] &= ~(1 << (index % 8));
          }
          ```
      acceptance_criteria:
      - Read and write fixed-size blocks to disk image file backend correctly
      - Implement block allocation bitmap tracking free and used blocks on disk
      - Handle block caching for performance with write-back or write-through policy
      - Support multiple configurable block sizes (1K, 2K, 4K) for flexibility
    - name: Inode Management
      description: Inode structure for file metadata
      skills:
      - Inode structure
      - Direct/indirect blocks
      - Permissions
      deliverables:
      - Inode structure with metadata including size, permissions, and timestamps
      - Direct block pointers for small file data block addressing
      - Indirect block pointers (single, double) for large file support
      - Inode allocation and free operations with inode bitmap tracking
      - Inode read and write operations persisting metadata to disk
      - File type and permissions stored in inode mode field
      hints:
        level1: |-
          Inode structure:
          ```c
          #define DIRECT_BLOCKS 12

          struct inode {
              uint16_t mode;           // File type and permissions
              uint16_t uid;            // Owner user ID
              uint16_t gid;            // Owner group ID
              uint32_t size;           // File size in bytes
              uint32_t atime;          // Access time
              uint32_t mtime;          // Modification time
              uint32_t ctime;          // Change time
              uint32_t links_count;    // Hard link count
              uint32_t blocks;         // Number of blocks
              uint32_t direct[DIRECT_BLOCKS];  // Direct block pointers
              uint32_t indirect;       // Single indirect
              uint32_t double_indirect; // Double indirect
          };
          ```
        level2: "Get block number for file offset:\n```c\nuint32_t inode_get_block(struct inode *inode, uint32_t file_block)\
          \ {\n    if (file_block < DIRECT_BLOCKS) {\n        return inode->direct[file_block];\n    }\n    \n    file_block\
          \ -= DIRECT_BLOCKS;\n    uint32_t ptrs_per_block = BLOCK_SIZE / sizeof(uint32_t);\n    \n    if (file_block < ptrs_per_block)\
          \ {\n        // Single indirect\n        uint32_t *indirect = read_block(inode->indirect);\n        return indirect[file_block];\n\
          \    }\n    \n    file_block -= ptrs_per_block;\n    // Double indirect\n    uint32_t *dbl = read_block(inode->double_indirect);\n\
          \    uint32_t *indirect = read_block(dbl[file_block / ptrs_per_block]);\n    return indirect[file_block % ptrs_per_block];\n\
          }\n```"
        level3: "Allocate blocks for file growth:\n```c\nint inode_grow(struct inode *inode, size_t new_size) {\n    size_t\
          \ current_blocks = (inode->size + BLOCK_SIZE - 1) / BLOCK_SIZE;\n    size_t needed_blocks = (new_size + BLOCK_SIZE\
          \ - 1) / BLOCK_SIZE;\n    \n    for (size_t i = current_blocks; i < needed_blocks; i++) {\n        uint32_t new_block\
          \ = alloc_block();\n        if (new_block == 0) return -ENOSPC;\n        \n        if (!inode_set_block(inode, i,\
          \ new_block)) {\n            // May need to allocate indirect blocks\n            if (!alloc_indirect_block(inode,\
          \ i)) {\n                return -ENOSPC;\n            }\n            inode_set_block(inode, i, new_block);\n   \
          \     }\n    }\n    inode->size = new_size;\n    return 0;\n}\n```"
      acceptance_criteria:
      - Store file metadata in inode including size, permissions, and timestamps
      - Allocate and free inodes using inode bitmap for tracking availability
      - Support direct and indirect block pointers for files of varying sizes
      - Handle inode table storage on disk with correct serialization format
    - name: Directory Operations
      description: Directory entries and path resolution
      skills:
      - Directory entries
      - Path parsing
      - Name lookup
      deliverables:
      - Directory entry structure mapping names to inode numbers
      - Add and remove directory entry operations for file management
      - Path to inode resolution traversing directory hierarchy
      - Create and delete file operations with directory entry management
      - Create and remove directory operations with recursive handling
      - Rename operations moving entries between directories atomically
      hints:
        level1: |-
          Directory entry:
          ```c
          #define MAX_NAME_LEN 255

          struct dir_entry {
              uint32_t inode;          // Inode number (0 = deleted)
              uint16_t rec_len;        // Total entry length
              uint8_t name_len;        // Name length
              uint8_t file_type;       // File type
              char name[MAX_NAME_LEN]; // File name
          };

          // File types
          #define FT_UNKNOWN  0
          #define FT_REG_FILE 1
          #define FT_DIR      2
          #define FT_SYMLINK  7
          ```
        level2: "Lookup name in directory:\n```c\nstruct inode *dir_lookup(struct inode *dir, const char *name) {\n    size_t\
          \ offset = 0;\n    while (offset < dir->size) {\n        struct dir_entry *entry = read_dir_entry(dir, offset);\n\
          \        \n        if (entry->inode != 0 && \n            entry->name_len == strlen(name) &&\n            strncmp(entry->name,\
          \ name, entry->name_len) == 0) {\n            return read_inode(entry->inode);\n        }\n        \n        offset\
          \ += entry->rec_len;\n    }\n    return NULL;  // Not found\n}\n```"
        level3: "Path resolution:\n```c\nstruct inode *resolve_path(const char *path) {\n    struct inode *inode = read_inode(ROOT_INODE);\n\
          \    \n    char *path_copy = strdup(path);\n    char *token = strtok(path_copy, \"/\");\n    \n    while (token\
          \ != NULL) {\n        if (!S_ISDIR(inode->mode)) {\n            free(path_copy);\n            return NULL;  // Not\
          \ a directory\n        }\n        \n        struct inode *next = dir_lookup(inode, token);\n        if (next ==\
          \ NULL) {\n            free(path_copy);\n            return NULL;  // Not found\n        }\n        \n        inode\
          \ = next;\n        token = strtok(NULL, \"/\");\n    }\n    \n    free(path_copy);\n    return inode;\n}\n```"
      acceptance_criteria:
      - Store directory entries as name-to-inode mappings within directory blocks
      - Support directory creation and deletion with proper reference counting
      - Implement path resolution traversing components like a/b/c to find target inode
      - Handle . and .. entries correctly for self and parent directory references
    - name: File Operations
      description: Read, write, truncate files
      skills:
      - File read/write
      - Truncation
      - Sparse files
      deliverables:
      - Read file data from block storage via inode pointers
      - Write file data allocating new blocks as needed for growth
      - Truncate file releasing blocks beyond new reduced size
      - Append to file extending allocation and updating inode size
      - Sparse file support with unallocated blocks reading as zeros
      - File hole handling for efficient large file storage with gaps
      hints:
        level1: "Read file:\n```c\nssize_t file_read(struct inode *inode, void *buf, size_t size, off_t offset) {\n    if\
          \ (offset >= inode->size) return 0;\n    if (offset + size > inode->size) {\n        size = inode->size - offset;\n\
          \    }\n    \n    size_t bytes_read = 0;\n    while (bytes_read < size) {\n        uint32_t block_num = (offset\
          \ + bytes_read) / BLOCK_SIZE;\n        uint32_t block_offset = (offset + bytes_read) % BLOCK_SIZE;\n        uint32_t\
          \ to_read = min(BLOCK_SIZE - block_offset, size - bytes_read);\n        \n        uint32_t disk_block = inode_get_block(inode,\
          \ block_num);\n        char *block_data = read_block(disk_block);\n        memcpy(buf + bytes_read, block_data +\
          \ block_offset, to_read);\n        \n        bytes_read += to_read;\n    }\n    return bytes_read;\n}\n```"
        level2: "Write file:\n```c\nssize_t file_write(struct inode *inode, const void *buf, size_t size, off_t offset) {\n\
          \    // Grow file if needed\n    if (offset + size > inode->size) {\n        int err = inode_grow(inode, offset\
          \ + size);\n        if (err) return err;\n    }\n    \n    size_t bytes_written = 0;\n    while (bytes_written <\
          \ size) {\n        uint32_t block_num = (offset + bytes_written) / BLOCK_SIZE;\n        uint32_t block_offset =\
          \ (offset + bytes_written) % BLOCK_SIZE;\n        uint32_t to_write = min(BLOCK_SIZE - block_offset, size - bytes_written);\n\
          \        \n        uint32_t disk_block = inode_get_block(inode, block_num);\n        char *block_data = read_block(disk_block);\n\
          \        memcpy(block_data + block_offset, buf + bytes_written, to_write);\n        write_block(disk_block, block_data);\n\
          \        \n        bytes_written += to_write;\n    }\n    \n    inode->mtime = time(NULL);\n    write_inode(inode);\n\
          \    return bytes_written;\n}\n```"
        level3: "Truncate file:\n```c\nint file_truncate(struct inode *inode, off_t length) {\n    if (length > inode->size)\
          \ {\n        // Extend with zeros (sparse)\n        return inode_grow(inode, length);\n    }\n    \n    // Free\
          \ blocks beyond new size\n    size_t new_blocks = (length + BLOCK_SIZE - 1) / BLOCK_SIZE;\n    size_t old_blocks\
          \ = (inode->size + BLOCK_SIZE - 1) / BLOCK_SIZE;\n    \n    for (size_t i = new_blocks; i < old_blocks; i++) {\n\
          \        uint32_t block = inode_get_block(inode, i);\n        if (block != 0) {\n            free_block(block);\n\
          \            inode_set_block(inode, i, 0);\n        }\n    }\n    \n    // Free indirect blocks if needed\n    shrink_indirect_blocks(inode,\
          \ new_blocks);\n    \n    inode->size = length;\n    inode->mtime = time(NULL);\n    write_inode(inode);\n    return\
          \ 0;\n}\n```"
      acceptance_criteria:
      - Create, read, write, and truncate files with correct data integrity
      - Support file seeking to arbitrary positions for random access reads and writes
      - Handle file holes (sparse files) reading unallocated blocks as zero bytes
      - Update timestamps on file operations reflecting access and modification times
    - name: FUSE Interface
      description: Mount filesystem via FUSE
      skills:
      - FUSE API
      - VFS operations
      - Mount/unmount
      deliverables:
      - FUSE operation callback registration for filesystem operations
      - getattr implementation returning file metadata from inode data
      - readdir implementation listing directory contents from entries
      - open, read, write, and release callbacks for file I/O
      - mkdir and rmdir callbacks for directory management operations
      - Mount and unmount lifecycle management for FUSE filesystem
      hints:
        level1: |-
          FUSE operations struct:
          ```c
          #define FUSE_USE_VERSION 31
          #include <fuse3/fuse.h>

          static struct fuse_operations myfs_ops = {
              .getattr  = myfs_getattr,
              .readdir  = myfs_readdir,
              .open     = myfs_open,
              .read     = myfs_read,
              .write    = myfs_write,
              .mkdir    = myfs_mkdir,
              .rmdir    = myfs_rmdir,
              .unlink   = myfs_unlink,
              .create   = myfs_create,
              .truncate = myfs_truncate,
          };

          int main(int argc, char *argv[]) {
              return fuse_main(argc, argv, &myfs_ops, NULL);
          }
          ```
        level2: "Implement getattr:\n```c\nstatic int myfs_getattr(const char *path, struct stat *stbuf,\n               \
          \         struct fuse_file_info *fi) {\n    memset(stbuf, 0, sizeof(struct stat));\n    \n    struct inode *inode\
          \ = resolve_path(path);\n    if (!inode) return -ENOENT;\n    \n    stbuf->st_ino = inode->ino;\n    stbuf->st_mode\
          \ = inode->mode;\n    stbuf->st_nlink = inode->links_count;\n    stbuf->st_uid = inode->uid;\n    stbuf->st_gid\
          \ = inode->gid;\n    stbuf->st_size = inode->size;\n    stbuf->st_blocks = inode->blocks;\n    stbuf->st_atime =\
          \ inode->atime;\n    stbuf->st_mtime = inode->mtime;\n    stbuf->st_ctime = inode->ctime;\n    \n    return 0;\n\
          }\n```"
        level3: "Implement readdir:\n```c\nstatic int myfs_readdir(const char *path, void *buf, fuse_fill_dir_t filler,\n\
          \                        off_t offset, struct fuse_file_info *fi,\n                        enum fuse_readdir_flags\
          \ flags) {\n    struct inode *dir = resolve_path(path);\n    if (!dir) return -ENOENT;\n    if (!S_ISDIR(dir->mode))\
          \ return -ENOTDIR;\n    \n    filler(buf, \".\", NULL, 0, 0);\n    filler(buf, \"..\", NULL, 0, 0);\n    \n    size_t\
          \ pos = 0;\n    while (pos < dir->size) {\n        struct dir_entry *entry = read_dir_entry(dir, pos);\n       \
          \ if (entry->inode != 0) {\n            char name[256];\n            strncpy(name, entry->name, entry->name_len);\n\
          \            name[entry->name_len] = '\\0';\n            filler(buf, name, NULL, 0, 0);\n        }\n        pos\
          \ += entry->rec_len;\n    }\n    \n    return 0;\n}\n```"
      acceptance_criteria:
      - Mount filesystem via FUSE so it appears as a regular mount point
      - Implement FUSE callbacks for getattr, read, write, and directory operations
      - Handle concurrent access from multiple processes accessing the mounted filesystem
      - Support unmount and cleanup releasing all resources and flushing pending data
  reverse-proxy:
    name: Reverse Proxy
    description: Build an Nginx-like reverse proxy with load balancing, caching, and SSL termination
    category: Networking
    difficulty: advanced
    estimated_hours: 55
    skills:
    - HTTP parsing
    - Load balancing
    - Connection pooling
    - SSL/TLS
    - Caching
    - Health checks
    prerequisites:
    - HTTP basics
    - Networking
    - Async I/O
    learning_outcomes:
    - Understand reverse proxy architecture
    - Implement load balancing algorithms
    - Build connection pooling for performance
    - Handle SSL termination
    milestones:
    - name: HTTP Proxy Core
      description: Basic HTTP request forwarding
      skills:
      - HTTP parsing
      - Request forwarding
      - Response handling
      deliverables:
      - Client connection acceptance on configurable listen address
      - HTTP request parsing extracting method, path, and headers
      - Upstream request forwarding to configured backend servers
      - Response relay sending backend response back to client
      - Header manipulation adding X-Forwarded-For and Via headers
      - Error handling returning appropriate status on backend failure
      hints:
        level1: "Basic proxy loop:\n```python\nimport asyncio\n\nasync def handle_client(reader, writer):\n    # Read request\n\
          \    request = await read_http_request(reader)\n    \n    # Connect to upstream\n    upstream_reader, upstream_writer\
          \ = await asyncio.open_connection(\n        'backend.local', 8080\n    )\n    \n    # Forward request\n    upstream_writer.write(request.raw)\n\
          \    await upstream_writer.drain()\n    \n    # Read and forward response\n    response = await read_http_response(upstream_reader)\n\
          \    writer.write(response.raw)\n    await writer.drain()\n    \n    writer.close()\n    upstream_writer.close()\n\
          ```"
        level2: "Add headers:\n```python\ndef modify_request(request, client_addr):\n    # Add X-Forwarded headers\n    request.headers['X-Forwarded-For']\
          \ = client_addr[0]\n    request.headers['X-Forwarded-Proto'] = 'https' if ssl else 'http'\n    request.headers['X-Real-IP']\
          \ = client_addr[0]\n    \n    # Remove hop-by-hop headers\n    for header in ['Connection', 'Keep-Alive', 'Transfer-Encoding']:\n\
          \        request.headers.pop(header, None)\n    \n    return request\n```"
        level3: |-
          Streaming for large bodies:
          ```python
          async def proxy_body(source, dest, content_length):
              remaining = content_length
              while remaining > 0:
                  chunk_size = min(remaining, 64 * 1024)
                  chunk = await source.read(chunk_size)
                  if not chunk:
                      break
                  dest.write(chunk)
                  await dest.drain()
                  remaining -= len(chunk)
          ```
      acceptance_criteria:
      - Accept incoming HTTP requests on configured listen port
      - Forward complete request to selected backend server
      - Return backend response including status, headers, and body to client
      - Handle both HTTP/1.1 and HTTP/2 protocol versions
    - name: Load Balancing
      description: Distribute requests across backends
      skills:
      - Load balancing algorithms
      - Backend selection
      - Weighted distribution
      deliverables:
      - Round-robin load balancing distributing requests evenly
      - Least-connections algorithm routing to least busy backend
      - Weighted round-robin supporting different backend capacities
      - IP hash sticky sessions keeping client on same backend
      - Backend configuration file defining upstream server list
      - Dynamic backend updates adding or removing servers without restart
      hints:
        level1: "Round-robin:\n```python\nclass RoundRobinBalancer:\n    def __init__(self, backends):\n        self.backends\
          \ = backends\n        self.index = 0\n    \n    def next(self):\n        backend = self.backends[self.index]\n \
          \       self.index = (self.index + 1) % len(self.backends)\n        return backend\n```"
        level2: "Least connections:\n```python\nclass LeastConnectionsBalancer:\n    def __init__(self, backends):\n     \
          \   self.backends = {b: 0 for b in backends}\n    \n    def next(self):\n        backend = min(self.backends, key=self.backends.get)\n\
          \        self.backends[backend] += 1\n        return backend\n    \n    def release(self, backend):\n        self.backends[backend]\
          \ -= 1\n```"
        level3: "IP hash for sticky sessions:\n```python\nclass IPHashBalancer:\n    def __init__(self, backends):\n     \
          \   self.backends = backends\n    \n    def next(self, client_ip):\n        # Consistent hashing\n        hash_val\
          \ = hash(client_ip)\n        index = hash_val % len(self.backends)\n        return self.backends[index]\n```"
      acceptance_criteria:
      - Implement round-robin distribution across backend servers
      - Support weighted distribution based on backend server capacity
      - Perform periodic health checks against backend endpoints
      - Remove unhealthy backends from active pool automatically
    - name: Connection Pooling
      description: Reuse connections to backends
      skills:
      - Connection pools
      - Keep-alive
      - Pool management
      deliverables:
      - Persistent connection pool maintained per backend server
      - HTTP keep-alive connection reuse reducing TCP handshake overhead
      - Configurable pool size limits preventing resource exhaustion
      - Connection timeout evicting idle connections from pool
      - Health check integration removing broken connections from pool
      - Pool metrics tracking active, idle, and total connection counts
      hints:
        level1: "Simple connection pool:\n```python\nclass ConnectionPool:\n    def __init__(self, host, port, max_size=10):\n\
          \        self.host = host\n        self.port = port\n        self.max_size = max_size\n        self.pool = asyncio.Queue(maxsize=max_size)\n\
          \        self.size = 0\n    \n    async def acquire(self):\n        try:\n            return self.pool.get_nowait()\n\
          \        except asyncio.QueueEmpty:\n            if self.size < self.max_size:\n                self.size += 1\n\
          \                return await self.create_connection()\n            return await self.pool.get()\n    \n    async\
          \ def release(self, conn):\n        if conn.is_healthy():\n            await self.pool.put(conn)\n        else:\n\
          \            self.size -= 1\n            conn.close()\n```"
        level2: "Connection wrapper with keep-alive:\n```python\nclass PooledConnection:\n    def __init__(self, reader, writer,\
          \ pool):\n        self.reader = reader\n        self.writer = writer\n        self.pool = pool\n        self.last_used\
          \ = time.time()\n    \n    def is_healthy(self):\n        # Check if connection is still alive\n        if time.time()\
          \ - self.last_used > 60:\n            return False\n        return not self.reader.at_eof()\n    \n    async def\
          \ __aenter__(self):\n        return self\n    \n    async def __aexit__(self, *args):\n        self.last_used =\
          \ time.time()\n        await self.pool.release(self)\n```"
        level3: |-
          Pool with health checks:
          ```python
          class HealthyConnectionPool(ConnectionPool):
              async def health_check_loop(self):
                  while True:
                      await asyncio.sleep(30)
                      # Check all idle connections
                      healthy = []
                      while not self.pool.empty():
                          conn = await self.pool.get()
                          if conn.is_healthy():
                              healthy.append(conn)
                          else:
                              conn.close()
                              self.size -= 1
                      for conn in healthy:
                          await self.pool.put(conn)
          ```
      acceptance_criteria:
      - Maintain persistent keep-alive connections to backend servers
      - Implement separate connection pool for each backend server
      - Handle connection timeouts by evicting stale connections
      - Enforce maximum connection limits per backend pool
    - name: Caching
      description: Cache responses for performance
      skills:
      - Cache storage
      - Cache invalidation
      - Cache headers
      deliverables:
      - In-memory response cache with configurable maximum size
      - Cache key generation from request method, URL, and headers
      - TTL and max-age expiration based on response Cache-Control header
      - Cache-Control header parsing respecting no-cache and no-store directives
      - Conditional request support using ETag and If-Modified-Since headers
      - Cache bypass rules for authenticated or dynamic requests
      hints:
        level1: "Simple cache:\n```python\nclass ResponseCache:\n    def __init__(self, max_size=1000):\n        self.cache\
          \ = {}\n        self.max_size = max_size\n    \n    def key(self, request):\n        return f\"{request.method}:{request.host}:{request.path}\"\
          \n    \n    def get(self, request):\n        k = self.key(request)\n        if k in self.cache:\n            entry\
          \ = self.cache[k]\n            if entry.is_valid():\n                return entry.response\n            del self.cache[k]\n\
          \        return None\n    \n    def put(self, request, response, ttl):\n        if len(self.cache) >= self.max_size:\n\
          \            self.evict()\n        self.cache[self.key(request)] = CacheEntry(response, ttl)\n```"
        level2: |-
          Parse Cache-Control:
          ```python
          def parse_cache_control(response):
              cc = response.headers.get('Cache-Control', '')
              directives = {}
              for part in cc.split(','):
                  part = part.strip()
                  if '=' in part:
                      key, value = part.split('=', 1)
                      directives[key] = value
                  else:
                      directives[part] = True
              return directives

          def is_cacheable(response):
              cc = parse_cache_control(response)
              if cc.get('no-store') or cc.get('private'):
                  return False
              if response.status_code not in [200, 203, 204, 206, 300, 301, 404, 501]:
                  return False
              return True
          ```
        level3: "Conditional requests:\n```python\nasync def serve_with_cache(request, cache):\n    cached = cache.get(request)\n\
          \    \n    if cached:\n        # Check if client has valid cached copy\n        if request.headers.get('If-None-Match')\
          \ == cached.etag:\n            return Response(status=304)\n        return cached.response\n    \n    # Fetch from\
          \ upstream\n    response = await fetch_upstream(request)\n    \n    if is_cacheable(response):\n        ttl = get_ttl(response)\n\
          \        cache.put(request, response, ttl)\n    \n    return response\n```"
      acceptance_criteria:
      - Cache GET responses based on Cache-Control header directives
      - Generate unique cache keys from request attributes
      - Support cache invalidation by TTL expiry and explicit purge
      - Respect Cache-Control no-cache and no-store directives correctly
    - name: SSL Termination
      description: Handle HTTPS connections
      skills:
      - TLS/SSL
      - Certificate management
      - SNI
      deliverables:
      - SSL/TLS context setup with certificate and private key loading
      - Certificate loading from PEM files with chain validation
      - SNI support serving different certificates per domain name
      - TLS version configuration enforcing minimum TLS 1.2
      - Cipher suite selection prioritizing strong modern ciphers
      - HTTP to HTTPS redirect for plaintext requests
      hints:
        level1: |-
          SSL server setup:
          ```python
          import ssl

          def create_ssl_context(cert_file, key_file):
              ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
              ctx.load_cert_chain(cert_file, key_file)
              ctx.minimum_version = ssl.TLSVersion.TLSv1_2
              return ctx

          async def start_https_server(host, port, cert, key):
              ssl_ctx = create_ssl_context(cert, key)
              server = await asyncio.start_server(
                  handle_client, host, port, ssl=ssl_ctx
              )
              return server
          ```
        level2: "SNI for multiple domains:\n```python\nclass SNIContext:\n    def __init__(self):\n        self.contexts =\
          \ {}  # domain -> ssl_context\n    \n    def add_certificate(self, domain, cert, key):\n        ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n\
          \        ctx.load_cert_chain(cert, key)\n        self.contexts[domain] = ctx\n    \n    def sni_callback(self, ssl_obj,\
          \ server_name, original_ctx):\n        if server_name in self.contexts:\n            ssl_obj.context = self.contexts[server_name]\n\
          \n# Usage\ndefault_ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\ndefault_ctx.sni_callback = sni_handler.sni_callback\n\
          ```"
        level3: "HTTPS redirect:\n```python\nasync def handle_http(reader, writer):\n    request = await read_http_request(reader)\n\
          \    \n    # Redirect to HTTPS\n    host = request.headers.get('Host', 'localhost')\n    location = f'https://{host}{request.path}'\n\
          \    \n    response = f'''HTTP/1.1 301 Moved Permanently\r\nLocation: {location}\r\nContent-Length: 0\r\nConnection:\
          \ close\r\n\r\n'''\n    writer.write(response.encode())\n    await writer.drain()\n    writer.close()\n```"
      acceptance_criteria:
      - Terminate TLS connections at proxy and forward plain HTTP to backends
      - Support SNI to serve multiple domains with different certificates
      - Forward decrypted requests to backend servers over plain HTTP
      - Handle certificate reload without restarting the proxy process
  lock-free-structures:
    name: Lock-free Data Structures
    description: Build lock-free concurrent data structures using atomic operations and CAS
    category: Performance
    difficulty: advanced
    estimated_hours: 50
    skills:
    - Atomic operations
    - Compare-and-swap
    - Memory ordering
    - ABA problem
    - Hazard pointers
    - Lock-free algorithms
    prerequisites:
    - Concurrency basics
    - Memory model
    - C/C++ or Rust
    learning_outcomes:
    - Understand lock-free programming principles
    - Implement CAS-based algorithms
    - Handle ABA problem and memory reclamation
    - Build high-performance concurrent structures
    milestones:
    - name: Atomic Operations
      description: Understanding atomics and memory ordering
      skills:
      - Atomics
      - Memory ordering
      - CAS
      deliverables:
      - Atomic load and store wrappers that read and write shared variables with specified memory ordering
      - Compare-and-swap wrapper that atomically updates a variable only if it equals the expected value
      - Fetch-and-add operation that atomically increments a shared counter and returns the previous value
      - Memory ordering modes supporting relaxed, acquire, release, and sequentially consistent semantics
      - Compiler and CPU memory barrier primitives that prevent instruction reordering across fence points
      - Atomic reference counter that safely tracks shared ownership using atomic increment and decrement
      hints:
        level1: |-
          Basic CAS loop:
          ```c
          #include <stdatomic.h>

          typedef struct {
              _Atomic int value;
          } AtomicInt;

          void atomic_increment(AtomicInt *a) {
              int old = atomic_load(&a->value);
              while (!atomic_compare_exchange_weak(&a->value, &old, old + 1)) {
                  // CAS failed, old is updated with current value
                  // Loop and retry
              }
          }
          ```
        level2: |-
          Memory ordering:
          ```c
          // Producer-consumer with acquire-release
          _Atomic int data;
          _Atomic int flag;

          // Producer
          void produce(int value) {
              atomic_store_explicit(&data, value, memory_order_relaxed);
              atomic_store_explicit(&flag, 1, memory_order_release);  // Release
          }

          // Consumer
          int consume() {
              while (atomic_load_explicit(&flag, memory_order_acquire) == 0);  // Acquire
              return atomic_load_explicit(&data, memory_order_relaxed);
          }
          ```
        level3: |-
          Atomic reference counting:
          ```c
          typedef struct {
              _Atomic int ref_count;
              void *data;
          } RefCounted;

          void retain(RefCounted *obj) {
              atomic_fetch_add(&obj->ref_count, 1);
          }

          void release(RefCounted *obj) {
              if (atomic_fetch_sub(&obj->ref_count, 1) == 1) {
                  // We were the last reference
                  free(obj->data);
                  free(obj);
              }
          }
          ```
      acceptance_criteria:
      - Compare-and-swap wrapper correctly returns success or failure and the observed value on failure
      - Memory ordering semantics for relaxed, acquire, release, and seq_cst are correctly applied to each operation
      - ABA problem is explained and demonstrated with a test case showing the scenario and its consequences
      - Atomic counter incremented by multiple threads produces the exact expected total with no lost updates
    - name: Lock-free Stack
      description: Treiber stack implementation
      skills:
      - Treiber stack
      - CAS-based push/pop
      - ABA handling
      deliverables:
      - Node structure containing a data payload and an atomic pointer to the next node in the stack
      - Lock-free push operation that uses CAS to atomically prepend a new node to the stack top
      - Lock-free pop operation that uses CAS to atomically remove and return the top node from the stack
      - ABA problem demonstration showing how a naive CAS loop can incorrectly succeed on a reused pointer
      - Tagged pointer solution that appends a monotonic counter to pointers to detect ABA conditions
      - Stack traversal function that iterates through all nodes from top to bottom for debugging
      hints:
        level1: "Treiber stack structure:\n```c\ntypedef struct Node {\n    void *data;\n    struct Node *next;\n} Node;\n\
          \ntypedef struct {\n    _Atomic(Node*) top;\n} LockFreeStack;\n\nvoid push(LockFreeStack *stack, void *data) {\n\
          \    Node *new_node = malloc(sizeof(Node));\n    new_node->data = data;\n    \n    Node *old_top = atomic_load(&stack->top);\n\
          \    do {\n        new_node->next = old_top;\n    } while (!atomic_compare_exchange_weak(&stack->top, &old_top,\
          \ new_node));\n}\n```"
        level2: "Lock-free pop:\n```c\nvoid *pop(LockFreeStack *stack) {\n    Node *old_top = atomic_load(&stack->top);\n\
          \    Node *new_top;\n    \n    do {\n        if (old_top == NULL) {\n            return NULL;  // Stack is empty\n\
          \        }\n        new_top = old_top->next;\n    } while (!atomic_compare_exchange_weak(&stack->top, &old_top,\
          \ new_top));\n    \n    void *data = old_top->data;\n    // WARNING: Can't free old_top here (ABA problem)\n   \
          \ return data;\n}\n```"
        level3: "Tagged pointer to solve ABA:\n```c\ntypedef struct {\n    Node *ptr;\n    uint64_t tag;  // Version counter\n\
          } TaggedPtr;\n\n// Pack pointer and tag into 128-bit value for double-width CAS\n// Or use lower bits of aligned\
          \ pointer for tag\n\nvoid *pop_safe(LockFreeStack *stack) {\n    TaggedPtr old_top = atomic_load(&stack->top);\n\
          \    TaggedPtr new_top;\n    \n    do {\n        if (old_top.ptr == NULL) return NULL;\n        new_top.ptr = old_top.ptr->next;\n\
          \        new_top.tag = old_top.tag + 1;  // Increment tag\n    } while (!atomic_compare_exchange_weak(&stack->top,\
          \ &old_top, new_top));\n    \n    return old_top.ptr->data;\n}\n```"
      acceptance_criteria:
      - Treiber stack uses CAS on the top pointer to push and pop nodes without any mutex locks
      - Concurrent push and pop operations from multiple threads produce no lost or duplicated elements
      - Stack operations are linearizable meaning each appears to occur atomically at some point in time
      - Benchmark shows lock-free stack throughput compared against a mutex-protected stack under contention
    - name: Lock-free Queue
      description: Michael-Scott queue implementation
      skills:
      - MS queue
      - Two-pointer queue
      - Helping mechanism
      deliverables:
      - Queue structure with atomic head and tail pointers for the enqueue and dequeue ends
      - Dummy node initializer that creates an empty queue with both head and tail pointing to a sentinel node
      - Lock-free enqueue operation that appends a new node at the tail using two-step CAS
      - Lock-free dequeue operation that removes and returns the node at the head using CAS
      - Tail-update helping mechanism where dequeuers advance a lagging tail pointer they observe
      - Linearizability argument documenting why the queue appears atomic to all concurrent observers
      hints:
        level1: |-
          Queue structure:
          ```c
          typedef struct Node {
              void *data;
              _Atomic(struct Node*) next;
          } Node;

          typedef struct {
              _Atomic(Node*) head;
              _Atomic(Node*) tail;
          } LockFreeQueue;

          void init(LockFreeQueue *q) {
              Node *dummy = malloc(sizeof(Node));
              dummy->data = NULL;
              atomic_store(&dummy->next, NULL);
              atomic_store(&q->head, dummy);
              atomic_store(&q->tail, dummy);
          }
          ```
        level2: "Enqueue with helping:\n```c\nvoid enqueue(LockFreeQueue *q, void *data) {\n    Node *new_node = malloc(sizeof(Node));\n\
          \    new_node->data = data;\n    atomic_store(&new_node->next, NULL);\n    \n    while (1) {\n        Node *tail\
          \ = atomic_load(&q->tail);\n        Node *next = atomic_load(&tail->next);\n        \n        if (tail == atomic_load(&q->tail))\
          \ {  // Still consistent?\n            if (next == NULL) {\n                // Tail is really last, try to link\
          \ new node\n                if (atomic_compare_exchange_weak(&tail->next, &next, new_node)) {\n                \
          \    // Success, try to update tail\n                    atomic_compare_exchange_weak(&q->tail, &tail, new_node);\n\
          \                    return;\n                }\n            } else {\n                // Tail is behind, help advance\
          \ it\n                atomic_compare_exchange_weak(&q->tail, &tail, next);\n            }\n        }\n    }\n}\n\
          ```"
        level3: "Dequeue:\n```c\nvoid *dequeue(LockFreeQueue *q) {\n    while (1) {\n        Node *head = atomic_load(&q->head);\n\
          \        Node *tail = atomic_load(&q->tail);\n        Node *next = atomic_load(&head->next);\n        \n       \
          \ if (head == atomic_load(&q->head)) {\n            if (head == tail) {\n                if (next == NULL) {\n \
          \                   return NULL;  // Queue is empty\n                }\n                // Tail is behind, help\
          \ advance\n                atomic_compare_exchange_weak(&q->tail, &tail, next);\n            } else {\n        \
          \        // Read data before CAS\n                void *data = next->data;\n                if (atomic_compare_exchange_weak(&q->head,\
          \ &head, next)) {\n                    // Don't free head (memory reclamation needed)\n                    return\
          \ data;\n                }\n            }\n        }\n    }\n}\n```"
      acceptance_criteria:
      - Michael-Scott queue algorithm is implemented with separate CAS operations on head and tail pointers
      - Dummy sentinel node separates head and tail operations to avoid contention between enqueue and dequeue
      - FIFO ordering is preserved so elements dequeued appear in exactly the order they were enqueued
      - Empty queue dequeue correctly returns a not-found indicator without blocking or crashing
    - name: Hazard Pointers
      description: Safe memory reclamation
      skills:
      - Hazard pointers
      - Memory reclamation
      - Deferred free
      deliverables:
      - Hazard pointer registry that maintains a global list of per-thread hazard pointer slots
      - Protect and release operations that announce and clear a thread's intent to access a shared node
      - Retired list that collects nodes removed from the data structure but not yet safe to free
      - Scan-and-reclaim routine that frees retired nodes not referenced by any thread's hazard pointers
      - Thread-local hazard pointer slots that minimize contention on the global registry during access
      - Integration adapter that adds hazard pointer protection to existing lock-free stack and queue operations
      hints:
        level1: |-
          Hazard pointer structure:
          ```c
          #define MAX_THREADS 64
          #define HP_PER_THREAD 2

          typedef struct {
              _Atomic(void*) hp[MAX_THREADS][HP_PER_THREAD];
              void *retired[MAX_THREADS][1024];  // Per-thread retired list
              int retired_count[MAX_THREADS];
          } HazardPointerDomain;

          void hp_protect(HazardPointerDomain *d, int thread_id, int hp_index, void *ptr) {
              atomic_store(&d->hp[thread_id][hp_index], ptr);
          }

          void hp_clear(HazardPointerDomain *d, int thread_id, int hp_index) {
              atomic_store(&d->hp[thread_id][hp_index], NULL);
          }
          ```
        level2: "Retire and reclaim:\n```c\nvoid hp_retire(HazardPointerDomain *d, int thread_id, void *ptr) {\n    d->retired[thread_id][d->retired_count[thread_id]++]\
          \ = ptr;\n    \n    if (d->retired_count[thread_id] >= 100) {\n        hp_scan(d, thread_id);\n    }\n}\n\nvoid\
          \ hp_scan(HazardPointerDomain *d, int thread_id) {\n    // Collect all hazard pointers\n    void *protected[MAX_THREADS\
          \ * HP_PER_THREAD];\n    int pcount = 0;\n    for (int t = 0; t < MAX_THREADS; t++) {\n        for (int h = 0; h\
          \ < HP_PER_THREAD; h++) {\n            void *hp = atomic_load(&d->hp[t][h]);\n            if (hp) protected[pcount++]\
          \ = hp;\n        }\n    }\n    \n    // Free retired nodes not in protected set\n    // ...\n}\n```"
        level3: "Use with stack:\n```c\nvoid *pop_safe(LockFreeStack *stack, HazardPointerDomain *hp, int tid) {\n    while\
          \ (1) {\n        Node *old_top = atomic_load(&stack->top);\n        if (old_top == NULL) return NULL;\n        \n\
          \        // Protect old_top with hazard pointer\n        hp_protect(hp, tid, 0, old_top);\n        \n        //\
          \ Verify it's still the top\n        if (old_top != atomic_load(&stack->top)) continue;\n        \n        Node\
          \ *new_top = old_top->next;\n        if (atomic_compare_exchange_weak(&stack->top, &old_top, new_top)) {\n     \
          \       void *data = old_top->data;\n            hp_clear(hp, tid, 0);\n            hp_retire(hp, tid, old_top);\
          \  // Safe deferred free\n            return data;\n        }\n    }\n}\n```"
      acceptance_criteria:
      - Hazard pointer scheme correctly prevents premature deallocation of nodes still in use by other threads
      - In-use pointers are tracked per thread so the scan routine knows which retired nodes are still live
      - Retired nodes not appearing in any thread's hazard list are safely reclaimed and freed
      - Thread exit cleanup releases all hazard pointer slots and reclaims all remaining retired nodes
    - name: Lock-free Hash Map
      description: Concurrent hash map with lock-free operations
      skills:
      - Concurrent hashing
      - Split-ordered lists
      - Resizing
      deliverables:
      - Hash bucket array with each bucket containing an atomic pointer to a linked list of entries
      - Lock-free insert operation that adds a key-value pair to the appropriate bucket using CAS
      - Lock-free lookup operation that traverses the bucket chain to find a matching key without locks
      - Lock-free delete operation that marks and physically removes an entry from its bucket chain
      - Atomic resize mechanism that gradually migrates entries from old buckets to a new larger array
      - Load factor monitor that triggers resize when the average bucket chain length exceeds a threshold
      hints:
        level1: |-
          Simple lock-free map structure:
          ```c
          typedef struct Entry {
              uint64_t key;
              void *value;
              _Atomic(struct Entry*) next;
          } Entry;

          typedef struct {
              _Atomic(Entry*) buckets[INITIAL_SIZE];
              _Atomic size_t size;
              _Atomic size_t count;
          } LockFreeHashMap;

          uint64_t hash(uint64_t key, size_t size) {
              return key % size;
          }
          ```
        level2: "Lock-free insert:\n```c\nbool insert(LockFreeHashMap *map, uint64_t key, void *value) {\n    Entry *new_entry\
          \ = malloc(sizeof(Entry));\n    new_entry->key = key;\n    new_entry->value = value;\n    \n    size_t idx = hash(key,\
          \ atomic_load(&map->size));\n    \n    while (1) {\n        Entry *head = atomic_load(&map->buckets[idx]);\n   \
          \     \n        // Check if key exists\n        for (Entry *e = head; e != NULL; e = atomic_load(&e->next)) {\n\
          \            if (e->key == key) {\n                free(new_entry);\n                return false;  // Already exists\n\
          \            }\n        }\n        \n        atomic_store(&new_entry->next, head);\n        if (atomic_compare_exchange_weak(&map->buckets[idx],\
          \ &head, new_entry)) {\n            atomic_fetch_add(&map->count, 1);\n            return true;\n        }\n   \
          \ }\n}\n```"
        level3: "Lock-free delete with marking:\n```c\n// Use pointer's low bit as \"marked for deletion\" flag\n#define MARKED(p)\
          \ ((Entry*)((uintptr_t)(p) | 1))\n#define UNMARKED(p) ((Entry*)((uintptr_t)(p) & ~1))\n#define IS_MARKED(p) ((uintptr_t)(p)\
          \ & 1)\n\nbool delete(LockFreeHashMap *map, uint64_t key) {\n    size_t idx = hash(key, atomic_load(&map->size));\n\
          \    \n    while (1) {\n        Entry **prev = &map->buckets[idx];\n        Entry *curr = atomic_load(prev);\n \
          \       \n        while (curr != NULL) {\n            Entry *next = atomic_load(&curr->next);\n            \n  \
          \          if (IS_MARKED(next)) {\n                // Help remove marked node\n                atomic_compare_exchange_weak(prev,\
          \ &curr, UNMARKED(next));\n                curr = atomic_load(prev);\n            } else if (curr->key == key) {\n\
          \                // Mark for deletion\n                if (atomic_compare_exchange_weak(&curr->next, &next, MARKED(next)))\
          \ {\n                    // Physically remove\n                    atomic_compare_exchange_weak(prev, &curr, next);\n\
          \                    return true;\n                }\n            } else {\n                prev = &curr->next;\n\
          \                curr = next;\n            }\n        }\n        return false;\n    }\n}\n```"
      acceptance_criteria:
      - Lock-free hash map supports concurrent insert, lookup, and delete without any mutex locks
      - Bucket resizing migrates entries to a larger array without blocking concurrent readers or writers
      - Concurrent reads and writes from multiple threads produce correct results with no lost updates or phantom reads
      - Throughput benchmark compares lock-free hash map operations per second against a mutex-based hash map
  fuzzer:
    name: Fuzzing Framework
    description: Build a coverage-guided fuzzer like AFL for automated bug finding
    category: Security
    difficulty: advanced
    estimated_hours: 55
    skills:
    - Coverage instrumentation
    - Mutation strategies
    - Corpus management
    - Crash detection
    - Input minimization
    - Parallel fuzzing
    prerequisites:
    - Binary basics
    - Testing
    - Process management
    learning_outcomes:
    - Understand fuzzing principles and techniques
    - Implement coverage-guided mutation
    - Build crash detection and triage
    - Design efficient fuzzing campaigns
    milestones:
    - name: Target Execution
      description: Execute target program with inputs
      skills:
      - Process execution
      - Timeout handling
      - Exit code analysis
      deliverables:
      - Fork and exec wrapper for launching target program with test input
      - Stdin and file input modes for delivering test data to target
      - Timeout enforcement killing target if execution exceeds time limit
      - Crash detection via signal analysis identifying segfaults and aborts
      - Resource limits preventing target from consuming excessive system resources
      - Exit status collection capturing return code and signal information
      hints:
        level1: "Basic target runner:\n```python\nimport subprocess\nimport signal\n\nclass TargetRunner:\n    def __init__(self,\
          \ target_cmd, timeout=1.0):\n        self.target_cmd = target_cmd\n        self.timeout = timeout\n    \n    def\
          \ run(self, input_data):\n        try:\n            result = subprocess.run(\n                self.target_cmd,\n\
          \                input=input_data,\n                capture_output=True,\n                timeout=self.timeout\n\
          \            )\n            return ExecutionResult(\n                exit_code=result.returncode,\n            \
          \    stdout=result.stdout,\n                stderr=result.stderr,\n                crashed=result.returncode < 0\n\
          \            )\n        except subprocess.TimeoutExpired:\n            return ExecutionResult(timeout=True)\n```"
        level2: "Detect crash signals:\n```python\ndef classify_crash(exit_code):\n    if exit_code >= 0:\n        return\
          \ None\n    \n    sig = -exit_code\n    crash_signals = {\n        signal.SIGSEGV: 'segfault',\n        signal.SIGABRT:\
          \ 'abort',\n        signal.SIGFPE: 'floating_point',\n        signal.SIGBUS: 'bus_error',\n        signal.SIGILL:\
          \ 'illegal_instruction',\n    }\n    return crash_signals.get(sig, f'signal_{sig}')\n```"
        level3: "Persistent mode (fork server):\n```c\n// In instrumented target\nvoid __afl_forkserver() {\n    while (1)\
          \ {\n        // Wait for fuzzer\n        int status;\n        read(FORKSRV_FD, &status, 4);\n        \n        pid_t\
          \ child = fork();\n        if (child == 0) {\n            // Child: close forkserver fds and continue\n        \
          \    close(FORKSRV_FD);\n            return;\n        }\n        \n        // Parent: report child pid and wait\n\
          \        write(FORKSRV_FD + 1, &child, 4);\n        waitpid(child, &status, 0);\n        write(FORKSRV_FD + 1, &status,\
          \ 4);\n    }\n}\n```"
      acceptance_criteria:
      - Execute target program with provided test input data via configured delivery
      - Capture exit code and crash signals to classify execution outcome
      - Handle target timeout by killing the process after configurable deadline
      - Support different input delivery methods including stdin, file, and argv
    - name: Coverage Tracking
      description: Track code coverage for guidance
      skills:
      - Instrumentation
      - Edge coverage
      - Bitmap
      deliverables:
      - Coverage bitmap for recording visited edge and branch information
      - Edge and branch coverage tracking via compile-time instrumentation
      - Compile-time instrumentation inserting coverage probes into target code
      - Coverage comparison detecting differences between execution runs
      - New coverage detection identifying previously unseen edge transitions
      - Coverage visualization showing which code paths have been exercised
      hints:
        level1: "Coverage bitmap:\n```python\nclass CoverageBitmap:\n    def __init__(self, size=65536):\n        self.size\
          \ = size\n        self.bitmap = bytearray(size)\n        self.virgin = bytearray(b'\\xff' * size)  # Never-seen\
          \ bits\n    \n    def has_new_coverage(self, exec_bitmap):\n        new_bits = False\n        for i in range(self.size):\n\
          \            if exec_bitmap[i] and self.virgin[i]:\n                # New coverage found\n                self.virgin[i]\
          \ &= ~exec_bitmap[i]\n                self.bitmap[i] |= exec_bitmap[i]\n                new_bits = True\n      \
          \  return new_bits\n```"
        level2: |-
          Compile-time instrumentation (LLVM pass concept):
          ```c
          // Injected at each basic block
          uint8_t __afl_area[65536];
          uint32_t __afl_prev_loc;

          void __afl_trace(uint32_t cur_loc) {
              // Edge = prev_loc XOR cur_loc
              __afl_area[cur_loc ^ __afl_prev_loc]++;
              __afl_prev_loc = cur_loc >> 1;
          }

          // Or use compiler flag: clang -fsanitize-coverage=trace-pc-guard
          ```
        level3: "Coverage-guided input selection:\n```python\nclass Corpus:\n    def __init__(self):\n        self.inputs\
          \ = []  # (input_data, coverage_hash)\n    \n    def add_if_interesting(self, input_data, coverage):\n        cov_hash\
          \ = hash(bytes(coverage))\n        \n        # Check if this coverage is new\n        if self.bitmap.has_new_coverage(coverage):\n\
          \            self.inputs.append(CorpusEntry(\n                data=input_data,\n                coverage=coverage,\n\
          \                found_at=time.time()\n            ))\n            return True\n        return False\n    \n   \
          \ def select(self):\n        # Favor smaller inputs and inputs that found new coverage recently\n        weights\
          \ = [1.0 / (len(e.data) + 1) for e in self.inputs]\n        return random.choices(self.inputs, weights=weights)[0]\n\
          ```"
      acceptance_criteria:
      - Instrument target for code coverage using llvm-cov or similar tooling
      - Track basic block and edge coverage using shared memory bitmap
      - Detect new coverage from test inputs by comparing against global bitmap
      - Build and maintain coverage bitmap recording all observed edge transitions
    - name: Mutation Engine
      description: Generate new test inputs via mutation
      skills:
      - Bit flips
      - Arithmetic
      - Dictionary
      - Splice
      deliverables:
      - Bit flip mutations toggling individual bits at each position
      - Byte flip mutations inverting full bytes for broader changes
      - Arithmetic mutations adding and subtracting small values from fields
      - Block operations including byte insertion, deletion, and overwrite
      - Dictionary-based mutations inserting known interesting byte sequences
      - Havoc mode applying random combinations of multiple mutations per input
      hints:
        level1: "Basic mutations:\n```python\nclass Mutator:\n    def bit_flip(self, data, pos):\n        data = bytearray(data)\n\
          \        byte_pos = pos // 8\n        bit_pos = pos % 8\n        data[byte_pos] ^= (1 << bit_pos)\n        return\
          \ bytes(data)\n    \n    def byte_flip(self, data, pos):\n        data = bytearray(data)\n        data[pos] ^= 0xFF\n\
          \        return bytes(data)\n    \n    def interesting_value(self, data, pos, size):\n        interesting = [0,\
          \ 1, -1, 127, 128, 255, 256, 32767, 65535]\n        data = bytearray(data)\n        val = random.choice(interesting)\n\
          \        # Write val at pos with given size\n        return bytes(data)\n```"
        level2: |-
          Arithmetic and block mutations:
          ```python
          def arithmetic(self, data, pos, size):
              data = bytearray(data)
              val = int.from_bytes(data[pos:pos+size], 'little')
              delta = random.randint(-35, 35)
              new_val = (val + delta) & ((1 << (size*8)) - 1)
              data[pos:pos+size] = new_val.to_bytes(size, 'little')
              return bytes(data)

          def insert_bytes(self, data, pos, count):
              new_bytes = bytes([random.randint(0, 255) for _ in range(count)])
              return data[:pos] + new_bytes + data[pos:]

          def delete_bytes(self, data, pos, count):
              return data[:pos] + data[pos+count:]
          ```
        level3: "Havoc mode:\n```python\ndef havoc(self, data):\n    data = bytearray(data)\n    num_mutations = random.randint(1,\
          \ 16)\n    \n    mutations = [\n        self.bit_flip_random,\n        self.byte_flip_random,\n        self.arithmetic_random,\n\
          \        self.overwrite_random,\n        self.insert_random,\n        self.delete_random,\n        self.splice,\n\
          \        self.dictionary_insert,\n    ]\n    \n    for _ in range(num_mutations):\n        mutation = random.choice(mutations)\n\
          \        data = mutation(data)\n    \n    return bytes(data)\n```"
      acceptance_criteria:
      - Implement bit flip mutations that toggle bits at each position sequentially
      - Support byte insertion and deletion mutations changing input structure
      - Implement arithmetic mutations adding and subtracting values from integer fields
      - Use coverage feedback to guide mutation strategy toward unexplored code paths
    - name: Corpus Management
      description: Manage and minimize test corpus
      skills:
      - Corpus storage
      - Minimization
      - Deduplication
      deliverables:
      - Corpus storage saving interesting inputs as individual files on disk
      - Input minimization reducing test case size while preserving crash behavior
      - Corpus distillation removing redundant inputs with duplicate coverage
      - Crash deduplication grouping crashes by unique stack trace signatures
      - Coverage-based scoring prioritizing inputs that cover rare edge transitions
      - Queue scheduling selecting next input for mutation based on coverage score
      hints:
        level1: "Corpus storage:\n```python\nclass CorpusManager:\n    def __init__(self, corpus_dir):\n        self.corpus_dir\
          \ = Path(corpus_dir)\n        self.queue_dir = self.corpus_dir / 'queue'\n        self.crashes_dir = self.corpus_dir\
          \ / 'crashes'\n        self.queue_dir.mkdir(parents=True, exist_ok=True)\n        self.crashes_dir.mkdir(exist_ok=True)\n\
          \    \n    def save_input(self, data, coverage_hash):\n        filename = f'id:{self.next_id():06d},cov:{coverage_hash:08x}'\n\
          \        path = self.queue_dir / filename\n        path.write_bytes(data)\n        return path\n    \n    def save_crash(self,\
          \ data, crash_type):\n        sig = hashlib.sha256(data).hexdigest()[:8]\n        filename = f'{crash_type}_{sig}'\n\
          \        path = self.crashes_dir / filename\n        path.write_bytes(data)\n```"
        level2: "Input minimization:\n```python\ndef minimize(self, data, check_fn):\n    \"\"\"Minimize input while preserving\
          \ interesting behavior\"\"\"\n    # Binary search for minimum size\n    while len(data) > 1:\n        mid = len(data)\
          \ // 2\n        \n        # Try first half\n        if check_fn(data[:mid]):\n            data = data[:mid]\n  \
          \          continue\n        \n        # Try second half\n        if check_fn(data[mid:]):\n            data = data[mid:]\n\
          \            continue\n        \n        # Try removing chunks\n        reduced = False\n        chunk_size = max(1,\
          \ len(data) // 16)\n        for i in range(0, len(data), chunk_size):\n            candidate = data[:i] + data[i+chunk_size:]\n\
          \            if check_fn(candidate):\n                data = candidate\n                reduced = True\n       \
          \         break\n        \n        if not reduced:\n            break\n    \n    return data\n```"
        level3: "Crash deduplication:\n```python\nclass CrashDeduplicator:\n    def __init__(self):\n        self.seen_crashes\
          \ = set()\n    \n    def get_crash_signature(self, crash_info):\n        # Use stack trace hash for deduplication\n\
          \        if crash_info.stack_trace:\n            frames = crash_info.stack_trace[:5]  # Top 5 frames\n         \
          \   return hash(tuple(frames))\n        \n        # Fallback to coverage hash at crash point\n        return hash(bytes(crash_info.coverage[-100:]))\n\
          \    \n    def is_unique(self, crash_info):\n        sig = self.get_crash_signature(crash_info)\n        if sig\
          \ in self.seen_crashes:\n            return False\n        self.seen_crashes.add(sig)\n        return True\n```"
      acceptance_criteria:
      - Store inputs that increase code coverage into the persistent corpus directory
      - Minimize corpus by removing redundant inputs that cover same code paths
      - Support initial seed corpus loading user-provided starting inputs for fuzzing
      - Export crash-inducing inputs with reproduction information for bug reporting
    - name: Fuzzing Loop
      description: Main fuzzing orchestration
      skills:
      - Scheduling
      - Statistics
      - Parallel fuzzing
      deliverables:
      - Main fuzzing loop orchestrating selection, mutation, and execution cycle
      - Statistics and reporting displaying executions per second and coverage metrics
      - Parallel and distributed fuzzing running multiple instances simultaneously
      - Sync between fuzzer instances sharing interesting inputs across workers
      - Adaptive mutation selection favoring strategies that discover new coverage
      - Timeout and resource management preventing runaway target processes
      hints:
        level1: "Main fuzzing loop:\n```python\nclass Fuzzer:\n    def run(self):\n        self.load_corpus()\n        \n\
          \        while True:\n            # Select input from corpus\n            entry = self.corpus.select()\n       \
          \     \n            # Mutate\n            for _ in range(self.mutations_per_input):\n                mutated = self.mutator.mutate(entry.data)\n\
          \                \n                # Execute\n                result = self.runner.run(mutated)\n              \
          \  self.stats.total_execs += 1\n                \n                # Check for crash\n                if result.crashed:\n\
          \                    self.save_crash(mutated, result)\n                    continue\n                \n        \
          \        # Check for new coverage\n                if self.corpus.add_if_interesting(mutated, result.coverage):\n\
          \                    self.stats.new_paths += 1\n            \n            self.print_stats()\n```"
        level2: "Statistics:\n```python\nclass FuzzerStats:\n    def __init__(self):\n        self.start_time = time.time()\n\
          \        self.total_execs = 0\n        self.new_paths = 0\n        self.crashes = 0\n        self.timeouts = 0\n\
          \    \n    def print_status(self):\n        elapsed = time.time() - self.start_time\n        exec_rate = self.total_execs\
          \ / elapsed if elapsed > 0 else 0\n        \n        print(f'''\n        Fuzzer Status\n        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\
          \        Runtime     : {elapsed:.0f}s\n        Executions  : {self.total_execs}\n        Exec/sec    : {exec_rate:.0f}\n\
          \        Corpus size : {self.corpus_size}\n        Crashes     : {self.crashes}\n        Coverage    : {self.coverage_pct:.1f}%\n\
          \        ''')\n```"
        level3: "Parallel fuzzing with sync:\n```python\nclass ParallelFuzzer:\n    def __init__(self, num_workers, sync_dir):\n\
          \        self.num_workers = num_workers\n        self.sync_dir = Path(sync_dir)\n    \n    def run_worker(self,\
          \ worker_id):\n        fuzzer = Fuzzer(corpus_dir=self.sync_dir / f'worker_{worker_id}')\n        \n        while\
          \ True:\n            # Run fuzzing iterations\n            fuzzer.fuzz_one()\n            \n            # Periodically\
          \ sync with other workers\n            if fuzzer.stats.total_execs % 1000 == 0:\n                self.sync_corpus(worker_id)\n\
          \    \n    def sync_corpus(self, worker_id):\n        # Import interesting inputs from other workers\n        for\
          \ other_dir in self.sync_dir.glob('worker_*'):\n            if other_dir.name == f'worker_{worker_id}':\n      \
          \          continue\n            for input_file in (other_dir / 'queue').glob('*'):\n                self.try_import(input_file)\n\
          ```"
      acceptance_criteria:
      - Pick input from corpus based on coverage-weighted selection strategy
      - Mutate selected input and execute target to observe coverage and crashes
      - Track coverage bitmap and update corpus when new edges are discovered
      - Report crashes with full reproduction steps including input and crash details
  profiler:
    name: CPU/Memory Profiler
    description: Build a sampling profiler with flame graphs and memory tracking
    category: Performance
    difficulty: intermediate
    estimated_hours: 45
    skills:
    - Stack sampling
    - Timer signals
    - Symbol resolution
    - Flame graphs
    - Memory tracking
    - Profile analysis
    prerequisites:
    - C programming
    - Process internals
    - Debug symbols
    learning_outcomes:
    - Understand sampling-based profiling
    - Implement stack trace collection
    - Build flame graph visualization
    - Design memory allocation tracking
    milestones:
    - name: Stack Sampling
      description: Periodically sample call stacks
      skills:
      - Signal handling
      - Stack walking
      - Timer setup
      deliverables:
      - Timer-based sampling using SIGPROF or ITIMER configuration
      - Signal handler capturing call stack snapshot on each sample
      - Stack frame walking traversing frame pointers to build call chain
      - Sample storage accumulating captured stacks for later analysis
      - Configurable sampling frequency controlling overhead versus accuracy
      - Low-overhead design minimizing impact on profiled application
      hints:
        level1: "Setup sampling timer:\n```c\n#include <signal.h>\n#include <sys/time.h>\n\nvoid setup_profiler(int frequency_hz)\
          \ {\n    struct sigaction sa;\n    sa.sa_handler = profile_signal_handler;\n    sa.sa_flags = SA_RESTART;\n    sigaction(SIGPROF,\
          \ &sa, NULL);\n    \n    struct itimerval timer;\n    timer.it_interval.tv_sec = 0;\n    timer.it_interval.tv_usec\
          \ = 1000000 / frequency_hz;\n    timer.it_value = timer.it_interval;\n    setitimer(ITIMER_PROF, &timer, NULL);\n\
          }\n```"
        level2: "Capture stack in signal handler:\n```c\n#include <execinfo.h>\n\n#define MAX_FRAMES 128\n#define MAX_SAMPLES\
          \ 100000\n\nvoid *samples[MAX_SAMPLES][MAX_FRAMES];\nint sample_depths[MAX_SAMPLES];\nint sample_count = 0;\n\n\
          void profile_signal_handler(int sig) {\n    if (sample_count >= MAX_SAMPLES) return;\n    \n    void *frames[MAX_FRAMES];\n\
          \    int depth = backtrace(frames, MAX_FRAMES);\n    \n    memcpy(samples[sample_count], frames, depth * sizeof(void*));\n\
          \    sample_depths[sample_count] = depth;\n    sample_count++;\n}\n```"
        level3: "Use libunwind for better stacks:\n```c\n#define UNW_LOCAL_ONLY\n#include <libunwind.h>\n\nint capture_stack(void\
          \ **buffer, int max_depth) {\n    unw_cursor_t cursor;\n    unw_context_t context;\n    \n    unw_getcontext(&context);\n\
          \    unw_init_local(&cursor, &context);\n    \n    int depth = 0;\n    while (unw_step(&cursor) > 0 && depth < max_depth)\
          \ {\n        unw_word_t pc;\n        unw_get_reg(&cursor, UNW_REG_IP, &pc);\n        buffer[depth++] = (void*)pc;\n\
          \    }\n    return depth;\n}\n```"
      acceptance_criteria:
      - Sample call stacks at configurable regular time intervals
      - Capture both user-space and kernel stack frames when available
      - Support different sampling rates from 10Hz to 10KHz
      - Support targeting specific processes or individual threads
    - name: Symbol Resolution
      description: Convert addresses to function names
      skills:
      - Debug symbols
      - DWARF
      - addr2line
      deliverables:
      - Symbol table loading from ELF binary and shared library files
      - Address-to-function mapping resolving instruction pointers to names
      - Source file and line number information from debug metadata
      - C++ name demangling converting mangled symbols to readable names
      - Shared library symbol handling resolving addresses in loaded .so files
      - Symbol cache reducing repeated lookups for hot addresses
      hints:
        level1: |-
          Use dladdr for basic resolution:
          ```c
          #include <dlfcn.h>

          char *resolve_symbol(void *addr) {
              Dl_info info;
              if (dladdr(addr, &info) && info.dli_sname) {
                  return strdup(info.dli_sname);
              }
              // Fallback to address
              char buf[32];
              snprintf(buf, sizeof(buf), "0x%lx", (unsigned long)addr);
              return strdup(buf);
          }
          ```
        level2: "Use addr2line for source info:\n```python\nimport subprocess\n\nclass SymbolResolver:\n    def __init__(self,\
          \ binary):\n        self.binary = binary\n        self.cache = {}\n    \n    def resolve(self, address):\n     \
          \   if address in self.cache:\n            return self.cache[address]\n        \n        result = subprocess.run(\n\
          \            ['addr2line', '-f', '-e', self.binary, hex(address)],\n            capture_output=True, text=True\n\
          \        )\n        lines = result.stdout.strip().split('\\n')\n        func = lines[0] if lines else '??'\n   \
          \     location = lines[1] if len(lines) > 1 else '??:0'\n        \n        self.cache[address] = (func, location)\n\
          \        return (func, location)\n```"
        level3: "Demangle C++ names:\n```python\nimport subprocess\n\ndef demangle(name):\n    if not name.startswith('_Z'):\n\
          \        return name\n    \n    result = subprocess.run(\n        ['c++filt', name],\n        capture_output=True,\
          \ text=True\n    )\n    return result.stdout.strip()\n\n# Or use __cxa_demangle in C\n```"
      acceptance_criteria:
      - Load symbol tables from ELF binaries and shared libraries
      - Resolve raw memory addresses to human-readable function names
      - Parse DWARF debug information for source file and line numbers
      - Support JIT-compiled code symbol resolution when mapping is available
    - name: Flame Graph Generation
      description: Visualize profiles as flame graphs
      skills:
      - Data aggregation
      - SVG generation
      - Interactive UI
      deliverables:
      - Stack aggregation merging identical call chains with sample counts
      - Folded stack format output compatible with standard flame graph tools
      - Interactive SVG flame graph with color-coded function categories
      - Color coding distinguishing user code, library, and kernel functions
      - Zoom and search functionality for navigating large flame graphs
      - Differential flame graph comparing two profiles side by side
      hints:
        level1: "Generate folded stacks:\n```python\ndef generate_folded_stacks(samples):\n    stacks = {}\n    for sample\
          \ in samples:\n        # Reverse so caller is first\n        stack_str = ';'.join(reversed(sample.frames))\n   \
          \     stacks[stack_str] = stacks.get(stack_str, 0) + 1\n    \n    # Output in folded format\n    for stack, count\
          \ in sorted(stacks.items()):\n        print(f'{stack} {count}')\n\n# Output format:\n# main;foo;bar 42\n# main;foo;baz\
          \ 17\n```"
        level2: "Simple SVG flame graph:\n```python\nclass FlameGraph:\n    def __init__(self, width=1200, row_height=16):\n\
          \        self.width = width\n        self.row_height = row_height\n        self.svg_parts = []\n    \n    def generate(self,\
          \ folded_stacks):\n        total_samples = sum(folded_stacks.values())\n        \n        # Build tree from stacks\n\
          \        root = self.build_tree(folded_stacks)\n        \n        # Calculate widths and positions\n        self.layout(root,\
          \ 0, self.width, 0)\n        \n        # Render to SVG\n        self.render(root)\n        \n        return self.to_svg()\n\
          \    \n    def render_frame(self, name, x, width, y, samples):\n        color = self.color_for(name)\n        self.svg_parts.append(f'''\n\
          \            <rect x=\"{x}\" y=\"{y}\" width=\"{width}\" height=\"{self.row_height}\" \n                  fill=\"\
          {color}\" />\n            <text x=\"{x+2}\" y=\"{y+12}\">{name}</text>\n        ''')\n```"
        level3: |-
          Flame graph generation in Python:

          from collections import defaultdict
          from dataclasses import dataclass
          from typing import List, Dict
          import html

          @dataclass
          class StackSample:
              frames: List[str]  # Bottom to top
              count: int = 1

          class FlameGraphGenerator:
              def __init__(self, width=1200, row_height=16):
                  self.width = width
                  self.row_height = row_height
                  self.min_width = 0.1  # Minimum visible width percentage

              def generate(self, samples: List[StackSample]) -> str:
                  # Aggregate stacks
                  folded = defaultdict(int)
                  for sample in samples:
                      stack_key = ';'.join(sample.frames)
                      folded[stack_key] += sample.count

                  total_samples = sum(folded.values())

                  # Build tree
                  root = {'name': 'all', 'value': total_samples, 'children': {}}
                  for stack, count in folded.items():
                      node = root
                      for frame in stack.split(';'):
                          if frame not in node['children']:
                              node['children'][frame] = {'name': frame, 'value': 0, 'children': {}}
                          node = node['children'][frame]
                          node['value'] += count

                  # Generate SVG
                  svg_parts = [f'''<svg xmlns="http://www.w3.org/2000/svg"
                      width="{self.width}" height="{self._calc_height(root)}"
                      style="font-family: monospace; font-size: 12px;">''']

                  self._render_node(svg_parts, root, 0, self.width, 0, total_samples)

                  svg_parts.append('</svg>')
                  return '\n'.join(svg_parts)

              def _render_node(self, svg, node, x, width, depth, total):
                  y = depth * self.row_height
                  pct = node['value'] / total * 100

                  if pct >= self.min_width:
                      color = self._color_for(node['name'])
                      title = f"{node['name']} ({node['value']} samples, {pct:.1f}%)"

                      svg.append(f'''
                          <g class="frame">
                              <title>{html.escape(title)}</title>
                              <rect x="{x}" y="{y}" width="{width}" height="{self.row_height-1}"
                                    fill="{color}" rx="2"/>
                              <text x="{x+2}" y="{y+12}" fill="#000"
                                    clip-path="url(#clip)">{html.escape(node['name'][:int(width/7)])}</text>
                          </g>''')

                  # Render children
                  child_x = x
                  for child in sorted(node['children'].values(), key=lambda c: -c['value']):
                      child_width = width * child['value'] / node['value']
                      self._render_node(svg, child, child_x, child_width, depth + 1, total)
                      child_x += child_width

              def _color_for(self, name):
                  if '[kernel]' in name:
                      return '#f8c49c'  # Orange for kernel
                  elif name.startswith('lib'):
                      return '#afd8f8'  # Blue for libraries
                  return '#9ddb9d'  # Green for user code
      acceptance_criteria:
      - Aggregate captured samples by unique call stack signature
      - Generate interactive SVG flame graph from aggregated stack data
      - Support zoom and search interaction within flame graph visualization
      - Support inverted flame graph (icicle chart) showing callee perspective
    - name: Memory Profiling
      description: Track memory allocations
      skills:
      - Malloc interception
      - Allocation tracking
      - Leak detection
      deliverables:
      - Malloc and free interception hooking allocation functions
      - Allocation size tracking recording bytes allocated per call site
      - Stack trace capture at each allocation recording calling context
      - Memory usage timeline showing heap size over program execution
      - Leak detection identifying allocations without matching free calls
      - Memory allocation flame graph showing hot allocation call paths
      hints:
        level1: |-
          Intercept malloc with LD_PRELOAD:
          ```c
          #define _GNU_SOURCE
          #include <dlfcn.h>

          static void* (*real_malloc)(size_t) = NULL;
          static void (*real_free)(void*) = NULL;

          void __attribute__((constructor)) init() {
              real_malloc = dlsym(RTLD_NEXT, "malloc");
              real_free = dlsym(RTLD_NEXT, "free");
          }

          void *malloc(size_t size) {
              void *ptr = real_malloc(size);
              record_allocation(ptr, size);
              return ptr;
          }

          void free(void *ptr) {
              record_free(ptr);
              real_free(ptr);
          }
          ```
        level2: |-
          Track allocations with stack traces:
          ```c
          typedef struct {
              void *ptr;
              size_t size;
              void *stack[16];
              int stack_depth;
          } Allocation;

          #define MAX_ALLOCATIONS 1000000
          Allocation allocations[MAX_ALLOCATIONS];
          int alloc_count = 0;

          void record_allocation(void *ptr, size_t size) {
              Allocation *a = &allocations[alloc_count++];
              a->ptr = ptr;
              a->size = size;
              a->stack_depth = backtrace(a->stack, 16);
          }
          ```
        level3: "Leak detection:\n```python\ndef find_leaks(allocations, frees):\n    allocated = {a.ptr: a for a in allocations}\n\
          \    \n    for f in frees:\n        if f.ptr in allocated:\n            del allocated[f.ptr]\n    \n    # Remaining\
          \ allocations are leaks\n    leaks = list(allocated.values())\n    \n    # Group by allocation site\n    by_stack\
          \ = {}\n    for leak in leaks:\n        stack_key = tuple(leak.stack)\n        if stack_key not in by_stack:\n \
          \           by_stack[stack_key] = {'count': 0, 'bytes': 0, 'stack': leak.stack}\n        by_stack[stack_key]['count']\
          \ += 1\n        by_stack[stack_key]['bytes'] += leak.size\n    \n    return sorted(by_stack.values(), key=lambda\
          \ x: -x['bytes'])\n```"
      acceptance_criteria:
      - Track all heap allocations recording size and call stack origin
      - Identify top allocation call sites ranked by total bytes allocated
      - Detect memory leaks by finding allocations never freed before exit
      - Generate allocation flame graph showing memory-heavy code paths
  build-vpn:
    id: build-vpn
    name: Build Your Own VPN
    description: Build a VPN that creates encrypted tunnels for secure communication. Learn TUN/TAP interfaces, encryption,
      key exchange, and network routing.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Network programming (sockets, TCP/UDP)
    - Cryptography basics (AES, RSA)
    - Linux networking (iptables, routing)
    - TUN/TAP virtual interfaces
    languages:
      recommended:
      - Go
      - Rust
      - C
      also_possible:
      - Python
    resources:
    - name: TUN/TAP Interface Tutorial
      url: https://www.kernel.org/doc/Documentation/networking/tuntap.txt
      type: documentation
    - name: WireGuard Whitepaper
      url: https://www.wireguard.com/papers/wireguard.pdf
      type: paper
    milestones:
    - id: 1
      name: TUN/TAP Interface
      description: Create and configure a TUN device for IP packet capture.
      acceptance_criteria:
      - TUN device is created and visible via ip link show command
      - Raw IP packets are read from TUN file descriptor without protocol information header
      - Writing IP packets to TUN interface delivers them to local network stack
      - Interface IP address and MTU are configured and verified with ping to TUN address
      - TUN device persists while file descriptor is open and is cleaned up on close
      hints:
        level1: TUN device captures IP packets. Open /dev/net/tun with specific ioctl flags to create virtual interface.
        level2: Use TUNSETIFF ioctl with IFF_TUN | IFF_NO_PI flags. Set interface IP with SIOCSIFADDR. Bring up with SIOCSIFFLAGS.
        level3: |-
          import os
          import fcntl
          import struct

          TUNSETIFF = 0x400454ca
          IFF_TUN = 0x0001
          IFF_NO_PI = 0x1000

          def create_tun(name='tun0'):
              '''Create TUN device and return file descriptor'''
              fd = os.open('/dev/net/tun', os.O_RDWR)

              # struct ifreq: 16 bytes name + 2 bytes flags
              ifr = struct.pack('16sH', name.encode(), IFF_TUN | IFF_NO_PI)
              fcntl.ioctl(fd, TUNSETIFF, ifr)

              # Configure IP (use subprocess for simplicity)
              import subprocess
              subprocess.run(['ip', 'addr', 'add', '10.0.0.1/24', 'dev', name])
              subprocess.run(['ip', 'link', 'set', name, 'up'])

              return fd

          def read_packet(fd):
              '''Read IP packet from TUN'''
              return os.read(fd, 65535)

          def write_packet(fd, packet):
              '''Write IP packet to TUN'''
              os.write(fd, packet)
      pitfalls:
      - Forgetting IFF_NO_PI causes 4-byte header
      - Must run as root
      - Interface disappears when fd closed
      concepts:
      - Virtual network interfaces
      - TUN vs TAP
      - IP packet structure
      estimated_hours: 6-10
      deliverables:
      - TUN device creation opening /dev/net/tun with proper ioctl flags
      - Packet reading from TUN interface capturing outbound IP packets
      - Packet writing to TUN interface injecting inbound IP packets
      - Interface configuration setting IP address, netmask, and MTU via ioctl
    - id: 2
      name: UDP Transport Layer
      description: Create UDP socket for tunneling encrypted packets between VPN endpoints.
      acceptance_criteria:
      - UDP server listens on configured port and receives packets from remote peers
      - Packets read from TUN are encapsulated in UDP and sent to remote VPN endpoint
      - Packets received from UDP are extracted and written to local TUN interface
      - Server mode handles multiple client connections tracking each by source address
      - Select or poll multiplexes TUN file descriptor and UDP socket for concurrent I/O
      hints:
        level1: VPN tunnels packets over UDP. Read from TUN, send over UDP socket. Receive from UDP, write to TUN.
        level2: Use select/poll to multiplex TUN fd and UDP socket. Server tracks client addresses. Consider MTU (encrypt
          adds overhead).
        level3: |-
          import socket
          import select

          class VPNTunnel:
              def __init__(self, tun_fd, local_port, remote_addr=None):
                  self.tun_fd = tun_fd
                  self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                  self.sock.bind(('0.0.0.0', local_port))
                  self.remote_addr = remote_addr  # (ip, port) for client mode
                  self.clients = {}  # For server: virtual_ip -> (real_ip, port)

              def run(self):
                  while True:
                      readable, _, _ = select.select([self.tun_fd, self.sock], [], [])

                      for fd in readable:
                          if fd == self.tun_fd:
                              # Packet from local network -> send to tunnel
                              packet = os.read(self.tun_fd, 65535)
                              self.send_to_tunnel(packet)
                          else:
                              # Packet from tunnel -> inject to local network
                              data, addr = self.sock.recvfrom(65535)
                              self.receive_from_tunnel(data, addr)

              def send_to_tunnel(self, packet):
                  # Extract dest IP from packet, find client, send
                  if self.remote_addr:
                      self.sock.sendto(packet, self.remote_addr)
                  else:
                      # Server: route based on dest IP
                      dest_ip = self.get_dest_ip(packet)
                      if dest_ip in self.clients:
                          self.sock.sendto(packet, self.clients[dest_ip])

              def receive_from_tunnel(self, data, addr):
                  os.write(self.tun_fd, data)
                  # Track client for routing
                  src_ip = self.get_src_ip(data)
                  self.clients[src_ip] = addr
      pitfalls:
      - MTU issues (encryption overhead)
      - NAT traversal
      - Packet fragmentation
      concepts:
      - UDP tunneling
      - Multiplexing I/O
      - Client-server architecture
      estimated_hours: 6-8
      deliverables:
      - UDP socket for peer-to-peer tunnel communication on configurable port
      - Packet encapsulation wrapping TUN packets inside UDP datagrams for transport
      - Peer address management tracking remote endpoint addresses for routing
      - NAT traversal basics enabling connections through network address translation
    - id: 3
      name: Encryption Layer
      description: Add encryption to tunnel traffic using symmetric encryption (AES-GCM).
      acceptance_criteria:
      - Packets are encrypted with AES-256-GCM before being sent over UDP transport
      - Decryption verifies authentication tag and rejects tampered or corrupted packets
      - Each packet uses unique nonce and nonce reuse with same key never occurs
      - Anti-replay window rejects packets with previously seen nonce counter values
      - Encrypted tunnel passes data correctly verified by ping through VPN tunnel
      hints:
        level1: Encrypt before send, decrypt after receive. AES-GCM provides encryption + authentication. Each packet needs
          unique nonce.
        level2: 'Packet format: [nonce (12 bytes)][ciphertext][tag (16 bytes)]. Use incrementing counter for nonce. Never
          reuse nonce with same key.'
        level3: |-
          from cryptography.hazmat.primitives.ciphers.aead import AESGCM
          import os
          import struct

          class EncryptedTunnel:
              def __init__(self, key):
                  '''key: 32 bytes for AES-256'''
                  self.aesgcm = AESGCM(key)
                  self.send_counter = 0
                  self.recv_window = set()  # Anti-replay

              def encrypt(self, plaintext):
                  '''Encrypt packet, return nonce + ciphertext + tag'''
                  # 12-byte nonce: 4 bytes sender_id + 8 bytes counter
                  nonce = struct.pack('>I', 0) + struct.pack('>Q', self.send_counter)
                  self.send_counter += 1

                  ciphertext = self.aesgcm.encrypt(nonce, plaintext, None)
                  return nonce + ciphertext  # Tag is appended by AESGCM

              def decrypt(self, data):
                  '''Decrypt packet, verify auth tag'''
                  nonce = data[:12]
                  ciphertext = data[12:]

                  # Anti-replay check
                  counter = struct.unpack('>Q', nonce[4:])[0]
                  if counter in self.recv_window:
                      raise ValueError("Replay attack detected")
                  self.recv_window.add(counter)

                  # Decrypt and verify (raises InvalidTag if tampered)
                  plaintext = self.aesgcm.decrypt(nonce, ciphertext, None)
                  return plaintext

          # Integration with tunnel
          def send_encrypted(tunnel, packet, crypto):
              encrypted = crypto.encrypt(packet)
              tunnel.sock.sendto(encrypted, tunnel.remote_addr)

          def receive_encrypted(tunnel, crypto):
              data, addr = tunnel.sock.recvfrom(65535)
              try:
                  packet = crypto.decrypt(data)
                  os.write(tunnel.tun_fd, packet)
              except Exception as e:
                  print(f"Decrypt failed: {e}")  # Drop invalid packets
      pitfalls:
      - Nonce reuse is catastrophic
      - Forgetting auth tag verification
      - Key management
      concepts:
      - Authenticated encryption
      - Nonces and IVs
      - Anti-replay protection
      estimated_hours: 8-12
      deliverables:
      - Key derivation from shared secret producing AES-256 encryption key material
      - Packet encryption using AES-GCM authenticated encryption before UDP transmission
      - Packet decryption and authentication tag verification after UDP reception
      - Nonce and initialization vector management ensuring unique nonce per encrypted packet
    - id: 4
      name: Key Exchange
      description: Implement secure key exchange using Diffie-Hellman or similar protocol.
      acceptance_criteria:
      - Ephemeral key pairs are generated fresh for each new VPN session connection
      - Public keys are exchanged over UDP and both sides derive identical shared secret
      - HKDF derives separate encryption keys for each traffic direction from shared secret
      - Session keys provide perfect forward secrecy so past traffic remains secure if keys leak
      - Key exchange completes within reasonable time and tunnel becomes usable afterward
      hints:
        level1: Use Elliptic Curve Diffie-Hellman (X25519). Each side generates keypair, exchanges public key, computes shared
          secret.
        level2: HKDF expands shared secret into encryption key. Include session identifiers in HKDF info to bind key to session.
        level3: |-
          from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PrivateKey, X25519PublicKey
          from cryptography.hazmat.primitives.kdf.hkdf import HKDF
          from cryptography.hazmat.primitives import hashes

          class KeyExchange:
              def __init__(self):
                  self.private_key = X25519PrivateKey.generate()
                  self.public_key = self.private_key.public_key()

              def get_public_bytes(self):
                  '''Get public key bytes to send to peer'''
                  from cryptography.hazmat.primitives.serialization import Encoding, PublicFormat
                  return self.public_key.public_bytes(Encoding.Raw, PublicFormat.Raw)

              def derive_shared_key(self, peer_public_bytes):
                  '''Derive shared encryption key from peer's public key'''
                  peer_public = X25519PublicKey.from_public_bytes(peer_public_bytes)
                  shared_secret = self.private_key.exchange(peer_public)

                  # Derive 32-byte AES key using HKDF
                  hkdf = HKDF(
                      algorithm=hashes.SHA256(),
                      length=32,
                      salt=None,
                      info=b'vpn-encryption-key'
                  )
                  return hkdf.derive(shared_secret)

          # Handshake protocol
          def client_handshake(sock, server_addr):
              kex = KeyExchange()

              # Send our public key
              sock.sendto(b'HELLO' + kex.get_public_bytes(), server_addr)

              # Receive server's public key
              data, _ = sock.recvfrom(65535)
              if data[:5] != b'HELLO':
                  raise ValueError("Invalid handshake")

              server_public = data[5:37]
              encryption_key = kex.derive_shared_key(server_public)

              return encryption_key
      pitfalls:
      - Not verifying peer identity (MITM)
      - Reusing static keys
      - Weak random number generation
      concepts:
      - Diffie-Hellman key exchange
      - Perfect forward secrecy
      - Key derivation functions
      estimated_hours: 8-10
      deliverables:
      - Diffie-Hellman or ECDH key exchange deriving shared secret from ephemeral keys
      - Peer authentication verifying remote endpoint identity during handshake
      - Session key establishment deriving encryption keys using HKDF from shared secret
      - Key rotation periodically renegotiating session keys for forward secrecy
    - id: 5
      name: Routing and NAT
      description: Configure routing tables and NAT for full VPN functionality.
      acceptance_criteria:
      - All client traffic is routed through VPN tunnel after route table configuration
      - Route to VPN server IP is preserved via original gateway preventing routing loop
      - Server NAT masquerade allows VPN clients to access internet through server external interface
      - Split tunneling routes only configured destination subnets through VPN leaving rest direct
      - Original routing table is restored cleanly when VPN connection disconnects
      hints:
        level1: Add route for 0.0.0.0/0 via TUN interface. Server needs iptables MASQUERADE for NAT. Save/restore original
          routes.
        level2: Use 'ip route' to manage routes. Keep route to VPN server via original gateway. Set up iptables FORWARD chain
          for server.
        level3: |-
          import subprocess
          import json

          class RoutingManager:
              def __init__(self, tun_name, vpn_server_ip, local_gateway):
                  self.tun_name = tun_name
                  self.vpn_server_ip = vpn_server_ip
                  self.local_gateway = local_gateway
                  self.original_routes = []

              def setup_client_routes(self):
                  '''Route all traffic through VPN'''
                  # Save original default route
                  result = subprocess.run(['ip', 'route', 'show', 'default'], capture_output=True, text=True)
                  self.original_routes.append(result.stdout.strip())

                  # Keep route to VPN server through original gateway
                  subprocess.run(['ip', 'route', 'add', f'{self.vpn_server_ip}/32', 'via', self.local_gateway])

                  # Route everything else through VPN
                  subprocess.run(['ip', 'route', 'del', 'default'])
                  subprocess.run(['ip', 'route', 'add', 'default', 'dev', self.tun_name])

                  # DNS through VPN
                  subprocess.run(['resolvectl', 'dns', self.tun_name, '10.0.0.1'])

              def setup_server_nat(self, external_interface='eth0'):
                  '''Enable NAT on server for VPN clients'''
                  # Enable IP forwarding
                  with open('/proc/sys/net/ipv4/ip_forward', 'w') as f:
                      f.write('1')

                  # Masquerade VPN traffic
                  subprocess.run([
                      'iptables', '-t', 'nat', '-A', 'POSTROUTING',
                      '-s', '10.0.0.0/24', '-o', external_interface, '-j', 'MASQUERADE'
                  ])

                  # Allow forwarding
                  subprocess.run([
                      'iptables', '-A', 'FORWARD',
                      '-i', self.tun_name, '-o', external_interface, '-j', 'ACCEPT'
                  ])
                  subprocess.run([
                      'iptables', '-A', 'FORWARD',
                      '-i', external_interface, '-o', self.tun_name, '-m', 'state',
                      '--state', 'ESTABLISHED,RELATED', '-j', 'ACCEPT'
                  ])

              def cleanup(self):
                  '''Restore original routing'''
                  subprocess.run(['ip', 'route', 'del', 'default'])
                  for route in self.original_routes:
                      subprocess.run(['ip', 'route', 'add'] + route.split())
      pitfalls:
      - Locking yourself out (SSH)
      - DNS leaks
      - IPv6 leaks
      - Forgetting to restore routes
      concepts:
      - IP routing
      - NAT/masquerading
      - iptables
      - Split tunneling
      estimated_hours: 10-15
      deliverables:
      - Route table manipulation configuring default gateway through VPN tunnel interface
      - Default gateway setup redirecting all traffic through VPN while keeping VPN server reachable
      - NAT masquerading on server translating VPN client addresses to server external address
      - Split tunneling configuration routing only specified subnets through VPN tunnel
  cache-optimized-structures:
    id: cache-optimized-structures
    name: Cache-Optimized Data Structures
    description: Build data structures optimized for CPU cache efficiency. Learn cache-oblivious algorithms, memory layouts,
      and how to achieve better performance through cache-aware design.
    difficulty: advanced
    estimated_hours: 35-55
    prerequisites:
    - Data structures (arrays, trees, hash tables)
    - Understanding of CPU cache hierarchy
    - Memory alignment concepts
    - Performance profiling basics
    languages:
      recommended:
      - C
      - C++
      - Rust
      also_possible:
      - Go
      - Zig
    resources:
    - name: What Every Programmer Should Know About Memory
      url: https://people.freebsd.org/~lstewart/articles/cpumemory.pdf
      type: paper
    - name: Cache-Oblivious Algorithms
      url: https://en.wikipedia.org/wiki/Cache-oblivious_algorithm
      type: article
    milestones:
    - id: 1
      name: Cache Fundamentals & Benchmarking
      description: Understand cache behavior and build benchmarking tools to measure cache performance.
      acceptance_criteria:
      - Benchmark detects L1, L2, L3 cache sizes by measuring access latency at different array sizes
      - Sequential access benchmark runs at least 10x faster than random access for large arrays
      - Cache line stride benchmark shows constant time for strides up to 64 bytes then increasing
      - Performance counters report cache miss ratio for each benchmark configuration
      - Results document at least 10x difference between cache-friendly and cache-hostile code
      hints:
        level1: CPU caches are faster but smaller. L1 ~32KB, L2 ~256KB, L3 ~8MB. Access patterns matter more than algorithms
          for performance.
        level2: Sequential access prefetches next cache line. Random access causes cache misses. Measure with perf stat -e
          cache-misses,cache-references.
        level3: |-
          #include <stdio.h>
          #include <stdlib.h>
          #include <time.h>

          #define ARRAY_SIZE (64 * 1024 * 1024)  // 64MB
          #define CACHE_LINE 64

          // Sequential access - cache friendly
          double benchmark_sequential(int* arr, size_t n, int iterations) {
              clock_t start = clock();
              volatile int sum = 0;
              for (int iter = 0; iter < iterations; iter++) {
                  for (size_t i = 0; i < n; i++) {
                      sum += arr[i];
                  }
              }
              return (double)(clock() - start) / CLOCKS_PER_SEC;
          }

          // Random access - cache hostile
          double benchmark_random(int* arr, size_t n, int iterations) {
              // Create random permutation
              size_t* indices = malloc(n * sizeof(size_t));
              for (size_t i = 0; i < n; i++) indices[i] = i;
              for (size_t i = n - 1; i > 0; i--) {
                  size_t j = rand() % (i + 1);
                  size_t tmp = indices[i]; indices[i] = indices[j]; indices[j] = tmp;
              }

              clock_t start = clock();
              volatile int sum = 0;
              for (int iter = 0; iter < iterations; iter++) {
                  for (size_t i = 0; i < n; i++) {
                      sum += arr[indices[i]];
                  }
              }
              double time = (double)(clock() - start) / CLOCKS_PER_SEC;
              free(indices);
              return time;
          }

          // Measure cache line effect
          void benchmark_stride(int* arr, size_t n) {
              printf("Stride\tTime(ms)\n");
              for (int stride = 1; stride <= 256; stride *= 2) {
                  clock_t start = clock();
                  volatile int sum = 0;
                  for (int iter = 0; iter < 100; iter++) {
                      for (size_t i = 0; i < n; i += stride) {
                          sum += arr[i];
                      }
                  }
                  double ms = (double)(clock() - start) / CLOCKS_PER_SEC * 1000;
                  printf("%d\t%.2f\n", stride, ms);
              }
          }

          int main() {
              int* arr = aligned_alloc(CACHE_LINE, ARRAY_SIZE);
              size_t n = ARRAY_SIZE / sizeof(int);

              // Initialize
              for (size_t i = 0; i < n; i++) arr[i] = i;

              printf("Sequential: %.3f sec\n", benchmark_sequential(arr, n, 10));
              printf("Random: %.3f sec\n", benchmark_random(arr, n, 10));

              benchmark_stride(arr, n);

              free(arr);
              return 0;
          }

          // Compile: gcc -O2 -o cache_bench cache_bench.c
          // Profile: perf stat -e cache-misses,cache-references ./cache_bench
      pitfalls:
      - Compiler optimizations hiding effects
      - Warmup effects
      - System noise in benchmarks
      concepts:
      - Cache hierarchy
      - Cache lines
      - Spatial/temporal locality
      - Prefetching
      estimated_hours: 6-8
      deliverables:
      - Cache hierarchy measurement tool detecting L1, L2, and L3 cache sizes
      - Performance counter reader collecting cache miss and reference statistics
      - Benchmark harness measuring sequential versus random access timing differences
      - Cache miss measurement comparing access patterns with perf or cachegrind profiling
    - id: 2
      name: Array of Structs vs Struct of Arrays
      description: Implement and compare AoS vs SoA memory layouts for better cache utilization.
      acceptance_criteria:
      - AoS particle system stores x, y, z, vx, vy, vz per particle in contiguous struct
      - SoA particle system stores separate arrays for x, y, z, vx, vy, vz values
      - SoA outperforms AoS when updating only position fields without accessing velocity
      - AoS performs competitively when all fields of each entity are accessed together
      - Benchmark demonstrates SIMD vectorization opportunity with SoA memory layout
      hints:
        level1: 'AoS: struct Particle { x, y, z, vx, vy, vz }. SoA: struct Particles { float* x, *y, *z, *vx, *vy, *vz }.
          SoA better when accessing single field across many entities.'
        level2: If you only update positions (x,y,z), AoS loads velocity too (wasted). SoA loads only what you need. SoA also
          enables SIMD vectorization.
        level3: |-
          #include <stdio.h>
          #include <stdlib.h>
          #include <time.h>
          #include <immintrin.h>

          #define N 1000000

          // Array of Structs
          typedef struct {
              float x, y, z;
              float vx, vy, vz;
              float mass;
              int id;
          } ParticleAoS;

          // Struct of Arrays
          typedef struct {
              float* x;
              float* y;
              float* z;
              float* vx;
              float* vy;
              float* vz;
              float* mass;
              int* id;
              size_t count;
          } ParticlesSoA;

          // AoS: Update positions
          void update_positions_aos(ParticleAoS* particles, size_t n, float dt) {
              for (size_t i = 0; i < n; i++) {
                  particles[i].x += particles[i].vx * dt;
                  particles[i].y += particles[i].vy * dt;
                  particles[i].z += particles[i].vz * dt;
              }
          }

          // SoA: Update positions (cache-friendly, SIMD-friendly)
          void update_positions_soa(ParticlesSoA* p, float dt) {
              // Scalar version
              for (size_t i = 0; i < p->count; i++) {
                  p->x[i] += p->vx[i] * dt;
                  p->y[i] += p->vy[i] * dt;
                  p->z[i] += p->vz[i] * dt;
              }
          }

          // SoA with SIMD (AVX)
          void update_positions_soa_simd(ParticlesSoA* p, float dt) {
              __m256 dt_vec = _mm256_set1_ps(dt);
              size_t i;
              for (i = 0; i + 8 <= p->count; i += 8) {
                  __m256 x = _mm256_loadu_ps(&p->x[i]);
                  __m256 vx = _mm256_loadu_ps(&p->vx[i]);
                  x = _mm256_fmadd_ps(vx, dt_vec, x);
                  _mm256_storeu_ps(&p->x[i], x);

                  // Same for y, z...
              }
              // Handle remainder
              for (; i < p->count; i++) {
                  p->x[i] += p->vx[i] * dt;
              }
          }

          // AoS: Access all fields (AoS wins here)
          float compute_kinetic_energy_aos(ParticleAoS* particles, size_t n) {
              float total = 0;
              for (size_t i = 0; i < n; i++) {
                  float v2 = particles[i].vx * particles[i].vx +
                             particles[i].vy * particles[i].vy +
                             particles[i].vz * particles[i].vz;
                  total += 0.5f * particles[i].mass * v2;
              }
              return total;
          }

          int main() {
              // Benchmark both layouts...
              return 0;
          }
      pitfalls:
      - Not aligning SoA arrays
      - Forgetting remainder in SIMD loops
      - Over-optimizing when AoS is actually fine
      concepts:
      - Data-oriented design
      - Memory layout
      - SIMD vectorization
      - Cache line utilization
      estimated_hours: 6-10
      deliverables:
      - AoS implementation storing all fields of each entity contiguously in memory
      - SoA implementation storing each field separately in its own contiguous array
      - Access pattern comparison benchmarking single-field versus all-field operations
      - Performance benchmarking with cache miss profiling for both memory layouts
    - id: 3
      name: Cache-Friendly Hash Table
      description: Implement a hash table optimized for cache performance using open addressing and linear probing.
      acceptance_criteria:
      - Open addressing with linear probing keeps probe sequence in contiguous memory
      - Separate key and value arrays improve cache utilization during key-only probing phase
      - Robin Hood hashing reduces maximum probe distance for more consistent lookup performance
      - Benchmark shows fewer cache misses per lookup compared to chaining-based hash table
      - Hash table maintains correctness under high load factor with proper resize handling
      hints:
        level1: Chaining is cache-hostile (pointer chasing). Open addressing with linear probing keeps data contiguous. Keys-only
          array for initial probe is cache-friendly.
        level2: 'Robin Hood: when inserting, if probe distance > existing element''s probe distance, swap and continue with
          evicted element. Reduces variance in probe lengths.'
        level3: |-
          #include <stdint.h>
          #include <stdlib.h>
          #include <string.h>

          typedef struct {
              uint64_t* keys;      // Separate array for cache-friendly probing
              uint64_t* values;
              uint8_t* distances;  // Probe distance (for Robin Hood)
              size_t capacity;
              size_t size;
          } CacheFriendlyMap;

          #define EMPTY_KEY 0
          #define TOMBSTONE UINT64_MAX

          CacheFriendlyMap* cfmap_create(size_t capacity) {
              CacheFriendlyMap* map = malloc(sizeof(CacheFriendlyMap));
              map->capacity = capacity;
              map->size = 0;
              map->keys = calloc(capacity, sizeof(uint64_t));
              map->values = calloc(capacity, sizeof(uint64_t));
              map->distances = calloc(capacity, sizeof(uint8_t));
              return map;
          }

          static inline size_t hash(uint64_t key) {
              // Fast hash function
              key ^= key >> 33;
              key *= 0xff51afd7ed558ccd;
              key ^= key >> 33;
              return key;
          }

          void cfmap_insert(CacheFriendlyMap* map, uint64_t key, uint64_t value) {
              size_t idx = hash(key) % map->capacity;
              uint8_t dist = 0;

              while (1) {
                  if (map->keys[idx] == EMPTY_KEY || map->keys[idx] == TOMBSTONE) {
                      // Found empty slot
                      map->keys[idx] = key;
                      map->values[idx] = value;
                      map->distances[idx] = dist;
                      map->size++;
                      return;
                  }

                  if (map->keys[idx] == key) {
                      // Update existing
                      map->values[idx] = value;
                      return;
                  }

                  // Robin Hood: steal from rich (low probe distance)
                  if (dist > map->distances[idx]) {
                      // Swap
                      uint64_t tmp_key = map->keys[idx];
                      uint64_t tmp_val = map->values[idx];
                      uint8_t tmp_dist = map->distances[idx];

                      map->keys[idx] = key;
                      map->values[idx] = value;
                      map->distances[idx] = dist;

                      key = tmp_key;
                      value = tmp_val;
                      dist = tmp_dist;
                  }

                  idx = (idx + 1) % map->capacity;
                  dist++;
              }
          }

          uint64_t* cfmap_get(CacheFriendlyMap* map, uint64_t key) {
              size_t idx = hash(key) % map->capacity;
              uint8_t dist = 0;

              while (map->keys[idx] != EMPTY_KEY) {
                  if (map->keys[idx] == key) {
                      return &map->values[idx];
                  }

                  // Robin Hood: can stop early if we've probed further than any element could be
                  if (dist > map->distances[idx]) {
                      return NULL;
                  }

                  idx = (idx + 1) % map->capacity;
                  dist++;
              }

              return NULL;
          }
      pitfalls:
      - Load factor too high (>70%)
      - Poor hash function
      - Not handling resize properly
      concepts:
      - Open addressing
      - Linear probing
      - Robin Hood hashing
      - Cache-conscious design
      estimated_hours: 8-12
      deliverables:
      - Open addressing hash table with linear probing for cache-friendly lookups
      - Cache line aligned storage keeping keys in separate array from values
      - Software prefetching hints improving lookup performance for probable next access
      - Performance comparison benchmarking against chaining-based standard hash map
    - id: 4
      name: Cache-Oblivious B-Tree
      description: Implement a van Emde Boas layout B-tree that performs well regardless of cache size.
      acceptance_criteria:
      - Van Emde Boas layout places frequently co-accessed tree nodes in same cache line
      - Search achieves O(log_B N) cache misses without knowing cache parameters explicitly
      - vEB layout outperforms standard layout for tree sizes exceeding L2 cache capacity
      - Performance scales well across different hardware with varying cache hierarchy sizes
      - Benchmark measures cache miss count confirming theoretical access pattern improvement
      hints:
        level1: 'van Emde Boas layout: recursively split tree, store left subtree, then right subtree. Top half of tree in
          first half of memory, recursively.'
        level2: 'For a complete binary tree of height h: split at h/2. Store top subtree, then all bottom subtrees contiguously.
          Achieves O(log_B N) cache misses.'
        level3: |-
          #include <stdlib.h>
          #include <string.h>
          #include <math.h>

          // Standard B-tree node
          typedef struct BTreeNode {
              int* keys;
              struct BTreeNode** children;
              int num_keys;
              int is_leaf;
          } BTreeNode;

          // van Emde Boas layout for implicit complete binary search tree
          // Maps tree position to memory position for cache-oblivious access

          typedef struct {
              int* data;          // All keys in vEB layout
              size_t size;
              size_t capacity;    // Must be 2^k - 1
          } vEBTree;

          // Calculate vEB position for a node at given level and index
          // This is the key insight: recursive layout
          size_t veb_position(size_t height, size_t level, size_t index) {
              if (height == 1) {
                  return 0;
              }

              size_t top_height = height / 2;
              size_t bottom_height = height - top_height;
              size_t top_size = (1 << top_height) - 1;
              size_t bottom_size = (1 << bottom_height) - 1;

              if (level < top_height) {
                  // Node is in top subtree
                  return veb_position(top_height, level, index);
              } else {
                  // Node is in one of the bottom subtrees
                  size_t bottom_index = index >> (height - level - 1);
                  size_t local_index = index & ((1 << (height - level - 1)) - 1);
                  size_t local_level = level - top_height;

                  return top_size +
                         bottom_index * bottom_size +
                         veb_position(bottom_height, local_level, local_index);
              }
          }

          vEBTree* veb_create(size_t height) {
              vEBTree* tree = malloc(sizeof(vEBTree));
              tree->capacity = (1 << height) - 1;
              tree->data = malloc(tree->capacity * sizeof(int));
              tree->size = 0;
              memset(tree->data, -1, tree->capacity * sizeof(int));  // -1 = empty
              return tree;
          }

          // Build vEB tree from sorted array
          void veb_build(vEBTree* tree, int* sorted, size_t n, size_t height) {
              // For each position in sorted array, compute its vEB position
              for (size_t i = 0; i < n; i++) {
                  // Binary search tree position: level and index
                  // Root is level 0, index 0
                  // Left child of (l, i) is (l+1, 2*i), right is (l+1, 2*i+1)

                  // For a balanced BST built from sorted array:
                  // Element at sorted[i] goes to specific tree position
                  size_t tree_pos = /* compute tree position for sorted element i */;
                  size_t veb_pos = veb_position(height, /* level */, /* index */);
                  tree->data[veb_pos] = sorted[i];
              }
              tree->size = n;
          }

          // Search in vEB layout - same traversal, different memory access pattern
          int veb_search(vEBTree* tree, int key, size_t height) {
              size_t level = 0;
              size_t index = 0;

              while (level < height) {
                  size_t pos = veb_position(height, level, index);

                  if (tree->data[pos] == -1) return 0;  // Not found
                  if (tree->data[pos] == key) return 1;  // Found

                  // Go left or right
                  if (key < tree->data[pos]) {
                      index = 2 * index;      // Left child
                  } else {
                      index = 2 * index + 1;  // Right child
                  }
                  level++;
              }

              return 0;
          }
      pitfalls:
      - Complex index calculations
      - Not handling non-power-of-2 sizes
      - Overhead may exceed benefit for small trees
      concepts:
      - Cache-oblivious algorithms
      - van Emde Boas layout
      - Memory hierarchy independence
      estimated_hours: 10-15
      deliverables:
      - Van Emde Boas memory layout storing tree nodes in cache-optimal recursive order
      - Recursive layout construction converting sorted data into vEB-ordered array
      - Search implementation traversing vEB layout with cache-oblivious access pattern
      - Performance analysis comparing vEB layout against standard pointer-based B-tree
    - id: 5
      name: Blocked Matrix Operations
      description: Implement cache-blocked matrix multiplication and other operations.
      acceptance_criteria:
      - Naive multiplication serves as baseline with known poor cache behavior for large matrices
      - Blocked multiplication achieves 2-10x speedup over naive for matrices exceeding cache size
      - Auto-tuner selects block size that minimizes execution time through empirical measurement
      - Blocked matrix transpose outperforms naive transpose for matrices larger than L1 cache
      - Blocking technique extends to LU decomposition with similar cache performance improvement
      hints:
        level1: Naive matrix multiply has poor cache behavior for large matrices. Blocked/tiled approach processes submatrices
          that fit in cache.
        level2: 'Block size should fit in L1/L2 cache. For L1=32KB, 3 matrices of floats: sqrt(32KB / 3 / 4) â‰ˆ 50. Try 32
          or 64.'
        level3: |-
          #include <stdio.h>
          #include <stdlib.h>
          #include <string.h>
          #include <time.h>

          // Naive O(n^3) - cache hostile for large n
          void matmul_naive(float* A, float* B, float* C, int n) {
              for (int i = 0; i < n; i++) {
                  for (int j = 0; j < n; j++) {
                      float sum = 0;
                      for (int k = 0; k < n; k++) {
                          sum += A[i * n + k] * B[k * n + j];  // B access is cache-hostile
                      }
                      C[i * n + j] = sum;
                  }
              }
          }

          // Blocked/Tiled - cache friendly
          void matmul_blocked(float* A, float* B, float* C, int n, int block_size) {
              memset(C, 0, n * n * sizeof(float));

              for (int i0 = 0; i0 < n; i0 += block_size) {
                  for (int j0 = 0; j0 < n; j0 += block_size) {
                      for (int k0 = 0; k0 < n; k0 += block_size) {
                          // Multiply blocks
                          int i_max = (i0 + block_size < n) ? i0 + block_size : n;
                          int j_max = (j0 + block_size < n) ? j0 + block_size : n;
                          int k_max = (k0 + block_size < n) ? k0 + block_size : n;

                          for (int i = i0; i < i_max; i++) {
                              for (int k = k0; k < k_max; k++) {
                                  float a_ik = A[i * n + k];
                                  for (int j = j0; j < j_max; j++) {
                                      C[i * n + j] += a_ik * B[k * n + j];
                                  }
                              }
                          }
                      }
                  }
              }
          }

          // Auto-tune block size
          int find_optimal_block_size(int n) {
              float* A = malloc(n * n * sizeof(float));
              float* B = malloc(n * n * sizeof(float));
              float* C = malloc(n * n * sizeof(float));

              // Initialize with random values
              for (int i = 0; i < n * n; i++) {
                  A[i] = (float)rand() / RAND_MAX;
                  B[i] = (float)rand() / RAND_MAX;
              }

              int best_block = 16;
              double best_time = 1e9;

              for (int block = 16; block <= 256 && block <= n; block *= 2) {
                  clock_t start = clock();
                  matmul_blocked(A, B, C, n, block);
                  double elapsed = (double)(clock() - start) / CLOCKS_PER_SEC;

                  printf("Block %d: %.3f sec\n", block, elapsed);

                  if (elapsed < best_time) {
                      best_time = elapsed;
                      best_block = block;
                  }
              }

              free(A); free(B); free(C);
              return best_block;
          }

          int main() {
              int n = 1024;

              float* A = aligned_alloc(64, n * n * sizeof(float));
              float* B = aligned_alloc(64, n * n * sizeof(float));
              float* C = aligned_alloc(64, n * n * sizeof(float));

              // Initialize
              for (int i = 0; i < n * n; i++) {
                  A[i] = (float)rand() / RAND_MAX;
                  B[i] = (float)rand() / RAND_MAX;
              }

              clock_t start;

              start = clock();
              matmul_naive(A, B, C, n);
              printf("Naive: %.3f sec\n", (double)(clock() - start) / CLOCKS_PER_SEC);

              start = clock();
              matmul_blocked(A, B, C, n, 64);
              printf("Blocked (64): %.3f sec\n", (double)(clock() - start) / CLOCKS_PER_SEC);

              free(A); free(B); free(C);
              return 0;
          }
      pitfalls:
      - Block size too large (exceeds cache)
      - Not handling non-block-aligned sizes
      - Memory alignment issues
      concepts:
      - Loop tiling
      - Blocking
      - Cache blocking
      - Auto-tuning
      estimated_hours: 6-10
      deliverables:
      - Naive matrix multiplication as baseline with O(n^3) cache-hostile access pattern
      - Blocked matrix multiplication processing submatrices that fit in cache
      - Block size auto-tuning selecting optimal tile size based on measured cache performance
      - Extended blocked operations for matrix transpose and LU decomposition
  sandbox:
    id: sandbox
    name: Process Sandbox
    description: Build a process sandbox using Linux security features to isolate untrusted code. Learn namespaces, seccomp,
      capabilities, and cgroups for defense in depth.
    difficulty: intermediate
    estimated_hours: 25-40
    prerequisites:
    - Linux system calls
    - Process management (fork, exec)
    - Basic understanding of Linux security
    - C programming
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python (with ctypes)
    resources:
    - name: Linux Namespaces
      url: https://man7.org/linux/man-pages/man7/namespaces.7.html
      type: documentation
    - name: Seccomp BPF
      url: https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html
      type: documentation
    milestones:
    - id: 1
      name: Process Namespaces
      description: Use Linux namespaces to isolate process view of system resources.
      acceptance_criteria:
      - Create new PID namespace where sandboxed process sees itself as PID 1
      - Create new mount namespace providing isolated filesystem view
      - Create new network namespace blocking all network access by default
      - Create new UTS namespace isolating hostname from host system
      - Verify namespace isolation by inspecting /proc entries from inside sandbox
      hints:
        level1: Use clone() with CLONE_NEWPID, CLONE_NEWNS, CLONE_NEWNET flags. Or unshare() in existing process.
        level2: After clone with CLONE_NEWPID, child is PID 1 in new namespace. Mount new /proc to see only namespace processes.
        level3: |-
          #define _GNU_SOURCE
          #include <sched.h>
          #include <stdio.h>
          #include <stdlib.h>
          #include <unistd.h>
          #include <sys/wait.h>
          #include <sys/mount.h>

          #define STACK_SIZE (1024 * 1024)

          int child_func(void* arg) {
              printf("Child PID in namespace: %d\n", getpid());  // Should be 1

              // Mount new /proc for this namespace
              if (mount("proc", "/proc", "proc", 0, NULL) == -1) {
                  perror("mount /proc");
              }

              // Set new hostname
              sethostname("sandbox", 7);

              // Show isolation
              system("echo Hostname: $(hostname)");
              system("echo 'Processes:' && ps aux");

              // Run sandboxed program
              char* argv[] = {"/bin/sh", NULL};
              execv("/bin/sh", argv);

              return 0;
          }

          int main() {
              char* stack = malloc(STACK_SIZE);
              if (!stack) {
                  perror("malloc");
                  return 1;
              }

              int flags = CLONE_NEWPID |   // New PID namespace
                          CLONE_NEWNS |    // New mount namespace
                          CLONE_NEWNET |   // New network namespace (no network)
                          CLONE_NEWUTS |   // New UTS namespace (hostname)
                          SIGCHLD;

              pid_t pid = clone(child_func, stack + STACK_SIZE, flags, NULL);
              if (pid == -1) {
                  perror("clone");
                  return 1;
              }

              printf("Parent: child PID = %d\n", pid);
              waitpid(pid, NULL, 0);

              free(stack);
              return 0;
          }
      pitfalls:
      - Forgetting SIGCHLD flag
      - Not mounting /proc in new namespace
      - Need root/CAP_SYS_ADMIN
      concepts:
      - Linux namespaces
      - Process isolation
      - clone() system call
      estimated_hours: 5-8
      deliverables:
      - PID namespace isolation giving sandboxed process its own PID 1
      - Network namespace isolation preventing external network access
      - Mount namespace setup providing isolated filesystem view
      - User namespace mapping limiting privilege inside sandbox
    - id: 2
      name: Filesystem Isolation
      description: Create isolated filesystem using chroot or pivot_root with minimal root filesystem.
      acceptance_criteria:
      - Create minimal root filesystem with only required binaries and libraries
      - Use chroot or pivot_root to change root directory for sandboxed process
      - Mount /proc, /dev, and /sys with appropriate restricted permissions
      - Prevent container escape via /proc/*/root or similar path traversals
      - Verify that parent host filesystem is completely inaccessible from sandbox
      hints:
        level1: chroot changes apparent root but can be escaped. pivot_root in mount namespace is more secure. Create minimal
          rootfs with busybox.
        level2: pivot_root requires mount namespace. After pivot_root, unmount old root. Block /proc/*/root access with seccomp
          or mount restrictions.
        level3: |-
          #define _GNU_SOURCE
          #include <sched.h>
          #include <stdio.h>
          #include <stdlib.h>
          #include <unistd.h>
          #include <sys/mount.h>
          #include <sys/stat.h>
          #include <sys/syscall.h>

          int pivot_root(const char* new_root, const char* put_old) {
              return syscall(SYS_pivot_root, new_root, put_old);
          }

          int setup_rootfs(const char* rootfs) {
              // Make rootfs a mount point
              if (mount(rootfs, rootfs, NULL, MS_BIND | MS_REC, NULL) == -1) {
                  perror("mount bind rootfs");
                  return -1;
              }

              // Create put_old directory
              char put_old[256];
              snprintf(put_old, sizeof(put_old), "%s/.old_root", rootfs);
              mkdir(put_old, 0700);

              // Change to new root
              if (chdir(rootfs) == -1) {
                  perror("chdir");
                  return -1;
              }

              // pivot_root
              if (pivot_root(".", ".old_root") == -1) {
                  perror("pivot_root");
                  return -1;
              }

              // Change to new root
              if (chdir("/") == -1) {
                  perror("chdir /");
                  return -1;
              }

              // Unmount old root
              if (umount2("/.old_root", MNT_DETACH) == -1) {
                  perror("umount old root");
                  return -1;
              }
              rmdir("/.old_root");

              // Mount essential filesystems
              mount("proc", "/proc", "proc", MS_NOSUID | MS_NODEV | MS_NOEXEC, NULL);
              mount("tmpfs", "/tmp", "tmpfs", MS_NOSUID | MS_NODEV, "size=64M");
              mount("tmpfs", "/dev", "tmpfs", MS_NOSUID, "size=64K,mode=755");

              // Create minimal /dev entries
              mknod("/dev/null", S_IFCHR | 0666, makedev(1, 3));
              mknod("/dev/zero", S_IFCHR | 0666, makedev(1, 5));
              mknod("/dev/random", S_IFCHR | 0444, makedev(1, 8));
              mknod("/dev/urandom", S_IFCHR | 0444, makedev(1, 9));

              return 0;
          }

          // Create minimal rootfs (run once during setup)
          void create_minimal_rootfs(const char* path) {
              char cmd[512];

              mkdir(path, 0755);
              snprintf(cmd, sizeof(cmd), "mkdir -p %s/{bin,lib,lib64,proc,dev,tmp,etc}", path);
              system(cmd);

              // Copy busybox for basic utilities
              snprintf(cmd, sizeof(cmd), "cp /bin/busybox %s/bin/", path);
              system(cmd);

              // Create symlinks for common commands
              snprintf(cmd, sizeof(cmd), "cd %s/bin && for cmd in sh ls cat echo ps; do ln -s busybox $cmd; done", path);
              system(cmd);

              // Copy required libraries
              snprintf(cmd, sizeof(cmd), "ldd /bin/busybox | grep -o '/lib[^ ]*' | xargs -I {} cp {} %s/lib/", path);
              system(cmd);
          }
      pitfalls:
      - Forgetting to unmount old root
      - Missing /dev entries
      - Library dependencies
      concepts:
      - chroot vs pivot_root
      - Filesystem namespaces
      - Minimal rootfs
      estimated_hours: 5-8
      deliverables:
      - Chroot or pivot_root changing filesystem root for sandbox
      - Read-only root filesystem preventing writes to system directories
      - Tmpfs mounts providing writable scratch space at specified paths
      - Bind mount restrictions blocking access to sensitive host paths
    - id: 3
      name: Seccomp System Call Filtering
      description: Use seccomp-BPF to restrict which system calls the sandboxed process can make.
      acceptance_criteria:
      - Create seccomp filter using BPF program syntax
      - Whitelist only safe system calls needed for sandboxed workload
      - Kill sandboxed process immediately on forbidden system call attempt
      - Allow read and write calls but block open of sensitive file paths
      - Test that filter correctly blocks dangerous operations like ptrace
      hints:
        level1: 'Seccomp BPF filters syscalls before execution. Whitelist approach: deny all, allow specific. Return SECCOMP_RET_KILL
          for violations.'
        level2: Use libseccomp for easier filter creation. Filter can inspect syscall number and arguments. Install filter
          with prctl(PR_SET_SECCOMP).
        level3: |-
          #include <stdio.h>
          #include <stdlib.h>
          #include <unistd.h>
          #include <sys/prctl.h>
          #include <linux/seccomp.h>
          #include <linux/filter.h>
          #include <linux/audit.h>
          #include <sys/syscall.h>

          // Using raw BPF for demonstration (libseccomp is easier in practice)
          int install_seccomp_filter() {
              // Allow: read, write, exit, exit_group, brk, mmap, mprotect
              // Kill on: open, openat, execve, fork, clone, socket, etc.

              struct sock_filter filter[] = {
                  // Load syscall number
                  BPF_STMT(BPF_LD | BPF_W | BPF_ABS, offsetof(struct seccomp_data, nr)),

                  // Allow read (0)
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_read, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),

                  // Allow write (1)
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_write, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),

                  // Allow exit (60)
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_exit, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),

                  // Allow exit_group (231)
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_exit_group, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),

                  // Allow brk (12) - memory allocation
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_brk, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),

                  // Allow mmap (9), mprotect (10), munmap (11)
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_mmap, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_mprotect, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_munmap, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),

                  // Allow fstat (5), close (3)
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_fstat, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),
                  BPF_JUMP(BPF_JMP | BPF_JEQ | BPF_K, __NR_close, 0, 1),
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_ALLOW),

                  // Kill on anything else
                  BPF_STMT(BPF_RET | BPF_K, SECCOMP_RET_KILL),
              };

              struct sock_fprog prog = {
                  .len = sizeof(filter) / sizeof(filter[0]),
                  .filter = filter,
              };

              // No new privileges
              if (prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0) == -1) {
                  perror("prctl(NO_NEW_PRIVS)");
                  return -1;
              }

              // Install filter
              if (prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, &prog) == -1) {
                  perror("prctl(SECCOMP)");
                  return -1;
              }

              return 0;
          }

          // Using libseccomp (much easier)
          #include <seccomp.h>

          int install_seccomp_libseccomp() {
              scmp_filter_ctx ctx = seccomp_init(SCMP_ACT_KILL);  // Default: kill
              if (!ctx) return -1;

              // Whitelist safe syscalls
              seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(read), 0);
              seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 0);
              seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit), 0);
              seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit_group), 0);
              seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(brk), 0);
              seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(mmap), 0);
              seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(close), 0);

              // Allow write only to stdout/stderr (fd 1, 2)
              seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 1,
                               SCMP_A0(SCMP_CMP_EQ, 1));  // stdout
              seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 1,
                               SCMP_A0(SCMP_CMP_EQ, 2));  // stderr

              prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0);
              seccomp_load(ctx);
              seccomp_release(ctx);

              return 0;
          }
      pitfalls:
      - Missing required syscalls (glibc uses many)
      - Architecture-specific syscall numbers
      - Not setting NO_NEW_PRIVS
      concepts:
      - Seccomp BPF
      - System call filtering
      - Whitelist vs blacklist
      estimated_hours: 6-10
      deliverables:
      - Seccomp-BPF filter program restricting allowed system calls
      - System call whitelist specifying exactly which calls are permitted
      - Argument-level filtering restricting parameters of allowed calls
      - BPF filter compilation generating kernel-loadable program
    - id: 4
      name: Resource Limits with Cgroups
      description: Use cgroups to limit CPU, memory, and I/O resources for sandboxed processes.
      acceptance_criteria:
      - Create dedicated cgroup for sandboxed process and children
      - Set memory limit to configurable threshold (e.g., 64MB maximum)
      - Set CPU limit to configurable share (e.g., 10% of one core)
      - Set I/O bandwidth limit capping disk throughput rate
      - Verify that resource limits are enforced when sandbox exceeds them
      hints:
        level1: Cgroups v2 uses unified hierarchy at /sys/fs/cgroup. Create subdirectory, write PID to cgroup.procs, set limits
          in controller files.
        level2: memory.max for memory limit, cpu.max for CPU (e.g., '10000 100000' = 10%), io.max for I/O limits.
        level3: |-
          #include <stdio.h>
          #include <stdlib.h>
          #include <string.h>
          #include <unistd.h>
          #include <sys/stat.h>
          #include <fcntl.h>

          #define CGROUP_ROOT "/sys/fs/cgroup"

          typedef struct {
              char* name;
              size_t memory_bytes;
              int cpu_percent;      // 0-100
              size_t io_read_bps;
              size_t io_write_bps;
          } CgroupLimits;

          int cgroup_write(const char* cgroup, const char* file, const char* value) {
              char path[512];
              snprintf(path, sizeof(path), "%s/%s/%s", CGROUP_ROOT, cgroup, file);

              int fd = open(path, O_WRONLY);
              if (fd == -1) {
                  perror(path);
                  return -1;
              }

              write(fd, value, strlen(value));
              close(fd);
              return 0;
          }

          int cgroup_create(CgroupLimits* limits) {
              char path[512];
              snprintf(path, sizeof(path), "%s/%s", CGROUP_ROOT, limits->name);

              // Create cgroup directory
              if (mkdir(path, 0755) == -1 && errno != EEXIST) {
                  perror("mkdir cgroup");
                  return -1;
              }

              // Enable controllers
              cgroup_write("", "cgroup.subtree_control", "+memory +cpu +io");

              // Set memory limit
              if (limits->memory_bytes > 0) {
                  char value[64];
                  snprintf(value, sizeof(value), "%zu", limits->memory_bytes);
                  cgroup_write(limits->name, "memory.max", value);

                  // Disable swap
                  cgroup_write(limits->name, "memory.swap.max", "0");
              }

              // Set CPU limit (cpu.max: $MAX $PERIOD)
              // 10% = 10000 us per 100000 us period
              if (limits->cpu_percent > 0) {
                  char value[64];
                  int quota = limits->cpu_percent * 1000;  // microseconds
                  snprintf(value, sizeof(value), "%d 100000", quota);
                  cgroup_write(limits->name, "cpu.max", value);
              }

              // Set I/O limits (need device major:minor)
              // Example: limit /dev/sda (8:0)
              if (limits->io_read_bps > 0 || limits->io_write_bps > 0) {
                  char value[128];
                  snprintf(value, sizeof(value), "8:0 rbps=%zu wbps=%zu",
                           limits->io_read_bps, limits->io_write_bps);
                  cgroup_write(limits->name, "io.max", value);
              }

              return 0;
          }

          int cgroup_add_process(const char* cgroup_name, pid_t pid) {
              char value[32];
              snprintf(value, sizeof(value), "%d", pid);
              return cgroup_write(cgroup_name, "cgroup.procs", value);
          }

          int cgroup_destroy(const char* name) {
              char path[512];
              snprintf(path, sizeof(path), "%s/%s", CGROUP_ROOT, name);

              // Move processes to parent first
              // Then rmdir
              rmdir(path);
              return 0;
          }

          // Example usage
          int main() {
              CgroupLimits limits = {
                  .name = "sandbox",
                  .memory_bytes = 64 * 1024 * 1024,  // 64 MB
                  .cpu_percent = 10,                   // 10% of one core
                  .io_read_bps = 10 * 1024 * 1024,    // 10 MB/s read
                  .io_write_bps = 5 * 1024 * 1024,    // 5 MB/s write
              };

              cgroup_create(&limits);

              pid_t pid = fork();
              if (pid == 0) {
                  // Child: run in cgroup
                  cgroup_add_process("sandbox", getpid());

                  // Memory test - will be killed if exceeds limit
                  char* mem = malloc(100 * 1024 * 1024);  // Try 100MB (over limit)
                  if (mem) memset(mem, 'x', 100 * 1024 * 1024);

                  exit(0);
              }

              waitpid(pid, NULL, 0);
              cgroup_destroy("sandbox");

              return 0;
          }
      pitfalls:
      - Cgroups v1 vs v2 differences
      - Need root for cgroup operations
      - Controller not enabled
      concepts:
      - Cgroups
      - Resource limits
      - CPU quotas
      - Memory limits
      estimated_hours: 5-8
      deliverables:
      - Memory limit cgroup restricting maximum resident memory usage
      - CPU limit cgroup throttling processor time allocation
      - PID limit cgroup capping maximum number of spawned processes
      - I/O bandwidth limit cgroup throttling disk read and write rates
    - id: 5
      name: Capability Dropping
      description: Drop Linux capabilities to run with minimal privileges.
      acceptance_criteria:
      - List all current process capabilities before and after dropping
      - Drop all Linux capabilities except the minimum required set
      - Set PR_SET_NO_NEW_PRIVS flag blocking future privilege escalation
      - Verify capabilities are correctly dropped using capget or /proc
      - Test that privileged operations like mounting and network config fail
      hints:
        level1: Linux capabilities split root power into units. Drop caps you don't need. CAP_NET_RAW for ping, CAP_SYS_ADMIN
          for mount, etc.
        level2: Use capset() or libcap. Clear effective, permitted, and inheritable sets. Set PR_SET_NO_NEW_PRIVS to prevent
          regaining caps.
        level3: |-
          #include <stdio.h>
          #include <stdlib.h>
          #include <unistd.h>
          #include <sys/capability.h>
          #include <sys/prctl.h>

          void print_caps(const char* label) {
              cap_t caps = cap_get_proc();
              char* caps_text = cap_to_text(caps, NULL);
              printf("%s: %s\n", label, caps_text);
              cap_free(caps_text);
              cap_free(caps);
          }

          int drop_capabilities(cap_value_t* keep_caps, int num_keep) {
              // Get current caps
              cap_t caps = cap_get_proc();
              if (!caps) {
                  perror("cap_get_proc");
                  return -1;
              }

              // Clear all caps
              if (cap_clear(caps) == -1) {
                  perror("cap_clear");
                  cap_free(caps);
                  return -1;
              }

              // Add back only the ones we need
              if (num_keep > 0) {
                  if (cap_set_flag(caps, CAP_PERMITTED, num_keep, keep_caps, CAP_SET) == -1 ||
                      cap_set_flag(caps, CAP_EFFECTIVE, num_keep, keep_caps, CAP_SET) == -1) {
                      perror("cap_set_flag");
                      cap_free(caps);
                      return -1;
                  }
              }

              // Apply
              if (cap_set_proc(caps) == -1) {
                  perror("cap_set_proc");
                  cap_free(caps);
                  return -1;
              }

              cap_free(caps);

              // Prevent regaining caps through exec
              if (prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0) == -1) {
                  perror("prctl NO_NEW_PRIVS");
                  return -1;
              }

              return 0;
          }

          // Drop to nobody user after dropping caps
          int drop_user() {
              // Set groups
              if (setgroups(0, NULL) == -1) {
                  perror("setgroups");
                  return -1;
              }

              // Set GID then UID (order matters!)
              if (setgid(65534) == -1) {  // nobody
                  perror("setgid");
                  return -1;
              }
              if (setuid(65534) == -1) {  // nobody
                  perror("setuid");
                  return -1;
              }

              return 0;
          }

          int main() {
              print_caps("Before");

              // Keep only CAP_NET_BIND_SERVICE (for binding to ports < 1024)
              cap_value_t keep[] = { CAP_NET_BIND_SERVICE };

              // Or keep nothing for maximum restriction
              drop_capabilities(NULL, 0);

              print_caps("After drop_capabilities");

              // Also drop to unprivileged user
              drop_user();

              printf("Now running as UID %d, GID %d\n", getuid(), getgid());

              // Test: this should fail now
              if (setuid(0) == 0) {
                  printf("ERROR: Was able to become root!\n");
                  return 1;
              }
              printf("Good: Cannot become root\n");

              // Test: this should fail
              if (unlink("/etc/passwd") == 0) {
                  printf("ERROR: Was able to delete /etc/passwd!\n");
                  return 1;
              }
              printf("Good: Cannot delete protected files\n");

              return 0;
          }

          // Compile: gcc -o caps caps.c -lcap
      pitfalls:
      - Order of setgid/setuid matters
      - Some caps needed for basic operations
      - Ambient caps can re-enable dropped caps
      concepts:
      - Linux capabilities
      - Principle of least privilege
      - Privilege dropping
      estimated_hours: 4-6
      deliverables:
      - Capability bounding set defining maximum possible privileges
      - Capability drop logic removing all unnecessary Linux capabilities
      - No-new-privileges flag preventing privilege escalation after exec
      - Privilege verification utility confirming capabilities are dropped
  vulnerability-scanner:
    id: vulnerability-scanner
    name: Vulnerability Scanner
    description: Build a network vulnerability scanner that discovers hosts, identifies services, and checks for known vulnerabilities.
      Learn network scanning, service fingerprinting, and CVE detection.
    difficulty: advanced
    estimated_hours: 40-60
    prerequisites:
    - TCP/IP networking
    - Socket programming
    - Understanding of common vulnerabilities
    - Basic security concepts
    languages:
      recommended:
      - Python
      - Go
      also_possible:
      - Rust
      - C
    resources:
    - name: Nmap Network Scanning
      url: https://nmap.org/book/toc.html
      type: book
    - name: NIST NVD API
      url: https://nvd.nist.gov/developers/vulnerabilities
      type: documentation
    milestones:
    - id: 1
      name: Host Discovery
      description: Implement various techniques to discover live hosts on a network.
      acceptance_criteria:
      - ICMP echo ping scan detects responsive hosts in a target range
      - TCP SYN scan for common ports
      - ARP scan discovers hosts on the local network segment by MAC address
      - Handle rate limiting to avoid detection
      - Report discovered hosts with response times
      hints:
        level1: Use raw sockets for ICMP/TCP SYN. ARP only works on local subnet. Parallelize with asyncio or threading for
          speed.
        level2: ICMP may be blocked by firewalls. TCP SYN to port 80/443 often gets through. ARP is most reliable for local
          network.
        level3: |-
          import socket
          import struct
          import asyncio
          import time

          class HostDiscovery:
              def __init__(self, timeout=1):
                  self.timeout = timeout

              async def icmp_ping(self, ip):
                  '''Send ICMP echo request'''
                  try:
                      # Need raw socket (requires root)
                      sock = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_ICMP)
                      sock.settimeout(self.timeout)

                      # ICMP echo request
                      icmp_type = 8  # Echo request
                      icmp_code = 0
                      checksum = 0
                      identifier = 1
                      sequence = 1

                      # Calculate checksum
                      header = struct.pack('!BBHHH', icmp_type, icmp_code, checksum, identifier, sequence)
                      checksum = self.calculate_checksum(header)
                      header = struct.pack('!BBHHH', icmp_type, icmp_code, checksum, identifier, sequence)

                      start = time.time()
                      sock.sendto(header, (ip, 0))

                      # Wait for reply
                      data, addr = sock.recvfrom(1024)
                      rtt = (time.time() - start) * 1000
                      sock.close()

                      return {'ip': ip, 'alive': True, 'method': 'icmp', 'rtt_ms': rtt}
                  except (socket.timeout, socket.error):
                      return {'ip': ip, 'alive': False}

              async def tcp_syn_probe(self, ip, port=80):
                  '''TCP connect to check if host is alive'''
                  try:
                      reader, writer = await asyncio.wait_for(
                          asyncio.open_connection(ip, port),
                          timeout=self.timeout
                      )
                      writer.close()
                      await writer.wait_closed()
                      return {'ip': ip, 'alive': True, 'method': 'tcp', 'port': port}
                  except:
                      return {'ip': ip, 'alive': False}

              async def scan_network(self, network_cidr):
                  '''Scan entire network'''
                  import ipaddress
                  network = ipaddress.ip_network(network_cidr, strict=False)

                  tasks = []
                  for ip in network.hosts():
                      ip_str = str(ip)
                      # Try multiple methods in parallel
                      tasks.append(self.icmp_ping(ip_str))
                      tasks.append(self.tcp_syn_probe(ip_str, 80))
                      tasks.append(self.tcp_syn_probe(ip_str, 443))

                  results = await asyncio.gather(*tasks, return_exceptions=True)

                  # Deduplicate by IP
                  alive_hosts = {}
                  for r in results:
                      if isinstance(r, dict) and r.get('alive'):
                          alive_hosts[r['ip']] = r

                  return list(alive_hosts.values())

              @staticmethod
              def calculate_checksum(data):
                  if len(data) % 2:
                      data += b'\x00'

                  total = 0
                  for i in range(0, len(data), 2):
                      total += (data[i] << 8) + data[i + 1]

                  total = (total >> 16) + (total & 0xffff)
                  total += total >> 16
                  return (~total) & 0xffff

          # Usage
          async def main():
              scanner = HostDiscovery(timeout=2)
              hosts = await scanner.scan_network('192.168.1.0/24')
              for host in hosts:
                  print(f"Alive: {host['ip']} via {host['method']}")

          asyncio.run(main())
      pitfalls:
      - Raw sockets need root
      - ICMP often blocked
      - Rate limiting needed
      concepts:
      - ICMP protocol
      - TCP handshake
      - ARP protocol
      - Network scanning
      estimated_hours: 6-10
      deliverables:
      - ICMP ping sweep sending echo requests to a range of IP addresses
      - TCP and UDP host probing detecting live hosts via port response analysis
      - ARP scanning for local network
      - Host database storing discovered hosts with IP, MAC, and response time data
    - id: 2
      name: Port Scanning
      description: Implement various port scanning techniques to identify open services.
      acceptance_criteria:
      - TCP connect scan reports open, closed, and filtered port states
      - TCP SYN half-open scan detects open ports without completing the three-way handshake
      - UDP scan with common port payloads
      - Service version detection reads banners to identify software name and version
      - Scan 1000 common ports in under 30 seconds
      hints:
        level1: Connect scan is reliable but slow. SYN scan is faster and stealthier. UDP is unreliable (no response â‰  closed).
        level2: Use asyncio for concurrent scanning. SYN scan needs raw sockets. For UDP, send protocol-specific probes to
          elicit response.
        level3: |-
          import asyncio
          import socket

          # Top 20 ports to always scan
          TOP_PORTS = [21, 22, 23, 25, 53, 80, 110, 111, 135, 139,
                       143, 443, 445, 993, 995, 1723, 3306, 3389, 5900, 8080]

          class PortScanner:
              def __init__(self, timeout=1, concurrency=100):
                  self.timeout = timeout
                  self.semaphore = asyncio.Semaphore(concurrency)

              async def tcp_connect_scan(self, ip, port):
                  '''Full TCP connect scan'''
                  async with self.semaphore:
                      try:
                          reader, writer = await asyncio.wait_for(
                              asyncio.open_connection(ip, port),
                              timeout=self.timeout
                          )

                          # Try to get banner
                          banner = None
                          try:
                              writer.write(b'\r\n')
                              await writer.drain()
                              banner = await asyncio.wait_for(reader.read(1024), timeout=0.5)
                              banner = banner.decode('utf-8', errors='ignore').strip()
                          except:
                              pass

                          writer.close()
                          await writer.wait_closed()

                          return {
                              'port': port,
                              'state': 'open',
                              'banner': banner
                          }
                      except asyncio.TimeoutError:
                          return {'port': port, 'state': 'filtered'}
                      except ConnectionRefusedError:
                          return {'port': port, 'state': 'closed'}
                      except:
                          return {'port': port, 'state': 'error'}

              async def scan_host(self, ip, ports=None):
                  '''Scan all ports on a host'''
                  if ports is None:
                      ports = TOP_PORTS + list(range(1, 1001))  # Top + first 1000
                      ports = list(set(ports))

                  tasks = [self.tcp_connect_scan(ip, port) for port in ports]
                  results = await asyncio.gather(*tasks)

                  open_ports = [r for r in results if r['state'] == 'open']
                  return {'ip': ip, 'ports': open_ports}

              def identify_service(self, port, banner):
                  '''Guess service from port and banner'''
                  services = {
                      21: 'ftp',
                      22: 'ssh',
                      23: 'telnet',
                      25: 'smtp',
                      53: 'dns',
                      80: 'http',
                      110: 'pop3',
                      143: 'imap',
                      443: 'https',
                      445: 'smb',
                      3306: 'mysql',
                      5432: 'postgresql',
                      6379: 'redis',
                      27017: 'mongodb',
                  }

                  service = services.get(port, 'unknown')

                  # Banner analysis
                  if banner:
                      banner_lower = banner.lower()
                      if 'ssh' in banner_lower:
                          service = 'ssh'
                          # Extract version: SSH-2.0-OpenSSH_8.2p1
                          if 'openssh' in banner_lower:
                              service = f'ssh (OpenSSH)'
                      elif 'apache' in banner_lower:
                          service = 'http (Apache)'
                      elif 'nginx' in banner_lower:
                          service = 'http (nginx)'
                      elif 'mysql' in banner_lower:
                          service = 'mysql'
                      elif 'ftp' in banner_lower:
                          service = 'ftp'

                  return service

          async def main():
              scanner = PortScanner(timeout=1, concurrency=200)

              result = await scanner.scan_host('192.168.1.1')

              print(f"Scan results for {result['ip']}:")
              for port_info in result['ports']:
                  service = scanner.identify_service(port_info['port'], port_info.get('banner'))
                  print(f"  {port_info['port']}/tcp - {port_info['state']} - {service}")
                  if port_info.get('banner'):
                      print(f"    Banner: {port_info['banner'][:60]}...")

          asyncio.run(main())
      pitfalls:
      - Too aggressive scanning triggers IDS
      - Firewall may give false positives
      - UDP scanning is slow
      concepts:
      - TCP/UDP ports
      - Port states
      - Banner grabbing
      - Service detection
      estimated_hours: 8-12
      deliverables:
      - TCP connect scan establishing full connections to detect open ports
      - SYN scan sending half-open packets to detect ports without completing handshake
      - UDP scanning sending protocol-specific payloads to detect open UDP services
      - Service version detection parsing banners to identify running software versions
    - id: 3
      name: Service Fingerprinting
      description: Identify specific service versions through probing and response analysis.
      acceptance_criteria:
      - HTTP server identification (Server header, behavior)
      - SSH version detection extracts protocol and software version from banner
      - SSL/TLS certificate analysis extracts issuer, expiry, and cipher suite information
      - Database service identification detects MySQL, PostgreSQL, Redis, and MongoDB instances
      - Match service responses against known signature database for accurate identification
      hints:
        level1: Send protocol-specific probes. HTTP HEAD request returns Server header. SSL cert has issuer info. Each service
          has unique response patterns.
        level2: Create signature database with regex patterns. SSL/TLS version affects vulnerability (SSLv3 = POODLE). Check
          supported ciphers for weak ones.
        level3: |-
          import ssl
          import socket
          import re
          import asyncio

          class ServiceFingerprint:
              def __init__(self, timeout=5):
                  self.timeout = timeout

              async def probe_http(self, ip, port):
                  '''Probe HTTP service'''
                  try:
                      reader, writer = await asyncio.wait_for(
                          asyncio.open_connection(ip, port),
                          timeout=self.timeout
                      )

                      # Send HTTP request
                      request = f"HEAD / HTTP/1.1\r\nHost: {ip}\r\nConnection: close\r\n\r\n"
                      writer.write(request.encode())
                      await writer.drain()

                      response = await asyncio.wait_for(reader.read(4096), timeout=self.timeout)
                      response = response.decode('utf-8', errors='ignore')

                      writer.close()
                      await writer.wait_closed()

                      return self.parse_http_response(response)
                  except Exception as e:
                      return {'error': str(e)}

              def parse_http_response(self, response):
                  '''Extract service info from HTTP response'''
                  info = {}

                  # Status line
                  match = re.search(r'HTTP/(\d\.\d) (\d+)', response)
                  if match:
                      info['http_version'] = match.group(1)
                      info['status_code'] = int(match.group(2))

                  # Server header
                  match = re.search(r'Server: ([^\r\n]+)', response, re.I)
                  if match:
                      info['server'] = match.group(1)
                      info.update(self.parse_server_header(match.group(1)))

                  # X-Powered-By
                  match = re.search(r'X-Powered-By: ([^\r\n]+)', response, re.I)
                  if match:
                      info['powered_by'] = match.group(1)

                  return info

              def parse_server_header(self, server):
                  '''Parse detailed version from Server header'''
                  info = {}

                  patterns = [
                      (r'Apache/(\d+\.\d+\.\d+)', 'apache', 'version'),
                      (r'nginx/(\d+\.\d+\.\d+)', 'nginx', 'version'),
                      (r'Microsoft-IIS/(\d+\.\d+)', 'iis', 'version'),
                      (r'PHP/(\d+\.\d+\.\d+)', 'php', 'version'),
                      (r'OpenSSL/(\d+\.\d+\.\d+\w*)', 'openssl', 'version'),
                  ]

                  for pattern, name, key in patterns:
                      match = re.search(pattern, server, re.I)
                      if match:
                          info[name] = match.group(1)

                  return info

              async def probe_ssl(self, ip, port):
                  '''Probe SSL/TLS service'''
                  try:
                      context = ssl.create_default_context()
                      context.check_hostname = False
                      context.verify_mode = ssl.CERT_NONE

                      reader, writer = await asyncio.wait_for(
                          asyncio.open_connection(ip, port, ssl=context),
                          timeout=self.timeout
                      )

                      # Get SSL info
                      ssl_object = writer.get_extra_info('ssl_object')

                      info = {
                          'protocol': ssl_object.version(),
                          'cipher': ssl_object.cipher(),
                      }

                      # Get certificate
                      cert = ssl_object.getpeercert(binary_form=True)
                      if cert:
                          import cryptography.x509
                          x509 = cryptography.x509.load_der_x509_certificate(cert)
                          info['subject'] = x509.subject.rfc4514_string()
                          info['issuer'] = x509.issuer.rfc4514_string()
                          info['not_after'] = str(x509.not_valid_after)

                          # Check for weak signature
                          sig_algo = x509.signature_algorithm_oid._name
                          if 'sha1' in sig_algo.lower() or 'md5' in sig_algo.lower():
                              info['weak_signature'] = True

                      writer.close()
                      await writer.wait_closed()

                      return info
                  except Exception as e:
                      return {'error': str(e)}

              async def probe_ssh(self, ip, port=22):
                  '''Probe SSH service'''
                  try:
                      reader, writer = await asyncio.wait_for(
                          asyncio.open_connection(ip, port),
                          timeout=self.timeout
                      )

                      # SSH sends banner first
                      banner = await asyncio.wait_for(reader.readline(), timeout=2)
                      banner = banner.decode('utf-8', errors='ignore').strip()

                      writer.close()
                      await writer.wait_closed()

                      info = {'banner': banner}

                      # Parse SSH banner: SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.2
                      match = re.match(r'SSH-(\d\.\d)-(\S+)', banner)
                      if match:
                          info['protocol'] = match.group(1)
                          info['software'] = match.group(2)

                          # Check for old/vulnerable versions
                          if 'OpenSSH' in info['software']:
                              ver_match = re.search(r'OpenSSH_(\d+\.\d+)', info['software'])
                              if ver_match:
                                  version = float(ver_match.group(1))
                                  if version < 7.0:
                                      info['vulnerable'] = 'OpenSSH < 7.0 has known vulnerabilities'

                      return info
                  except Exception as e:
                      return {'error': str(e)}

          async def main():
              fp = ServiceFingerprint()

              # Probe different services
              http_info = await fp.probe_http('example.com', 80)
              print("HTTP:", http_info)

              ssl_info = await fp.probe_ssl('example.com', 443)
              print("SSL:", ssl_info)

          asyncio.run(main())
      pitfalls:
      - Services may hide version info
      - Custom banners can mislead
      - SSL probing may fail on SNI
      concepts:
      - Service fingerprinting
      - Banner analysis
      - SSL/TLS analysis
      estimated_hours: 8-12
      deliverables:
      - Banner grabbing module connecting to open ports and capturing service identification strings
      - Service version detection matching banners against known software signature patterns
      - OS fingerprinting analyzing TCP/IP stack behavior to identify the host operating system
      - Technology stack identification detecting frameworks, languages, and platforms behind services
    - id: 4
      name: Vulnerability Detection
      description: Check discovered services against known vulnerability databases.
      acceptance_criteria:
      - Query NVD/CVE database for version-based vulnerabilities
      - Check for common misconfigurations such as open directories and default pages
      - Test for specific vulnerabilities (e.g., default credentials)
      - Generate vulnerability report with severity scores
      - Cache CVE data for offline use
      hints:
        level1: 'NVD has API for CVE lookup by product/version (CPE). Common checks: default creds, exposed admin panels,
          outdated TLS.'
        level2: Build CPE string from fingerprint (cpe:2.3:a:vendor:product:version). Check CVSS score for severity. Some
          vulns need active testing.
        level3: |-
          import json
          import sqlite3
          import aiohttp
          import asyncio
          from datetime import datetime, timedelta

          class VulnerabilityScanner:
              NVD_API = "https://services.nvd.nist.gov/rest/json/cves/2.0"

              def __init__(self, db_path='cve_cache.db'):
                  self.db_path = db_path
                  self.init_db()

              def init_db(self):
                  '''Initialize CVE cache database'''
                  conn = sqlite3.connect(self.db_path)
                  conn.execute('''
                      CREATE TABLE IF NOT EXISTS cve_cache (
                          cpe TEXT,
                          cve_id TEXT,
                          description TEXT,
                          cvss_score REAL,
                          severity TEXT,
                          cached_at TIMESTAMP,
                          PRIMARY KEY (cpe, cve_id)
                      )
                  ''')
                  conn.commit()
                  conn.close()

              async def query_nvd(self, cpe_string):
                  '''Query NVD for CVEs affecting a CPE'''
                  async with aiohttp.ClientSession() as session:
                      params = {
                          'cpeName': cpe_string,
                          'resultsPerPage': 100
                      }

                      async with session.get(self.NVD_API, params=params) as resp:
                          if resp.status == 200:
                              data = await resp.json()
                              return self.parse_nvd_response(data)
                          return []

              def parse_nvd_response(self, data):
                  '''Parse NVD API response'''
                  vulnerabilities = []

                  for item in data.get('vulnerabilities', []):
                      cve = item.get('cve', {})

                      vuln = {
                          'cve_id': cve.get('id'),
                          'description': self.get_description(cve),
                          'cvss_score': self.get_cvss_score(cve),
                          'severity': self.get_severity(cve),
                          'references': self.get_references(cve)
                      }

                      vulnerabilities.append(vuln)

                  return vulnerabilities

              def get_description(self, cve):
                  descriptions = cve.get('descriptions', [])
                  for desc in descriptions:
                      if desc.get('lang') == 'en':
                          return desc.get('value', '')
                  return ''

              def get_cvss_score(self, cve):
                  metrics = cve.get('metrics', {})

                  # Try CVSS 3.1, then 3.0, then 2.0
                  for version in ['cvssMetricV31', 'cvssMetricV30', 'cvssMetricV2']:
                      if version in metrics and metrics[version]:
                          return metrics[version][0].get('cvssData', {}).get('baseScore', 0)

                  return 0

              def get_severity(self, cve):
                  score = self.get_cvss_score(cve)
                  if score >= 9.0:
                      return 'CRITICAL'
                  elif score >= 7.0:
                      return 'HIGH'
                  elif score >= 4.0:
                      return 'MEDIUM'
                  elif score > 0:
                      return 'LOW'
                  return 'UNKNOWN'

              def get_references(self, cve):
                  refs = cve.get('references', [])
                  return [r.get('url') for r in refs[:5]]

              def build_cpe(self, service_info):
                  '''Build CPE string from service fingerprint'''
                  # CPE 2.3 format: cpe:2.3:a:vendor:product:version
                  cpe_mappings = {
                      'apache': 'cpe:2.3:a:apache:http_server:{version}',
                      'nginx': 'cpe:2.3:a:nginx:nginx:{version}',
                      'openssh': 'cpe:2.3:a:openbsd:openssh:{version}',
                      'mysql': 'cpe:2.3:a:oracle:mysql:{version}',
                      'postgresql': 'cpe:2.3:a:postgresql:postgresql:{version}',
                      'php': 'cpe:2.3:a:php:php:{version}',
                  }

                  for key, template in cpe_mappings.items():
                      if key in service_info:
                          version = service_info[key]
                          return template.format(version=version)

                  return None

              async def check_common_vulns(self, ip, port, service_info):
                  '''Check for common misconfigurations'''
                  vulns = []

                  # Default credentials check
                  defaults = await self.check_default_creds(ip, port, service_info)
                  vulns.extend(defaults)

                  # SSL/TLS checks
                  if service_info.get('protocol'):
                      ssl_vulns = self.check_ssl_vulns(service_info)
                      vulns.extend(ssl_vulns)

                  # Outdated software
                  if service_info.get('openssh'):
                      version = float(service_info['openssh'].split('.')[0])
                      if version < 7:
                          vulns.append({
                              'type': 'outdated_software',
                              'description': f'OpenSSH {service_info["openssh"]} is outdated',
                              'severity': 'HIGH'
                          })

                  return vulns

              async def check_default_creds(self, ip, port, service_info):
                  '''Check for default credentials'''
                  vulns = []

                  # Common default credential pairs
                  defaults = [
                      ('admin', 'admin'),
                      ('admin', 'password'),
                      ('root', 'root'),
                      ('admin', ''),
                  ]

                  # Would implement actual login attempts here
                  # This is a placeholder

                  return vulns

              def check_ssl_vulns(self, ssl_info):
                  '''Check for SSL/TLS vulnerabilities'''
                  vulns = []

                  protocol = ssl_info.get('protocol', '')

                  # Check for deprecated protocols
                  if 'SSLv3' in protocol:
                      vulns.append({
                          'type': 'ssl_vulnerability',
                          'cve': 'CVE-2014-3566',
                          'description': 'SSLv3 POODLE vulnerability',
                          'severity': 'HIGH'
                      })

                  if 'TLSv1.0' in protocol or 'TLSv1.1' in protocol:
                      vulns.append({
                          'type': 'ssl_vulnerability',
                          'description': f'{protocol} is deprecated',
                          'severity': 'MEDIUM'
                      })

                  # Check cipher strength
                  cipher = ssl_info.get('cipher', ('', '', 0))
                  if cipher[2] < 128:  # Key length
                      vulns.append({
                          'type': 'weak_cipher',
                          'description': f'Weak cipher: {cipher[0]} ({cipher[2]} bits)',
                          'severity': 'HIGH'
                      })

                  # Weak signature algorithm
                  if ssl_info.get('weak_signature'):
                      vulns.append({
                          'type': 'weak_signature',
                          'description': 'Certificate uses weak signature algorithm (SHA1/MD5)',
                          'severity': 'MEDIUM'
                      })

                  return vulns

              def generate_report(self, scan_results):
                  '''Generate vulnerability report'''
                  report = {
                      'scan_time': datetime.now().isoformat(),
                      'summary': {
                          'hosts_scanned': len(scan_results),
                          'critical': 0,
                          'high': 0,
                          'medium': 0,
                          'low': 0
                      },
                      'findings': []
                  }

                  for host in scan_results:
                      for vuln in host.get('vulnerabilities', []):
                          severity = vuln.get('severity', 'UNKNOWN')
                          if severity in report['summary']:
                              report['summary'][severity.lower()] += 1

                          report['findings'].append({
                              'host': host['ip'],
                              'port': host.get('port'),
                              **vuln
                          })

                  # Sort by severity
                  severity_order = {'CRITICAL': 0, 'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}
                  report['findings'].sort(key=lambda x: severity_order.get(x.get('severity'), 99))

                  return report
      pitfalls:
      - NVD API rate limits
      - CPE matching is imprecise
      - False positives from version detection
      concepts:
      - CVE/NVD database
      - CVSS scoring
      - CPE naming
      - Vulnerability assessment
      estimated_hours: 10-15
      deliverables:
      - Banner analysis module extracting version numbers for vulnerability correlation
      - CVE database integration querying NVD for known vulnerabilities by software version
      - Version-based vulnerability matching correlating detected versions with CVE entries
      - Configuration weakness checker testing for common misconfigurations and insecure defaults
    - id: 5
      name: Report Generation
      description: Generate comprehensive scan reports in multiple formats.
      acceptance_criteria:
      - HTML report includes a summary dashboard with severity distribution charts
      - JSON export produces machine-readable output for pipeline automation integration
      - Group findings by severity level with critical issues listed first
      - Include remediation suggestions with specific steps for each vulnerability
      - Executive summary provides a non-technical overview suitable for management review
      hints:
        level1: Use Jinja2 for HTML templates. JSON is easy with json.dumps. Group vulns by host, then severity. Add links
          to CVE details.
        level2: Include CVSS scores and severity colors (red=critical, orange=high). Add remediation from CVE references or
          generic advice.
        level3: |-
          from jinja2 import Template
          import json
          from datetime import datetime

          class ReportGenerator:
              HTML_TEMPLATE = '''
          <!DOCTYPE html>
          <html>
          <head>
              <title>Vulnerability Scan Report</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 20px; }
                  .summary { display: flex; gap: 20px; margin-bottom: 30px; }
                  .stat-card { padding: 20px; border-radius: 8px; min-width: 120px; text-align: center; }
                  .critical { background: #dc3545; color: white; }
                  .high { background: #fd7e14; color: white; }
                  .medium { background: #ffc107; color: black; }
                  .low { background: #28a745; color: white; }
                  .finding { border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 4px; }
                  .finding-critical { border-left: 4px solid #dc3545; }
                  .finding-high { border-left: 4px solid #fd7e14; }
                  .finding-medium { border-left: 4px solid #ffc107; }
                  .finding-low { border-left: 4px solid #28a745; }
                  .severity-badge { padding: 2px 8px; border-radius: 4px; font-size: 12px; }
                  table { width: 100%; border-collapse: collapse; }
                  th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }
                  th { background: #f5f5f5; }
              </style>
          </head>
          <body>
              <h1>Vulnerability Scan Report</h1>
              <p>Generated: {{ scan_time }}</p>

              <h2>Executive Summary</h2>
              <div class="summary">
                  <div class="stat-card critical">
                      <h3>{{ summary.critical }}</h3>
                      <p>Critical</p>
                  </div>
                  <div class="stat-card high">
                      <h3>{{ summary.high }}</h3>
                      <p>High</p>
                  </div>
                  <div class="stat-card medium">
                      <h3>{{ summary.medium }}</h3>
                      <p>Medium</p>
                  </div>
                  <div class="stat-card low">
                      <h3>{{ summary.low }}</h3>
                      <p>Low</p>
                  </div>
              </div>

              <h2>Findings</h2>
              {% for finding in findings %}
              <div class="finding finding-{{ finding.severity|lower }}">
                  <h3>
                      <span class="severity-badge {{ finding.severity|lower }}">{{ finding.severity }}</span>
                      {% if finding.cve_id %}{{ finding.cve_id }}{% else %}{{ finding.type }}{% endif %}
                  </h3>
                  <p><strong>Host:</strong> {{ finding.host }}{% if finding.port %}:{{ finding.port }}{% endif %}</p>
                  <p><strong>Description:</strong> {{ finding.description }}</p>
                  {% if finding.cvss_score %}
                  <p><strong>CVSS Score:</strong> {{ finding.cvss_score }}</p>
                  {% endif %}
                  {% if finding.remediation %}
                  <p><strong>Remediation:</strong> {{ finding.remediation }}</p>
                  {% endif %}
                  {% if finding.references %}
                  <p><strong>References:</strong></p>
                  <ul>
                  {% for ref in finding.references %}
                      <li><a href="{{ ref }}" target="_blank">{{ ref }}</a></li>
                  {% endfor %}
                  </ul>
                  {% endif %}
              </div>
              {% endfor %}

              <h2>Hosts Scanned</h2>
              <table>
                  <tr>
                      <th>Host</th>
                      <th>Open Ports</th>
                      <th>Vulnerabilities</th>
                  </tr>
                  {% for host in hosts %}
                  <tr>
                      <td>{{ host.ip }}</td>
                      <td>{{ host.ports|join(', ') }}</td>
                      <td>{{ host.vuln_count }}</td>
                  </tr>
                  {% endfor %}
              </table>
          </body>
          </html>
          '''

              REMEDIATION_DB = {
                  'outdated_software': 'Update to the latest stable version',
                  'ssl_vulnerability': 'Disable SSLv3 and TLSv1.0/1.1, use TLSv1.2 or TLSv1.3',
                  'weak_cipher': 'Configure server to use strong ciphers (AES-256-GCM, CHACHA20)',
                  'weak_signature': 'Renew certificate with SHA-256 or stronger signature',
                  'default_credentials': 'Change default credentials immediately',
              }

              def __init__(self, scan_results):
                  self.results = scan_results

              def add_remediation(self, findings):
                  '''Add remediation suggestions'''
                  for finding in findings:
                      vuln_type = finding.get('type', '')
                      if vuln_type in self.REMEDIATION_DB:
                          finding['remediation'] = self.REMEDIATION_DB[vuln_type]
                      elif finding.get('cve_id'):
                          finding['remediation'] = f"See CVE details: https://nvd.nist.gov/vuln/detail/{finding['cve_id']}"
                  return findings

              def generate_html(self, output_path):
                  '''Generate HTML report'''
                  template = Template(self.HTML_TEMPLATE)

                  # Prepare data
                  findings = self.add_remediation(self.results.get('findings', []))

                  # Group hosts
                  hosts = {}
                  for finding in findings:
                      ip = finding['host']
                      if ip not in hosts:
                          hosts[ip] = {'ip': ip, 'ports': set(), 'vuln_count': 0}
                      if finding.get('port'):
                          hosts[ip]['ports'].add(finding['port'])
                      hosts[ip]['vuln_count'] += 1

                  for host in hosts.values():
                      host['ports'] = sorted(host['ports'])

                  html = template.render(
                      scan_time=self.results.get('scan_time', datetime.now().isoformat()),
                      summary=self.results.get('summary', {}),
                      findings=findings,
                      hosts=list(hosts.values())
                  )

                  with open(output_path, 'w') as f:
                      f.write(html)

                  return output_path

              def generate_json(self, output_path):
                  '''Generate JSON report'''
                  with open(output_path, 'w') as f:
                      json.dump(self.results, f, indent=2, default=str)
                  return output_path

              def generate_executive_summary(self):
                  '''Generate text executive summary'''
                  summary = self.results.get('summary', {})
                  findings = self.results.get('findings', [])

                  total_vulns = sum([summary.get(s, 0) for s in ['critical', 'high', 'medium', 'low']])

                  text = f"""
          VULNERABILITY SCAN EXECUTIVE SUMMARY
          ====================================
          Scan Date: {self.results.get('scan_time', 'N/A')}
          Hosts Scanned: {summary.get('hosts_scanned', 0)}

          RISK OVERVIEW:
          - Critical: {summary.get('critical', 0)}
          - High: {summary.get('high', 0)}
          - Medium: {summary.get('medium', 0)}
          - Low: {summary.get('low', 0)}
          - Total: {total_vulns}

          TOP FINDINGS:
          """
                  # Add top 5 critical/high findings
                  critical_high = [f for f in findings if f.get('severity') in ['CRITICAL', 'HIGH']][:5]
                  for i, finding in enumerate(critical_high, 1):
                      text += f"""
          {i}. [{finding.get('severity')}] {finding.get('cve_id') or finding.get('type')}
             Host: {finding.get('host')}
             {finding.get('description', '')[:100]}...
          """

                  return text

          # Usage
          def main():
              # Example scan results
              results = {
                  'scan_time': datetime.now().isoformat(),
                  'summary': {
                      'hosts_scanned': 5,
                      'critical': 2,
                      'high': 5,
                      'medium': 10,
                      'low': 3
                  },
                  'findings': [
                      {
                          'host': '192.168.1.10',
                          'port': 443,
                          'severity': 'CRITICAL',
                          'cve_id': 'CVE-2021-44228',
                          'description': 'Log4j RCE vulnerability',
                          'cvss_score': 10.0,
                          'references': ['https://nvd.nist.gov/vuln/detail/CVE-2021-44228']
                      },
                      # ... more findings
                  ]
              }

              gen = ReportGenerator(results)
              gen.generate_html('report.html')
              gen.generate_json('report.json')
              print(gen.generate_executive_summary())

          if __name__ == '__main__':
              main()
      pitfalls:
      - Large reports slow to generate
      - Missing context for findings
      - Overwhelming detail for executives
      concepts:
      - Report generation
      - Data visualization
      - Risk communication
      estimated_hours: 6-10
      deliverables:
      - Vulnerability report structure organizing findings by host, service, and severity
      - Severity classification using CVSS scores to rank findings as critical, high, medium, or low
      - Remediation recommendations providing actionable fix guidance for each finding
      - Export formats supporting JSON for automation and HTML for human review
