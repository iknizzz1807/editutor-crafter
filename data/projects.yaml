# EduTutor Crafter - Projects Data (Unified)
# Source of truth for all domains, projects, and milestones
# Version: 2.0.0 - Added SE Practices, bridge projects, DSA prerequisites
#
# Structure:
#   1. PREREQUISITES - Required knowledge before starting
#   2. DOMAINS - Full taxonomy with all difficulty levels
#   3. EXPERT PROJECTS - Detailed specifications

# =============================================================================
# PREREQUISITES - Required Foundation Knowledge
# =============================================================================
# These topics are ASSUMED, not taught. Learners should have this foundation
# before starting intermediate+ projects.
#
# Based on ACM/IEEE CS2023 curriculum standards and industry requirements.

prerequisites:
  # ---------------------------------------------------------------------------
  # ALGORITHMS & DATA STRUCTURES (Critical - 40-50% of tech interviews)
  # ---------------------------------------------------------------------------
  algorithms_and_data_structures:
    importance: critical
    note: |
      This curriculum focuses on "Build Your Own X" applied projects.
      DSA knowledge is a PREREQUISITE, not core content.
      Recommended: Complete a DSA course before starting Advanced/Expert projects.

    required_for_intermediate:
      - Arrays, strings, basic operations
      - Hash tables (dictionaries/maps)
      - Linked lists basics
      - Stacks and queues
      - Basic sorting (quicksort, mergesort)
      - Binary search
      - Time complexity (Big O basics)

    required_for_advanced:
      - Trees (binary, BST, traversals)
      - Graphs (representation, BFS, DFS)
      - Recursion and backtracking
      - Dynamic programming basics
      - Heaps and priority queues

    required_for_expert:
      - Advanced trees (Red-Black, B-trees)
      - Graph algorithms (Dijkstra, topological sort)
      - Advanced DP patterns
      - Bit manipulation
      - Amortized analysis

    recommended_resources:
      - name: "LeetCode"
        url: "https://leetcode.com"
        type: practice
      - name: "NeetCode Roadmap"
        url: "https://neetcode.io/roadmap"
        type: curriculum
      - name: "Algorithms by Sedgewick"
        url: "https://algs4.cs.princeton.edu/"
        type: book
      - name: "MIT 6.006 Introduction to Algorithms"
        url: "https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-spring-2020/"
        type: course

  # ---------------------------------------------------------------------------
  # MATHEMATICAL FOUNDATIONS
  # ---------------------------------------------------------------------------
  mathematics:
    importance: medium
    note: Required for AI/ML, Graphics, and some Systems projects.

    topics:
      - Discrete math (sets, logic, proofs)
      - Linear algebra (vectors, matrices) - for ML/Graphics
      - Probability and statistics - for ML
      - Calculus basics - for ML (derivatives, gradients)

# =============================================================================
# DOMAINS & PROJECT CATALOG
# =============================================================================
# All projects organized by domain and difficulty level
# Expert projects reference detailed specs below

domains:
  # ---------------------------------------------------------------------------
  # 1. APPLICATION DEVELOPMENT
  # ---------------------------------------------------------------------------
  - id: app-dev
    name: Application Development
    icon: "web"
    subdomains:
      - id: web-frontend
        name: Web Frontend
        topics: [HTML/CSS, JavaScript, React, Vue, Svelte]
      - id: web-backend
        name: Web Backend
        topics: [APIs, Auth, REST, GraphQL]
      - id: web-fullstack
        name: Full-stack
        topics: [Integration, Deployment]
      - id: mobile
        name: Mobile Development
        topics: [iOS, Android, React Native, Flutter]
      - id: desktop
        name: Desktop & CLI
        topics: [Electron, Tauri, CLI Tools]

    projects:
      beginner:
        - id: todo-app
          name: Todo App
          description: Basic CRUD application
        - id: weather-app
          name: Weather App
          description: API consumption, async data
        - id: portfolio-site
          name: Portfolio Website
          description: Static site, responsive design
        - id: calculator
          name: Calculator
          description: UI state management

      intermediate:
        - id: blog-platform
          name: Blog Platform
          description: Full CRUD, auth, markdown
        - id: chat-app
          name: Real-time Chat
          description: WebSocket, real-time updates
        - id: ecommerce-basic
          name: E-commerce (Basic)
          description: Cart, checkout flow

      advanced:
        - id: social-network
          name: Social Network
          description: Feed, followers, notifications
        - id: video-streaming
          name: Video Streaming
          description: HLS, adaptive bitrate

      expert:
        - id: build-react
          name: Build Your Own React
          description: Virtual DOM, reconciliation, hooks, fiber
          detailed: false  # Basic milestones only
        - id: build-bundler
          name: Build Your Own Bundler
          description: Module resolution, tree shaking, code splitting
          detailed: false
        - id: build-spreadsheet
          name: Build Your Own Spreadsheet
          description: Excel-like app with formulas
          detailed: true  # Full specs in expert-projects section
        - id: build-web-framework
          name: Build Your Own Web Framework
          description: Express/Django clone
          detailed: false

  # ---------------------------------------------------------------------------
  # 2. SYSTEMS & LOW-LEVEL
  # ---------------------------------------------------------------------------
  - id: systems
    name: Systems & Low-Level
    icon: "cpu"
    subdomains:
      - id: systems-prog
        name: Systems Programming
        topics: [Memory, Concurrency, I/O]
      - id: networking
        name: Networking
        topics: [TCP/UDP, HTTP, DNS]
      - id: os
        name: Operating Systems
        topics: [Process, Memory, Kernel]

    projects:
      beginner:
        - id: cat-clone
          name: Cat Clone
          description: File reading, stdout
        - id: wc-clone
          name: Wc Clone
          description: Line/word/byte counting
        - id: grep-clone
          name: Grep Clone
          description: Pattern matching basics
        - id: file-copy
          name: File Copy (cp clone)
          description: File I/O, permissions

      intermediate:
        - id: shell-basic
          name: Shell (Basic)
          description: Pipes, redirects
        - id: http-server-basic
          name: HTTP Server (Basic)
          description: Static file serving
        - id: memory-pool
          name: Memory Pool Allocator
          description: Fixed-size block allocation
        # BRIDGE PROJECTS - fill gap to Expert
        - id: process-spawner
          name: Process Spawner
          description: fork/exec, process lifecycle, wait
          bridge: true
        - id: signal-handler
          name: Signal Handler
          description: SIGINT, SIGTERM, signal masks
          bridge: true

      advanced:
        - id: http2-server
          name: HTTP/2 Server
          description: Multiplexing, HPACK
        - id: container-basic
          name: Container (Basic)
          description: Namespaces isolation
        # BRIDGE PROJECTS - fill gap to Expert
        - id: mini-shell
          name: Mini Shell
          description: Job control, background processes, history
          bridge: true
        - id: virtual-memory-sim
          name: Virtual Memory Simulator
          description: Page tables, TLB, page replacement
          bridge: true
        - id: cgroups-explorer
          name: Cgroups Explorer
          description: CPU/memory limits, resource isolation
          bridge: true

      expert:
        - id: build-docker
          name: Build Your Own Docker
          description: Container runtime
          detailed: true
          languages: [go, rust, c]
        - id: build-shell
          name: Build Your Own Shell
          description: Full Unix shell
          detailed: true
          languages: [c, rust, go]
        - id: build-allocator
          name: Build Your Own Memory Allocator
          description: malloc/free implementation
          detailed: true
          languages: [c, rust, zig]
        - id: build-os
          name: Build Your Own OS
          description: Operating system kernel
          detailed: true
          languages: [c, rust, zig]
        - id: build-tcp-stack
          name: Build Your Own TCP/IP Stack
          description: Network stack implementation
          detailed: true
          languages: [c, rust, go]

  # ---------------------------------------------------------------------------
  # 3. DATA & STORAGE
  # ---------------------------------------------------------------------------
  - id: data-storage
    name: Data & Storage
    icon: "database"
    subdomains:
      - id: databases
        name: Databases
        topics: [SQL, NoSQL, B-trees, WAL]
      - id: data-eng
        name: Data Engineering
        topics: [ETL, Stream processing]

    projects:
      beginner:
        - id: json-db
          name: JSON File Database
          description: File-based storage
        - id: kv-memory
          name: In-memory Key-Value Store
          description: Hash map based

      intermediate:
        - id: btree-impl
          name: B-tree Implementation
          description: Insert, delete, rebalance
        - id: sql-parser
          name: SQL Parser
          description: SELECT, WHERE, JOIN

      advanced:
        - id: query-optimizer
          name: Query Optimizer
          description: Cost estimation
        - id: wal-impl
          name: WAL Implementation
          description: Write-ahead logging

      expert:
        - id: build-redis
          name: Build Your Own Redis
          description: In-memory data store
          detailed: true
        - id: build-sqlite
          name: Build Your Own SQLite
          description: Embedded SQL database
          detailed: true
        - id: build-kafka
          name: Build Your Own Kafka
          description: Distributed message queue
          detailed: false

  # ---------------------------------------------------------------------------
  # 4. DISTRIBUTED & CLOUD
  # ---------------------------------------------------------------------------
  # Note: Blockchain moved here as an application of consensus algorithms
  # (per ACM/IEEE CS2023 - blockchain is not a standalone knowledge area)
  - id: distributed
    name: Distributed & Cloud
    icon: "cloud"
    subdomains:
      - id: distributed-sys
        name: Distributed Systems
        topics: [CAP, Consensus, Sharding, Replication]
      - id: cloud-devops
        name: Cloud & DevOps
        topics: [Containers, K8s, CI/CD]
      - id: consensus
        name: Consensus & Coordination
        topics: [Leader Election, Raft, Paxos, 2PC]
      - id: blockchain-apps
        name: Blockchain Applications
        topics: [Proof of Work, Smart Contracts, P2P Networks]
        note: Application of consensus, not core competency

    projects:
      beginner:
        - id: service-discovery
          name: Service Discovery
          description: Registry, health checks
        - id: rpc-basic
          name: RPC Framework (Basic)
          description: Remote procedure calls, serialization
          bridge: true

      intermediate:
        - id: load-balancer-basic
          name: Load Balancer (Basic)
          description: Round-robin
        - id: rate-limiter
          name: Rate Limiter
          description: Token bucket
        # BRIDGE PROJECTS - prepare for consensus
        - id: leader-election
          name: Leader Election
          description: Bully algorithm, ring election
          bridge: true
        - id: replicated-log
          name: Replicated Log
          description: Append-only log, basic replication
          bridge: true
        - id: vector-clocks
          name: Vector Clocks
          description: Logical time, causality tracking
          bridge: true

      advanced:
        - id: distributed-cache
          name: Distributed Cache
          description: Consistent hashing
        # BRIDGE PROJECTS - prepare for Raft/Blockchain
        - id: gossip-protocol
          name: Gossip Protocol
          description: Epidemic broadcast, failure detection
          bridge: true
        - id: 2pc-impl
          name: Two-Phase Commit
          description: Distributed transactions basics
          bridge: true

      expert:
        - id: build-raft
          name: Build Your Own Raft
          description: Consensus algorithm
          detailed: false
          languages: [go, rust, java]
          note: Prerequisite - complete leader-election and replicated-log first
        - id: build-blockchain
          name: Build Your Own Blockchain
          description: Proof of work, transactions, P2P
          detailed: true
          languages: [python, go, rust, javascript]
          note: Application of consensus - good after understanding distributed basics
        - id: build-distributed-kv
          name: Build Your Own Distributed KV Store
          description: Partitioning, replication, consistency
          detailed: false
          languages: [go, rust, java]

  # ---------------------------------------------------------------------------
  # 5. AI & MACHINE LEARNING
  # ---------------------------------------------------------------------------
  - id: ai-ml
    name: AI & Machine Learning
    icon: "brain"
    subdomains:
      - id: classical-ml
        name: Classical ML
        topics: [Regression, Classification, Trees]
      - id: deep-learning
        name: Deep Learning
        topics: [Neural networks, CNNs, Transformers]
      - id: nlp
        name: NLP
        topics: [Tokenization, Language models]

    projects:
      beginner:
        - id: linear-regression
          name: Linear Regression
          description: Gradient descent
        - id: knn
          name: KNN Classifier
          description: Distance metrics

      intermediate:
        - id: neural-network-basic
          name: Neural Network (micrograd)
          description: Forward/backward pass
        - id: word2vec
          name: Word Embeddings
          description: Skip-gram, CBOW

      advanced:
        - id: transformer-scratch
          name: Transformer from Scratch
          description: Attention mechanism
        - id: gan
          name: GAN
          description: Generator, discriminator

      expert:
        - id: build-nn-framework
          name: Build Your Own Neural Network Framework
          description: PyTorch/TensorFlow clone
          detailed: true
        - id: build-transformer
          name: Build Your Own Transformer
          description: Full transformer/GPT
          detailed: true

  # ---------------------------------------------------------------------------
  # 6. GAME DEVELOPMENT
  # ---------------------------------------------------------------------------
  - id: game-dev
    name: Game Development
    icon: "gamepad"
    subdomains:
      - id: game-prog
        name: Game Programming
        topics: [Game loop, Physics, AI]
      - id: graphics
        name: Graphics
        topics: [2D/3D rendering, Shaders]
      - id: engine-dev
        name: Engine Development
        topics: [ECS, Physics, Audio]

    projects:
      beginner:
        - id: pong
          name: Pong
          description: Basic game loop
        - id: snake
          name: Snake
          description: Grid movement
        - id: tetris
          name: Tetris
          description: Rotation, line clearing

      intermediate:
        - id: platformer
          name: Platformer
          description: Gravity, jumping
        - id: topdown-shooter
          name: Top-down Shooter
          description: Enemies, projectiles

      advanced:
        - id: software-3d
          name: Software 3D Renderer
          description: No GPU, pure math
        - id: ecs-arch
          name: ECS Architecture
          description: Entity-component-system

      expert:
        - id: build-game-engine
          name: Build Your Own Game Engine
          description: Full 2D/3D engine
          detailed: true
        - id: build-raytracer
          name: Build Your Own Ray Tracer
          description: Path tracing renderer
          detailed: true

  # ---------------------------------------------------------------------------
  # 7. LANGUAGES & COMPILERS
  # ---------------------------------------------------------------------------
  - id: compilers
    name: Languages & Compilers
    icon: "code"
    subdomains:
      - id: parsing
        name: Parsing & Lexing
        topics: [Lexers, Parsers, AST]
      - id: interpreters
        name: Interpreters
        topics: [Tree-walking, Bytecode VMs]
      - id: compilers-sub
        name: Compilers
        topics: [IR, Code generation, LLVM]
      - id: runtime
        name: Runtime Systems
        topics: [GC, JIT]

    projects:
      beginner:
        - id: calculator-parser
          name: Calculator Parser
          description: Arithmetic expressions
        - id: json-parser
          name: JSON Parser
          description: Recursive descent
        - id: tokenizer
          name: Tokenizer/Lexer
          description: Token types, state machine
          bridge: true

      intermediate:
        - id: lisp-interp
          name: Lisp Interpreter
          description: S-expressions, eval
        - id: bytecode-vm
          name: Bytecode VM
          description: Stack-based
        # BRIDGE PROJECTS - following Crafting Interpreters sequence
        - id: ast-builder
          name: AST Builder
          description: Parse expressions to AST, pretty-print
          bridge: true
        - id: ast-interpreter
          name: AST Tree-Walking Interpreter
          description: Evaluate AST directly
          bridge: true

      advanced:
        - id: type-checker
          name: Type Checker
          description: Type inference
        - id: simple-gc
          name: Simple GC
          description: Mark-sweep
        # BRIDGE PROJECTS
        - id: bytecode-compiler
          name: Bytecode Compiler
          description: AST to bytecode, instruction encoding
          bridge: true
        - id: wasm-emitter
          name: WebAssembly Emitter
          description: Emit .wasm from simple AST
          bridge: true
          note: Adds WebAssembly as compilation target

      expert:
        - id: build-interpreter
          name: Build Your Own Interpreter (Lox)
          description: Crafting Interpreters
          detailed: true
          languages: [java, c, rust, go, python]
        - id: build-gc
          name: Build Your Own Garbage Collector
          description: Memory management
          detailed: true
          languages: [c, rust, zig]
        - id: build-regex
          name: Build Your Own Regex Engine
          description: NFA/DFA, Thompson construction
          detailed: true
          languages: [c, rust, go, python, javascript]

  # ---------------------------------------------------------------------------
  # 8. SECURITY
  # ---------------------------------------------------------------------------
  - id: security
    name: Security
    icon: "shield"
    subdomains:
      - id: crypto
        name: Cryptography
        topics: [Symmetric, Asymmetric, TLS]
      - id: web-sec
        name: Web Security
        topics: [OWASP, Auth]

    projects:
      beginner:
        - id: hash-impl
          name: Hash Function
          description: SHA-256 from spec
        - id: password-hashing
          name: Password Hashing
          description: bcrypt, salt

      intermediate:
        - id: aes-impl
          name: AES Implementation
          description: Block cipher
        - id: jwt-impl
          name: JWT Library
          description: Sign, verify

      advanced:
        - id: https-client
          name: HTTPS Client
          description: TLS handshake

      expert:
        - id: build-tls
          name: Build Your Own TLS
          description: TLS 1.3
          detailed: false

  # ---------------------------------------------------------------------------
  # 9. CS FUNDAMENTALS
  # ---------------------------------------------------------------------------
  - id: cs-fundamentals
    name: CS Fundamentals
    icon: "graduation-cap"
    subdomains:
      - id: data-structures
        name: Data Structures
        topics: [Arrays, Trees, Graphs, Hash tables]
      - id: algorithms
        name: Algorithms
        topics: [Sorting, Graphs, DP]

    projects:
      beginner:
        - id: linked-list
          name: Linked List
          description: Single, double, circular
        - id: stack-queue
          name: Stack & Queue
          description: Array and linked

      intermediate:
        - id: bst
          name: Binary Search Tree
          description: Insert, delete, traversal
        - id: hash-table
          name: Hash Table
          description: Chaining, open addressing

      advanced:
        - id: red-black-tree
          name: Red-Black Tree
          description: Balanced tree
        - id: graph-algos
          name: Graph Algorithms
          description: BFS, DFS, Dijkstra

      expert:
        - id: build-btree
          name: Build Your Own B-tree
          description: Disk-friendly tree
          detailed: false

  # ---------------------------------------------------------------------------
  # 10. SPECIALIZED (Dev Tools, Networking, Embedded)
  # ---------------------------------------------------------------------------
  - id: specialized
    name: Specialized
    icon: "wrench"
    subdomains:
      - id: dev-tools
        name: Developer Tools
        topics: [Git, Editors, LSP, Build systems]
      - id: networking-adv
        name: Advanced Networking
        topics: [P2P, Proxy, Load balancer]
      - id: embedded
        name: Embedded & Emulation
        topics: [RTOS, Emulators]

    projects:
      expert:
        - id: build-git
          name: Build Your Own Git
          description: Version control
          detailed: true
        - id: build-text-editor
          name: Build Your Own Text Editor
          description: Vim-like editor
          detailed: true
        - id: build-bittorrent
          name: Build Your Own BitTorrent
          description: P2P file sharing
          detailed: true
        - id: build-dns
          name: Build Your Own DNS Server
          description: Recursive resolver, authoritative server
          detailed: true
        - id: build-debugger
          name: Build Your Own Debugger
          description: GDB-like
          detailed: false
        - id: build-lsp
          name: Build Your Own LSP Server
          description: Language server protocol
          detailed: true
        - id: build-emulator
          name: Build Your Own Emulator
          description: NES/GameBoy/CHIP-8
          detailed: false
        - id: build-browser
          name: Build Your Own Browser
          description: Browser engine
          detailed: false

  # ---------------------------------------------------------------------------
  # 11. SOFTWARE ENGINEERING PRACTICES (NEW)
  # ---------------------------------------------------------------------------
  # Based on ACM/IEEE CS2023 recommendations and industry best practices
  # Critical for production readiness - fills gap identified in curriculum review
  - id: software-engineering
    name: Software Engineering Practices
    icon: "check-circle"
    note: |
      This domain addresses production engineering skills often missing from
      "build from scratch" curricula. Essential for professional readiness.

    subdomains:
      - id: testing
        name: Testing & Quality
        topics: [Unit Testing, Integration Testing, TDD, Property-based Testing]
      - id: cicd
        name: CI/CD & Automation
        topics: [Pipelines, GitHub Actions, Jenkins, Deployment]
      - id: observability
        name: Observability
        topics: [Logging, Metrics, Tracing, OpenTelemetry]
      - id: practices
        name: Engineering Practices
        topics: [Code Review, Documentation, Agile, Design Patterns]

    projects:
      beginner:
        - id: unit-testing-basics
          name: Unit Testing Fundamentals
          description: Write tests for existing code, pytest/jest basics
        - id: git-workflow
          name: Git Workflow Mastery
          description: Branching, PRs, rebasing, conflict resolution
        - id: documentation-project
          name: Documentation Project
          description: Write docs for a codebase, README, API docs

      intermediate:
        - id: tdd-kata
          name: TDD Kata Series
          description: Red-green-refactor cycle, test-first development
        - id: ci-pipeline
          name: CI Pipeline Setup
          description: GitHub Actions/Jenkins for build, test, lint
        - id: logging-structured
          name: Structured Logging
          description: Log levels, JSON logs, log aggregation
        - id: code-review-practice
          name: Code Review Practice
          description: Review real PRs, give/receive feedback

      advanced:
        - id: integration-testing
          name: Integration Testing Suite
          description: Test containers, database mocking, API testing
        - id: cd-deployment
          name: CD with Blue-Green Deployment
          description: Zero-downtime deploys, rollback strategies
        - id: metrics-dashboard
          name: Metrics & Alerting Dashboard
          description: Prometheus, Grafana, custom metrics
        - id: distributed-tracing
          name: Distributed Tracing
          description: OpenTelemetry, Jaeger, trace propagation

      expert:
        - id: build-test-framework
          name: Build Your Own Test Framework
          description: pytest/jest clone with assertions, fixtures, runners
          detailed: true
        - id: build-ci-system
          name: Build Your Own CI System
          description: Pipeline executor, job scheduling, artifact storage
          detailed: false
        - id: build-observability-platform
          name: Build Your Own Observability Platform
          description: Collect logs/metrics/traces, storage, query, visualize
          detailed: false

# =============================================================================
# EXPERT PROJECTS - DETAILED SPECIFICATIONS
# =============================================================================
# 18 projects with full details: acceptance criteria, 3-level hints, pitfalls
# Projects marked detailed: true above have full specs here

# =============================================================================
# BUILD YOUR OWN REDIS
# =============================================================================
build-redis:
  id: build-redis
  name: Build Your Own Redis
  description: |
    Build an in-memory data structure store that implements the Redis protocol.
    You'll learn about TCP servers, protocol parsing, data structures, and persistence.

  difficulty: expert
  estimated_hours: 40-60

  prerequisites:
    - TCP/IP networking basics
    - Hash tables and data structures
    - Concurrency fundamentals
    - File I/O

  languages:
    recommended: [Go, Rust, C]
    also_possible: [Python, Java, TypeScript]

  resources:
    tutorials:
      - name: "CodeCrafters Redis Challenge"
        url: "https://app.codecrafters.io/courses/redis/overview"
        type: interactive
      - name: "Build Your Own Redis (Book)"
        url: "https://build-your-own.org/redis/"
        type: book

    documentation:
      - name: "Redis Protocol Specification (RESP)"
        url: "https://redis.io/docs/reference/protocol-spec/"
      - name: "Redis Commands Reference"
        url: "https://redis.io/commands/"
      - name: "Redis Internals"
        url: "https://redis.io/docs/reference/internals/"

    books:
      - name: "Redis in Action"
        author: "Josiah Carlson"
        isbn: "978-1617290855"
      - name: "The Design and Implementation of Redis"
        note: "Read Redis source code with antirez comments"

  milestones:
    - id: redis-01
      name: TCP Server + RESP Protocol
      description: |
        Create a TCP server that listens on port 6379 and responds to PING with +PONG.
        Implement basic RESP (Redis Serialization Protocol) parsing.

      acceptance_criteria:
        - Server binds to port 6379
        - Server accepts TCP connections
        - Responds to PING with "+PONG\r\n"
        - Handles multiple sequential commands from same client
        - Clean shutdown on SIGINT

      hints:
        - level1: "Start with a simple TCP server using your language's net library. Accept connections in a loop."
        - level2: "RESP Simple Strings start with '+', errors with '-'. PING expects '+PONG\\r\\n' response."
        - level3: |
            RESP format examples:
            - Simple String: +OK\r\n
            - Error: -ERR unknown command\r\n
            - Integer: :1000\r\n
            - Bulk String: $5\r\nhello\r\n
            - Array: *2\r\n$4\r\nPING\r\n$4\r\nPONG\r\n

      pitfalls:
        - "Forgetting \\r\\n line endings (CRLF, not just LF)"
        - "Not handling partial reads (TCP is a stream, not message-based)"
        - "Blocking the main thread on single client"

      concepts:
        - TCP sockets and server lifecycle
        - RESP protocol encoding/decoding
        - Basic network I/O

      estimated_hours: 3-4

      tests:
        - command: "echo 'PING' | nc localhost 6379"
          expect: "+PONG"
        - command: "redis-cli PING"
          expect: "PONG"

    - id: redis-02
      name: GET/SET/DEL Commands
      description: |
        Implement basic key-value operations. Store data in an in-memory hash map.

      acceptance_criteria:
        - SET key value stores the key-value pair
        - GET key returns the value or nil
        - DEL key removes the key and returns 1 (or 0 if not exists)
        - Keys are case-sensitive
        - Values can be any string (including binary)

      hints:
        - level1: "Use a hash map (dict/map) to store key-value pairs."
        - level2: "Parse RESP arrays for commands. SET comes as *3\\r\\n$3\\r\\nSET\\r\\n$3\\r\\nkey\\r\\n$5\\r\\nvalue\\r\\n"
        - level3: "GET returns Bulk String ($length\\r\\nvalue\\r\\n) or Null Bulk String ($-1\\r\\n)"

      pitfalls:
        - "Not handling binary-safe strings (values can contain any bytes)"
        - "Forgetting null response for non-existent keys"
        - "Case sensitivity issues with keys"

      concepts:
        - Hash table implementation
        - RESP array parsing
        - Command dispatch pattern

      estimated_hours: 2-3

    - id: redis-03
      name: Expiration (TTL)
      description: |
        Add key expiration support. Keys can be set with PX (milliseconds) or EX (seconds) options.

      acceptance_criteria:
        - SET key value PX milliseconds sets expiration
        - SET key value EX seconds sets expiration
        - GET returns nil for expired keys
        - TTL key returns remaining time (-1 if no expiry, -2 if not exists)
        - Expired keys are cleaned up (lazy or active deletion)

      hints:
        - level1: "Store expiration timestamp alongside each value."
        - level2: "Check expiration on GET (lazy deletion). Optionally run background cleanup."
        - level3: |
            Two deletion strategies:
            1. Lazy: Check on access, delete if expired
            2. Active: Background goroutine/thread periodically scans and deletes
            Redis uses both. Start with lazy, add active later.

      pitfalls:
        - "Using relative time instead of absolute timestamp"
        - "Not handling clock drift in distributed scenarios"
        - "Memory leaks from never-accessed expired keys (need active deletion)"

      concepts:
        - TTL and expiration strategies
        - Lazy vs active deletion
        - Time handling and precision

      estimated_hours: 2-3

    - id: redis-04
      name: Data Structures (List, Set, Hash)
      description: |
        Implement Redis data structure commands beyond simple strings.

      acceptance_criteria:
        - LPUSH/RPUSH adds elements to list
        - LPOP/RPOP removes and returns elements
        - LRANGE returns range of elements
        - SADD adds to set, SMEMBERS returns all members
        - HSET/HGET for hash field operations

      hints:
        - level1: "Use native data structures: arrays for lists, sets for sets, nested maps for hashes."
        - level2: "Lists in Redis are doubly-linked for O(1) push/pop at both ends."
        - level3: "For sorted sets (ZADD), consider skip list or balanced tree for O(log n) operations."

      pitfalls:
        - "Using array for list (O(n) insert at head)"
        - "Not handling type errors (LPUSH on a string key)"
        - "LRANGE negative indices handling"

      concepts:
        - Linked lists vs arrays
        - Set data structure
        - Skip lists (for sorted sets)

      estimated_hours: 4-6

    - id: redis-05
      name: Persistence (RDB Snapshots)
      description: |
        Implement point-in-time snapshots using RDB format. SAVE blocks, BGSAVE forks.

      acceptance_criteria:
        - SAVE command creates RDB file synchronously
        - BGSAVE forks child process for background save
        - Server loads RDB on startup if file exists
        - RDB file format is binary with checksums

      hints:
        - level1: "Start with custom binary format. Later match Redis RDB format."
        - level2: "BGSAVE uses fork() - child inherits memory snapshot via copy-on-write."
        - level3: |
            RDB file structure:
            - Magic: "REDIS" + version
            - Database selector
            - Key-value pairs with type byte
            - EOF marker
            - CRC64 checksum

      pitfalls:
        - "Blocking main thread during save"
        - "Not using fork() for BGSAVE (child should have memory snapshot)"
        - "Corrupted RDB from incomplete writes (use temp file + rename)"

      concepts:
        - Binary serialization
        - Process forking and copy-on-write
        - Atomic file operations

      estimated_hours: 5-8

      resources:
        - name: "RDB File Format"
          url: "https://rdb.fnordig.de/file_format.html"

    - id: redis-06
      name: Persistence (AOF)
      description: |
        Implement Append-Only File logging for durability. Every write is logged.

      acceptance_criteria:
        - All write commands appended to AOF file
        - AOF file contains RESP-formatted commands
        - Server replays AOF on startup
        - BGREWRITEAOF compacts the file
        - Configurable fsync policy (always, everysec, no)

      hints:
        - level1: "Simply append each write command to a file in RESP format."
        - level2: "BGREWRITEAOF: fork, iterate current state, write minimal commands to rebuild."
        - level3: |
            fsync strategies:
            - always: fsync after every write (safest, slowest)
            - everysec: fsync once per second (good balance)
            - no: let OS decide (fastest, riskier)

      pitfalls:
        - "AOF growing unbounded without rewrite"
        - "Not handling concurrent writes during BGREWRITEAOF"
        - "Data loss from buffered but not fsynced writes"

      concepts:
        - Write-ahead logging
        - fsync and durability guarantees
        - Log compaction

      estimated_hours: 4-6

    - id: redis-07
      name: Pub/Sub
      description: |
        Implement publish/subscribe messaging pattern.

      acceptance_criteria:
        - SUBSCRIBE channel subscribes client to channel
        - PUBLISH channel message sends to all subscribers
        - UNSUBSCRIBE removes subscription
        - Subscribed clients receive messages with special format
        - Pattern subscriptions (PSUBSCRIBE) optional

      hints:
        - level1: "Maintain map of channel -> list of subscriber connections."
        - level2: "Subscribed clients enter special mode - only pub/sub commands allowed."
        - level3: "Message format: *3\\r\\n$7\\r\\nmessage\\r\\n$channel\\r\\n$message\\r\\n"

      pitfalls:
        - "Not blocking other commands in subscribed state"
        - "Memory leaks from disconnected subscribers"
        - "Race conditions in publish to multiple subscribers"

      concepts:
        - Pub/Sub pattern
        - Observer pattern
        - Connection state management

      estimated_hours: 3-4

    - id: redis-08
      name: Cluster Mode (Sharding)
      description: |
        Implement horizontal scaling with hash slot based sharding.

      acceptance_criteria:
        - 16384 hash slots distributed across nodes
        - CLUSTER SLOTS returns slot distribution
        - -MOVED redirect for keys on other nodes
        - Client can follow redirects
        - Hash tags for multi-key operations

      hints:
        - level1: "CRC16(key) % 16384 gives the slot. Each node owns a range of slots."
        - level2: "On wrong node, return -MOVED slot ip:port. Client should retry."
        - level3: "Hash tags: {user123}.profile and {user123}.settings go to same slot."

      pitfalls:
        - "Not handling key migration during resharding"
        - "Cross-slot operations (MGET across nodes)"
        - "Network partitions and split brain"

      concepts:
        - Consistent hashing
        - Hash slots and key routing
        - Distributed systems fundamentals

      estimated_hours: 8-12

# =============================================================================
# BUILD YOUR OWN SQLITE
# =============================================================================
build-sqlite:
  id: build-sqlite
  name: Build Your Own SQLite
  description: |
    Build an embedded SQL database that stores data in a single file.
    You'll learn about SQL parsing, B-trees, query execution, and transactions.

  difficulty: expert
  estimated_hours: 60-100

  prerequisites:
    - B-tree data structure
    - SQL basics
    - File I/O and binary formats
    - Basic compiler concepts (lexing, parsing)

  languages:
    recommended: [C, Rust, Go]
    also_possible: [Python, Java]

  resources:
    tutorials:
      - name: "Let's Build a Simple Database"
        url: "https://cstack.github.io/db_tutorial/"
        type: blog_series
        note: "Excellent C tutorial, incomplete but covers fundamentals"
      - name: "CodeCrafters SQLite Challenge"
        url: "https://app.codecrafters.io/courses/sqlite/overview"
        type: interactive

    documentation:
      - name: "SQLite File Format"
        url: "https://www.sqlite.org/fileformat2.html"
      - name: "SQLite Architecture"
        url: "https://www.sqlite.org/arch.html"
      - name: "SQLite B-Tree Module"
        url: "https://sqlite.org/btreemodule.html"

    books:
      - name: "Database Internals"
        author: "Alex Petrov"
        isbn: "978-1492040347"
        note: "Excellent for understanding storage engines"
      - name: "SQLite source code"
        note: "Well-commented, readable C code"

    courses:
      - name: "CMU 15-445 Database Systems"
        url: "https://15445.courses.cs.cmu.edu"
        note: "Andy Pavlo's excellent database course"

  milestones:
    - id: sqlite-01
      name: SQL Tokenizer
      description: |
        Build a lexer that converts SQL text into tokens.

      acceptance_criteria:
        - Tokenizes keywords (SELECT, FROM, WHERE, INSERT, etc.)
        - Handles identifiers (table names, column names)
        - Parses string literals (single quotes)
        - Parses numeric literals (integers, floats)
        - Handles operators (=, <, >, <=, >=, <>)
        - Ignores whitespace and comments

      hints:
        - level1: "Use a state machine or regular expressions for each token type."
        - level2: "SQL keywords are case-insensitive. Normalize to uppercase."
        - level3: |
            Token types to implement:
            - KEYWORD: SELECT, FROM, WHERE, INSERT, UPDATE, DELETE, CREATE, etc.
            - IDENTIFIER: table/column names
            - STRING: 'hello'
            - NUMBER: 123, 3.14
            - OPERATOR: =, <>, <, >, etc.
            - PUNCTUATION: (, ), ,, ;

      pitfalls:
        - "Not handling escaped quotes in strings ('it''s')"
        - "Case sensitivity (keywords insensitive, identifiers depend on config)"
        - "Unicode identifiers"

      concepts:
        - Lexical analysis
        - Finite state machines
        - Token representation

      estimated_hours: 3-4

    - id: sqlite-02
      name: SQL Parser (AST)
      description: |
        Build a parser that converts tokens into an Abstract Syntax Tree.

      acceptance_criteria:
        - Parses SELECT statements with columns, FROM, WHERE
        - Parses INSERT statements
        - Parses CREATE TABLE with column definitions
        - Handles expressions (arithmetic, comparison)
        - Reports syntax errors with line/column

      hints:
        - level1: "Use recursive descent parsing - one function per grammar rule."
        - level2: "For expressions, use Pratt parsing or precedence climbing for operators."
        - level3: |
            Grammar sketch:
            select := SELECT columns FROM table [WHERE expr]
            columns := * | column (, column)*
            expr := term ((AND|OR) term)*
            term := value (op value)?

      pitfalls:
        - "Left recursion in grammar (a + b + c)"
        - "Operator precedence (AND vs OR)"
        - "Not handling parentheses for grouping"

      concepts:
        - Recursive descent parsing
        - AST design
        - Operator precedence

      estimated_hours: 5-8

    - id: sqlite-03
      name: B-tree Page Format
      description: |
        Implement the on-disk B-tree page structure.

      acceptance_criteria:
        - Fixed page size (4096 bytes default)
        - Page header with cell count, free space pointer
        - Cell pointer array for variable-size cells
        - Interior nodes store keys + child page pointers
        - Leaf nodes store keys + values

      hints:
        - level1: "Each page is a node in the B-tree. Start with leaf nodes only."
        - level2: |
            Page layout:
            [Header (8-12 bytes)]
            [Cell Pointer Array (2 bytes each)]
            [Free space]
            [Cells (grow from bottom)]
        - level3: "SQLite uses B+ tree variant where all data is in leaves, interior nodes only have keys."

      pitfalls:
        - "Cell overflow (value too large for page)"
        - "Page fragmentation after deletions"
        - "Endianness when writing multi-byte integers"

      concepts:
        - B-tree structure
        - Page-based storage
        - Variable-length records

      estimated_hours: 6-10

      resources:
        - name: "SQLite File Format - B-tree Pages"
          url: "https://www.sqlite.org/fileformat2.html#b_tree_pages"

    - id: sqlite-04
      name: Table Storage
      description: |
        Store table rows in B-tree leaves with rowid as key.

      acceptance_criteria:
        - Each table has its own B-tree
        - Rowid is auto-incrementing primary key
        - Rows stored as serialized column values
        - CREATE TABLE creates new B-tree root page
        - Table schema stored in sqlite_master table

      hints:
        - level1: "Simplest: rowid -> serialized row bytes. Key is rowid, value is row."
        - level2: "sqlite_master stores schema: CREATE TABLE statements as text."
        - level3: |
            Record format (simplified):
            [Header size][Type1][Type2]...[Value1][Value2]...
            Types: NULL, INT8/16/32/64, FLOAT, BLOB, TEXT

      pitfalls:
        - "Not handling NULL values"
        - "Rowid gaps after deletes"
        - "Variable-length integer encoding"

      concepts:
        - Row storage formats
        - Schema management
        - Type serialization

      estimated_hours: 4-6

    - id: sqlite-05
      name: SELECT Execution (Table Scan)
      description: |
        Execute SELECT queries by scanning all rows.

      acceptance_criteria:
        - SELECT * FROM table returns all rows
        - SELECT col1, col2 returns specific columns
        - Rows returned in rowid order
        - Handle non-existent tables with error

      hints:
        - level1: "Iterate B-tree leaves from leftmost to rightmost."
        - level2: "Deserialize each row, project requested columns."
        - level3: "Use cursor abstraction: open, next, get_column, close."

      pitfalls:
        - "Column name case sensitivity"
        - "NULL handling in output"
        - "Memory management for large result sets"

      concepts:
        - B-tree traversal
        - Projection operator
        - Cursor pattern

      estimated_hours: 3-4

    - id: sqlite-06
      name: INSERT/UPDATE/DELETE
      description: |
        Implement data modification operations.

      acceptance_criteria:
        - INSERT INTO table VALUES (...) adds row
        - INSERT with column list
        - UPDATE table SET col=val WHERE condition
        - DELETE FROM table WHERE condition
        - Proper rowid assignment for inserts

      hints:
        - level1: "INSERT: serialize row, insert into B-tree with next rowid."
        - level2: "DELETE: find matching rows, remove from B-tree, handle rebalancing."
        - level3: "UPDATE: can be DELETE + INSERT, or in-place if size unchanged."

      pitfalls:
        - "B-tree rebalancing after delete"
        - "Updating primary key"
        - "Constraint violations"

      concepts:
        - B-tree insertion and deletion
        - Record modification
        - Constraint checking

      estimated_hours: 5-8

    - id: sqlite-07
      name: WHERE Clause and Indexes
      description: |
        Implement filtering and secondary indexes.

      acceptance_criteria:
        - WHERE with comparison operators
        - WHERE with AND/OR
        - CREATE INDEX creates secondary B-tree
        - Query uses index when beneficial
        - EXPLAIN shows query plan

      hints:
        - level1: "Full table scan with filter: scan all, test predicate, output matches."
        - level2: "Index B-tree: key=indexed column, value=rowid. Lookup, then fetch row."
        - level3: "Simple query planner: if WHERE on indexed column, use index seek."

      pitfalls:
        - "Index not used for non-equality predicates"
        - "Maintaining index on INSERT/UPDATE/DELETE"
        - "Choosing between index scan vs table scan"

      concepts:
        - Secondary indexes
        - Query planning basics
        - Filter predicates

      estimated_hours: 6-10

    - id: sqlite-08
      name: Query Planner
      description: |
        Implement cost-based query optimization.

      acceptance_criteria:
        - Estimates row counts for tables
        - Chooses between table scan and index scan
        - Handles simple JOINs
        - EXPLAIN QUERY PLAN shows chosen plan

      hints:
        - level1: "Store row count per table. Estimate selectivity of predicates."
        - level2: "Cost model: table scan = N rows, index lookup = log(N) + matching rows."
        - level3: "For JOINs, consider nested loop vs hash join. Start with nested loop."

      pitfalls:
        - "Stale statistics"
        - "Wrong cardinality estimates"
        - "Exponential plan space for many JOINs"

      concepts:
        - Cost-based optimization
        - Cardinality estimation
        - Join algorithms

      estimated_hours: 8-12

    - id: sqlite-09
      name: Transactions (BEGIN/COMMIT/ROLLBACK)
      description: |
        Implement ACID transactions.

      acceptance_criteria:
        - BEGIN starts transaction
        - COMMIT makes changes permanent
        - ROLLBACK undoes all changes since BEGIN
        - Changes not visible to other connections until commit
        - Crash recovery maintains consistency

      hints:
        - level1: "Shadow paging: copy pages before modify, commit = swap pointers."
        - level2: "Or WAL: write changes to separate log, replay on recovery."
        - level3: |
            ACID properties:
            - Atomicity: all or nothing (rollback support)
            - Consistency: constraints always valid
            - Isolation: concurrent txns don't interfere
            - Durability: committed = permanent (fsync)

      pitfalls:
        - "Partial writes (torn pages)"
        - "Lock ordering deadlocks"
        - "Long-running transactions blocking others"

      concepts:
        - ACID properties
        - Shadow paging vs WAL
        - Crash recovery

      estimated_hours: 8-12

    - id: sqlite-10
      name: WAL Mode
      description: |
        Implement Write-Ahead Logging for better concurrency.

      acceptance_criteria:
        - WAL file stores changes before applying to main DB
        - Readers don't block writers
        - Checkpointing merges WAL into main DB
        - Crash recovery replays committed WAL entries

      hints:
        - level1: "WAL append-only: each commit appends frames with changed pages."
        - level2: "Read: check WAL first (most recent), fallback to main DB."
        - level3: |
            WAL advantages over rollback journal:
            - Readers and writer concurrent
            - Faster commits (sequential writes)
            - Better crash recovery

      pitfalls:
        - "WAL growing unbounded without checkpoint"
        - "Readers pinning old WAL frames"
        - "WAL file corruption detection"

      concepts:
        - Write-ahead logging
        - MVCC basics
        - Checkpointing

      estimated_hours: 8-12

# =============================================================================
# BUILD YOUR OWN INTERPRETER (Lox - Crafting Interpreters)
# =============================================================================
build-interpreter:
  id: build-interpreter
  name: Build Your Own Interpreter (Lox)
  description: |
    Build a complete interpreter for the Lox programming language.
    Based on the excellent "Crafting Interpreters" book by Bob Nystrom.

  difficulty: expert
  estimated_hours: 40-80

  prerequisites:
    - Basic programming language concepts
    - Recursion
    - Tree data structures
    - Object-oriented programming

  languages:
    recommended: [Java, C, Rust]
    also_possible: [Python, Go, TypeScript]
    note: "Book uses Java for tree-walker, C for bytecode VM"

  resources:
    primary:
      - name: "Crafting Interpreters (Free Online Book)"
        url: "https://craftinginterpreters.com"
        type: book
        note: "THE resource. Follow it chapter by chapter."

    supplementary:
      - name: "Writing An Interpreter In Go"
        url: "https://interpreterbook.com"
        author: "Thorsten Ball"
      - name: "CodeCrafters Interpreter Challenge"
        url: "https://app.codecrafters.io/courses/interpreter/overview"
        type: interactive

    source_code:
      - name: "Crafting Interpreters GitHub"
        url: "https://github.com/munificent/craftinginterpreters"
        note: "Reference implementations and test suite"

  milestones:
    - id: lox-01
      name: Scanner (Lexer)
      description: |
        Build a scanner that converts Lox source code into tokens.
        Chapter 4 of Crafting Interpreters.

      acceptance_criteria:
        - Recognizes all Lox tokens (keywords, operators, literals)
        - Handles string and number literals
        - Tracks line numbers for error reporting
        - Ignores whitespace and comments
        - Reports lexical errors gracefully

      hints:
        - level1: "Consume characters one by one. Match against known patterns."
        - level2: |
            Token types: ( ) { } , . - + ; / * ! != = == > >= < <=
            Keywords: and class else false for fun if nil or print return super this true var while
        - level3: "Use a switch on first character, then peek ahead for two-char tokens."

      pitfalls:
        - "Confusing = (assignment) and == (equality)"
        - "Not handling unterminated strings"
        - "Newlines inside string literals"

      concepts:
        - Lexical analysis
        - Token representation
        - Error handling

      estimated_hours: 2-3

    - id: lox-02
      name: Representing Code (AST)
      description: |
        Define the abstract syntax tree classes for Lox.
        Chapter 5 of Crafting Interpreters.

      acceptance_criteria:
        - Expression classes for binary, unary, grouping, literal
        - Pretty-printer using visitor pattern
        - Statement classes for print, expression, var
        - AST can represent any valid Lox program

      hints:
        - level1: "Use the Visitor pattern for operations on AST nodes."
        - level2: "Generate classes from grammar rules. Expr -> Binary, Unary, Literal, Grouping..."
        - level3: |
            Expr types:
            - Binary: left op right (1 + 2)
            - Unary: op right (-1, !true)
            - Literal: value (42, "hello", true)
            - Grouping: expression ((1 + 2))

      pitfalls:
        - "Visitor pattern boilerplate"
        - "Mutable vs immutable AST nodes"
        - "Parent/child references creating cycles"

      concepts:
        - Abstract Syntax Trees
        - Visitor pattern
        - Expression vs Statement

      estimated_hours: 2-3

    - id: lox-03
      name: Parsing Expressions
      description: |
        Build a recursive descent parser for expressions.
        Chapter 6 of Crafting Interpreters.

      acceptance_criteria:
        - Parses arithmetic expressions with correct precedence
        - Handles parentheses for grouping
        - Parses comparison and equality
        - Reports syntax errors with context

      hints:
        - level1: "One function per precedence level. Lower precedence = higher in call stack."
        - level2: |
            Precedence (low to high):
            equality (== !=)
            comparison (< > <= >=)
            term (+ -)
            factor (* /)
            unary (! -)
            primary (literals, grouping)
        - level3: "Pratt parsing is elegant alternative. Each token has prefix/infix rules + precedence."

      pitfalls:
        - "Left recursion causes infinite loop"
        - "Forgetting to consume the closing paren"
        - "Error recovery - synchronizing after error"

      concepts:
        - Recursive descent parsing
        - Operator precedence
        - Error recovery

      estimated_hours: 3-4

    - id: lox-04
      name: Evaluating Expressions
      description: |
        Build a tree-walking interpreter to evaluate expressions.
        Chapter 7 of Crafting Interpreters.

      acceptance_criteria:
        - Evaluates arithmetic operations
        - Handles unary minus and negation
        - Implements truthiness (only nil and false are falsy)
        - String concatenation with +
        - Runtime error for type mismatches

      hints:
        - level1: "Implement Visitor that returns evaluated value. Recursively evaluate children."
        - level2: "Lox is dynamically typed. Operations check types at runtime."
        - level3: |
            Truthiness: false and nil are falsy, everything else truthy.
            Equality: nil == nil, numbers by value, strings by content.

      pitfalls:
        - "Division by zero"
        - "String + non-string"
        - "Java null vs Lox nil confusion"

      concepts:
        - Tree-walking interpretation
        - Dynamic typing
        - Runtime type checking

      estimated_hours: 2-3

    - id: lox-05
      name: Statements and State
      description: |
        Add statements, variables, and assignment.
        Chapter 8 of Crafting Interpreters.

      acceptance_criteria:
        - Print statement outputs to stdout
        - Variable declarations with var
        - Assignment expressions (a = b)
        - Expression statements
        - Environment stores variable bindings

      hints:
        - level1: "Environment is a map from variable name to value."
        - level2: "var a = 1; creates binding. a = 2; updates existing binding."
        - level3: "Assignment is an expression (returns value) but lowest precedence."

      pitfalls:
        - "Using undeclared variable"
        - "Assignment to non-variable (1 = 2)"
        - "Scoping issues (covered in later chapter)"

      concepts:
        - Statement vs Expression
        - Environment and bindings
        - Assignment as expression

      estimated_hours: 2-3

    - id: lox-06
      name: Control Flow
      description: |
        Add if, while, for, and logical operators.
        Chapter 9 of Crafting Interpreters.

      acceptance_criteria:
        - if/else with proper scoping
        - while loops
        - for loops (desugared to while)
        - Logical and/or with short-circuit evaluation

      hints:
        - level1: "for (init; cond; incr) body -> { init; while (cond) { body; incr; } }"
        - level2: "Short-circuit: a and b returns a if falsy, else b. a or b returns a if truthy, else b."
        - level3: "No 'break' in Lox. Challenge: add it (requires unwinding)."

      pitfalls:
        - "Dangling else (resolved by nearest if)"
        - "Forgetting short-circuit (evaluating both sides)"
        - "Infinite loops without timeout"

      concepts:
        - Conditional execution
        - Loop desugaring
        - Short-circuit evaluation

      estimated_hours: 2-3

    - id: lox-07
      name: Functions
      description: |
        Add function declarations and calls.
        Chapter 10 of Crafting Interpreters.

      acceptance_criteria:
        - fun keyword declares functions
        - Parameters and return values
        - Return statement exits early
        - Functions are first-class values
        - Call stack for recursion

      hints:
        - level1: "Function value stores: name, parameters, body AST, and defining environment."
        - level2: "Call: create new environment for local vars, execute body, return result."
        - level3: "Return implemented as exception/longjmp - unwinds call stack to caller."

      pitfalls:
        - "Stack overflow from infinite recursion"
        - "Not restoring environment after call"
        - "Return outside function"

      concepts:
        - First-class functions
        - Call frames
        - Return values

      estimated_hours: 3-4

    - id: lox-08
      name: Closures
      description: |
        Implement lexical scoping and closures.
        Chapter 11 of Crafting Interpreters.

      acceptance_criteria:
        - Functions capture enclosing environment
        - Nested functions work correctly
        - Closures persist after outer function returns
        - Resolving variables at compile time (semantic analysis)

      hints:
        - level1: "Closure = function + captured environment. Environment chains to enclosing scopes."
        - level2: "Resolver pass: determine which declaration each variable refers to."
        - level3: |
            Example:
            fun makeCounter() {
              var i = 0;
              fun count() {
                i = i + 1;
                return i;
              }
              return count;
            }
            var counter = makeCounter();
            counter(); // 1
            counter(); // 2

      pitfalls:
        - "Capturing variable vs capturing value"
        - "Variable resolution with shadowing"
        - "This in closures (next chapter)"

      concepts:
        - Lexical scoping
        - Closures
        - Environment chains

      estimated_hours: 4-6

    - id: lox-09
      name: Classes
      description: |
        Add class declarations, instances, and methods.
        Chapter 12 of Crafting Interpreters.

      acceptance_criteria:
        - class keyword declares classes
        - Instances created with ClassName()
        - Properties accessed/set with dot notation
        - Methods with implicit 'this'
        - Initializer method: init()

      hints:
        - level1: "Class is a map of method names to functions. Instance has class + fields map."
        - level2: "this bound when method is accessed. Method call binds this to instance."
        - level3: "init() is constructor. Called automatically on instance creation."

      pitfalls:
        - "this outside method"
        - "Returning from init()"
        - "Method vs function (this binding)"

      concepts:
        - Classes and instances
        - This binding
        - Constructors

      estimated_hours: 4-5

    - id: lox-10
      name: Inheritance
      description: |
        Add class inheritance and super calls.
        Chapter 13 of Crafting Interpreters.

      acceptance_criteria:
        - class Derived < Base syntax
        - Methods inherited from superclass
        - super.method() calls parent implementation
        - Proper method resolution order

      hints:
        - level1: "Subclass stores reference to superclass. Method lookup chains upward."
        - level2: "super is looked up in enclosing class, not dynamically."
        - level3: |
            class Doughnut {
              cook() { print "Fry"; }
            }
            class BostonCream < Doughnut {
              cook() {
                super.cook();
                print "Pipe cream";
              }
            }

      pitfalls:
        - "super outside class"
        - "Inheriting from non-class"
        - "Diamond inheritance (Lox has single inheritance)"

      concepts:
        - Single inheritance
        - Super calls
        - Method resolution

      estimated_hours: 3-4

# =============================================================================
# BUILD YOUR OWN DOCKER
# =============================================================================
build-docker:
  id: build-docker
  name: Build Your Own Docker
  description: |
    Build a container runtime that can run isolated processes using Linux primitives.
    You'll learn about namespaces, cgroups, and filesystem isolation.

  difficulty: expert
  estimated_hours: 30-50

  prerequisites:
    - Linux system administration
    - Process management (fork, exec)
    - Filesystem concepts
    - Basic networking

  languages:
    recommended: [Go, C, Rust]
    note: "Go is popular because Docker is written in Go"

  resources:
    tutorials:
      - name: "Containers from Scratch"
        url: "https://ericchiang.github.io/post/containers-from-scratch/"
        type: blog
        note: "Excellent hands-on introduction"
      - name: "Build Your Own Docker with Linux Namespaces and cgroups"
        url: "https://akashrajpurohit.com/blog/build-your-own-docker-with-linux-namespaces-cgroups-and-chroot-handson-guide/"
        type: blog
      - name: "Containers from Scratch (Video)"
        url: "https://www.youtube.com/watch?v=8fi7uSYlOdc"
        type: video
        author: "Liz Rice"

    documentation:
      - name: "Linux Namespaces man page"
        url: "https://man7.org/linux/man-pages/man7/namespaces.7.html"
      - name: "cgroups documentation"
        url: "https://www.kernel.org/doc/Documentation/cgroup-v2.txt"
      - name: "OCI Runtime Specification"
        url: "https://github.com/opencontainers/runtime-spec"

    books:
      - name: "Container Security"
        author: "Liz Rice"
        isbn: "978-1492056706"

  milestones:
    - id: docker-01
      name: Process Isolation (Namespaces)
      description: |
        Isolate a process using Linux namespaces (PID, UTS, mount).

      acceptance_criteria:
        - Process runs with its own PID namespace (sees itself as PID 1)
        - Process has its own hostname (UTS namespace)
        - Process has its own mount namespace
        - Uses clone() or unshare() syscalls

      hints:
        - level1: "Use CLONE_NEWPID, CLONE_NEWUTS, CLONE_NEWNS flags with clone()."
        - level2: "unshare command in bash: unshare --pid --uts --mount --fork /bin/bash"
        - level3: |
            Linux namespace types:
            - PID: Process IDs (isolated process tree)
            - UTS: Hostname and domain name
            - Mount: Filesystem mounts
            - Net: Network stack
            - User: User and group IDs
            - IPC: Inter-process communication
            - Cgroup: Cgroup root directory

      pitfalls:
        - "Not using CLONE_NEWPID correctly (need fork after unshare)"
        - "Proc filesystem still showing host PIDs"
        - "Permission issues (usually need root)"

      concepts:
        - Linux namespaces
        - Process isolation
        - clone() syscall

      estimated_hours: 4-6

    - id: docker-02
      name: Resource Limits (cgroups)
      description: |
        Limit container resources using cgroups (CPU, memory).

      acceptance_criteria:
        - Can limit memory usage (container OOM-killed if exceeds)
        - Can limit CPU shares/quota
        - Cgroup filesystem properly set up
        - Process added to cgroup before exec

      hints:
        - level1: "Create directory in /sys/fs/cgroup/memory/mycontainer. Write PID to tasks file."
        - level2: "Set memory.limit_in_bytes for memory limit. Use cpu.cfs_quota_us for CPU."
        - level3: |
            cgroup v2 paths:
            - /sys/fs/cgroup/mycontainer/memory.max
            - /sys/fs/cgroup/mycontainer/cpu.max
            - /sys/fs/cgroup/mycontainer/cgroup.procs

      pitfalls:
        - "cgroup v1 vs v2 (different paths and APIs)"
        - "Not cleaning up cgroups on container exit"
        - "Memory limits not accounting for kernel memory"

      concepts:
        - Control groups
        - Resource limiting
        - OOM killer

      estimated_hours: 3-4

    - id: docker-03
      name: Filesystem Isolation (chroot/pivot_root)
      description: |
        Give container its own root filesystem using chroot or pivot_root.

      acceptance_criteria:
        - Container sees only its own filesystem
        - Host filesystem is not accessible
        - Proc filesystem mounted in container
        - Works with any root filesystem (Alpine, Ubuntu mini)

      hints:
        - level1: "chroot /path/to/rootfs /bin/sh - simplest form"
        - level2: "pivot_root is more secure than chroot. Requires mount namespace."
        - level3: |
            Setup steps:
            1. Create mount namespace
            2. Mount new root somewhere
            3. pivot_root newroot putold
            4. Unmount putold
            5. Mount /proc, /sys, etc.

      pitfalls:
        - "chroot is escapable without mount namespace"
        - "Forgetting to mount /proc (breaks ps, top)"
        - "Not having required binaries in rootfs"

      concepts:
        - chroot jails
        - pivot_root
        - Root filesystem

      estimated_hours: 3-4

    - id: docker-04
      name: Layered Filesystem (OverlayFS)
      description: |
        Implement layered filesystem for efficient image storage.

      acceptance_criteria:
        - Multiple read-only layers merged into single view
        - Write layer captures all modifications
        - Copy-on-write semantics
        - Layers can be shared between containers

      hints:
        - level1: "OverlayFS: mount -t overlay overlay -o lowerdir=base,upperdir=changes,workdir=work target"
        - level2: "Multiple lower dirs: lowerdir=layer3:layer2:layer1 (later = higher priority)"
        - level3: |
            Docker layer structure:
            /var/lib/docker/overlay2/
              <layer-id>/
                diff/    # Layer contents
                lower    # Link to parent layer
                work/    # OverlayFS work directory

      pitfalls:
        - "OverlayFS doesn't support all filesystems"
        - "Renaming directories has copy-up overhead"
        - "Open file handles survive layer changes"

      concepts:
        - Union filesystems
        - Copy-on-write
        - Docker image layers

      estimated_hours: 4-6

    - id: docker-05
      name: Container Networking
      description: |
        Set up network namespace with virtual ethernet pair.

      acceptance_criteria:
        - Container has its own network stack
        - veth pair connects container to host bridge
        - Container can reach the internet (via NAT)
        - Containers can communicate with each other

      hints:
        - level1: "ip netns add container; ip link add veth0 type veth peer name veth1"
        - level2: "Move one end of veth into container namespace. Connect other to bridge."
        - level3: |
            Network setup steps:
            1. Create network namespace
            2. Create veth pair
            3. Move veth1 to container namespace
            4. Attach veth0 to docker0 bridge
            5. Configure IP addresses
            6. Set up iptables NAT rules

      pitfalls:
        - "DNS resolution (need to copy/bind mount /etc/resolv.conf)"
        - "iptables rules for masquerading"
        - "Bridge network vs host network"

      concepts:
        - Network namespaces
        - Virtual ethernet
        - Linux bridge networking

      estimated_hours: 5-8

    - id: docker-06
      name: Image Format and CLI
      description: |
        Implement OCI image format and Docker-compatible CLI.

      acceptance_criteria:
        - Pull images from Docker Hub (or local tar)
        - Parse OCI image manifest
        - Extract and layer filesystem
        - docker run equivalent command works

      hints:
        - level1: "OCI image is tarball with manifest.json pointing to layer tarballs."
        - level2: "Docker Hub API: GET /v2/<name>/manifests/<tag>"
        - level3: |
            CLI commands to implement:
            - run: Create and start container
            - exec: Run command in existing container
            - ps: List running containers
            - images: List local images

      pitfalls:
        - "Image manifest v1 vs v2"
        - "Content-addressable storage"
        - "Image and layer deduplication"

      concepts:
        - OCI specification
        - Container registry protocol
        - CLI design

      estimated_hours: 6-10

# =============================================================================
# BUILD YOUR OWN SHELL
# =============================================================================
build-shell:
  id: build-shell
  name: Build Your Own Shell
  description: |
    Build a Unix shell that can execute commands, handle pipes, redirections,
    and job control. A fundamental systems programming project.

  difficulty: advanced
  estimated_hours: 25-40

  prerequisites:
    - C programming
    - Unix process model (fork, exec)
    - File descriptors
    - Signal handling basics

  languages:
    recommended: [C, Rust, Go]
    note: "C is traditional, closest to system calls"

  resources:
    tutorials:
      - name: "Write a Shell in C"
        url: "https://brennan.io/2015/01/16/write-a-shell-in-c/"
        author: "Stephen Brennan"
        type: blog
      - name: "CodeCrafters Shell Challenge"
        url: "https://app.codecrafters.io/courses/shell/overview"
        type: interactive
      - name: "Purdue - Writing Your Own Shell (PDF)"
        url: "https://www.cs.purdue.edu/homes/grr/SystemsProgrammingBook/Book/Chapter5-WritingYourOwnShell.pdf"
        type: academic

    documentation:
      - name: "GNU Implementing a Shell"
        url: "https://www.gnu.org/software/libc/manual/html_node/Implementing-a-Shell.html"
      - name: "POSIX Shell Specification"
        url: "https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html"

  milestones:
    - id: shell-01
      name: Basic REPL and Command Execution
      description: |
        Read input, parse into command and arguments, execute with fork/exec.

      acceptance_criteria:
        - Displays prompt and reads user input
        - Parses command and arguments (space-separated)
        - Forks child process to execute command
        - Parent waits for child to complete
        - Handles command not found errors

      hints:
        - level1: "Use fgets() to read input, strtok() to split by spaces."
        - level2: "fork() returns 0 in child. In child, call execvp(cmd, args). Parent calls waitpid()."
        - level3: |
            Basic loop:
            while (1) {
              printf("> ");
              fgets(line, sizeof(line), stdin);
              char *args[] = parse(line);
              pid_t pid = fork();
              if (pid == 0) execvp(args[0], args);
              else waitpid(pid, &status, 0);
            }

      pitfalls:
        - "Forgetting to null-terminate args array for execvp"
        - "Not handling newline from fgets()"
        - "Zombie processes (not calling wait)"

      concepts:
        - REPL pattern
        - fork() and exec() family
        - Process creation and waiting

      estimated_hours: 3-4

    - id: shell-02
      name: Built-in Commands
      description: |
        Implement cd, exit, pwd, export, and other built-in commands.

      acceptance_criteria:
        - cd changes current directory
        - cd with no args goes to $HOME
        - exit terminates shell
        - pwd prints working directory
        - export sets environment variables

      hints:
        - level1: "Built-ins don't fork - they modify shell state directly."
        - level2: "Use chdir() for cd, getcwd() for pwd, setenv() for export."
        - level3: "Check if command is built-in BEFORE forking. Handle in parent process."

      pitfalls:
        - "Trying to cd in child process (only affects child)"
        - "Not expanding ~ to HOME directory"
        - "Environment variables not inherited by children"

      concepts:
        - Built-in vs external commands
        - Working directory
        - Environment variables

      estimated_hours: 2-3

    - id: shell-03
      name: I/O Redirection
      description: |
        Implement input (<), output (>), and append (>>) redirection.

      acceptance_criteria:
        - "cmd > file" redirects stdout to file
        - "cmd >> file" appends to file
        - "cmd < file" reads stdin from file
        - "cmd 2> file" redirects stderr
        - Combines work (cmd < in > out)

      hints:
        - level1: "Use open() to get file descriptor, dup2() to redirect."
        - level2: "dup2(fd, STDOUT_FILENO) makes stdout point to fd. Do this in child before exec."
        - level3: |
            In child, before exec:
            int fd = open(filename, O_WRONLY | O_CREAT | O_TRUNC, 0644);
            dup2(fd, STDOUT_FILENO);
            close(fd);
            execvp(cmd, args);

      pitfalls:
        - "Forgetting to close original fd after dup2"
        - "Wrong open() flags (O_TRUNC vs O_APPEND)"
        - "File permissions on created files"

      concepts:
        - File descriptors
        - dup2() system call
        - Unix I/O model

      estimated_hours: 2-3

    - id: shell-04
      name: Pipes
      description: |
        Implement command pipelines (cmd1 | cmd2 | cmd3).

      acceptance_criteria:
        - Two command pipeline works (ls | grep foo)
        - Multi-stage pipelines work (cat file | grep x | wc -l)
        - Proper cleanup of file descriptors
        - All pipeline stages run concurrently

      hints:
        - level1: "pipe() creates fd pair. pipe[0] for reading, pipe[1] for writing."
        - level2: "Fork for each command. Connect stdout of cmd1 to stdin of cmd2 via pipe."
        - level3: |
            For cmd1 | cmd2:
            int pipefd[2];
            pipe(pipefd);
            if (fork() == 0) {  // cmd1
              dup2(pipefd[1], STDOUT_FILENO);
              close(pipefd[0]); close(pipefd[1]);
              exec(cmd1);
            }
            if (fork() == 0) {  // cmd2
              dup2(pipefd[0], STDIN_FILENO);
              close(pipefd[0]); close(pipefd[1]);
              exec(cmd2);
            }
            close(pipefd[0]); close(pipefd[1]);
            wait(); wait();

      pitfalls:
        - "Not closing unused pipe ends (causes hang)"
        - "Parent not closing pipe fds"
        - "Order of fork/close operations"

      concepts:
        - Unix pipes
        - Process communication
        - File descriptor inheritance

      estimated_hours: 4-6

    - id: shell-05
      name: Background Jobs
      description: |
        Run commands in background with &, implement job listing.

      acceptance_criteria:
        - "cmd &" runs in background, shell returns immediately
        - jobs command lists background processes
        - Shell notifies when background job completes
        - Background jobs have job numbers [1], [2], etc.

      hints:
        - level1: "Don't waitpid() immediately for background jobs. Store PID in job list."
        - level2: "Use SIGCHLD handler to detect when background jobs finish."
        - level3: "Use waitpid(-1, &status, WNOHANG) in SIGCHLD handler to reap zombies."

      pitfalls:
        - "Zombie processes from background jobs"
        - "Race condition in SIGCHLD handler"
        - "Reaping wrong child"

      concepts:
        - Background execution
        - Asynchronous process management
        - Zombie processes

      estimated_hours: 3-4

    - id: shell-06
      name: Job Control (fg, bg, Ctrl+Z)
      description: |
        Full job control with suspend, resume, foreground/background.

      acceptance_criteria:
        - Ctrl+Z suspends foreground job
        - fg brings job to foreground
        - bg resumes suspended job in background
        - Proper terminal control group management

      hints:
        - level1: "Handle SIGTSTP (Ctrl+Z). Suspended jobs need to be resumed with SIGCONT."
        - level2: "Use setpgid() to put job in its own process group. Use tcsetpgrp() for foreground."
        - level3: |
            Job control flow:
            1. Create job, put in new process group
            2. If foreground: tcsetpgrp(terminal, job_pgid)
            3. On Ctrl+Z: shell regains terminal, job is stopped
            4. fg: tcsetpgrp to job, send SIGCONT, wait
            5. bg: send SIGCONT, don't wait

      pitfalls:
        - "Terminal control group confusion"
        - "Orphaned process groups"
        - "Signal handling race conditions"

      concepts:
        - Process groups
        - Terminal control
        - Signal handling (SIGTSTP, SIGCONT)

      estimated_hours: 5-8

# =============================================================================
# BUILD YOUR OWN GIT
# =============================================================================
build-git:
  id: build-git
  name: Build Your Own Git
  description: |
    Build a version control system that implements core Git operations.
    Understand content-addressable storage and the Git object model.

  difficulty: expert
  estimated_hours: 30-50

  prerequisites:
    - File I/O and hashing
    - Tree data structures
    - Basic compression (zlib)
    - Graph algorithms (for history)

  languages:
    recommended: [Python, Rust, Go, C]

  resources:
    tutorials:
      - name: "Write yourself a Git!"
        url: "https://wyag.thb.lt/"
        type: tutorial
        note: "Comprehensive Python implementation"
      - name: "CodeCrafters Git Challenge"
        url: "https://app.codecrafters.io/courses/git/overview"
        type: interactive
      - name: "Git Internals - freeCodeCamp"
        url: "https://www.freecodecamp.org/news/git-internals-objects-branches-create-repo/"
        type: blog

    documentation:
      - name: "Git Internals - Git Objects"
        url: "https://git-scm.com/book/en/v2/Git-Internals-Git-Objects"
      - name: "Git Internals - Packfiles"
        url: "https://git-scm.com/book/en/v2/Git-Internals-Packfiles"

  milestones:
    - id: git-01
      name: Repository Initialization
      description: |
        Implement git init - create .git directory structure.

      acceptance_criteria:
        - Creates .git directory
        - Creates .git/objects directory
        - Creates .git/refs/heads directory
        - Creates .git/HEAD file pointing to refs/heads/master
        - Can be called from any directory

      hints:
        - level1: "mkdir -p for nested directories. HEAD contains 'ref: refs/heads/master'"
        - level2: ".git/objects stores all content. .git/refs stores branch pointers."
        - level3: |
            .git structure:
            .git/
              HEAD           # ref: refs/heads/master
              objects/       # blob, tree, commit objects
              refs/
                heads/       # branch refs
                tags/        # tag refs

      concepts:
        - Git repository structure
        - References (HEAD, branches)

      estimated_hours: 1-2

    - id: git-02
      name: Object Storage (Blobs)
      description: |
        Implement hash-object and cat-file for blob objects.

      acceptance_criteria:
        - hash-object computes SHA-1 of content
        - Stores compressed object in .git/objects/xx/yyyy
        - cat-file retrieves and decompresses object
        - Object format: "blob {size}\0{content}"

      hints:
        - level1: "SHA-1 hash of 'blob {size}\\0{content}'. Store zlib-compressed."
        - level2: "Object path: first 2 chars of hash = directory, rest = filename."
        - level3: |
            def hash_object(data, type='blob'):
              header = f'{type} {len(data)}\0'.encode()
              store = header + data
              sha = hashlib.sha1(store).hexdigest()
              path = f'.git/objects/{sha[:2]}/{sha[2:]}'
              compressed = zlib.compress(store)
              write_file(path, compressed)
              return sha

      pitfalls:
        - "Forgetting null byte between header and content"
        - "Not compressing before storage"
        - "Binary vs text content handling"

      concepts:
        - Content-addressable storage
        - SHA-1 hashing
        - Zlib compression

      estimated_hours: 2-3

    - id: git-03
      name: Tree Objects
      description: |
        Implement tree objects that represent directory structure.

      acceptance_criteria:
        - Tree object stores list of (mode, name, hash) entries
        - ls-tree displays tree contents
        - write-tree creates tree from index/working directory
        - Nested trees for subdirectories

      hints:
        - level1: "Tree entries: mode (100644 for file, 40000 for dir), name, 20-byte hash."
        - level2: "Entries sorted by name. Each entry is binary: mode + space + name + null + hash."
        - level3: |
            Tree format:
            "tree {size}\0"
            {mode} {name}\0{20-byte-sha}
            {mode} {name}\0{20-byte-sha}
            ...

      pitfalls:
        - "Hash stored as binary (20 bytes), not hex (40 chars)"
        - "Sorting rules (directories vs files)"
        - "Mode formatting (no leading zeros except 100644)"

      concepts:
        - Tree data structure
        - Directory representation
        - Binary formats

      estimated_hours: 3-4

    - id: git-04
      name: Commit Objects
      description: |
        Implement commit objects with tree, parent, author, message.

      acceptance_criteria:
        - commit-tree creates commit object
        - Commit has tree hash, parent(s), author, committer, message
        - Timestamp in Unix format with timezone
        - Chained commits form history

      hints:
        - level1: "Commit points to tree. First commit has no parent."
        - level2: "Format: tree, parent (optional), author, committer, blank line, message."
        - level3: |
            Commit format:
            tree {tree-sha}
            parent {parent-sha}  # optional, can have multiple
            author {name} <{email}> {timestamp} {tz}
            committer {name} <{email}> {timestamp} {tz}

            {commit message}

      pitfalls:
        - "Timestamp format (seconds since epoch + timezone)"
        - "Handling merge commits (multiple parents)"
        - "Message can be multi-line"

      concepts:
        - Commit graph
        - Directed acyclic graph (DAG)
        - Immutable history

      estimated_hours: 2-3

    - id: git-05
      name: References and Branches
      description: |
        Implement branches as references to commits.

      acceptance_criteria:
        - branch creates new branch (ref file)
        - branch -d deletes branch
        - Branches stored in .git/refs/heads/
        - HEAD points to current branch
        - Detached HEAD when pointing to commit directly

      hints:
        - level1: "Branch = file containing commit hash. HEAD = file containing 'ref: refs/heads/xxx'."
        - level2: "update-ref command safely updates ref files."
        - level3: |
            # Create branch
            echo {commit-sha} > .git/refs/heads/{branch-name}
            # Switch branch (update HEAD)
            echo "ref: refs/heads/{branch}" > .git/HEAD

      pitfalls:
        - "Symbolic refs (HEAD) vs direct refs (branches)"
        - "Deleting branch you're on"
        - "Ref file locking for concurrent access"

      concepts:
        - Git references
        - Symbolic references
        - Branch management

      estimated_hours: 2-3

    - id: git-06
      name: Index (Staging Area)
      description: |
        Implement the staging area for preparing commits.

      acceptance_criteria:
        - add stages files to index
        - status shows staged/unstaged changes
        - Index is binary file .git/index
        - Stores (mode, sha, flags, path) per entry

      hints:
        - level1: "Index caches file info for quick status checks."
        - level2: "Binary format with header, entries, and optional extensions."
        - level3: |
            Index format (simplified):
            Header: DIRC, version (2-4), entry count
            Entries: ctime, mtime, dev, ino, mode, uid, gid,
                     size, sha, flags, path

      pitfalls:
        - "Index is binary, not text"
        - "Stat info for detecting changes"
        - "Path encoding and sorting"

      concepts:
        - Staging area concept
        - Binary file formats
        - File metadata caching

      estimated_hours: 4-6

    - id: git-07
      name: Diff Algorithm
      description: |
        Implement diff to show changes between versions.

      acceptance_criteria:
        - Shows line-by-line differences
        - Unified diff format
        - diff between working tree and index
        - diff between commits

      hints:
        - level1: "Use Myers diff algorithm or Longest Common Subsequence."
        - level2: "Unified diff: @@ -start,count +start,count @@ then - and + lines."
        - level3: |
            Myers algorithm finds shortest edit script.
            Key insight: d-path on k-diagonal.
            O((N+M)D) where D is edit distance.

      pitfalls:
        - "Binary files"
        - "Line ending differences"
        - "Large files / long diffs"

      concepts:
        - Diff algorithms (Myers, LCS)
        - Edit distance
        - Unified diff format

      estimated_hours: 4-6

    - id: git-08
      name: Merge (Three-way)
      description: |
        Implement three-way merge with conflict detection.

      acceptance_criteria:
        - Finds common ancestor (merge base)
        - Applies non-conflicting changes automatically
        - Marks conflicts with <<<< ==== >>>> markers
        - Creates merge commit with two parents

      hints:
        - level1: "Merge base = lowest common ancestor in commit graph."
        - level2: "Three-way: if only one side changed, take that. If both changed same, conflict."
        - level3: |
            For each file:
            base = file in merge-base
            ours = file in current branch
            theirs = file in other branch
            if ours == theirs: keep ours
            elif ours == base: take theirs
            elif theirs == base: take ours
            else: conflict

      pitfalls:
        - "Finding merge base in complex history"
        - "Handling renames during merge"
        - "Nested conflicts"

      concepts:
        - Three-way merge
        - Merge base calculation
        - Conflict resolution

      estimated_hours: 6-10

# =============================================================================
# BUILD YOUR OWN TEXT EDITOR
# =============================================================================
build-text-editor:
  id: build-text-editor
  name: Build Your Own Text Editor
  description: |
    Build a terminal-based text editor from scratch.
    Based on the kilo editor (~1000 lines of C).

  difficulty: advanced
  estimated_hours: 20-30

  prerequisites:
    - Terminal I/O
    - C or systems language
    - Basic data structures

  languages:
    recommended: [C, Rust, Go]

  resources:
    primary:
      - name: "Build Your Own Text Editor"
        url: "https://viewsourcecode.org/snaptoken/kilo/"
        type: tutorial
        note: "Step-by-step in 184 steps"

    reference:
      - name: "antirez/kilo source"
        url: "https://github.com/antirez/kilo"
      - name: "Hecto (Rust version)"
        url: "https://philippflenker.com/hecto/"

  milestones:
    - id: editor-01
      name: Raw Mode and Input
      description: |
        Put terminal in raw mode and read keypresses.

      acceptance_criteria:
        - Disables canonical mode (line buffering)
        - Disables echo
        - Reads individual keypresses
        - Handles Ctrl+Q to quit
        - Restores terminal state on exit

      hints:
        - level1: "Use tcgetattr/tcsetattr to modify terminal settings."
        - level2: "Disable ICANON and ECHO flags. Save original termios to restore later."
        - level3: |
            struct termios raw;
            tcgetattr(STDIN_FILENO, &raw);
            raw.c_lflag &= ~(ECHO | ICANON | ISIG | IEXTEN);
            raw.c_iflag &= ~(IXON | ICRNL | BRKINT | INPCK | ISTRIP);
            raw.c_oflag &= ~(OPOST);
            tcsetattr(STDIN_FILENO, TCSAFLUSH, &raw);

      pitfalls:
        - "Not restoring terminal on crash"
        - "Ctrl+C killing before cleanup"
        - "Different terminal emulators behaving differently"

      concepts:
        - Terminal modes (canonical vs raw)
        - termios structure
        - Signal handling for cleanup

      estimated_hours: 2-3

    - id: editor-02
      name: Screen Refresh
      description: |
        Clear screen and position cursor using escape sequences.

      acceptance_criteria:
        - Clears screen on refresh
        - Draws rows with ~ for empty lines
        - Positions cursor correctly
        - Uses escape sequences (VT100)

      hints:
        - level1: "\\x1b[2J clears screen. \\x1b[H positions cursor at top-left."
        - level2: "\\x1b[K clears line from cursor. Build screen in buffer, write once."
        - level3: |
            Common escape sequences:
            \x1b[2J    - Clear entire screen
            \x1b[H     - Move cursor to 1,1
            \x1b[{r};{c}H - Move cursor to row,col
            \x1b[K     - Clear line from cursor
            \x1b[?25l  - Hide cursor
            \x1b[?25h  - Show cursor

      pitfalls:
        - "Flickering (write small chunks vs one big write)"
        - "Off-by-one in cursor positioning (1-indexed)"
        - "Screen size detection"

      concepts:
        - VT100 escape sequences
        - Screen buffers
        - Terminal graphics

      estimated_hours: 2-3

    - id: editor-03
      name: File Viewing
      description: |
        Load and display file contents with scrolling.

      acceptance_criteria:
        - Opens file from command line argument
        - Displays file contents
        - Scrolls with arrow keys
        - Shows filename in status bar
        - Handles files larger than screen

      hints:
        - level1: "Store lines in dynamic array. Track row offset for scrolling."
        - level2: "Only render visible rows (offset to offset+screen_rows)."
        - level3: |
            typedef struct {
              char *chars;
              int size;
            } Row;
            Row *rows;
            int numrows;
            int rowoff;  // scroll offset

      pitfalls:
        - "Memory allocation for lines"
        - "Horizontal scrolling (long lines)"
        - "Tabs rendering"

      concepts:
        - File I/O
        - Dynamic arrays
        - Viewport scrolling

      estimated_hours: 3-4

    - id: editor-04
      name: Text Editing
      description: |
        Insert and delete characters, handle Enter and Backspace.

      acceptance_criteria:
        - Insert character at cursor
        - Delete with Backspace and Delete keys
        - Enter creates new line
        - Track "dirty" state (modified)

      hints:
        - level1: "Insert: shift chars right, insert at cursor. Delete: shift chars left."
        - level2: "Enter: split current line at cursor, insert new row."
        - level3: |
            void insertChar(Row *row, int at, int c) {
              row->chars = realloc(row->chars, row->size + 2);
              memmove(&row->chars[at + 1], &row->chars[at], row->size - at + 1);
              row->chars[at] = c;
              row->size++;
            }

      pitfalls:
        - "Cursor at end of line edge cases"
        - "Deleting at beginning of line (join with previous)"
        - "Memory reallocation"

      concepts:
        - Text buffer operations
        - Gap buffer or simple array
        - Change tracking

      estimated_hours: 4-6

    - id: editor-05
      name: Save and Undo
      description: |
        Save file to disk and implement undo functionality.

      acceptance_criteria:
        - Ctrl+S saves file
        - Prompts for filename if new file
        - Confirms before quit with unsaved changes
        - Basic undo with Ctrl+Z

      hints:
        - level1: "Convert rows back to string with newlines, write to file."
        - level2: "Undo: keep stack of operations. Each edit pushes inverse operation."
        - level3: |
            Undo approaches:
            1. Memento: save entire state (memory heavy)
            2. Command pattern: save operations and inverse
            3. Piece table: inherently supports undo

      pitfalls:
        - "Handling write errors"
        - "Undo across multiple edits"
        - "Memory for undo history"

      concepts:
        - File writing
        - Undo architectures
        - Command pattern

      estimated_hours: 3-4

    - id: editor-06
      name: Search
      description: |
        Implement incremental search functionality.

      acceptance_criteria:
        - Ctrl+F enters search mode
        - Highlights matches as you type
        - Enter goes to next match
        - Escape cancels search

      hints:
        - level1: "Simple strstr() for each line."
        - level2: "Incremental: search on each keypress, restore cursor on cancel."
        - level3: |
            For incremental search:
            - Save original cursor position
            - On each char: search forward from current position
            - If found: move cursor, highlight
            - On Enter: stay at match
            - On Escape: restore original position

      pitfalls:
        - "Search wrapping at end of file"
        - "Case sensitivity"
        - "Restoring state on cancel"

      concepts:
        - Text searching
        - Incremental search UX
        - State management

      estimated_hours: 2-3

    - id: editor-07
      name: Syntax Highlighting
      description: |
        Add syntax highlighting for common languages.

      acceptance_criteria:
        - Highlights keywords in different color
        - Highlights strings and comments
        - Different rules per file type
        - Multi-line comment handling

      hints:
        - level1: "For each row, track highlighting state per character."
        - level2: "State machine: normal, string, comment. Different color per state."
        - level3: |
            Highlight types:
            - HL_NORMAL
            - HL_KEYWORD1 (if, while, for)
            - HL_KEYWORD2 (int, char, void)
            - HL_STRING
            - HL_COMMENT
            - HL_MLCOMMENT
            - HL_NUMBER

      pitfalls:
        - "Multi-line strings and comments"
        - "Escape sequences in strings"
        - "Performance on large files"

      concepts:
        - Syntax highlighting algorithms
        - State machines
        - ANSI color codes

      estimated_hours: 4-6

# =============================================================================
# BUILD YOUR OWN RAY TRACER
# =============================================================================
build-raytracer:
  id: build-raytracer
  name: Build Your Own Ray Tracer
  description: |
    Build a path tracing renderer following Ray Tracing in One Weekend.
    Renders photorealistic images with reflections, refractions, and soft shadows.

  difficulty: advanced
  estimated_hours: 20-40

  prerequisites:
    - Linear algebra (vectors, matrices)
    - Basic geometry
    - Some physics (light, optics)

  languages:
    recommended: [C++, Rust, Go]
    also_possible: [Python, JavaScript]

  resources:
    primary:
      - name: "Ray Tracing in One Weekend"
        url: "https://raytracing.github.io/books/RayTracingInOneWeekend.html"
        type: book
        note: "THE definitive resource, free online"
      - name: "Ray Tracing: The Next Week"
        url: "https://raytracing.github.io/books/RayTracingTheNextWeek.html"
        type: book
      - name: "Ray Tracing: The Rest of Your Life"
        url: "https://raytracing.github.io/books/RayTracingTheRestOfYourLife.html"
        type: book

    supplementary:
      - name: "Physically Based Rendering (PBRT)"
        url: "https://pbr-book.org/"
        note: "Advanced, production-quality reference"

  milestones:
    - id: ray-01
      name: Output an Image
      description: |
        Generate a simple PPM image with gradient colors.

      acceptance_criteria:
        - Outputs valid PPM image file
        - Shows color gradient (red/green varying by position)
        - Correct image dimensions
        - Opens in image viewer

      hints:
        - level1: "PPM format: P3 header, width height, max color, then R G B values."
        - level2: "Iterate y from top to bottom, x from left to right."
        - level3: |
            PPM format:
            P3
            {width} {height}
            255
            r g b r g b r g b ...

      concepts:
        - Image file formats
        - Color representation
        - Coordinate systems

      estimated_hours: 1

    - id: ray-02
      name: Ray Class and Background
      description: |
        Define ray class and render background gradient.

      acceptance_criteria:
        - Ray has origin and direction
        - point_at_t(t) returns position along ray
        - Background gradient based on ray direction
        - Camera shoots rays through pixel centers

      hints:
        - level1: "Ray: P(t) = origin + t * direction. t=0 at origin, t=1 at origin+direction."
        - level2: "Background: lerp between white and blue based on y component of unit direction."
        - level3: |
            Color ray_color(Ray& r) {
              Vec3 unit_dir = normalize(r.direction);
              float t = 0.5 * (unit_dir.y + 1.0);
              return (1-t)*white + t*blue;
            }

      concepts:
        - Ray representation
        - Linear interpolation (lerp)
        - Camera model basics

      estimated_hours: 1-2

    - id: ray-03
      name: Sphere Intersection
      description: |
        Add a sphere and test ray-sphere intersection.

      acceptance_criteria:
        - Sphere defined by center and radius
        - Ray-sphere intersection using quadratic formula
        - Returns closest positive hit
        - Sphere visible in rendered image

      hints:
        - level1: "Sphere equation: |P - C| = r. Substitute ray equation, solve quadratic."
        - level2: "at + bt + c = 0 where a=dd, b=2*d(o-c), c=(o-c)(o-c)-r"
        - level3: |
            discriminant = b*b - 4*a*c
            if (discriminant < 0) return false;
            t = (-b - sqrt(discriminant)) / (2*a);
            if (t < 0) t = (-b + sqrt(discriminant)) / (2*a);

      pitfalls:
        - "Choosing correct root (closest positive)"
        - "Numeric precision issues"
        - "Sphere behind camera"

      concepts:
        - Ray-sphere intersection
        - Quadratic formula
        - Hit detection

      estimated_hours: 1-2

    - id: ray-04
      name: Surface Normals and Multiple Objects
      description: |
        Compute normals and support multiple objects with closest hit.

      acceptance_criteria:
        - Normal at hit point computed correctly
        - Normals visualized as colors
        - Multiple spheres in scene
        - Finds closest intersection

      hints:
        - level1: "Normal at hit point on sphere: (hit_point - center) / radius"
        - level2: "Hittable interface with hit() method. Scene is list of hittables."
        - level3: "Track t_min, t_max. Update t_max when hit found to find closest."

      concepts:
        - Surface normals
        - Object-oriented design
        - Closest hit algorithm

      estimated_hours: 2-3

    - id: ray-05
      name: Antialiasing
      description: |
        Add antialiasing by shooting multiple rays per pixel.

      acceptance_criteria:
        - Multiple samples per pixel
        - Random offset within pixel
        - Average color of all samples
        - Smooth edges on spheres

      hints:
        - level1: "For each pixel, shoot N rays with random offset. Average results."
        - level2: "Offset: (x + random()) / width instead of x / width"
        - level3: |
            for (int s = 0; s < samples_per_pixel; s++) {
              float u = (x + random_float()) / (width - 1);
              float v = (y + random_float()) / (height - 1);
              color += ray_color(camera.get_ray(u, v));
            }
            color /= samples_per_pixel;

      concepts:
        - Antialiasing
        - Monte Carlo sampling
        - Random number generation

      estimated_hours: 1-2

    - id: ray-06
      name: Diffuse Materials
      description: |
        Implement Lambertian (diffuse) material.

      acceptance_criteria:
        - Scattered ray goes in random hemisphere direction
        - Color attenuates with each bounce
        - Maximum bounce depth (recursion limit)
        - Soft shadows from diffuse surfaces

      hints:
        - level1: "On hit, spawn new ray in random direction in hemisphere around normal."
        - level2: "Lambertian: direction = normal + random_unit_vector()"
        - level3: |
            Color ray_color(Ray& r, int depth) {
              if (depth <= 0) return black;
              if (world.hit(r, hit_record)) {
                Vec3 target = hit_point + normal + random_unit_vector();
                return 0.5 * ray_color(Ray(hit_point, target - hit_point), depth-1);
              }
              return background;
            }

      pitfalls:
        - "Infinite recursion without depth limit"
        - "Shadow acne (self-intersection)"
        - "Correct hemisphere sampling"

      concepts:
        - Lambertian reflection
        - Recursive ray tracing
        - Material scattering

      estimated_hours: 2-3

    - id: ray-07
      name: Metal and Reflections
      description: |
        Implement metallic (reflective) materials with fuzz.

      acceptance_criteria:
        - Perfect reflections (mirror)
        - Fuzzy reflections with adjustable roughness
        - Reflection formula: r = d - 2*(dn)*n
        - Material assigned per object

      hints:
        - level1: "Reflect direction around normal. Incident = incoming ray direction."
        - level2: "Fuzz: add random vector scaled by fuzziness to reflected direction."
        - level3: |
            Vec3 reflect(Vec3& v, Vec3& n) {
              return v - 2*dot(v, n)*n;
            }
            Vec3 scattered = reflect(ray.dir, normal) + fuzz*random_in_unit_sphere();

      concepts:
        - Specular reflection
        - Roughness/fuzz
        - Material system

      estimated_hours: 2-3

    - id: ray-08
      name: Dielectrics (Glass)
      description: |
        Implement glass with refraction and Fresnel effects.

      acceptance_criteria:
        - Snell's law for refraction
        - Total internal reflection at critical angle
        - Schlick approximation for Fresnel
        - Realistic glass appearance

      hints:
        - level1: "Snell's law: n1*sin(1) = n2*sin(2). Glass IOR  1.5"
        - level2: "Total internal reflection when sin(2) > 1. Just reflect."
        - level3: |
            Schlick approximation for reflectance:
            r0 = ((n1-n2)/(n1+n2))^2
            reflectance = r0 + (1-r0)*(1-cos(theta))^5

      pitfalls:
        - "Getting the IOR ratio right (inside vs outside)"
        - "Hollow glass spheres (negative radius trick)"
        - "Total internal reflection check"

      concepts:
        - Refraction (Snell's law)
        - Total internal reflection
        - Fresnel equations

      estimated_hours: 3-4

    - id: ray-09
      name: Positionable Camera
      description: |
        Implement camera with position, look-at, and field of view.

      acceptance_criteria:
        - Camera positioned anywhere in scene
        - Points at specified target
        - Adjustable field of view
        - Up vector for camera orientation

      hints:
        - level1: "Camera basis vectors: w (back), u (right), v (up)."
        - level2: "vfov = vertical field of view. viewport_height = 2 * tan(vfov/2)"
        - level3: |
            w = normalize(lookfrom - lookat);
            u = normalize(cross(vup, w));
            v = cross(w, u);
            horizontal = viewport_width * u;
            vertical = viewport_height * v;
            lower_left = origin - horizontal/2 - vertical/2 - w;

      concepts:
        - Camera coordinate system
        - Field of view
        - Look-at transformation

      estimated_hours: 2-3

    - id: ray-10
      name: Depth of Field
      description: |
        Add defocus blur (depth of field) effect.

      acceptance_criteria:
        - Adjustable aperture size
        - Focus distance setting
        - Objects at focus distance are sharp
        - Objects closer/farther are blurred

      hints:
        - level1: "Random origin on lens disk, ray still goes through focus point."
        - level2: "Larger aperture = more blur. Aperture = 0 = pinhole camera."
        - level3: |
            Vec3 rd = lens_radius * random_in_unit_disk();
            Vec3 offset = u * rd.x + v * rd.y;
            return Ray(origin + offset, lower_left + s*horizontal + t*vertical - origin - offset);

      concepts:
        - Thin lens model
        - Depth of field
        - Aperture and focus

      estimated_hours: 2-3

# =============================================================================
# BUILD YOUR OWN OPERATING SYSTEM
# =============================================================================
build-os:
  id: build-os
  name: Build Your Own Operating System
  description: |
    Build an operating system kernel from scratch. One of the most challenging
    and rewarding projects in computer science.

  difficulty: expert
  estimated_hours: 100-200

  prerequisites:
    - x86 assembly language
    - C programming (low-level)
    - Computer architecture
    - Data structures

  languages:
    required: [C, Assembly]
    note: "Rust also popular (see os.phil-opp.com)"

  resources:
    primary:
      - name: "OSDev Wiki"
        url: "https://wiki.osdev.org"
        type: wiki
        note: "THE resource for OS development"
      - name: "Writing an OS in Rust"
        url: "https://os.phil-opp.com/"
        type: tutorial
        note: "Excellent modern tutorial"
      - name: "os-tutorial (GitHub)"
        url: "https://github.com/cfenollosa/os-tutorial"
        type: tutorial
      - name: "The Little OS Book"
        url: "https://littleosbook.github.io/"
        type: book

    books:
      - name: "Operating Systems: Three Easy Pieces"
        url: "https://pages.cs.wisc.edu/~remzi/OSTEP/"
        note: "Free, excellent theory"
      - name: "xv6: A simple Unix-like teaching OS"
        url: "https://pdos.csail.mit.edu/6.828/2012/xv6.html"
        note: "MIT's teaching OS"

  milestones:
    - id: os-01
      name: Bootloader
      description: |
        Write a bootloader that loads your kernel into memory.

      acceptance_criteria:
        - BIOS loads bootloader from first sector
        - Bootloader prints message to screen
        - Loads kernel from disk to memory
        - Jumps to kernel entry point

      hints:
        - level1: "MBR is 512 bytes at sector 0. BIOS loads to 0x7C00."
        - level2: "Use BIOS int 0x13 to read more sectors. Print with int 0x10."
        - level3: |
            ; bootloader.asm
            [org 0x7c00]
            mov si, msg
            call print
            jmp $
            print: ...
            msg: db "Hello from bootloader", 0
            times 510-($-$$) db 0
            dw 0xAA55  ; boot signature

      pitfalls:
        - "Boot signature must be 0xAA55 at bytes 510-511"
        - "Real mode limitations (16-bit, 1MB address space)"
        - "Use GRUB/Multiboot instead for real projects"

      concepts:
        - BIOS boot process
        - Real mode x86
        - Disk I/O

      estimated_hours: 5-10

    - id: os-02
      name: Protected Mode and GDT
      description: |
        Switch CPU from real mode to 32-bit protected mode.

      acceptance_criteria:
        - Sets up Global Descriptor Table
        - Switches to 32-bit protected mode
        - Can access full 4GB address space
        - Kernel runs in protected mode

      hints:
        - level1: "GDT defines memory segments. Need code and data segments."
        - level2: "Disable interrupts, load GDT, set CR0.PE=1, far jump to flush pipeline."
        - level3: |
            ; Switch to protected mode
            cli
            lgdt [gdt_descriptor]
            mov eax, cr0
            or eax, 1
            mov cr0, eax
            jmp CODE_SEG:protected_mode_start

      pitfalls:
        - "Far jump required after switching modes"
        - "Segment registers must be reloaded"
        - "Interrupts must be disabled during switch"

      concepts:
        - CPU protection rings
        - Memory segmentation
        - Global Descriptor Table

      estimated_hours: 5-10

    - id: os-03
      name: Interrupts and Exceptions
      description: |
        Set up Interrupt Descriptor Table and handle exceptions.

      acceptance_criteria:
        - IDT with 256 entries
        - Handles divide-by-zero exception
        - Handles page fault exception
        - Handles keyboard interrupt
        - Programs PIC for hardware interrupts

      hints:
        - level1: "IDT maps interrupt number to handler address."
        - level2: "Need ISR stubs in assembly to save state, call C handler, restore state."
        - level3: |
            IDT entry format:
            - offset_low (16 bits)
            - selector (16 bits)
            - zero (8 bits)
            - type_attr (8 bits)
            - offset_high (16 bits)

      pitfalls:
        - "Must remap PIC (default conflicts with exceptions)"
        - "Stack alignment for C handlers"
        - "Acknowledging interrupts (EOI)"

      concepts:
        - Interrupt handling
        - Exception types
        - PIC programming

      estimated_hours: 8-12

    - id: os-04
      name: Physical Memory Manager
      description: |
        Implement allocation of physical page frames.

      acceptance_criteria:
        - Detects available memory regions
        - Bitmap or free list of page frames
        - alloc_frame() returns physical address
        - free_frame() returns frame to pool
        - Handles memory below and above 1MB

      hints:
        - level1: "Get memory map from BIOS (E820) or Multiboot info."
        - level2: "Bitmap: 1 bit per 4KB page. 4GB = 1M pages = 128KB bitmap."
        - level3: |
            Simple bitmap allocator:
            uint8_t bitmap[NUM_PAGES / 8];
            void* alloc_frame() {
              for each bit in bitmap:
                if bit is 0: set to 1, return address
            }

      pitfalls:
        - "Kernel code/data in memory map"
        - "Memory holes (ISA, PCI)"
        - "Allocator data structures in physical memory"

      concepts:
        - Physical memory layout
        - Bitmap allocator
        - Page frame allocation

      estimated_hours: 6-10

    - id: os-05
      name: Virtual Memory and Paging
      description: |
        Implement paging for virtual memory.

      acceptance_criteria:
        - Creates page directory and page tables
        - Maps kernel to higher half (e.g., 0xC0000000)
        - Enables paging (CR0.PG)
        - Page fault handler allocates pages

      hints:
        - level1: "Page directory has 1024 entries, each points to page table."
        - level2: "Page table has 1024 entries, each maps 4KB page."
        - level3: |
            // Map virtual to physical
            pd_index = virt >> 22;
            pt_index = (virt >> 12) & 0x3FF;
            page_directory[pd_index] = page_table | 0x3;
            page_table[pt_index] = phys | 0x3;

      pitfalls:
        - "Identity mapping during transition"
        - "TLB invalidation (invlpg)"
        - "Page table self-mapping for easy access"

      concepts:
        - Two-level paging
        - Virtual address translation
        - Higher half kernel

      estimated_hours: 10-15

    - id: os-06
      name: Kernel Heap
      description: |
        Implement kmalloc and kfree for kernel dynamic allocation.

      acceptance_criteria:
        - kmalloc(size) returns kernel memory
        - kfree(ptr) releases memory
        - Handles various allocation sizes
        - Grows heap as needed

      hints:
        - level1: "Start simple: bump allocator (no free)."
        - level2: "Linked list of free blocks. First-fit or best-fit."
        - level3: |
            Block header:
            struct block {
              size_t size;
              bool free;
              struct block* next;
            };

      pitfalls:
        - "Fragmentation"
        - "Alignment requirements"
        - "Expanding heap (more virtual pages)"

      concepts:
        - Heap management
        - Free list allocator
        - Memory fragmentation

      estimated_hours: 5-8

    - id: os-07
      name: Process and Scheduler
      description: |
        Implement processes and round-robin scheduling.

      acceptance_criteria:
        - Process Control Block structure
        - Context switching between processes
        - Round-robin scheduler
        - Timer-driven preemption

      hints:
        - level1: "PCB holds: state, registers, page directory, stack pointer."
        - level2: "Context switch: save current registers, load next registers."
        - level3: |
            void schedule() {
              save_context(current);
              current = pick_next_process();
              load_context(current);
              switch_page_directory(current->cr3);
            }

      pitfalls:
        - "Saving/restoring all registers"
        - "Stack switching"
        - "First process bootstrap"

      concepts:
        - Process model
        - Context switching
        - Scheduling algorithms

      estimated_hours: 12-20

    - id: os-08
      name: System Calls
      description: |
        Implement system call interface for user programs.

      acceptance_criteria:
        - System call via interrupt (e.g., int 0x80)
        - Passes arguments in registers
        - Kernel validates user pointers
        - Returns result to user

      hints:
        - level1: "Int 0x80 traps to kernel. Syscall number in eax."
        - level2: "Lookup syscall number in table, call handler."
        - level3: |
            void syscall_handler(regs) {
              int num = regs->eax;
              int ret = syscall_table[num](regs->ebx, regs->ecx, ...);
              regs->eax = ret;
            }

      pitfalls:
        - "User/kernel mode transition"
        - "Validating user pointers"
        - "Reentrancy issues"

      concepts:
        - System call interface
        - User/kernel boundary
        - Privilege levels

      estimated_hours: 6-10

# =============================================================================
# BUILD YOUR OWN NEURAL NETWORK FRAMEWORK (micrograd)
# =============================================================================
build-nn-framework:
  id: build-nn-framework
  name: Build Your Own Neural Network Framework
  description: |
    Build an autograd engine and neural network library from scratch.
    Based on Karpathy's micrograd - the simplest way to understand deep learning.

  difficulty: advanced
  estimated_hours: 15-25

  prerequisites:
    - Calculus (derivatives, chain rule)
    - Basic linear algebra
    - Python programming
    - Understanding of neural networks

  languages:
    recommended: [Python]
    also_possible: [Rust, Go, JavaScript, C++]

  resources:
    primary:
      - name: "micrograd (Karpathy)"
        url: "https://github.com/karpathy/micrograd"
        type: code
        note: "94 lines that teach neural networks"
      - name: "Neural Networks: Zero to Hero (Video)"
        url: "https://www.youtube.com/watch?v=VMj-3S1tku0"
        author: "Andrej Karpathy"
        type: video
        note: "2.5 hour walkthrough building micrograd"

    supplementary:
      - name: "PyTorch Autograd Tutorial"
        url: "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
      - name: "Automatic Differentiation Wikipedia"
        url: "https://en.wikipedia.org/wiki/Automatic_differentiation"

  milestones:
    - id: nn-01
      name: Value Class with Operations
      description: |
        Create Value class that tracks operations for backpropagation.

      acceptance_criteria:
        - Value holds a scalar number
        - Supports +, -, *, / operations
        - Operations return new Value objects
        - Tracks children (inputs) and operation

      hints:
        - level1: "Value has data, grad, _children, _op attributes."
        - level2: "__add__, __mul__, etc. return new Value with self and other as children."
        - level3: |
            class Value:
              def __init__(self, data, _children=(), _op=''):
                self.data = data
                self.grad = 0.0
                self._children = set(_children)
                self._op = _op

              def __add__(self, other):
                return Value(self.data + other.data, (self, other), '+')

      concepts:
        - Computational graph
        - Operator overloading
        - Value tracking

      estimated_hours: 2-3

    - id: nn-02
      name: Backward Pass (Autograd)
      description: |
        Implement backward() to compute gradients using chain rule.

      acceptance_criteria:
        - Each operation stores its gradient function (_backward)
        - backward() traverses graph in topological order
        - Computes gradients for all leaf Values
        - Correctly handles multiple uses of same Value

      hints:
        - level1: "Each operation has local gradient. Chain rule multiplies them."
        - level2: "Topological sort ensures children processed before parents."
        - level3: |
            def __add__(self, other):
              out = Value(self.data + other.data, (self, other), '+')
              def _backward():
                self.grad += 1.0 * out.grad  # d(a+b)/da = 1
                other.grad += 1.0 * out.grad  # d(a+b)/db = 1
              out._backward = _backward
              return out

            def backward(self):
              topo = []
              visited = set()
              def build_topo(v):
                if v not in visited:
                  visited.add(v)
                  for child in v._children:
                    build_topo(child)
                  topo.append(v)
              build_topo(self)
              self.grad = 1.0
              for v in reversed(topo):
                v._backward()

      pitfalls:
        - "grad += not grad = (accumulate for reused values)"
        - "Topological order matters"
        - "Initialize output.grad = 1.0"

      concepts:
        - Backpropagation
        - Chain rule
        - Topological sort

      estimated_hours: 3-4

    - id: nn-03
      name: Activation Functions
      description: |
        Implement ReLU, tanh, exp, and other activation functions.

      acceptance_criteria:
        - tanh() activation with correct gradient
        - relu() activation (max(0, x))
        - exp() for softmax later
        - Each has correct _backward

      hints:
        - level1: "tanh(x) gradient: 1 - tanh(x)^2"
        - level2: "relu(x): x if x > 0 else 0. Gradient: 1 if x > 0 else 0."
        - level3: |
            def tanh(self):
              t = math.tanh(self.data)
              out = Value(t, (self,), 'tanh')
              def _backward():
                self.grad += (1 - t**2) * out.grad
              out._backward = _backward
              return out

      concepts:
        - Activation functions
        - Non-linearity
        - Gradient of common functions

      estimated_hours: 1-2

    - id: nn-04
      name: Neuron and Layer Classes
      description: |
        Build Neuron and Layer classes for neural network structure.

      acceptance_criteria:
        - Neuron has weights, bias, and activation
        - Layer contains multiple neurons
        - MLP (Multi-Layer Perceptron) chains layers
        - Forward pass works through network

      hints:
        - level1: "Neuron: sum(w*x) + b, then activation."
        - level2: "Layer: list of neurons, each produces one output."
        - level3: |
            class Neuron:
              def __init__(self, nin):
                self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]
                self.b = Value(0)

              def __call__(self, x):
                act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)
                return act.tanh()

            class Layer:
              def __init__(self, nin, nout):
                self.neurons = [Neuron(nin) for _ in range(nout)]

              def __call__(self, x):
                return [n(x) for n in self.neurons]

      concepts:
        - Neural network architecture
        - Forward propagation
        - Parameters

      estimated_hours: 2-3

    - id: nn-05
      name: Loss Functions
      description: |
        Implement mean squared error and cross-entropy loss.

      acceptance_criteria:
        - MSE loss for regression
        - Sum of squared errors
        - Loss is a single Value
        - Gradient flows back through loss

      hints:
        - level1: "MSE = mean((pred - target)^2)"
        - level2: "With Value class: sum((p - t)**2 for p,t in zip(pred, target))"
        - level3: |
            def mse_loss(predictions, targets):
              loss = sum((p - t)**2 for p, t in zip(predictions, targets))
              return loss

      concepts:
        - Loss functions
        - Regression vs classification
        - Gradient of loss

      estimated_hours: 1-2

    - id: nn-06
      name: Training Loop
      description: |
        Implement gradient descent optimization to train networks.

      acceptance_criteria:
        - Forward pass computes predictions
        - Backward pass computes gradients
        - Update step adjusts parameters
        - Zero gradients before each iteration

      hints:
        - level1: "1. Forward, 2. Loss, 3. Zero grads, 4. Backward, 5. Update params."
        - level2: "Update: param.data -= learning_rate * param.grad"
        - level3: |
            for epoch in range(epochs):
              # Forward
              ypred = [n(x) for x in X]
              loss = mse_loss(ypred, y)

              # Backward
              for p in model.parameters():
                p.grad = 0
              loss.backward()

              # Update
              for p in model.parameters():
                p.data -= learning_rate * p.grad

      pitfalls:
        - "Not zeroing gradients (accumulates from prev iteration)"
        - "Learning rate too high/low"
        - "Not calling backward() after computing loss"

      concepts:
        - Gradient descent
        - Training loop
        - Learning rate

      estimated_hours: 2-3

# =============================================================================
# BUILD YOUR OWN GARBAGE COLLECTOR
# =============================================================================
build-gc:
  id: build-gc
  name: Build Your Own Garbage Collector
  description: |
    Implement automatic memory management with mark-sweep collection.
    Understand how languages like Python, Java, Go manage memory.

  difficulty: advanced
  estimated_hours: 15-25

  prerequisites:
    - Memory management concepts
    - Pointers and manual memory
    - Graph traversal algorithms
    - C or systems language

  languages:
    recommended: [C, C++, Rust]

  resources:
    tutorials:
      - name: "Baby's First Garbage Collector"
        url: "https://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/"
        author: "Bob Nystrom"
        type: blog
        note: "Excellent introduction"
      - name: "Writing a Mark-Sweep GC"
        url: "https://dmitrysoshnikov.com/compilers/writing-a-mark-sweep-garbage-collector/"
        type: blog
      - name: "Crafting Interpreters - Garbage Collection"
        url: "https://craftinginterpreters.com/garbage-collection.html"
        type: book_chapter

    books:
      - name: "The Garbage Collection Handbook"
        author: "Jones, Hosking, Moss"
        note: "Comprehensive reference"

  milestones:
    - id: gc-01
      name: Object Representation
      description: |
        Define how objects are represented with GC metadata.

      acceptance_criteria:
        - Object header with type info
        - Mark bit for GC
        - Objects allocated on managed heap
        - Object can reference other objects

      hints:
        - level1: "Each object has header with mark bit and type tag."
        - level2: "All objects in linked list for easy iteration during sweep."
        - level3: |
            typedef struct Object {
              unsigned char marked;
              ObjectType type;
              struct Object* next;  // linked list of all objects
              union {
                int intValue;
                struct { Object* head; Object* tail; } pair;
              };
            } Object;

      concepts:
        - Object headers
        - Type tags
        - Managed heap

      estimated_hours: 2-3

    - id: gc-02
      name: Root Set and Stack Scanning
      description: |
        Identify GC roots (stack, globals) that keep objects alive.

      acceptance_criteria:
        - Tracks stack frames
        - Identifies root pointers on stack
        - Global variables as roots
        - VM registers as roots (if applicable)

      hints:
        - level1: "Roots = all pointers the program can directly access."
        - level2: "For simple VM: stack array is the roots."
        - level3: |
            typedef struct VM {
              Object* stack[STACK_MAX];
              int stackSize;
              Object* firstObject;  // linked list head
            } VM;

            // Roots are stack[0..stackSize-1]

      pitfalls:
        - "Missing roots = premature collection"
        - "Conservative vs precise stack scanning"
        - "Interior pointers"

      concepts:
        - GC roots
        - Stack scanning
        - Reachability

      estimated_hours: 2-3

    - id: gc-03
      name: Mark Phase
      description: |
        Traverse object graph and mark all reachable objects.

      acceptance_criteria:
        - Starts from all roots
        - Marks each visited object
        - Follows all references (DFS/BFS)
        - Handles cycles (don't revisit marked)

      hints:
        - level1: "DFS: recursively mark object, then mark its children."
        - level2: "Check marked bit to avoid infinite loops on cycles."
        - level3: |
            void mark(Object* obj) {
              if (obj == NULL || obj->marked) return;
              obj->marked = 1;

              if (obj->type == OBJ_PAIR) {
                mark(obj->pair.head);
                mark(obj->pair.tail);
              }
            }

            void markAll(VM* vm) {
              for (int i = 0; i < vm->stackSize; i++) {
                mark(vm->stack[i]);
              }
            }

      pitfalls:
        - "Stack overflow on deep object graphs"
        - "Forgetting to check marked bit"
        - "Missing some reference types"

      concepts:
        - Graph traversal
        - Marking algorithm
        - Cycle handling

      estimated_hours: 2-3

    - id: gc-04
      name: Sweep Phase
      description: |
        Free all unmarked objects and reset marks.

      acceptance_criteria:
        - Iterates all allocated objects
        - Frees unmarked objects
        - Resets mark bit on surviving objects
        - Updates object linked list

      hints:
        - level1: "Walk linked list, free unmarked, keep marked."
        - level2: "Need pointer to previous to remove from list."
        - level3: |
            void sweep(VM* vm) {
              Object** obj = &vm->firstObject;
              while (*obj) {
                if (!(*obj)->marked) {
                  Object* garbage = *obj;
                  *obj = garbage->next;
                  free(garbage);
                } else {
                  (*obj)->marked = 0;  // reset for next GC
                  obj = &(*obj)->next;
                }
              }
            }

      pitfalls:
        - "Not resetting mark bits (everything collected next time)"
        - "Linked list corruption"
        - "Double free"

      concepts:
        - Sweeping algorithm
        - Memory reclamation
        - Object lifecycle

      estimated_hours: 2-3

    - id: gc-05
      name: GC Triggering
      description: |
        Decide when to run garbage collection.

      acceptance_criteria:
        - GC triggered when allocation threshold reached
        - Threshold grows with live heap size
        - Can also trigger manually
        - Tracks allocation count/bytes

      hints:
        - level1: "Simple: GC every N allocations."
        - level2: "Better: GC when numObjects > threshold. Double threshold after GC."
        - level3: |
            void* allocate(VM* vm, size_t size) {
              if (vm->numObjects >= vm->maxObjects) {
                gc(vm);
                vm->maxObjects = vm->numObjects * 2;
              }
              // ... allocate and return
            }

      pitfalls:
        - "Too frequent GC (slow)"
        - "Too rare GC (out of memory)"
        - "GC during GC"

      concepts:
        - GC scheduling
        - Heap growth policy
        - Allocation pacing

      estimated_hours: 2-3

    - id: gc-06
      name: Generational GC (Optional)
      description: |
        Implement generational collection for better performance.

      acceptance_criteria:
        - Young and old generation
        - Minor GC collects young only
        - Objects promoted after surviving GC
        - Write barrier tracks old->young pointers

      hints:
        - level1: "Most objects die young. Collect young generation frequently."
        - level2: "Remember set: old objects pointing to young."
        - level3: |
            Generational hypothesis:
            - Most objects die young
            - Old objects rarely point to young objects
            - Minor GC: only scan young gen + remembered set
            - Major GC: scan everything

      pitfalls:
        - "Write barrier overhead"
        - "Premature promotion"
        - "Remembered set overflow"

      concepts:
        - Generational hypothesis
        - Minor vs major GC
        - Write barriers

      estimated_hours: 4-6

# =============================================================================
# BUILD YOUR OWN GAME ENGINE
# =============================================================================
build-game-engine:
  id: build-game-engine
  name: Build Your Own Game Engine
  description: |
    Build a 2D game engine with ECS architecture, rendering, physics, and audio.
    Create reusable infrastructure for building games.

  difficulty: expert
  estimated_hours: 60-100

  prerequisites:
    - Graphics programming basics
    - Linear algebra
    - C++ or similar language
    - Basic physics concepts

  languages:
    recommended: [C++, Rust]
    also_possible: [C, Go]

  resources:
    courses:
      - name: "Pikuma - 2D Game Engine in C++"
        url: "https://pikuma.com/courses/cpp-2d-game-engine-development"
        type: course

    tutorials:
      - name: "Game Programming Patterns"
        url: "https://gameprogrammingpatterns.com/"
        author: "Bob Nystrom"
        type: book
      - name: "Lazy Foo SDL Tutorials"
        url: "https://lazyfoo.net/tutorials/SDL/"
        type: tutorial

    books:
      - name: "Game Engine Architecture"
        author: "Jason Gregory"
        note: "Industry standard reference"

  milestones:
    - id: engine-01
      name: Window and Rendering Loop
      description: |
        Create window, handle events, implement game loop.

      acceptance_criteria:
        - Opens window with SDL/GLFW
        - Handles quit event
        - Fixed timestep game loop
        - Clears and presents frame

      hints:
        - level1: "SDL_Init, SDL_CreateWindow, SDL_CreateRenderer."
        - level2: "Game loop: process events, update, render, cap FPS."
        - level3: |
            while (running) {
              uint32_t frameStart = SDL_GetTicks();

              while (SDL_PollEvent(&event)) {
                if (event.type == SDL_QUIT) running = false;
              }

              update(deltaTime);
              render();

              uint32_t frameTime = SDL_GetTicks() - frameStart;
              if (frameTime < FRAME_DELAY) {
                SDL_Delay(FRAME_DELAY - frameTime);
              }
            }

      concepts:
        - Game loop patterns
        - Fixed vs variable timestep
        - Event handling

      estimated_hours: 3-4

    - id: engine-02
      name: Sprite Rendering
      description: |
        Load textures and render sprites with transformations.

      acceptance_criteria:
        - Loads PNG/JPG textures
        - Renders sprites at position
        - Supports rotation and scale
        - Texture atlas support

      hints:
        - level1: "SDL_image to load textures. SDL_RenderCopy to draw."
        - level2: "SDL_RenderCopyEx for rotation. SrcRect for atlas regions."
        - level3: |
            struct Sprite {
              SDL_Texture* texture;
              SDL_Rect srcRect;  // region in texture
              int width, height;
            };

            void render(Sprite& sprite, Vector2 pos, float rotation) {
              SDL_Rect dst = {pos.x, pos.y, sprite.width, sprite.height};
              SDL_RenderCopyEx(renderer, sprite.texture, &sprite.srcRect, &dst,
                               rotation, NULL, SDL_FLIP_NONE);
            }

      concepts:
        - Texture loading
        - Sprite rendering
        - Texture atlases

      estimated_hours: 3-4

    - id: engine-03
      name: Entity Component System
      description: |
        Implement ECS architecture for game objects.

      acceptance_criteria:
        - Entities are just IDs
        - Components are pure data
        - Systems process entities with specific components
        - Efficient component storage (arrays per type)

      hints:
        - level1: "Entity = uint32_t ID. Components stored in separate arrays."
        - level2: "Signature = bitset of which components entity has."
        - level3: |
            using Entity = uint32_t;

            struct TransformComponent {
              Vector2 position;
              float rotation;
              float scale;
            };

            struct SpriteComponent {
              Texture* texture;
              SDL_Rect srcRect;
            };

            class Registry {
              std::unordered_map<Entity, TransformComponent> transforms;
              std::unordered_map<Entity, SpriteComponent> sprites;
              // ...
            };

      pitfalls:
        - "Over-engineering ECS"
        - "Too many tiny components"
        - "Cache unfriendly access patterns"

      concepts:
        - Entity Component System
        - Data-oriented design
        - Composition over inheritance

      estimated_hours: 6-10

    - id: engine-04
      name: Input System
      description: |
        Handle keyboard, mouse, and gamepad input.

      acceptance_criteria:
        - Keyboard key states (pressed, held, released)
        - Mouse position and buttons
        - Input mapping (action -> keys)
        - Gamepad support

      hints:
        - level1: "SDL_GetKeyboardState for current frame. Track previous for press/release."
        - level2: "Action mapping: 'jump' -> [SPACE, GAMEPAD_A]"
        - level3: |
            class InputManager {
              const Uint8* currentKeys;
              Uint8 previousKeys[SDL_NUM_SCANCODES];

              bool isKeyPressed(SDL_Scancode key) {
                return currentKeys[key] && !previousKeys[key];
              }

              void update() {
                memcpy(previousKeys, currentKeys, SDL_NUM_SCANCODES);
                currentKeys = SDL_GetKeyboardState(NULL);
              }
            };

      concepts:
        - Input handling
        - Action mapping
        - State tracking (press vs hold)

      estimated_hours: 3-4

    - id: engine-05
      name: Physics (Collision Detection)
      description: |
        Implement basic 2D collision detection and response.

      acceptance_criteria:
        - AABB collision detection
        - Circle collision detection
        - Collision response (push out)
        - Collision layers/masks

      hints:
        - level1: "AABB: check overlap on both axes."
        - level2: "Separate axis theorem for convex polygons."
        - level3: |
            bool aabbCollision(AABB a, AABB b) {
              return a.x < b.x + b.w &&
                     a.x + a.w > b.x &&
                     a.y < b.y + b.h &&
                     a.y + a.h > b.y;
            }

            // Collision response: find minimum push vector
            Vector2 getMTV(AABB a, AABB b) {
              float overlapX = std::min(a.x + a.w, b.x + b.w) - std::max(a.x, b.x);
              float overlapY = std::min(a.y + a.h, b.y + b.h) - std::max(a.y, b.y);
              if (overlapX < overlapY) return {overlapX, 0};
              else return {0, overlapY};
            }

      concepts:
        - AABB collision
        - Collision response
        - Broad vs narrow phase

      estimated_hours: 5-8

    - id: engine-06
      name: Audio System
      description: |
        Play sound effects and background music.

      acceptance_criteria:
        - Loads WAV/MP3 files
        - Plays sound effects (fire and forget)
        - Plays background music (looping)
        - Volume control

      hints:
        - level1: "SDL_mixer for simple audio. Mix_LoadWAV, Mix_PlayChannel."
        - level2: "Music on separate channel. Sound effects on multiple channels."
        - level3: |
            class AudioManager {
              std::unordered_map<std::string, Mix_Chunk*> sounds;
              Mix_Music* currentMusic;

              void playSound(const std::string& name) {
                Mix_PlayChannel(-1, sounds[name], 0);
              }

              void playMusic(const std::string& path, bool loop) {
                currentMusic = Mix_LoadMUS(path.c_str());
                Mix_PlayMusic(currentMusic, loop ? -1 : 0);
              }
            };

      concepts:
        - Audio playback
        - Sound vs music
        - Audio channels

      estimated_hours: 3-4

    - id: engine-07
      name: Scene Management
      description: |
        Implement scenes/levels with serialization.

      acceptance_criteria:
        - Scene contains entities and their components
        - Load scene from file (JSON/YAML)
        - Save scene to file
        - Switch between scenes

      hints:
        - level1: "Scene = list of entity definitions. Each has list of components."
        - level2: "JSON: {entities: [{components: {transform: {x, y}, sprite: {...}}}]}"
        - level3: |
            // Scene file (JSON)
            {
              "entities": [
                {
                  "tag": "player",
                  "components": {
                    "transform": {"x": 100, "y": 200},
                    "sprite": {"texture": "player.png"},
                    "rigidbody": {"mass": 1.0}
                  }
                }
              ]
            }

      concepts:
        - Scene graphs
        - Serialization
        - Level loading

      estimated_hours: 4-6

# =============================================================================
# PROJECT: BUILD YOUR OWN MEMORY ALLOCATOR
# =============================================================================
# Implement malloc/free from scratch
# Based on: Doug Lea's malloc, jemalloc, mimalloc concepts

build-allocator:
  id: "build-allocator"
  name: "Build Your Own Memory Allocator"
  domain: "systems"
  difficulty: "expert"
  languages: ["c", "rust", "zig"]

  description: |
    Implement a memory allocator (malloc/free/realloc) from scratch.
    Learn about memory management, fragmentation, and allocation strategies.

  learning_outcomes:
    - Understand heap memory management
    - Learn free list and buddy system algorithms
    - Handle fragmentation and coalescing
    - Implement thread-safe allocation
    - Optimize for allocation patterns

  prerequisites:
    - C pointers and memory layout
    - System calls (brk, mmap)
    - Basic data structures (linked lists)
    - Understanding of virtual memory

  resources:
    primary:
      - name: "Memory Allocators 101"
        url: "https://arjunsreedharan.org/post/148675821737/memory-allocators-101-write-a-simple-memory"
      - name: "Writing a Memory Allocator (OSTEP)"
        url: "https://pages.cs.wisc.edu/~remzi/OSTEP/vm-freespace.pdf"
    reference:
      - name: "Doug Lea's malloc"
        url: "http://gee.cs.oswego.edu/dl/html/malloc.html"
      - name: "jemalloc paper"
        url: "https://people.freebsd.org/~jasone/jemalloc/bsdcan2006/jemalloc.pdf"
      - name: "mimalloc"
        url: "https://microsoft.github.io/mimalloc/"

  milestones:
    - id: allocator-01
      name: Bump Allocator
      description: |
        Start with the simplest allocator: bump/arena allocator.
        Memory is allocated sequentially, freed all at once.

      acceptance_criteria:
        - Allocate memory from a fixed-size buffer
        - Track allocation offset
        - Reset frees all memory at once
        - Handle alignment requirements

      hints:
        - level1: "Use a single pointer to track the 'bump' position in the buffer."
        - level2: "Alignment: round up the allocation address to the required boundary (usually 8 or 16 bytes)."
        - level3: |
            struct BumpAllocator {
              char *buffer;      // Base of memory region
              size_t offset;     // Current position
              size_t capacity;   // Total size
            };

            void *alloc(BumpAllocator *a, size_t size, size_t align) {
              size_t aligned = (a->offset + align - 1) & ~(align - 1);
              if (aligned + size > a->capacity) return NULL;
              void *ptr = a->buffer + aligned;
              a->offset = aligned + size;
              return ptr;
            }

      pitfalls:
        - "Forgetting alignment causes crashes on some architectures"
        - "Buffer overflow if not checking capacity"

      concepts:
        - Memory alignment
        - Arena allocation pattern
        - Pointer arithmetic

      estimated_hours: 2-3

    - id: allocator-02
      name: Free List Allocator
      description: |
        Implement a free list allocator that tracks free blocks.
        Support individual malloc() and free() calls.

      acceptance_criteria:
        - Maintain linked list of free blocks
        - Allocate from first-fit or best-fit block
        - Free returns block to the free list
        - Handle block splitting

      hints:
        - level1: "Store the free list metadata inside the free blocks themselves."
        - level2: |
            Each block header contains: size, is_free flag, next pointer.
            On malloc, traverse free list to find a suitable block.
        - level3: |
            struct BlockHeader {
              size_t size;
              int is_free;
              struct BlockHeader *next;
            };

            // First-fit allocation
            void *malloc(size_t size) {
              BlockHeader *curr = head;
              while (curr) {
                if (curr->is_free && curr->size >= size) {
                  curr->is_free = 0;
                  return (void*)(curr + 1);  // Return memory after header
                }
                curr = curr->next;
              }
              // Need more memory: call sbrk() or mmap()
            }

      pitfalls:
        - "External fragmentation: many small free blocks"
        - "Forgetting to track block sizes for free()"
        - "Header corruption causes silent memory bugs"

      concepts:
        - Free list data structure
        - First-fit vs best-fit allocation
        - Block metadata management

      estimated_hours: 4-5

    - id: allocator-03
      name: Block Coalescing
      description: |
        Merge adjacent free blocks to reduce fragmentation.
        Implement both immediate and deferred coalescing.

      acceptance_criteria:
        - Merge with next free block on free()
        - Merge with previous free block (boundary tags)
        - Reduce external fragmentation measurably
        - Handle edge cases (first/last block)

      hints:
        - level1: "To merge with the previous block, you need to know its size. Use 'boundary tags' (footer)."
        - level2: |
            Store the block size at both the start (header) and end (footer).
            On free(), check if adjacent blocks are free and merge.
        - level3: |
            // With boundary tags
            struct Block {
              size_t size;        // Header
              // ... payload ...
              size_t size_footer; // Footer (same value as header)
            };

            void free(void *ptr) {
              BlockHeader *block = (BlockHeader*)ptr - 1;
              block->is_free = 1;

              // Coalesce with next block
              BlockHeader *next = (BlockHeader*)((char*)block + block->size);
              if (next->is_free) {
                block->size += next->size;
                block->next = next->next;
              }

              // Coalesce with previous block (using footer)
              size_t *prev_footer = (size_t*)block - 1;
              BlockHeader *prev = (BlockHeader*)((char*)block - *prev_footer);
              if (prev->is_free) {
                prev->size += block->size;
                prev->next = block->next;
              }
            }

      pitfalls:
        - "Footer/header mismatch corrupts coalescing"
        - "Not handling the first and last blocks specially"

      concepts:
        - Boundary tags
        - Coalescing strategies
        - Memory fragmentation

      estimated_hours: 3-4

    - id: allocator-04
      name: Segregated Free Lists
      description: |
        Use multiple free lists organized by block size.
        Faster allocation by searching appropriate size class.

      acceptance_criteria:
        - Maintain separate free lists for size classes
        - Instant O(1) allocation for common sizes
        - Promote/demote blocks between size classes
        - Handle large allocations separately

      hints:
        - level1: "Common size classes: 8, 16, 32, 64, 128, 256, 512, 1024+ bytes."
        - level2: |
            Create an array of free lists, one per size class.
            malloc(size) finds the appropriate size class and pops from that list.
        - level3: |
            #define NUM_SIZE_CLASSES 10
            BlockHeader *free_lists[NUM_SIZE_CLASSES];

            int size_class(size_t size) {
              // Map size to class index
              if (size <= 8) return 0;
              if (size <= 16) return 1;
              if (size <= 32) return 2;
              // ...
            }

            void *malloc(size_t size) {
              int cls = size_class(size);
              if (free_lists[cls]) {
                BlockHeader *block = free_lists[cls];
                free_lists[cls] = block->next;
                return (void*)(block + 1);
              }
              // Split from larger class or allocate new
            }

      pitfalls:
        - "Internal fragmentation if size classes too coarse"
        - "Memory bloat if not recycling across classes"

      concepts:
        - Segregated storage
        - Size classes
        - Trade-offs in allocator design

      estimated_hours: 4-5

    - id: allocator-05
      name: Buddy System Allocator
      description: |
        Implement the buddy system for power-of-two allocations.
        Fast splitting and merging with O(log n) complexity.

      acceptance_criteria:
        - Allocate in power-of-two sizes
        - Split larger blocks into buddies
        - Merge buddy pairs on free
        - Track buddy relationships efficiently

      hints:
        - level1: "A block's 'buddy' is at an address that differs by exactly the block size."
        - level2: |
            For a block at address A with size 2^k:
            - Buddy address = A XOR 2^k
            - Buddies can only merge if both are free AND same size
        - level3: |
            // Buddy address calculation
            uintptr_t buddy_addr(uintptr_t addr, size_t size) {
              return addr ^ size;
            }

            // Maintain free lists per power-of-two level
            BlockHeader *free_lists[MAX_LEVELS];

            void free(void *ptr, size_t size) {
              int level = log2(size);
              uintptr_t addr = (uintptr_t)ptr;
              uintptr_t buddy = buddy_addr(addr, size);

              // Check if buddy is free at same level
              if (is_free(buddy, level)) {
                // Remove buddy from free list, merge, free at next level
                remove_from_list(buddy, level);
                uintptr_t merged = min(addr, buddy);
                free((void*)merged, size * 2);
              } else {
                // Add to free list at current level
                add_to_list(addr, level);
              }
            }

      pitfalls:
        - "Internal fragmentation (e.g., 129 bytes needs 256)"
        - "Buddy tracking bitmap can be memory-intensive"

      concepts:
        - Buddy system algorithm
        - Power-of-two allocation
        - Recursive splitting/merging

      estimated_hours: 5-6

    - id: allocator-06
      name: Thread-Safe Allocator
      description: |
        Make the allocator thread-safe without sacrificing performance.
        Per-thread caches, lock-free techniques, or arenas.

      acceptance_criteria:
        - No data races under concurrent allocation
        - Minimal lock contention
        - Per-thread caching for hot paths
        - Correct memory visibility

      hints:
        - level1: "Start with a global lock, then optimize to per-size-class locks or thread-local caches."
        - level2: |
            Thread-local caches (TLS):
            - Each thread has its own small cache of freed blocks
            - Only access global free list when cache is empty/full
        - level3: |
            __thread BlockHeader *thread_cache[NUM_SIZE_CLASSES];

            void *malloc(size_t size) {
              int cls = size_class(size);

              // Try thread-local cache first (no lock needed)
              if (thread_cache[cls]) {
                BlockHeader *block = thread_cache[cls];
                thread_cache[cls] = block->next;
                return (void*)(block + 1);
              }

              // Fall back to global allocator with lock
              pthread_mutex_lock(&global_lock);
              void *ptr = global_malloc(size);
              pthread_mutex_unlock(&global_lock);
              return ptr;
            }

      pitfalls:
        - "False sharing on cache lines"
        - "Thread-local caches can hoard memory"
        - "Memory ordering issues without proper barriers"

      concepts:
        - Thread-local storage
        - Lock contention
        - Memory ordering and barriers

      estimated_hours: 5-6

# =============================================================================
# PROJECT: BUILD YOUR OWN TCP STACK
# =============================================================================
# Implement TCP/IP from scratch in userspace
# Based on: RFC 793, lwIP, smoltcp

build-tcp-stack:
  id: "build-tcp-stack"
  name: "Build Your Own TCP Stack"
  domain: "networking"
  difficulty: "expert"
  languages: ["c", "rust", "go"]

  description: |
    Implement a TCP/IP stack in userspace using raw sockets or TUN/TAP.
    Learn the details of network protocols and state machines.

  learning_outcomes:
    - Understand TCP state machine
    - Implement reliable data transfer
    - Handle congestion control
    - Parse and construct network packets

  prerequisites:
    - Network basics (IP, ports, sockets)
    - Binary data handling
    - State machines
    - Understanding of RFC specifications

  resources:
    primary:
      - name: "TCP/IP Illustrated, Vol. 1"
        url: "https://www.amazon.com/TCP-Illustrated-Vol-Addison-Wesley-Professional/dp/0201633469"
      - name: "Let's code a TCP/IP stack (series)"
        url: "https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/"
    reference:
      - name: "RFC 793 - TCP"
        url: "https://www.rfc-editor.org/rfc/rfc793"
      - name: "smoltcp (Rust)"
        url: "https://github.com/smoltcp-rs/smoltcp"
      - name: "lwIP"
        url: "https://savannah.nongnu.org/projects/lwip/"

  milestones:
    - id: tcp-01
      name: TUN/TAP Device Setup
      description: |
        Create a virtual network interface using TUN/TAP.
        This allows your userspace code to send/receive raw IP packets.

      acceptance_criteria:
        - Open and configure TUN device
        - Read raw IP packets from the device
        - Write raw IP packets to the device
        - Basic packet hex dump for debugging

      hints:
        - level1: "Use /dev/net/tun on Linux. Set IFF_TUN flag for IP packets (no Ethernet header)."
        - level2: |
            You need to configure the TUN device with an IP address:
            $ sudo ip addr add 10.0.0.1/24 dev tun0
            $ sudo ip link set up dev tun0
        - level3: |
            int tun_alloc(char *dev) {
              struct ifreq ifr;
              int fd = open("/dev/net/tun", O_RDWR);
              if (fd < 0) return fd;

              memset(&ifr, 0, sizeof(ifr));
              ifr.ifr_flags = IFF_TUN | IFF_NO_PI;  // TUN device, no packet info
              strncpy(ifr.ifr_name, dev, IFNAMSIZ);

              if (ioctl(fd, TUNSETIFF, &ifr) < 0) {
                close(fd);
                return -1;
              }
              return fd;
            }

      pitfalls:
        - "Need root/CAP_NET_ADMIN to create TUN device"
        - "Forgetting to bring the interface up"
        - "Confusion between TUN (IP) and TAP (Ethernet)"

      concepts:
        - Virtual network interfaces
        - Raw packet I/O
        - Linux networking APIs

      estimated_hours: 2-3

    - id: tcp-02
      name: IP Packet Parsing
      description: |
        Parse incoming IP packets and construct outgoing IP headers.
        Handle IPv4 header fields and checksum.

      acceptance_criteria:
        - Parse IP version, header length, total length
        - Extract source/destination addresses
        - Calculate and verify IP header checksum
        - Handle IP options (or skip them)

      hints:
        - level1: "IPv4 header is 20+ bytes. First nibble is version (4), second nibble is header length in 32-bit words."
        - level2: |
            IP checksum: one's complement sum of all 16-bit words in header.
            If checksum is correct, sum of all words including checksum = 0xFFFF.
        - level3: |
            struct IPv4Header {
              uint8_t  version_ihl;     // Version (4 bits) + IHL (4 bits)
              uint8_t  tos;
              uint16_t total_length;
              uint16_t identification;
              uint16_t flags_fragoffset;
              uint8_t  ttl;
              uint8_t  protocol;        // 6 = TCP, 17 = UDP
              uint16_t checksum;
              uint32_t src_addr;
              uint32_t dst_addr;
            };

            uint16_t ip_checksum(void *data, size_t len) {
              uint32_t sum = 0;
              uint16_t *ptr = data;
              while (len > 1) {
                sum += *ptr++;
                len -= 2;
              }
              if (len) sum += *(uint8_t*)ptr;
              while (sum >> 16) sum = (sum & 0xFFFF) + (sum >> 16);
              return ~sum;
            }

      pitfalls:
        - "Endianness: network byte order is big-endian (use ntohs/htons)"
        - "Variable header length due to IP options"

      concepts:
        - IP packet structure
        - Header checksums
        - Network byte order

      estimated_hours: 3-4

    - id: tcp-03
      name: ICMP Echo (Ping)
      description: |
        Implement ICMP echo request/reply to test your IP layer.
        This validates IP parsing before moving to TCP complexity.

      acceptance_criteria:
        - Parse ICMP header (type, code, checksum)
        - Respond to Echo Request (type 8) with Echo Reply (type 0)
        - Calculate ICMP checksum correctly
        - Can ping your stack from host: ping 10.0.0.2

      hints:
        - level1: "ICMP is simple: 8-byte header + payload. Echo just swaps src/dst IP and changes type 80."
        - level2: |
            ICMP header for Echo:
            - Type: 1 byte (8=request, 0=reply)
            - Code: 1 byte (0 for echo)
            - Checksum: 2 bytes
            - Identifier: 2 bytes
            - Sequence: 2 bytes
            - Data: variable
        - level3: |
            struct ICMPHeader {
              uint8_t  type;
              uint8_t  code;
              uint16_t checksum;
              uint16_t id;
              uint16_t sequence;
            };

            void handle_icmp(IPPacket *pkt) {
              ICMPHeader *icmp = (ICMPHeader*)pkt->payload;
              if (icmp->type == 8) {  // Echo Request
                // Build Echo Reply
                icmp->type = 0;
                icmp->checksum = 0;
                icmp->checksum = checksum(icmp, pkt->payload_len);
                // Swap src/dst IP and send
                swap(&pkt->src_ip, &pkt->dst_ip);
                send_ip_packet(pkt);
              }
            }

      pitfalls:
        - "Must recalculate checksum after changing type"
        - "Don't forget to swap IP addresses"
        - "Ping timeout if reply is malformed"

      concepts:
        - ICMP protocol basics
        - Network debugging with ping
        - Validating IP layer before TCP

      estimated_hours: 2-3

    - id: tcp-04
      name: TCP Segment Parsing
      description: |
        Parse TCP headers including flags, sequence numbers, and options.
        Calculate TCP checksum with pseudo-header.

      acceptance_criteria:
        - Parse TCP header fields (ports, seq, ack, flags)
        - Handle TCP options (MSS, window scale, timestamps)
        - Calculate TCP checksum with IP pseudo-header
        - Construct valid TCP segments

      hints:
        - level1: "TCP header is 20+ bytes. Data offset field tells you where payload starts."
        - level2: |
            TCP checksum includes a 'pseudo-header':
            - Source IP (4 bytes)
            - Destination IP (4 bytes)
            - Zero byte + Protocol (6) + TCP length
        - level3: |
            struct TCPHeader {
              uint16_t src_port;
              uint16_t dst_port;
              uint32_t seq_num;
              uint32_t ack_num;
              uint8_t  data_offset;  // Upper 4 bits = header length / 4
              uint8_t  flags;        // FIN, SYN, RST, PSH, ACK, URG
              uint16_t window;
              uint16_t checksum;
              uint16_t urgent_ptr;
            };

            // TCP flags
            #define TCP_FIN 0x01
            #define TCP_SYN 0x02
            #define TCP_RST 0x04
            #define TCP_PSH 0x08
            #define TCP_ACK 0x10

      pitfalls:
        - "Pseudo-header must be included in checksum"
        - "Options affect header length"
        - "Sequence numbers are 32-bit and wrap around"

      concepts:
        - TCP segment structure
        - Pseudo-header checksum
        - TCP flags and their meanings

      estimated_hours: 3-4

    - id: tcp-05
      name: TCP Connection State Machine
      description: |
        Implement the TCP state machine for connection establishment and teardown.
        Handle SYN, SYN-ACK, FIN handshakes.

      acceptance_criteria:
        - Complete 3-way handshake (LISTEN  SYN_RECEIVED  ESTABLISHED)
        - Handle client connection (SYN_SENT  ESTABLISHED)
        - Implement connection teardown (FIN sequence)
        - Track connection state correctly

      hints:
        - level1: "Key states: CLOSED, LISTEN, SYN_SENT, SYN_RECEIVED, ESTABLISHED, FIN_WAIT_1/2, TIME_WAIT, CLOSE_WAIT, LAST_ACK, CLOSING"
        - level2: |
            3-way handshake:
            1. Client sends SYN (seq=x)
            2. Server sends SYN-ACK (seq=y, ack=x+1)
            3. Client sends ACK (ack=y+1)

            4-way teardown:
            1. FIN  ACK
            2. FIN  ACK
        - level3: |
            enum TCPState {
              CLOSED, LISTEN, SYN_SENT, SYN_RECEIVED,
              ESTABLISHED, FIN_WAIT_1, FIN_WAIT_2,
              CLOSE_WAIT, LAST_ACK, TIME_WAIT, CLOSING
            };

            void handle_segment(Connection *conn, TCPHeader *tcp, uint8_t *data, size_t len) {
              switch (conn->state) {
                case LISTEN:
                  if (tcp->flags & TCP_SYN) {
                    // Send SYN-ACK, move to SYN_RECEIVED
                    send_syn_ack(conn, tcp);
                    conn->state = SYN_RECEIVED;
                  }
                  break;

                case SYN_RECEIVED:
                  if (tcp->flags & TCP_ACK) {
                    conn->state = ESTABLISHED;
                  }
                  break;
                // ... other states
              }
            }

      pitfalls:
        - "TIME_WAIT exists to handle delayed duplicates (2*MSL timeout)"
        - "Simultaneous open/close are valid but rare"
        - "RST handling varies by state"

      concepts:
        - TCP state machine (RFC 793)
        - Connection establishment/teardown
        - Sequence number initialization

      estimated_hours: 5-6

    - id: tcp-06
      name: Reliable Data Transfer
      description: |
        Implement reliable data transfer with sequence numbers, acknowledgments, and retransmission.

      acceptance_criteria:
        - Track send and receive sequence numbers
        - Buffer out-of-order segments
        - Retransmit unacknowledged data
        - Handle duplicate ACKs

      hints:
        - level1: "Maintain SND.UNA (oldest unacked), SND.NXT (next to send), RCV.NXT (next expected)."
        - level2: |
            Retransmission timer: start when sending data, reset on ACK.
            If timer expires, retransmit oldest unacknowledged segment.
        - level3: |
            struct Connection {
              // Send side
              uint32_t snd_una;  // Oldest unacknowledged byte
              uint32_t snd_nxt;  // Next byte to send
              uint32_t snd_wnd;  // Send window
              Buffer  *send_buf; // Data waiting to be sent/ACKed

              // Receive side
              uint32_t rcv_nxt;  // Next expected byte
              uint32_t rcv_wnd;  // Receive window
              Buffer  *recv_buf; // Received data (may have holes)
            };

            void on_ack(Connection *conn, uint32_t ack_num) {
              if (ack_num > conn->snd_una) {
                // ACK advances: remove ACKed data from send buffer
                buffer_consume(conn->send_buf, ack_num - conn->snd_una);
                conn->snd_una = ack_num;
                reset_retransmit_timer(conn);
              }
            }

      pitfalls:
        - "Sequence number wraparound (use modular arithmetic)"
        - "Zero-window probing to avoid deadlock"
        - "Silly window syndrome"

      concepts:
        - Sliding window protocol
        - Retransmission timeout (RTO)
        - Cumulative acknowledgments

      estimated_hours: 6-8

    - id: tcp-07
      name: Congestion Control
      description: |
        Implement TCP congestion control (slow start, congestion avoidance, fast retransmit/recovery).

      acceptance_criteria:
        - Slow start on connection establishment
        - Congestion avoidance after threshold
        - Fast retransmit on 3 duplicate ACKs
        - CWND adjustment on loss detection

      hints:
        - level1: "Maintain cwnd (congestion window) separate from rwnd (receiver window). Send min(cwnd, rwnd)."
        - level2: |
            Slow start: cwnd starts at 1-2 MSS, doubles each RTT
            Congestion avoidance: cwnd increases by 1 MSS per RTT (additive increase)
            On timeout: ssthresh = cwnd/2, cwnd = 1 MSS
            On 3 dup ACKs: fast retransmit, cwnd = ssthresh + 3
        - level3: |
            struct Connection {
              uint32_t cwnd;      // Congestion window
              uint32_t ssthresh;  // Slow start threshold
              uint32_t dup_acks;  // Duplicate ACK counter
            };

            void on_ack(Connection *conn, uint32_t ack_num) {
              if (ack_num > conn->snd_una) {
                // New ACK
                if (conn->cwnd < conn->ssthresh) {
                  // Slow start
                  conn->cwnd += MSS;
                } else {
                  // Congestion avoidance
                  conn->cwnd += MSS * MSS / conn->cwnd;
                }
                conn->dup_acks = 0;
              } else {
                // Duplicate ACK
                conn->dup_acks++;
                if (conn->dup_acks == 3) {
                  // Fast retransmit
                  conn->ssthresh = conn->cwnd / 2;
                  conn->cwnd = conn->ssthresh + 3 * MSS;
                  retransmit(conn);
                }
              }
            }

      pitfalls:
        - "Conflating congestion window with receive window"
        - "Not handling RTO vs fast retransmit differently"
        - "cwnd explosion in slow start"

      concepts:
        - AIMD (Additive Increase, Multiplicative Decrease)
        - Slow start and congestion avoidance
        - Fast retransmit and recovery

      estimated_hours: 5-6

# =============================================================================
# PROJECT: BUILD YOUR OWN TRANSFORMER
# =============================================================================
# Implement transformer architecture from scratch
# Based on: Attention Is All You Need, Karpathy's nanoGPT

build-transformer:
  id: "build-transformer"
  name: "Build Your Own Transformer"
  domain: "machine-learning"
  difficulty: "expert"
  languages: ["python"]

  description: |
    Implement a transformer model from scratch, including attention mechanism,
    positional encoding, and training loop. Build a small GPT-style model.

  learning_outcomes:
    - Understand self-attention mechanism
    - Implement multi-head attention
    - Build encoder/decoder architecture
    - Train a language model

  prerequisites:
    - NumPy and basic neural networks
    - Matrix operations and broadcasting
    - Backpropagation (autograd helpful)
    - Python programming

  resources:
    primary:
      - name: "Attention Is All You Need (paper)"
        url: "https://arxiv.org/abs/1706.03762"
      - name: "The Illustrated Transformer"
        url: "https://jalammar.github.io/illustrated-transformer/"
      - name: "nanoGPT by Karpathy"
        url: "https://github.com/karpathy/nanoGPT"
    reference:
      - name: "The Annotated Transformer"
        url: "https://nlp.seas.harvard.edu/2018/04/03/attention.html"
      - name: "minGPT"
        url: "https://github.com/karpathy/minGPT"
      - name: "Let's build GPT (video)"
        url: "https://www.youtube.com/watch?v=kCc8FmEb1nY"

  milestones:
    - id: transformer-01
      name: Tokenization and Embeddings
      description: |
        Build a simple tokenizer and embedding layer.
        Convert text to token IDs and learn vector representations.

      acceptance_criteria:
        - Character-level or simple word tokenizer
        - Embedding lookup table
        - Handle vocabulary and special tokens
        - Batch text encoding/decoding

      hints:
        - level1: "Start with character-level tokenization: each unique character is a token."
        - level2: |
            Embedding is just a lookup table: token_id  vector
            Use nn.Embedding or implement as matrix indexing
        - level3: |
            class Tokenizer:
                def __init__(self, text):
                    chars = sorted(set(text))
                    self.char_to_id = {c: i for i, c in enumerate(chars)}
                    self.id_to_char = {i: c for i, c in enumerate(chars)}
                    self.vocab_size = len(chars)

                def encode(self, text):
                    return [self.char_to_id[c] for c in text]

                def decode(self, ids):
                    return ''.join(self.id_to_char[i] for i in ids)

            class Embedding:
                def __init__(self, vocab_size, embed_dim):
                    self.weight = np.random.randn(vocab_size, embed_dim) * 0.02

                def forward(self, token_ids):
                    return self.weight[token_ids]  # Shape: (batch, seq, embed)

      pitfalls:
        - "Unknown tokens at inference time"
        - "Embedding initialization scale matters"

      concepts:
        - Tokenization strategies
        - Embedding lookup
        - Vocabulary handling

      estimated_hours: 2-3

    - id: transformer-02
      name: Positional Encoding
      description: |
        Implement positional encoding to give the model sequence position information.
        Transformers have no inherent notion of position without this.

      acceptance_criteria:
        - Sinusoidal positional encoding
        - OR learnable positional embeddings
        - Add to token embeddings
        - Handle variable sequence lengths

      hints:
        - level1: "Sinusoidal encoding uses sin/cos functions at different frequencies for each position."
        - level2: |
            PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
            PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

            Or simply learn position embeddings like token embeddings.
        - level3: |
            def sinusoidal_encoding(max_seq_len, d_model):
                pe = np.zeros((max_seq_len, d_model))
                position = np.arange(max_seq_len)[:, np.newaxis]
                div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

                pe[:, 0::2] = np.sin(position * div_term)
                pe[:, 1::2] = np.cos(position * div_term)
                return pe

            # Learned positional embeddings (simpler)
            class PositionEmbedding:
                def __init__(self, max_seq_len, d_model):
                    self.weight = np.random.randn(max_seq_len, d_model) * 0.02

                def forward(self, seq_len):
                    return self.weight[:seq_len]

      pitfalls:
        - "Must add positional encoding, not concatenate"
        - "Sequence length limits with learned embeddings"

      concepts:
        - Why position matters for transformers
        - Sinusoidal vs learned positions
        - Extrapolation to longer sequences

      estimated_hours: 1-2

    - id: transformer-03
      name: Scaled Dot-Product Attention
      description: |
        Implement the core attention mechanism.
        Compute attention weights and weighted sum of values.

      acceptance_criteria:
        - Compute Q, K, V from input
        - Calculate attention scores with scaling
        - Apply softmax for attention weights
        - Compute weighted sum of values

      hints:
        - level1: "Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V"
        - level2: |
            Q, K, V are linear projections of the input.
            Scaling by sqrt(d_k) prevents softmax saturation.
        - level3: |
            def attention(Q, K, V, mask=None):
                # Q, K, V: (batch, seq_len, d_k)
                d_k = Q.shape[-1]

                # Attention scores: (batch, seq_len, seq_len)
                scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)

                # Apply mask for causal attention
                if mask is not None:
                    scores = scores + mask * (-1e9)

                # Softmax over last dimension
                weights = softmax(scores, axis=-1)

                # Weighted sum of values
                output = np.matmul(weights, V)
                return output, weights

      pitfalls:
        - "Forgetting the scaling factor causes vanishing gradients"
        - "Mask must be applied before softmax"
        - "Numerical stability in softmax"

      concepts:
        - Query, Key, Value intuition
        - Attention as soft dictionary lookup
        - Why scaling is necessary

      estimated_hours: 3-4

    - id: transformer-04
      name: Multi-Head Attention
      description: |
        Extend to multi-head attention for learning different attention patterns.
        Project to multiple heads, attend, concatenate, project back.

      acceptance_criteria:
        - Split into multiple attention heads
        - Run attention in parallel for each head
        - Concatenate head outputs
        - Final linear projection

      hints:
        - level1: "Split d_model into num_heads of size d_k = d_model / num_heads. Run attention separately, concatenate."
        - level2: |
            For efficiency, compute all heads in one batch operation:
            - Reshape (batch, seq, d_model)  (batch, seq, num_heads, d_k)
            - Transpose to (batch, num_heads, seq, d_k)
            - Run attention on this shape
        - level3: |
            class MultiHeadAttention:
                def __init__(self, d_model, num_heads):
                    self.num_heads = num_heads
                    self.d_k = d_model // num_heads

                    self.W_q = Linear(d_model, d_model)
                    self.W_k = Linear(d_model, d_model)
                    self.W_v = Linear(d_model, d_model)
                    self.W_o = Linear(d_model, d_model)

                def forward(self, x, mask=None):
                    batch, seq_len, _ = x.shape

                    # Project and reshape to (batch, num_heads, seq, d_k)
                    Q = self.W_q(x).reshape(batch, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
                    K = self.W_k(x).reshape(batch, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
                    V = self.W_v(x).reshape(batch, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)

                    # Attention for all heads at once
                    attn_out, _ = attention(Q, K, V, mask)

                    # Reshape back and project
                    attn_out = attn_out.transpose(0, 2, 1, 3).reshape(batch, seq_len, -1)
                    return self.W_o(attn_out)

      pitfalls:
        - "d_model must be divisible by num_heads"
        - "Reshape/transpose order matters"

      concepts:
        - Why multiple heads help
        - Parallel attention patterns
        - Head specialization

      estimated_hours: 3-4

    - id: transformer-05
      name: Transformer Block
      description: |
        Build a complete transformer block with attention, feed-forward network,
        layer normalization, and residual connections.

      acceptance_criteria:
        - Multi-head self-attention sublayer
        - Feed-forward network (2-layer MLP with ReLU/GELU)
        - Layer normalization (pre-norm or post-norm)
        - Residual connections around each sublayer

      hints:
        - level1: "Block = LayerNorm  Attention  Residual  LayerNorm  FFN  Residual"
        - level2: |
            Pre-norm (GPT-style): x = x + Attention(LayerNorm(x))
            Post-norm (original): x = LayerNorm(x + Attention(x))

            Pre-norm is easier to train deep networks.
        - level3: |
            class TransformerBlock:
                def __init__(self, d_model, num_heads, d_ff):
                    self.attention = MultiHeadAttention(d_model, num_heads)
                    self.ffn = FeedForward(d_model, d_ff)
                    self.ln1 = LayerNorm(d_model)
                    self.ln2 = LayerNorm(d_model)

                def forward(self, x, mask=None):
                    # Pre-norm style (GPT)
                    x = x + self.attention(self.ln1(x), mask)
                    x = x + self.ffn(self.ln2(x))
                    return x

            class FeedForward:
                def __init__(self, d_model, d_ff):
                    self.linear1 = Linear(d_model, d_ff)
                    self.linear2 = Linear(d_ff, d_model)

                def forward(self, x):
                    return self.linear2(gelu(self.linear1(x)))

      pitfalls:
        - "Forgetting residual connections causes training collapse"
        - "Layer norm vs batch norm confusion"

      concepts:
        - Residual connections for gradient flow
        - Layer normalization
        - Feed-forward expansion ratio (usually 4x)

      estimated_hours: 3-4

    - id: transformer-06
      name: GPT Language Model
      description: |
        Stack transformer blocks to build a GPT-style decoder-only language model.
        Add causal masking and output projection.

      acceptance_criteria:
        - Stack N transformer blocks
        - Causal (autoregressive) attention mask
        - Output projection to vocabulary logits
        - Generate text autoregressively

      hints:
        - level1: "Causal mask ensures position i can only attend to positions 0..i (prevent seeing the future)."
        - level2: |
            Causal mask is an upper-triangular matrix of -inf:
            mask[i][j] = 0 if j <= i else -inf

            After all blocks, project to vocab_size logits with a Linear layer.
        - level3: |
            class GPT:
                def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_len):
                    self.token_embed = Embedding(vocab_size, d_model)
                    self.pos_embed = PositionEmbedding(max_seq_len, d_model)
                    self.blocks = [TransformerBlock(d_model, num_heads, d_model * 4)
                                   for _ in range(num_layers)]
                    self.ln_f = LayerNorm(d_model)
                    self.head = Linear(d_model, vocab_size)

                def forward(self, token_ids):
                    seq_len = token_ids.shape[1]
                    x = self.token_embed(token_ids) + self.pos_embed(seq_len)

                    # Causal mask
                    mask = np.triu(np.ones((seq_len, seq_len)) * -1e9, k=1)

                    for block in self.blocks:
                        x = block(x, mask)

                    x = self.ln_f(x)
                    logits = self.head(x)  # (batch, seq, vocab_size)
                    return logits

                def generate(self, start_tokens, max_new_tokens):
                    tokens = start_tokens
                    for _ in range(max_new_tokens):
                        logits = self.forward(tokens)
                        next_logits = logits[:, -1, :]  # Last position
                        next_token = np.argmax(next_logits, axis=-1, keepdims=True)
                        tokens = np.concatenate([tokens, next_token], axis=1)
                    return tokens

      pitfalls:
        - "Causal mask must prevent future positions"
        - "Tying embedding and output weights (optional optimization)"
        - "Memory grows with sequence length squared"

      concepts:
        - Decoder-only architecture
        - Autoregressive generation
        - Causal attention masking

      estimated_hours: 4-5

    - id: transformer-07
      name: Training Loop
      description: |
        Train the GPT model on text data.
        Implement cross-entropy loss, backpropagation, and optimization.

      acceptance_criteria:
        - Cross-entropy loss for next-token prediction
        - Mini-batch training with data loading
        - AdamW optimizer
        - Learning rate scheduling (warmup + decay)

      hints:
        - level1: "Loss = cross_entropy(logits[:, :-1], targets[:, 1:]). Predict next token from each position."
        - level2: |
            AdamW separates weight decay from gradient update.
            Warmup: linearly increase LR for first N steps.
            Cosine decay: smoothly decrease LR to near zero.
        - level3: |
            def train_step(model, batch, optimizer):
                tokens = batch  # (batch_size, seq_len)
                inputs = tokens[:, :-1]
                targets = tokens[:, 1:]

                logits = model.forward(inputs)

                # Cross-entropy loss
                loss = cross_entropy_loss(logits.reshape(-1, vocab_size),
                                          targets.reshape(-1))

                # Backprop
                grads = backward(loss)
                optimizer.step(grads)

                return loss

            # Learning rate schedule with warmup
            def get_lr(step, warmup_steps, max_steps, max_lr):
                if step < warmup_steps:
                    return max_lr * step / warmup_steps
                # Cosine decay
                decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)
                return max_lr * 0.5 * (1 + np.cos(np.pi * decay_ratio))

      pitfalls:
        - "Gradient clipping prevents exploding gradients"
        - "Batch size affects learning dynamics"
        - "Shuffling data each epoch"

      concepts:
        - Language model training objective
        - AdamW optimizer
        - Learning rate scheduling

      estimated_hours: 4-5

# =============================================================================
# PROJECT: BUILD YOUR OWN BLOCKCHAIN
# =============================================================================
# Implement a simple cryptocurrency with proof-of-work
# Based on: Bitcoin whitepaper, Naivecoin tutorial

build-blockchain:
  id: "build-blockchain"
  name: "Build Your Own Blockchain"
  domain: "distributed-systems"
  difficulty: "expert"
  languages: ["python", "javascript", "go", "rust"]

  description: |
    Implement a simple blockchain with proof-of-work mining,
    transactions, and peer-to-peer networking.

  learning_outcomes:
    - Understand blockchain data structure
    - Implement proof-of-work consensus
    - Handle transactions and UTXO
    - Build peer-to-peer network

  prerequisites:
    - Cryptographic hashing (SHA-256)
    - Basic networking
    - JSON serialization
    - Understanding of consensus problems

  resources:
    primary:
      - name: "Bitcoin Whitepaper"
        url: "https://bitcoin.org/bitcoin.pdf"
      - name: "Naivecoin Tutorial"
        url: "https://lhartikk.github.io/"
      - name: "Learn Blockchains by Building One"
        url: "https://hackernoon.com/learn-blockchains-by-building-one-117428612f46"
    reference:
      - name: "Mastering Bitcoin"
        url: "https://github.com/bitcoinbook/bitcoinbook"
      - name: "But how does bitcoin actually work?"
        url: "https://www.youtube.com/watch?v=bBC-nXj3Ng4"

  milestones:
    - id: blockchain-01
      name: Block Structure and Hashing
      description: |
        Define the block data structure and implement cryptographic hashing.
        Each block contains data, timestamp, previous hash, and its own hash.

      acceptance_criteria:
        - Block contains index, timestamp, data, prev_hash, hash
        - SHA-256 hash of block contents
        - Genesis block (first block with no previous)
        - Blocks linked by previous_hash

      hints:
        - level1: "Hash the concatenation of: index + timestamp + data + previous_hash."
        - level2: |
            Block hash = SHA256(index + timestamp + data + previous_hash + nonce)
            The nonce is for proof-of-work (added later).
        - level3: |
            import hashlib
            import time

            class Block:
                def __init__(self, index, data, previous_hash):
                    self.index = index
                    self.timestamp = time.time()
                    self.data = data
                    self.previous_hash = previous_hash
                    self.nonce = 0
                    self.hash = self.calculate_hash()

                def calculate_hash(self):
                    content = f"{self.index}{self.timestamp}{self.data}{self.previous_hash}{self.nonce}"
                    return hashlib.sha256(content.encode()).hexdigest()

            def create_genesis_block():
                return Block(0, "Genesis Block", "0")

      pitfalls:
        - "Hash must be deterministic (same input  same hash)"
        - "Timestamp format must be consistent"

      concepts:
        - Cryptographic hash functions
        - Hash chains
        - Immutability through hashing

      estimated_hours: 2-3

    - id: blockchain-02
      name: Blockchain and Validation
      description: |
        Create a chain of blocks and implement validation rules.
        Ensure chain integrity by verifying hashes.

      acceptance_criteria:
        - Add new blocks to the chain
        - Validate each block's hash is correct
        - Validate previous_hash matches
        - Detect tampering

      hints:
        - level1: "To validate, recalculate each block's hash and compare with stored hash."
        - level2: |
            Validation checks:
            1. Block's stored hash == recalculated hash
            2. Block's previous_hash == previous block's hash
            3. Genesis block has index 0 and previous_hash "0"
        - level3: |
            class Blockchain:
                def __init__(self):
                    self.chain = [create_genesis_block()]

                def get_latest_block(self):
                    return self.chain[-1]

                def add_block(self, new_block):
                    new_block.previous_hash = self.get_latest_block().hash
                    new_block.hash = new_block.calculate_hash()
                    self.chain.append(new_block)

                def is_valid(self):
                    for i in range(1, len(self.chain)):
                        current = self.chain[i]
                        previous = self.chain[i - 1]

                        if current.hash != current.calculate_hash():
                            return False
                        if current.previous_hash != previous.hash:
                            return False
                    return True

      pitfalls:
        - "Modifying data without recalculating hash"
        - "Not validating the entire chain"

      concepts:
        - Chain integrity
        - Tamper detection
        - Validation rules

      estimated_hours: 2-3

    - id: blockchain-03
      name: Proof of Work
      description: |
        Implement proof-of-work mining to secure block creation.
        Find a nonce that makes the hash meet difficulty target.

      acceptance_criteria:
        - Hash must start with N zeros (difficulty)
        - Mine by incrementing nonce until valid
        - Adjustable difficulty
        - Track mining time/iterations

      hints:
        - level1: "Keep incrementing nonce until hash starts with '0000' (for difficulty 4)."
        - level2: |
            Difficulty target: hash must be less than 2^(256-difficulty)
            Or simply: hash must start with `difficulty` zeros in hex.
        - level3: |
            def mine_block(block, difficulty):
                target = "0" * difficulty
                while not block.hash.startswith(target):
                    block.nonce += 1
                    block.hash = block.calculate_hash()
                print(f"Block mined! Nonce: {block.nonce}")
                return block

            # Usage
            new_block = Block(1, "Some data", prev_hash)
            mined_block = mine_block(new_block, difficulty=4)

            # Difficulty adjustment (every N blocks)
            def adjust_difficulty(chain, target_time=10):
                if len(chain) % 10 == 0:
                    actual_time = chain[-1].timestamp - chain[-10].timestamp
                    if actual_time < target_time * 10:
                        return current_difficulty + 1
                    elif actual_time > target_time * 10:
                        return max(1, current_difficulty - 1)

      pitfalls:
        - "Mining can take very long with high difficulty"
        - "Difficulty adjustment is crucial for stable block time"

      concepts:
        - Proof of work consensus
        - Mining and nonce search
        - Difficulty adjustment

      estimated_hours: 3-4

    - id: blockchain-04
      name: Transactions
      description: |
        Add transaction support with inputs, outputs, and digital signatures.
        Implement the UTXO (Unspent Transaction Output) model.

      acceptance_criteria:
        - Transaction structure with inputs/outputs
        - Digital signature verification
        - UTXO tracking
        - Coinbase transaction for mining reward

      hints:
        - level1: "Each transaction input references a previous output. Outputs specify recipient and amount."
        - level2: |
            UTXO model:
            - Inputs: references to unspent outputs (txid + output_index)
            - Outputs: new unspent amounts (address + amount)
            - Signature proves ownership of input
        - level3: |
            from ecdsa import SigningKey, SECP256k1

            class Transaction:
                def __init__(self):
                    self.inputs = []   # [{txid, output_index, signature}]
                    self.outputs = []  # [{address, amount}]

                def sign(self, private_key, input_index):
                    # Sign the transaction data with private key
                    message = self.get_signing_data()
                    signature = private_key.sign(message.encode())
                    self.inputs[input_index]['signature'] = signature

                def verify(self, public_key, input_index):
                    message = self.get_signing_data()
                    signature = self.inputs[input_index]['signature']
                    return public_key.verify(signature, message.encode())

            class UTXO:
                def __init__(self):
                    self.unspent = {}  # {txid_outputindex: {address, amount}}

                def add_outputs(self, txid, outputs):
                    for i, out in enumerate(outputs):
                        self.unspent[f"{txid}_{i}"] = out

                def spend(self, txid, output_index):
                    del self.unspent[f"{txid}_{output_index}"]

      pitfalls:
        - "Double spending: same UTXO used twice"
        - "Sum of outputs must <= sum of inputs"
        - "Coinbase tx creates new coins (mining reward)"

      concepts:
        - UTXO model
        - Digital signatures (ECDSA)
        - Transaction validation

      estimated_hours: 5-6

    - id: blockchain-05
      name: Wallets
      description: |
        Create wallet functionality with key generation, balance tracking, and transaction creation.

      acceptance_criteria:
        - Generate public/private key pairs
        - Derive address from public key
        - Track wallet balance from UTXO
        - Create and sign transactions

      hints:
        - level1: "Wallet = private key. Address = hash of public key."
        - level2: |
            Balance = sum of all UTXO belonging to this address.
            To send: find UTXO >= amount, create inputs, create outputs (recipient + change).
        - level3: |
            from ecdsa import SigningKey, SECP256k1
            import hashlib

            class Wallet:
                def __init__(self):
                    self.private_key = SigningKey.generate(curve=SECP256k1)
                    self.public_key = self.private_key.get_verifying_key()
                    self.address = self._generate_address()

                def _generate_address(self):
                    pub_bytes = self.public_key.to_string()
                    return hashlib.sha256(pub_bytes).hexdigest()[:40]

                def get_balance(self, utxo_set):
                    balance = 0
                    for key, output in utxo_set.unspent.items():
                        if output['address'] == self.address:
                            balance += output['amount']
                    return balance

                def create_transaction(self, recipient, amount, utxo_set):
                    # Find enough UTXO
                    inputs = []
                    total = 0
                    for key, output in utxo_set.unspent.items():
                        if output['address'] == self.address:
                            inputs.append({'utxo': key})
                            total += output['amount']
                            if total >= amount:
                                break

                    if total < amount:
                        raise Exception("Insufficient balance")

                    tx = Transaction()
                    tx.inputs = inputs
                    tx.outputs = [
                        {'address': recipient, 'amount': amount},
                        {'address': self.address, 'amount': total - amount}  # Change
                    ]
                    # Sign inputs
                    for i in range(len(tx.inputs)):
                        tx.sign(self.private_key, i)
                    return tx

      pitfalls:
        - "Losing private key = losing all funds"
        - "Forgetting change output"

      concepts:
        - Public key cryptography
        - Address derivation
        - Wallet balance calculation

      estimated_hours: 3-4

    - id: blockchain-06
      name: Peer-to-Peer Network
      description: |
        Implement P2P networking for block and transaction propagation.
        Handle chain synchronization and conflict resolution.

      acceptance_criteria:
        - Nodes discover and connect to peers
        - Broadcast new blocks and transactions
        - Sync chain on startup
        - Longest chain wins (conflict resolution)

      hints:
        - level1: "Nodes maintain list of peer addresses. On new block, broadcast to all peers."
        - level2: |
            Messages: GET_BLOCKS, BLOCKS, NEW_BLOCK, NEW_TX
            On startup, request blockchain from peers.
            Longest valid chain wins.
        - level3: |
            import socket
            import json
            import threading

            class P2PNode:
                def __init__(self, host, port, blockchain):
                    self.host = host
                    self.port = port
                    self.blockchain = blockchain
                    self.peers = []

                def start(self):
                    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    server.bind((self.host, self.port))
                    server.listen(5)
                    threading.Thread(target=self.accept_connections, args=(server,)).start()

                def connect_to_peer(self, host, port):
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.connect((host, port))
                    self.peers.append(sock)
                    self.sync_chain(sock)

                def broadcast_block(self, block):
                    message = json.dumps({'type': 'NEW_BLOCK', 'data': block.to_dict()})
                    for peer in self.peers:
                        peer.send(message.encode())

                def handle_message(self, sock, message):
                    msg = json.loads(message)
                    if msg['type'] == 'NEW_BLOCK':
                        block = Block.from_dict(msg['data'])
                        if self.blockchain.add_block(block):
                            self.broadcast_block(block)  # Relay
                    elif msg['type'] == 'GET_CHAIN':
                        chain_data = [b.to_dict() for b in self.blockchain.chain]
                        sock.send(json.dumps({'type': 'CHAIN', 'data': chain_data}).encode())

                def resolve_conflicts(self, new_chain):
                    # Longest valid chain wins
                    if len(new_chain) > len(self.blockchain.chain):
                        if self.is_valid_chain(new_chain):
                            self.blockchain.chain = new_chain
                            return True
                    return False

      pitfalls:
        - "Race conditions with concurrent block arrivals"
        - "Orphan blocks when out of order"
        - "Network partition handling"

      concepts:
        - Peer-to-peer networking
        - Consensus and conflict resolution
        - Gossip protocol

      estimated_hours: 5-6

# =============================================================================
# PROJECT: BUILD YOUR OWN BITTORRENT CLIENT
# =============================================================================
# Implement a BitTorrent client from scratch
# Based on: BitTorrent BEP specifications

build-bittorrent:
  id: "build-bittorrent"
  name: "Build Your Own BitTorrent Client"
  domain: "networking"
  difficulty: "expert"
  languages: ["python", "go", "rust", "javascript"]

  description: |
    Implement a BitTorrent client that can parse torrent files,
    connect to trackers, download pieces from peers, and assemble files.

  learning_outcomes:
    - Understand P2P file sharing
    - Implement Bencode encoding
    - Handle BitTorrent peer protocol
    - Manage piece downloading and verification

  prerequisites:
    - TCP socket programming
    - Binary data handling
    - SHA-1 hashing
    - Concurrent programming

  resources:
    primary:
      - name: "BitTorrent Protocol Specification"
        url: "https://www.bittorrent.org/beps/bep_0003.html"
      - name: "Building a BitTorrent Client"
        url: "https://blog.jse.li/posts/torrent/"
      - name: "CodeCrafters BitTorrent"
        url: "https://codecrafters.io/challenges/bittorrent"
    reference:
      - name: "Bencode specification"
        url: "https://wiki.theory.org/BitTorrentSpecification#Bencoding"
      - name: "Unofficial BitTorrent Specification"
        url: "https://wiki.theory.org/BitTorrentSpecification"

  milestones:
    - id: bittorrent-01
      name: Bencode Parser
      description: |
        Implement Bencode encoding/decoding used in .torrent files.
        Handle strings, integers, lists, and dictionaries.

      acceptance_criteria:
        - Decode bencoded strings (e.g., "4:spam")
        - Decode integers (e.g., "i42e")
        - Decode lists (e.g., "l4:spam4:eggse")
        - Decode dictionaries
        - Encode back to bencode

      hints:
        - level1: |
            Bencode types:
            - Strings: <length>:<data> (e.g., "4:spam")
            - Integers: i<number>e (e.g., "i42e")
            - Lists: l<items>e
            - Dicts: d<key><value>...e (keys are strings, sorted)
        - level2: |
            Parse recursively. First character determines type:
            - Digit  string
            - 'i'  integer
            - 'l'  list
            - 'd'  dictionary
        - level3: |
            def decode(data, index=0):
                if data[index].isdigit():
                    # String: find ':', get length, extract data
                    colon = data.index(':', index)
                    length = int(data[index:colon])
                    start = colon + 1
                    return data[start:start + length], start + length

                elif data[index] == 'i':
                    # Integer: find 'e'
                    end = data.index('e', index)
                    return int(data[index + 1:end]), end + 1

                elif data[index] == 'l':
                    # List
                    result = []
                    index += 1
                    while data[index] != 'e':
                        item, index = decode(data, index)
                        result.append(item)
                    return result, index + 1

                elif data[index] == 'd':
                    # Dictionary
                    result = {}
                    index += 1
                    while data[index] != 'e':
                        key, index = decode(data, index)
                        value, index = decode(data, index)
                        result[key] = value
                    return result, index + 1

      pitfalls:
        - "Binary data in strings (not always UTF-8)"
        - "Dictionary keys must be sorted for encoding"
        - "Negative integers and leading zeros"

      concepts:
        - Recursive parsing
        - Binary encoding formats
        - Serialization

      estimated_hours: 2-3

    - id: bittorrent-02
      name: Torrent File Parsing
      description: |
        Parse .torrent files to extract metadata: tracker URL, file info, piece hashes.

      acceptance_criteria:
        - Extract announce URL (tracker)
        - Get file name, length, piece length
        - Extract piece hashes (SHA-1)
        - Calculate info hash

      hints:
        - level1: "Torrent file is bencoded dict with 'announce' and 'info' keys."
        - level2: |
            Key fields:
            - announce: tracker URL
            - info.name: file/folder name
            - info.length: total file size (single file)
            - info.piece length: bytes per piece (usually 256KB-1MB)
            - info.pieces: concatenated 20-byte SHA-1 hashes
            - info_hash = SHA-1 of bencoded 'info' dict
        - level3: |
            import hashlib

            def parse_torrent(path):
                with open(path, 'rb') as f:
                    data = f.read()

                torrent = bencode_decode(data)

                # Info hash = SHA-1 of bencoded info dict
                info_bencoded = bencode_encode(torrent['info'])
                info_hash = hashlib.sha1(info_bencoded).digest()

                # Split pieces into 20-byte hashes
                pieces_raw = torrent['info']['pieces']
                piece_hashes = [pieces_raw[i:i+20] for i in range(0, len(pieces_raw), 20)]

                return {
                    'announce': torrent['announce'],
                    'info_hash': info_hash,
                    'name': torrent['info']['name'],
                    'length': torrent['info']['length'],
                    'piece_length': torrent['info']['piece length'],
                    'piece_hashes': piece_hashes,
                }

      pitfalls:
        - "Multi-file torrents have different structure"
        - "Info hash must be exact (for tracker/peer handshake)"
        - "Pieces are binary, not hex"

      concepts:
        - Torrent file structure
        - Info hash identification
        - Piece-based file splitting

      estimated_hours: 2-3

    - id: bittorrent-03
      name: Tracker Communication
      description: |
        Contact the tracker to get a list of peers.
        Handle HTTP tracker protocol.

      acceptance_criteria:
        - Build tracker announce URL with required params
        - Parse bencoded tracker response
        - Extract peer IP addresses and ports
        - Handle compact peer format

      hints:
        - level1: |
            GET request to tracker with params:
            - info_hash, peer_id, port, uploaded, downloaded, left, compact=1
        - level2: |
            Compact peer format: 6 bytes per peer (4 bytes IP + 2 bytes port).
            Response contains 'peers' (compact or list) and 'interval'.
        - level3: |
            import requests
            import struct

            def get_peers(torrent, peer_id, port):
                params = {
                    'info_hash': torrent['info_hash'],
                    'peer_id': peer_id,
                    'port': port,
                    'uploaded': 0,
                    'downloaded': 0,
                    'left': torrent['length'],
                    'compact': 1,
                }
                response = requests.get(torrent['announce'], params=params)
                data = bencode_decode(response.content)

                # Parse compact peers (6 bytes each)
                peers_raw = data['peers']
                peers = []
                for i in range(0, len(peers_raw), 6):
                    ip = '.'.join(str(b) for b in peers_raw[i:i+4])
                    port = struct.unpack('>H', peers_raw[i+4:i+6])[0]
                    peers.append((ip, port))

                return peers

      pitfalls:
        - "info_hash must be URL-encoded binary"
        - "Some trackers return failure reason"
        - "UDP trackers use different protocol"

      concepts:
        - HTTP tracker protocol
        - Peer discovery
        - URL encoding of binary data

      estimated_hours: 3-4

    - id: bittorrent-04
      name: Peer Handshake
      description: |
        Connect to peers and perform BitTorrent handshake.
        Exchange capabilities and verify info hash.

      acceptance_criteria:
        - TCP connect to peer
        - Send handshake message
        - Receive and validate peer's handshake
        - Verify matching info hash

      hints:
        - level1: |
            Handshake format (68 bytes):
            - 1 byte: length of protocol string (19)
            - 19 bytes: "BitTorrent protocol"
            - 8 bytes: reserved (extensions)
            - 20 bytes: info_hash
            - 20 bytes: peer_id
        - level2: |
            After handshake, both peers know they're talking about the same torrent.
            Then exchange bitfield and interested/choke messages.
        - level3: |
            import socket

            def handshake(peer_ip, peer_port, info_hash, peer_id):
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.connect((peer_ip, peer_port))

                # Build handshake
                pstrlen = bytes([19])
                pstr = b"BitTorrent protocol"
                reserved = bytes(8)
                handshake = pstrlen + pstr + reserved + info_hash + peer_id

                sock.send(handshake)

                # Receive handshake (68 bytes)
                response = sock.recv(68)
                if len(response) < 68:
                    raise Exception("Invalid handshake")

                # Verify info_hash matches
                received_hash = response[28:48]
                if received_hash != info_hash:
                    raise Exception("Info hash mismatch")

                return sock

      pitfalls:
        - "Handshake must be exactly 68 bytes"
        - "Peer may disconnect immediately"
        - "Read exact bytes (TCP is stream-based)"

      concepts:
        - BitTorrent handshake protocol
        - TCP stream handling
        - Protocol negotiation

      estimated_hours: 2-3

    - id: bittorrent-05
      name: Peer Wire Protocol
      description: |
        Implement peer message protocol: choke, unchoke, interested, have, bitfield, request, piece.

      acceptance_criteria:
        - Parse length-prefixed messages
        - Handle choke/unchoke state
        - Send interested message
        - Request and receive pieces

      hints:
        - level1: |
            Message format: 4-byte length prefix + 1-byte message ID + payload
            IDs: 0=choke, 1=unchoke, 2=interested, 4=have, 5=bitfield, 6=request, 7=piece
        - level2: |
            Bitfield: one bit per piece, shows what peer has.
            Request: index (4), begin (4), length (4) - typically 16KB blocks.
            Piece: index (4), begin (4), block data.
        - level3: |
            import struct

            MSG_CHOKE = 0
            MSG_UNCHOKE = 1
            MSG_INTERESTED = 2
            MSG_HAVE = 4
            MSG_BITFIELD = 5
            MSG_REQUEST = 6
            MSG_PIECE = 7

            def recv_message(sock):
                length_data = sock.recv(4)
                length = struct.unpack('>I', length_data)[0]
                if length == 0:
                    return None, None  # Keep-alive

                msg_id = sock.recv(1)[0]
                payload = sock.recv(length - 1) if length > 1 else b''
                return msg_id, payload

            def send_interested(sock):
                # Length=1, ID=2
                sock.send(struct.pack('>IB', 1, MSG_INTERESTED))

            def send_request(sock, piece_index, begin, length):
                # Length=13, ID=6, then index, begin, length
                msg = struct.pack('>IBIII', 13, MSG_REQUEST, piece_index, begin, length)
                sock.send(msg)

      pitfalls:
        - "Must wait for unchoke before requesting"
        - "Track choked state per peer"
        - "Handle partial reads on TCP"

      concepts:
        - Length-prefixed protocol
        - State machine (choked/interested)
        - Block-based data transfer

      estimated_hours: 4-5

    - id: bittorrent-06
      name: Downloading and Assembly
      description: |
        Download all pieces, verify with SHA-1, and assemble into final file.
        Handle piece/block management.

      acceptance_criteria:
        - Track which pieces are downloaded
        - Request blocks within pieces
        - Verify piece hash after download
        - Write pieces to correct file offset

      hints:
        - level1: "Split each piece into 16KB blocks. Request blocks, reassemble piece, verify hash."
        - level2: |
            Piece download flow:
            1. Pick a piece to download
            2. Request 16KB blocks from offset 0
            3. Reassemble blocks into piece
            4. SHA-1 verify against piece_hashes
            5. Write to file at piece_index * piece_length
        - level3: |
            import hashlib

            BLOCK_SIZE = 16384  # 16KB

            class PieceManager:
                def __init__(self, torrent):
                    self.piece_length = torrent['piece_length']
                    self.piece_hashes = torrent['piece_hashes']
                    self.total_length = torrent['length']
                    self.num_pieces = len(self.piece_hashes)
                    self.downloaded = [False] * self.num_pieces
                    self.file = open(torrent['name'], 'wb')
                    self.file.truncate(self.total_length)

                def download_piece(self, sock, piece_index):
                    piece_size = min(self.piece_length,
                                     self.total_length - piece_index * self.piece_length)

                    data = b''
                    offset = 0
                    while offset < piece_size:
                        block_size = min(BLOCK_SIZE, piece_size - offset)
                        send_request(sock, piece_index, offset, block_size)

                        msg_id, payload = recv_message(sock)
                        if msg_id == MSG_PIECE:
                            idx, begin = struct.unpack('>II', payload[:8])
                            block = payload[8:]
                            data += block
                            offset += len(block)

                    # Verify hash
                    if hashlib.sha1(data).digest() != self.piece_hashes[piece_index]:
                        raise Exception(f"Piece {piece_index} hash mismatch")

                    # Write to file
                    self.file.seek(piece_index * self.piece_length)
                    self.file.write(data)
                    self.downloaded[piece_index] = True

      pitfalls:
        - "Last piece may be smaller than piece_length"
        - "Hash mismatch means corrupted data (re-download)"
        - "Concurrent downloads need synchronization"

      concepts:
        - Piece and block management
        - File I/O with seeking
        - Data integrity verification

      estimated_hours: 4-5

    - id: bittorrent-07
      name: Seeding (Uploading)
      description: |
        Implement uploading/seeding to share pieces with other peers.
        Complete the P2P cycle by contributing back to the swarm.

      acceptance_criteria:
        - Send bitfield message after handshake
        - Respond to request messages with piece data
        - Handle multiple concurrent upload requests
        - Announce "have" when new pieces complete

      hints:
        - level1: "After handshake, send your bitfield (which pieces you have). Then wait for requests."
        - level2: |
            Upload flow:
            1. Receive REQUEST message (index, begin, length)
            2. Read block from local file
            3. Send PIECE message (index, begin, data)

            Choking strategy: limit number of unchoked peers (usually 4).
        - level3: |
            def handle_request(sock, piece_index, begin, length):
                # Read from local file
                offset = piece_index * piece_length + begin
                file.seek(offset)
                data = file.read(length)

                # Send piece message
                msg = struct.pack('>IBII', 9 + len(data), MSG_PIECE, piece_index, begin)
                msg += data
                sock.send(msg)

            # Choking algorithm (simplified tit-for-tat)
            class ChokingManager:
                def __init__(self, max_unchoked=4):
                    self.max_unchoked = max_unchoked
                    self.unchoked_peers = set()

                def update(self, peers):
                    # Unchoke top uploaders (reciprocation)
                    sorted_peers = sorted(peers, key=lambda p: p.upload_rate, reverse=True)
                    new_unchoked = set(sorted_peers[:self.max_unchoked])

                    for peer in new_unchoked - self.unchoked_peers:
                        peer.send_unchoke()
                    for peer in self.unchoked_peers - new_unchoked:
                        peer.send_choke()

                    self.unchoked_peers = new_unchoked

      pitfalls:
        - "Must handle requests only from unchoked peers"
        - "Rate limiting to avoid bandwidth saturation"
        - "Concurrent file reads need synchronization"

      concepts:
        - Tit-for-tat choking algorithm
        - Upload bandwidth management
        - P2P reciprocation incentives

      estimated_hours: 3-4

# =============================================================================
# PROJECT: BUILD YOUR OWN SPREADSHEET
# =============================================================================
# Implement a spreadsheet with formulas and reactive updates
# Based on: Excel/Google Sheets concepts

build-spreadsheet:
  id: "build-spreadsheet"
  name: "Build Your Own Spreadsheet"
  domain: "applications"
  difficulty: "expert"
  languages: ["javascript", "python", "rust"]

  description: |
    Implement a spreadsheet application with formula parsing,
    cell references, and reactive updates when dependencies change.

  learning_outcomes:
    - Parse and evaluate formulas
    - Build dependency graph
    - Implement reactive updates
    - Handle circular references

  prerequisites:
    - Basic parsing concepts
    - Graph algorithms (topological sort)
    - UI basics (optional for web version)
    - Expression evaluation

  resources:
    primary:
      - name: "Spreadsheet Implementation Technology"
        url: "https://www.amazon.com/Spreadsheet-Implementation-Technology-Peter-Sestoft/dp/0262693488"
      - name: "How to Build a Spreadsheet"
        url: "https://jamesfisher.com/2018/03/31/spreadsheet.html"
    reference:
      - name: "Dependency-Directed Backtracking"
        url: "https://en.wikipedia.org/wiki/Dependency-directed_backtracking"
      - name: "Excel Formula Parsing"
        url: "https://docs.microsoft.com/en-us/office/client-developer/excel/excel-recalculation"

  milestones:
    - id: spreadsheet-01
      name: Cell Grid and Basic Input
      description: |
        Create a cell grid structure that stores values.
        Support text and number input.

      acceptance_criteria:
        - Grid of cells (e.g., A1 to Z100)
        - Store and retrieve cell values
        - Cell addressing (column letter + row number)
        - Display formatted values

      hints:
        - level1: "Use a dictionary/hashmap with cell address as key (e.g., 'A1', 'B2')."
        - level2: |
            Cell address parsing:
            - Column: A=0, B=1, ..., Z=25, AA=26, AB=27
            - Row: 1-indexed

            Store both raw value and display value.
        - level3: |
            class Cell:
                def __init__(self):
                    self.raw_value = ""      # What user typed
                    self.computed_value = "" # Evaluated result
                    self.formula = None      # Parsed formula (if starts with =)
                    self.dependencies = []   # Cells this depends on

            class Spreadsheet:
                def __init__(self, rows=100, cols=26):
                    self.cells = {}

                def get_cell(self, address):
                    if address not in self.cells:
                        self.cells[address] = Cell()
                    return self.cells[address]

                def set_cell(self, address, value):
                    cell = self.get_cell(address)
                    cell.raw_value = value
                    if value.startswith('='):
                        cell.formula = self.parse_formula(value[1:])
                    else:
                        cell.computed_value = self.parse_literal(value)

                @staticmethod
                def address_to_coords(address):
                    # "B3" -> (col=1, row=2)
                    col = 0
                    i = 0
                    while i < len(address) and address[i].isalpha():
                        col = col * 26 + (ord(address[i].upper()) - ord('A') + 1)
                        i += 1
                    row = int(address[i:]) - 1
                    return col - 1, row

      pitfalls:
        - "Column addressing is base-26 but 1-indexed (A=1, not A=0)"
        - "Multi-letter columns (AA, AB) need proper parsing"

      concepts:
        - Cell addressing
        - Value storage and display
        - Input parsing

      estimated_hours: 2-3

    - id: spreadsheet-02
      name: Formula Parser
      description: |
        Parse spreadsheet formulas into an AST.
        Support arithmetic, cell references, and functions.

      acceptance_criteria:
        - Parse arithmetic expressions (+ - * / ^)
        - Parse cell references (A1, B$2, $C$3)
        - Parse function calls (SUM, AVG, IF)
        - Handle operator precedence

      hints:
        - level1: "Use recursive descent parsing. Numbers, cell refs, and functions are atoms. Combine with operators."
        - level2: |
            Grammar (simplified):
            expr   := term (('+' | '-') term)*
            term   := factor (('*' | '/') factor)*
            factor := atom ('^' factor)?
            atom   := NUMBER | CELL_REF | FUNCTION '(' args ')' | '(' expr ')'
        - level3: |
            class FormulaParser:
                def __init__(self, text):
                    self.text = text
                    self.pos = 0

                def parse(self):
                    return self.parse_expr()

                def parse_expr(self):
                    left = self.parse_term()
                    while self.match('+', '-'):
                        op = self.previous()
                        right = self.parse_term()
                        left = BinaryOp(op, left, right)
                    return left

                def parse_term(self):
                    left = self.parse_factor()
                    while self.match('*', '/'):
                        op = self.previous()
                        right = self.parse_factor()
                        left = BinaryOp(op, left, right)
                    return left

                def parse_atom(self):
                    if self.match_number():
                        return NumberNode(float(self.previous()))
                    if self.match_cell_ref():
                        return CellRefNode(self.previous())
                    if self.match_function():
                        func_name = self.previous()
                        self.expect('(')
                        args = self.parse_args()
                        self.expect(')')
                        return FunctionNode(func_name, args)
                    if self.match('('):
                        expr = self.parse_expr()
                        self.expect(')')
                        return expr
                    raise ParseError(f"Unexpected: {self.peek()}")

      pitfalls:
        - "Cell refs can have $ for absolute references"
        - "Ranges like A1:B5 need special handling"
        - "Operator precedence (^ before * before +)"

      concepts:
        - Recursive descent parsing
        - Abstract syntax tree
        - Operator precedence

      estimated_hours: 4-5

    - id: spreadsheet-03
      name: Formula Evaluation
      description: |
        Evaluate parsed formulas to compute cell values.
        Handle cell references and built-in functions.

      acceptance_criteria:
        - Evaluate arithmetic operations
        - Resolve cell references
        - Implement SUM, AVG, MIN, MAX, IF
        - Handle errors (DIV/0, circular ref)

      hints:
        - level1: "Walk the AST recursively. For cell refs, look up the cell's computed value."
        - level2: |
            Functions that take ranges:
            - SUM(A1:A10) - expand range to list of cell refs
            - IF(cond, true_val, false_val)
        - level3: |
            class Evaluator:
                def __init__(self, spreadsheet):
                    self.spreadsheet = spreadsheet

                def evaluate(self, node):
                    if isinstance(node, NumberNode):
                        return node.value

                    if isinstance(node, CellRefNode):
                        cell = self.spreadsheet.get_cell(node.address)
                        return cell.computed_value

                    if isinstance(node, BinaryOp):
                        left = self.evaluate(node.left)
                        right = self.evaluate(node.right)
                        if node.op == '+': return left + right
                        if node.op == '-': return left - right
                        if node.op == '*': return left * right
                        if node.op == '/':
                            if right == 0:
                                return ErrorValue("#DIV/0!")
                            return left / right

                    if isinstance(node, FunctionNode):
                        args = [self.evaluate(arg) for arg in node.args]
                        if node.name == 'SUM':
                            return sum(args)
                        if node.name == 'AVG':
                            return sum(args) / len(args)
                        if node.name == 'IF':
                            return args[1] if args[0] else args[2]
                        # ... more functions

      pitfalls:
        - "Cell might have formula that hasn't been evaluated yet"
        - "Type errors (SUM of text)"
        - "Error propagation (A1 has error, B1=A1+1 should also error)"

      concepts:
        - AST evaluation
        - Function implementation
        - Error handling

      estimated_hours: 3-4

    - id: spreadsheet-04
      name: Dependency Graph
      description: |
        Track which cells depend on which other cells.
        Detect circular references.

      acceptance_criteria:
        - Extract cell references from formulas
        - Build dependency graph
        - Detect cycles (circular references)
        - Report circular ref error

      hints:
        - level1: "For each formula, collect all CellRefNodes. That's the dependency list."
        - level2: |
            Dependency graph is directed: A1 depends on B1 means edge B1  A1.
            Cycle detection: DFS with visited/in-progress states.
        - level3: |
            class DependencyGraph:
                def __init__(self):
                    self.deps = {}  # cell -> list of cells it depends on
                    self.rdeps = {} # cell -> list of cells that depend on it

                def set_dependencies(self, cell, depends_on):
                    # Remove old reverse deps
                    if cell in self.deps:
                        for old_dep in self.deps[cell]:
                            self.rdeps[old_dep].discard(cell)

                    self.deps[cell] = set(depends_on)

                    # Add new reverse deps
                    for dep in depends_on:
                        if dep not in self.rdeps:
                            self.rdeps[dep] = set()
                        self.rdeps[dep].add(cell)

                def detect_cycle(self, start_cell):
                    visited = set()
                    in_progress = set()

                    def dfs(cell):
                        if cell in in_progress:
                            return True  # Cycle found
                        if cell in visited:
                            return False
                        in_progress.add(cell)
                        for dep in self.deps.get(cell, []):
                            if dfs(dep):
                                return True
                        in_progress.remove(cell)
                        visited.add(cell)
                        return False

                    return dfs(start_cell)

      pitfalls:
        - "Self-reference (A1 = A1 + 1) is simplest cycle"
        - "Indirect cycles (A1B1C1A1)"
        - "Must update graph when formula changes"

      concepts:
        - Directed graphs
        - Cycle detection (DFS)
        - Dependency tracking

      estimated_hours: 3-4

    - id: spreadsheet-05
      name: Reactive Recalculation
      description: |
        When a cell changes, automatically recalculate all dependent cells.
        Use topological sort for correct order.

      acceptance_criteria:
        - Identify all cells affected by a change
        - Topological sort for recalculation order
        - Recalculate in correct order
        - Efficient: only recalculate what's needed

      hints:
        - level1: "When A1 changes, find all cells in rdeps[A1], their rdeps, etc. Then topo-sort."
        - level2: |
            Topological sort ensures we calculate dependencies before dependents.
            Kahn's algorithm or DFS-based approach works.
        - level3: |
            def recalculate_from(self, changed_cell):
                # Find all affected cells (transitive closure of rdeps)
                affected = set()
                queue = [changed_cell]
                while queue:
                    cell = queue.pop()
                    for dependent in self.graph.rdeps.get(cell, []):
                        if dependent not in affected:
                            affected.add(dependent)
                            queue.append(dependent)

                # Topological sort
                in_degree = {cell: 0 for cell in affected}
                for cell in affected:
                    for dep in self.graph.deps.get(cell, []):
                        if dep in affected:
                            in_degree[cell] += 1

                sorted_cells = []
                queue = [c for c in affected if in_degree[c] == 0]
                while queue:
                    cell = queue.pop(0)
                    sorted_cells.append(cell)
                    for dependent in self.graph.rdeps.get(cell, []):
                        if dependent in in_degree:
                            in_degree[dependent] -= 1
                            if in_degree[dependent] == 0:
                                queue.append(dependent)

                # Recalculate in order
                for cell in sorted_cells:
                    self.evaluate_cell(cell)

      pitfalls:
        - "Recalc order matters: B1=A1+1, C1=B1+1 must calc B1 first"
        - "Don't recalculate cells not affected by change"
        - "Handle errors gracefully in chain"

      concepts:
        - Topological sorting
        - Reactive programming
        - Incremental computation

      estimated_hours: 4-5

    - id: spreadsheet-06
      name: Range Operations
      description: |
        Support cell ranges (A1:B5) in formulas.
        Expand ranges for functions like SUM.

      acceptance_criteria:
        - Parse range syntax (A1:B5)
        - Expand range to list of cells
        - Support in functions (SUM, AVG)
        - Handle mixed refs ($A1:B$5)

      hints:
        - level1: "A1:B5 means all cells from A1 to B5 (rectangle). Expand to list during evaluation."
        - level2: |
            Range expansion:
            - A1:A5 = [A1, A2, A3, A4, A5]
            - A1:B2 = [A1, A2, B1, B2]
        - level3: |
            class RangeNode:
                def __init__(self, start, end):
                    self.start = start  # e.g., "A1"
                    self.end = end      # e.g., "B5"

                def expand(self):
                    start_col, start_row = address_to_coords(self.start)
                    end_col, end_row = address_to_coords(self.end)

                    cells = []
                    for row in range(start_row, end_row + 1):
                        for col in range(start_col, end_col + 1):
                            cells.append(coords_to_address(col, row))
                    return cells

            # In evaluator:
            if isinstance(node, RangeNode):
                cells = node.expand()
                return [self.spreadsheet.get_cell(c).computed_value for c in cells]

            # SUM(A1:A5) evaluates RangeNode, gets list, sums it
            if node.name == 'SUM':
                values = []
                for arg in node.args:
                    result = self.evaluate(arg)
                    if isinstance(result, list):
                        values.extend(result)
                    else:
                        values.append(result)
                return sum(v for v in values if isinstance(v, (int, float)))

      pitfalls:
        - "Range dependencies: A1:A5 depends on all 5 cells"
        - "Empty cells in range (treat as 0 or skip?)"
        - "Mixed absolute/relative in ranges"

      concepts:
        - Range notation and expansion
        - Aggregate functions
        - Handling collections in formulas

      estimated_hours: 3-4

    - id: spreadsheet-07
      name: User Interface
      description: |
        Build a basic UI for the spreadsheet.
        Show grid, edit cells, display formulas/values.

      acceptance_criteria:
        - Display cell grid
        - Edit cell on click/selection
        - Show formula bar for active cell
        - Display computed values in grid

      hints:
        - level1: "Web: use HTML table or CSS grid. Terminal: ncurses. Focus on edit experience."
        - level2: |
            UI flow:
            1. Click cell  show formula in formula bar
            2. Edit formula bar  update cell on Enter
            3. Cell shows computed value, not formula
            4. Selection highlight
        - level3: |
            // React example
            function SpreadsheetUI({ spreadsheet }) {
              const [activeCell, setActiveCell] = useState('A1');
              const [editValue, setEditValue] = useState('');

              const handleCellClick = (address) => {
                setActiveCell(address);
                const cell = spreadsheet.getCell(address);
                setEditValue(cell.rawValue);
              };

              const handleSubmit = () => {
                spreadsheet.setCell(activeCell, editValue);
                // Trigger recalculation
              };

              return (
                <div>
                  <FormulaBar
                    cell={activeCell}
                    value={editValue}
                    onChange={setEditValue}
                    onSubmit={handleSubmit}
                  />
                  <Grid
                    spreadsheet={spreadsheet}
                    activeCell={activeCell}
                    onCellClick={handleCellClick}
                  />
                </div>
              );
            }

      pitfalls:
        - "Performance with many cells (virtualize)"
        - "Keyboard navigation (arrow keys, Tab, Enter)"
        - "Copy/paste support"

      concepts:
        - Grid rendering
        - Cell editing UX
        - Formula bar vs cell display

      estimated_hours: 4-6

# =============================================================================
# PROJECT: BUILD YOUR OWN DNS SERVER
# =============================================================================
# Based on: RFC 1035, CodeCrafters DNS, Emil Hernvall's dnsguide
build-dns:
  id: "build-dns"
  name: "Build Your Own DNS Server"
  domain: "networking"
  difficulty: "expert"
  languages: ["rust", "go", "python", "c"]

  description: |
    Implement a DNS server that can respond to queries and recursively resolve
    domain names. Learn about the DNS protocol, UDP networking, and caching.

  learning_outcomes:
    - Understand DNS protocol and packet format
    - Implement recursive resolution
    - Handle UDP server programming
    - Build efficient caching

  prerequisites:
    - UDP socket programming
    - Binary data parsing
    - Understanding of domain name hierarchy
    - Basic caching concepts

  resources:
    primary:
      - name: "CodeCrafters DNS Challenge"
        url: "https://app.codecrafters.io/courses/dns-server/overview"
      - name: "DNS Guide (Rust)"
        url: "https://github.com/EmilHernvall/dnsguide"
    reference:
      - name: "RFC 1035 - DNS Protocol"
        url: "https://www.rfc-editor.org/rfc/rfc1035"
      - name: "How DNS Works"
        url: "https://howdns.works/"

  milestones:
    - id: dns-01
      name: UDP Server & Packet Parsing
      description: |
        Create a UDP server that listens on port 53 and parses DNS query packets.

      acceptance_criteria:
        - Bind to UDP port 53
        - Receive DNS query packets
        - Parse DNS header (ID, flags, counts)
        - Parse question section (domain name, type, class)

      hints:
        - level1: "DNS uses UDP port 53. Domain names are encoded as length-prefixed labels."
        - level2: |
            DNS header is 12 bytes:
            - ID (2 bytes), Flags (2 bytes)
            - QDCOUNT, ANCOUNT, NSCOUNT, ARCOUNT (2 bytes each)

            Domain names: 3www6google3com0 = www.google.com
        - level3: |
            struct DNSHeader {
              uint16_t id;
              uint16_t flags;  // QR, Opcode, AA, TC, RD, RA, Z, RCODE
              uint16_t qdcount;
              uint16_t ancount;
              uint16_t nscount;
              uint16_t arcount;
            };

            def parse_domain_name(data, offset):
                labels = []
                while data[offset] != 0:
                    length = data[offset]
                    if length & 0xC0 == 0xC0:  # Compression pointer
                        pointer = ((length & 0x3F) << 8) | data[offset + 1]
                        return parse_domain_name(data, pointer)[0], offset + 2
                    labels.append(data[offset+1:offset+1+length].decode())
                    offset += length + 1
                return '.'.join(labels), offset + 1

      pitfalls:
        - "Domain name compression uses pointers (0xC0 prefix)"
        - "Network byte order (big-endian)"
        - "Need root/sudo for port 53"

      concepts:
        - DNS packet format
        - Domain name encoding
        - UDP server basics

      estimated_hours: 3-4

    - id: dns-02
      name: Building DNS Responses
      description: |
        Construct valid DNS response packets with answer records.
        Handle A records (IPv4 addresses).

      acceptance_criteria:
        - Build response header with correct flags
        - Include original question in response
        - Add answer records (A type)
        - Valid response passes dig/nslookup

      hints:
        - level1: "Response mirrors the question, then adds answer section. Set QR=1 (response)."
        - level2: |
            Answer record format:
            - Name (domain, can use compression)
            - Type (2 bytes, A=1)
            - Class (2 bytes, IN=1)
            - TTL (4 bytes)
            - RDLENGTH (2 bytes)
            - RDATA (4 bytes for A record = IP address)
        - level3: |
            def build_response(query, answers):
                response = bytearray()

                # Header
                response += struct.pack('>H', query.id)
                flags = 0x8000  # QR=1 (response)
                flags |= 0x0080  # RA=1 (recursion available)
                response += struct.pack('>H', flags)
                response += struct.pack('>HHHH', 1, len(answers), 0, 0)

                # Question (copy from query)
                response += query.question_bytes

                # Answers
                for name, ip, ttl in answers:
                    response += encode_domain(name)
                    response += struct.pack('>HHIH', 1, 1, ttl, 4)  # A, IN, TTL, len
                    response += socket.inet_aton(ip)

                return response

      pitfalls:
        - "Must copy question section exactly"
        - "Compression pointers help reduce packet size"
        - "RCODE must be 0 for success"

      concepts:
        - DNS response structure
        - Resource record format
        - Name compression

      estimated_hours: 3-4

    - id: dns-03
      name: Forwarding Resolver
      description: |
        Forward queries to an upstream DNS server (like 8.8.8.8) and relay responses.

      acceptance_criteria:
        - Forward query to upstream server
        - Receive upstream response
        - Relay response back to client
        - Handle timeouts

      hints:
        - level1: "Send the query (possibly modified) to 8.8.8.8:53, wait for response, send back to client."
        - level2: |
            Forwarding flow:
            1. Receive query from client
            2. Create new query with new ID (for tracking)
            3. Send to upstream (8.8.8.8)
            4. Receive response, swap ID back
            5. Send to original client
        - level3: |
            def forward_query(query, upstream="8.8.8.8"):
                sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                sock.settimeout(5.0)

                # Send query to upstream
                sock.sendto(query.raw_bytes, (upstream, 53))

                # Wait for response
                try:
                    response, _ = sock.recvfrom(512)
                    return response
                except socket.timeout:
                    return build_servfail_response(query)

      pitfalls:
        - "Must handle upstream timeout"
        - "Query ID should be tracked for matching"
        - "512 byte UDP limit (use EDNS for larger)"

      concepts:
        - DNS forwarding
        - Timeout handling
        - Query/response matching

      estimated_hours: 2-3

    - id: dns-04
      name: Recursive Resolution
      description: |
        Implement full recursive resolution starting from root servers.
        Follow referrals through the DNS hierarchy.

      acceptance_criteria:
        - Start from root servers (a.root-servers.net, etc.)
        - Follow NS referrals
        - Handle glue records
        - Reach authoritative answer

      hints:
        - level1: "Root  TLD (.com)  Authoritative (google.com). Follow NS records in authority section."
        - level2: |
            Recursive resolution:
            1. Ask root for google.com  get .com NS
            2. Ask .com NS for google.com  get google.com NS
            3. Ask google.com NS  get A record (answer)

            Glue records: IP addresses in additional section
        - level3: |
            def resolve_recursive(domain, qtype='A'):
                # Start from root
                nameservers = ROOT_SERVERS  # ['198.41.0.4', ...]

                while True:
                    for ns in nameservers:
                        response = query_server(ns, domain, qtype)

                        if response.has_answer():
                            return response.answers

                        if response.has_authority():
                            # Get NS records and their IPs (glue)
                            new_ns = []
                            for ns_record in response.authority:
                                if ns_record.type == 'NS':
                                    # Look for glue in additional
                                    ip = find_glue(response.additional, ns_record.data)
                                    if ip:
                                        new_ns.append(ip)
                            if new_ns:
                                nameservers = new_ns
                                break

      pitfalls:
        - "Infinite loop protection (max iterations)"
        - "Glue records may be missing (need separate lookup)"
        - "CNAME chaining"

      concepts:
        - DNS hierarchy
        - Referral following
        - Glue records

      estimated_hours: 5-6

    - id: dns-05
      name: Caching
      description: |
        Add response caching with TTL-based expiration.
        Cache answers, authority, and negative responses.

      acceptance_criteria:
        - Cache successful answers
        - Respect TTL for expiration
        - Cache negative responses (NXDOMAIN)
        - Return cached results when valid

      hints:
        - level1: "Store responses keyed by (domain, type). Check TTL before returning."
        - level2: |
            Cache entry:
            - Key: (domain, qtype)
            - Value: answer records
            - Expires: time.now() + min(record TTLs)

            Negative caching: store NXDOMAIN with SOA minimum TTL
        - level3: |
            class DNSCache:
                def __init__(self):
                    self.cache = {}

                def get(self, domain, qtype):
                    key = (domain.lower(), qtype)
                    if key in self.cache:
                        entry = self.cache[key]
                        if time.time() < entry['expires']:
                            # Adjust TTLs in returned records
                            remaining = int(entry['expires'] - time.time())
                            return self._adjust_ttl(entry['records'], remaining)
                        else:
                            del self.cache[key]
                    return None

                def put(self, domain, qtype, records):
                    if not records:
                        return
                    min_ttl = min(r.ttl for r in records)
                    self.cache[(domain.lower(), qtype)] = {
                        'records': records,
                        'expires': time.time() + min_ttl
                    }

      pitfalls:
        - "Must decrement TTL in cached responses"
        - "Don't cache records with TTL=0"
        - "Cache size limits to prevent memory exhaustion"

      concepts:
        - TTL-based caching
        - Negative caching
        - Cache invalidation

      estimated_hours: 3-4

    - id: dns-06
      name: Multiple Record Types
      description: |
        Support additional record types: AAAA, CNAME, MX, TXT, NS.

      acceptance_criteria:
        - Parse and respond to AAAA queries (IPv6)
        - Handle CNAME records (follow chain)
        - Support MX records (mail servers)
        - Support TXT records

      hints:
        - level1: "Each type has different RDATA format. AAAA=16 bytes, MX=preference+domain, TXT=length+string."
        - level2: |
            Record types:
            - A (1): 4-byte IPv4
            - AAAA (28): 16-byte IPv6
            - CNAME (5): domain name
            - MX (15): 2-byte preference + domain
            - TXT (16): length-prefixed strings
            - NS (2): domain name
        - level3: |
            def parse_rdata(rtype, rdata, packet):
                if rtype == 1:  # A
                    return socket.inet_ntoa(rdata)
                elif rtype == 28:  # AAAA
                    return socket.inet_ntop(socket.AF_INET6, rdata)
                elif rtype == 5:  # CNAME
                    return parse_domain_name(packet, rdata_offset)[0]
                elif rtype == 15:  # MX
                    preference = struct.unpack('>H', rdata[:2])[0]
                    exchange = parse_domain_name(packet, rdata_offset + 2)[0]
                    return (preference, exchange)
                elif rtype == 16:  # TXT
                    texts = []
                    i = 0
                    while i < len(rdata):
                        length = rdata[i]
                        texts.append(rdata[i+1:i+1+length].decode())
                        i += length + 1
                    return texts

      pitfalls:
        - "CNAME requires following the chain"
        - "MX has priority/preference sorting"
        - "TXT can have multiple strings"

      concepts:
        - DNS record types
        - RDATA parsing
        - CNAME resolution

      estimated_hours: 3-4

    - id: dns-07
      name: Authoritative Zone
      description: |
        Serve as authoritative server for a zone.
        Load zone file and respond authoritatively.

      acceptance_criteria:
        - Parse zone file format
        - Respond with AA (Authoritative Answer) flag
        - Handle SOA queries
        - NXDOMAIN for non-existent names

      hints:
        - level1: "Zone file has SOA, NS, and resource records. Set AA=1 for authoritative responses."
        - level2: |
            Zone file format (simplified):
            $ORIGIN example.com.
            @       SOA  ns1.example.com. admin.example.com. (serial refresh retry expire minimum)
            @       NS   ns1.example.com.
            @       A    93.184.216.34
            www     A    93.184.216.34
            mail    MX   10 mail.example.com.
        - level3: |
            class Zone:
                def __init__(self, origin, records):
                    self.origin = origin
                    self.records = {}  # {(name, type): [records]}

                def lookup(self, name, qtype):
                    # Normalize name relative to origin
                    if name.endswith(self.origin):
                        relative = name[:-len(self.origin)-1] or '@'
                    else:
                        return None, 'NXDOMAIN'

                    key = (relative.lower(), qtype)
                    if key in self.records:
                        return self.records[key], 'NOERROR'

                    # Check if name exists with other types
                    for (n, t), recs in self.records.items():
                        if n == relative.lower():
                            return [], 'NOERROR'  # Name exists, no records of this type

                    return None, 'NXDOMAIN'

      pitfalls:
        - "SOA serial must increment on changes"
        - "NXDOMAIN vs NODATA (name exists but no records of type)"
        - "Wildcard records (*)"

      concepts:
        - Authoritative DNS
        - Zone files
        - DNS response codes

      estimated_hours: 4-5

# =============================================================================
# PROJECT: BUILD YOUR OWN REGEX ENGINE
# =============================================================================
# Based on: Thompson's construction, Russ Cox articles
build-regex:
  id: "build-regex"
  name: "Build Your Own Regex Engine"
  domain: "compilers"
  difficulty: "expert"
  languages: ["c", "rust", "go", "python", "javascript"]

  description: |
    Implement a regular expression engine using finite automata.
    Learn about NFA, DFA, and Thompson's construction algorithm.

  learning_outcomes:
    - Understand finite automata (NFA/DFA)
    - Implement Thompson's construction
    - Convert NFA to DFA (subset construction)
    - Handle regex syntax and parsing

  prerequisites:
    - Basic parsing (tokenization)
    - Graph traversal
    - Set operations
    - Understanding of formal languages

  resources:
    primary:
      - name: "Russ Cox - Implementing Regular Expressions"
        url: "https://swtch.com/~rsc/regexp/"
      - name: "Thompson's Construction Algorithm"
        url: "https://en.wikipedia.org/wiki/Thompson%27s_construction"
    reference:
      - name: "Denis Kyashif - Implementing a Regex Engine"
        url: "https://deniskyashif.com/2019/02/17/implementing-a-regular-expression-engine/"
      - name: "Visualizing Thompson's Construction"
        url: "https://medium.com/swlh/visualizing-thompsons-construction-algorithm-for-nfas-step-by-step-f92ef378581b"

  milestones:
    - id: regex-01
      name: Regex Parsing
      description: |
        Parse regex syntax into an AST. Handle concatenation, alternation (|),
        and quantifiers (*, +, ?).

      acceptance_criteria:
        - Tokenize regex string
        - Build AST for concatenation
        - Handle alternation (a|b)
        - Handle quantifiers (*, +, ?)
        - Support grouping with parentheses

      hints:
        - level1: "Convert to postfix notation first (like Shunting-yard), then build AST."
        - level2: |
            Operators by precedence (low to high):
            - Alternation: |
            - Concatenation: (implicit)
            - Quantifiers: *, +, ?

            Make concatenation explicit: "ab"  "a.b"
        - level3: |
            def to_postfix(regex):
                # Add explicit concatenation operator
                explicit = add_concat_operator(regex)
                output = []
                op_stack = []
                precedence = {'|': 1, '.': 2, '*': 3, '+': 3, '?': 3}

                for c in explicit:
                    if c.isalnum() or c == '':
                        output.append(c)
                    elif c == '(':
                        op_stack.append(c)
                    elif c == ')':
                        while op_stack and op_stack[-1] != '(':
                            output.append(op_stack.pop())
                        op_stack.pop()  # Remove '('
                    else:  # Operator
                        while (op_stack and op_stack[-1] != '(' and
                               precedence.get(op_stack[-1], 0) >= precedence.get(c, 0)):
                            output.append(op_stack.pop())
                        op_stack.append(c)

                while op_stack:
                    output.append(op_stack.pop())
                return ''.join(output)

      pitfalls:
        - "Concatenation is implicit in regex syntax"
        - "Escape characters (\\*, \\+)"
        - "Empty regex and empty groups"

      concepts:
        - Regex syntax
        - Shunting-yard algorithm
        - AST construction

      estimated_hours: 3-4

    - id: regex-02
      name: NFA Construction (Thompson's)
      description: |
        Build an NFA from the regex AST using Thompson's construction.
        Each operator has a specific NFA template.

      acceptance_criteria:
        - NFA state representation
        - Thompson construction for literals
        - Thompson construction for concatenation
        - Thompson construction for alternation
        - Thompson construction for Kleene star

      hints:
        - level1: "Each character creates a 2-state NFA. Combine using templates for |, ., *"
        - level2: |
            Thompson's templates:
            - Literal 'a': start --a--> accept
            - Concatenation: connect end of first to start of second
            - Alternation: new start with  to both, both ends to new accept
            - Kleene star: -loop with new start/accept
        - level3: |
            class State:
                def __init__(self):
                    self.transitions = {}  # {char: [states]} or {'': [states]}
                    self.is_accept = False

            def thompson_literal(char):
                start = State()
                accept = State()
                accept.is_accept = True
                start.transitions[char] = [accept]
                return start, accept

            def thompson_concat(nfa1, nfa2):
                # Connect nfa1's accept to nfa2's start via 
                nfa1[1].is_accept = False
                nfa1[1].transitions.setdefault('', []).append(nfa2[0])
                return nfa1[0], nfa2[1]

            def thompson_alternation(nfa1, nfa2):
                start = State()
                accept = State()
                accept.is_accept = True
                start.transitions[''] = [nfa1[0], nfa2[0]]
                nfa1[1].is_accept = False
                nfa2[1].is_accept = False
                nfa1[1].transitions.setdefault('', []).append(accept)
                nfa2[1].transitions.setdefault('', []).append(accept)
                return start, accept

            def thompson_star(nfa):
                start = State()
                accept = State()
                accept.is_accept = True
                start.transitions[''] = [nfa[0], accept]
                nfa[1].is_accept = False
                nfa[1].transitions.setdefault('', []).extend([nfa[0], accept])
                return start, accept

      pitfalls:
        - "-transitions are key to Thompson's elegance"
        - "Each NFA fragment has exactly one start and one accept"
        - "Don't forget to clear is_accept when combining"

      concepts:
        - Thompson's construction
        - -transitions
        - NFA structure

      estimated_hours: 4-5

    - id: regex-03
      name: NFA Simulation
      description: |
        Simulate NFA execution on input strings.
        Use -closure for handling epsilon transitions.

      acceptance_criteria:
        - Compute -closure of a state set
        - Simulate NFA step by step
        - Determine if input is accepted
        - Handle backtracking via state sets

      hints:
        - level1: "Track all possible current states (a set). For each char, compute next states + -closure."
        - level2: |
            NFA simulation:
            1. Start with -closure({start})
            2. For each input character:
               - Find all states reachable via that character
               - Compute -closure of those states
            3. Accept if final set contains an accept state
        - level3: |
            def epsilon_closure(states):
                closure = set(states)
                stack = list(states)
                while stack:
                    state = stack.pop()
                    for next_state in state.transitions.get('', []):
                        if next_state not in closure:
                            closure.add(next_state)
                            stack.append(next_state)
                return closure

            def simulate_nfa(nfa_start, input_string):
                current = epsilon_closure({nfa_start})

                for char in input_string:
                    next_states = set()
                    for state in current:
                        for next_state in state.transitions.get(char, []):
                            next_states.add(next_state)
                    current = epsilon_closure(next_states)

                    if not current:
                        return False

                return any(s.is_accept for s in current)

      pitfalls:
        - "-closure must be computed at start and after each step"
        - "Empty current set means no match"
        - "This is O(n*m) where n=input length, m=NFA states"

      concepts:
        - -closure computation
        - Set-based simulation
        - NFA acceptance

      estimated_hours: 3-4

    - id: regex-04
      name: NFA to DFA (Subset Construction)
      description: |
        Convert NFA to DFA using the subset construction algorithm.
        Each DFA state represents a set of NFA states.

      acceptance_criteria:
        - Implement subset construction
        - DFA states are NFA state sets
        - DFA has no -transitions
        - DFA accepts same language as NFA

      hints:
        - level1: "Each DFA state = frozenset of NFA states. Compute transitions for each unique set."
        - level2: |
            Subset construction:
            1. DFA start = -closure(NFA start)
            2. For each unmarked DFA state:
               - For each symbol in alphabet:
                 - Compute next NFA states
                 - -closure gives new DFA state
               - Add new DFA states to worklist
            3. DFA state is accept if contains any NFA accept
        - level3: |
            def nfa_to_dfa(nfa_start, alphabet):
                dfa_states = {}
                start_closure = frozenset(epsilon_closure({nfa_start}))
                dfa_states[start_closure] = {'transitions': {}}
                worklist = [start_closure]

                while worklist:
                    current = worklist.pop()
                    for symbol in alphabet:
                        next_nfa = set()
                        for nfa_state in current:
                            next_nfa.update(nfa_state.transitions.get(symbol, []))
                        if next_nfa:
                            next_closure = frozenset(epsilon_closure(next_nfa))
                            if next_closure not in dfa_states:
                                dfa_states[next_closure] = {'transitions': {}}
                                worklist.append(next_closure)
                            dfa_states[current]['transitions'][symbol] = next_closure

                # Mark accept states
                for state_set, state_data in dfa_states.items():
                    state_data['is_accept'] = any(s.is_accept for s in state_set)

                return start_closure, dfa_states

      pitfalls:
        - "DFA can have exponential states (2^n worst case)"
        - "Use frozenset for hashable state representation"
        - "Dead states (no outgoing transitions) are valid"

      concepts:
        - Subset construction
        - Powerset states
        - DFA determinism

      estimated_hours: 4-5

    - id: regex-05
      name: DFA Minimization
      description: |
        Minimize DFA by merging equivalent states.
        Use Hopcroft's or table-filling algorithm.

      acceptance_criteria:
        - Identify distinguishable states
        - Merge equivalent states
        - Produce minimal DFA
        - Preserve language acceptance

      hints:
        - level1: "Two states are equivalent if they have same accept status and same transitions to equivalent states."
        - level2: |
            Table-filling algorithm:
            1. Mark (accept, non-accept) pairs as distinguishable
            2. Repeat: mark (s1, s2) if  symbol where ((s1,a), (s2,a)) is marked
            3. Unmarked pairs are equivalent - merge them
        - level3: |
            def minimize_dfa(dfa_start, dfa_states, alphabet):
                states = list(dfa_states.keys())
                n = len(states)
                distinguishable = [[False] * n for _ in range(n)]

                # Initial: accept vs non-accept
                for i in range(n):
                    for j in range(i + 1, n):
                        if dfa_states[states[i]]['is_accept'] != dfa_states[states[j]]['is_accept']:
                            distinguishable[i][j] = True

                # Iterate until no changes
                changed = True
                while changed:
                    changed = False
                    for i in range(n):
                        for j in range(i + 1, n):
                            if distinguishable[i][j]:
                                continue
                            for symbol in alphabet:
                                next_i = dfa_states[states[i]]['transitions'].get(symbol)
                                next_j = dfa_states[states[j]]['transitions'].get(symbol)
                                if next_i and next_j:
                                    idx_i, idx_j = states.index(next_i), states.index(next_j)
                                    if idx_i > idx_j:
                                        idx_i, idx_j = idx_j, idx_i
                                    if distinguishable[idx_i][idx_j]:
                                        distinguishable[i][j] = True
                                        changed = True
                                        break

                # Merge equivalent states
                # ... (group unmarked pairs, create merged DFA)

      pitfalls:
        - "Dead states should be handled specially"
        - "Start state must be preserved"
        - "O(n) space for distinguishability table"

      concepts:
        - State equivalence
        - DFA minimization
        - Myhill-Nerode theorem

      estimated_hours: 4-5

    - id: regex-06
      name: Extended Regex Features
      description: |
        Add common regex features: character classes, anchors, capturing groups.

      acceptance_criteria:
        - Character classes [a-z], [^0-9]
        - Anchors ^ (start) and $ (end)
        - Dot (.) matches any character
        - Capturing groups and backreferences (optional)

      hints:
        - level1: "Character classes expand to alternation. Anchors are special states."
        - level2: |
            Implementation:
            - [a-z]  expand to (a|b|c|...|z) or use range check
            - [^...]  negated class
            - .  match any except newline
            - ^  only match at position 0
            - $  only match at end
        - level3: |
            class CharClass:
                def __init__(self, chars, negated=False):
                    self.chars = set(chars)
                    self.negated = negated

                def matches(self, char):
                    in_class = char in self.chars
                    return not in_class if self.negated else in_class

            def parse_char_class(regex, pos):
                # regex[pos] == '['
                negated = regex[pos + 1] == '^'
                start = pos + 2 if negated else pos + 1
                chars = set()
                i = start
                while regex[i] != ']':
                    if i + 2 < len(regex) and regex[i + 1] == '-':
                        # Range a-z
                        for c in range(ord(regex[i]), ord(regex[i + 2]) + 1):
                            chars.add(chr(c))
                        i += 3
                    else:
                        chars.add(regex[i])
                        i += 1
                return CharClass(chars, negated), i + 1

      pitfalls:
        - "Range handling: a-z vs a- (literal dash)"
        - "Escaping inside character classes"
        - "Backreferences make regex non-regular (exponential)"

      concepts:
        - Regex extensions
        - Character classes
        - Anchor handling

      estimated_hours: 4-5

# =============================================================================
# PROJECT: BUILD YOUR OWN LSP SERVER
# =============================================================================
# Based on: Microsoft LSP Specification, tower-lsp, LSP4J
build-lsp:
  id: "build-lsp"
  name: "Build Your Own LSP Server"
  domain: "dev-tools"
  difficulty: "expert"
  languages: ["rust", "typescript", "python", "java", "go"]

  description: |
    Implement a Language Server Protocol server that provides IDE features
    like autocomplete, go-to-definition, and diagnostics.

  learning_outcomes:
    - Understand LSP protocol and JSON-RPC
    - Implement document synchronization
    - Provide code intelligence features
    - Handle editor communication

  prerequisites:
    - JSON-RPC basics
    - AST manipulation
    - Understanding of IDE features
    - Basic language parsing

  resources:
    primary:
      - name: "LSP Specification"
        url: "https://microsoft.github.io/language-server-protocol/"
      - name: "VS Code Language Server Guide"
        url: "https://code.visualstudio.com/api/language-extensions/language-server-extension-guide"
    reference:
      - name: "tower-lsp (Rust)"
        url: "https://github.com/ebkalderon/tower-lsp"
      - name: "LSP4J (Java)"
        url: "https://github.com/eclipse/lsp4j"
      - name: "Toptal LSP Tutorial"
        url: "https://www.toptal.com/javascript/language-server-protocol-tutorial"

  milestones:
    - id: lsp-01
      name: JSON-RPC Transport
      description: |
        Implement the JSON-RPC 2.0 transport layer with LSP headers.
        Handle stdin/stdout communication.

      acceptance_criteria:
        - Parse LSP message headers (Content-Length)
        - Read/write JSON-RPC messages
        - Handle request/response/notification
        - Proper error responses

      hints:
        - level1: "LSP uses headers like HTTP: Content-Length: N\\r\\n\\r\\n{json}"
        - level2: |
            JSON-RPC format:
            - Request: {jsonrpc: "2.0", id: 1, method: "...", params: {...}}
            - Response: {jsonrpc: "2.0", id: 1, result: {...}}
            - Notification: {jsonrpc: "2.0", method: "...", params: {...}} (no id)
            - Error: {jsonrpc: "2.0", id: 1, error: {code: ..., message: ...}}
        - level3: |
            import json
            import sys

            def read_message():
                # Read headers
                headers = {}
                while True:
                    line = sys.stdin.readline().strip()
                    if not line:
                        break
                    key, value = line.split(': ', 1)
                    headers[key] = value

                # Read content
                length = int(headers['Content-Length'])
                content = sys.stdin.read(length)
                return json.loads(content)

            def write_message(msg):
                content = json.dumps(msg)
                header = f"Content-Length: {len(content)}\r\n\r\n"
                sys.stdout.write(header + content)
                sys.stdout.flush()

            def send_response(id, result):
                write_message({"jsonrpc": "2.0", "id": id, "result": result})

      pitfalls:
        - "Content-Length is in bytes, not characters (UTF-8)"
        - "Headers end with \\r\\n\\r\\n"
        - "Notifications have no id field"

      concepts:
        - JSON-RPC 2.0
        - LSP transport layer
        - Stdin/stdout IPC

      estimated_hours: 2-3

    - id: lsp-02
      name: Initialize and Shutdown
      description: |
        Handle LSP lifecycle: initialize, initialized, shutdown, exit.
        Report server capabilities.

      acceptance_criteria:
        - Handle initialize request
        - Return server capabilities
        - Handle initialized notification
        - Proper shutdown sequence

      hints:
        - level1: "Initialize returns capabilities object describing what features you support."
        - level2: |
            Lifecycle:
            1. Client sends 'initialize' request
            2. Server returns capabilities
            3. Client sends 'initialized' notification
            4. ... normal operation ...
            5. Client sends 'shutdown' request
            6. Client sends 'exit' notification
        - level3: |
            def handle_initialize(params):
                return {
                    "capabilities": {
                        "textDocumentSync": {
                            "openClose": True,
                            "change": 1,  # Full sync
                        },
                        "completionProvider": {
                            "triggerCharacters": ["."]
                        },
                        "hoverProvider": True,
                        "definitionProvider": True,
                        "referencesProvider": True,
                        "documentSymbolProvider": True,
                    }
                }

            def handle_shutdown():
                return None  # Success

            def handle_exit():
                sys.exit(0)

      pitfalls:
        - "Must not process requests before initialize"
        - "Shutdown returns null, not empty object"
        - "Exit without shutdown should return code 1"

      concepts:
        - LSP lifecycle
        - Server capabilities
        - Protocol handshake

      estimated_hours: 2-3

    - id: lsp-03
      name: Document Synchronization
      description: |
        Track open documents and their contents.
        Handle open, change, close notifications.

      acceptance_criteria:
        - Handle textDocument/didOpen
        - Handle textDocument/didChange (full sync)
        - Handle textDocument/didClose
        - Maintain document state

      hints:
        - level1: "Keep a dict of {uri: content}. Update on didChange."
        - level2: |
            Document events:
            - didOpen: new document with initial content
            - didChange: contentChanges array (full or incremental)
            - didClose: remove from tracking

            Full sync (change=1): entire content in each change
            Incremental (change=2): only changed ranges
        - level3: |
            class DocumentStore:
                def __init__(self):
                    self.documents = {}

                def open(self, uri, text, version):
                    self.documents[uri] = {
                        'content': text,
                        'version': version
                    }

                def change(self, uri, changes, version):
                    # Full sync - just replace
                    if changes:
                        self.documents[uri] = {
                            'content': changes[-1]['text'],
                            'version': version
                        }

                def close(self, uri):
                    if uri in self.documents:
                        del self.documents[uri]

                def get_content(self, uri):
                    return self.documents.get(uri, {}).get('content')

      pitfalls:
        - "URI vs file path conversion"
        - "Version numbers for conflict detection"
        - "Incremental sync is more complex but efficient"

      concepts:
        - Document synchronization
        - Content tracking
        - Incremental updates

      estimated_hours: 2-3

    - id: lsp-04
      name: Diagnostics (Errors/Warnings)
      description: |
        Publish diagnostics (errors, warnings) for documents.
        Parse code and report issues.

      acceptance_criteria:
        - Parse document for errors
        - Publish diagnostics notification
        - Include line/column positions
        - Clear diagnostics when fixed

      hints:
        - level1: "After each change, re-parse and send textDocument/publishDiagnostics."
        - level2: |
            Diagnostic structure:
            - range: {start: {line, character}, end: {line, character}}
            - severity: 1=Error, 2=Warning, 3=Info, 4=Hint
            - source: your server name
            - message: description
        - level3: |
            def publish_diagnostics(uri, content):
                diagnostics = []
                try:
                    ast = parse(content)
                    # Semantic analysis
                    errors = analyze(ast)
                    for error in errors:
                        diagnostics.append({
                            "range": {
                                "start": {"line": error.line, "character": error.col},
                                "end": {"line": error.line, "character": error.col + len(error.token)}
                            },
                            "severity": 1,  # Error
                            "source": "my-lsp",
                            "message": error.message
                        })
                except ParseError as e:
                    diagnostics.append({
                        "range": {"start": {"line": e.line, "character": e.col},
                                  "end": {"line": e.line, "character": e.col + 1}},
                        "severity": 1,
                        "source": "my-lsp",
                        "message": str(e)
                    })

                send_notification("textDocument/publishDiagnostics", {
                    "uri": uri,
                    "diagnostics": diagnostics
                })

      pitfalls:
        - "Lines and characters are 0-indexed"
        - "Must send empty array to clear diagnostics"
        - "Debounce on rapid changes"

      concepts:
        - Diagnostic reporting
        - Error recovery in parsing
        - Real-time analysis

      estimated_hours: 3-4

    - id: lsp-05
      name: Completion (Autocomplete)
      description: |
        Provide code completion suggestions at cursor position.
        Handle completion item details.

      acceptance_criteria:
        - Handle textDocument/completion request
        - Return completion items
        - Filter based on prefix
        - Include item details (kind, documentation)

      hints:
        - level1: "Get cursor position, find prefix, return matching symbols."
        - level2: |
            CompletionItem fields:
            - label: what user sees
            - kind: 1=Text, 2=Method, 3=Function, 6=Variable, 7=Class...
            - detail: type signature
            - documentation: description
            - insertText: what gets inserted (may differ from label)
        - level3: |
            def handle_completion(params):
                uri = params['textDocument']['uri']
                line = params['position']['line']
                char = params['position']['character']

                content = documents.get_content(uri)
                lines = content.split('\n')
                current_line = lines[line][:char]

                # Find prefix (word before cursor)
                prefix = ''
                for c in reversed(current_line):
                    if c.isalnum() or c == '_':
                        prefix = c + prefix
                    else:
                        break

                # Get symbols in scope
                symbols = get_symbols_in_scope(uri, line)

                # Filter and return
                items = []
                for sym in symbols:
                    if sym.name.startswith(prefix):
                        items.append({
                            "label": sym.name,
                            "kind": symbol_kind_to_completion_kind(sym.kind),
                            "detail": sym.type_signature,
                            "documentation": sym.doc
                        })
                return items

      pitfalls:
        - "Trigger characters (like '.') need special handling"
        - "Completion context (invoked vs triggered)"
        - "Performance on large symbol tables"

      concepts:
        - Code completion
        - Symbol resolution
        - Scope analysis

      estimated_hours: 4-5

    - id: lsp-06
      name: Go to Definition & References
      description: |
        Implement navigation features: go to definition, find references.
        Build symbol table with locations.

      acceptance_criteria:
        - Handle textDocument/definition
        - Handle textDocument/references
        - Return location(s) for symbol
        - Handle multi-file projects

      hints:
        - level1: "Build symbol table with definition locations. On request, find symbol at cursor, return its location."
        - level2: |
            Location structure:
            {
              uri: "file:///...",
              range: {start: {line, character}, end: {line, character}}
            }

            For references, include the definition as well (unless excludeDeclaration).
        - level3: |
            class SymbolTable:
                def __init__(self):
                    self.symbols = {}  # {name: SymbolInfo}

                def add_definition(self, name, uri, range, kind):
                    if name not in self.symbols:
                        self.symbols[name] = {'definition': None, 'references': []}
                    self.symbols[name]['definition'] = {'uri': uri, 'range': range}
                    self.symbols[name]['kind'] = kind

                def add_reference(self, name, uri, range):
                    if name in self.symbols:
                        self.symbols[name]['references'].append({'uri': uri, 'range': range})

            def handle_definition(params):
                uri = params['textDocument']['uri']
                pos = params['position']
                symbol_name = get_symbol_at_position(uri, pos)
                if symbol_name and symbol_name in symbol_table.symbols:
                    defn = symbol_table.symbols[symbol_name]['definition']
                    if defn:
                        return defn
                return None

            def handle_references(params):
                uri = params['textDocument']['uri']
                pos = params['position']
                include_decl = params.get('context', {}).get('includeDeclaration', True)
                symbol_name = get_symbol_at_position(uri, pos)

                if symbol_name and symbol_name in symbol_table.symbols:
                    refs = symbol_table.symbols[symbol_name]['references'][:]
                    if include_decl and symbol_table.symbols[symbol_name]['definition']:
                        refs.insert(0, symbol_table.symbols[symbol_name]['definition'])
                    return refs
                return []

      pitfalls:
        - "Symbol resolution across files"
        - "Handling shadowed names"
        - "Rebuild symbol table on file changes"

      concepts:
        - Symbol tables
        - Definition/reference tracking
        - Cross-file navigation

      estimated_hours: 4-5

# =============================================================================
# PROJECT: BUILD YOUR OWN TEST FRAMEWORK
# =============================================================================
# Based on: pytest, Jest architecture
build-test-framework:
  id: "build-test-framework"
  name: "Build Your Own Test Framework"
  domain: "software-engineering"
  difficulty: "expert"
  languages: ["python", "javascript", "rust", "go"]

  description: |
    Implement a test framework with test discovery, assertions,
    fixtures, and reporting. Learn about testing architecture.

  learning_outcomes:
    - Understand test framework architecture
    - Implement test discovery and collection
    - Build assertion library
    - Handle fixtures and setup/teardown

  prerequisites:
    - Reflection/introspection in your language
    - Basic testing concepts
    - CLI development
    - Understanding of decorators/attributes

  resources:
    primary:
      - name: "pytest internals"
        url: "https://docs.pytest.org/en/latest/how-to/writing_plugins.html"
      - name: "Jest architecture"
        url: "https://jestjs.io/docs/architecture"
    reference:
      - name: "Building a Testing Framework"
        url: "https://codewithoutrules.com/2016/10/15/testing-framework-from-scratch/"

  milestones:
    - id: test-01
      name: Test Discovery
      description: |
        Automatically discover test files and test functions/methods.
        Use naming conventions or decorators.

      acceptance_criteria:
        - Find test files (test_*.py or *_test.py)
        - Find test functions (def test_*)
        - Find test classes and methods
        - Support custom patterns

      hints:
        - level1: "Walk directory, filter by pattern, import modules, find functions starting with 'test_'."
        - level2: |
            Discovery steps:
            1. Glob for test files: test_*.py, *_test.py
            2. Import each module
            3. Inspect for test_* functions and Test* classes
            4. Collect into test items
        - level3: |
            import importlib.util
            import inspect
            from pathlib import Path

            def discover_tests(start_dir):
                tests = []
                for path in Path(start_dir).rglob('test_*.py'):
                    module = import_module_from_path(path)
                    for name, obj in inspect.getmembers(module):
                        if name.startswith('test_') and callable(obj):
                            tests.append(TestItem(module, name, obj))
                        elif inspect.isclass(obj) and name.startswith('Test'):
                            for method_name, method in inspect.getmembers(obj, inspect.isfunction):
                                if method_name.startswith('test_'):
                                    tests.append(TestItem(module, f"{name}.{method_name}", method, cls=obj))
                return tests

            def import_module_from_path(path):
                spec = importlib.util.spec_from_file_location(path.stem, path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                return module

      pitfalls:
        - "Import errors should be caught and reported"
        - "Test isolation (fresh imports)"
        - "Package __init__.py handling"

      concepts:
        - Test discovery
        - Module introspection
        - Naming conventions

      estimated_hours: 3-4

    - id: test-02
      name: Assertion Library
      description: |
        Implement assertion functions with helpful error messages.
        Show expected vs actual on failure.

      acceptance_criteria:
        - assert_equal(expected, actual)
        - assert_true/assert_false
        - assert_raises (exception testing)
        - Clear diff on failure

      hints:
        - level1: "Raise AssertionError with message showing expected vs actual."
        - level2: |
            Good assertion message:
            "Expected 5, got 3"
            "Expected [1, 2, 3], got [1, 2, 4]"
                                     ^

            For collections, show diff.
        - level3: |
            class AssertionError(Exception):
                pass

            def assert_equal(expected, actual, msg=None):
                if expected != actual:
                    diff = generate_diff(expected, actual)
                    message = msg or f"Expected {expected!r}, got {actual!r}"
                    if diff:
                        message += f"\nDiff:\n{diff}"
                    raise AssertionError(message)

            def assert_raises(exception_type, callable_obj, *args, **kwargs):
                try:
                    callable_obj(*args, **kwargs)
                except exception_type:
                    return  # Success
                except Exception as e:
                    raise AssertionError(f"Expected {exception_type.__name__}, got {type(e).__name__}: {e}")
                raise AssertionError(f"Expected {exception_type.__name__} but no exception raised")

            def generate_diff(expected, actual):
                if isinstance(expected, str) and isinstance(actual, str):
                    import difflib
                    return '\n'.join(difflib.unified_diff(
                        expected.splitlines(), actual.splitlines(),
                        fromfile='expected', tofile='actual', lineterm=''
                    ))
                return None

      pitfalls:
        - "repr() vs str() for accurate display"
        - "Floating point comparison (approx)"
        - "Deep equality for nested structures"

      concepts:
        - Assertion design
        - Error messages
        - Diff generation

      estimated_hours: 2-3

    - id: test-03
      name: Test Runner
      description: |
        Execute tests and collect results.
        Handle pass, fail, error, skip states.

      acceptance_criteria:
        - Run each test in isolation
        - Catch and record exceptions
        - Track pass/fail/error/skip counts
        - Measure test duration

      hints:
        - level1: "For each test, call it in try/except, record result."
        - level2: |
            Test states:
            - PASS: completed without exception
            - FAIL: AssertionError raised
            - ERROR: other exception raised
            - SKIP: marked to skip

            Record: test name, state, duration, exception info
        - level3: |
            import time
            import traceback

            class TestResult:
                def __init__(self, name, state, duration, exception=None):
                    self.name = name
                    self.state = state  # 'pass', 'fail', 'error', 'skip'
                    self.duration = duration
                    self.exception = exception
                    self.traceback = traceback.format_exc() if exception else None

            def run_tests(tests):
                results = []
                for test in tests:
                    if test.skip:
                        results.append(TestResult(test.name, 'skip', 0))
                        continue

                    start = time.time()
                    try:
                        if test.cls:
                            instance = test.cls()
                            if hasattr(instance, 'setup'):
                                instance.setup()
                            test.func(instance)
                            if hasattr(instance, 'teardown'):
                                instance.teardown()
                        else:
                            test.func()
                        state = 'pass'
                        exc = None
                    except AssertionError as e:
                        state = 'fail'
                        exc = e
                    except Exception as e:
                        state = 'error'
                        exc = e

                    duration = time.time() - start
                    results.append(TestResult(test.name, state, duration, exc))
                return results

      pitfalls:
        - "Setup/teardown should run even on failure"
        - "Capture stdout/stderr during tests"
        - "Test isolation (global state)"

      concepts:
        - Test execution
        - Exception handling
        - State tracking

      estimated_hours: 3-4

    - id: test-04
      name: Fixtures
      description: |
        Implement fixtures for setup/teardown and dependency injection.
        Support fixture scopes (function, class, module).

      acceptance_criteria:
        - Define fixtures with decorator
        - Inject fixtures into test functions
        - Support setup/teardown in fixtures
        - Handle fixture dependencies

      hints:
        - level1: "Fixtures are functions that return values. Inject by matching parameter names."
        - level2: |
            Fixture scopes:
            - function: new instance per test
            - class: shared within test class
            - module: shared within module
            - session: shared across all tests

            Fixtures can depend on other fixtures.
        - level3: |
            fixtures = {}

            def fixture(scope='function'):
                def decorator(func):
                    fixtures[func.__name__] = {
                        'func': func,
                        'scope': scope,
                        'cached': {},  # scope_key -> value
                    }
                    return func
                return decorator

            def get_fixture_value(name, scope_key):
                fix = fixtures[name]
                if fix['scope'] != 'function' and scope_key in fix['cached']:
                    return fix['cached'][scope_key]

                # Resolve fixture dependencies
                params = inspect.signature(fix['func']).parameters
                kwargs = {}
                for param in params:
                    if param in fixtures:
                        kwargs[param] = get_fixture_value(param, scope_key)

                value = fix['func'](**kwargs)
                if fix['scope'] != 'function':
                    fix['cached'][scope_key] = value
                return value

            def run_test_with_fixtures(test):
                params = inspect.signature(test.func).parameters
                kwargs = {}
                for param in params:
                    if param in fixtures:
                        kwargs[param] = get_fixture_value(param, get_scope_key(test))
                test.func(**kwargs)

      pitfalls:
        - "Fixture teardown (yield fixtures)"
        - "Circular fixture dependencies"
        - "Fixture scope caching"

      concepts:
        - Dependency injection
        - Fixture scoping
        - Setup/teardown patterns

      estimated_hours: 4-5

    - id: test-05
      name: Test Selection & Filtering
      description: |
        Allow running specific tests by name, pattern, or marker.
        Support -k (keyword) and -m (marker) filters.

      acceptance_criteria:
        - Filter by test name pattern
        - Support markers/tags (@pytest.mark.slow)
        - Run single test by full name
        - Exclude patterns

      hints:
        - level1: "Match test names against patterns. Markers are function attributes."
        - level2: |
            Selection options:
            - mytests.py::test_specific - run single test
            - -k "login" - run tests with "login" in name
            - -m "slow" - run tests marked @slow
            - --ignore pattern - exclude matching
        - level3: |
            def mark(name):
                def decorator(func):
                    if not hasattr(func, '_markers'):
                        func._markers = set()
                    func._markers.add(name)
                    return func
                return decorator

            def filter_tests(tests, keyword=None, markers=None, name_pattern=None):
                filtered = tests

                if keyword:
                    filtered = [t for t in filtered if keyword in t.name]

                if markers:
                    filtered = [t for t in filtered
                                if hasattr(t.func, '_markers') and markers & t.func._markers]

                if name_pattern:
                    import fnmatch
                    filtered = [t for t in filtered if fnmatch.fnmatch(t.name, name_pattern)]

                return filtered

      pitfalls:
        - "Marker inheritance in classes"
        - "Pattern escaping"
        - "Empty filter should run all"

      concepts:
        - Test filtering
        - Markers/tags
        - Pattern matching

      estimated_hours: 2-3

    - id: test-06
      name: Reporting & Output
      description: |
        Generate test reports with different formats.
        Show progress, failures, and summary.

      acceptance_criteria:
        - Progress output (dots or verbose)
        - Failure details with traceback
        - Summary statistics
        - JUnit XML output (optional)

      hints:
        - level1: "Print . for pass, F for fail, E for error. Show summary at end."
        - level2: |
            Output formats:
            - Dots: ...F..E. (default)
            - Verbose: test_name ... PASSED
            - JUnit XML: for CI integration

            Failure report: test name, assertion message, traceback
        - level3: |
            class Reporter:
                def __init__(self, verbose=False):
                    self.verbose = verbose
                    self.results = []

                def report_result(self, result):
                    self.results.append(result)
                    if self.verbose:
                        status = {'pass': 'PASSED', 'fail': 'FAILED', 'error': 'ERROR', 'skip': 'SKIPPED'}
                        print(f"{result.name} ... {status[result.state]}")
                    else:
                        symbols = {'pass': '.', 'fail': 'F', 'error': 'E', 'skip': 's'}
                        print(symbols[result.state], end='', flush=True)

                def print_summary(self):
                    print(f"\n\n{'='*60}")
                    passed = sum(1 for r in self.results if r.state == 'pass')
                    failed = sum(1 for r in self.results if r.state == 'fail')
                    errors = sum(1 for r in self.results if r.state == 'error')
                    skipped = sum(1 for r in self.results if r.state == 'skip')

                    # Print failures
                    for r in self.results:
                        if r.state in ('fail', 'error'):
                            print(f"\n{r.state.upper()}: {r.name}")
                            print(r.traceback)

                    total_time = sum(r.duration for r in self.results)
                    print(f"\n{passed} passed, {failed} failed, {errors} errors, {skipped} skipped in {total_time:.2f}s")

      pitfalls:
        - "Color output (terminal detection)"
        - "Parallel test output interleaving"
        - "Unicode in test names"

      concepts:
        - Test reporting
        - Output formatting
        - CI integration

      estimated_hours: 3-4

# =============================================================================
# METADATA
# =============================================================================
metadata:
  version: "2.0.0"
  last_updated: "2025-01-25"
  status: "Comprehensive - 11 domains + detailed expert projects"

  changes_v2:
    - "Added prerequisites section (DSA, Math requirements)"
    - "Added Software Engineering Practices domain (Testing, CI/CD, Observability)"
    - "Added bridge projects between Advanced and Expert levels"
    - "Moved Blockchain into Distributed Systems (per ACM/IEEE CS2023)"
    - "Added DNS Server, Regex Engine, LSP Server, Test Framework detailed specs"
    - "Added Rust/WebAssembly language options"

  stats:
    domains: 11
    total_projects: 100+
    expert_projects_detailed: 22
    expert_projects_basic: 12+
    total_milestones: 156
    bridge_projects: 15+

  expert_projects_with_full_details:
    - build-redis          # In-memory data store (8 milestones)
    - build-sqlite         # Embedded SQL database (10 milestones)
    - build-interpreter    # Lox language (10 milestones)
    - build-docker         # Container runtime (6 milestones)
    - build-shell          # Unix shell (6 milestones)
    - build-git            # Version control (8 milestones)
    - build-text-editor    # Terminal editor (7 milestones)
    - build-raytracer      # Path tracing renderer (10 milestones)
    - build-os             # Operating system kernel (8 milestones)
    - build-nn-framework   # Neural network / autograd (6 milestones)
    - build-gc             # Garbage collector (6 milestones)
    - build-game-engine    # 2D game engine with ECS (7 milestones)
    - build-allocator      # Memory allocator (6 milestones)
    - build-tcp-stack      # TCP/IP stack (7 milestones)
    - build-transformer    # Transformer architecture (7 milestones)
    - build-blockchain     # Cryptocurrency (6 milestones)
    - build-bittorrent     # P2P file sharing (7 milestones)
    - build-spreadsheet    # Excel clone (7 milestones)
    # NEW in v2.0
    - build-dns            # DNS server (7 milestones)
    - build-regex          # Regex engine (6 milestones)
    - build-lsp            # LSP server (6 milestones)
    - build-test-framework # Test framework (6 milestones)

  expert_projects_basic_only:
    - build-react          # Virtual DOM, reconciliation
    - build-bundler        # Module resolution, tree shaking
    - build-web-framework  # Express/Django clone
    - build-kafka          # Distributed message queue
    - build-raft           # Consensus algorithm
    - build-tls            # TLS 1.3
    - build-btree          # Disk-friendly tree
    - build-debugger       # GDB-like
    - build-emulator       # NES/GameBoy/CHIP-8
    - build-browser        # Browser engine
    - build-ci-system      # CI/CD system
    - build-observability  # Observability platform

  references:
    - name: "CodeCrafters"
      url: "https://codecrafters.io"
    - name: "Crafting Interpreters"
      url: "https://craftinginterpreters.com"
    - name: "Build Your Own X (GitHub)"
      url: "https://github.com/codecrafters-io/build-your-own-x"
    - name: "Let's Build a Simple Database"
      url: "https://cstack.github.io/db_tutorial/"
    - name: "Ray Tracing in One Weekend"
      url: "https://raytracing.github.io/"
    - name: "OSDev Wiki"
      url: "https://wiki.osdev.org"
    - name: "Writing an OS in Rust"
      url: "https://os.phil-opp.com/"
    - name: "Austin Henley's Challenging Projects"
      url: "https://austinhenley.com/blog/challengingprojects.html"
    - name: "Saminiir TCP/IP Stack"
      url: "https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/"
    - name: "Karpathy's nanoGPT"
      url: "https://github.com/karpathy/nanoGPT"
