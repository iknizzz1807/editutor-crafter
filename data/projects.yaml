domains:
- id: app-dev
  name: Application Development
  icon: üåê
  subdomains:
  - name: Web Frontend
  - name: Web Backend
  - name: Full-stack
  - name: Mobile
  - name: Desktop & CLI
  projects:
    beginner:
    - id: rest-api-design
      name: Production REST API
      description: Build a production-grade REST API with authentication, validation, rate limiting, and proper error handling.
      difficulty: beginner
      estimated_hours: 20-30
      essence: Request routing and middleware composition with stateless authentication tokens, combining HTTP protocol semantics, concurrent connection handling, and algorithmic rate limiting to process client requests through validation, authorization, and serialization layers before persisting state changes.
      why_important: Production REST APIs are the backbone of modern web and mobile applications, making this essential infrastructure that every backend developer must master. This project teaches industry-standard patterns for secure, scalable API design that directly applies to real-world backend engineering roles.
      learning_outcomes:
      - Implement CRUD operations with proper HTTP verb semantics and status codes
      - Design resource-based URL structures following REST architectural constraints
      - Build JWT-based authentication with token generation, validation, and refresh mechanisms
      - Implement role-based access control with middleware authorization layers
      - Validate and sanitize input data using schema validation libraries
      - Handle errors consistently with structured error responses and proper status codes
      - Apply rate limiting algorithms to prevent API abuse and ensure fair resource usage
      - Structure API responses with consistent data serialization and pagination
      skills:
      - RESTful Architecture
      - HTTP Protocol
      - JWT Authentication
      - Input Validation
      - Rate Limiting
      - Error Handling
      - API Security
      - Middleware Patterns
      tags:
      - api
      - beginner-friendly
      - crud
      - endpoints
      - go
      - node.js
      - python
      - validation
      - versioning
      architecture_doc: architecture-docs/rest-api-design/index.md
      languages:
        recommended:
        - Go
        - Python
        - Node.js
        also_possible:
        - Rust
        - Java
      resources:
      - name: REST API Design Best Practices
        url: https://restfulapi.net/
        type: tutorial
      - name: OpenAPI Specification
        url: https://swagger.io/specification/
        type: documentation
      prerequisites:
      - type: skill
        name: HTTP basics
      - type: skill
        name: JSON
      - type: skill
        name: Database basics
      milestones:
      - id: rest-api-design-m1
        name: CRUD Operations
        description: Implement Create, Read, Update, Delete operations with proper HTTP methods.
        acceptance_criteria:
        - POST /resources creates new resource and returns 201 Created
        - GET /resources returns paginated list of all resources
        - GET /resources/:id returns single resource or 404 Not Found
        - PUT/PATCH /resources/:id updates resource and returns new state
        - DELETE /resources/:id removes resource and returns 204 No Content
        pitfalls:
        - Using POST for updates or GET for mutations
        - Returning 200 for resource creation instead of 201
        - Not handling concurrent updates
        - Missing Content-Type headers
        concepts:
        - REST principles
        - HTTP methods
        - Status codes
        - Resource modeling
        skills:
        - HTTP API Design
        - Resource Modeling
        - Database Operations
        - JSON Serialization
        deliverables:
        - Resource endpoints following RESTful URL naming conventions
        - HTTP method mapping to create, read, update, and delete operations
        - Response formatting with consistent JSON structure and envelope
        - Appropriate HTTP status codes for success and error responses
        estimated_hours: 4-6
      - id: rest-api-design-m2
        name: Input Validation
        description: Validate all input data and return meaningful errors.
        acceptance_criteria:
        - Validate request body fields against defined JSON schema
        - Validate query parameters for type, range, and allowed values
        - Validate path parameters for correct format and existence
        - Return detailed error messages listing each validation failure
        - Sanitize all input to prevent injection and XSS attacks
        pitfalls:
        - Only validating types, not business rules
        - Exposing internal error details
        - Not validating query/path params
        - SQL/NoSQL injection through unvalidated input
        concepts:
        - Schema validation
        - Error handling
        - Input sanitization
        - Security
        skills:
        - Input Validation
        - Security Best Practices
        - Error Response Design
        - Data Sanitization
        deliverables:
        - Request body validation against JSON schema definition
        - Schema-based validation with type, format, and constraint checks
        - Structured error response listing all validation failures
        - Input sanitization removing dangerous characters and patterns
        estimated_hours: 4-6
      - id: rest-api-design-m3
        name: Authentication & Authorization
        description: Implement JWT authentication and role-based access control.
        acceptance_criteria:
        - Support user registration and login returning access tokens
        - Generate and validate JWT tokens with configurable expiration
        - Implement token refresh mechanism for long-lived sessions
        - Enforce role-based access control on protected endpoints
        - Protect routes requiring authentication with middleware guard
        pitfalls:
        - Storing secrets in code
        - No token expiration or too long expiry
        - Not invalidating tokens on password change
        - RBAC bypass through direct object access
        concepts:
        - JWT tokens
        - Password hashing
        - RBAC
        - Token refresh
        skills:
        - Authentication Systems
        - Security Token Management
        - Password Security
        - Access Control Implementation
        deliverables:
        - JWT-based authentication with token generation and validation
        - API key authentication option for service-to-service calls
        - Role-based access control restricting endpoints by user role
        - Protected route middleware rejecting unauthenticated requests
        estimated_hours: 6-8
      - id: rest-api-design-m4
        name: Rate Limiting & Throttling
        description: Protect API from abuse with rate limiting.
        acceptance_criteria:
        - Enforce per-user rate limits based on authentication identity
        - Enforce per-endpoint rate limits with different thresholds
        - Implement sliding window algorithm for smooth rate limiting
        - Include X-RateLimit headers showing limit, remaining, and reset time
        - Degrade gracefully under load by shedding lowest-priority requests
        pitfalls:
        - Rate limits per IP bypass with proxies
        - Not handling Redis failures gracefully
        - Clock skew in distributed systems
        - Rate limit headers revealing too much info
        concepts:
        - Rate limiting algorithms
        - Sliding window
        - Token bucket
        - Redis
        skills:
        - Rate Limiting Implementation
        - Distributed Caching
        - API Protection
        - Performance Optimization
        deliverables:
        - Rate limit middleware intercepting and counting requests
        - Standard rate limit headers included in all HTTP responses
        - HTTP 429 response body with error message and retry guidance
        - Per-client limit configuration supporting different tiers
        estimated_hours: 5-7
    intermediate:
    - id: blog-platform
      name: Blog Platform
      description: Full CRUD, auth, markdown
      difficulty: intermediate
      estimated_hours: 25-35
      essence: HTTP request handling with stateful session management, relational data modeling with foreign key constraints, and secure password hashing with token-based authentication flows.
      why_important: Building a blog platform teaches full-stack development fundamentals used in most web applications‚Äîdatabase schema design, RESTful API patterns, authentication security, and connecting frontend interfaces to backend services.
      learning_outcomes:
      - Design normalized database schemas with foreign key relationships for users and posts
      - Implement secure authentication with password hashing and JWT token validation
      - Build RESTful API endpoints following HTTP method conventions for CRUD operations
      - Parse and sanitize markdown content to prevent XSS vulnerabilities
      - Handle stateful sessions and authorization middleware in backend routes
      - Create dynamic frontend components that consume REST API data
      - Implement form validation and error handling across client and server
      - Deploy a full-stack application with environment-based configuration
      skills:
      - RESTful API Design
      - JWT Authentication
      - Database Schema Design
      - Markdown Parsing
      - Password Hashing
      - Session Management
      - HTTP Request Handling
      - SQL Queries
      tags:
      - comments
      - crud
      - data-structures
      - framework
      - intermediate
      - javascript
      - markdown
      - python
      - typescript
      architecture_doc: architecture-docs/blog-platform/index.md
      languages:
        recommended:
        - JavaScript
        - Python
        - TypeScript
        also_possible:
        - Go
        - Ruby
        - PHP
      resources:
      - name: Build a Blog with Next.js
        url: https://nextjs.org/learn/basics/create-nextjs-app
        type: tutorial
      - name: Flask Mega-Tutorial
        url: https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world
        type: tutorial
      prerequisites:
      - type: skill
        name: HTML/CSS/JavaScript
      - type: skill
        name: Basic backend knowledge (Node.js or Python)
      - type: skill
        name: Database basics (SQL or MongoDB)
      milestones:
      - id: blog-platform-m1
        name: Project Setup & Database Schema
        description: Set up the project structure and design the database.
        acceptance_criteria:
        - Project is initialized with the chosen web framework and all dependencies are installable
        - Database connection is established and verified with a health check query on startup
        - User table includes columns for id, email, password_hash, name, and created_at with unique email constraint
        - Post table includes columns for id, title, content, author_id foreign key, created_at, and updated_at
        - Migration system supports running pending migrations forward and rolling back the most recent migration
        pitfalls:
        - Not indexing frequently queried columns
        - Storing passwords in plain text
        - Not using foreign key constraints
        concepts:
        - Database design
        - ORM usage
        - Migrations
        skills:
        - SQL query optimization
        - Database schema design
        - ORM configuration and setup
        - Environment configuration management
        deliverables:
        - Database schema for users, posts, and comments with foreign key relationships and indexes
        - Migration files for schema versioning that support forward and rollback migrations
        - Database connection pooling setup with configurable pool size and connection timeout
        - Basic project structure with route definitions, middleware setup, and configuration files
        estimated_hours: 3-4
      - id: blog-platform-m2
        name: User Authentication
        description: Implement user registration and login with secure password handling.
        acceptance_criteria:
        - Registration accepts email and password, validates format, and creates a new user if email is unique
        - Password is hashed with bcrypt using a cost factor of at least 10 before storing in the database
        - Login verifies credentials and returns a signed JWT or session cookie with configurable expiration
        - Protected routes return 401 Unauthorized when the request lacks a valid authentication token
        - Logout invalidates the session or token so that subsequent requests with that credential are rejected
        pitfalls:
        - Storing JWT in localStorage (XSS vulnerable)
        - Not validating email format
        - Exposing whether email exists on failed login
        concepts:
        - Password hashing
        - JWT authentication
        - HTTP-only cookies
        - CSRF protection
        skills:
        - Secure authentication implementation
        - Session management
        - Password security best practices
        - API endpoint protection
        deliverables:
        - User registration endpoint that validates input, hashes the password, and creates the user record
        - Login endpoint that verifies credentials and returns a JWT or session token for authenticated access
        - Password reset functionality with time-limited token sent via email for secure credential recovery
        - Protected route middleware that validates the auth token and rejects unauthenticated requests with 401
        estimated_hours: 5-6
      - id: blog-platform-m3
        name: Blog CRUD Operations
        description: Implement create, read, update, delete for blog posts.
        acceptance_criteria:
        - Create post stores a new post with title, content, and the authenticated user as author
        - List posts endpoint returns paginated results with configurable page size and total count
        - Single post endpoint returns the full post content including author name and comment count
        - Edit endpoint returns 403 Forbidden when a user attempts to modify another user's post
        - Delete endpoint returns 403 Forbidden when a user attempts to delete another user's post
        - Markdown content is rendered to HTML for display while preserving the raw markdown in storage
        pitfalls:
        - XSS from rendering user markdown (sanitize HTML)
        - N+1 queries when loading posts with authors
        - Not checking ownership on edit/delete
        concepts:
        - CRUD operations
        - Authorization
        - Markdown rendering
        - Pagination
        skills:
        - RESTful API design
        - Authorization and access control
        - Content sanitization
        - Database query optimization
        deliverables:
        - Create post endpoint accepting title, content, and tags from the authenticated author
        - Read posts endpoint with cursor-based pagination, sorting, and optional tag filtering
        - Update post endpoint that allows only the original author to modify title and content
        - Delete post endpoint with soft-delete option that marks posts as deleted without removing data
        estimated_hours: 6-8
      - id: blog-platform-m4
        name: Frontend UI
        description: Build the frontend interface for the blog.
        acceptance_criteria:
        - Homepage displays a paginated list of posts sorted by newest first with title and excerpt
        - Post detail page renders the full markdown content as HTML with author name and publish date
        - Login and registration forms validate input client-side and display server error messages
        - Post editor provides a split-pane with markdown input on the left and live preview on the right
        - Responsive layout adjusts to mobile screen widths without horizontal scrolling or overlapping elements
        pitfalls:
        - Not handling loading states
        - No error boundaries
        - SEO issues with client-side rendering
        concepts:
        - Component architecture
        - Form handling
        - State management
        skills:
        - React/Vue component development
        - Client-side routing
        - Form validation
        - Responsive design implementation
        deliverables:
        - Post listing page with pagination controls, post previews, and author attribution
        - Single post view page showing full content, author info, and threaded comments section
        - Post editor with a markdown input area and live HTML preview panel for authoring
        - Responsive design using CSS grid or flexbox that adapts layout for desktop and mobile viewports
        estimated_hours: 6-8
    - id: chat-app
      name: Real-time Chat
      description: WebSocket, real-time updates
      difficulty: intermediate
      estimated_hours: 25-35
      essence: Persistent full-duplex TCP connections enabling bidirectional message streaming between clients and servers, with event-driven broadcasting patterns, connection lifecycle management, and distributed state synchronization challenges across unreliable network topologies.
      why_important: Real-time communication powers critical modern applications from collaborative tools to live trading platforms, teaching you event-driven architecture, stateful connection management, and the complexities of synchronizing state across unreliable networks‚Äîskills directly applicable to any system requiring low-latency updates.
      learning_outcomes:
      - Implement WebSocket handshake protocol and maintain persistent bidirectional connections
      - Design event-driven message broadcasting systems with room-based multicasting
      - Build authentication mechanisms for stateful WebSocket connections using token validation
      - Implement message persistence with database storage and history pagination
      - Handle connection lifecycle events including reconnection logic and presence tracking
      - Debug network latency issues and implement message ordering guarantees
      - Secure WebSocket connections against CSRF, injection attacks, and origin validation bypass
      - Optimize connection pooling and implement rate limiting to prevent DoS attacks
      skills:
      - WebSocket Protocol
      - Event-driven Architecture
      - Stateful Connection Management
      - Real-time Broadcasting
      - Token-based Authentication
      - Message Queue Patterns
      - Connection Lifecycle Handling
      - Network Security
      tags:
      - intermediate
      - javascript
      - presence
      - real-time
      - typescript
      - websockets
      architecture_doc: architecture-docs/chat-app/index.md
      languages:
        recommended:
        - JavaScript
        - TypeScript
        also_possible:
        - Go
        - Python
        - Rust
      resources:
      - name: Socket.io Chat Tutorial
        url: https://socket.io/get-started/chat
        type: tutorial
      - name: WebSocket API - MDN
        url: https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API
        type: documentation
      prerequisites:
      - type: skill
        name: JavaScript/Node.js
      - type: skill
        name: Basic HTML/CSS
      - type: skill
        name: Understanding of HTTP
      milestones:
      - id: chat-app-m1
        name: WebSocket Server Setup
        description: Set up WebSocket server and basic connection handling.
        acceptance_criteria:
        - Server accepts WebSocket upgrade requests and establishes persistent connections
        - Connection events are logged when clients connect and disconnect from server
        - Server correctly handles malformed messages without crashing or dropping other clients
        - Connection state tracks all active clients and removes disconnected ones promptly
        pitfalls:
        - Not handling connection errors
        - Memory leaks from not cleaning up closed connections
        - Not implementing heartbeat/ping-pong
        concepts:
        - WebSocket protocol
        - Connection lifecycle
        - Event-driven architecture
        skills:
        - WebSocket Server Configuration
        - Connection Lifecycle Management
        - Event-Driven Server Architecture
        - Network Protocol Implementation
        - Error Handling for Network Connections
        deliverables:
        - WebSocket server implementation accepting upgrade requests from HTTP clients
        - Connection handling managing client connect and disconnect lifecycle events
        - Message routing dispatching incoming messages to appropriate handler functions
        - Connection state tracking maintaining set of currently connected client sockets
        estimated_hours: 2-3
      - id: chat-app-m2
        name: Message Broadcasting
        description: Implement sending and receiving messages.
        acceptance_criteria:
        - Broadcast delivers message to all connected clients except the original sender
        - Messages include sender username, text content, and server-assigned timestamp
        - Join notification is broadcast to room members when a new user enters the room
        - Typing indicator is broadcast when user starts typing and cleared after timeout
        pitfalls:
        - Not checking readyState before sending
        - Invalid JSON crashing server
        - Missing message validation
        concepts:
        - Broadcasting pattern
        - JSON message protocol
        - Error handling
        skills:
        - Message Broadcasting to Multiple Clients
        - JSON Protocol Design and Validation
        - Real-time Data Transmission
        - WebSocket State Management
        - Server-Side Message Processing
        deliverables:
        - Room-based broadcasting delivering messages to all clients in target room
        - Message format including sender name, content text, and UTC timestamp
        - User join and leave event notifications broadcast to other room members
        - Typing indicator messages notifying room members when user is composing
        estimated_hours: 3-4
      - id: chat-app-m3
        name: Chat Rooms
        description: Implement multiple chat rooms/channels.
        acceptance_criteria:
        - Users can create new rooms and join existing rooms by name
        - Room member list is updated when users join or leave the room
        - Room listing endpoint returns all available rooms with member counts
        - Joining a room loads configurable number of recent messages from history
        - Messages sent to a room are delivered only to members currently in that room
        pitfalls:
        - Not removing user from room on disconnect
        - Room names not sanitized
        - Empty rooms accumulating
        concepts:
        - Room-based messaging
        - Namespaces
        - User presence
        skills:
        - Multi-Room Architecture Design
        - User Session Management
        - Dynamic Group Communication
        - Namespace and Channel Organization
        - Client-Server State Synchronization
        deliverables:
        - Room creation and joining allowing users to enter named chat channels
        - Room member list management tracking which users are in each room
        - Message history loading providing recent messages when user joins room
        - Room-based message routing delivering messages only to members of target room
        estimated_hours: 4-5
      - id: chat-app-m4
        name: User Authentication & Persistence
        description: Add user authentication and message history.
        acceptance_criteria:
        - Users must authenticate before WebSocket connection is accepted by server
        - Messages are stored in database with sender, room, content, and timestamp fields
        - Message history loads previous messages when user joins room with pagination
        - Typing indicator events broadcast to room members showing who is currently typing
        - Online and offline status updates broadcast when authenticated users connect or disconnect
        pitfalls:
        - Loading too much history (pagination needed)
        - Typing indicator spam
        - Race conditions with presence
        concepts:
        - WebSocket authentication
        - Message persistence
        - Presence system
        skills:
        - WebSocket Authentication Patterns
        - Database Integration for Chat History
        - Message Persistence and Retrieval
        - User Presence Tracking
        - Pagination for Large Datasets
        deliverables:
        - User login and registration with username and password credentials
        - Session management maintaining authenticated state across WebSocket connections
        - Message persistence storing chat messages in database for history retrieval
        - Chat history loading retrieving past messages with pagination support
        estimated_hours: 6-8
    - id: ecommerce-basic
      name: E-commerce (Basic)
      description: Cart, checkout flow
      difficulty: beginner
      estimated_hours: 20-35
      essence: Stateful session management across HTTP's stateless protocol, normalized relational data modeling for products/users/orders with foreign key constraints, and coordinating atomic transactions between cart state, inventory deduction, and payment processing.
      why_important: Building this teaches you full-stack web architecture patterns, database modeling for relational data, and payment integration‚Äîfoundational skills for most modern web applications.
      learning_outcomes:
      - Implement RESTful API endpoints with proper HTTP methods and status codes
      - Design normalized database schemas for products, users, orders, and cart relationships
      - Build stateless authentication with session tokens or JWT
      - Integrate third-party payment APIs with webhook handling
      - Implement input validation and sanitization to prevent SQL injection and XSS
      - Design responsive front-end interfaces with asynchronous data fetching
      - Handle transactional operations for order creation and inventory updates
      - Debug full-stack applications across client, server, and database layers
      skills:
      - RESTful API Design
      - Database Schema Modeling
      - Session Management
      - Payment Integration
      - Input Validation
      - Client-Server Architecture
      - SQL Queries
      - HTTP Protocol
      tags:
      - beginner-friendly
      - cart
      - checkout
      - data-structures
      - inventory
      - javascript/node.js
      - python/flask
      - ruby/rails
      architecture_doc: architecture-docs/ecommerce-basic/index.md
      languages:
        recommended:
        - JavaScript/Node.js
        - Python/Flask
        - Ruby/Rails
        also_possible:
        - PHP
        - Go
        - Java/Spring
      resources:
      - type: tutorial
        name: Full Stack E-commerce
        url: https://www.freecodecamp.org/news/how-to-build-an-e-commerce-app/
      - type: video
        name: Build an Online Store
        url: https://www.youtube.com/results?search_query=build+ecommerce+site+tutorial
      prerequisites:
      - type: skill
        name: HTML/CSS/JS
      - type: skill
        name: REST API basics
      - type: skill
        name: Database basics
      milestones:
      - id: ecommerce-basic-m1
        name: Product Catalog
        description: Display products with images, prices, and descriptions.
        acceptance_criteria:
        - Product listing page displays paginated catalog with sort options
        - Product detail page shows full product information and availability
        - Category filtering narrows product listing to selected category branch
        - Search functionality matches products by name, description, and attributes
        pitfalls:
        - Price precision (use cents)
        - Large image handling
        - N+1 queries
        concepts:
        - CRUD operations
        - Filtering/search
        - Data modeling
        skills:
        - Database schema design
        - REST API development
        - Image optimization and CDN usage
        - Search query implementation
        - Pagination handling
        deliverables:
        - Product model with name, description, price, and inventory fields
        - Category hierarchy supporting nested subcategories for organization
        - Product listing API with pagination and sorting options
        - Product search functionality with keyword and filter matching
        estimated_hours: 4-6
      - id: ecommerce-basic-m2
        name: Shopping Cart
        description: Implement add to cart, update quantities, and remove items.
        acceptance_criteria:
        - Add and remove items from cart with correct subtotal recalculation
        - Update quantities with validation against available inventory stock levels
        - Cart persistence survives page refresh and browser session restoration
        - Price calculations correctly apply quantities and reflect current product prices
        pitfalls:
        - Price changes between add and checkout
        - Stock validation
        - Session expiry
        concepts:
        - State management
        - Session storage
        - Data synchronization
        skills:
        - Session management
        - Frontend state handling
        - API design for cart operations
        - Optimistic UI updates
        - Local storage synchronization
        deliverables:
        - Cart session management linking cart to user or anonymous session
        - Add and remove items operations with proper validation
        - Quantity update operations with inventory availability checking
        - Cart persistence across page loads and browser sessions
        estimated_hours: 4-6
      - id: ecommerce-basic-m3
        name: User Authentication
        description: Implement user registration, login, and profile management.
        acceptance_criteria:
        - Registration validates email format and password strength requirements
        - Login authenticates credentials and creates secure user session
        - Password hashing uses bcrypt or argon2 to prevent plaintext storage
        - Session management tracks authenticated state with secure cookie handling
        pitfalls:
        - Timing attacks on login
        - Password requirements
        - Token invalidation
        concepts:
        - Authentication
        - Password security
        - Session management
        skills:
        - Password hashing with bcrypt/argon2
        - JWT or session token implementation
        - Secure cookie configuration
        - OAuth integration basics
        - User input validation
        deliverables:
        - User registration with email and password validation
        - Login and logout flow with proper session management
        - Password hashing using bcrypt or argon2 for secure storage
        - Session management with secure cookies and expiration handling
        estimated_hours: 5-8
      - id: ecommerce-basic-m4
        name: Checkout Process
        description: Implement checkout flow with order creation.
        acceptance_criteria:
        - Address collection validates required shipping fields before proceeding
        - Order summary displays itemized list with quantities and total price
        - Order creation persists order record and clears the shopping cart
        - Inventory updates decrement stock for each purchased item at checkout
        pitfalls:
        - Race conditions on stock
        - Abandoned checkouts
        - Partial order failures
        concepts:
        - Transactions
        - Inventory management
        - Order lifecycle
        skills:
        - Database transaction management
        - Inventory locking mechanisms
        - Order state machine design
        - Payment gateway integration
        - Email notification systems
        deliverables:
        - Address collection form with validation for shipping details
        - Order creation assembling cart items into a persistent order record
        - Payment integration stub simulating payment gateway interaction
        - Order confirmation page displaying summary and order number
        estimated_hours: 7-15
    - id: grpc-service
      name: gRPC Microservice
      description: Build a gRPC service with streaming, error handling, and interceptors.
      difficulty: intermediate
      estimated_hours: 25-40
      essence: HTTP/2-based remote procedure calls using Protocol Buffer binary serialization, with support for four streaming patterns (unary, server-stream, client-stream, bidirectional) and middleware-based request interception for cross-cutting concerns in distributed systems.
      why_important: Building this project teaches you the foundations of modern microservice architecture and high-performance inter-service communication patterns used at scale by companies like Google, Netflix, and Uber.
      learning_outcomes:
      - Define service contracts with Protocol Buffers including message types and RPC methods
      - 'Implement all four RPC patterns: unary, server streaming, client streaming, and bidirectional streaming'
      - Build custom interceptors for authentication, logging, and error handling
      - Handle backpressure and flow control in streaming scenarios
      - Debug serialization issues and optimize message schemas for performance
      - Implement proper error handling with status codes and metadata
      - Write integration tests for streaming RPCs and edge cases
      - Deploy language-agnostic services with cross-platform compatibility
      skills:
      - Protocol Buffers
      - HTTP/2 Protocol
      - Bidirectional Streaming
      - RPC Interceptors
      - Service Contracts
      - Binary Serialization
      - Async I/O Patterns
      - Microservice Communication
      tags:
      - backend
      - bidirectional
      - code-generation
      - go
      - intermediate
      - protobuf
      - protocols
      - rust
      - service
      - streaming
      architecture_doc: architecture-docs/grpc-service/index.md
      languages:
        recommended:
        - Go
        - Rust
        also_possible:
        - Java
        - Python
        - C++
      resources:
      - name: gRPC Official Docs
        url: https://grpc.io/docs/
        type: documentation
      - name: Protocol Buffers
        url: https://developers.google.com/protocol-buffers
        type: documentation
      prerequisites:
      - type: skill
        name: Protocol Buffers
      - type: skill
        name: RPC concepts
      - type: skill
        name: Go or similar
      milestones:
      - id: grpc-service-m1
        name: Proto Definition & Code Generation
        description: Define service contract with Protocol Buffers.
        acceptance_criteria:
        - Define message types with proper field numbering and type annotations
        - Define service methods including unary, server streaming, and bidirectional streaming
        - Generate server and client code from proto definitions using protoc compiler
        - Version proto files properly with package namespacing and backward compatibility
        - Document APIs with proto comments that generate readable documentation
        pitfalls:
        - Changing field numbers breaks compatibility
        - Using required fields (proto3 doesn't have them)
        - Not setting default enum value to UNSPECIFIED
        - Large messages exceed gRPC size limits
        concepts:
        - Protocol Buffers
        - Service definition
        - Code generation
        - API versioning
        skills:
        - Protocol Buffer Schema Design
        - Code Generation Tools
        - API Versioning Strategies
        - Message Serialization
        deliverables:
        - Protocol buffer definition with message and service declarations
        - Code generation setup using protoc with language-specific plugins
        - Message types with typed fields and nested structures
        - Service definition with unary and streaming RPC method declarations
        estimated_hours: 4-6
      - id: grpc-service-m2
        name: Server Implementation
        description: Implement gRPC server with all RPC types.
        acceptance_criteria:
        - Unary RPC implementation handles single request-response operations correctly
        - Server streaming RPC sends multiple responses for a single client request
        - Client streaming RPC receives multiple messages and returns single response
        - Bidirectional streaming handles concurrent send and receive message flows
        - Graceful shutdown completes in-flight requests before stopping the server
        pitfalls:
        - Blocking in streaming without timeout
        - Not handling context cancellation
        - Forgetting UnimplementedServer for forward compatibility
        - Memory leaks from unclosed streams
        concepts:
        - gRPC server
        - Streaming RPCs
        - Graceful shutdown
        - Status codes
        skills:
        - gRPC Server Implementation
        - Stream Management
        - Context Handling
        - Service Lifecycle Management
        - Error Code Mapping
        deliverables:
        - gRPC server setup with listener and service registration
        - Service implementation handling all defined RPC method types
        - Request handling with input validation and business logic
        - Error responses using proper gRPC status codes and messages
        estimated_hours: 8-12
      - id: grpc-service-m3
        name: Interceptors & Middleware
        description: Add cross-cutting concerns with interceptors.
        acceptance_criteria:
        - Logging interceptor records method name, duration, and status code for each RPC
        - Authentication interceptor validates bearer tokens from gRPC metadata headers
        - Rate limiting interceptor throttles requests exceeding per-client limits
        - Error handling and recovery interceptor catches panics and returns Internal status
        - Request and response validation interceptor checks message constraints before processing
        pitfalls:
        - Interceptor order matters (auth before logging?)
        - Recovery interceptor not catching all panics
        - Context values lost in interceptor chain
        - Stream interceptors more complex than unary
        concepts:
        - gRPC interceptors
        - Middleware pattern
        - Context propagation
        - Error handling
        skills:
        - Interceptor Development
        - Cross-Cutting Concerns
        - Context Propagation
        - Middleware Chaining
        deliverables:
        - Server interceptors for unary and streaming RPC types
        - Logging interceptor recording method, duration, and status for each call
        - Authentication interceptor validating tokens from request metadata
        - Metrics interceptor collecting latency and error rate per RPC method
        estimated_hours: 6-10
      - id: grpc-service-m4
        name: Client & Testing
        description: Build robust client and comprehensive tests.
        acceptance_criteria:
        - Client retries failed requests with configurable exponential backoff delays
        - Connection pooling reuses gRPC connections across multiple client calls
        - Deadline and timeout handling cancels requests exceeding configured duration
        - Unit tests use mock server to verify client behavior without real network calls
        - Integration tests verify full request-response flow with in-process gRPC server
        pitfalls:
        - Not setting deadline causes hanging requests
        - Retry on non-idempotent methods causes duplicates
        - Connection not reused (creating new per request)
        - Tests not cleaning up connections
        concepts:
        - gRPC client
        - Retry policies
        - Connection management
        - Testing strategies
        skills:
        - gRPC Client Implementation
        - Connection Pooling
        - Retry Logic
        - Integration Testing
        - Load Balancing
        deliverables:
        - gRPC client setup with connection management and configuration
        - Client calls with retry policy and exponential backoff on failures
        - Integration tests verifying end-to-end RPC behavior with real server
        - Mock server for unit testing client code without network dependency
        estimated_hours: 6-10
    - id: circuit-breaker
      name: Circuit Breaker Pattern
      description: Implement circuit breaker for resilient microservices communication.
      difficulty: intermediate
      estimated_hours: 15-25
      essence: State machine managing failure thresholds and timeout windows to automatically block requests to failing services, preventing cascading failures across distributed systems through closed, open, and half-open state transitions with sliding window failure rate tracking.
      why_important: Circuit breakers are critical for production microservices resilience, preventing cascading failures that can take down entire systems and teaching you how to build fault-tolerant distributed architectures.
      learning_outcomes:
      - Implement state transitions between closed, open, and half-open circuit states
      - Design sliding window algorithms for tracking failure rates and success metrics
      - Build timeout mechanisms and automatic recovery detection logic
      - Integrate circuit breakers with HTTP clients and gRPC communication layers
      - Configure failure thresholds, timeout periods, and half-open request limits
      - Implement fallback strategies for graceful degradation during outages
      - Test resilience patterns using chaos engineering and failure injection
      - Monitor circuit breaker metrics and state changes for observability
      skills:
      - Fault Tolerance Patterns
      - State Machine Design
      - Failure Rate Tracking
      - Distributed Systems Resilience
      - Timeout Management
      - Chaos Engineering
      - Service Degradation
      - Observability Metrics
      tags:
      - failure-detection
      - fallback
      - fault-tolerance
      - go
      - half-open
      - intermediate
      - java
      - resilience
      architecture_doc: architecture-docs/circuit-breaker/index.md
      languages:
        recommended:
        - Go
        - Java
        also_possible:
        - Python
        - TypeScript
      resources:
      - name: Circuit Breaker Pattern
        url: https://martinfowler.com/bliki/CircuitBreaker.html
        type: article
      - name: Hystrix
        url: https://github.com/Netflix/Hystrix/wiki
        type: reference
      prerequisites:
      - type: skill
        name: Microservices basics
      - type: skill
        name: Concurrency
      milestones:
      - id: circuit-breaker-m1
        name: Basic Circuit Breaker
        description: Implement closed, open, half-open states.
        acceptance_criteria:
        - Closed state passes requests through to downstream service normally
        - Open state fails all requests immediately without calling downstream
        - Half-open state allows limited test requests through to probe recovery
        - State transitions occur based on configurable failure threshold count
        - Thread-safe implementation handles concurrent request processing correctly
        pitfalls:
        - Race conditions without proper locking
        - Not resetting failure count on success
        - Half-open allows too many requests
        - Timer not being reset properly
        concepts:
        - Circuit breaker states
        - Failure detection
        - Recovery testing
        - Thread safety
        skills:
        - State machine implementation
        - Concurrent programming with mutexes
        - Timeout handling
        - Failure threshold configuration
        - Atomic operations
        deliverables:
        - State machine with closed, open, and half-open states
        - Failure threshold tracking with configurable limits
        - Configurable timeout duration for open state
        - Success threshold counter for recovery transitions
        estimated_hours: 5-8
      - id: circuit-breaker-m2
        name: Advanced Features
        description: Add sliding window, metrics, and fallbacks.
        acceptance_criteria:
        - Sliding window calculates failure rate over configurable time period
        - Configurable error classification distinguishes transient from permanent failures
        - Fallback function executes and returns alternative response when circuit is open
        - Metrics expose current state, failure count, and success rate for observability
        - Bulkhead enforces concurrency limit per downstream service independently
        pitfalls:
        - Sliding window bucket rotation timing issues
        - Fallback also failing (need fallback for fallback)
        - Bulkhead too small causes unnecessary rejections
        - Error classifier too aggressive
        concepts:
        - Sliding window
        - Fallback patterns
        - Bulkhead pattern
        - Error classification
        skills:
        - Time-series data structures
        - Graceful degradation patterns
        - Resource isolation techniques
        - Circuit breaker metrics collection
        - Error categorization logic
        deliverables:
        - Bulkhead pattern for concurrency isolation per service
        - Configurable fallback functions for graceful degradation
        - Metrics collection for circuit state and failure rates
        - Per-service configuration with independent circuit parameters
        estimated_hours: 6-10
      - id: circuit-breaker-m3
        name: Integration & Testing
        description: Integrate with HTTP/gRPC clients and test chaos scenarios.
        acceptance_criteria:
        - HTTP client wrapper applies circuit breaker to outgoing requests automatically
        - gRPC interceptor applies circuit breaker to RPC calls transparently
        - Per-service circuit breakers operate independently without interference
        - Chaos testing injects failures and verifies circuit opens after threshold
        - Dashboard displays current circuit states for all registered services
        pitfalls:
        - Chaos testing in production without safeguards
        - Circuit breaker per-request instead of per-service
        - Not exposing circuit state for debugging
        - Test flakiness due to timing
        concepts:
        - Client integration
        - Chaos engineering
        - Per-service isolation
        - Observability
        skills:
        - HTTP client middleware integration
        - gRPC interceptor implementation
        - Chaos testing with fault injection
        - Service-level circuit isolation
        - Distributed system monitoring
        deliverables:
        - HTTP client integration with automatic circuit breaker wrapping
        - Decorator or wrapper pattern for transparent circuit breaker application
        - Integration tests verifying end-to-end circuit breaker behavior
        - Chaos testing framework for injecting controlled failures
        estimated_hours: 6-10
    - id: file-upload-service
      name: File Upload Service
      description: Chunked uploads, resumable, virus scanning
      difficulty: advanced
      estimated_hours: '35'
      essence: Stateful chunked binary transfer protocol with byte-range offset tracking and resumption semantics, unified storage backend abstraction across heterogeneous cloud provider APIs, and stream-based malware detection without full file materialization.
      why_important: Building this teaches production-grade file handling patterns used by services like Dropbox and Google Drive, covering critical real-world challenges like unreliable networks, multi-cloud storage strategies, and security validation that every backend engineer encounters.
      learning_outcomes:
      - Implement tus.io resumable upload protocol with chunk offset tracking and resume capability
      - Design storage abstraction layer supporting multiple backends (S3, GCS, local filesystem) with unified interface
      - Build multipart upload handlers with concurrent chunk processing and assembly logic
      - Integrate ClamAV daemon for stream-based virus scanning without full file buffering
      - Implement MIME type validation and file signature verification for security
      - Handle partial upload cleanup and orphaned chunk garbage collection
      - Design idempotent upload operations with checksum verification for data integrity
      - Implement rate limiting and quota management for upload endpoints
      skills:
      - HTTP Protocol Design
      - Storage Abstraction Patterns
      - Stream Processing
      - Virus Scanning Integration
      - Multipart Upload Handling
      - Checksum Verification
      - Binary Data Handling
      - Cloud Storage APIs
      tags:
      - advanced
      - chunked
      - file-transfer
      - files
      - resumable
      - s3
      - scanning
      - service
      - storage
      - uploads
      architecture_doc: architecture-docs/file-upload-service/index.md
      languages:
        recommended:
        - Go
        - Python
        - Rust
        also_possible: []
      resources:
      - name: tus.io Resumable Upload Protocol
        url: https://tus.io/protocols/resumable-upload
        type: documentation
      - name: AWS S3 Multipart Upload Guide
        url: https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html
        type: documentation
      - name: ClamAV Documentation
        url: https://docs.clamav.net/
        type: documentation
      - name: Dropbox Storage Abstraction
        url: https://dropbox.tech/infrastructure/abstracting-cloud-storage-backends-with-object-store
        type: article
      - name: Chunked File Upload Tutorial
        url: https://medium.com/@selieshjksofficial/streamlining-large-file-uploads-with-chunk-uploads-in-node-js-and-express-js-e40d00c26c2d
        type: tutorial
      prerequisites:
      - type: project
        id: http-server-basic
        name: HTTP Server (Basic)
      milestones:
      - id: file-upload-service-m1
        name: Chunked Upload Protocol
        description: Implement tus.io-style resumable upload protocol
        acceptance_criteria:
        - Initialize multipart upload returning upload ID for subsequent chunk uploads
        - Accept chunk uploads with part numbers and associate with upload session
        - Track chunk completion status showing which parts are received and pending
        - Complete upload by assembling all received chunks into the final file
        pitfalls:
        - Pre-allocate file to avoid fragmentation
        - 'Offset mismatch: client must track and resume from server''s offset'
        - Chunk checksum prevents corruption from network errors
        - Expiry cleanup must not delete in-progress uploads
        concepts:
        - HTTP Range requests and Content-Range headers
        - Append-only file operations and atomic offset tracking
        - Upload session state management and expiration
        - File integrity verification using checksums (MD5, SHA-256)
        skills:
        - Chunked uploads
        - Resume logic
        - HTTP headers
        deliverables:
        - Chunk upload endpoint accepting individual file parts with metadata
        - Chunk ordering with part number tracking for correct reassembly
        - Resume interrupted uploads by tracking which chunks are received
        - Chunk assembly combining all parts into the final complete file
        estimated_hours: '11.5'
      - id: file-upload-service-m2
        name: Storage Abstraction
        description: Implement storage backends (local, S3, GCS) with common interface
        acceptance_criteria:
        - Abstract storage interface supports local, S3, and GCS backends uniformly
        - Handle storage credentials and authentication for cloud backends securely
        - Generate signed download URLs with configurable expiration for secure access
        - Support storage migration moving files between backends without data loss
        pitfalls:
        - 'S3 multipart: parts must be at least 5MB (except last)'
        - 'Local storage: sanitize keys to prevent path traversal'
        - S3 eventually consistent for overwrite - use versioning
        - Streaming large files - don't load entire file in memory
        concepts:
        - Interface-based abstraction and dependency injection
        - Object storage multipart upload protocols
        - Streaming I/O and buffered reading/writing
        - Cloud SDK credential management and authentication
        skills:
        - Storage abstraction
        - S3 API
        - Multipart uploads
        deliverables:
        - Storage interface defining read, write, and delete operations
        - Local filesystem backend implementing the storage interface
        - S3-compatible backend for cloud object storage integration
        - Storage selection based on configuration for deployment flexibility
        estimated_hours: '11.5'
      - id: file-upload-service-m3
        name: Virus Scanning & Validation
        description: Implement file validation with type checking and virus scanning
        acceptance_criteria:
        - Scan uploaded files with ClamAV or similar antivirus engine before storage
        - Validate file types using magic bytes rather than relying on file extension
        - Enforce configurable size limits and reject uploads exceeding the maximum
        - Quarantine suspicious files in isolated storage for review rather than deleting
        pitfalls:
        - Never trust file extension alone - always check magic bytes
        - Virus scan before storing, not after
        - ClamAV can timeout on large files - set appropriate limits
        - Quarantine infected files, don't delete immediately (for forensics)
        concepts:
        - Magic number detection for file type identification
        - Async task queues for background scanning
        - Signature-based malware detection engines
        - File quarantine and retention policies
        skills:
        - File validation
        - MIME detection
        - Antivirus integration
        deliverables:
        - File type validation using magic bytes for content verification
        - Size limit enforcement rejecting uploads exceeding maximum threshold
        - Virus scan integration with ClamAV or equivalent scanning engine
        - Quarantine handling isolating suspicious files from normal storage
        estimated_hours: '11.5'
    - id: websocket-server
      name: WebSocket Server
      description: Real-time bidirectional communication
      difficulty: intermediate
      estimated_hours: '30'
      essence: Binary framing protocol with masking/unmasking operations, SHA-1-based handshake negotiation over HTTP upgrade, and stateful connection lifecycle management over raw TCP sockets with opcode-driven message demultiplexing.
      why_important: WebSockets enable real-time bidirectional communication essential for chat apps, live updates, gaming, and collaborative tools.
      learning_outcomes:
      - Master WebSocket handshake and frame parsing
      - Implement connection lifecycle management
      - Handle binary and text message types
      - Build heartbeat/ping-pong mechanisms
      skills:
      - Binary Protocol Design
      - HTTP Upgrade Handshake
      - Frame Parsing
      - Connection State Management
      - SHA-1 Hashing
      - Base64 Encoding
      - TCP Socket Programming
      - Heartbeat Mechanisms
      tags:
      - bidirectional
      - frames
      - intermediate
      - networking
      - protocols
      - real-time
      - service
      - upgrade
      architecture_doc: architecture-docs/websocket-server/index.md
      languages:
        recommended:
        - JavaScript
        - Python
        - Go
        also_possible: []
      resources:
      - name: RFC 6455 WebSocket Protocol
        url: https://datatracker.ietf.org/doc/html/rfc6455
        type: documentation
      - name: Writing WebSocket Servers - MDN
        url: https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_servers
        type: tutorial
      - name: Python websockets Documentation
        url: https://websockets.readthedocs.io/en/stable/intro/tutorial1.html
        type: documentation
      - name: WebSocket Tutorial - JavaScript.info
        url: https://javascript.info/websocket
        type: tutorial
      prerequisites:
      - type: project
        id: http-server-basic
        name: HTTP Server (Basic)
      - type: skill
        name: TCP socket programming
      milestones:
      - id: websocket-server-m1
        name: HTTP Upgrade Handshake
        description: Implement WebSocket handshake by parsing HTTP upgrade request and computing Sec-WebSocket-Accept using SHA-1 and Base64.
        acceptance_criteria:
        - Parse HTTP upgrade request and extract all WebSocket-specific headers
        - Validate Sec-WebSocket-Key header is present and base64-encoded correctly
        - Generate Sec-WebSocket-Accept response using SHA-1 hash of key plus magic GUID
        - Complete protocol upgrade by sending 101 response and switching to WebSocket framing
        pitfalls:
        - Missing CRLF at end of headers causes handshake failure
        - Case-sensitive header parsing breaks with some clients
        - Not validating WebSocket version header
        concepts:
        - HTTP header parsing and RFC 2616 compliance
        - SHA-1 cryptographic hashing for handshake validation
        - Base64 encoding for binary-to-text representation
        - HTTP upgrade mechanism and protocol switching
        - GUID concatenation for WebSocket key derivation
        skills:
        - HTTP Protocol Implementation
        - Cryptographic Hash Functions
        - String Parsing and Manipulation
        - Protocol Handshake Design
        deliverables:
        - 'Upgrade request detection identifying HTTP requests with Connection: Upgrade header'
        - Sec-WebSocket-Key processing extracting and validating the client's handshake key
        - Accept header generation computing the SHA-1 hash of key concatenated with the magic GUID
        - Protocol upgrade response sending 101 Switching Protocols with correct WebSocket headers
        estimated_hours: '7.5'
      - id: websocket-server-m2
        name: Frame Parsing
        description: Parse WebSocket frames including opcode, masking, and payload length handling for small and large messages.
        acceptance_criteria:
        - Parse WebSocket frame header including opcode and extended payload length
        - Handle continuation frames by buffering fragments until a FIN frame arrives
        - Unmask client frames by applying the 4-byte masking key via XOR operation
        - Support text and binary frame types with correct UTF-8 validation for text
        pitfalls:
        - Forgetting to unmask client frames (clients MUST mask)
        - Integer overflow with 64-bit payload lengths
        - Not handling fragmented messages (FIN=0)
        concepts:
        - Binary frame structure and bit-level operations
        - XOR masking and unmasking algorithm
        - Variable-length integer encoding for payload sizes
        - Frame opcodes for control and data messages
        - Message fragmentation and reassembly
        skills:
        - Binary Protocol Parsing
        - Bitwise Operations
        - Buffer Management
        - State Machine Implementation
        deliverables:
        - Frame header parser extracting opcode, mask bit, and payload length fields
        - Payload masking and unmasking applying the 4-byte XOR mask to frame data
        - Frame type dispatch routing text, binary, and control frames to appropriate handlers
        - Fragmented message assembly concatenating continuation frames into a complete message
        estimated_hours: '7.5'
      - id: websocket-server-m3
        name: Connection Management
        description: Manage multiple WebSocket connections with proper state tracking, broadcasting, and graceful disconnection.
        acceptance_criteria:
        - Track active connections with unique identifiers and metadata
        - Handle connection state (OPEN, CLOSING, CLOSED)
        - Implement clean close handshake exchanging close frames before TCP teardown
        - Support connection metadata storing client info and custom attributes per session
        pitfalls:
        - Not handling disconnections during broadcast causes cascade failures
        - Memory leak from not cleaning up closed connections
        - Race conditions when modifying connection dict during iteration
        concepts:
        - Concurrent connection handling with event loops or threads
        - Thread-safe data structures for shared state
        - Broadcasting patterns and fan-out messaging
        - Connection lifecycle state machines
        - Graceful shutdown and resource cleanup
        skills:
        - Concurrency and Synchronization
        - Resource Lifecycle Management
        - Event-Driven Architecture
        - Thread Safety
        deliverables:
        - Connection state tracking maintaining lifecycle state for each active WebSocket session
        - Ping and pong frames for keep-alive detecting and maintaining idle connections
        - Close handshake implementation sending and receiving close frames with status codes
        - Connection timeout logic closing sessions that remain idle beyond a configured threshold
        estimated_hours: '7.5'
      - id: websocket-server-m4
        name: Ping/Pong Heartbeat
        description: Implement heartbeat mechanism using ping/pong frames to detect dead connections and maintain NAT mappings.
        acceptance_criteria:
        - Send periodic ping frames at the configured heartbeat interval
        - Handle pong responses by resetting the connection's liveness timer
        - Detect dead connections when pong is not received within the timeout window
        - Configure heartbeat interval and timeout threshold per server or per connection
        pitfalls:
        - Using wall clock time fails with system time changes
        - Too aggressive ping interval wastes bandwidth
        - Not initializing last_pong on connect causes immediate disconnect
        concepts:
        - Monotonic clock for elapsed time measurement
        - Control frame opcodes for ping and pong
        - Keepalive mechanisms and NAT traversal
        - Timeout-based connection health checks
        - Asynchronous timer scheduling
        skills:
        - Timer and Scheduling Mechanisms
        - Network Keepalive Strategies
        - Time-Based State Management
        - Asynchronous Programming
        deliverables:
        - Ping frame sender dispatching periodic ping frames to each connected client
        - Pong frame response handler processing client pong replies and updating liveness state
        - Connection timeout detection marking connections as dead when pong is not received
        - Keep-alive interval configuration setting the time between consecutive ping frames
        estimated_hours: '7.5'
    - id: notification-service
      name: Notification Service
      description: Push notifications, email, SMS delivery
      difficulty: expert
      estimated_hours: '45'
      essence: Asynchronous message routing across heterogeneous delivery channels with queue-based decoupling, webhook-driven status reconciliation, template interpolation, and preference-based filtering to guarantee at-least-once delivery semantics across unreliable third-party APIs.
      why_important: Building this teaches you distributed systems patterns essential for production infrastructure, including message queues, event-driven architecture, third-party API integration, and observability for asynchronous workflows that form the backbone of modern SaaS applications.
      learning_outcomes:
      - Implement a plugin-based channel abstraction layer with strategy pattern for multi-provider routing
      - Design message queue architecture with RabbitMQ or Kafka for reliable asynchronous processing
      - Build template engines with variable interpolation, localization support, and secure rendering
      - Implement webhook-based delivery status tracking with idempotent event processing
      - Design granular user preference systems with opt-in/opt-out controls and compliance features
      - Build retry mechanisms with exponential backoff and dead-letter queues for failed deliveries
      - Implement rate limiting and circuit breakers for third-party API integrations
      - Design analytics pipelines for tracking delivery metrics, open rates, and click-through data
      skills:
      - Message Queue Architecture
      - Event-Driven Systems
      - Third-Party API Integration
      - Template Rendering Engines
      - Asynchronous Processing
      - Distributed Systems Patterns
      - Webhook Processing
      - Rate Limiting & Circuit Breakers
      tags:
      - concurrency
      - email
      - expert
      - messaging
      - notifications
      - push
      - service
      - sms
      - webhooks
      architecture_doc: architecture-docs/notification-service/index.md
      languages:
        recommended:
        - Go
        - Python
        - Java
        also_possible: []
      resources:
      - name: RabbitMQ Go Tutorial
        url: https://www.rabbitmq.com/tutorials/tutorial-one-go
        type: tutorial
      - name: Firebase Cloud Messaging
        url: https://firebase.google.com/docs/cloud-messaging
        type: documentation
      - name: Twilio SMS API
        url: https://www.twilio.com/docs/messaging/api
        type: documentation
      - name: SendGrid API Reference
        url: https://www.twilio.com/docs/sendgrid/api-reference
        type: documentation
      - name: Notification Service Design Guide
        url: https://www.geeksforgeeks.org/system-design/design-notification-services-system-design/
        type: article
      prerequisites:
      - type: skill
        name: Message queue/pub-sub basics
      - type: project
        id: rest-api-design
        name: Production REST API
      milestones:
      - id: notification-service-m1
        name: Channel Abstraction & Routing
        description: Implement pluggable channels (email, SMS, push) with intelligent routing
        acceptance_criteria:
        - Channel interface defines a common contract for send, validate, and format across all notification backends
        - Notifications are routed to the appropriate channel based on notification type and user preferences
        - Channel-specific formatting transforms the notification payload into the format required by each backend
        - Fallback channels are attempted in order when the primary channel fails to deliver the notification
        pitfalls:
        - Provider fallback must track failures to avoid cascading errors
        - SMS costs money - only use for truly urgent notifications
        - Push token can become invalid - handle token refresh errors
        - Quiet hours must consider user's timezone, not server's
        concepts:
        - Adapter pattern for channel implementations
        - Circuit breaker pattern for provider failover
        - Message queue prioritization and routing
        - Rate limiting per channel type
        - Idempotency keys for delivery guarantees
        skills:
        - Channel abstraction
        - Provider fallback
        - Routing logic
        deliverables:
        - Channel interface defining send, validate, and format methods for email, SMS, and push notification backends
        - Concrete channel implementations for email via SMTP, SMS via Twilio, and push via FCM or APNs
        - Routing rules engine that selects the appropriate channel based on notification type and user preference
        - Fallback channel configuration that tries alternative delivery channels when the primary channel fails
        estimated_hours: '11'
      - id: notification-service-m2
        name: Template System
        description: Implement notification templates with localization and personalization
        acceptance_criteria:
        - Notification templates are defined with named placeholders for dynamic content substitution
        - Variable substitution replaces all placeholders with their corresponding data values at send time
        - Localization selects the correct language variant of a template based on the user's locale preference
        - Template preview renders a template with sample data for visual review before enabling it in production
        pitfalls:
        - SMS templates must fit in 160 chars (or pay for multiple segments)
        - HTML email needs inline styles - CSS classes often stripped
        - Template variables need escaping for XSS prevention in HTML
        - Locale detection should fall back gracefully (vi -> vi-VN -> en)
        concepts:
        - Mustache/Handlebars template syntax and rendering
        - Context-free grammar for safe variable interpolation
        - ICU MessageFormat for pluralization rules
        - Content Security Policy for HTML sanitization
        - Fallback locale chain resolution
        skills:
        - Template engines
        - i18n
        - Content personalization
        deliverables:
        - Template storage system that manages notification templates identified by name and version
        - Variable substitution engine that replaces placeholder tokens in templates with dynamic data values
        - Localization support that selects the correct template translation based on the recipient's locale
        - Template versioning system that tracks template changes and allows rollback to previous versions
        estimated_hours: '11'
      - id: notification-service-m3
        name: User Preferences & Unsubscribe
        description: Implement user notification preferences with granular controls and one-click unsubscribe
        acceptance_criteria:
        - Notification preferences are stored per user and per category controlling which channels receive which notifications
        - Unsubscribe requests immediately suppress future notifications for the specified channel or category
        - Notification categories like marketing, transactional, and alerts can be individually toggled by the user
        - Quiet hours settings suppress non-urgent notifications during the user's configured do-not-disturb window
        pitfalls:
        - Transactional emails (receipts, password reset) can't be unsubscribed per CAN-SPAM
        - One-click unsubscribe required for marketing - RFC 8058
        - Unsubscribe token must be signed to prevent enumeration attacks
        - 'GDPR: must honor unsubscribe within 10 days (do it immediately)'
        concepts:
        - HMAC signing for unsubscribe tokens
        - Preference inheritance and override hierarchy
        - List-Unsubscribe header format RFC 8058
        - Transactional vs marketing email classification
        - Audit logging for consent changes
        skills:
        - Preference management
        - GDPR compliance
        - Unsubscribe handling
        deliverables:
        - Preference storage that persists per-user notification preferences for each channel and category
        - Channel opt-out mechanism that allows users to disable specific notification channels entirely
        - Unsubscribe link generator that creates tokenized one-click unsubscribe URLs for email notifications
        - Preference management API with endpoints to read, update, and reset notification preferences
        estimated_hours: '11'
      - id: notification-service-m4
        name: Delivery Tracking & Analytics
        description: Implement delivery status tracking, open/click tracking, and analytics
        acceptance_criteria:
        - Delivery status is tracked through sent, delivered, opened, clicked, bounced, and failed states for each notification
        - Open and click events are recorded with timestamps and linked back to the originating notification
        - Delivery metrics including delivery rate, bounce rate, and open rate are calculated per channel and campaign
        - Alerts fire when delivery failure rate exceeds a configurable threshold indicating a channel issue
        pitfalls:
        - Email opens tracked via pixel are unreliable (image blocking)
        - Apple Mail Privacy Protection prefetches pixels - inflates open rates
        - Don't track transactional emails for privacy (password resets)
        - Spam complaints must trigger immediate unsubscribe
        concepts:
        - Webhook payload signatures for delivery events
        - UTM parameters for click attribution
        - Transparent 1x1 GIF tracking pixel
        - Event stream processing for real-time metrics
        - Bounce classification soft vs hard
        skills:
        - Event tracking
        - Pixel tracking
        - Analytics
        deliverables:
        - Delivery status tracker that records sent, delivered, bounced, and failed states for each notification
        - Open and click tracker that detects when email recipients open a message or click embedded links
        - Bounce handler that processes hard and soft bounces and updates recipient reachability status
        - Analytics dashboard that visualizes delivery rates, open rates, and click rates across channels and time
        estimated_hours: '11'
    - id: graphql-server
      name: GraphQL Server
      description: Schema-first API with resolvers, dataloaders, subscriptions
      difficulty: intermediate
      estimated_hours: '40'
      essence: Schema-first type system with resolver functions for field-level data fetching, request-scoped batching and caching through DataLoader to collapse redundant database queries into single batch operations, and persistent WebSocket connections enabling server-initiated push of real-time data updates to subscribed clients.
      why_important: Building this teaches you efficient API architecture patterns that prevent common performance pitfalls (N+1 queries), and gives you hands-on experience with real-time bidirectional protocols used in production systems like chat, notifications, and live dashboards.
      learning_outcomes:
      - Design type-safe GraphQL schemas
      - Implement efficient data fetching with dataloaders
      - Handle authentication and authorization in GraphQL
      - Build real-time features with subscriptions
      skills:
      - GraphQL schema design
      - Resolver patterns
      - N+1 query prevention
      - Real-time subscriptions
      - Authentication in GraphQL
      - Error handling
      tags:
      - api
      - dataloaders
      - intermediate
      - resolvers
      - schema
      - service
      architecture_doc: architecture-docs/graphql-server/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible: []
      resources:
      - name: GraphQL Official Documentation
        url: https://graphql.org/learn/
        type: documentation
      - name: How to GraphQL Full Tutorial
        url: https://www.howtographql.com/
        type: tutorial
      - name: Apollo Server Subscriptions Guide
        url: https://www.apollographql.com/docs/apollo-server/data/subscriptions
        type: documentation
      - name: Solving N+1 with DataLoader
        url: https://www.graphql-js.org/docs/n1-dataloader/
        type: article
      - name: gqlgen Getting Started (Go)
        url: https://gqlgen.com/getting-started/
        type: tutorial
      prerequisites:
      - type: skill
        name: REST API basics
      - type: skill
        name: Database queries
      milestones:
      - id: graphql-server-m1
        name: Schema & Type System
        description: Define GraphQL schema with types, queries, mutations
        acceptance_criteria:
        - Define types, queries, and mutations using GraphQL SDL syntax
        - Support custom scalar types with serialize, parseValue, and parseLiteral methods
        - Implement interfaces and unions for polymorphic type relationships
        - Validate schema at startup and report errors for invalid definitions
        pitfalls:
        - Making too many fields nullable leading to complex null handling
        - Creating overly nested type hierarchies that are hard to query
        - Forgetting to define input types for mutation arguments
        - Not planning for schema evolution and backward compatibility
        - Missing descriptions on types and fields for documentation
        concepts:
        - Schema Definition Language (SDL) syntax and type system
        - Scalar types vs Object types vs Interface types
        - Non-nullable fields (!) and list types ([Type])
        - Query vs Mutation vs Subscription operation types
        - Schema stitching and federation patterns
        skills:
        - SDL
        - Type definitions
        - Schema design
        deliverables:
        - GraphQL schema with Object types defining entity structure
        - Query type with field resolvers for data retrieval operations
        - Mutation type for CRUD operations modifying server state
        - Input types for mutations with validated field definitions
        - Custom scalar types such as DateTime and JSON with serialization
        - Enum types for fixed-value fields with explicit allowed values
        estimated_hours: '10'
      - id: graphql-server-m2
        name: Resolvers & Data Fetching
        description: Implement resolvers that fetch data from database
        acceptance_criteria:
        - Implement resolver functions per field that fetch correct data from database
        - Pass context object containing database connection and user info to all resolvers
        - Handle resolver errors gracefully and return structured GraphQL error responses
        - Support async resolvers for non-blocking database and external service calls
        pitfalls:
        - Making database queries inside every resolver causing N+1 problems
        - Not handling rejected promises leading to unhandled errors
        - Exposing sensitive data by returning entire database models
        - Blocking the event loop with synchronous operations
        - Forgetting to validate and sanitize resolver arguments
        concepts:
        - Resolver function signature with parent, args, context, info parameters
        - Context object for sharing dependencies across resolvers
        - Async/await patterns for database and API calls
        - Field-level resolvers vs default property resolution
        - Error handling and formatting in resolver functions
        skills:
        - Resolver functions
        - Context
        - Database queries
        deliverables:
        - Root query resolvers handling top-level data fetching operations
        - Field resolvers for nested types loading related entity data
        - Mutation resolvers with input validation and error handling
        - Context setup providing database connection and auth info to resolvers
        - Error handling in resolvers with structured GraphQL error responses
        - Resolver composition patterns for reusable middleware and authorization
        estimated_hours: '10'
      - id: graphql-server-m3
        name: DataLoader & N+1 Prevention
        description: Batch and cache database queries to prevent N+1 problem
        acceptance_criteria:
        - Batch database requests with DataLoader reducing queries from N+1 to 2
        - Cache loaded results within a single request to avoid duplicate fetches
        - Clear DataLoader cache between requests to prevent cross-request data leaks
        - Show N+1 queries eliminated by comparing query counts in request logs
        pitfalls:
        - Sharing DataLoader instances across requests causing data leaks
        - Not properly batching related queries in the same tick
        - Creating new DataLoader instances inside resolvers
        - Forgetting to handle batch function errors for individual items
        - Caching mutable data without invalidation strategy
        concepts:
        - DataLoader batching window and request coalescing
        - Cache key function for proper entity identification
        - Per-request scoping to prevent cache leakage between users
        - Batch loading function implementation patterns
        - Prime and clear methods for cache management
        skills:
        - DataLoader
        - Batching
        - Caching
        deliverables:
        - DataLoader instance for each entity type in the schema
        - Batch function implementation loading multiple records in single query
        - Per-request caching avoiding duplicate loads within one GraphQL operation
        - DataLoader integration in resolver context for transparent batching
        - Handling batch errors mapping failures to individual requested keys
        - Cache invalidation strategy preventing stale data across requests
        estimated_hours: '10'
      - id: graphql-server-m4
        name: Subscriptions
        description: Real-time updates via WebSocket subscriptions
        acceptance_criteria:
        - WebSocket connection established for GraphQL subscription transport layer
        - Publish events to all active subscribers on matching subscription channels
        - Handle subscription filtering so subscribers receive only relevant event data
        - Clean up subscription resources on client disconnect or connection timeout
        pitfalls:
        - Not cleaning up subscription connections causing memory leaks
        - Missing authentication checks on subscription connections
        - Sending too much data in subscription payloads
        - Not handling WebSocket disconnect events properly
        - Forgetting to filter subscription events per user permissions
        concepts:
        - WebSocket protocol upgrade and connection lifecycle
        - PubSub pattern for decoupling publishers and subscribers
        - Subscription resolver returning AsyncIterator or AsyncGenerator
        - Topic-based routing and wildcard subscriptions
        - Connection initialization and authentication for subscriptions
        skills:
        - WebSocket
        - PubSub
        - Event-driven
        deliverables:
        - WebSocket transport setup for persistent subscription connections
        - PubSub implementation for event broadcasting to subscriber channels
        - Subscription resolvers yielding events via async iterator pattern
        - Filtered subscriptions delivering only events matching subscriber criteria
        - Authentication for subscriptions validating credentials on WebSocket connect
        - Connection lifecycle handling for connect, disconnect, and keep-alive events
        estimated_hours: '10'
    - id: background-job-processor
      name: Background Job Processor
      description: Async task queue like Sidekiq/Celery with retries, scheduling
      difficulty: intermediate
      estimated_hours: '50'
      essence: Durable message persistence in Redis with atomic enqueue/dequeue operations, worker pool concurrency management, and failure recovery through retry scheduling with exponential backoff while maintaining job ordering and idempotency guarantees across distributed worker nodes.
      why_important: Building this teaches you distributed systems fundamentals essential for scalable applications‚Äîfrom handling failure scenarios gracefully to understanding concurrency patterns that power production systems at companies like Stripe, GitHub, and Shopify.
      learning_outcomes:
      - Design reliable async job processing systems
      - Implement exponential backoff and retry logic
      - Handle job failures gracefully
      - Build monitoring and observability for background jobs
      skills:
      - Message queues
      - Worker processes
      - Job scheduling
      - Retry strategies
      - Concurrency control
      - Job persistence
      tags:
      - data-structures
      - devops
      - intermediate
      - queues
      - retries
      - scheduling
      - workers
      architecture_doc: architecture-docs/background-job-processor/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible: []
      resources:
      - name: Celery Documentation
        url: https://docs.celeryq.dev/en/stable/getting-started/first-steps-with-celery.html
        type: documentation
      - name: 'RQ: Simple Job Queues for Python'
        url: https://python-rq.org/
        type: documentation
      - name: Redis Streams
        url: https://redis.io/docs/latest/develop/data-types/streams/
        type: documentation
      - name: AWS Retry with Backoff Pattern
        url: https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/retry-backoff.html
        type: article
      - name: Exponential Backoff in Distributed Systems
        url: https://betterstack.com/community/guides/monitoring/exponential-backoff/
        type: tutorial
      prerequisites:
      - type: skill
        name: Redis basics
      - type: skill
        name: Process management
      milestones:
      - id: background-job-processor-m1
        name: Job Queue Core
        description: Basic job enqueueing and storage in Redis
        acceptance_criteria:
        - Jobs are enqueued with a JSON-serialized payload to a Redis list and are retrievable in FIFO order
        - Multiple named queues with configurable priority weights control the order workers poll them
        - Job serialization and deserialization round-trips correctly preserving all payload fields and types
        - Payloads larger than 1MB are rejected at enqueue time with a descriptive validation error
        pitfalls:
        - Not handling Redis connection failures during enqueue operations
        - Forgetting to set TTL on job data causing memory bloat
        - Using blocking operations that prevent graceful shutdown
        - Serializing non-serializable objects like file handles or database connections
        concepts:
        - Redis LPUSH/RPOP for FIFO queue semantics
        - Job serialization with msgpack or JSON for cross-language compatibility
        - Atomic operations to prevent race conditions in multi-worker scenarios
        - Connection pooling to Redis for efficient resource usage
        skills:
        - Redis lists
        - Job serialization
        - Queue operations
        deliverables:
        - Job class with JSON serialization for storing job type, arguments, and metadata
        - Enqueue operation using Redis LPUSH to atomically add jobs to the queue
        - Multiple named queues with independent priority levels for different job categories
        - Job ID generation using UUIDs or ULID to uniquely identify each enqueued job
        - Job payload validation that rejects malformed or oversized payloads before enqueuing
        - Queue inspection APIs that return queue length, peek at pending jobs, and list queue names
        estimated_hours: '10'
      - id: background-job-processor-m2
        name: Worker Process
        description: Worker that processes jobs from queue
        acceptance_criteria:
        - Workers poll configured queues and dispatch jobs to the correct handler based on job type
        - Configurable concurrency allows a single worker process to execute multiple jobs in parallel
        - Graceful shutdown on SIGTERM completes the currently running job before the worker process exits
        - Worker heartbeat is updated at a regular interval and stale workers are detected by the monitoring system
        pitfalls:
        - Not catching exceptions in job execution leading to worker crashes
        - Blocking indefinitely on queue reads without timeout
        - Failing to release locks when worker process dies unexpectedly
        - Running CPU-bound tasks without proper process isolation
        concepts:
        - BRPOP for blocking queue reads with timeout handling
        - Worker heartbeat mechanism to detect crashed processes
        - Graceful shutdown with signal handling and job completion
        - Process forking vs threading for job isolation
        skills:
        - Process management
        - Job execution
        - Error handling
        deliverables:
        - Worker main loop that polls queues using BRPOPLPUSH for reliable dequeue with backup
        - Job class registry that maps job type strings to their handler classes for dispatch
        - Job execution wrapper with configurable per-job timeout that kills long-running jobs
        - Graceful shutdown handler that catches SIGTERM and finishes the current job before exiting
        - Worker heartbeat that periodically reports liveness to Redis so dead workers can be detected
        - Concurrent job processing using a thread or process pool for parallel execution
        estimated_hours: '10'
      - id: background-job-processor-m3
        name: Retry & Error Handling
        description: Automatic retries with exponential backoff
        acceptance_criteria:
        - Exponential backoff retries jobs at increasing intervals (1s, 2s, 4s, 8s, ...) with optional jitter
        - Jobs that exhaust their maximum retry count are moved to the dead letter queue with full error history
        - Error details including exception class, message, and stack trace are stored with each failed job record
        - Dead letter queue jobs can be manually retried or deleted through the management API
        pitfalls:
        - Not implementing maximum retry limits causing infinite loops
        - Using fixed retry delays that overload the system
        - Losing error context between retry attempts
        - Retrying non-idempotent operations causing duplicate side effects
        concepts:
        - Exponential backoff algorithm to prevent thundering herd
        - Dead letter queue for jobs exceeding max retry attempts
        - Idempotency keys to handle duplicate job processing
        - Retry state tracking with attempt counter and timestamps
        skills:
        - Retry strategies
        - Dead letter queue
        - Error tracking
        deliverables:
        - Exponential backoff calculator that computes retry delays as base * 2^attempt with optional jitter
        - Retry queue implemented as a Redis sorted set with scheduled execution times as scores
        - Dead letter queue that stores permanently failed jobs after exceeding the maximum retry count
        - Error serialization that captures exception class, message, and backtrace for failed job records
        - Maximum retry limit that is configurable per job type with a global default fallback
        - Custom retry strategy hooks allowing per-job-type override of the default backoff algorithm
        estimated_hours: '10'
      - id: background-job-processor-m4
        name: Scheduling & Cron
        description: Schedule jobs for future execution or recurring
        acceptance_criteria:
        - Delayed jobs are enqueued for future execution and only become available to workers at the scheduled time
        - Recurring jobs with cron syntax are automatically re-enqueued according to their schedule expression
        - Unique job constraints prevent the same recurring job from being enqueued more than once per schedule period
        - Scheduler crash recovery detects missed schedule windows on restart and enqueues overdue jobs immediately
        pitfalls:
        - Not handling daylight saving time transitions in scheduled jobs
        - Using local time instead of UTC for scheduling
        - Missing scheduled jobs during worker downtime without catch-up logic
        - Cron expressions that skip intended execution times due to parsing errors
        concepts:
        - Redis sorted sets for timestamp-based job scheduling
        - Cron expression parsing and next execution calculation
        - Timezone handling for scheduled jobs across regions
        - Polling vs event-driven scheduling architecture trade-offs
        skills:
        - Cron parsing
        - Scheduled jobs
        - Time handling
        deliverables:
        - Delayed job scheduling that enqueues a job for execution at a specific future timestamp
        - Recurring job definitions that specify a job class and arguments to run on a repeated schedule
        - Cron expression parser supporting standard five-field (minute, hour, day, month, weekday) expressions
        - Scheduler process that polls for due scheduled jobs and enqueues them into the appropriate queue
        - Timezone handling that evaluates cron expressions in the configured timezone rather than UTC only
        - Unique job constraints that prevent duplicate enqueue of the same scheduled job within one period
        estimated_hours: '10'
      - id: background-job-processor-m5
        name: Monitoring & Dashboard
        description: Real-time monitoring and web dashboard
        acceptance_criteria:
        - Dashboard tracks queue depth, processing rate, and error rate updated in near real-time
        - Web dashboard displays job counts, worker status, and recent failures in an accessible interface
        - Alerting triggers when queue backlog exceeds a threshold or error rate spikes above the configured limit
        - Job search allows filtering by job ID, status, queue name, and enqueue time range
        pitfalls:
        - Storing all job history causing unbounded memory growth
        - Not implementing pagination for large job lists
        - Exposing sensitive job data in monitoring interface
        - Dashboard queries blocking worker operations on shared Redis instance
        concepts:
        - WebSocket or SSE for real-time metric updates
        - Aggregating job metrics without blocking worker processes
        - Rate limiting dashboard API to prevent Redis overload
        - Queue length and processing rate as key performance indicators
        skills:
        - Metrics
        - Web UI
        - Real-time updates
        deliverables:
        - Job count metrics per queue reporting pending, active, completed, and failed job totals
        - Worker status tracking showing active workers, their current job, and last heartbeat time
        - Job history log recording execution time, status, and error details for completed and failed jobs
        - Failure rate metrics calculating rolling error rates per queue and per job type
        - Web dashboard UI displaying real-time queue depths, worker status, and job history
        - Manual retry and delete controls for managing jobs in the dead letter queue from the dashboard
        estimated_hours: '10'
    advanced:
    - id: social-network
      name: Social Network
      description: Feed, followers, notifications
      difficulty: advanced
      estimated_hours: 60-80
      essence: Graph-based relationship modeling combined with fan-out write/read hybrid architectures for feed generation, real-time bi-directional communication through WebSocket connections, and distributed cache coordination to handle millions of concurrent social interactions at scale.
      why_important: Building this project teaches the core infrastructure patterns behind platforms like Twitter and Instagram‚Äîspecifically how to architect systems that handle complex many-to-many relationships, optimize read-heavy workloads through strategic caching, and deliver real-time updates without database bottlenecks. These skills are fundamental for any engineer working on user-facing platforms at scale.
      learning_outcomes:
      - Implement fan-out-on-write and fan-out-on-read feed architectures with hybrid optimization for celebrity accounts
      - Design graph-based data models for follower relationships using adjacency lists or graph databases
      - Build WebSocket-based notification systems with Redis pub/sub for multi-server coordination
      - Implement distributed caching strategies using Redis lists for timeline storage and cache invalidation
      - Design horizontal sharding schemes for user data distribution across database instances
      - Build message queue systems with Kafka for asynchronous notification delivery and retry logic
      - Implement rate limiting and priority queues to prevent spam and ensure critical notifications deliver first
      - Optimize N+1 query problems in social graphs through strategic denormalization and batch loading
      skills:
      - Fan-out Architecture
      - Graph Data Modeling
      - WebSocket Real-time Communication
      - Distributed Caching
      - Message Queue Systems
      - Horizontal Sharding
      - Redis Pub/Sub
      - Database Denormalization
      tags:
      - advanced
      - feed
      - followers
      - go
      - graph
      - javascript
      - likes
      - networking
      - notifications
      - python
      architecture_doc: architecture-docs/social-network/index.md
      languages:
        recommended:
        - JavaScript
        - Python
        - Go
        also_possible:
        - Ruby
        - Java
        - Elixir
      resources:
      - name: Feed Architecture
        url: https://www.youtube.com/watch?v=QmX2NPkJTKg
        type: video
      - name: System Design Social Network
        url: https://www.youtube.com/results?search_query=system+design+social+network
        type: video
      prerequisites:
      - type: skill
        name: Full-stack web development
      - type: skill
        name: Database design
      - type: skill
        name: Caching concepts
      - type: skill
        name: Real-time systems
      milestones:
      - id: social-network-m1
        name: User Profiles & Follow System
        description: Build user profiles and the follow/follower relationship.
        acceptance_criteria:
        - User profile with bio, avatar, links
        - Follow and unfollow toggles update relationship records atomically
        - Follower and following lists return paginated results with counts
        - Follower and following counts are accurate after bulk operations
        - Profile editing persists changes and validates field constraints
        pitfalls:
        - Self-follow allowed
        - Count drift from denormalization
        - N+1 queries for follower lists
        concepts:
        - Self-referential relations
        - Denormalization
        - Count caching
        skills:
        - Database schema design
        - Many-to-many relationships
        - SQL query optimization
        - API endpoint design
        deliverables:
        - User profile CRUD API supporting bio, avatar, and links fields
        - Follow and unfollow endpoints with duplicate-follow prevention
        - Paginated follower and following list retrieval endpoints
        - Profile privacy settings controlling visibility of posts and follower lists
        estimated_hours: 8-10
      - id: social-network-m2
        name: Posts & Feed (Fan-out on Write)
        description: Implement posts and the home feed using fan-out on write.
        acceptance_criteria:
        - Create text and image posts with validation and storage
        - User's own post list returns all authored posts in order
        - Home feed (posts from followed users)
        - Feed pagination uses cursor-based approach for stable page results
        - Post timestamps display correctly in multiple time zones
        pitfalls:
        - Fan-out blocking post creation (use queue)
        - Celebrity problem (millions of followers)
        - Offset pagination performance
        concepts:
        - Fan-out on write vs read
        - Feed generation
        - Cursor pagination
        skills:
        - Asynchronous processing
        - Message queue implementation
        - Database indexing strategies
        - Pagination techniques
        - Background job systems
        deliverables:
        - Post creation endpoint supporting text and image content types
        - Fan-out service that writes new posts to each follower's feed
        - Feed retrieval endpoint returning chronologically ordered posts
        - Cursor-based feed pagination with stable ordering across pages
        estimated_hours: 10-12
      - id: social-network-m3
        name: Likes, Comments & Interactions
        description: Add social interactions to posts.
        acceptance_criteria:
        - Like and unlike toggle updates count atomically without race conditions
        - Comment on posts with text content and author attribution
        - Like counts update in real-time and reflect accurate totals
        - Comment threads support nested replies with correct parent-child ordering
        - Share and repost duplicates content with attribution to original author
        pitfalls:
        - Double-like race condition
        - Comment ordering (newest vs oldest)
        - Deep nested replies complexity
        concepts:
        - Social interactions
        - Optimistic updates
        - Comment systems
        skills:
        - Transaction management
        - Race condition handling
        - Atomic operations
        - Frontend state management
        - Nested data structures
        deliverables:
        - Like and unlike functionality with idempotent toggle behavior
        - Threaded comment system supporting nested replies to posts
        - Real-time interaction count aggregation for likes and comments
        - Notification trigger events emitted on like, comment, and share actions
        estimated_hours: 8-10
      - id: social-network-m4
        name: Notifications
        description: Build a notification system for social activity.
        acceptance_criteria:
        - Notification fires when a new user follows the recipient
        - Notification fires for likes and comments on user's posts
        - Notification badge count reflects accurate unread total in real-time
        - Mark as read updates status and decrements badge count
        - Real-time notification delivery pushes events within two seconds
        pitfalls:
        - Notification spam (batch similar notifications)
        - Notifying yourself
        - Notification count going negative
        concepts:
        - Notification system
        - Real-time delivery
        - Batching
        skills:
        - Push Notification Architecture
        - Real-time Event Delivery
        - Notification Batching and Deduplication
        - WebSocket Integration
        deliverables:
        - Notification type registry for follow, like, comment, and mention events
        - Notification delivery pipeline routing events to recipient inboxes
        - Read and unread status tracking with bulk mark-as-read support
        - Notification preference settings allowing per-type opt-in and opt-out
        estimated_hours: 8-10
      - id: social-network-m5
        name: Search & Discovery
        description: Add search and content discovery features.
        acceptance_criteria:
        - Search users by name or username with partial match support
        - Search posts by content keywords returning ranked results
        - Trending posts and hashtags update based on recent engagement metrics
        - Suggested users to follow are based on mutual connections or interests
        - Explore page surfaces popular content from outside the user's network
        pitfalls:
        - Search too slow on large datasets
        - Trending manipulation (spam)
        - Recommendation bubbles
        concepts:
        - Search implementation
        - Trending algorithms
        - Recommendation systems
        skills:
        - Full-text search
        - Search indexing
        - Algorithm design
        - Content ranking systems
        - Database query performance
        deliverables:
        - User search endpoint with full-text matching on name and username
        - Post search endpoint with keyword and hashtag content matching
        - Hashtag extraction and indexing system for post content
        - Trending topics aggregation based on recent hashtag and engagement volume
        estimated_hours: 10-12
      - id: social-network-m6
        name: Performance & Scaling
        description: Optimize for performance and prepare for scale.
        acceptance_criteria:
        - Redis caching for feeds reduces database read load by 80 percent
        - Background job processing handles fan-out and notification delivery asynchronously
        - CDN serves media assets with cache-control headers and low latency
        - Database indexing is optimized for common query patterns on feeds and profiles
        - Load testing confirms system handles target concurrent user count
        pitfalls:
        - Cache invalidation complexity
        - Celebrity user problem
        - Hot partition on popular content
        concepts:
        - Caching strategies
        - Background jobs
        - Performance optimization
        skills:
        - Redis caching
        - Database query optimization
        - Load testing
        - Horizontal scaling
        - Worker queue management
        deliverables:
        - Caching strategy using Redis for hot feed and profile data
        - Database sharding scheme partitioning data by user ID range
        - CDN integration for serving uploaded media assets efficiently
        - Rate limiting middleware protecting API endpoints from abuse
        estimated_hours: 12-15
    - id: video-streaming
      name: Video Streaming
      description: HLS, adaptive bitrate
      difficulty: intermediate
      estimated_hours: 30-50
      essence: Chunked multipart upload handling, CPU-intensive FFmpeg transcoding pipelines generating multi-bitrate variants, and HTTP-based adaptive streaming delivery with segment-level quality switching based on client bandwidth conditions.
      why_important: Building this teaches you production-grade video infrastructure patterns used by platforms like YouTube and Netflix, including handling large file uploads, CPU-intensive transcoding workflows, and adaptive streaming protocols critical for modern web applications.
      learning_outcomes:
      - Implement chunked file upload with resumable progress tracking for large media files
      - Build FFmpeg transcoding pipelines to generate multiple quality variants from source videos
      - Design HLS manifest generation with multi-bitrate playlist configuration
      - Implement adaptive bitrate streaming with quality ladder creation (360p, 720p, 1080p)
      - Handle video processing job queues and background worker architecture
      - Build custom video player with quality switching and playback state persistence
      - Debug media streaming issues including buffering, codec compatibility, and segment timing
      - Optimize video delivery using HTTP range requests and CDN integration
      skills:
      - HLS Protocol
      - FFmpeg Transcoding
      - Chunked Upload
      - Adaptive Bitrate Streaming
      - Media Processing Pipelines
      - Background Job Queues
      - Video Codec Handling
      - CDN Integration
      tags:
      - adaptive-bitrate
      - file-transfer
      - go
      - hls
      - intermediate
      - manifest
      - node.js
      - python
      - segments
      architecture_doc: architecture-docs/video-streaming/index.md
      languages:
        recommended:
        - Node.js
        - Python
        - Go
        also_possible:
        - Java
        - Rust
      resources:
      - type: article
        name: HLS Streaming Explained
        url: https://www.cloudflare.com/learning/video/what-is-hls-streaming/
      - type: tool
        name: FFmpeg Documentation
        url: https://ffmpeg.org/documentation.html
      prerequisites:
      - type: skill
        name: HTTP server
      - type: skill
        name: File handling
      - type: skill
        name: Basic frontend
      milestones:
      - id: video-streaming-m1
        name: Video Upload
        description: Handle large video file uploads with progress tracking.
        acceptance_criteria:
        - Chunked upload support allows resumable uploads of large video files
        - Progress tracking reports bytes uploaded and estimated completion percentage
        - File validation rejects unsupported formats and files exceeding size limits
        - Storage management organizes uploaded files with unique identifiers and directory structure
        pitfalls:
        - Memory issues with large files
        - Incomplete uploads
        - Storage cleanup
        concepts:
        - Chunked transfer
        - Resumable uploads
        - File validation
        skills:
        - Multipart form data handling
        - Stream-based file processing
        - Client-side chunking implementation
        - File type and size validation
        - Progress tracking with WebSockets
        deliverables:
        - Upload endpoint accepting video file submissions via multipart form POST
        - Chunked upload support allowing large files to be sent in sequential parts
        - Video storage service persisting uploaded files to object storage or local disk
        - Metadata extraction service reading duration, resolution, and codec from uploaded files
        estimated_hours: 5-8
      - id: video-streaming-m2
        name: Video Transcoding
        description: Convert videos to streaming-friendly formats using FFmpeg.
        acceptance_criteria:
        - FFmpeg integration transcodes uploaded videos into target format and bitrate
        - Multiple quality levels are generated from a single source file
        - Progress monitoring reports transcoding percentage and estimated time remaining
        - Background processing runs transcoding jobs asynchronously without blocking uploads
        pitfalls:
        - FFmpeg memory usage
        - Codec compatibility
        - Interrupted transcoding
        concepts:
        - Video codecs
        - HLS format
        - Background jobs
        skills:
        - FFmpeg command-line integration
        - Video codec selection and parameters
        - Job queue management with Bull or RabbitMQ
        - Async task monitoring and error recovery
        - Container format conversion (MP4, WebM)
        deliverables:
        - FFmpeg integration invoking transcoding commands as subprocess pipelines
        - Multiple quality level output producing 360p, 720p, and 1080p renditions
        - Codec selection logic choosing optimal output codec based on target device compatibility
        - Transcoding queue managing parallel jobs with configurable concurrency limits
        estimated_hours: 8-12
      - id: video-streaming-m3
        name: Adaptive Streaming
        description: Serve HLS streams with quality adaptation.
        acceptance_criteria:
        - HLS manifest serving returns valid M3U8 playlist with segment references
        - Segment serving delivers TS files with correct MIME type headers
        - Byte-range requests support partial content delivery for efficient seeking
        - CDN-friendly headers include cache-control and content-length for edge caching
        pitfalls:
        - CORS for cross-origin players
        - Playlist caching issues
        - Seeking accuracy
        concepts:
        - HLS protocol
        - HTTP range requests
        - Caching strategies
        skills:
        - HLS manifest generation and serving
        - HTTP streaming protocols implementation
        - CDN integration and cache headers
        - Cross-origin resource sharing configuration
        - Efficient byte-range request handling
        deliverables:
        - HLS segment generation splitting transcoded video into fixed-duration TS segments
        - Manifest file creation producing M3U8 playlists listing all segments in order
        - Quality variant playlists referencing multiple rendition playlists for adaptive switching
        - Segment serving endpoint delivering TS files with correct content-type and caching headers
        estimated_hours: 6-10
      - id: video-streaming-m4
        name: Video Player Integration
        description: Build frontend player with quality switching and progress tracking.
        acceptance_criteria:
        - HLS.js integration plays adaptive streams in browsers without native HLS support
        - Quality selector allows manual rendition switching during playback
        - Progress bar displays playback position and supports click-to-seek navigation
        - Playback analytics record view duration, quality switches, and buffering events
        pitfalls:
        - Browser compatibility
        - Memory leaks on unmount
        - Bandwidth estimation
        concepts:
        - Adaptive bitrate
        - Media APIs
        - Analytics
        skills:
        - Video.js or HLS.js integration
        - Quality level switching logic
        - Event-driven playback state management
        - Browser Media Source Extensions API
        - User engagement analytics collection
        deliverables:
        - HLS.js integration initializing the player with a master playlist URL
        - Quality switching UI allowing manual selection or automatic adaptive bitrate
        - Playback controls providing play, pause, seek, and volume adjustment buttons
        - Progress tracking overlay displaying current playback position and total duration
        estimated_hours: 11-20
    - id: api-gateway
      name: API Gateway
      description: Build an API gateway that handles routing, authentication, rate limiting, and request transformation.
      difficulty: advanced
      estimated_hours: 50-70
      essence: HTTP reverse proxy architecture implementing request multiplexing, distributed rate limiting via token bucket algorithms, middleware-based authentication pipelines with JWT validation, and concurrent connection management for routing traffic across backend service clusters.
      why_important: Building this teaches critical distributed systems patterns used in production infrastructure at scale, including request multiplexing, circuit breaking, and observability instrumentation that form the backbone of modern microservices architectures.
      learning_outcomes:
      - Implement reverse proxy routing with dynamic service discovery and load balancing
      - Design token bucket or sliding window rate limiting algorithms with distributed state
      - Build authentication and authorization middleware with JWT validation and claim-based access control
      - Create HTTP request/response transformation pipelines with header manipulation and body parsing
      - Implement circuit breaker patterns to handle backend service failures gracefully
      - Design plugin architecture with hot-reloading for extensibility
      - Build observability layer with structured logging, metrics collection, and distributed tracing
      - Handle connection pooling and HTTP/2 multiplexing for optimal backend communication
      skills:
      - Reverse Proxy Design
      - Rate Limiting Algorithms
      - JWT Authentication
      - HTTP/2 Protocol
      - Circuit Breaker Pattern
      - Distributed Tracing
      - Middleware Architecture
      - Service Discovery
      tags:
      - advanced
      - aggregation
      - api
      - authentication
      - go
      - rate-limiting
      - routing
      - rust
      - service
      architecture_doc: architecture-docs/api-gateway/index.md
      languages:
        recommended:
        - Go
        - Rust
        also_possible:
        - Node.js
        - Java
      resources:
      - name: Kong Gateway
        url: https://docs.konghq.com/
        type: reference
      - name: NGINX as API Gateway
        url: https://www.nginx.com/blog/deploying-nginx-plus-as-an-api-gateway/
        type: article
      prerequisites:
      - type: skill
        name: REST APIs
      - type: skill
        name: Networking
      - type: skill
        name: Load balancing concepts
      milestones:
      - id: api-gateway-m1
        name: Reverse Proxy & Routing
        description: Route requests to appropriate backend services.
        acceptance_criteria:
        - Path-based routing correctly maps URL prefixes to backend services (e.g., /api/users -> user-service)
        - Host-based routing directs requests to different backends based on the Host header (e.g., api.example.com)
        - Load balancing distributes requests across healthy backend instances using the configured strategy
        - Health checks detect unhealthy backends and remove them from the routing pool until they recover
        - Circuit breaker opens after repeated failures to a backend and stops sending traffic for a cooldown period
        pitfalls:
        - Not forwarding original client IP
        - Health checks blocking request handling
        - Memory leaks from unclosed connections
        - Thundering herd on backend recovery
        concepts:
        - Reverse proxy
        - Load balancing
        - Health checks
        - Service discovery
        skills:
        - HTTP Protocol Implementation
        - Load Balancing Algorithms
        - Service Health Monitoring
        - Connection Pool Management
        deliverables:
        - Request routing rules engine that matches incoming requests to backend services
        - Path-based routing that forwards requests to services based on URL prefix or pattern matching
        - Header-based routing that selects backend services based on request header values like Host or X-Version
        - Load balancing across multiple backend instances using round-robin or weighted algorithms
        estimated_hours: 12-18
      - id: api-gateway-m2
        name: Request/Response Transformation
        description: Modify requests and responses as they pass through.
        acceptance_criteria:
        - Header manipulation correctly adds, removes, and modifies headers on both requests and responses
        - Request body transformation rewrites JSON payloads according to configurable mapping rules
        - Response body transformation filters or reshapes backend response data before client delivery
        - URL rewriting modifies the request path and query parameters before forwarding to the backend
        - Request aggregation combines responses from multiple backend calls into a single client response
        pitfalls:
        - Large body buffering causes memory issues
        - Transformation errors not handled gracefully
        - Aggregation timeout causes partial responses
        - Content-Length mismatch after transformation
        concepts:
        - Request transformation
        - Response transformation
        - URL rewriting
        - API aggregation
        skills:
        - Stream Processing
        - JSON/XML Parsing and Serialization
        - HTTP Header Manipulation
        - Response Aggregation Patterns
        deliverables:
        - Header manipulation middleware that adds, removes, or modifies HTTP headers on requests and responses
        - Request body transformation pipeline that rewrites payloads before forwarding to backends
        - Response modification layer that alters backend responses before returning them to the client
        - Protocol translation module that converts between REST and gRPC or other protocol formats
        estimated_hours: 10-14
      - id: api-gateway-m3
        name: Authentication & Authorization Layer
        description: Centralized auth handling at gateway level.
        acceptance_criteria:
        - JWT validation verifies RS256/HS256 signatures and rejects expired or malformed tokens at the gateway
        - API key authentication accepts valid keys and returns 401 Unauthorized for missing or invalid keys
        - OAuth2 token introspection calls the authorization server and caches active/inactive results
        - Authenticated user context (user ID, roles, scopes) is passed to backend services via headers
        - Auth caching stores validated tokens to avoid repeated verification calls for the same token
        pitfalls:
        - Caching tokens too long misses revocations
        - Not validating token audience/issuer
        - API keys in logs or error messages
        - Auth failures not rate limited (brute force)
        concepts:
        - Centralized authentication
        - Token introspection
        - Auth caching
        - Header propagation
        skills:
        - JWT Token Validation
        - OAuth2/OIDC Integration
        - Secure Token Storage
        - Rate Limiting Implementation
        deliverables:
        - JWT validation middleware that verifies token signature, expiration, and required claims
        - API key authentication that looks up and validates keys from a header or query parameter
        - OAuth2 token introspection that verifies opaque tokens against the authorization server
        - Rate limiting per client that enforces request quotas based on API key or JWT subject
        estimated_hours: 10-15
      - id: api-gateway-m4
        name: Observability & Plugins
        description: Add logging, metrics, tracing, and plugin system.
        acceptance_criteria:
        - Structured access logs include method, path, status code, response time, and client IP for every request
        - Prometheus-compatible metrics endpoint exposes request rate, latency percentiles, and error counts
        - Distributed tracing propagates OpenTelemetry trace-id and span-id headers through the gateway to backends
        - Plugin architecture allows loading custom middleware modules without modifying the gateway core code
        - Dynamic configuration reload applies routing and plugin changes without restarting the gateway process
        pitfalls:
        - High cardinality labels in metrics
        - Logging sensitive data (passwords, tokens)
        - Trace sampling too aggressive loses important traces
        - Plugin panics crash entire gateway
        concepts:
        - Observability
        - Prometheus metrics
        - OpenTelemetry tracing
        - Plugin architecture
        skills:
        - Metrics Collection and Exposition
        - Distributed Tracing
        - Structured Logging
        - Plugin System Design
        - Error Recovery and Isolation
        deliverables:
        - Request and response logging middleware that captures method, path, status, and latency
        - Metrics collection module exporting request count, latency histograms, and error rates
        - Distributed tracing header propagation that injects and forwards trace context (W3C or B3 format)
        - Plugin system allowing custom middleware to be loaded and chained at runtime without recompilation
        estimated_hours: 12-18
    - id: distributed-tracing
      name: Distributed Tracing System
      description: Build a distributed tracing system to track requests across microservices.
      difficulty: advanced
      estimated_hours: 40-60
      essence: Causal operation correlation through hierarchical span recording with nanosecond-precision timing, persistent trace identifier propagation across service boundaries via standardized headers, and efficient storage/retrieval of time-series trace data for reconstructing distributed request flows.
      why_important: Building a tracing system teaches the full observability stack from instrumentation to storage to visualization, critical skills for debugging production microservices where requests span dozens of services and failures cascade unpredictably.
      learning_outcomes:
      - Implement W3C Trace Context propagation across service boundaries using traceparent and tracestate headers
      - Design hierarchical span data structures with parent-child relationships and timing instrumentation
      - Build a high-throughput collector service that ingests, batches, and persists trace data at scale
      - Implement trace storage with efficient query patterns for trace ID lookup and time-range searches
      - Design span processors for sampling decisions, attribute enrichment, and data validation
      - Build timeline visualization queries that reconstruct request flows from distributed span data
      - Debug context propagation issues across heterogeneous services using different frameworks and languages
      - Implement baggage propagation for cross-cutting concerns like tenant IDs and feature flags
      skills:
      - Context Propagation
      - Span Instrumentation
      - High-Throughput Data Ingestion
      - Time-Series Storage
      - Sampling Strategies
      - Trace Visualization
      - gRPC/Thrift Protocols
      - Microservices Observability
      tags:
      - advanced
      - context-propagation
      - distributed
      - go
      - java
      - sampling
      - spans
      architecture_doc: architecture-docs/distributed-tracing/index.md
      languages:
        recommended:
        - Go
        - Java
        also_possible:
        - Rust
        - Python
      resources:
      - name: OpenTelemetry
        url: https://opentelemetry.io/docs/
        type: documentation
      - name: Jaeger Architecture
        url: https://www.jaegertracing.io/docs/architecture/
        type: documentation
      prerequisites:
      - type: skill
        name: Microservices
      - type: skill
        name: Networking
      - type: skill
        name: Data storage
      milestones:
      - id: distributed-tracing-m1
        name: Trace Context & Propagation
        description: Implement trace/span IDs and context propagation.
        acceptance_criteria:
        - Generate globally unique trace and span IDs for each operation
        - Propagate trace context via HTTP headers using W3C Trace Context format
        - Propagate trace context via gRPC metadata for inter-service tracing
        - Parent-child span relationships correctly model the call tree hierarchy
        - Context injection and extraction work across HTTP and gRPC boundaries
        pitfalls:
        - Trace ID not propagated to async operations
        - Invalid trace context parsing crashes
        - Generating new trace ID when should continue existing
        - Not handling malformed traceparent
        concepts:
        - Trace context
        - W3C Trace Context spec
        - Context propagation
        - Span hierarchy
        skills:
        - Context propagation across service boundaries
        - W3C Trace Context header parsing
        - UUID generation for trace identifiers
        - Async operation instrumentation
        deliverables:
        - Trace ID generation using 128-bit random identifiers
        - Span ID generation using 64-bit random identifiers
        - Context propagation via W3C Trace Context or B3 headers
        - Baggage items for cross-service key-value metadata propagation
        estimated_hours: 8-12
      - id: distributed-tracing-m2
        name: Span Recording
        description: Record span data with timing, tags, and logs.
        acceptance_criteria:
        - Start and end spans with precise timing for duration calculation
        - Add tags and attributes to spans for filtering and searching
        - Add log events within spans to record notable activities and data
        - Record errors and exceptions with status and detailed messages on spans
        - Span status correctly reflects OK or Error outcome of the operation
        pitfalls:
        - Span End() never called (memory leak)
        - Too many attributes causes large payloads
        - High cardinality attribute values
        - Blocking on export channel
        concepts:
        - Span recording
        - Attributes and events
        - Error recording
        - Batch export
        skills:
        - Structured span lifecycle management
        - Memory-efficient attribute storage
        - Non-blocking asynchronous export
        - Error context capture
        deliverables:
        - Span start and finish recording with precise timestamps
        - Span attributes for tagging with key-value metadata pairs
        - Span events and logs for recording in-span activities
        - Error recording with exception details and stack traces
        estimated_hours: 8-12
      - id: distributed-tracing-m3
        name: Collector & Storage
        description: Build collector to receive, process, and store traces.
        acceptance_criteria:
        - Receive spans via HTTP or gRPC ingestion endpoint from instrumented services
        - Process and enrich spans with service metadata before storage
        - Store span data in time-series database with efficient indexing for queries
        - Head-based and tail-based sampling strategies reduce storage volume effectively
        - Tail-based sampling retains complete traces for slow or error requests
        pitfalls:
        - Memory exhaustion holding pending traces
        - Sampling bias toward short traces
        - Clock skew affecting duration calculations
        - Hot partition in storage by trace ID
        concepts:
        - Span collection
        - Enrichment
        - Head vs tail sampling
        - Trace storage
        skills:
        - High-throughput stream processing
        - Time-series data storage optimization
        - Probabilistic sampling algorithms
        - Distributed system clock synchronization
        deliverables:
        - Span ingestion API accepting spans via HTTP and gRPC endpoints
        - Storage backend for persisting spans in a time-series database
        - Sampling strategies to reduce volume while preserving important traces
        - Batch processing for efficient span ingestion and storage writes
        estimated_hours: 12-18
      - id: distributed-tracing-m4
        name: Query & Visualization
        description: Query traces and visualize in timeline view.
        acceptance_criteria:
        - Search traces by service name, operation name, and custom tag values
        - Time range queries filter traces within specified start and end timestamps
        - Trace timeline visualization displays span hierarchy with duration bars
        - Service dependency graph shows directed edges between calling and called services
        - Latency percentile calculations (p50, p95, p99) identify slow operations
        pitfalls:
        - Query without time range scans entire database
        - Deep traces cause stack overflow in tree building
        - Timezone issues in timestamp display
        - Missing spans leave gaps in visualization
        concepts:
        - Trace querying
        - Service dependencies
        - Timeline visualization
        - Aggregations
        skills:
        - Time-range indexed querying
        - Recursive tree traversal for trace reconstruction
        - Service dependency graph construction
        - Timezone-aware timestamp rendering
        deliverables:
        - Trace search API supporting service, operation, and tag queries
        - Trace timeline view showing span hierarchy and durations
        - Service dependency map derived from observed inter-service traces
        - Latency analysis with percentile distribution across operations
        estimated_hours: 12-16
    - id: collaborative-editor
      name: Collaborative Editor
      description: Real-time collaborative editing like Google Docs
      difficulty: advanced
      estimated_hours: '50'
      essence: Conflict-free concurrent editing through distributed state synchronization algorithms (CRDTs or Operational Transformation), requiring mathematical correctness for merge operations, causal ordering of edits, and real-time propagation of document mutations across unreliable network connections.
      why_important: Collaborative editing is used in Google Docs, Notion, Figma. Understanding CRDTs and OT is valuable for any real-time collaborative application.
      learning_outcomes:
      - Implement Operational Transformation or CRDTs
      - Handle concurrent edits without conflicts
      - Build cursor presence and selection sync
      - Design efficient document synchronization
      skills:
      - CRDT Implementation
      - Operational Transformation
      - WebSocket Real-time Sync
      - Distributed Consensus
      - Concurrent State Management
      - Conflict Resolution Algorithms
      - Cursor Presence Protocols
      tags:
      - advanced
      - algorithms
      - concurrency
      - crdt
      - distributed-systems
      - operational-transform
      - real-time
      - sync
      architecture_doc: architecture-docs/collaborative-editor/index.md
      languages:
        recommended:
        - JavaScript
        - TypeScript
        - Go
        also_possible: []
      resources:
      - name: Yjs CRDT Documentation
        url: https://docs.yjs.dev/
        type: documentation
      - name: CRDT Papers Collection
        url: https://crdt.tech/papers.html
        type: paper
      - name: Operational Transformation Article
        url: https://hackernoon.com/operational-transformation-the-real-time-collaborative-editing-algorithm-bf8756683f66
        type: article
      - name: Collaborative Editing with OT Tutorial
        url: https://srijancse.medium.com/how-real-time-collaborative-editing-work-operational-transformation-ac4902d75682
        type: tutorial
      - name: A Comprehensive Study of CRDTs
        url: https://pages.lip6.fr/Marc.Shapiro/papers/RR-7687.pdf
        type: paper
      prerequisites:
      - type: project
        id: chat-app
        name: Real-time Chat
      - type: skill
        name: WebSocket protocol
      - type: skill
        name: Conflict resolution algorithms
      milestones:
      - id: collaborative-editor-m1
        name: Operation-based CRDT
        description: Implement a sequence CRDT (like RGA or YATA) for conflict-free text insertion and deletion.
        acceptance_criteria:
        - Implement RGA (Replicated Growable Array) for ordered text
        - Handle concurrent insert operations at the same position correctly
        - Ensure convergence across all replicas after all operations are applied
        - Support delete operations that correctly remove characters from shared document
        pitfalls:
        - Timestamp collision when sites have same clock
        - Memory grows unbounded with tombstones
        - O(n) lookup for every operation is slow on large docs
        concepts:
        - Logical clocks and Lamport timestamps for ordering
        - Tombstone-based deletion with garbage collection
        - Position identifiers in sequence CRDTs
        - Causal ordering and happened-before relationships
        skills:
        - CRDT implementation and algorithms
        - Distributed systems conflict resolution
        - Data structure design for collaborative editing
        - Performance optimization for large documents
        deliverables:
        - Insert and delete operation types with positional metadata
        - Causal ordering mechanism using vector clocks or similar
        - Convergence guarantees across all connected replicas
        - Operation merging logic for concurrent edits
        estimated_hours: '12.5'
      - id: collaborative-editor-m2
        name: Cursor Presence
        description: Synchronize cursor positions and selections across all connected editors with colored indicators.
        acceptance_criteria:
        - Track and store cursor positions per connected user in real time
        - Broadcast cursor update events to all connected peers in real-time
        - Display user name and assigned color label at each remote cursor position
        - Handle cursor position recalculation after local or remote document edits
        pitfalls:
        - Cursor flickers when transforming on every keystroke
        - Selection can become inverted (end < start) after transform
        - Not handling cursor at document boundaries
        concepts:
        - Coordinate transformation across concurrent edits
        - Real-time state synchronization protocols
        - Visual feedback and UI state management
        - Position-based indexing in mutable documents
        skills:
        - WebSocket real-time communication
        - UI state synchronization
        - Cursor tracking and transformation
        - Client-side rendering optimization
        deliverables:
        - Cursor position sharing protocol between connected users
        - User color assignment for distinguishing remote cursors
        - Remote cursor rendering overlay in the editor view
        - Cursor position transformation after concurrent document edits
        estimated_hours: '12.5'
      - id: collaborative-editor-m3
        name: Operational Transformation
        description: Implement OT as alternative to CRDTs, with transform functions for insert/delete operations.
        acceptance_criteria:
        - Transform operations against concurrent operations to preserve intent
        - Maintain document consistency across all connected clients after transforms
        - Handle operation priority and ordering for conflicting concurrent edits
        - Support intention preservation so each user's edits have expected effect
        pitfalls:
        - 'Transform functions must be composable: T(T(a,b),c) = T(a,T(b,c))'
        - Overlapping delete transforms are notoriously tricky
        - History grows unbounded without garbage collection
        concepts:
        - Transform function correctness properties (TP1, TP2)
        - Composition and inverse operations in OT
        - Vector clocks for causality tracking
        - Convergence guarantees in distributed systems
        skills:
        - Operational Transformation algorithms
        - Formal verification of transform properties
        - Concurrent operation handling
        - State-based replication techniques
        deliverables:
        - Operation representation supporting insert and delete types
        - Transform function implementation for concurrent operation pairs
        - Server-side operation ordering with total order guarantee
        - Client-side operation buffer for pending unacknowledged operations
        estimated_hours: '12.5'
      - id: collaborative-editor-m4
        name: Undo/Redo with Collaboration
        description: Implement undo/redo that works correctly with concurrent edits from multiple users.
        acceptance_criteria:
        - Track operation history per user for selective undo capability
        - Implement selective undo that reverses only the current user's own operations
        - Handle undo of merged operations that were transformed against concurrent edits
        - Support redo after undo to reapply previously reversed operations correctly
        pitfalls:
        - Undoing a delete requires storing the deleted text
        - Undo stack becomes invalid if user disconnects/reconnects
        - Group related operations (e.g., typing) into single undo unit
        concepts:
        - Inverse operations for reversible edits
        - Command pattern with operation history
        - Operation composition for grouped actions
        - Causal dependencies in undo operations
        skills:
        - Undo/redo stack management
        - Event sourcing patterns
        - Collaborative editing semantics
        - State reconciliation after reconnection
        deliverables:
        - Local undo stack tracking user's own operations
        - Selective undo that reverses only the current user's operations
        - Redo functionality that reapplies previously undone operations
        - Undo transformation against concurrent edits from other users
        estimated_hours: '12.5'
    - id: media-processing
      name: Media Processing Pipeline
      description: Image/video transcoding, thumbnails, CDN
      difficulty: advanced
      estimated_hours: '40'
      essence: Concurrent media transformation pipeline managing codec-level video transcoding with FFmpeg, asynchronous image format conversion across multiple workers, and distributed job queue orchestration with real-time state synchronization for processing progress across unreliable worker processes.
      why_important: Building this teaches you production-grade async processing architecture, codec-level media manipulation, and distributed systems patterns that are essential for scalable content platforms and streaming services.
      learning_outcomes:
      - Implement image resizing and format conversion with Pillow handling WebP and AVIF optimization
      - Design async task queue architecture with Celery and RabbitMQ for distributed processing
      - Build FFmpeg video transcoding pipeline supporting H.264 and VP9 codecs for web delivery
      - Implement real-time progress tracking across worker processes with state management
      - Debug memory-efficient streaming processing for large media files
      - Design resilient error handling and retry logic for failed transcoding jobs
      - Optimize codec parameters for quality-to-size tradeoffs in web video delivery
      - Implement thumbnail generation with frame extraction from video sources
      skills:
      - Async Task Queues
      - Video Codec Transcoding
      - Image Format Optimization
      - FFmpeg Pipeline Design
      - Distributed Worker Systems
      - Message Broker Patterns
      - Progress Tracking Systems
      - Memory-Efficient Streaming
      tags:
      - advanced
      - ffmpeg
      - images
      - media
      - media-processing
      - streaming
      - thumbnails
      - transcoding
      - video
      architecture_doc: architecture-docs/media-processing/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible: []
      resources:
      - name: FFmpeg Official Documentation
        url: https://www.ffmpeg.org/documentation.html
        type: documentation
      - name: Pillow Python Imaging Library
        url: https://pillow.readthedocs.io/
        type: documentation
      - name: Celery Distributed Task Queue
        url: https://www.linode.com/docs/guides/task-queue-celery-rabbitmq/
        type: tutorial
      - name: Modern Image Formats Guide
        url: https://www.smashingmagazine.com/2021/09/modern-image-formats-avif-webp/
        type: article
      - name: Web Video Codec Reference
        url: https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs
        type: documentation
      prerequisites:
      - type: skill
        name: Message queue/pub-sub basics
      - type: skill
        name: File I/O and streaming
      milestones:
      - id: media-processing-m1
        name: Image Processing
        description: Implement image resizing, format conversion, and optimization
        acceptance_criteria:
        - Images are resized to multiple target dimensions while preserving aspect ratio or using center-crop
        - Format conversion between JPEG, PNG, and WebP produces valid output with configurable quality levels
        - Thumbnails are generated at standard sizes with smart cropping that focuses on the image center of interest
        - EXIF metadata including orientation, camera, and GPS data is read, preserved, or stripped as configured
        pitfalls:
        - 'EXIF orientation: rotate image according to tag before processing'
        - WebP/AVIF save significant bandwidth but check browser support
        - Lanczos is best for downscaling; use different for upscaling
        - Strip EXIF from output for privacy (location data)
        concepts:
        - Color space conversion (RGB/CMYK/YUV)
        - Image interpolation algorithms (nearest-neighbor/bilinear/bicubic/Lanczos)
        - Lossy vs lossless compression trade-offs
        - Image metadata (EXIF/IPTC/XMP) handling
        - Pixel density and aspect ratio preservation
        skills:
        - Image formats
        - Resizing algorithms
        - Optimization
        deliverables:
        - Image loader that reads JPEG, PNG, WebP, and GIF files into an in-memory pixel buffer
        - Resize and crop engine that scales images to target dimensions with configurable aspect ratio handling
        - Format converter that transcodes images between JPEG, PNG, WebP, and AVIF with quality settings
        - Thumbnail generator that produces small preview images at standard sizes with smart cropping
        estimated_hours: '13.5'
      - id: media-processing-m2
        name: Video Transcoding
        description: Implement video transcoding with FFmpeg for web playback
        acceptance_criteria:
        - Videos are transcoded to multiple format and codec combinations including H.264, H.265, and VP9
        - Adaptive bitrate variants are generated at multiple resolutions and bitrates for streaming playback
        - Video thumbnail frames are extracted at configurable time offsets as still image files
        - HLS and DASH output generates properly segmented manifest files and media segments for adaptive playback
        pitfalls:
        - H.264 baseline profile for maximum compatibility
        - -movflags +faststart enables progressive download
        - HLS segment size affects startup time vs seeking
        - Video processing is CPU-intensive - use job queue
        concepts:
        - Container formats vs codec selection (MP4/MKV/WebM with H.264/H.265/VP9)
        - Constant Rate Factor (CRF) for quality-based encoding
        - Keyframe intervals and GOP structure for seeking
        - Adaptive bitrate streaming (HLS/DASH) segmentation
        - Audio/video synchronization and stream mapping
        skills:
        - FFmpeg
        - Video codecs
        - HLS streaming
        deliverables:
        - FFmpeg integration layer that wraps command-line FFmpeg calls with progress parsing and error handling
        - Codec selection module that maps input formats to optimal output codecs based on target platform
        - Bitrate controller that manages constant and variable bitrate encoding with target file size constraints
        - Resolution scaler that transcodes video to multiple resolution variants for adaptive streaming
        estimated_hours: '13.5'
      - id: media-processing-m3
        name: Processing Queue & Progress
        description: Implement async processing queue with progress tracking
        acceptance_criteria:
        - Media processing jobs are queued with priority ordering and executed by available worker processes
        - Processing progress is tracked and queryable showing percentage complete and estimated time remaining
        - Failed jobs are retried with exponential backoff up to a configurable maximum retry count
        - Webhook notifications are sent on job completion, failure, and progress milestone events
        pitfalls:
        - Video progress is hard to estimate - use stages not percentage
        - Webhook delivery can fail - implement retry
        - Clean up temp files after processing
        - 'Memory limits: process one large video at a time'
        concepts:
        - Message queue patterns (pub/sub, work queues, priority queues)
        - Idempotent job processing and duplicate detection
        - Webhook signature verification and authentication
        - Exponential backoff for retry strategies
        - Process isolation and resource limits (memory/CPU quotas)
        skills:
        - Job queues
        - Progress tracking
        - Webhooks
        deliverables:
        - Job queue that accepts media processing requests and schedules them for worker execution
        - Progress tracker that reports percentage completion for each active processing job in real time
        - Webhook notification sender that posts job status updates to configured callback URLs
        - Error handler that retries failed jobs with exponential backoff and reports permanent failures
        estimated_hours: '13.5'
    - id: multi-tenant-saas
      name: Multi-tenant SaaS Backend
      description: Tenant isolation, row-level security, billing integration
      difficulty: advanced
      estimated_hours: '60'
      essence: Request-scoped tenant context propagation through middleware layers coupled with database-level isolation mechanisms‚Äîdiscriminator columns with row-level security policies, schema-per-tenant partitioning, or database-per-tenant sharding‚Äîthat enforce data boundaries while managing per-tenant configuration state, usage metering, and the architectural trade-offs between resource sharing efficiency and isolation guarantees.
      why_important: Building this teaches you production SaaS architecture patterns that are critical for any backend engineer working on B2B platforms, combining database security, distributed systems design, and business logic that directly impacts revenue and compliance.
      learning_outcomes:
      - Design scalable multi-tenant architectures
      - Implement secure tenant data isolation
      - Handle tenant-specific customizations
      - Build usage-based billing systems
      skills:
      - Multi-tenancy patterns
      - Row-level security
      - Tenant isolation
      - Database design
      - Request context
      - Billing integration
      tags:
      - advanced
      - backend
      - isolation
      - multi-tenancy
      - row-level-security
      - security
      - tenants
      architecture_doc: architecture-docs/multi-tenant-saas/index.md
      languages:
        recommended:
        - Python
        - Go
        - Java
        also_possible: []
      resources:
      - name: PostgreSQL Row-Level Security
        url: https://www.postgresql.org/docs/current/ddl-rowsecurity.html
        type: documentation
      - name: AWS Multi-Tenant Architectures Guidance
        url: https://aws.amazon.com/solutions/guidance/multi-tenant-architectures-on-aws/
        type: documentation
      - name: Azure SQL Multi-Tenant Patterns
        url: https://learn.microsoft.com/en-us/azure/azure-sql/database/saas-tenancy-app-design-patterns
        type: documentation
      - name: Designing Postgres for Multi-Tenancy
        url: https://www.crunchydata.com/blog/designing-your-postgres-database-for-multi-tenancy
        type: article
      - name: SaaS Architecture Fundamentals
        url: https://docs.aws.amazon.com/whitepapers/latest/saas-architecture-fundamentals/re-defining-multi-tenancy.html
        type: documentation
      prerequisites:
      - type: skill
        name: REST API
      - type: skill
        name: Database design
      - type: skill
        name: Authentication
      milestones:
      - id: multi-tenant-saas-m1
        name: Tenant Data Model
        description: Design database schema for multi-tenancy
        acceptance_criteria:
        - All application data tables include a tenant_id column with a foreign key to the tenant table
        - Tenant context is extracted from each request and made available throughout the request lifecycle
        - Per-tenant configuration settings are stored and retrieved for customizing tenant behavior
        - Tenant creation provisions a new record with default settings and tenant deletion soft-deletes all data
        pitfalls:
        - Missing tenant_id in junction tables breaking isolation
        - Forgetting indexes on tenant_id columns causing slow queries
        - Using GUID tenant identifiers without considering index size
        - Not planning for tenant data migration or backup strategies
        - Circular foreign key dependencies when modeling tenant relationships
        concepts:
        - Tenant identifier propagation through foreign keys
        - Composite primary keys with tenant_id prefix
        - Database-level isolation vs shared schema patterns
        - Index design for multi-tenant queries
        - Tenant metadata and configuration storage
        skills:
        - Schema design
        - Foreign keys
        - Indexes
        deliverables:
        - Tenant table with columns for ID, name, subdomain, plan, settings JSON, and creation timestamp
        - Tenant ID foreign key column added to all application data tables for ownership tracking
        - Composite database indexes that include tenant_id as a prefix for efficient tenant-scoped queries
        - Tenant provisioning workflow that creates the tenant record and initializes default settings
        - Tenant subdomain and slug mapping table that resolves custom subdomains to tenant IDs
        - Soft delete flag on tenant data that marks records as deleted without physically removing them
        estimated_hours: '12'
      - id: multi-tenant-saas-m2
        name: Request Context & Isolation
        description: Automatic tenant context injection in requests
        acceptance_criteria:
        - Tenant is extracted from the request subdomain, X-Tenant-ID header, or JWT tenant claim
        - Database queries are automatically filtered by the current tenant context without manual WHERE clauses
        - Cross-tenant data access attempts are blocked and logged as security audit events
        - Tenant context is included in all log entries for request tracing and security auditing
        pitfalls:
        - Thread-local context leaking between async requests
        - Setting tenant context after database queries already executed
        - Not validating tenant authorization before setting context
        - Missing tenant context cleanup causing cross-tenant data leaks
        - Forgetting to propagate context through background jobs or queues
        concepts:
        - Request-scoped tenant context storage and lifecycle
        - Middleware ordering for authentication before tenant resolution
        - Thread-local or async context for tenant propagation
        - ORM query filters automatically scoped to current tenant
        - Tenant resolution from JWT claims or subdomain parsing
        skills:
        - Middleware
        - Context management
        - ORM hooks
        deliverables:
        - Tenant resolver that identifies the tenant from request subdomain, header, or JWT claim
        - Request-scoped tenant context that stores the current tenant ID for the duration of each request
        - Automatic tenant ID injection middleware that appends tenant_id filters to all database queries
        - Tenant validation middleware that rejects requests with missing or invalid tenant identification
        - Cross-tenant access prevention guard that blocks any query attempting to access another tenant's data
        - Admin superuser bypass that allows platform operators to access data across all tenants
        estimated_hours: '12'
      - id: multi-tenant-saas-m3
        name: Row-Level Security
        description: Database-level tenant isolation with RLS
        acceptance_criteria:
        - RLS policies restrict all data access to rows matching the current session's tenant ID
        - Isolation is verified by attempting cross-tenant queries which return zero rows instead of other tenant data
        - Admin and superuser roles can bypass RLS policies for platform-level operations and migrations
        - No data leakage occurs when multiple tenants query the same tables simultaneously in parallel requests
        pitfalls:
        - RLS policies not applied to superuser connections
        - Forgetting to set session variable before query execution
        - Complex RLS policies causing full table scans
        - Not testing RLS with different user permission levels
        - Bypass vulnerabilities through database functions or triggers
        concepts:
        - PostgreSQL row-level security policy definition and enforcement
        - Session variables for passing tenant_id to RLS policies
        - Policy performance impact on complex queries
        - RLS policies for INSERT, UPDATE, DELETE, and SELECT operations
        - Combining RLS with application-level security checks
        skills:
        - PostgreSQL RLS
        - Policies
        - Session variables
        deliverables:
        - Row-level security enablement on all tenant data tables in the database
        - Tenant isolation RLS policies that restrict SELECT, INSERT, UPDATE, and DELETE to the current tenant
        - Session variable setter that configures the current tenant ID in the database connection context
        - Per-operation RLS policies for SELECT, INSERT, UPDATE, and DELETE with appropriate tenant checks
        - Migration bypass mechanism that disables RLS for schema migration scripts running as superuser
        - RLS policy test suite that verifies isolation between multiple test tenants
        estimated_hours: '12'
      - id: multi-tenant-saas-m4
        name: Tenant Customization
        description: Per-tenant features, branding, and configuration
        acceptance_criteria:
        - Per-tenant feature flags enable or disable specific features based on tenant plan and configuration
        - Tenant branding applies custom logo, primary color, and accent color throughout the application UI
        - Tenant-specific integrations like webhooks and API keys are configured and stored per tenant
        - Settings management UI allows tenant administrators to view and update their configuration options
        pitfalls:
        - Configuration cache invalidation across distributed systems
        - Not versioning tenant customizations causing rollback issues
        - Feature flag checks bypassing tenant context validation
        - Allowing dangerous tenant configurations without validation
        - Performance degradation from uncached per-request theme lookups
        concepts:
        - Feature flag evaluation with tenant-specific overrides
        - Hierarchical configuration with tenant, group, and global levels
        - Dynamic theme loading and CSS variable injection
        - Tenant-specific routing and subdomain mapping
        - Configuration caching strategies for performance
        skills:
        - Feature flags
        - Configuration
        - Theming
        deliverables:
        - Tenant settings schema that defines configurable options with types, defaults, and validation rules
        - Feature flag system that enables or disables features per tenant based on their subscription plan
        - Plan-based feature access control that gates premium features behind specific subscription tiers
        - Custom branding module that applies tenant-specific logo, colors, and styling to the application
        - Tenant-specific webhook configuration that triggers callbacks for events relevant to each tenant
        - Settings API and management UI that allows tenants to view and update their configuration options
        estimated_hours: '12'
      - id: multi-tenant-saas-m5
        name: Usage Tracking & Billing
        description: Track usage metrics and integrate with billing
        acceptance_criteria:
        - Usage metrics including API calls, storage, and compute are tracked and aggregated per tenant per period
        - Usage-based billing computes charges from metered events and generates accurate invoice line items
        - Invoices are generated automatically at the end of each billing period with itemized usage details
        - Quota enforcement blocks or throttles API requests when the tenant exceeds their plan limits
        pitfalls:
        - Double-counting usage events without idempotency keys
        - Not handling billing provider webhook retry storms
        - Race conditions when checking quota limits under high load
        - Missing usage events due to async processing failures
        - Incorrect timezone handling in usage period calculations
        concepts:
        - Event-driven metering with idempotent event processing
        - Aggregation strategies for high-volume usage data
        - Integration with Stripe or similar billing provider webhooks
        - Rate limiting and quota enforcement at API gateway
        - Usage data retention and analytics reporting
        skills:
        - Metering
        - Billing APIs
        - Usage limits
        deliverables:
        - Usage event tracker that records metered actions like API calls, storage bytes, and compute minutes per tenant
        - Usage aggregation pipeline that rolls up raw events into daily and monthly totals per tenant
        - Plan limits enforcer that blocks or throttles tenants exceeding their subscription plan quotas
        - Stripe billing integration that creates subscriptions, invoices, and processes payments for each tenant
        - Usage-based pricing calculator that computes charges based on metered usage beyond plan allowances
        - Overage handler that applies additional charges or restricts access when plan limits are exceeded
        estimated_hours: '12'
    expert:
    - id: build-react
      name: Build Your Own React
      description: Virtual DOM, reconciliation, hooks, fiber
      difficulty: expert
      estimated_hours: 60-100
      essence: Component tree representation as lightweight JavaScript objects with tree-diffing algorithms for minimal DOM updates, fiber-based work scheduling for interruptible rendering with cooperative concurrency, and closure-based hooks implementing stateful logic in function components.
      why_important: Building this demystifies React's core mechanisms‚Äîvirtual DOM reconciliation, fiber scheduling, and hooks closures‚Äîgiving you deep insight into how modern UI frameworks optimize rendering performance and manage component lifecycle, knowledge directly applicable to debugging production issues and making informed architectural decisions.
      learning_outcomes:
      - Implement virtual DOM representation using plain JavaScript objects and createElement functions
      - Build a reconciliation algorithm that efficiently diffs virtual DOM trees using keys and type comparison
      - Design a fiber-based scheduler with work units that can be paused and resumed for time-slicing
      - Implement useState hook with closure-based state persistence across renders
      - Create useEffect hook with dependency tracking and cleanup function support
      - Build a commit phase that batches DOM mutations for optimal performance
      - Debug stack vs fiber reconciliation differences and understand cooperative scheduling
      - Implement functional component rendering with hooks array indexing
      skills:
      - Virtual DOM Implementation
      - Tree Diffing Algorithms
      - Fiber Architecture
      - Closure-based State Management
      - Cooperative Scheduling
      - Reconciliation Algorithms
      - Component Lifecycle
      - Functional Programming Patterns
      tags:
      - build-from-scratch
      - expert
      - fiber
      - frontend
      - hooks
      - javascript
      - reconciliation
      - typescript
      - virtual-dom
      - web
      architecture_doc: architecture-docs/build-react/index.md
      languages:
        recommended:
        - JavaScript
        - TypeScript
        also_possible: []
      resources:
      - type: article
        name: Build your own React
        url: https://pomb.us/build-your-own-react/
      - type: video
        name: React Fiber Architecture
        url: https://github.com/acdlite/react-fiber-architecture
      prerequisites:
      - type: skill
        name: DOM manipulation
      - type: skill
        name: JavaScript advanced concepts
      - type: skill
        name: Tree data structures
      milestones:
      - id: build-react-m1
        name: Virtual DOM
        description: Create virtual DOM representation and rendering.
        acceptance_criteria:
        - createElement produces virtual nodes with type, props, and children properties
        - Virtual nodes represent both HTML elements and text content nodes
        - Render function creates matching real DOM elements from virtual node tree
        - Text nodes are correctly created for string and number child values
        pitfalls:
        - Handling null/undefined children
        - Event listener naming
        - SVG namespace
        concepts:
        - Virtual DOM
        - Declarative UI
        - Tree representation
        skills:
        - DOM manipulation
        - Tree traversal algorithms
        - Recursive data structure design
        - JSX-to-object transformation
        deliverables:
        - createElement function constructing virtual node tree structures
        - Virtual node structure storing type, props, and children
        - Render function converting virtual DOM tree to real DOM elements
        - Text node handling for plain string and number children
        estimated_hours: 8-12
      - id: build-react-m2
        name: Reconciliation (Diffing)
        description: Implement efficient DOM updates through reconciliation.
        acceptance_criteria:
        - Diff algorithm identifies minimal set of changes between two virtual trees
        - Patch operations update real DOM without recreating unchanged subtrees
        - Keyed children are reordered efficiently without unnecessary recreation
        - Event handler props are correctly attached and detached during updates
        pitfalls:
        - Key stability
        - Index as key problems
        - DOM node reference tracking
        concepts:
        - Tree diffing
        - Minimal updates
        - Keyed reconciliation
        skills:
        - Algorithm optimization
        - State diffing implementation
        - Memory-efficient updates
        - Child array reconciliation
        deliverables:
        - Diff algorithm comparing old and new virtual DOM trees
        - Patch application updating only changed portions of real DOM
        - Key-based reconciliation for efficient list reordering
        - Property diffing detecting added, removed, and changed attributes
        estimated_hours: 12-18
      - id: build-react-m3
        name: Fiber Architecture
        description: Implement interruptible rendering with fiber nodes.
        acceptance_criteria:
        - Fiber tree mirrors component hierarchy with parent, child, and sibling links
        - Work loop uses requestIdleCallback to process fibers without blocking main thread
        - Commit phase applies all DOM mutations atomically after render completes
        - In-progress render work can be interrupted and resumed without visual artifacts
        pitfalls:
        - Effect ordering
        - Interrupted state
        - Memory leaks in alternate
        concepts:
        - Cooperative scheduling
        - Incremental rendering
        - Work units
        skills:
        - Linked list data structures
        - Asynchronous rendering patterns
        - Priority-based scheduling
        - Double buffering techniques
        - Work loop implementation
        deliverables:
        - Fiber node structure linking parent, child, and sibling fibers
        - Work loop yielding control to browser between unit-of-work increments
        - Commit phase applying all accumulated DOM changes in single batch
        - Concurrent rendering support allowing work interruption and resumption
        estimated_hours: 15-25
      - id: build-react-m4
        name: Hooks
        description: Implement useState and useEffect hooks.
        acceptance_criteria:
        - useState returns current value and setter function triggering component re-render
        - useEffect runs cleanup function before re-running effect on dependency changes
        - Hooks called in different order across renders produce a descriptive error
        - Custom hooks can compose useState and useEffect for reusable stateful logic
        pitfalls:
        - Conditional hooks
        - Stale closures
        - Effect cleanup timing
        concepts:
        - Hooks pattern
        - State management
        - Side effects
        skills:
        - Closure management in JavaScript
        - Stateful function design
        - Dependency tracking systems
        - Cleanup function patterns
        - Hook dependency arrays
        deliverables:
        - useState hook managing component-local state with update triggers
        - useEffect hook running side effects after component render commits
        - Hook ordering rules enforcing consistent call order across renders
        - Custom hooks composing built-in hooks into reusable logic units
        estimated_hours: 25-45
    - id: build-bundler
      name: Build Your Own Bundler
      description: Module resolution, tree shaking, code splitting
      difficulty: expert
      estimated_hours: 50-80
      essence: Module graph construction through AST parsing, dependency resolution following Node.js semantics, and code generation with scope isolation while performing static analysis for dead code elimination.
      why_important: Building a bundler demystifies the build tools developers use daily and teaches compiler-like techniques (parsing, static analysis, code generation) that apply across many domains from transpilers to linters.
      learning_outcomes:
      - Implement AST parsing and traversal to extract import/export declarations
      - Build a module resolution algorithm following Node.js resolution semantics
      - Construct a dependency graph from static analysis of module relationships
      - Design tree shaking by detecting unused exports through reachability analysis
      - Generate executable bundles with proper module scope isolation
      - Handle circular dependencies and maintain correct execution order
      - Implement code splitting with dynamic import() statements
      - Optimize bundle output through dead code elimination and minification
      skills:
      - AST Parsing & Traversal
      - Dependency Graph Construction
      - Static Code Analysis
      - Module Resolution Algorithms
      - Tree Shaking
      - Code Generation
      - Build Tool Development
      - Graph Traversal Algorithms
      tags:
      - build-from-scratch
      - code-splitting
      - data-structures
      - expert
      - javascript
      - module-resolution
      - tree-shaking
      - typescript
      - web
      architecture_doc: architecture-docs/build-bundler/index.md
      languages:
        recommended:
        - JavaScript
        - TypeScript
        also_possible:
        - Go
        - Rust
      resources:
      - type: article
        name: Minipack - Simple bundler
        url: https://github.com/ronami/minipack
      - type: documentation
        name: Rollup Plugin Development
        url: https://rollupjs.org/plugin-development/
      prerequisites:
      - type: skill
        name: JavaScript AST
      - type: skill
        name: Module systems (CommonJS, ESM)
      - type: skill
        name: File system operations
      milestones:
      - id: build-bundler-m1
        name: Module Parsing
        description: Parse JavaScript files and extract dependencies.
        acceptance_criteria:
        - ES module import and export statements are correctly parsed and their specifiers extracted
        - Dependency list is built from all import, export-from, and require statements in each source file
        - Relative paths (./foo) and bare specifiers (lodash) are both recognized as module dependencies
        - Module dependency graph is constructed with nodes for each module and edges for each import relationship
        pitfalls:
        - Circular dependencies
        - Node modules resolution
        - File extensions
        concepts:
        - AST parsing
        - Dependency analysis
        - Module graph
        skills:
        - Working with JavaScript AST parsers (Babel/Acorn)
        - Extracting import/export statements
        - Building dependency graphs
        - Handling CommonJS and ES modules
        deliverables:
        - JavaScript and TypeScript parser integration that produces an AST from source files
        - Import and export statement extraction that identifies all static module dependencies
        - Dynamic import() call detection for identifying code-split points in the module graph
        - CommonJS require() call handling for backward compatibility with non-ESM modules
        estimated_hours: 10-15
      - id: build-bundler-m2
        name: Module Resolution
        description: Implement Node.js-style module resolution.
        acceptance_criteria:
        - Relative path resolution correctly resolves ./file and ../file imports to absolute filesystem paths
        - Node modules lookup traverses parent directories checking node_modules/package until found or root is reached
        - Package.json main, module, and exports fields are consulted in the correct priority order for entry resolution
        - Index file fallback resolves a bare directory import to index.js or index.ts within that directory
        pitfalls:
        - Symlinks
        - Package exports field
        - Conditional exports
        concepts:
        - Module resolution
        - Package management
        - Path resolution
        skills:
        - Implementing node_modules lookup algorithm
        - Resolving package.json main/exports fields
        - Handling relative and absolute paths
        - Working with symlinks and realpath
        deliverables:
        - Relative path resolver that converts ./foo and ../bar specifiers to absolute file system paths
        - Node modules resolution algorithm that walks up the directory tree checking node_modules folders
        - Package.json field handler that reads main, module, exports, and browser fields for entry point resolution
        - Dependency graph constructor that assembles the full transitive closure of all module dependencies
        estimated_hours: 8-12
      - id: build-bundler-m3
        name: Bundle Generation
        description: Generate a single JavaScript bundle from module graph.
        acceptance_criteria:
        - Each module is wrapped in a function scope so its top-level variables do not pollute the global namespace
        - Module registry tracks loaded modules by ID and returns cached exports on subsequent require calls
        - Import and export bindings are correctly rewritten to use the runtime module loader's API
        - Source maps are generated in v3 format and correctly map bundled positions to original file and line numbers
        pitfalls:
        - Circular dependencies
        - Live bindings
        - Default export handling
        concepts:
        - Code generation
        - Module wrapping
        - Runtime
        skills:
        - Code generation and string manipulation
        - Implementing module wrapper functions
        - Managing module scope and closures
        - Handling runtime module loader
        deliverables:
        - Module wrapping that encloses each module's code in a function scope to prevent variable leaks
        - Runtime module loader code that manages a module registry, executes modules on demand, and caches exports
        - Topological sort of the module dependency graph to determine correct initialization order
        - Source map generation that maps bundled output positions back to original source file locations
        estimated_hours: 15-25
      - id: build-bundler-m4
        name: Tree Shaking
        description: Eliminate unused code from the bundle.
        acceptance_criteria:
        - Used exports are tracked across the module graph and only reachable code is included in the output
        - Unused exports and their associated code are removed from the final bundle output
        - 'Modules marked with sideEffects: false in package.json are fully removed when none of their exports are used'
        - Code with side effects (e.g., top-level function calls, global assignments) is preserved even if exports are unused
        pitfalls:
        - Side effect detection
        - Re-exports
        - Dynamic access patterns
        concepts:
        - Dead code elimination
        - Static analysis
        - Purity
        skills:
        - Static analysis of code usage
        - Tracking variable and function references
        - Detecting side effects in code
        - Optimizing bundle size through elimination
        deliverables:
        - Dead code detection via static analysis of export usage across the entire module graph
        - Side effect annotation handling that reads sideEffects field from package.json to identify pure modules
        - Unused export removal that eliminates exported symbols not imported by any other module in the bundle
        - Bundle size optimization reporting that shows bytes saved by tree-shaking per module
        estimated_hours: 17-28
    - id: build-spreadsheet
      name: Build Your Own Spreadsheet
      description: Excel-like app with formulas
      difficulty: expert
      estimated_hours: 50-80
      essence: Expression parsing, abstract syntax tree construction, and topological sort-based dependency resolution to enable cell formula evaluation and propagation through a directed acyclic graph of cell references.
      why_important: Building this project teaches the fundamentals of interpreter design (lexing, parsing, AST evaluation) and graph algorithms (topological sort, cycle detection) that power real-world developer tools, compilers, and reactive systems used across the software industry.
      learning_outcomes:
      - Implement a lexer and parser to convert formula strings into abstract syntax trees
      - Design and evaluate ASTs with proper operator precedence and function call handling
      - Build a dependency graph tracker that detects circular references in cell formulas
      - Implement topological sorting with Kahn's algorithm for correct recalculation order
      - Handle bidirectional data flow where cell changes trigger cascading updates through dependents
      - Implement efficient dirty-marking and demand-driven recalculation strategies
      - Design undo/redo systems using command pattern and memento state snapshots
      - Debug complex state synchronization issues in reactive data structures
      skills:
      - Abstract Syntax Trees
      - Recursive Descent Parsing
      - Dependency Graph Management
      - Topological Sort Algorithms
      - Expression Evaluation
      - Reactive State Management
      - Lexical Analysis
      - Command Pattern Design
      tags:
      - build-from-scratch
      - cells
      - dependency-graph
      - expert
      - formulas
      - javascript
      - recalculation
      - typescript
      architecture_doc: architecture-docs/build-spreadsheet/index.md
      languages:
        recommended:
        - JavaScript
        - TypeScript
        also_possible:
        - Python
        - Rust
      resources:
      - type: article
        name: Building a Spreadsheet Engine
        url: https://leanrada.com/notes/spreadsheet-engine/
      - type: video
        name: How Excel Recalculates
        url: https://www.youtube.com/watch?v=R0hhDgvWbUU
      prerequisites:
      - type: skill
        name: DOM manipulation
      - type: skill
        name: Graph algorithms
      - type: skill
        name: Expression parsing
      - type: skill
        name: Event handling
      milestones:
      - id: build-spreadsheet-m1
        name: Grid & Cell Rendering
        description: Build the spreadsheet grid with editable cells.
        acceptance_criteria:
        - Grid renders at least 26 columns and 100 rows with scrollable viewport
        - Double-clicking a cell enters edit mode with cursor in inline text input
        - Selected cell is visually distinct with highlighted border or background color
        - Scrolling pans the grid while keeping row and column headers visible
        pitfalls:
        - Rendering too many cells
        - Slow scrolling
        - Focus management issues
        concepts:
        - Virtual scrolling
        - DOM optimization
        - Keyboard navigation
        skills:
        - DOM manipulation and event handling
        - Performance optimization for large datasets
        - Implementing keyboard shortcuts
        - Managing application state
        deliverables:
        - Scrollable grid rendering cells in rows and columns layout
        - Cell editing UI with inline text input on double-click activation
        - Selection highlighting showing currently active cell with border
        - Column and row headers displaying A-Z labels and row numbers
        estimated_hours: 10-15
      - id: build-spreadsheet-m2
        name: Formula Parser
        description: Parse and evaluate spreadsheet formulas.
        acceptance_criteria:
        - Formulas starting with = are parsed into evaluable expression trees
        - Cell references like A1 and B2 resolve to their current numeric values
        - Arithmetic operators +, -, *, / evaluate with correct precedence and associativity
        - SUM(A1:A10) correctly sums all values in the specified cell range
        pitfalls:
        - Operator precedence errors
        - Not handling negative numbers
        - String vs number coercion
        concepts:
        - Lexical analysis
        - Recursive descent parsing
        - AST construction
        skills:
        - Writing parsers and lexers
        - Building abstract syntax trees
        - Implementing operator precedence
        - Type coercion and validation
        deliverables:
        - Expression tokenizer splitting formulas into operators, numbers, and cell references
        - Recursive descent parser building expression tree from tokenized formula
        - Cell reference resolver fetching values from referenced cell addresses
        - Built-in function library supporting SUM, AVG, MIN, MAX, and COUNT
        estimated_hours: 12-18
      - id: build-spreadsheet-m3
        name: Dependency Graph & Recalculation
        description: Track cell dependencies and recalculate in correct order.
        acceptance_criteria:
        - Changing a cell value triggers recalculation of all dependent cells automatically
        - Topological sort ensures cells are recalculated after their dependencies update
        - Circular reference is detected and displays error instead of hanging indefinitely
        - Only cells directly or transitively depending on changed cell are recalculated
        pitfalls:
        - Infinite loops from circular refs
        - Recalculating too much
        - Wrong recalc order
        concepts:
        - Directed graphs
        - Topological sorting
        - Cycle detection
        skills:
        - Graph algorithms and data structures
        - Implementing topological sort
        - Detecting circular dependencies
        - Optimizing bulk updates
        deliverables:
        - Cell dependency tracking recording which cells reference which others
        - Topological sort determining correct recalculation order for dependent cells
        - Circular reference detection preventing infinite recalculation loops
        - Incremental recalculation updating only affected cells when source changes
        estimated_hours: 12-18
      - id: build-spreadsheet-m4
        name: Advanced Features
        description: Add formatting, copy/paste, and undo/redo.
        acceptance_criteria:
        - Pasting a formula adjusts relative cell references to match new position
        - Undo reverts the most recent cell change and redo reapplies it correctly
        - CSV export produces valid comma-separated file openable by other spreadsheet programs
        - Number formatting displays cells as currency, percentage, or fixed decimal places
        pitfalls:
        - Memory leaks in undo stack
        - Not handling absolute references ($A$1)
        - Paste overwriting formulas
        concepts:
        - Command pattern
        - Reference adjustment
        - State management
        skills:
        - Implementing design patterns
        - Clipboard API integration
        - Building undo/redo systems
        - Cell range manipulation
        deliverables:
        - Copy and paste support duplicating cell values and formulas with reference adjustment
        - Undo and redo stack tracking cell modifications for reversal
        - CSV import and export converting between spreadsheet data and file format
        - Cell formatting options for number display, alignment, and font style
        estimated_hours: 15-20
    - id: build-web-framework
      name: Build Your Own Web Framework
      description: Express/Django clone
      difficulty: expert
      estimated_hours: 40-70
      essence: HTTP server abstraction with path-to-handler routing, chain-of-responsibility middleware pipeline for request/response transformation, and string-based template rendering with variable interpolation.
      why_important: Building a web framework reveals the abstractions underlying all modern web development, teaching you how Express, Django, and Flask actually work under the hood and enabling you to architect scalable web applications with deep understanding of the request processing pipeline.
      learning_outcomes:
      - Implement URL routing with path parameter extraction and HTTP method dispatching
      - Design middleware pipeline using chain-of-responsibility pattern for request/response transformation
      - Build request parser for handling JSON, form-urlencoded, and multipart data
      - Implement response helper methods for status codes, headers, and content-type negotiation
      - Create template engine with variable interpolation, loops, and conditionals
      - Design plugin architecture for extensibility and third-party middleware integration
      - Handle error propagation through middleware stack with custom error handlers
      - Implement route grouping, prefixing, and nested routing patterns
      skills:
      - HTTP Protocol Design
      - Middleware Architecture
      - URL Pattern Matching
      - Request/Response Handling
      - Template Engines
      - Chain of Responsibility
      - Parser Implementation
      - Plugin Systems
      tags:
      - api
      - build-from-scratch
      - expert
      - framework
      - go
      - javascript/node.js
      - middleware
      - python
      - routing
      - templating
      - web
      architecture_doc: architecture-docs/build-web-framework/index.md
      languages:
        recommended:
        - JavaScript/Node.js
        - Python
        - Go
        also_possible:
        - Ruby
        - Rust
      resources:
      - type: repository
        name: Express.js source
        url: https://github.com/expressjs/express
      - type: article
        name: Build Express from Scratch
        url: https://www.freecodecamp.org/news/express-explained-with-examples-installation-routing-middleware-and-more/
      prerequisites:
      - type: skill
        name: HTTP server
      - type: skill
        name: Request/response handling
      - type: skill
        name: Basic routing
      milestones:
      - id: build-web-framework-m1
        name: Routing
        description: Implement URL routing with parameters and methods.
        acceptance_criteria:
        - Route registration associates GET/POST/PUT/DELETE method with URL pattern and handler
        - URL parameters like /users/:id extract named values from matching path segments
        - Route matching returns correct handler or 404 response for unmatched paths
        - Route groups share common prefix so /api/users and /api/posts share /api prefix
        pitfalls:
        - Route ordering matters
        - URL encoding
        - Trailing slashes
        concepts:
        - URL routing
        - Pattern matching
        - HTTP methods
        skills:
        - HTTP request handling
        - Pattern matching and regex
        - RESTful API design
        - URL parameter extraction
        deliverables:
        - Route registration mapping HTTP method and URL pattern to handler functions
        - URL parameter extraction parsing named segments from request path
        - Route matching selecting correct handler for incoming request method and path
        - Route group prefixing organizing related routes under common URL prefix
        estimated_hours: 6-10
      - id: build-web-framework-m2
        name: Middleware
        description: Implement middleware pipeline for request processing.
        acceptance_criteria:
        - Middleware executes in registration order before request reaches route handler
        - Calling next() passes control to the next middleware or final route handler
        - Logging middleware logs HTTP method, URL path, status code, and response duration
        - Error middleware catches unhandled exceptions and returns 500 response with message
        pitfalls:
        - Calling next multiple times
        - Async error handling
        - Middleware ordering
        concepts:
        - Middleware pattern
        - Pipeline
        - Error propagation
        skills:
        - Function composition
        - Async control flow
        - Error boundary implementation
        - Request/response transformation
        deliverables:
        - Middleware chain processing request through ordered sequence of middleware functions
        - next() function passing control from current middleware to next in chain
        - Built-in logging middleware recording request method, path, and response time
        - Error handling middleware catching exceptions and returning error responses
        estimated_hours: 10-15
      - id: build-web-framework-m3
        name: Request/Response Enhancement
        description: Add convenience methods and body parsing.
        acceptance_criteria:
        - JSON body parser decodes request body and makes parsed object available on request
        - Response JSON helper sets Content-Type header and serializes object to JSON body
        - Cookie methods read incoming cookies by name and set outgoing cookies with options
        - Query parameters are parsed from URL and accessible as key-value dictionary
        pitfalls:
        - Large body handling
        - Content-Type edge cases
        - Cookie security
        concepts:
        - Request parsing
        - Response helpers
        - HTTP cookies
        skills:
        - Stream processing
        - JSON/form data parsing
        - HTTP header manipulation
        - Cookie management
        - Content negotiation
        deliverables:
        - Request body parsing supporting JSON, form-urlencoded, and multipart formats
        - Response helpers for sending JSON, HTML, redirects, and file downloads
        - Cookie handling for reading request cookies and setting response cookies
        - Query parameter parsing extracting key-value pairs from URL query string
        estimated_hours: 8-12
      - id: build-web-framework-m4
        name: Template Engine
        description: Implement a simple template engine for HTML rendering.
        acceptance_criteria:
        - Template variables like {{ name }} are replaced with corresponding context values
        - HTML special characters in variable output are escaped to prevent XSS attacks
        - If/else blocks conditionally include template sections based on context values
        - Child templates extend base template overriding only defined block sections
        pitfalls:
        - XSS prevention
        - Template injection
        - Performance with large templates
        concepts:
        - Template compilation
        - Code generation
        - HTML escaping
        skills:
        - String interpolation and parsing
        - AST manipulation
        - HTML sanitization
        - Template caching strategies
        deliverables:
        - Variable interpolation substituting template placeholders with provided values
        - HTML escaping preventing cross-site scripting by encoding special characters
        - Control flow directives supporting if/else conditionals and for-each loops
        - Template inheritance allowing child templates to extend base layout templates
        estimated_hours: 16-33
    - id: build-graphql-engine
      name: Build Your Own GraphQL Engine
      description: Query parsing, execution, schema stitching like Hasura
      difficulty: expert
      estimated_hours: '120'
      essence: Parsing GraphQL queries into abstract syntax trees, validating them against a type system schema, and compiling them into optimized SQL queries through database metadata reflection and query planning to eliminate N+1 problems.
      why_important: Building a GraphQL engine requires mastering query language design, compiler construction, and database query optimization‚Äîskills critical for backend infrastructure, API design, and understanding how modern data layers like Hasura, PostGraphile, and Apollo Server work under the hood.
      learning_outcomes:
      - Deep understanding of GraphQL specification
      - Implement query parsing and validation
      - Build automatic schema generation from database
      - Optimize GraphQL queries to SQL
      skills:
      - GraphQL spec
      - Query parsing
      - Schema introspection
      - Database reflection
      - Query optimization
      - Real-time subscriptions
      tags:
      - api
      - build-from-scratch
      - expert
      - framework
      - game-dev
      - introspection
      - resolvers
      - schema
      - subscriptions
      architecture_doc: architecture-docs/build-graphql-engine/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible: []
      resources:
      - name: GraphQL Official Specification
        url: https://spec.graphql.org/
        type: documentation
      - name: How to GraphQL Tutorial
        url: https://www.howtographql.com/
        type: tutorial
      - name: Building a GraphQL to SQL Compiler
        url: https://hasura.io/blog/building-a-graphql-to-sql-compiler-on-postgres-ms-sql-and-mysql
        type: article
      - name: GraphQL Introspection Guide
        url: https://graphql.org/learn/introspection/
        type: documentation
      - name: GraphQL Compiler Library
        url: https://github.com/kensho-technologies/graphql-compiler
        type: tool
      prerequisites:
      - type: skill
        name: GraphQL server
      - type: skill
        name: Database internals
      - type: skill
        name: Compiler basics
      milestones:
      - id: build-graphql-engine-m1
        name: GraphQL Parser
        description: Parse GraphQL queries into AST
        acceptance_criteria:
        - Parser converts a valid GraphQL query string into a complete AST representing all operations and fields
        - Queries, mutations, and subscription operations are all recognized and represented as distinct AST operation types
        - Named fragments and inline fragments are parsed and their selection sets are correctly attached in the AST
        - Parse errors include the line number, column number, and a description of the expected vs. actual token
        pitfalls:
        - Not handling string escaping and unicode in literals
        - Failing to validate balanced braces and parentheses
        - Missing edge cases in fragment spread parsing
        - Not preserving source location metadata for errors
        - Confusing field arguments with directive arguments
        concepts:
        - Tokenization of GraphQL query language syntax
        - Recursive descent parsing for nested selection sets
        - Building abstract syntax trees from token streams
        - Handling GraphQL operations, fragments, and directives
        - Position tracking for error reporting
        skills:
        - Lexer
        - Parser
        - AST
        deliverables:
        - GraphQL lexer and tokenizer that breaks a query string into tokens (names, strings, numbers, punctuation)
        - Recursive descent parser that builds an abstract syntax tree from the token stream
        - Query document AST representing operations, selection sets, fields, arguments, and directives
        - Fragment definition and spread handling that expands named fragments into their selection sets
        - Variable definition parsing that captures variable names, types, and default values from the operation header
        - Directive parsing that reads @-prefixed directives with their argument lists on fields and fragments
        estimated_hours: '24'
      - id: build-graphql-engine-m2
        name: Schema & Type System
        description: Build type system and schema representation
        acceptance_criteria:
        - All GraphQL type kinds (Object, Scalar, Enum, Interface, Union, InputObject, List, NonNull) are supported
        - Schema is constructed from type definitions and validated for consistency (e.g., no circular input types)
        - Schema validation detects errors such as missing required fields, undefined types, and duplicate type names
        - Custom scalar types can be defined with serialize, parseValue, and parseLiteral functions
        pitfalls:
        - Creating circular type references without detection
        - Not validating interface implementation completeness
        - Forgetting to check field argument type compatibility
        - Missing validation for input object recursion
        - Allowing invalid combinations of type modifiers
        concepts:
        - Schema definition language and type introspection
        - Scalar types, object types, and interface inheritance
        - Input types versus output types distinction
        - Non-null and list type wrapper validation
        - Schema directives and custom metadata
        skills:
        - Type system
        - Introspection
        - Validation
        deliverables:
        - Scalar type definitions for built-in types (Int, Float, String, Boolean, ID) and custom scalars
        - Object type definitions with field lists, argument definitions, and interface implementations
        - Input type definitions for mutation arguments with field validation and default values
        - Enum type definitions mapping symbolic names to their allowed value sets
        - Interface and union type definitions supporting polymorphic field resolution and type conditions
        - Introspection query support implementing __schema, __type, and __typename meta-fields per the spec
        estimated_hours: '24'
      - id: build-graphql-engine-m3
        name: Query Execution
        description: Execute queries against schema
        acceptance_criteria:
        - Queries are executed against the schema and return a data object matching the requested selection set shape
        - Each field is resolved by calling its resolver function with parent value, arguments, and context
        - Nested fields are resolved recursively with each level receiving the parent field's resolved value
        - Non-null fields that resolve to null propagate the null to the nearest nullable parent field per the GraphQL spec
        pitfalls:
        - Not handling null propagation in non-null fields
        - Forgetting to batch concurrent field resolutions
        - Missing error context in nested resolver failures
        - Not implementing proper resolver timeout handling
        - Executing resolvers in wrong dependency order
        concepts:
        - Field resolver execution and data fetching
        - Asynchronous resolution and promise batching
        - Error boundary handling and partial results
        - Context propagation through resolver chain
        - N+1 query problem and dataloader pattern
        skills:
        - Execution
        - Resolvers
        - Error handling
        deliverables:
        - Field resolver dispatch that calls the correct resolver function for each field in the selection set
        - Argument coercion that converts raw input values to the expected types defined in the schema
        - List and non-null type handling that enforces nullability rules and wraps results in arrays as specified
        - Error collection mechanism that gathers field-level errors without aborting the entire query execution
        - Parallel field execution that resolves sibling fields concurrently when their resolvers are independent
        - Execution context object that carries authentication, data loaders, and request-scoped state to resolvers
        estimated_hours: '24'
      - id: build-graphql-engine-m4
        name: Database Schema Reflection
        description: Auto-generate GraphQL schema from database
        acceptance_criteria:
        - Database tables and columns are introspected and their names and types are read from the information schema
        - GraphQL object types are auto-generated from tables with one field per column using appropriate scalar types
        - Database column types (integer, varchar, boolean, timestamp) are correctly mapped to GraphQL scalar types
        - Foreign key relationships produce nested object fields that resolve to the related table's GraphQL type
        pitfalls:
        - Not handling database-specific type mappings correctly
        - Missing detection of self-referential relationships
        - Generating conflicting names for join tables
        - Not respecting database permission and visibility rules
        - Failing to handle nullable versus required columns
        concepts:
        - Database metadata querying and table introspection
        - Mapping SQL types to GraphQL scalar types
        - Foreign key detection for relationship generation
        - Generating pagination and filtering capabilities
        - Handling composite primary keys and indexes
        skills:
        - Database introspection
        - Schema generation
        - Relationships
        deliverables:
        - Table-to-type mapper that converts database tables into corresponding GraphQL object types
        - Column-to-field mapper that converts database columns into typed GraphQL fields with appropriate scalar types
        - Foreign key relationship detector that creates GraphQL fields for related objects based on FK constraints
        - Primary key query generator that creates by-ID lookup query fields for each reflected table type
        - Filter argument generator that creates where-clause input types for each table's queryable columns
        - Pagination argument support that adds first, last, after, and before arguments to collection fields
        estimated_hours: '24'
      - id: build-graphql-engine-m5
        name: Query to SQL Compilation
        description: Compile GraphQL queries to efficient SQL
        acceptance_criteria:
        - GraphQL queries compile to efficient SQL statements that select only the columns requested in the query
        - Nested object fields generate SQL JOINs on foreign key relationships instead of issuing separate queries
        - N+1 query problem is prevented by batching related-object lookups into a single SQL query with IN clause
        - Filtering, pagination (limit/offset or cursor), and ordering are correctly translated to SQL WHERE, LIMIT, and ORDER BY clauses
        pitfalls:
        - Generating cartesian products instead of proper joins
        - Not optimizing deeply nested relationship queries
        - Missing index hints for filtered queries
        - Forgetting to add LIMIT clauses for unbounded queries
        - Not handling database-specific SQL dialect differences
        concepts:
        - Query analysis and execution plan generation
        - Converting nested selections into SQL JOINs
        - Lateral joins for one-to-many relationships
        - WHERE clause generation from GraphQL arguments
        - Preventing SQL injection through parameterization
        skills:
        - Query planning
        - JOIN optimization
        - Batching
        deliverables:
        - Selection-to-SELECT mapper that converts GraphQL field selections into SQL SELECT column lists
        - Nested selection-to-JOIN compiler that translates nested object fields into SQL JOIN clauses on foreign keys
        - Filter-to-WHERE translator that converts GraphQL filter arguments into parameterized SQL WHERE conditions
        - Order and limit handler that applies sorting and pagination to SQL queries from GraphQL arguments
        - Aggregate query support that compiles count, sum, avg, min, and max GraphQL fields into SQL aggregations
        - Query batching with DataLoader pattern that combines multiple resolver calls into a single SQL query
        estimated_hours: '24'
- id: systems
  name: Systems & Low-Level
  icon: ‚öôÔ∏è
  subdomains:
  - name: Systems Programming
  - name: Networking
  - name: Operating Systems
  projects:
    beginner: []
    intermediate:
    - id: http-server-basic
      name: HTTP Server (Basic)
      description: Static file serving
      difficulty: intermediate
      estimated_hours: 12-20
      essence: Socket-level TCP server implementation that parses HTTP/1.1 protocol messages, manages file I/O for static content delivery, and coordinates concurrent client connections through multithreading or async I/O.
      why_important: Building this establishes foundational knowledge of how web infrastructure actually works beneath frameworks and libraries, teaching you network programming patterns used across backend services, distributed systems, and real-time applications.
      learning_outcomes:
      - Implement TCP socket binding, listening, and accept loops with proper error handling
      - Parse HTTP request lines and headers according to RFC 7230 specifications
      - Design file system access patterns with proper path validation and security constraints
      - Build concurrent request handlers using threads, thread pools, or async I/O primitives
      - Handle TCP connection lifecycle including graceful shutdown and resource cleanup
      - Debug network protocols using packet capture tools and low-level system calls
      - Implement HTTP response formatting with correct status codes and headers
      - Manage shared state and race conditions in multithreaded server architectures
      skills:
      - TCP/IP Socket Programming
      - HTTP Protocol Implementation
      - Concurrent Programming
      - System I/O Operations
      - Network Debugging
      - Thread Synchronization
      - File System Security
      - Error Handling
      tags:
      - c
      - concurrency
      - go
      - intermediate
      - networking
      - protocols
      - request-response
      - rust
      - service
      - static-files
      architecture_doc: architecture-docs/http-server-basic/index.md
      languages:
        recommended:
        - C
        - Go
        - Rust
        also_possible:
        - Python
        - Java
      resources:
      - name: HTTP/1.1 Specification (RFC 7230)
        url: https://tools.ietf.org/html/rfc7230
        type: specification
      - name: Beej's Guide to Network Programming
        url: https://beej.us/guide/bgnet/
        type: tutorial
      prerequisites:
      - type: skill
        name: TCP/IP basics
      - type: skill
        name: Socket programming
      - type: skill
        name: File I/O
      milestones:
      - id: http-server-basic-m1
        name: TCP Server Basics
        description: Create a listening TCP server.
        acceptance_criteria:
        - Bind to port 8080 and listen for incoming TCP connections
        - Accept connections from multiple sequential client requests
        - Read complete request data from the client socket buffer
        - Send hardcoded HTTP response with proper status line and headers
        - Close connection and free resources after response is sent
        pitfalls:
        - Byte order (htons)
        - Not handling partial reads
        - Forgetting to close client fd
        concepts:
        - Sockets
        - TCP connection lifecycle
        - Bind/Listen/Accept
        skills:
        - Socket Programming
        - Network I/O
        - Error Handling
        - Resource Management
        deliverables:
        - Socket creation and binding to configurable port address
        - Connection acceptance loop handling incoming client connections
        - Client handling reading request data and sending response
        - Connection cleanup closing client sockets after response delivery
        estimated_hours: 2-3
      - id: http-server-basic-m2
        name: HTTP Request Parsing
        description: Parse HTTP request line and headers.
        acceptance_criteria:
        - Parse method, path, and version from HTTP request line correctly
        - Parse headers into key-value pairs handling whitespace around values
        - Handle GET requests and return appropriate responses based on path
        - Extract Host header value for virtual host routing support
        pitfalls:
        - CRLF vs LF line endings
        - Whitespace around header values
        - Buffer overflow on long lines
        concepts:
        - HTTP message format
        - Request parsing
        - Header handling
        skills:
        - String Parsing
        - Protocol Implementation
        - Buffer Management
        - State Machine Design
        deliverables:
        - Request line parsing extracting method, path, and HTTP version
        - Header parsing converting header lines into key-value pair structure
        - Body handling reading content based on Content-Length header value
        - Error responses for malformed requests with appropriate status codes
        estimated_hours: 2-3
      - id: http-server-basic-m3
        name: Static File Serving
        description: Serve files from a directory.
        acceptance_criteria:
        - Map URL path to filesystem path within configured document root directory
        - Detect and set Content-Type header based on file extension or magic bytes
        - Send file contents in response body with correct Content-Length header
        - Handle 404 Not Found when requested file does not exist in document root
        - Prevent directory traversal attacks by validating resolved path stays within root
        pitfalls:
        - Directory traversal (../)
        - Missing Content-Type
        - Binary file handling
        concepts:
        - Path resolution
        - MIME types
        - Security validation
        skills:
        - File I/O
        - Path Handling
        - MIME Type Resolution
        - Security Validation
        deliverables:
        - File reading from document root mapped to URL path
        - MIME type detection based on file extension for Content-Type header
        - 404 handling returning Not Found response for missing files
        - Directory listing displaying files when path maps to a directory
        estimated_hours: 3-4
      - id: http-server-basic-m4
        name: Concurrent Connections
        description: Handle multiple clients concurrently.
        acceptance_criteria:
        - Thread-per-connection model handles multiple simultaneous client connections
        - Thread pool limits concurrency to prevent resource exhaustion under load
        - Non-blocking with select or poll handles connections without one-thread-per-client
        - Graceful shutdown completes in-flight requests before stopping the server
        pitfalls:
        - Thread safety
        - File descriptor leaks
        - Resource exhaustion
        concepts:
        - Concurrency models
        - Threading
        - Connection handling
        skills:
        - Concurrent Programming
        - Thread Management
        - Synchronization
        - Resource Pooling
        deliverables:
        - Thread per connection model spawning handler for each client
        - Thread pool option limiting concurrent handlers for resource control
        - Non-blocking I/O option using select or poll for event-driven handling
        - Connection limits preventing resource exhaustion from too many clients
        estimated_hours: 4-6
    - id: memory-pool
      name: Memory Pool Allocator
      description: Fixed-size block allocation
      difficulty: intermediate
      estimated_hours: 10-15
      essence: Pre-allocated fixed-size block management through free list data structures, enabling O(1) allocation and deallocation by trading general-purpose flexibility for predictable performance and reduced fragmentation.
      why_important: Building this teaches low-level memory management fundamentals critical for systems programming, game engines, embedded systems, and high-performance applications where allocation speed and predictability matter more than general-purpose flexibility.
      learning_outcomes:
      - Implement free list data structures for tracking available memory blocks
      - Design block allocation and deallocation algorithms with O(1) complexity
      - Build dynamic pool expansion mechanisms to handle memory exhaustion
      - Debug memory corruption issues using canary values and integrity checks
      - Understand internal vs external fragmentation trade-offs in allocator design
      - Implement thread-safe memory operations using mutexes or lock-free techniques
      - Measure and optimize allocator performance vs standard malloc/free
      - Handle memory alignment requirements for different data types
      skills:
      - Memory Management
      - Data Structure Design
      - Low-Level C/Rust
      - Linked List Implementation
      - Thread Synchronization
      - Performance Optimization
      - Debugging Techniques
      - Systems Programming
      tags:
      - allocation
      - c
      - fixed-size
      - intermediate
      - pools
      - rust
      - systems
      - zig
      architecture_doc: architecture-docs/memory-pool/index.md
      languages:
        recommended:
        - C
        - Rust
        - Zig
        also_possible:
        - C++
      resources:
      - name: Memory Pool Design
        url: https://en.wikipedia.org/wiki/Memory_pool
        type: article
      - name: Writing a Pool Allocator
        url: https://dmitrysoshnikov.com/compilers/writing-a-pool-allocator/
        type: tutorial
      - name: 'Untangling Lifetimes: The Arena Allocator'
        url: https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator
        type: article
      - name: A Fixed Block Memory Allocator in C
        url: https://www.codeproject.com/Articles/1272619/A-Fixed-Block-Memory-Allocator-in-C
        type: tutorial
      prerequisites:
      - type: skill
        name: C pointers
      - type: skill
        name: Memory layout
      - type: skill
        name: Data structures
      milestones:
      - id: memory-pool-m1
        name: Fixed-Size Pool
        description: Allocate a pool of fixed-size blocks.
        acceptance_criteria:
        - Pool initializes with N blocks of size S bytes carved from a single contiguous memory allocation
        - Allocate returns a pointer to a free block or NULL when the pool is completely exhausted
        - Free returns the block to the pool making it available for subsequent allocation requests
        - Both allocate and free operations complete in O(1) constant time regardless of pool size
        pitfalls:
        - Block size smaller than pointer
        - Double free
        - Use after free
        concepts:
        - Free list
        - Pointer aliasing
        - Memory layout
        skills:
        - Memory alignment and layout
        - Pointer arithmetic and type casting
        - Data structure implementation
        - Low-level debugging techniques
        deliverables:
        - Pool initializer that pre-allocates a contiguous memory region divided into N equal-sized blocks
        - Block allocator that returns the next available block from the free list in constant time
        - Block deallocator that returns a freed block to the front of the free list in constant time
        - Free list manager that maintains a singly-linked list threading through the unused blocks
        estimated_hours: 3-4
      - id: memory-pool-m2
        name: Pool Growing
        description: Allow pool to grow when exhausted.
        acceptance_criteria:
        - A new chunk is automatically allocated when the pool's free list is empty and an alloc is requested
        - Multiple chunks are chained together in a linked list for unified pool management and cleanup
        - All allocated chunks are tracked and freed during pool destruction to prevent memory leaks
        - Optional maximum pool size limit prevents unbounded growth and returns NULL when the limit is reached
        pitfalls:
        - Memory leak on destroy
        - Infinite growth
        - Fragmentation across chunks
        concepts:
        - Dynamic allocation
        - Chunk management
        - Resource limits
        skills:
        - Dynamic memory management
        - Linked list manipulation
        - Resource lifecycle management
        - Capacity planning and growth strategies
        deliverables:
        - Automatic pool expander that allocates a new chunk of blocks when the free list is empty
        - Chunk allocator that creates additional memory regions with the same block size as the original pool
        - Memory tracker that maintains a list of all allocated chunks for cleanup during pool destruction
        - Pool statistics collector that reports total blocks, used blocks, free blocks, and chunk count
        estimated_hours: 2-3
      - id: memory-pool-m3
        name: Thread Safety & Debugging
        description: Add thread safety and debugging features.
        acceptance_criteria:
        - Mutex or atomic CAS protects alloc and free operations from data corruption under concurrent access
        - Double free attempts are detected and reported with the block address and call site information
        - Memory poisoning fills freed blocks with a sentinel pattern to detect use-after-free access
        - Statistics tracking reports allocation count, free count, peak usage, and current utilization in real time
        pitfalls:
        - Lock ordering deadlocks
        - Overhead of debugging
        - False positives in detection
        concepts:
        - Thread safety
        - Memory debugging
        - Defensive programming
        skills:
        - Mutex and lock implementation
        - Race condition prevention
        - Memory profiling and leak detection
        - Concurrent data structure design
        deliverables:
        - Lock-free or mutex-protected allocation option that ensures thread-safe pool access under contention
        - Per-thread pool variant that gives each thread its own free list to eliminate cross-thread contention
        - Leak detector that reports allocated blocks not freed when the pool is destroyed
        - Double-free detector that identifies and reports attempts to free a block that is already free
        estimated_hours: 3-4
    - id: process-spawner
      name: Process Spawner
      description: fork/exec, process lifecycle
      difficulty: intermediate
      estimated_hours: 10-15
      essence: Process lifecycle management through fork/exec system calls for process creation and replacement, pipe-based inter-process communication using file descriptor pairs, and coordinating worker process pools with proper state tracking, resource cleanup, and zombie process prevention through wait mechanisms.
      why_important: Building this teaches low-level operating system primitives that underpin every server, shell, and container runtime, giving you deep understanding of how languages like Node.js, Python's subprocess module, and Docker actually spawn and manage processes under the hood.
      learning_outcomes:
      - Implement fork/exec to spawn child processes and handle process IDs correctly
      - Design pipe-based communication channels with proper file descriptor management
      - Handle process termination with wait/waitpid to prevent zombie processes
      - Build a worker pool with load distribution and process lifecycle management
      - Debug race conditions and signal handling in multi-process programs
      - Implement proper error handling for system call failures
      - Manage file descriptor inheritance and cleanup across process boundaries
      skills:
      - Process Management
      - System Calls
      - Inter-Process Communication
      - File Descriptors
      - Concurrency Control
      - Unix Programming
      - Resource Management
      - Signal Handling
      tags:
      - c
      - exec
      - fork
      - go
      - intermediate
      - lifecycle
      - rust
      - systems
      architecture_doc: architecture-docs/process-spawner/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible:
        - Python
      resources:
      - name: fork(2) man page
        url: https://man7.org/linux/man-pages/man2/fork.2.html
        type: documentation
      - name: Advanced Programming in Unix
        url: https://www.apuebook.com/
        type: book
      prerequisites:
      - type: skill
        name: Basic C programming
      - type: skill
        name: Unix basics
      - type: skill
        name: System calls
      milestones:
      - id: process-spawner-m1
        name: Basic Fork/Exec
        description: Spawn a child process to run a command.
        acceptance_criteria:
        - Create child process using fork() system call
        - Execute external command in child using exec() family
        - Parent process waits for child to complete using waitpid()
        - Handle fork and exec errors with informative messages
        pitfalls:
        - Forgetting to exec
        - Using exit() instead of _exit()
        - Not handling signals
        concepts:
        - fork()
        - exec()
        - Process creation
        skills:
        - Process lifecycle management
        - System call error handling
        - Child process execution
        - Unix process API usage
        deliverables:
        - fork() call creating child process from parent
        - exec() family function replacing child process image
        - Exit status collection using waitpid() in parent
        - Error handling for failed fork and exec system calls
        estimated_hours: 2-3
      - id: process-spawner-m2
        name: Pipe Communication
        description: Set up pipes for parent-child communication.
        acceptance_criteria:
        - Create pipe file descriptors before forking child process
        - Redirect child stdin and stdout to pipe endpoints using dup2
        - Parent reads from and writes to child via pipe descriptors
        - Close unused pipe ends in both parent and child processes
        pitfalls:
        - Pipe deadlock
        - Forgetting to close ends
        - Buffer full blocking
        concepts:
        - Pipes
        - IPC
        - File descriptor redirection
        skills:
        - Inter-process communication
        - File descriptor manipulation
        - Pipe-based data transfer
        - Non-blocking I/O handling
        deliverables:
        - Pipe creation before fork for parent-child communication
        - Parent-to-child and child-to-parent data transfer channels
        - Bi-directional pipe setup using two pipe pairs
        - Proper pipe cleanup closing all unused file descriptor ends
        estimated_hours: 3-4
      - id: process-spawner-m3
        name: Process Pool
        description: Manage a pool of worker processes.
        acceptance_criteria:
        - Spawn configurable number of worker processes at startup
        - Distribute incoming work items to available worker processes
        - Handle worker process crashes by respawning replacements
        - Perform clean shutdown terminating all workers gracefully
        pitfalls:
        - Zombie processes
        - Signal handling races
        - Resource cleanup
        concepts:
        - Process pools
        - SIGCHLD
        - Worker management
        skills:
        - Signal-driven process management
        - Concurrent process coordination
        - Resource lifecycle management
        - Worker pool implementation
        - Zombie process prevention
        deliverables:
        - Worker process creation spawning N child processes
        - Task distribution mechanism sending work to idle workers
        - Result collection aggregating output from worker processes
        - Pool lifecycle management including startup and shutdown
        estimated_hours: 4-5
    - id: signal-handler
      name: Signal Handler
      description: SIGINT, SIGTERM, signal masks
      difficulty: intermediate
      estimated_hours: 8-12
      essence: Asynchronous interrupt handling with async-signal-safe constraints, signal masking for atomic critical sections, and integration of OS-level interrupts into select/poll event loops using the self-pipe trick.
      why_important: Building this teaches you systems programming fundamentals essential for reliable servers, daemons, and CLI tools, addressing real-world challenges like graceful shutdown, preventing race conditions in multi-signal scenarios, and correctly handling interrupts without corrupting program state.
      learning_outcomes:
      - Implement signal handlers using sigaction with proper async-signal-safe constraints
      - Design critical sections protected by signal masks using sigprocmask
      - Build the self-pipe trick to integrate signals into select/poll event loops
      - Debug race conditions arising from signal delivery during system calls
      - Handle graceful shutdown sequences with SIGTERM and SIGINT coordination
      - Implement reentrant code that avoids non-async-signal-safe functions like malloc and printf
      - Work with volatile sig_atomic_t for safe signal-to-main-thread communication
      skills:
      - Signal masking
      - Async-signal safety
      - Self-pipe pattern
      - Event loop integration
      - Graceful shutdown
      - Race condition handling
      - POSIX signal API
      - Reentrant programming
      tags:
      - async-signal-safe
      - c
      - intermediate
      - masks
      - rust
      - sigint
      - sigterm
      - systems
      architecture_doc: architecture-docs/signal-handler/index.md
      languages:
        recommended:
        - C
        - Rust
        also_possible:
        - Go
        - Python
      resources:
      - name: signal(7) man page
        url: https://man7.org/linux/man-pages/man7/signal.7.html
        type: documentation
      - name: Signal Safety
        url: https://man7.org/linux/man-pages/man7/signal-safety.7.html
        type: documentation
      prerequisites:
      - type: skill
        name: C programming
      - type: skill
        name: Unix basics
      - type: skill
        name: Process concepts
      milestones:
      - id: signal-handler-m1
        name: Basic Signal Handling
        description: Install signal handlers for common signals.
        acceptance_criteria:
        - Handle SIGINT signal triggered by Ctrl+C keyboard interrupt
        - Handle SIGTERM signal for graceful process termination
        - Use sigaction() instead of signal() for portable reliable handling
        - Set SA_RESTART flag so interrupted system calls restart automatically
        pitfalls:
        - Non-reentrant functions in handler
        - Race conditions
        - Missing SA_RESTART
        concepts:
        - Signal handlers
        - sigaction()
        - Async-signal safety
        skills:
        - Signal handler installation
        - Async-safe function usage
        - Signal disposition management
        - Handler cleanup and restoration
        deliverables:
        - Signal handler registration using sigaction() system call
        - Signal delivery handling for SIGINT, SIGTERM, and SIGHUP
        - Async-signal-safe handler function using only safe operations
        - SA_RESTART flag configuration ensuring interrupted syscalls restart
        estimated_hours: 2-3
      - id: signal-handler-m2
        name: Signal Masking
        description: Block and unblock signals for critical sections.
        acceptance_criteria:
        - Block signals during critical code sections using sigprocmask()
        - Manipulate signal masks using sigprocmask() with SIG_BLOCK and SIG_UNBLOCK
        - Handle pending signals delivered when mask is removed
        - Support per-thread signal masks in multithreaded programs
        pitfalls:
        - Forgetting to unblock
        - Deadlock with nested blocking
        - Thread safety
        concepts:
        - Signal masks
        - Critical sections
        - Pending signals
        skills:
        - Signal set manipulation
        - Critical section protection
        - Signal mask state management
        - Pending signal inspection
        deliverables:
        - Signal mask manipulation blocking and unblocking specific signals
        - Critical section signal blocking preventing interruption of sensitive code
        - Pending signal inspection checking which signals are queued
        - Per-thread signal mask support using pthread_sigmask()
        estimated_hours: 2-3
      - id: signal-handler-m3
        name: Self-Pipe Trick
        description: Integrate signals with select/poll event loop.
        acceptance_criteria:
        - Create non-blocking self-pipe for signal notification delivery
        - Write signal number byte to pipe inside async-signal-safe handler
        - Monitor pipe read end using select() or poll() in main event loop
        - Handle signal safely in main loop context after event notification
        pitfalls:
        - Pipe buffer full
        - Non-blocking write
        - Multiple signals coalescing
        concepts:
        - Self-pipe trick
        - Event loops
        - Signal integration
        skills:
        - Event-driven signal handling
        - Non-blocking I/O operations
        - Signal-safe IPC mechanisms
        - Event loop integration
        deliverables:
        - Non-blocking pipe pair for signal-to-event-loop notification
        - Signal handler writing single byte to pipe on signal receipt
        - Event loop integration reading pipe alongside other I/O descriptors
        - Multiplexed I/O combining signal pipe with socket descriptors in select/poll
        estimated_hours: 3-4
    - id: build-strace
      name: System Call Tracer
      description: ptrace-based syscall interception and decoding
      difficulty: intermediate
      estimated_hours: 20-30
      essence: Process introspection through kernel-mediated interception of system calls, requiring register manipulation, argument decoding from memory, and process state management across fork/exec boundaries.
      why_important: Understanding ptrace-based tracing reveals how debuggers, profilers, and security tools interact with the kernel to observe program behavior, a foundational skill for systems programming and tooling development.
      learning_outcomes:
      - Implement ptrace-based process attachment and syscall interception using PTRACE_SYSCALL
      - Decode syscall arguments by reading process registers and dereferencing pointer arguments from tracee memory
      - Handle multi-process tracing across fork and exec by following child processes with PTRACE_TRACEME
      - Parse and display syscall numbers, return values, and error codes using kernel ABI knowledge
      - Build filtering mechanisms to selectively trace syscalls by name or category
      - Generate timing statistics and syscall frequency analysis from traced execution
      - Manage process state transitions including signals, stops, and continues
      - Debug race conditions and handle edge cases in parent-child ptrace synchronization
      skills:
      - ptrace System Call
      - Process Control
      - Register Manipulation
      - Memory Inspection
      - System Call ABI
      - Signal Handling
      - Multi-process Debugging
      - Binary Analysis
      tags:
      - ptrace
      - debugging
      - system-calls
      - linux
      - process-control
      architecture_doc: architecture-docs/build-strace/index.md
      languages:
        recommended:
        - C
        - Rust
        also_possible:
        - Go
      resources:
      - name: ptrace(2) man page
        url: https://man7.org/linux/man-pages/man2/ptrace.2.html
        type: reference
      - name: How strace works
        url: https://blog.packagecloud.io/how-does-strace-work/
        type: article
      prerequisites:
      - type: project
        name: process-spawner
      - type: project
        name: signal-handler
      milestones:
      - id: build-strace-m1
        name: Basic ptrace and Syscall Intercept
        description: Use ptrace to attach to a child process and intercept syscalls.
        acceptance_criteria:
        - Fork a child process and use PTRACE_TRACEME to allow tracing
        - Stop at each syscall entry/exit using PTRACE_SYSCALL
        - Read syscall number from registers using PTRACE_GETREGS
        - Print syscall number and return value for each intercepted syscall
        pitfalls:
        - ptrace stops twice per syscall (entry and exit), must track which state you're in
        - Register layout differs between x86 and x86-64 (orig_rax vs orig_eax)
        - Child must call PTRACE_TRACEME before exec, not after
        concepts:
        - ptrace system call
        - Process tracing
        - Register access
        skills:
        - System-level debugging with ptrace
        - Process control and lifecycle management
        - CPU register inspection and manipulation
        - Low-level syscall interception
        deliverables:
        - Child process creation with PTRACE_TRACEME setup
        - Syscall interception loop using PTRACE_SYSCALL and waitpid
        - Register reading to extract syscall numbers and return values
        - Basic output showing syscall number and result for each call
        estimated_hours: 5-7
      - id: build-strace-m2
        name: Argument Decoding
        description: Decode syscall arguments including string and pointer arguments.
        acceptance_criteria:
        - Map syscall numbers to names using a syscall table (x86-64)
        - Decode integer arguments from registers (rdi, rsi, rdx, r10, r8, r9)
        - Read string arguments from traced process memory using PTRACE_PEEKDATA
        - Format output like strace showing name(args) = return_value
        pitfalls:
        - String arguments are pointers in the tracee's address space, must read word-by-word
        - PTRACE_PEEKDATA reads one word at a time, need loop for strings
        - Some syscalls have different argument meanings depending on flags
        concepts:
        - Syscall ABI
        - Remote memory reading
        - Argument formatting
        skills:
        - Remote process memory access
        - Pointer dereferencing across address spaces
        - String marshalling from traced processes
        - System call argument interpretation
        - ABI-specific data extraction
        deliverables:
        - Syscall number to name mapping table for x86-64
        - Register-based argument extraction for each syscall
        - Remote string reading from tracee memory via PTRACE_PEEKDATA
        - Formatted output matching strace-style display
        estimated_hours: 5-7
      - id: build-strace-m3
        name: Multi-process and Fork Following
        description: Handle traced processes that fork or exec new programs.
        acceptance_criteria:
        - Detect and follow child processes created by fork/vfork/clone
        - Use PTRACE_O_TRACEFORK and related options for automatic child tracing
        - Track multiple processes with PID-tagged output
        - Handle exec correctly, continuing to trace the new program
        pitfalls:
        - Must set PTRACE_O_TRACEFORK before the tracee calls fork
        - waitpid(-1) needed to catch events from any traced process
        - PTRACE_EVENT_EXEC changes the tracee's memory map entirely
        concepts:
        - Multi-process tracing
        - Fork following
        - ptrace options
        skills:
        - Multi-process coordination and tracking
        - Event-driven process management
        - Dynamic process tree construction
        - Fork and exec lifecycle handling
        deliverables:
        - Automatic fork/vfork/clone following with PTRACE_O_TRACE* options
        - Multi-process tracking with PID-prefixed output
        - Correct exec handling continuing trace into new program
        - Process tree tracking showing parent-child relationships
        estimated_hours: 5-8
      - id: build-strace-m4
        name: Filtering and Statistics
        description: Add syscall filtering and summary statistics.
        acceptance_criteria:
        - Filter output to show only specified syscalls (like strace -e trace=)
        - Collect timing data for each syscall using clock_gettime
        - Generate summary table showing call counts, errors, and time spent per syscall
        - Support output to file with -o flag
        pitfalls:
        - Timing should measure wall-clock time between syscall entry and exit
        - Error detection requires checking return value against -errno range
        - File output must handle concurrent writes from multiple traced processes
        concepts:
        - Syscall filtering
        - Performance measurement
        - Statistical aggregation
        skills:
        - Performance profiling and timing
        - Statistical data aggregation
        - Syscall filtering and pattern matching
        - Concurrent output management
        - Error classification and reporting
        deliverables:
        - Syscall filter supporting trace expressions like trace=open,read,write
        - Per-syscall timing measurement with microsecond precision
        - Summary statistics table with counts, errors, and cumulative time
        - File output option for non-intrusive tracing
        estimated_hours: 5-8
    - id: build-event-loop
      name: Event Loop with epoll
      description: Single-threaded server handling 10K+ connections
      difficulty: intermediate
      estimated_hours: 20-30
      essence: Scalable I/O multiplexing using the reactor pattern where a single-threaded event loop uses epoll to monitor thousands of file descriptors and dispatch events to registered handlers, combined with efficient timer management via timing wheel data structures for scheduling future events.
      why_important: Building this teaches you the foundational architecture behind high-performance servers like NGINX, Redis, and Node.js, and demonstrates how systems achieve concurrency without threads through non-blocking I/O and event-driven programming‚Äîcritical knowledge for backend infrastructure and systems programming roles.
      learning_outcomes:
      - Implement epoll-based I/O multiplexing with edge-triggered and level-triggered modes
      - Design a reactor pattern event loop with event registration and callback dispatch
      - Build a timing wheel data structure for O(1) timer insertion and expiration
      - Handle non-blocking socket operations with proper EAGAIN/EWOULDBLOCK error handling
      - Implement zero-copy data transfer strategies for high-throughput network I/O
      - Debug file descriptor leaks and event handler race conditions in single-threaded contexts
      - Build a complete HTTP/1.1 parser and state machine within an event-driven architecture
      - Measure and optimize event loop latency under 10K+ concurrent connections
      skills:
      - epoll/kqueue system APIs
      - Non-blocking I/O
      - Reactor pattern
      - Timer wheel algorithms
      - Single-threaded concurrency
      - Network socket programming
      - HTTP protocol implementation
      - Performance profiling
      tags:
      - event-loop
      - epoll
      - async-io
      - networking
      - reactor-pattern
      architecture_doc: architecture-docs/build-event-loop/index.md
      languages:
        recommended:
        - C
        - Rust
        also_possible:
        - Go
      resources:
      - name: epoll(7) man page
        url: https://man7.org/linux/man-pages/man7/epoll.7.html
        type: reference
      - name: The C10K Problem
        url: http://www.kegel.com/c10k.html
        type: article
      prerequisites:
      - type: project
        name: http-server-basic
      milestones:
      - id: build-event-loop-m1
        name: epoll Basics
        description: Set up epoll-based event loop with non-blocking socket I/O.
        acceptance_criteria:
        - Create epoll instance and register listening socket with EPOLLIN
        - Set all sockets to non-blocking mode using fcntl(O_NONBLOCK)
        - Accept connections and register them with epoll for read events
        - Handle read/write events in the event loop without blocking
        pitfalls:
        - Must handle EAGAIN/EWOULDBLOCK on non-blocking reads and writes
        - Level-triggered epoll re-notifies if data remains, edge-triggered does not
        - Forgetting to set listening socket non-blocking causes accept() to block
        concepts:
        - epoll API
        - Non-blocking I/O
        - Event-driven programming
        skills:
        - Socket programming with non-blocking I/O
        - Event loop architecture and control flow
        - Using epoll for scalable I/O multiplexing
        - Handling partial reads and writes in network code
        deliverables:
        - epoll instance creation and socket registration
        - Non-blocking socket configuration for all connections
        - Event loop processing EPOLLIN and EPOLLOUT events
        - Basic echo server handling multiple concurrent connections
        estimated_hours: 5-7
      - id: build-event-loop-m2
        name: Timer Wheel and Timeouts
        description: Implement a timer wheel for efficient timeout management.
        acceptance_criteria:
        - Implement hierarchical timer wheel with O(1) insert and delete
        - Support connection idle timeouts that close inactive connections
        - Use epoll_wait timeout parameter to drive timer expiration checks
        - Handle timer cancellation when connections become active again
        pitfalls:
        - Timer wheel slot count should be power of 2 for efficient modulo
        - Must handle timer expiration during the same tick as new registration
        - epoll_wait timeout should be minimum of next timer expiration
        concepts:
        - Timer wheel algorithm
        - Timeout management
        - Connection lifecycle
        skills:
        - Implementing efficient data structures for timers
        - Managing connection timeouts and cleanup
        - Coordinating event loop timing with timer expirations
        deliverables:
        - Timer wheel data structure with configurable slot count and resolution
        - O(1) timer insertion, deletion, and cancellation
        - Idle timeout enforcement closing inactive connections
        - Integration with epoll_wait timeout for timer-driven wakeups
        estimated_hours: 5-7
      - id: build-event-loop-m3
        name: Async Task Scheduling
        description: Add callback-based async task scheduling to the event loop.
        acceptance_criteria:
        - Support registering callbacks for read-ready, write-ready, and timer events
        - Implement deferred task queue for scheduling work in the next iteration
        - Support one-shot and repeating timer callbacks
        - Provide clean API for registering interest in file descriptor events
        pitfalls:
        - Callbacks must not modify the event registration set during iteration
        - Deferred tasks should run after all I/O events in the current tick
        - Must handle callback re-entrancy (callback registers new callback)
        concepts:
        - Reactor pattern
        - Callback scheduling
        - Event dispatch
        skills:
        - Designing callback-based asynchronous APIs
        - Managing execution order in event-driven systems
        - Handling re-entrant code safely
        - Task queue implementation and management
        deliverables:
        - Callback registration API for file descriptor events
        - Deferred task queue executing after I/O dispatch
        - One-shot and repeating timer callback support
        - Clean event loop API hiding epoll details
        estimated_hours: 5-8
      - id: build-event-loop-m4
        name: HTTP Server on Event Loop
        description: Build a complete HTTP server using the event loop framework.
        acceptance_criteria:
        - Parse HTTP requests incrementally as data arrives on non-blocking sockets
        - Handle partial reads and writes correctly with buffer management
        - Support keep-alive connections with proper timeout handling
        - Benchmark showing 10K+ concurrent connections with acceptable latency
        pitfalls:
        - HTTP parsing must handle requests split across multiple read events
        - Write buffer can fill up requiring EPOLLOUT registration
        - Keep-alive connections need idle timeout to prevent resource exhaustion
        concepts:
        - Incremental parsing
        - Buffer management
        - Connection keep-alive
        skills:
        - Building protocol parsers for streaming data
        - Managing per-connection state and buffers
        - Implementing HTTP protocol with keep-alive support
        - Backpressure handling with write buffers
        deliverables:
        - Incremental HTTP request parser handling partial data
        - Write buffer management with EPOLLOUT backpressure
        - Keep-alive support with configurable idle timeouts
        - Benchmark results demonstrating C10K capability
        estimated_hours: 5-8
    - id: build-kernel-module
      name: Linux Kernel Module
      description: Character device driver with /dev/ interface
      difficulty: intermediate
      estimated_hours: 20-30
      essence: Bridging kernel and user space through system call interfaces, implementing safe concurrent access to shared kernel resources, and managing hardware-like abstractions using character device semantics with file operations and ioctl commands.
      why_important: Kernel programming is fundamental for systems developers working on embedded systems, device drivers, or OS internals, and mastering kernel-userspace boundaries and concurrency primitives like mutexes and spinlocks is essential for building reliable low-level software that powers modern computing infrastructure.
      learning_outcomes:
      - Implement loadable kernel modules with proper initialization and cleanup routines
      - Design character device drivers with file_operations structure for read/write/open/release
      - Build ioctl interfaces for custom device control commands from userspace
      - Create proc filesystem entries for runtime kernel module introspection
      - Implement mutex and spinlock-based synchronization for concurrent device access
      - Debug kernel code using printk, dmesg, and kernel oops analysis
      - Manage kernel memory allocation and safe data transfer between kernel and user space
      - Handle module parameters and proper error handling in kernel context
      skills:
      - Kernel Module Development
      - Character Device Drivers
      - Kernel Synchronization Primitives
      - ioctl Implementation
      - proc Filesystem Interface
      - Kernel-User Memory Management
      - Concurrent Access Control
      - System Call Interface Design
      tags:
      - linux-kernel
      - device-drivers
      - kernel-programming
      - systems
      - concurrency
      architecture_doc: architecture-docs/build-kernel-module/index.md
      languages:
        recommended:
        - C
        also_possible: []
      resources:
      - name: Linux Kernel Module Programming Guide
        url: https://sysprog21.github.io/lkmpg/
        type: tutorial
      - name: Linux Device Drivers, 3rd Edition
        url: https://lwn.net/Kernel/LDD3/
        type: book
      prerequisites:
      - type: skill
        name: C programming
      - type: project
        name: signal-handler
      milestones:
      - id: build-kernel-module-m1
        name: Hello World Module
        description: Create a loadable kernel module with init and exit functions.
        acceptance_criteria:
        - Module compiles against current kernel headers using Kbuild Makefile
        - insmod loads module and printk outputs to kernel log (dmesg)
        - rmmod cleanly unloads module with cleanup message
        - Module info visible via modinfo showing author, description, license
        pitfalls:
        - Must declare MODULE_LICENSE("GPL") or kernel will taint and restrict symbol access
        - Kernel headers version must match running kernel exactly
        - printk uses log levels (KERN_INFO, KERN_ERR) not newlines for message separation
        concepts:
        - Kernel module lifecycle
        - Kbuild system
        - Kernel logging
        skills:
        - Kernel module compilation and loading
        - Basic kernel API usage
        - Kernel logging and debugging
        deliverables:
        - Kernel module with module_init and module_exit functions
        - Kbuild Makefile compiling against installed kernel headers
        - Module metadata with MODULE_LICENSE, MODULE_AUTHOR, MODULE_DESCRIPTION
        - Verified loading/unloading with dmesg output confirmation
        estimated_hours: 3-5
      - id: build-kernel-module-m2
        name: Character Device Driver
        description: Implement a character device with read/write from userspace.
        acceptance_criteria:
        - Register character device with alloc_chrdev_region for dynamic major number
        - Create /dev/ node automatically using class_create and device_create
        - Implement file_operations with open, release, read, and write handlers
        - Userspace can echo/cat to/from the device node to store and retrieve data
        pitfalls:
        - Must use copy_to_user/copy_from_user for kernel/user memory transfer
        - Never directly dereference userspace pointers from kernel code
        - read handler must return 0 at EOF to avoid infinite read loops
        concepts:
        - Character devices
        - File operations
        - Kernel/user boundary
        skills:
        - Character device registration
        - Userspace-kernel data transfer
        - File operations implementation
        - Device file creation
        deliverables:
        - Dynamic major number allocation with alloc_chrdev_region
        - Automatic /dev/ node creation via device class
        - file_operations struct with open, release, read, write handlers
        - Kernel buffer for storing data written from userspace
        estimated_hours: 5-8
      - id: build-kernel-module-m3
        name: ioctl and proc Interface
        description: Add ioctl commands and /proc filesystem entries.
        acceptance_criteria:
        - Implement unlocked_ioctl handler with custom command definitions using _IOW/_IOR macros
        - Create /proc entry using proc_create showing device statistics
        - Support ioctl commands for buffer resize, clear, and status query
        - Userspace test program exercises all ioctl commands and reads /proc entry
        pitfalls:
        - ioctl command numbers must be unique, use _IOW/_IOR/_IOWR macros with magic number
        - /proc read callback must handle partial reads and offset correctly
        - seq_file interface is preferred over raw proc_read for /proc entries
        concepts:
        - ioctl interface
        - /proc filesystem
        - Kernel-user communication
        skills:
        - ioctl command design
        - /proc filesystem integration
        - Kernel-userspace communication patterns
        - Custom device control interfaces
        deliverables:
        - ioctl handler with properly defined command numbers using macros
        - /proc filesystem entry showing device statistics and configuration
        - Userspace test program exercising all ioctl commands
        - Header file shared between kernel module and userspace for ioctl definitions
        estimated_hours: 5-8
      - id: build-kernel-module-m4
        name: Concurrent Access and Locking
        description: Handle concurrent access to the device from multiple processes.
        acceptance_criteria:
        - Protect shared device buffer with mutex for concurrent read/write safety
        - Implement wait queue for blocking read when buffer is empty
        - Support poll/select by implementing poll file operation
        - Stress test with multiple concurrent readers and writers shows no data corruption
        pitfalls:
        - Cannot use sleeping locks (mutex) in interrupt context or with spinlock held
        - wait_event_interruptible must check condition to avoid spurious wakeups
        - poll handler must call poll_wait and return correct mask bits
        concepts:
        - Kernel synchronization
        - Wait queues
        - Poll/select support
        skills:
        - Kernel concurrency primitives
        - Spinlock and mutex usage
        - Wait queue management
        - Poll/select mechanism implementation
        - Race condition prevention
        deliverables:
        - Mutex-protected buffer for concurrent access safety
        - Blocking read with wait queue waking on new data arrival
        - Poll/select support enabling multiplexed I/O on the device
        - Stress test demonstrating correct behavior under concurrent load
        estimated_hours: 5-8
    advanced:
    - id: http2-server
      name: HTTP/2 Server
      description: Multiplexing, HPACK
      difficulty: advanced
      estimated_hours: 30-50
      essence: Binary framing layer with frame type parsing and serialization, multiplexed stream state machines managing concurrent request/response pairs, HPACK header compression using static/dynamic table indexing, and window-based flow control preventing buffer overflow at connection and stream levels.
      why_important: Building an HTTP/2 server demystifies modern web infrastructure by teaching you how browsers and servers actually communicate, preparing you for performance optimization, protocol design, and low-level network programming roles.
      learning_outcomes:
      - Implement binary frame parsing and serialization for HTTP/2's frame types
      - Design state machines to manage stream lifecycle transitions and dependencies
      - Build HPACK encoder/decoder with static table lookups and dynamic table eviction
      - Implement window-based flow control with WINDOW_UPDATE frame handling
      - Handle stream prioritization using dependency trees and weight calculations
      - Debug binary protocol implementations using frame-level inspection
      - Implement concurrent stream multiplexing over a single TCP connection
      - Design robust error handling for protocol violations and connection failures
      skills:
      - Binary Protocol Design
      - State Machine Implementation
      - Header Compression
      - Flow Control Algorithms
      - Stream Multiplexing
      - Network Protocol Debugging
      - Concurrent Programming
      - Performance Optimization
      tags:
      - advanced
      - binary
      - c
      - go
      - hpack
      - multiplexing
      - networking
      - rust
      - service
      - streaming
      - streams
      architecture_doc: architecture-docs/http2-server/index.md
      languages:
        recommended:
        - Go
        - Rust
        - C
        also_possible:
        - Java
        - Python
      resources:
      - type: specification
        name: RFC 7540 - HTTP/2
        url: https://httpwg.org/specs/rfc7540.html
      - type: specification
        name: RFC 7541 - HPACK
        url: https://httpwg.org/specs/rfc7541.html
      prerequisites:
      - type: skill
        name: HTTP/1.1 server
      - type: skill
        name: TLS
      - type: skill
        name: Binary protocols
      milestones:
      - id: http2-server-m1
        name: Binary Framing
        description: Parse and emit HTTP/2 frames.
        acceptance_criteria:
        - Frame header parsing correctly reads 9-byte header with length, type, flags, and stream ID
        - Frame type handling supports DATA, HEADERS, SETTINGS, PING, GOAWAY, and WINDOW_UPDATE
        - Frame flags correctly interpreted per frame type for END_STREAM and END_HEADERS
        - Payload extraction reads correct number of bytes based on frame length field
        pitfalls:
        - Endianness issues
        - Frame length limits
        - Reserved bit handling
        concepts:
        - Binary protocols
        - Frame-based messaging
        - Protocol parsing
        skills:
        - Binary protocol parsing
        - Low-level byte manipulation
        - Network protocol implementation
        deliverables:
        - Frame header parsing extracting 9-byte header with length, type, and flags
        - Frame types implementation for DATA, HEADERS, SETTINGS, and more
        - Frame serialization encoding frames into binary wire format
        - Frame validation checking length limits and reserved bit constraints
        estimated_hours: 5-8
      - id: http2-server-m2
        name: HPACK Compression
        description: Implement header compression with static/dynamic tables.
        acceptance_criteria:
        - Static table lookup resolves indexed headers from the predefined 61-entry table
        - Dynamic table management adds and evicts entries maintaining size constraint
        - Huffman decoding decompresses encoded header values to original string form
        - Integer encoding and decoding handles 5, 6, and 7-bit prefix formats correctly
        pitfalls:
        - Table size eviction
        - Index off-by-one errors
        - Huffman boundary handling
        concepts:
        - Header compression
        - Table-based encoding
        - Huffman coding
        skills:
        - Compression algorithm implementation
        - Dynamic data structure management
        - Huffman encoding/decoding
        deliverables:
        - Static table with predefined 61 header name-value entries
        - Dynamic table with FIFO eviction and configurable size limit
        - Header encoding compressing headers using indexed and literal representations
        - Header decoding reconstructing headers from HPACK-compressed wire format
        estimated_hours: 8-12
      - id: http2-server-m3
        name: Stream Management
        description: Handle multiplexed streams with proper state machines.
        acceptance_criteria:
        - Stream states transition correctly between idle, open, half-closed, and closed
        - Stream ID allocation uses odd numbers for client and even for server streams
        - Priority handling schedules stream data based on weight and dependency tree
        - Concurrent stream limits enforce maximum active streams per connection setting
        pitfalls:
        - State machine violations
        - Stream ID exhaustion
        - Dependency cycles
        concepts:
        - Multiplexing
        - State machines
        - Concurrency
        skills:
        - Concurrent stream handling
        - State machine implementation
        - Resource lifecycle management
        deliverables:
        - Stream state machine implementing idle, open, half-closed, and closed states
        - Stream prioritization with weight and dependency configuration
        - Stream creation assigning IDs for client and server-initiated streams
        - RST_STREAM handling for stream cancellation and error signaling
        estimated_hours: 8-12
      - id: http2-server-m4
        name: Flow Control
        description: Implement connection and stream-level flow control.
        acceptance_criteria:
        - Window sizes track available send capacity at connection and stream levels
        - WINDOW_UPDATE frames correctly increment the receiver's available send window
        - Connection-level windows limit total DATA frame bytes across all streams combined
        - Backpressure handling queues data when send window hits zero until window update arrives
        pitfalls:
        - Window underflow/overflow
        - Deadlocks from exhausted windows
        - Forgetting connection window
        concepts:
        - Flow control
        - Backpressure
        - Resource management
        skills:
        - Window-based flow control
        - Backpressure handling
        - Concurrent resource accounting
        deliverables:
        - Window update frame handling for adjusting flow control window sizes
        - Connection-level flow control tracking aggregate send window across streams
        - Stream-level flow control tracking per-stream send window independently
        - Backpressure handling queueing data when send window is exhausted
        estimated_hours: 9-18
    - id: container-basic
      name: Container (Basic)
      description: Namespaces isolation
      difficulty: advanced
      estimated_hours: 15-25
      essence: Kernel-level process and resource isolation using namespace syscalls (clone, unshare) and cgroup controllers to partition system resources without hardware virtualization.
      why_important: Building containers from scratch demystifies Docker/Kubernetes internals and teaches fundamental Linux kernel primitives used in production container orchestration, making you effective at debugging container issues and optimizing containerized deployments.
      learning_outcomes:
      - Implement process isolation using PID namespaces with clone() or unshare() syscalls
      - Configure mount namespaces with pivot_root to isolate filesystem views
      - Create network namespaces and virtual ethernet pairs for container networking
      - Apply cgroup controllers to enforce CPU, memory, and I/O resource limits
      - Debug namespace-related issues by understanding /proc filesystem mechanics
      - Build a minimal container runtime handling process lifecycle and cleanup
      - Implement security boundaries using namespace capabilities and privilege dropping
      - Understand container escape vulnerabilities and mitigation strategies
      skills:
      - Linux Kernel APIs
      - System Programming
      - Process Isolation
      - Resource Management
      - Network Virtualization
      - Filesystem Manipulation
      - Low-Level Debugging
      - Container Security
      tags:
      - advanced
      - c
      - cgroups
      - go
      - multi-tenancy
      - namespaces
      - rootfs
      - rust
      architecture_doc: architecture-docs/container-basic/index.md
      languages:
        recommended:
        - C
        - Go
        - Rust
        also_possible:
        - Python
      resources:
      - name: Linux Namespaces
        url: https://man7.org/linux/man-pages/man7/namespaces.7.html
        type: documentation
      - name: Containers from Scratch
        url: https://ericchiang.github.io/post/containers-from-scratch/
        type: article
      prerequisites:
      - type: skill
        name: Linux system calls
      - type: skill
        name: Process management
      - type: skill
        name: Filesystem basics
      milestones:
      - id: container-basic-m1
        name: Process Namespace
        description: Isolate process tree with PID namespace.
        acceptance_criteria:
        - Create new PID namespace isolating child process IDs from host
        - Child process sees itself as PID 1 within the new namespace
        - Parent process sees the real host PID of the child process
        - Handle zombie process reaping within the namespace correctly
        pitfalls:
        - Stack direction
        - PID 1 responsibilities
        - Zombie processes
        concepts:
        - PID namespaces
        - clone()
        - Process isolation
        skills:
        - System call programming
        - Process management and lifecycle
        - Low-level Linux programming
        - Error handling for syscalls
        deliverables:
        - PID namespace creation using clone or unshare syscall
        - Process isolation verification comparing inner and outer PIDs
        - Init process setup running as PID 1 in namespace
        - Namespace unsharing from parent process context
        estimated_hours: 3-4
      - id: container-basic-m2
        name: Mount Namespace
        description: Isolate filesystem mounts.
        acceptance_criteria:
        - New mount namespace isolates filesystem mounts from the host
        - Private mount propagation prevents mount events leaking to host
        - Pivot root switches container to new filesystem root directory
        - Mount essential filesystems like /proc and /sys inside container
        pitfalls:
        - Mount propagation
        - pivot_root requirements
        - Device nodes
        concepts:
        - Mount namespaces
        - pivot_root
        - Filesystem isolation
        skills:
        - Filesystem operations and manipulation
        - Directory tree management
        - System call debugging
        - Root filesystem configuration
        deliverables:
        - Mount namespace creation for filesystem isolation
        - Root filesystem setup with minimal directory structure
        - Bind mount configuration for shared host directories
        - Proc and sys filesystem mounting inside the namespace
        estimated_hours: 4-5
      - id: container-basic-m3
        name: Network Namespace
        description: Isolate network stack.
        acceptance_criteria:
        - Create network namespace with isolated network interfaces and routing
        - Set up veth pair connecting container namespace to host bridge
        - Configure IP addresses on both ends of the veth pair correctly
        - Enable container networking with outbound connectivity through NAT
        pitfalls:
        - Namespace timing
        - veth cleanup
        - NAT configuration
        concepts:
        - Network namespaces
        - veth pairs
        - Container networking
        skills:
        - Network interface configuration
        - Virtual device management
        - Namespace lifecycle management
        - Network troubleshooting and debugging
        deliverables:
        - Network namespace creation for isolated network stack
        - Veth pair setup connecting container to host network
        - Bridge networking configuration for multi-container communication
        - NAT configuration for outbound internet access from container
        estimated_hours: 4-5
      - id: container-basic-m4
        name: Cgroups (Resource Limits)
        description: Limit CPU, memory, and other resources.
        acceptance_criteria:
        - Create cgroup for container process and assign PID correctly
        - Set memory limit and verify OOM behavior when exceeded
        - Set CPU quota and period to restrict CPU usage percentage
        - Clean up cgroup hierarchy on container exit to prevent leaks
        pitfalls:
        - cgroups v1 vs v2
        - Controller availability
        - Cleanup order
        concepts:
        - cgroups
        - Resource limits
        - Container resources
        skills:
        - Resource monitoring and profiling
        - Control group configuration
        - Filesystem hierarchy manipulation
        - System resource management
        deliverables:
        - Cgroup creation and process assignment for resource control
        - Memory limit configuration with enforcement verification
        - CPU limit configuration using quota and period settings
        - Process limit to cap number of tasks in container
        estimated_hours: 4-5
    - id: mini-shell
      name: Mini Shell
      description: Job control, background processes
      difficulty: advanced
      estimated_hours: 25-40
      essence: Process lifecycle orchestration through fork/exec/wait, file descriptor manipulation for I/O redirection and pipes, signal-based asynchronous event handling, and process group management for terminal foreground/background control.
      why_important: Building a shell demystifies how operating systems manage processes and reveals the coordination between kernel syscalls, terminal drivers, and user-space programs. This project develops foundational systems programming skills essential for backend engineering, DevOps tooling, and understanding how production applications interact with the OS.
      learning_outcomes:
      - Implement process creation and lifecycle management using fork, exec, and wait system calls
      - Design parsers for command-line syntax including pipelines and I/O redirection operators
      - Build signal handlers for SIGINT, SIGTSTP, and SIGCHLD with proper signal masking
      - Implement process group management and terminal control using setpgid and tcsetpgrp
      - Handle asynchronous job state transitions between foreground, background, stopped, and terminated
      - Debug race conditions and signal delivery timing issues in concurrent process coordination
      - Manage file descriptor plumbing for pipes and redirections across multiple processes
      - Implement proper cleanup and orphan process prevention through waitpid with WNOHANG
      skills:
      - Process management
      - Signal handling
      - Job control
      - System calls (fork/exec/wait)
      - File descriptors and pipes
      - Terminal control
      - Inter-process communication
      - Race condition debugging
      tags:
      - advanced
      - c
      - job-control
      - pipes
      - redirection
      - rust
      - scheduling
      - systems
      architecture_doc: architecture-docs/mini-shell/index.md
      languages:
        recommended:
        - C
        - Rust
        also_possible:
        - Go
        - Zig
      resources:
      - type: article
        name: Writing Your Own Shell
        url: https://brennan.io/2015/01/16/write-a-shell-in-c/
      - type: book
        name: Advanced Programming in Unix Environment - Ch 9
        url: https://www.apuebook.com/
      prerequisites:
      - type: skill
        name: Process spawner
      - type: skill
        name: Signal handling
      - type: skill
        name: Unix process model
      milestones:
      - id: mini-shell-m1
        name: Command Execution
        description: Parse and execute simple commands with arguments.
        acceptance_criteria:
        - Command line input is parsed into program name and argument tokens respecting whitespace boundaries
        - Quoted strings with single or double quotes are treated as single tokens preserving internal spaces
        - External commands are located via PATH search and executed in a forked child process
        - Built-in commands cd, exit, and pwd execute within the shell process without forking a child
        pitfalls:
        - Forgetting to null-terminate args
        - Not handling empty input
        - Memory leaks in tokenizer
        concepts:
        - Lexical analysis
        - fork/exec pattern
        - Process creation
        skills:
        - Process management
        - Command-line parsing
        - System call programming
        - Memory management in C
        - Error handling and validation
        deliverables:
        - Process launcher using fork and exec system calls to run external programs as child processes
        - PATH searcher that locates executable files by scanning directories listed in the PATH environment variable
        - Command-not-found error handler that prints a descriptive message when the executable cannot be located
        - Exit status capturer that stores the child process return code for use in conditionals and $? expansion
        estimated_hours: 4-6
      - id: mini-shell-m2
        name: Pipes and Redirection
        description: Implement pipelines and I/O redirection.
        acceptance_criteria:
        - Input redirection with < reads the command's stdin from the specified file instead of the terminal
        - Output redirection with > writes the command's stdout to the specified file, creating or truncating it
        - Pipeline syntax cmd1 | cmd2 | cmd3 chains standard output to standard input across multiple commands
        - File creation respects umask and output redirection creates the file with standard permissions if it does not exist
        pitfalls:
        - Fd leaks causing hangs
        - Not closing pipe ends
        - Wrong redirection precedence
        concepts:
        - File descriptors
        - dup2
        - Pipe communication
        skills:
        - Inter-process communication
        - File descriptor manipulation
        - Pipeline construction
        - I/O redirection patterns
        - Resource cleanup
        deliverables:
        - Pipe connector that chains the stdout of one command to the stdin of the next using the pipe system call
        - Stdin redirector that opens a file and connects it to the command's standard input via dup2
        - Stdout redirector that creates or truncates a file and connects it to the command's standard output via dup2
        - Append redirector that opens a file in append mode and connects it to the command's standard output
        estimated_hours: 5-8
      - id: mini-shell-m3
        name: Job Control
        description: Implement background jobs and job management.
        acceptance_criteria:
        - Appending & to a command line launches the command as a background job printing its job number and PID
        - The jobs built-in lists all current jobs with their job number, state, and command string
        - The fg and bg built-ins resume a stopped job in the foreground or background by job number
        - Process groups are set correctly so that signals from the terminal reach only the foreground job group
        pitfalls:
        - Race conditions with setpgid
        - Terminal control issues
        - Zombie processes
        concepts:
        - Process groups
        - Session management
        - Terminal control
        skills:
        - Job control implementation
        - Process group management
        - Terminal session handling
        - Background process orchestration
        - Process state tracking
        deliverables:
        - Foreground and background job manager that tracks running and stopped jobs with their process groups
        - SIGTSTP handler that suspends the foreground job when the user presses Ctrl+Z and returns to the prompt
        - Built-in fg and bg commands that resume stopped jobs in the foreground or background respectively
        - Job table manager that assigns job numbers and tracks each job's PID, state, and command string
        estimated_hours: 8-12
      - id: mini-shell-m4
        name: Signal Handling
        description: Handle Ctrl+C, Ctrl+Z and job notifications properly.
        acceptance_criteria:
        - Ctrl+C sends SIGINT only to the foreground job's process group, not to the shell process
        - Ctrl+Z sends SIGTSTP to the foreground job's process group suspending it and returning to the prompt
        - SIGCHLD is handled to reap background jobs on completion and report their exit status to the user
        - Signal masks are saved before fork and restored to defaults in the child process after fork
        pitfalls:
        - Signal handler races
        - Async-signal-safe functions
        - Interrupted system calls
        concepts:
        - Signal masking
        - Async-signal safety
        - Job notification
        skills:
        - Signal handler implementation
        - Asynchronous event handling
        - Safe signal programming
        - Job status notification
        - Race condition prevention
        deliverables:
        - SIGINT handler that delivers the interrupt signal to the foreground job only, not to the shell itself
        - SIGTSTP handler that delivers the stop signal to the foreground job and updates its state to stopped
        - Child process signal forwarder that relays terminal-generated signals to the foreground process group
        - Signal mask restorer that resets signal dispositions to default in child processes after fork
        estimated_hours: 8-14
    - id: virtual-memory-sim
      name: Virtual Memory Simulator
      description: Page tables, TLB
      difficulty: advanced
      estimated_hours: 20-30
      essence: Multi-level page table address translation with TLB caching for fast virtual-to-physical memory mapping, implementing page fault handling and replacement algorithms (LRU, FIFO, optimal) when physical memory is exhausted.
      why_important: Building this teaches you how operating systems provide memory isolation and efficient address translation, skills essential for systems programming, OS development, and understanding performance bottlenecks in memory-intensive applications.
      learning_outcomes:
      - Implement single-level and multi-level page table structures with address translation logic
      - Design and build a Translation Lookaside Buffer cache with hit/miss handling
      - Implement page replacement algorithms including LRU, FIFO, and optimal replacement
      - Debug address translation edge cases including page faults and invalid memory accesses
      - Build hierarchical page tables to reduce memory overhead for sparse address spaces
      - Measure and optimize memory access performance with TLB hit rates and page fault statistics
      - Implement page table walking algorithms for multi-level address resolution
      - Design memory management policies balancing space overhead versus lookup performance
      skills:
      - Virtual memory management
      - Address translation
      - Page table structures
      - TLB caching
      - Page replacement algorithms
      - Memory hierarchy optimization
      - Systems programming
      - Performance profiling
      tags:
      - advanced
      - c
      - page-tables
      - python
      - replacement
      - rust
      - tlb
      architecture_doc: architecture-docs/virtual-memory-sim/index.md
      languages:
        recommended:
        - C
        - Rust
        - Python
        also_possible:
        - Go
        - Java
      resources:
      - type: book
        name: OSTEP - Address Translation
        url: https://pages.cs.wisc.edu/~remzi/OSTEP/vm-mechanism.pdf
      - type: book
        name: OSTEP - Paging
        url: https://pages.cs.wisc.edu/~remzi/OSTEP/vm-paging.pdf
      prerequisites:
      - type: skill
        name: Memory concepts
      - type: skill
        name: Binary/hex
      - type: skill
        name: Data structures
      milestones:
      - id: virtual-memory-sim-m1
        name: Page Table
        description: Implement single-level page table with address translation.
        acceptance_criteria:
        - Virtual to physical address translation returns correct frame number and offset
        - Page table entries store flags for valid, dirty, referenced, and read-write permissions
        - Handle page faults by detecting invalid page table entries on lookup
        - Valid and invalid bits are set and cleared correctly on page load and eviction
        pitfalls:
        - Off-by-one in bit shifting
        - Forgetting offset
        - Protection check order
        concepts:
        - Address translation
        - Page tables
        - Memory protection
        skills:
        - Bitwise operations for address manipulation
        - Data structure design for page tables
        - Memory access pattern optimization
        - Virtual to physical address mapping
        deliverables:
        - Page table entry structure containing frame number, valid bit, and permission flags
        - Page table construction initializing entries for each virtual page in the address space
        - Page table lookup translating a virtual page number to a physical frame number
        - Valid and invalid bits indicating whether a page is currently resident in memory
        estimated_hours: 4-6
      - id: virtual-memory-sim-m2
        name: TLB
        description: Add Translation Lookaside Buffer for faster translations.
        acceptance_criteria:
        - TLB lookup is checked before consulting the full page table
        - TLB miss triggers a full page table walk and caches the result
        - TLB eviction uses LRU or random replacement when all slots are occupied
        - TLB flush clears all entries on process context switch to prevent stale mappings
        pitfalls:
        - TLB coherency with page table
        - Context switch handling
        - ASID management
        concepts:
        - Caching
        - Locality
        - TLB shootdown
        skills:
        - Cache implementation and management
        - Performance optimization through caching
        - Context switch handling in OS
        - Hash table or associative array usage
        deliverables:
        - TLB structure implementing a small fast-lookup cache of recent page translations
        - TLB lookup function checking for a matching virtual page number entry
        - TLB miss handling logic triggering a page table walk and updating the TLB
        - TLB flush operation invalidating all entries on context switch or page table change
        estimated_hours: 4-6
      - id: virtual-memory-sim-m3
        name: Multi-level Page Tables
        description: Implement hierarchical page tables to save memory.
        acceptance_criteria:
        - Two or three-level tables translate addresses through successive index lookups
        - Sparse address space handling avoids allocating tables for unmapped regions
        - On-demand table allocation creates second-level tables only when first accessed
        - Page table walks correctly traverse all levels and produce the final frame number
        pitfalls:
        - Index extraction order
        - Table pointer vs PTE confusion
        - Memory overhead calculation
        concepts:
        - Hierarchical structures
        - Sparse data
        - Memory efficiency
        skills:
        - Multi-level data structure implementation
        - Pointer arithmetic and indirection
        - Memory-efficient sparse data structures
        - Bit field extraction and manipulation
        deliverables:
        - Two-level page table splitting virtual address into directory and table indices
        - Page directory structure mapping directory entries to second-level page tables
        - Page table hierarchy supporting configurable number of translation levels
        - Memory savings achieved by allocating second-level tables only for mapped regions
        estimated_hours: 5-8
      - id: virtual-memory-sim-m4
        name: Page Replacement
        description: Implement page replacement algorithms when memory is full.
        acceptance_criteria:
        - FIFO replacement evicts pages in the order they were loaded into memory
        - LRU replacement evicts the page with the oldest last-access timestamp
        - Clock algorithm sweeps reference bits and evicts the first unreferenced page found
        - Track working set size reporting the number of distinct pages accessed in a window
        pitfalls:
        - Belady's anomaly with FIFO
        - Dirty page handling
        - Thrashing
        concepts:
        - Page replacement
        - Working set
        - Thrashing
        skills:
        - Algorithm implementation and comparison
        - LRU and FIFO queue management
        - Performance metrics collection and analysis
        - State tracking for dirty pages
        deliverables:
        - FIFO replacement policy evicting the oldest loaded page on memory full condition
        - LRU replacement policy evicting the least recently accessed page on memory full
        - Clock algorithm approximating LRU using a circular buffer with reference bits
        - Page fault handler loading the requested page from disk and updating the page table
        estimated_hours: 7-10
    - id: filesystem
      name: Filesystem Implementation
      description: Simple filesystem with inodes, directories, journaling
      difficulty: advanced
      estimated_hours: '60'
      essence: Block-level storage abstraction layered with inode-based metadata structures for file organization, directory tree traversal through linked entries, and crash consistency achieved through write-ahead journaling that serializes filesystem operations to a sequential log before applying changes to primary data structures.
      why_important: Building a filesystem teaches you how operating systems manage persistent storage at the lowest level, skills directly applicable to database internals, storage systems engineering, and understanding performance bottlenecks in any I/O-intensive application.
      learning_outcomes:
      - Understand filesystem internals and on-disk layouts
      - Implement inode-based file management
      - Build directory tree operations
      - Handle crash recovery with journaling
      skills:
      - Block devices
      - Inode structures
      - Directory entries
      - Journaling
      - FUSE interface
      - Caching
      tags:
      - advanced
      - directories
      - inodes
      - journaling
      - vfs
      architecture_doc: architecture-docs/filesystem/index.md
      languages:
        recommended: &id001
        - C
        - Rust
        - Go
        also_possible: []
      resources:
      - name: Writing a FUSE Filesystem Tutorial
        url: https://www.cs.nmsu.edu/~pfeiffer/fuse-tutorial/
        type: tutorial
      - name: 'Operating Systems: Three Easy Pieces'
        url: https://pages.cs.wisc.edu/~remzi/OSTEP/
        type: book
      - name: OSDev Wiki - Journaling
        url: https://wiki.osdev.org/Journaling
        type: documentation
      - name: libfuse API Documentation
        url: https://libfuse.github.io/doxygen/
        type: documentation
      - name: File-System Journaling (OSTEP Chapter)
        url: https://pages.cs.wisc.edu/~remzi/OSTEP/file-journaling.pdf
        type: paper
      prerequisites:
      - type: skill
        name: File I/O
      - type: skill
        name: Data structures
      - type: skill
        name: C programming
      milestones:
      - id: filesystem-m1
        name: Block Layer
        description: Raw block device read/write operations
        acceptance_criteria:
        - Read and write fixed-size blocks to disk image file backend correctly
        - Implement block allocation bitmap tracking free and used blocks on disk
        - Handle block caching for performance with write-back or write-through policy
        - Support multiple configurable block sizes (1K, 2K, 4K) for flexibility
        pitfalls:
        - Not handling partial block writes or read failures
        - Forgetting to initialize superblock magic numbers and version fields
        - Bitmap corruption from concurrent allocation without locking
        - Writing beyond device boundaries without size validation
        concepts:
        - Block-level I/O and sector alignment requirements
        - Superblock metadata for filesystem layout and configuration
        - Free block bitmap management and allocation strategies
        - Block caching and write-back policies
        skills:
        - Block I/O
        - Superblock
        - Bitmap allocation
        deliverables:
        - Block device abstraction providing fixed-size read and write operations
        - Superblock with filesystem metadata including size and block count
        - Block bitmap for tracking free and allocated block status
        - Block allocation and deallocation with bitmap updates
        - Disk image file backend for testing without physical device
        - Block caching layer reducing disk I/O for frequently accessed blocks
        estimated_hours: '12'
      - id: filesystem-m2
        name: Inode Management
        description: Inode structure for file metadata
        acceptance_criteria:
        - Store file metadata in inode including size, permissions, and timestamps
        - Allocate and free inodes using inode bitmap for tracking availability
        - Support direct and indirect block pointers for files of varying sizes
        - Handle inode table storage on disk with correct serialization format
        pitfalls:
        - Not handling indirect block chain traversal correctly
        - Forgetting to update timestamps on inode modifications
        - Leaking blocks when indirect pointers are not freed
        - Off-by-one errors in block offset calculations
        concepts:
        - Inode metadata structure with file type and permissions
        - Direct blocks for small files and indirect blocks for large files
        - Multi-level indirect block pointers for scalability
        - Reference counting and link semantics
        skills:
        - Inode structure
        - Direct/indirect blocks
        - Permissions
        deliverables:
        - Inode structure with metadata including size, permissions, and timestamps
        - Direct block pointers for small file data block addressing
        - Indirect block pointers (single, double) for large file support
        - Inode allocation and free operations with inode bitmap tracking
        - Inode read and write operations persisting metadata to disk
        - File type and permissions stored in inode mode field
        estimated_hours: '12'
      - id: filesystem-m3
        name: Directory Operations
        description: Directory entries and path resolution
        acceptance_criteria:
        - Store directory entries as name-to-inode mappings within directory blocks
        - Support directory creation and deletion with proper reference counting
        - Implement path resolution traversing components like a/b/c to find target inode
        - Handle . and .. entries correctly for self and parent directory references
        pitfalls:
        - Not validating directory entry names for special characters
        - Missing checks for circular directory references
        - Race conditions in concurrent directory modifications
        - Forgetting to handle parent directory references correctly
        concepts:
        - Directory entry format with inode number and name
        - Path traversal algorithm with component-by-component resolution
        - Hard links versus symbolic links in directory entries
        - Directory consistency and ordering guarantees
        skills:
        - Directory entries
        - Path parsing
        - Name lookup
        deliverables:
        - Directory entry structure mapping names to inode numbers
        - Add and remove directory entry operations for file management
        - Path to inode resolution traversing directory hierarchy
        - Create and delete file operations with directory entry management
        - Create and remove directory operations with recursive handling
        - Rename operations moving entries between directories atomically
        estimated_hours: '12'
      - id: filesystem-m4
        name: File Operations
        description: Read, write, truncate files
        acceptance_criteria:
        - Create, read, write, and truncate files with correct data integrity
        - Support file seeking to arbitrary positions for random access reads and writes
        - Handle file holes (sparse files) reading unallocated blocks as zero bytes
        - Update timestamps on file operations reflecting access and modification times
        pitfalls:
        - Not handling writes beyond current file size
        - Forgetting to zero-fill gaps when creating sparse files
        - Race conditions between concurrent reads and writes
        - Memory leaks from not releasing cached file data
        concepts:
        - File offset tracking and seek operations
        - Buffered I/O versus direct block access
        - Sparse file representation with hole detection
        - File truncation and block deallocation
        skills:
        - File read/write
        - Truncation
        - Sparse files
        deliverables:
        - Read file data from block storage via inode pointers
        - Write file data allocating new blocks as needed for growth
        - Truncate file releasing blocks beyond new reduced size
        - Append to file extending allocation and updating inode size
        - Sparse file support with unallocated blocks reading as zeros
        - File hole handling for efficient large file storage with gaps
        estimated_hours: '12'
      - id: filesystem-m5
        name: FUSE Interface
        description: Mount filesystem via FUSE
        acceptance_criteria:
        - Mount filesystem via FUSE so it appears as a regular mount point
        - Implement FUSE callbacks for getattr, read, write, and directory operations
        - Handle concurrent access from multiple processes accessing the mounted filesystem
        - Support unmount and cleanup releasing all resources and flushing pending data
        pitfalls:
        - Not handling FUSE interrupts and timeouts properly
        - Deadlocks from blocking operations in FUSE callbacks
        - Memory management issues with FUSE buffers
        - Forgetting to unmount cleanly causing resource leaks
        concepts:
        - FUSE callback interface and operation mapping
        - VFS layer abstraction and system call handling
        - Filesystem metadata synchronization with kernel
        - User-space versus kernel-space context switching overhead
        skills:
        - FUSE API
        - VFS operations
        - Mount/unmount
        deliverables:
        - FUSE operation callback registration for filesystem operations
        - getattr implementation returning file metadata from inode data
        - readdir implementation listing directory contents from entries
        - open, read, write, and release callbacks for file I/O
        - mkdir and rmdir callbacks for directory management operations
        - Mount and unmount lifecycle management for FUSE filesystem
        estimated_hours: '12'
    - id: reverse-proxy
      name: Reverse Proxy
      description: Nginx-like proxy with load balancing, caching, SSL termination
      difficulty: advanced
      estimated_hours: '55'
      essence: HTTP request parsing and forwarding between clients and backend servers, implementing TCP connection pooling and persistent socket reuse to minimize handshake overhead, load distribution algorithms for traffic balancing, and SSL/TLS handshake processing with certificate management for terminating encrypted connections at the proxy layer.
      why_important: Reverse proxies are fundamental infrastructure components used by every major web service at scale, teaching you low-level network programming, concurrency patterns, and performance optimization techniques directly applicable to distributed systems engineering roles.
      learning_outcomes:
      - Understand reverse proxy architecture
      - Implement load balancing algorithms
      - Build connection pooling for performance
      - Handle SSL termination
      skills:
      - HTTP parsing
      - Load balancing
      - Connection pooling
      - SSL/TLS
      - Caching
      - Health checks
      tags:
      - advanced
      - caching
      - networking
      - routing
      - security
      - service
      - ssl-termination
      architecture_doc: architecture-docs/reverse-proxy/index.md
      languages:
        recommended: *id001
        also_possible: []
      resources:
      - name: RFC 9110 HTTP Semantics
        url: https://httpwg.org/specs/rfc9110.html
        type: documentation
      - name: NGINX Reverse Proxy Guide
        url: https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/
        type: documentation
      - name: Building a Reverse Proxy with Rust Pingora
        url: https://carlosmv.hashnode.dev/building-a-reverse-proxy-with-pingora-rust
        type: tutorial
      - name: Writing a Reverse Proxy in Go
        url: https://developer20.com/writing-proxy-in-go/
        type: tutorial
      - name: TLS Termination Proxy Concepts
        url: https://en.wikipedia.org/wiki/TLS_termination_proxy
        type: article
      prerequisites:
      - type: skill
        name: HTTP basics
      - type: skill
        name: Networking
      - type: skill
        name: Async I/O
      milestones:
      - id: reverse-proxy-m1
        name: HTTP Proxy Core
        description: Basic HTTP request forwarding
        acceptance_criteria:
        - Accept incoming HTTP requests on configured listen port
        - Forward complete request to selected backend server
        - Return backend response including status, headers, and body to client
        - Handle both HTTP/1.1 and HTTP/2 protocol versions
        pitfalls:
        - Not handling partial reads from TCP streams
        - Forgetting to parse Transfer-Encoding and Content-Length headers
        - Mixing up client and backend connection state
        - Not properly closing connections on errors
        - Buffering entire requests in memory causing OOM
        concepts:
        - HTTP/1.1 request and response message structure
        - TCP socket programming and non-blocking I/O
        - Stream parsing for chunked transfer encoding
        - Connection state management and timeouts
        skills:
        - HTTP parsing
        - Request forwarding
        - Response handling
        deliverables:
        - Client connection acceptance on configurable listen address
        - HTTP request parsing extracting method, path, and headers
        - Upstream request forwarding to configured backend servers
        - Response relay sending backend response back to client
        - Header manipulation adding X-Forwarded-For and Via headers
        - Error handling returning appropriate status on backend failure
        estimated_hours: '11'
      - id: reverse-proxy-m2
        name: Load Balancing
        description: Distribute requests across backends
        acceptance_criteria:
        - Implement round-robin distribution across backend servers
        - Support weighted distribution based on backend server capacity
        - Perform periodic health checks against backend endpoints
        - Remove unhealthy backends from active pool automatically
        pitfalls:
        - Not handling backend failure during request forwarding
        - Implementing sticky sessions without timeout mechanisms
        - Forgetting to update weights when backends go down
        - Not accounting for slow backends in load calculations
        - Using naive round-robin without health checks
        concepts:
        - Round-robin and least-connections algorithms
        - Health checking and backend availability detection
        - Weighted distribution and priority queuing
        - Session affinity and sticky sessions
        skills:
        - Load balancing algorithms
        - Backend selection
        - Weighted distribution
        deliverables:
        - Round-robin load balancing distributing requests evenly
        - Least-connections algorithm routing to least busy backend
        - Weighted round-robin supporting different backend capacities
        - IP hash sticky sessions keeping client on same backend
        - Backend configuration file defining upstream server list
        - Dynamic backend updates adding or removing servers without restart
        estimated_hours: '11'
      - id: reverse-proxy-m3
        name: Connection Pooling
        description: Reuse connections to backends
        acceptance_criteria:
        - Maintain persistent keep-alive connections to backend servers
        - Implement separate connection pool for each backend server
        - Handle connection timeouts by evicting stale connections
        - Enforce maximum connection limits per backend pool
        pitfalls:
        - Reusing closed or stale connections without validation
        - Not implementing connection limits causing resource exhaustion
        - Forgetting to handle connection pool exhaustion gracefully
        - Creating new connections on every request defeating pooling
        - Not cleaning up idle connections leading to memory leaks
        concepts:
        - Connection lifecycle and idle timeout management
        - Pool sizing strategies and resource limits
        - Connection validation before reuse
        - Thread-safe pool access and synchronization
        skills:
        - Connection pools
        - Keep-alive
        - Pool management
        deliverables:
        - Persistent connection pool maintained per backend server
        - HTTP keep-alive connection reuse reducing TCP handshake overhead
        - Configurable pool size limits preventing resource exhaustion
        - Connection timeout evicting idle connections from pool
        - Health check integration removing broken connections from pool
        - Pool metrics tracking active, idle, and total connection counts
        estimated_hours: '11'
      - id: reverse-proxy-m4
        name: Caching
        description: Cache responses for performance
        acceptance_criteria:
        - Cache GET responses based on Cache-Control header directives
        - Generate unique cache keys from request attributes
        - Support cache invalidation by TTL expiry and explicit purge
        - Respect Cache-Control no-cache and no-store directives correctly
        pitfalls:
        - Caching non-cacheable responses like POST requests
        - Not respecting Vary headers in cache key generation
        - Serving stale content beyond max-age limits
        - Not handling cache invalidation on backend errors
        - Storing sensitive data in shared cache
        concepts:
        - Cache-Control and ETag header semantics
        - LRU eviction policies and cache size limits
        - Cache key generation from request properties
        - Conditional requests and 304 Not Modified responses
        skills:
        - Cache storage
        - Cache invalidation
        - Cache headers
        deliverables:
        - In-memory response cache with configurable maximum size
        - Cache key generation from request method, URL, and headers
        - TTL and max-age expiration based on response Cache-Control header
        - Cache-Control header parsing respecting no-cache and no-store directives
        - Conditional request support using ETag and If-Modified-Since headers
        - Cache bypass rules for authenticated or dynamic requests
        estimated_hours: '11'
      - id: reverse-proxy-m5
        name: SSL Termination
        description: Handle HTTPS connections
        acceptance_criteria:
        - Terminate TLS connections at proxy and forward plain HTTP to backends
        - Support SNI to serve multiple domains with different certificates
        - Forward decrypted requests to backend servers over plain HTTP
        - Handle certificate reload without restarting the proxy process
        pitfalls:
        - Not validating certificate expiration dates
        - Hardcoding cipher suites allowing weak encryption
        - Mishandling SNI causing wrong certificate selection
        - Storing private keys in source code or logs
        - Not implementing proper TLS version negotiation
        concepts:
        - TLS handshake process and cipher suite negotiation
        - X.509 certificate validation and trust chains
        - Server Name Indication for virtual hosting
        - Private key security and certificate rotation
        skills:
        - TLS/SSL
        - Certificate management
        - SNI
        deliverables:
        - SSL/TLS context setup with certificate and private key loading
        - Certificate loading from PEM files with chain validation
        - SNI support serving different certificates per domain name
        - TLS version configuration enforcing minimum TLS 1.2
        - Cipher suite selection prioritizing strong modern ciphers
        - HTTP to HTTPS redirect for plaintext requests
        estimated_hours: '11'
    expert:
    - id: build-docker
      name: Build Your Own Docker
      description: Container runtime with namespaces, cgroups
      difficulty: expert
      estimated_hours: 30-50
      essence: Kernel-level process sandboxing through namespace isolation (PID, mount, network, UTS, IPC), cgroup-enforced resource boundaries, and copy-on-write filesystem layering with OverlayFS to create lightweight execution environments that share the host kernel while maintaining independent process trees, network stacks, and root filesystems.
      why_important: Building a container runtime demystifies the core technology behind Docker and Kubernetes, teaching low-level Linux kernel interfaces that are essential for systems programming, infrastructure engineering, and understanding production containerized environments at a fundamental level.
      learning_outcomes:
      - Implement process isolation using Linux namespaces (PID, UTS, mount, network, IPC)
      - Configure cgroup hierarchies to enforce CPU, memory, and I/O resource limits
      - Build filesystem isolation using chroot and pivot_root syscalls
      - Design layered filesystem storage with OverlayFS for image efficiency
      - Create virtual network interfaces with veth pairs and bridge networking
      - Parse and implement OCI runtime specification for container lifecycle management
      - Debug low-level Linux kernel interactions and syscall failures
      - Implement copy-on-write semantics for container filesystem layers
      skills:
      - Linux Kernel Programming
      - System Calls and Syscalls
      - Namespace Isolation
      - Cgroup Resource Management
      - OverlayFS and Union Filesystems
      - Network Namespaces
      - Virtual Ethernet (veth)
      - OCI Specification Compliance
      - Low-Level Systems Design
      tags:
      - build-from-scratch
      - c
      - cgroups
      - containers
      - expert
      - go
      - isolation
      - multi-tenancy
      - namespaces
      - rust
      architecture_doc: architecture-docs/build-docker/index.md
      languages:
        recommended:
        - Go
        - Rust
        - C
        also_possible: []
      resources:
      - name: Containers from Scratch
        url: https://ericchiang.github.io/post/containers-from-scratch/
        type: article
      - name: Containers from Scratch (Video) - Liz Rice
        url: https://www.youtube.com/watch?v=8fi7uSYlOdc
        type: video
      - name: Linux Namespaces man page
        url: https://man7.org/linux/man-pages/man7/namespaces.7.html
        type: documentation
      - name: OCI Runtime Specification
        url: https://github.com/opencontainers/runtime-spec
        type: documentation
      prerequisites:
      - type: skill
        name: Linux system administration
      - type: skill
        name: Process management (fork, exec)
      - type: skill
        name: Filesystem concepts
      - type: skill
        name: Basic networking
      milestones:
      - id: build-docker-m1
        name: Process Isolation (Namespaces)
        description: Isolate a process using Linux namespaces (PID, UTS, mount).
        acceptance_criteria:
        - Process runs inside its own PID namespace and sees itself as PID 1 with no visibility of host processes
        - Process has its own hostname via UTS namespace that can be set independently of the host's hostname
        - Process has its own mount namespace so mount and unmount operations do not affect the host filesystem
        - Namespaces are created using clone() with appropriate flags or unshare() after fork for each namespace type
        pitfalls:
        - Not using CLONE_NEWPID correctly
        - /proc still showing host PIDs
        - Permission issues (usually need root)
        concepts:
        - Linux namespaces
        - Process isolation
        - clone() syscall
        skills:
        - System programming
        - Process management
        - Low-level Linux APIs
        - Privilege escalation handling
        deliverables:
        - PID namespace creation that gives the container process its own process ID space starting at PID 1
        - Network namespace setup that provides an isolated network stack with its own interfaces and routing table
        - Mount namespace for filesystem isolation ensuring the container has its own mount point hierarchy
        - UTS namespace that allows the container to have its own hostname independent of the host system
        estimated_hours: 4-6
      - id: build-docker-m2
        name: Resource Limits (cgroups)
        description: Limit container resources using cgroups (CPU, memory).
        acceptance_criteria:
        - Memory limit enforcement causes the container process to be OOM-killed if it exceeds the configured limit
        - CPU shares or quota limit the container's CPU consumption to the configured percentage of available cycles
        - Cgroup filesystem hierarchy is correctly set up with the container's limits written before the process starts
        - Container process is added to the cgroup before exec so all resource limits are in effect from the start
        pitfalls:
        - cgroup v1 vs v2 differences
        - Not cleaning up cgroups on exit
        - Memory limits not accounting kernel memory
        concepts:
        - Control groups
        - Resource limiting
        - OOM killer
        skills:
        - Resource management
        - Kernel interfaces
        - Performance tuning
        - System monitoring
        deliverables:
        - Memory limit cgroup configuration that caps the container's resident memory usage at a specified value
        - CPU shares and quota cgroup settings that control the container's share of CPU time
        - PIDs limit cgroup that restricts the number of processes a container can create to prevent fork bombs
        - Cgroup filesystem interaction that writes limit values to the correct cgroup v1 or v2 control files
        estimated_hours: 3-4
      - id: build-docker-m3
        name: Filesystem Isolation (chroot/pivot_root)
        description: Give container its own root filesystem using chroot or pivot_root.
        acceptance_criteria:
        - Container sees only its own filesystem tree and cannot access any host filesystem paths
        - Host filesystem is completely inaccessible from within the container after pivot_root and old root unmount
        - Proc filesystem is mounted at /proc inside the container so tools like ps and top work correctly
        - Container works with any base root filesystem such as Alpine, Ubuntu, or Debian extracted from an image
        pitfalls:
        - chroot is escapable without mount namespace
        - Forgetting to mount /proc
        - Not having required binaries in rootfs
        concepts:
        - chroot jails
        - pivot_root
        - Root filesystem
        skills:
        - Filesystem operations
        - Root environment setup
        - Binary dependency management
        - Mount point configuration
        deliverables:
        - Root filesystem extraction that unpacks a tarball or image layers into a directory to serve as the container root
        - pivot_root() call that atomically swaps the container's root filesystem and isolates it from the host
        - Mounting /proc and /sys inside the container so process and system information is available within the namespace
        - Old root unmount that removes access to the host filesystem from within the container after pivot_root
        estimated_hours: 3-4
      - id: build-docker-m4
        name: Layered Filesystem (OverlayFS)
        description: Implement layered filesystem for efficient image storage.
        acceptance_criteria:
        - Multiple read-only lower layers are merged into a single unified view visible to the container process
        - All filesystem writes are captured in the upper (writable) layer without modifying any lower layer
        - Copy-on-write semantics allow reading from lower layers and writing modifications only to the upper layer
        - Common image layers are shared between containers so disk space is not duplicated for identical layers
        pitfalls:
        - OverlayFS doesn't support all filesystems
        - Renaming directories has copy-up overhead
        - Open file handles survive layer changes
        concepts:
        - Union filesystems
        - Copy-on-write
        - Docker image layers
        skills:
        - Storage optimization
        - Filesystem drivers
        - Copy-on-write semantics
        - Layer management
        deliverables:
        - OverlayFS mount configuration with lower (read-only), upper (writable), and work directory paths
        - Layer stacking logic that merges multiple read-only image layers into a unified filesystem view
        - Copy-on-write behavior where modifications to lower-layer files are written to the upper layer only
        - Layer caching and reuse mechanism that shares common layers across multiple container instances
        estimated_hours: 4-6
      - id: build-docker-m5
        name: Container Networking
        description: Set up network namespace with virtual ethernet pair.
        acceptance_criteria:
        - Container has its own isolated network stack with its own loopback, IP addresses, and routing table
        - A veth pair connects the container namespace to the host bridge so traffic can flow between them
        - Container can reach external internet addresses via NAT masquerading through the host's network interface
        - Containers on the same bridge network can communicate with each other using their assigned IP addresses
        pitfalls:
        - DNS resolution (need /etc/resolv.conf)
        - iptables rules for masquerading
        - Bridge vs host network
        concepts:
        - Network namespaces
        - Virtual ethernet
        - Linux bridge networking
        skills:
        - Network configuration
        - Virtual device management
        - IP routing and forwarding
        - DNS configuration
        deliverables:
        - Virtual ethernet pair (veth) creation that connects the container's network namespace to the host
        - Bridge network setup that connects multiple container veth endpoints on a shared virtual switch
        - NAT and masquerade rules using iptables that allow containers to reach external networks through the host
        - Port forwarding rules that map host ports to container ports for accepting inbound connections
        estimated_hours: 5-8
      - id: build-docker-m6
        name: Image Format and CLI
        description: Implement OCI image format and Docker-compatible CLI.
        acceptance_criteria:
        - Image pull downloads all layers and the manifest from Docker Hub or any OCI-compliant registry
        - OCI image manifest is parsed to extract the layer digests, config, and entry point command
        - Filesystem layers are extracted and stacked using OverlayFS to create the container root filesystem
        - Container run command equivalent creates all isolation primitives and launches the specified process
        pitfalls:
        - Image manifest v1 vs v2
        - Content-addressable storage
        - Image and layer deduplication
        concepts:
        - OCI specification
        - Container registry protocol
        - CLI design
        skills:
        - Specification compliance
        - Image registry protocols
        - Content addressing
        - CLI design patterns
        deliverables:
        - OCI image specification parser that reads manifests, config, and layer descriptors from an image archive
        - Image pull client that downloads image layers and manifests from a Docker registry over HTTP
        - Container run command that creates namespaces, applies cgroups, mounts the filesystem, and executes the entry point
        - Container lifecycle management with start, stop, and remove commands for managing running containers
        estimated_hours: 6-10
    - id: build-shell
      name: Build Your Own Shell
      description: Full Unix shell with job control
      difficulty: advanced
      estimated_hours: 25-40
      essence: Process lifecycle management through fork/exec system calls, inter-process communication via Unix pipes with file descriptor manipulation, and signal-based job control using process groups (SIGTSTP, SIGCONT, SIGCHLD, SIGTTIN, SIGTTOU) to orchestrate foreground/background process coordination.
      why_important: Building a shell demystifies the core Unix process model and teaches low-level systems programming skills essential for operating systems work, DevOps tooling, and understanding how command-line environments actually function beneath abstractions.
      learning_outcomes:
      - Implement process creation and management using fork() and the exec() family of system calls
      - Design and build inter-process communication using Unix pipes with proper file descriptor management
      - Handle signal-based process control including SIGTSTP, SIGCONT, SIGCHLD for job suspension and resumption
      - Parse and tokenize command-line input with support for special characters, quotes, and escape sequences
      - Implement I/O redirection by manipulating file descriptors with dup2() before exec()
      - Build process group management for foreground/background job control using setpgid() and tcsetpgrp()
      - Debug race conditions and handle zombie processes through proper signal handling and waitpid()
      - Implement built-in commands that modify shell state without forking new processes
      skills:
      - Process Management
      - System Calls (fork/exec)
      - Inter-Process Communication
      - Signal Handling
      - File Descriptor Manipulation
      - Job Control
      - POSIX APIs
      - String Parsing
      tags:
      - advanced
      - build-from-scratch
      - c
      - go
      - job-control
      - pipes
      - redirection
      - rust
      - scheduling
      - signals
      - systems
      architecture_doc: architecture-docs/build-shell/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible: []
      resources:
      - name: Write a Shell in C
        url: https://brennan.io/2015/01/16/write-a-shell-in-c/
        type: article
      - name: CodeCrafters Shell Challenge
        url: https://app.codecrafters.io/courses/shell/overview
        type: tool
      - name: GNU Implementing a Shell
        url: https://www.gnu.org/software/libc/manual/html_node/Implementing-a-Shell.html
        type: documentation
      prerequisites:
      - type: skill
        name: C programming
      - type: skill
        name: Unix process model (fork, exec)
      - type: skill
        name: File descriptors
      - type: skill
        name: Signal handling basics
      milestones:
      - id: build-shell-m1
        name: Basic REPL and Command Execution
        description: Read input, parse into command and arguments, execute with fork/exec.
        acceptance_criteria:
        - Shell displays prompt and waits for user input on each iteration
        - Tokenizer correctly splits command line respecting quoted strings
        - External commands are found in PATH and executed in child process
        - Shell reports non-zero exit codes when child process fails
        - Empty input lines are handled gracefully without errors
        pitfalls:
        - Forgetting to null-terminate args array
        - Not handling newline from fgets()
        - Zombie processes
        concepts:
        - REPL pattern
        - fork() and exec() family
        - Process creation and waiting
        skills:
        - Process management and system calls
        - Command-line parsing and tokenization
        - Error handling for system calls
        - Unix process lifecycle
        deliverables:
        - Read-eval-print loop displaying prompt and reading user input
        - Command line tokenizer splitting input into command and arguments
        - Process spawning executing external commands with fork and exec
        - Exit status collection using waitpid and reporting to user
        estimated_hours: 3-4
      - id: build-shell-m2
        name: Built-in Commands
        description: Implement cd, exit, pwd, export, and other built-in commands.
        acceptance_criteria:
        - cd changes directory and updates PWD environment variable accordingly
        - cd with no arguments changes to HOME directory by default
        - exit terminates shell returning specified or zero exit code to parent
        - export sets environment variables visible to subsequently spawned child processes
        - echo prints its arguments separated by spaces followed by newline
        pitfalls:
        - Trying to cd in child process (only affects child)
        - Not expanding ~ to HOME
        - Environment variables not inherited
        concepts:
        - Built-in vs external commands
        - Working directory
        - Environment variables
        skills:
        - Shell built-in command implementation
        - Directory navigation and path manipulation
        - Environment variable management
        - Command dispatch logic
        deliverables:
        - cd command changing current working directory with error handling
        - exit command terminating shell with optional exit code parameter
        - export command setting environment variables for child processes
        - echo command printing arguments with variable expansion support
        estimated_hours: 2-3
      - id: build-shell-m3
        name: I/O Redirection
        description: Implement input (<), output (>), and append (>>) redirection.
        acceptance_criteria:
        - Output redirect > creates or truncates file and writes command stdout to it
        - Input redirect < opens file and connects it to command stdin
        - Append redirect >> creates or appends to file without truncating existing content
        - Error redirect 2> writes command stderr to specified file separately from stdout
        - Multiple redirections can be combined in a single command line
        pitfalls:
        - Forgetting to close original fd after dup2
        - Wrong open() flags
        - File permissions on created files
        concepts:
        - File descriptors
        - dup2() system call
        - Unix I/O model
        skills:
        - File descriptor manipulation
        - File I/O operations
        - Unix redirection operators
        - System call error handling
        deliverables:
        - Output redirection writing command stdout to specified file
        - Input redirection reading command stdin from specified file
        - Append redirection adding command stdout to end of existing file
        - Error redirection writing command stderr to specified file
        estimated_hours: 2-3
      - id: build-shell-m4
        name: Pipes
        description: Implement command pipelines (cmd1 | cmd2 | cmd3).
        acceptance_criteria:
        - Pipe operator connects stdout of first command to stdin of second command
        - Multi-stage pipelines chain three or more commands passing data through each
        - All processes in pipeline run concurrently without deadlocking on full pipe buffers
        - Pipeline exit status reflects the exit code of the last command in the chain
        pitfalls:
        - Not closing unused pipe ends (causes hang)
        - Parent not closing pipe fds
        - Order of fork/close operations
        concepts:
        - Unix pipes
        - Process communication
        - File descriptor inheritance
        skills:
        - Inter-process communication with pipes
        - Multi-process coordination
        - File descriptor management across processes
        - Pipeline execution flow
        deliverables:
        - Pipe creation connecting stdout of left command to stdin of right command
        - Pipeline execution chaining multiple commands with pipe operators
        - Process group management for pipeline child processes
        - Pipeline exit status returning status of last command in chain
        estimated_hours: 4-6
      - id: build-shell-m5
        name: Background Jobs
        description: Run commands in background with &, implement job listing.
        acceptance_criteria:
        - Trailing & launches command in background and immediately shows next prompt
        - Job table stores PID, command string, and running status for each background job
        - jobs command displays job number, status, and command for all background processes
        - Completed background jobs are reported to user at next prompt display
        pitfalls:
        - Zombie processes from background jobs
        - Race condition in SIGCHLD handler
        - Reaping wrong child
        concepts:
        - Background execution
        - Asynchronous process management
        - Zombie processes
        skills:
        - Background process management
        - Signal handling for child processes
        - Asynchronous process monitoring
        - Job table data structures
        deliverables:
        - Background execution launching commands with trailing ampersand operator
        - Job table tracking background process IDs and status
        - jobs command listing all active background processes with status
        - wait command blocking until specified background job completes
        estimated_hours: 3-4
      - id: build-shell-m6
        name: Job Control (fg, bg, Ctrl+Z)
        description: Full job control with suspend, resume, foreground/background.
        acceptance_criteria:
        - Ctrl+Z sends SIGTSTP to foreground process and returns shell prompt
        - fg resumes specified job in foreground and waits for its completion
        - bg sends SIGCONT to suspended job allowing it to continue in background
        - Signal forwarding delivers SIGINT and SIGTSTP to correct foreground process group
        pitfalls:
        - Terminal control group confusion
        - Orphaned process groups
        - Signal handling race conditions
        concepts:
        - Process groups
        - Terminal control
        - Signal handling (SIGTSTP, SIGCONT)
        skills:
        - Terminal session management
        - Process group control
        - Signal-driven state transitions
        - Foreground and background process switching
        deliverables:
        - SIGTSTP handling suspending foreground process on Ctrl+Z input
        - fg command resuming suspended or background job in foreground
        - bg command resuming suspended job to continue running in background
        - Signal forwarding delivering terminal signals to foreground process group
        estimated_hours: 5-8
    - id: build-allocator
      name: Build Your Own Memory Allocator
      description: malloc/free implementation
      difficulty: expert
      estimated_hours: 40-60
      essence: Heap memory expansion through sbrk/mmap system calls, free block coalescing with boundary tags, and size-class segregation to minimize fragmentation while maintaining O(1) allocation under concurrent access.
      why_important: Building a memory allocator exposes you to fundamental OS-level concepts that underpin all high-level programming, teaching you how systems balance performance and memory utilization trade-offs that affect every production application.
      learning_outcomes:
      - Implement heap expansion using sbrk and mmap system calls
      - Design and maintain free block data structures using boundary tags
      - Build segregated free lists with size-class bucketing for efficient allocation
      - Debug memory corruption issues including use-after-free and double-free errors
      - Optimize allocator throughput and utilization through splitting and coalescing strategies
      - Implement thread-safe memory management using locks or lock-free techniques
      - Measure and analyze allocator performance metrics like fragmentation and throughput
      - Handle edge cases including alignment requirements and large allocation requests
      skills:
      - Systems Programming
      - Memory Management
      - Data Structure Design
      - Concurrency Control
      - Performance Optimization
      - Low-Level Debugging
      - Heap Fragmentation
      - System Calls
      tags:
      - build-from-scratch
      - c
      - expert
      - fragmentation
      - free-list
      - malloc
      - memory-management
      - rust
      - systems
      - zig
      architecture_doc: architecture-docs/build-allocator/index.md
      languages:
        recommended:
        - C
        - Rust
        - Zig
        also_possible: []
      resources:
      - type: article
        name: Implementing malloc
        url: https://moss.cs.iit.edu/cs351/slides/slides-malloc.pdf
      - type: book
        name: The C Programming Language
        url: https://en.wikipedia.org/wiki/The_C_Programming_Language
      - type: article
        name: Writing a Memory Allocator
        url: http://dmitrysoshnikov.com/compilers/writing-a-memory-allocator/
      prerequisites:
      - type: skill
        name: C programming
      - type: skill
        name: Pointers and memory
      - type: skill
        name: Data structures
      - type: skill
        name: System calls (sbrk, mmap)
      milestones:
      - id: build-allocator-m1
        name: Basic Allocator with sbrk
        description: Implement simple bump allocator using sbrk.
        acceptance_criteria:
        - Allocator requests memory from the OS via sbrk and returns a usable pointer to the caller
        - Block headers track allocation size and status so free can identify block boundaries
        - Basic malloc returns correctly aligned memory (at least 8-byte aligned on 64-bit systems)
        - Free marks a block as available and subsequent malloc calls can reuse that freed memory
        pitfalls:
        - Forgetting alignment
        - Memory leaks in metadata
        - Not handling sbrk failure
        concepts:
        - Heap management
        - Memory alignment
        - System calls
        skills:
        - Low-level memory manipulation
        - System call interfaces
        - Pointer arithmetic and alignment
        - Memory layout understanding
        deliverables:
        - sbrk() system call wrapper that extends the heap by the requested number of bytes
        - Block header structure containing the allocation size, in-use flag, and alignment padding
        - First-fit allocation strategy that scans the free list and returns the first sufficiently large block
        - Simple free() implementation that marks a block as unused and returns it to the free list
        estimated_hours: 8-12
      - id: build-allocator-m2
        name: Free List Management
        description: Implement efficient free block tracking.
        acceptance_criteria:
        - Explicit free list links free blocks together for O(free-blocks) allocation scanning
        - Block splitting divides a large free block into an allocated portion and a smaller free remainder
        - Block coalescing merges physically adjacent free blocks to reduce external fragmentation
        - First-fit, best-fit, and worst-fit strategies are all implemented and selectable at compile time
        pitfalls:
        - Fragmentation from not coalescing
        - Splitting blocks too small
        - Corrupting free list pointers
        concepts:
        - Free lists
        - Fragmentation
        - Allocation strategies
        skills:
        - Linked list implementation in unsafe contexts
        - Memory coalescing algorithms
        - Block splitting strategies
        - Fragmentation analysis
        deliverables:
        - Explicit free list with next and previous pointers embedded in free block payloads
        - Coalescing logic that merges adjacent free blocks into a single larger free block
        - Best-fit and next-fit allocation strategies selectable as alternatives to first-fit
        - Boundary tags at both ends of each block enabling efficient bidirectional coalescing
        estimated_hours: 10-15
      - id: build-allocator-m3
        name: Segregated Free Lists
        description: Implement size-class based allocation for better performance.
        acceptance_criteria:
        - Multiple segregated free lists are maintained, one per size class, for fast allocation lookup
        - Common allocation sizes (16, 32, 64, 128 bytes) are served from their dedicated free list in O(1)
        - External fragmentation is reduced compared to a single free list by grouping similar-sized blocks
        - Small allocations (under 64 bytes) are served efficiently without scanning large-block free lists
        pitfalls:
        - Internal fragmentation from size classes
        - Complex bookkeeping
        - Cache unfriendly access patterns
        concepts:
        - Size classes
        - Segregated storage
        - Performance optimization
        skills:
        - Cache-aware data structure design
        - Performance profiling and optimization
        - Bitmap or array-based indexing
        - Size class bucketing strategies
        deliverables:
        - Size class definitions with power-of-two boundaries (e.g., 16, 32, 64, 128, 256 bytes)
        - Separate free list maintained for each size class for constant-time lookup of fitting blocks
        - Fast allocation from the matching size class that returns a block without scanning other lists
        - Block splitting when a larger class block is used to satisfy a smaller class request
        estimated_hours: 10-15
      - id: build-allocator-m4
        name: Thread Safety & mmap
        description: Add thread safety and large allocation support.
        acceptance_criteria:
        - Thread-safe allocation and free operations work correctly under concurrent access from multiple threads
        - Large allocations above a configurable threshold (e.g., 128KB) use mmap instead of sbrk
        - Per-thread caches reduce cross-thread contention and improve allocation throughput in multithreaded programs
        - Memory debugging support detects double-free, use-after-free, and buffer overflow when enabled
        pitfalls:
        - Lock contention
        - Deadlocks
        - Memory leaks from lost blocks
        concepts:
        - Thread safety
        - Virtual memory
        - Memory debugging
        skills:
        - Mutex and lock implementation
        - Virtual memory system calls
        - Memory leak detection techniques
        - Concurrent data structure design
        - Thread-local storage patterns
        deliverables:
        - Per-thread arenas that give each thread its own heap region to reduce lock contention
        - Lock-free fast path for small allocations using thread-local free lists without mutex acquisition
        - mmap() fallback for large allocations that maps memory directly from the OS instead of using sbrk
        - Thread-local caching that buffers recently freed blocks for fast same-thread reallocation
        estimated_hours: 12-18
    - id: build-os
      name: Build Your Own OS
      description: Operating system kernel
      difficulty: expert
      estimated_hours: 100-200
      essence: Bootstrap code execution from firmware to protected/long mode, hardware interrupt vectoring through descriptor tables, and hierarchical address translation via multi-level page tables for process isolation.
      why_important: Building an OS kernel demystifies the abstraction layer between hardware and applications, teaching you the foundational systems concepts that underlie every modern computing platform.
      learning_outcomes:
      - Implement bootloader code that transitions from BIOS/UEFI firmware to protected/long mode
      - Design interrupt descriptor tables and interrupt service routines for hardware events
      - Build physical memory allocators using bitmap or buddy allocation strategies
      - Implement virtual memory with page table hierarchies and address translation
      - Create process control blocks and context switching mechanisms
      - Debug low-level code using QEMU/Bochs and serial port logging
      - Write freestanding code without standard library dependencies
      - Handle CPU exceptions and implement fault recovery mechanisms
      skills:
      - x86/x64 Assembly
      - Hardware Interrupts
      - Memory Management
      - Process Scheduling
      - Bootloader Development
      - Page Table Management
      - Kernel Programming
      - Low-Level Debugging
      tags:
      - bootloader
      - build-from-scratch
      - c
      - expert
      - interrupts
      - kernel
      - rust
      - scheduling
      - systems
      - zig
      architecture_doc: architecture-docs/build-os/index.md
      languages:
        recommended:
        - C
        - Rust
        - Zig
        also_possible: []
      resources:
      - type: book
        name: 'Operating Systems: Three Easy Pieces'
        url: https://pages.cs.wisc.edu/~remzi/OSTEP/
      - type: tutorial
        name: Writing an OS in Rust
        url: https://os.phil-opp.com/
      - type: reference
        name: OSDev Wiki
        url: https://wiki.osdev.org/
      prerequisites:
      - type: skill
        name: Assembly language
      - type: skill
        name: C programming
      - type: skill
        name: Computer architecture
      - type: skill
        name: Memory management
      milestones:
      - id: build-os-m1
        name: Bootloader & Kernel Entry
        description: Boot from BIOS/UEFI and enter kernel code.
        acceptance_criteria:
        - Bootloader loads kernel binary from disk into memory at correct address
        - GDT configures code and data segments for 32-bit protected mode operation
        - Kernel entry point zeroes BSS section and transfers control to main
        - VGA driver prints text characters with color attributes to screen
        pitfalls:
        - Wrong memory addresses
        - Forgetting to disable interrupts
        - GDT misconfiguration
        concepts:
        - Boot process
        - CPU modes
        - Memory layout
        skills:
        - Low-level assembly programming
        - BIOS/UEFI boot protocol implementation
        - x86 segmentation and protected mode
        - Bare metal C development
        - Linker script configuration
        deliverables:
        - Bootloader loading kernel from disk into protected mode
        - GDT setup transitioning processor to 32-bit protected mode
        - Kernel entry point initializing BSS and calling main function
        - VGA text mode driver printing characters to screen buffer
        estimated_hours: 15-25
      - id: build-os-m2
        name: Interrupts & Keyboard
        description: Handle hardware interrupts and keyboard input.
        acceptance_criteria:
        - IDT contains valid entries for at least CPU exception vectors 0-31
        - Interrupt handlers save and restore registers before returning with iret
        - PIC remapping moves IRQs to non-conflicting interrupt vector range
        - Keyboard driver converts PS/2 scancodes to printable ASCII characters
        pitfalls:
        - Not sending EOI
        - Wrong PIC remapping
        - Stack corruption in handlers
        concepts:
        - Interrupts
        - Hardware I/O
        - Device drivers
        skills:
        - Interrupt descriptor table setup
        - Hardware I/O port communication
        - PIC controller programming
        - Interrupt service routine development
        - Ring 0 privilege handling
        deliverables:
        - IDT setup registering interrupt handler entries for all vectors
        - Interrupt handlers for CPU exceptions and hardware IRQs
        - PIC or APIC configuration mapping hardware interrupts to vectors
        - Keyboard driver reading scancodes and converting to ASCII characters
        estimated_hours: 15-25
      - id: build-os-m3
        name: Memory Management
        description: Implement physical and virtual memory management.
        acceptance_criteria:
        - Physical frame allocator allocates and frees 4KB page frames without leaking
        - Page tables correctly map virtual addresses to physical frames with proper flags
        - Kernel heap allocator handles malloc and free without fragmentation issues
        - Memory-mapped I/O regions are marked as uncacheable in page table entries
        pitfalls:
        - TLB not flushed
        - Wrong page table structure
        - Kernel unmapped after enabling paging
        concepts:
        - Virtual memory
        - Paging
        - Memory protection
        skills:
        - Page table structure implementation
        - Physical memory allocator design
        - Virtual address translation
        - TLB management
        - Memory-mapped I/O handling
        deliverables:
        - Physical frame allocator tracking free and used memory pages
        - Page table setup enabling virtual-to-physical address translation
        - Kernel heap allocator providing dynamic memory allocation
        - Memory-mapped I/O support for hardware device registers
        estimated_hours: 25-40
      - id: build-os-m4
        name: Process Management
        description: Implement processes and basic scheduling.
        acceptance_criteria:
        - Process control block stores PID, register state, page tables, and status
        - Context switch correctly saves current and restores next process register state
        - Scheduler runs multiple processes in round-robin order with time slicing
        - System calls transition from user mode to kernel mode via interrupt mechanism
        pitfalls:
        - Stack corruption during switch
        - Not saving all registers
        - Deadlocks in scheduling
        concepts:
        - Context switching
        - Scheduling algorithms
        - Process isolation
        skills:
        - Task state segment configuration
        - Context switch assembly code
        - Cooperative and preemptive scheduling
        - Process control block design
        - Timer interrupt coordination
        deliverables:
        - Process control block storing process state and metadata
        - Context switching saving and restoring register state between processes
        - Basic round-robin scheduler selecting next process to run
        - System call interface dispatching user-mode requests to kernel functions
        estimated_hours: 30-50
    - id: build-tcp-stack
      name: Build Your Own TCP/IP Stack
      description: Network stack implementation
      difficulty: expert
      estimated_hours: 60-100
      essence: Byte-level protocol parsing with correct endianness handling, state machine management for connection lifecycle, and implementing reliable ordered delivery over unreliable packet networks through sequence numbers, acknowledgments, retransmission timers, and sliding window flow control.
      why_important: Building a TCP/IP stack from scratch teaches fundamental network programming concepts that underpin all modern distributed systems, from databases to microservices, and provides deep insight into how operating systems manage network communication at the kernel level.
      learning_outcomes:
      - Implement Ethernet frame parsing and ARP protocol for hardware address resolution
      - Design and implement IP packet routing with ICMP echo request/reply (ping)
      - Build TCP connection establishment using 3-way handshake and state machine transitions
      - Implement sliding window flow control with sequence numbers and acknowledgments
      - Handle packet retransmission with timeout-based reliability mechanisms
      - Debug low-level network protocols using packet capture and analysis tools
      - Implement binary protocol parsing with correct byte order handling (network vs host endianness)
      - Manage concurrent connection state across multiple TCP sessions
      skills:
      - Network Protocol Implementation
      - Binary Protocol Parsing
      - State Machine Design
      - Socket Programming
      - Flow Control Algorithms
      - Packet Analysis
      - Low-Level Systems Programming
      - Concurrency Management
      tags:
      - build-from-scratch
      - c
      - congestion
      - data-structures
      - expert
      - flow-control
      - go
      - ip
      - networking
      - packets
      - rust
      architecture_doc: architecture-docs/build-tcp-stack/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible: []
      resources:
      - type: book
        name: TCP/IP Illustrated Vol 1
        url: https://www.amazon.com/TCP-Illustrated-Vol-Addison-Wesley-Professional/dp/0201633469
      - type: specification
        name: RFC 793 - TCP
        url: https://tools.ietf.org/html/rfc793
      - type: tutorial
        name: Let's code a TCP/IP stack
        url: https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/
      prerequisites:
      - type: skill
        name: Networking fundamentals
      - type: skill
        name: C programming
      - type: skill
        name: Packet analysis
      - type: skill
        name: State machines
      milestones:
      - id: build-tcp-stack-m1
        name: Ethernet & ARP
        description: Handle Ethernet frames and ARP protocol.
        acceptance_criteria:
        - Raw socket or TAP device receives and sends Ethernet frames on network interface
        - Ethernet parser correctly extracts 6-byte source and destination MAC addresses
        - ARP reply is sent with correct MAC address when ARP request matches our IP
        - MAC address table caches ARP entries and evicts them after configurable timeout
        pitfalls:
        - Byte order (network vs host)
        - ARP cache poisoning
        - Broadcast handling
        concepts:
        - Layer 2 networking
        - Address resolution
        - Frame parsing
        skills:
        - Raw packet manipulation
        - Binary protocol implementation
        - Network hardware interfacing
        - Low-level memory management
        deliverables:
        - Raw socket or TAP device setup for capturing network frames
        - Ethernet frame parser extracting source MAC, destination MAC, and EtherType
        - ARP request and reply handler resolving IP addresses to MAC addresses
        - MAC address table caching resolved ARP entries with timeout expiration
        estimated_hours: 10-15
      - id: build-tcp-stack-m2
        name: IP & ICMP
        description: Implement IP routing and ICMP (ping).
        acceptance_criteria:
        - IP parser extracts source address, destination address, protocol, and payload correctly
        - IP checksum is computed correctly and verified on received packets
        - ICMP echo reply is sent in response to incoming echo request (ping responds)
        - Routing table selects correct next-hop gateway for destination IP address
        pitfalls:
        - Checksum calculation errors
        - Fragmentation handling
        - TTL not decremented
        concepts:
        - Layer 3 networking
        - Routing
        - Error handling
        skills:
        - IP packet routing and forwarding
        - Network layer error handling
        - Checksum computation and verification
        - Packet fragmentation and reassembly
        deliverables:
        - IPv4 packet parser extracting header fields and payload data
        - IPv4 packet builder constructing valid packets with checksum calculation
        - ICMP echo request and reply implementing ping functionality
        - IP routing table lookup selecting next hop for destination addresses
        estimated_hours: 12-18
      - id: build-tcp-stack-m3
        name: TCP Connection Management
        description: Implement TCP 3-way handshake and state machine.
        acceptance_criteria:
        - TCP parser extracts source port, destination port, sequence number, and flags
        - Three-way handshake completes successfully establishing bidirectional connection
        - State machine correctly transitions through LISTEN, SYN_SENT, ESTABLISHED, and other states
        - Four-way FIN handshake gracefully closes connection from either endpoint
        pitfalls:
        - Wrong state transitions
        - Sequence number wraparound
        - Not handling RST
        concepts:
        - State machines
        - Connection management
        - Reliability
        skills:
        - Finite state machine implementation
        - Connection lifecycle management
        - Sequence number tracking
        - Concurrent connection handling
        deliverables:
        - TCP segment parser extracting header fields, flags, and payload
        - Three-way handshake implementing SYN, SYN-ACK, ACK connection setup
        - Connection state machine tracking TCP states from LISTEN through CLOSED
        - Connection teardown implementing FIN handshake for graceful close
        estimated_hours: 18-25
      - id: build-tcp-stack-m4
        name: TCP Data Transfer & Flow Control
        description: Implement reliable data transfer with flow control.
        acceptance_criteria:
        - Sliding window allows sending multiple segments before requiring acknowledgment
        - Unacknowledged segments are retransmitted after retransmission timeout expires
        - Sender limits transmission to receiver advertised window size
        - Slow start exponentially increases congestion window until threshold is reached
        pitfalls:
        - Timer management bugs
        - Window calculation errors
        - Sequence wraparound
        concepts:
        - Flow control
        - Congestion control
        - Reliable delivery
        skills:
        - Sliding window protocol implementation
        - Retransmission timer management
        - Congestion window calculation
        - Out-of-order packet handling
        deliverables:
        - Sliding window mechanism controlling outstanding unacknowledged data bytes
        - Retransmission timer resending segments after acknowledgment timeout expires
        - Flow control using receiver window size to prevent buffer overflow
        - Congestion control implementing slow start and congestion avoidance phases
        estimated_hours: 20-30
    - id: io-uring-server
      name: io_uring High-Performance Server
      description: Submission/completion queue, zero-copy I/O, batched syscalls
      difficulty: expert
      estimated_hours: 35-50
      essence: Asynchronous I/O through shared ring buffers between kernel and userspace, eliminating syscall overhead by batching submission/completion queue operations and enabling zero-copy data transfer for high-throughput workloads.
      why_important: Building this teaches you modern Linux I/O architecture that outperforms traditional epoll/select mechanisms, a critical skill for high-performance systems like databases, web servers, and real-time applications where I/O latency directly impacts throughput.
      learning_outcomes:
      - Implement submission and completion queue management with memory-mapped ring buffers
      - Design zero-copy I/O pipelines using fixed buffers and buffer registration
      - Build batched syscall interfaces that minimize kernel transitions
      - Debug asynchronous I/O race conditions and completion ordering issues
      - Optimize network servers using linked operations and advanced SQ polling
      - Benchmark I/O subsystems and analyze performance differences between io_uring and epoll
      - Implement proper resource cleanup and error handling for kernel ring buffers
      skills:
      - Asynchronous I/O Programming
      - Ring Buffer Management
      - Zero-copy Data Transfer
      - Linux Kernel Interfaces
      - High-performance Networking
      - Systems Benchmarking
      - Memory-mapped I/O
      tags:
      - io-uring
      - async-io
      - high-performance
      - linux
      - networking
      architecture_doc: architecture-docs/io-uring-server/index.md
      languages:
        recommended:
        - C
        - Rust
        - Zig
        also_possible: []
      resources:
      - name: Lord of the io_uring
        url: https://unixism.net/loti/
        type: tutorial
      - name: io_uring Kernel Documentation
        url: https://kernel.dk/io_uring.pdf
        type: reference
      prerequisites:
      - type: project
        name: build-event-loop
      milestones:
      - id: io-uring-server-m1
        name: Basic SQ/CQ Operations
        description: Set up io_uring and perform basic submission/completion queue operations.
        acceptance_criteria:
        - Initialize io_uring instance with io_uring_setup and mmap the shared rings
        - Submit read/write operations through the submission queue (SQ)
        - Harvest completions from the completion queue (CQ) and process results
        - Demonstrate batched submission of multiple operations in a single syscall
        pitfalls:
        - SQ and CQ have separate sizes, CQ is typically 2x SQ size
        - Must maintain proper memory barriers between ring head/tail updates
        - io_uring_enter with IORING_ENTER_GETEVENTS blocks until completions available
        concepts:
        - io_uring architecture
        - Submission/completion queues
        - Batched syscalls
        skills:
        - Ring buffer management
        - System call batching
        - Memory barrier synchronization
        - Lock-free queue operations
        deliverables:
        - io_uring setup with mmap'd shared memory rings
        - SQE preparation and submission for read/write operations
        - CQE harvesting and result processing loop
        - Batched submission demonstrating multiple ops per io_uring_enter call
        estimated_hours: 8-12
      - id: io-uring-server-m2
        name: File I/O Server
        description: Build a file server using io_uring for all I/O operations.
        acceptance_criteria:
        - Serve file read requests using io_uring IORING_OP_READ instead of pread
        - Handle multiple concurrent file reads with proper buffer management
        - Implement fixed-buffer registration with io_uring_register for reduced overhead
        - Benchmark showing throughput improvement over synchronous read for parallel requests
        pitfalls:
        - Fixed buffers must be registered before use and unregistered on cleanup
        - Buffer lifetime must extend until CQE is harvested (async operation in flight)
        - IORING_OP_READ requires file descriptor and offset, not just buffer
        concepts:
        - Async file I/O
        - Buffer management
        - Fixed buffer registration
        skills:
        - Asynchronous file I/O
        - Buffer pool management
        - Fixed buffer registration and lifecycle
        - Direct I/O optimization
        deliverables:
        - File server using IORING_OP_READ for asynchronous file access
        - Buffer pool with proper lifetime management for in-flight operations
        - Fixed buffer registration reducing per-operation overhead
        - Throughput benchmark comparing io_uring vs synchronous file I/O
        estimated_hours: 8-12
      - id: io-uring-server-m3
        name: Network Server
        description: Build a TCP server using io_uring for accept, read, and write operations.
        acceptance_criteria:
        - Use IORING_OP_ACCEPT for async connection acceptance
        - Chain accept ‚Üí read ‚Üí write operations using io_uring multishot where applicable
        - Handle connection lifecycle entirely through io_uring without epoll
        - Support thousands of concurrent connections with stable performance
        pitfalls:
        - Multishot accept (IORING_ACCEPT_MULTISHOT) reuses single SQE for multiple accepts
        - Must re-submit read SQE after processing each completed read
        - Connection cleanup must cancel all in-flight operations for that socket
        concepts:
        - Async networking
        - Multishot operations
        - Connection management
        skills:
        - Asynchronous TCP server implementation
        - Multishot operation handling
        - Connection state tracking
        - Socket lifecycle management
        - Non-blocking network I/O
        deliverables:
        - IORING_OP_ACCEPT for async TCP connection handling
        - Full connection lifecycle (accept/read/write/close) via io_uring
        - Multishot operations for accept and receive where supported
        - Scalability test with thousands of concurrent connections
        estimated_hours: 8-12
      - id: io-uring-server-m4
        name: Zero-copy, Linked Ops, and Benchmarks
        description: Implement advanced io_uring features and benchmark against epoll.
        acceptance_criteria:
        - Implement IORING_OP_SEND_ZC for zero-copy network sends
        - Use linked SQEs to create operation chains (read ‚Üí transform ‚Üí write)
        - Implement IOSQE_IO_DRAIN for ordering guarantees when needed
        - Comprehensive benchmark comparing io_uring vs epoll under various workloads
        pitfalls:
        - Zero-copy send requires buffer to remain valid until notification callback
        - Linked SQEs fail as a chain if any operation fails (IOSQE_IO_LINK)
        - IO_DRAIN forces serialization, use sparingly to avoid defeating async benefits
        concepts:
        - Zero-copy I/O
        - Linked operations
        - Performance benchmarking
        skills:
        - Zero-copy buffer handling
        - Operation chaining with links
        - Performance profiling and benchmarking
        - Comparing async I/O models
        - Resource cleanup with notifications
        deliverables:
        - Zero-copy send implementation with proper buffer lifecycle management
        - Linked SQE chains for compound operations
        - Ordering control with IO_DRAIN for operations requiring serialization
        - Benchmark suite comparing io_uring vs epoll across file I/O, network, and mixed workloads
        estimated_hours: 8-12
    - id: high-cardinality-metrics
      name: Nova Metrics Engine
      description: A high-performance time-series database optimized for high-cardinality label data.
      difficulty: expert
      estimated_hours: 50-70
      essence: Time-series compression and inverted indexing targeting high-cardinality label sets, using delta-of-delta encoding and bit-packing to achieve sub-millisecond aggregations over millions of data points.
      why_important: Modern monitoring systems (like Prometheus or VictoriaMetrics) struggle with high cardinality. Building this teaches you advanced compression algorithms and data structure optimization for massive-scale time-series data.
      learning_outcomes:
      - Implement Delta-of-Delta compression for timestamp storage
      - Design XOR-based floating point compression (Gorilla encoding)
      - Build an inverted index for ultra-fast label-based searching
      - Implement a Write-Ahead Log (WAL) for durable ingestion at high volume
      - Design a query engine for real-time aggregations (Sum, Avg, Percentiles)
      - Implement data downsampling and retention policies
      - Optimize memory layout using mmap and specialized buffer pools
      - Handle ingestion backpressure and concurrent query processing
      skills:
      - Systems Programming
      - Database Internals
      - Compression Algorithms
      - Go/Rust
      - Performance Tuning
      tags:
      - databases
      - metrics
      - high-performance
      - time-series
      languages:
        recommended:
        - Go
        - Rust
        - C++
        also_possible: []
      milestones:
      - id: high-cardinality-metrics-m1
        name: TSDB Storage Layout
        description: Design the on-disk format and implementing base compression.
        acceptance_criteria:
        - Implement Delta-of-Delta encoding for sequence of timestamps
        - Implement XOR compression for floating point value streams
        - Achieve at least 10x compression ratio compared to raw 64-bit storage
        - Successful serialization and deserialization of data blocks
        pitfalls:
        - Rounding errors in XOR compression
        - Inefficient bit-packing logic
        - Not handling out-of-order samples
        concepts:
        - Delta-of-Delta
        - Gorilla Compression
        - Bit-packing
        estimated_hours: 12
      - id: high-cardinality-metrics-m2
        name: Inverted Index for Labels
        description: Build the indexing layer to handle high-cardinality metadata.
        acceptance_criteria:
        - Create a mapping from label key-value pairs to series IDs
        - Implement a posting list structure for efficient boolean query evaluation
        - Support sub-millisecond lookup for specific label combinations
        - Handle millions of unique series IDs without excessive memory usage
        pitfalls:
        - Index size explosion
        - Slow index updates during ingestion
        - Lock contention on the index
        concepts:
        - Inverted Index
        - Posting Lists
        - Cardinality Management
        estimated_hours: 15
      - id: high-cardinality-metrics-m3
        name: Real-time Query Engine
        description: Implement the aggregation logic over time-series blocks.
        acceptance_criteria:
        - Support range queries with Min, Max, and Average aggregations
        - Implement streaming aggregation to minimize memory allocation
        - Query results match expected values within 0.001 precision
        - Successful handling of concurrent queries during heavy ingestion
        pitfalls:
        - Allocating too much memory per query
        - Incorrect floating point summation (loss of precision)
        - Blocking ingestion during query
        concepts:
        - Aggregation Pipelines
        - Stream Processing
        - Vectorized Execution
        estimated_hours: 18
- id: data-storage
  name: Data & Storage
  icon: üóÑÔ∏è
  subdomains:
  - name: Databases
  - name: Data Engineering
  projects:
    beginner: []
    intermediate:
    - id: btree-impl
      name: B-tree Implementation
      description: Insert, delete, rebalance
      difficulty: intermediate
      estimated_hours: 15-25
      essence: Multi-way search tree with variable-key nodes and minimum degree constraints, maintaining balance through algorithmic splitting during insertion and key redistribution or merging during deletion to guarantee logarithmic height and minimize disk I/O operations.
      why_important: B-trees are the foundation of database indexes and file systems (used in PostgreSQL, MySQL, SQLite, NTFS, APFS), teaching you how to design data structures that bridge the performance gap between fast RAM and slow disk storage.
      learning_outcomes:
      - Implement multi-way tree nodes with variable key counts and child pointers
      - Design disk-friendly data structures that minimize I/O operations
      - Build search algorithms that traverse multi-level tree structures
      - Implement node splitting logic to maintain B-tree balance during insertions
      - Handle complex deletion cases with key borrowing from siblings
      - Implement node merging when underflow conditions occur
      - Debug tree invariants and validate B-tree properties after operations
      - Optimize memory layout for cache efficiency and disk block alignment
      skills:
      - Self-balancing Trees
      - Disk I/O Optimization
      - Cache-aware Algorithms
      - Tree Invariants
      - Memory Layout Design
      - Algorithm Complexity Analysis
      tags:
      - balanced-tree
      - c
      - data-structures
      - databases
      - disk-based
      - implementation
      - intermediate
      - python
      - range-queries
      - rust
      architecture_doc: architecture-docs/btree-impl/index.md
      languages:
        recommended:
        - C
        - Python
        - Rust
        also_possible:
        - Go
        - Java
      resources:
      - name: B-tree Wikipedia
        url: https://en.wikipedia.org/wiki/B-tree
        type: article
      - name: B-tree Visualization
        url: https://www.cs.usfca.edu/~galles/visualization/BTree.html
        type: tool
      prerequisites:
      - type: skill
        name: Trees basics
      - type: skill
        name: Algorithm complexity
      milestones:
      - id: btree-impl-m1
        name: B-tree Node Structure
        description: Define B-tree node with keys and children.
        acceptance_criteria:
        - Each node holds at most 2t-1 keys and at least t-1 keys (except the root which may have fewer)
        - Each internal node has exactly one more child pointer than it has keys (up to 2t children)
        - Keys within every node are maintained in sorted ascending order after every operation
        - Leaf indicator flag is set correctly and differentiates leaf nodes from internal nodes during traversal
        pitfalls:
        - Off-by-one in capacity
        - Not tracking leaf status
        - Children count mismatch
        concepts:
        - B-tree properties
        - Node capacity
        - Tree structure
        skills:
        - Struct design and memory layout
        - Array-based data structure implementation
        - Pointer management for tree nodes
        - Conditional logic for node types
        deliverables:
        - Node class with sorted keys array and children pointer array for internal and leaf nodes
        - Configurable minimum degree t that determines the maximum and minimum key capacity per node
        - Leaf versus internal node distinction flag used to select insertion and traversal behavior
        - Disk page representation mapping each node to a fixed-size page for persistent storage
        estimated_hours: 2-3
      - id: btree-impl-m2
        name: Search
        description: Implement search operation.
        acceptance_criteria:
        - Binary search within a node identifies the correct key position or child pointer in O(log t) time
        - Recursive traversal descends to the appropriate child node when the key is not found at the current level
        - Search returns the matching key and its associated value when the key exists in the tree
        - Search returns a not-found indicator without error when the key does not exist in the tree
        pitfalls:
        - Index bounds
        - Leaf check
        - Key comparison
        concepts:
        - Binary search
        - Tree traversal
        - Key lookup
        skills:
        - Binary search within sorted arrays
        - Recursive tree traversal
        - Pointer dereferencing and navigation
        - Conditional branching based on comparisons
        deliverables:
        - Binary search within a node's key array to locate the target key or correct child index
        - Recursive descent from the root through internal nodes to the correct child for the search key
        - Search result returning the found value and a boolean indicating whether the key was present
        - O(log n) time complexity with at most O(log_t n) node accesses for a tree of n keys
        estimated_hours: 2-3
      - id: btree-impl-m3
        name: Insert with Split
        description: Implement insertion with node splitting.
        acceptance_criteria:
        - Insertion into a non-full leaf places the key in sorted position without any structural changes
        - Full nodes are split proactively during the downward traversal, before the actual insertion
        - Splitting the root creates a new root with one key and two children, increasing tree height by one
        - Sorted order invariant is maintained across all nodes after every insertion operation
        pitfalls:
        - Split at wrong position
        - Not updating root
        - Child index after split
        concepts:
        - Node splitting
        - Proactive split
        - Tree growth
        skills:
        - Dynamic memory allocation for new nodes
        - Array element shifting and copying
        - Parent-child pointer updates
        - Recursive insertion with backtracking
        - Root pointer reassignment
        deliverables:
        - Leaf location finder that traverses the tree to identify the correct leaf for the new key
        - Proactive node splitting during descent that splits full nodes before inserting into them
        - Median key promotion that moves the middle key of a split node up into the parent node
        - Root split handler that creates a new root when the current root is full and needs splitting
        estimated_hours: 4-6
      - id: btree-impl-m4
        name: Delete with Rebalancing
        description: Implement deletion with borrowing and merging.
        acceptance_criteria:
        - Deleting a key from a leaf node with more than t-1 keys succeeds without restructuring
        - Deleting from an internal node replaces the key with its in-order predecessor or successor and recurses
        - Borrowing rotates a key from a sibling through the parent to restore the minimum occupancy invariant
        - Merging combines two siblings and their parent separator key into one node when borrowing is not possible
        pitfalls:
        - Merge vs borrow decision
        - Predecessor vs successor choice
        - Root shrinking
        concepts:
        - Node merging
        - Sibling borrowing
        - Tree shrinking
        skills:
        - Sibling node access and manipulation
        - Conditional rebalancing strategies
        - Key redistribution between nodes
        - Recursive deletion with propagation
        - Edge case handling for tree height reduction
        deliverables:
        - Delete from leaf node by removing the key and checking for minimum occupancy underflow
        - Delete from internal node by swapping with the in-order predecessor or successor, then deleting from leaf
        - Borrow from sibling operation that transfers a key through the parent to fix an underflowing node
        - Merge with sibling operation that combines two underflowing nodes and their shared parent key
        estimated_hours: 5-8
    - id: sql-parser
      name: SQL Parser
      description: SELECT, WHERE, JOIN
      difficulty: intermediate
      estimated_hours: 15-25
      essence: Lexical tokenization of SQL text followed by recursive descent parsing to construct Abstract Syntax Trees (ASTs) that capture query structure, operator precedence, and expression hierarchies for SELECT, INSERT, UPDATE, and DELETE statements.
      why_important: Building a SQL parser teaches you fundamental compiler construction techniques (lexing, parsing, AST manipulation) that apply across programming languages, data pipelines, and developer tooling, while demystifying how databases process queries.
      learning_outcomes:
      - Implement lexical analysis with tokenization rules for keywords, identifiers, operators, and literals
      - Design recursive descent parsers for hierarchical grammar rules
      - Build Abstract Syntax Tree (AST) data structures with proper node relationships
      - Parse complex WHERE clause expressions with operator precedence and associativity
      - Handle SQL syntax variations and ambiguities across different statement types
      - Implement error detection and recovery during parsing with helpful diagnostics
      - Traverse and validate AST structures for semantic correctness
      skills:
      - Lexical Analysis
      - Recursive Descent Parsing
      - Abstract Syntax Trees
      - Formal Grammars
      - Token Recognition
      - Expression Parsing
      - Compiler Design Fundamentals
      tags:
      - compilers
      - databases
      - go
      - intermediate
      - parsing
      - python
      - rust
      architecture_doc: architecture-docs/sql-parser/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible:
        - JavaScript
        - Java
      resources:
      - name: SQLite Grammar
        url: https://www.sqlite.org/lang.html
        type: documentation
      - name: Writing a SQL Parser
        url: https://blog.subnetzero.io/post/building-a-sql-parser/
        type: article
      prerequisites:
      - type: skill
        name: Tokenizer basics
      - type: skill
        name: Recursive descent parsing
      - type: skill
        name: Tree data structures
      milestones:
      - id: sql-parser-m1
        name: SQL Tokenizer
        description: Tokenize SQL statements into keywords, identifiers, literals.
        acceptance_criteria:
        - Recognize SQL keywords (SELECT, FROM, WHERE)
        - Handle identifiers and numbers with correct token type classification
        - Parse string literals enclosed in single or double quotes
        - Support operators (=, <, >, AND, OR)
        pitfalls:
        - Case sensitivity
        - Escape sequences in strings
        - Multi-char operators
        concepts:
        - Lexical analysis
        - Token types
        - SQL syntax
        skills:
        - String parsing and escaping
        - Token classification and categorization
        - Regular expression pattern matching
        - State machine design
        deliverables:
        - Keyword token recognition for SELECT, FROM, WHERE, and other SQL reserved words
        - Identifier token extraction for table names, column names, and aliases
        - Literal token parsing for string, integer, and floating-point values
        - Operator token handling for comparison, arithmetic, and logical operators
        estimated_hours: 3-4
      - id: sql-parser-m2
        name: SELECT Parser
        description: Parse SELECT statements into AST.
        acceptance_criteria:
        - Parse column list or star wildcard in SELECT clause correctly
        - Parse FROM clause with table name
        - Handle multiple comma-separated columns with correct AST node generation
        - Parse column and table aliases using AS keyword or implicit aliasing
        pitfalls:
        - Missing commas
        - Alias without AS
        - Reserved words as identifiers
        concepts:
        - Recursive descent
        - AST construction
        - Grammar rules
        skills:
        - Recursive parsing algorithms
        - Abstract syntax tree construction
        - Grammar rule implementation
        - Symbol table management
        deliverables:
        - SELECT clause parser supporting column lists and wildcard star expressions
        - FROM clause parser resolving table references and optional aliases
        - Column list parser handling qualified names like table.column notation
        - Table reference parser supporting aliased and multi-table FROM clauses
        estimated_hours: 4-5
      - id: sql-parser-m3
        name: WHERE Clause
        description: Parse WHERE expressions with operators and logic.
        acceptance_criteria:
        - Comparison operators (=, <, >, !=)
        - Boolean operators AND, OR, NOT bind with correct precedence and associativity
        - Parentheses override default operator precedence in expressions
        - NULL checks (IS NULL, IS NOT NULL)
        pitfalls:
        - Operator precedence
        - Unbalanced parentheses
        - NULL comparisons
        concepts:
        - Expression parsing
        - Precedence climbing
        - Boolean logic
        skills:
        - Expression tree building
        - Operator precedence handling
        - Boolean logic evaluation
        - Null-safe comparison operators
        deliverables:
        - Condition parser building expression trees from WHERE clause predicates
        - Comparison operator support for equals, not-equals, less-than, greater-than
        - Logical operator support combining conditions with AND, OR, and NOT
        - Expression tree builder producing correct AST with operator precedence
        estimated_hours: 4-5
      - id: sql-parser-m4
        name: INSERT/UPDATE/DELETE
        description: Parse modification statements.
        acceptance_criteria:
        - INSERT INTO table (cols) VALUES (vals)
        - UPDATE table SET col=val WHERE condition parses into correct AST
        - DELETE FROM table WHERE condition parses with proper predicate tree
        - Handle multiple value rows in a single INSERT statement
        pitfalls:
        - Column/value count mismatch
        - Missing WHERE in DELETE
        - Type mismatches
        concepts:
        - DML parsing
        - Statement types
        - Data modification
        skills:
        - Statement type discrimination
        - Data manipulation operations
        - Column-value mapping validation
        - Conditional update logic
        deliverables:
        - INSERT statement parser supporting column lists and value expressions
        - UPDATE statement parser handling SET clauses and WHERE conditions
        - DELETE statement parser supporting WHERE-filtered row deletion
        - Value list parser handling multiple rows and typed literal expressions
        estimated_hours: 4-5
    - id: etl-pipeline
      name: ETL Pipeline
      description: Extract, transform, load data processing
      difficulty: expert
      estimated_hours: '45'
      essence: Orchestrating complex data workflows as directed acyclic graphs with topological sorting for dependency resolution, implementing incremental extraction with change data capture and checkpointing, enforcing schema validation with evolution tracking, and building distributed monitoring systems with lineage tracking and failure recovery across heterogeneous data sources.
      why_important: Building this teaches you production-grade data engineering patterns used at scale in modern data platforms, including workflow orchestration, distributed processing, and data quality management‚Äîskills essential for backend and infrastructure engineering roles.
      learning_outcomes:
      - Implement DAG-based workflow execution with topological sorting and dependency resolution
      - Design incremental data extraction with change data capture and checkpointing mechanisms
      - Build schema validation systems with backward compatibility and evolution tracking
      - Implement data transformation pipelines with type safety and column-level lineage
      - Debug distributed pipeline failures with observability, logging, and retry strategies
      - Design monitoring systems with alerting for data quality anomalies and SLA violations
      - Implement job scheduling with cron expressions, data-driven triggers, and backfill support
      - Build connector abstractions for multiple data sources with unified extraction interfaces
      skills:
      - DAG Orchestration
      - Distributed Data Processing
      - Schema Evolution Management
      - Data Quality Validation
      - Pipeline Monitoring
      - Incremental Processing
      - Workflow Scheduling
      - Data Lineage Tracking
      tags:
      - batch
      - data-engineering
      - devops
      - etl
      - expert
      - extraction
      - loading
      - pipelines
      - scheduling
      - streaming
      - transformation
      architecture_doc: architecture-docs/etl-pipeline/index.md
      languages:
        recommended:
        - Python
        - Scala
        - Java
        also_possible: []
      resources:
      - name: Apache Airflow Documentation
        url: https://airflow.apache.org/docs/
        type: documentation
      - name: Databricks Spark ETL Tutorial
        url: https://docs.databricks.com/aws/en/getting-started/etl-quick-start
        type: tutorial
      - name: Data Validation in ETL Guide
        url: https://www.integrate.io/blog/data-validation-etl/
        type: article
      - name: Dagster Orchestration Platform
        url: https://dagster.io
        type: tool
      - name: Schema Evolution Best Practices
        url: https://dataterrain.com/handling-schema-evolution-etl-data-transformation
        type: article
      prerequisites:
      - type: project
        id: job-scheduler
      milestones:
      - id: etl-pipeline-m1
        name: Pipeline DAG Definition
        description: Implement pipeline definition with dependencies as directed acyclic graph
        acceptance_criteria:
        - Define tasks with dependencies in YAML or Python configuration format
        - Parse DAG definition from YAML or Python configuration files correctly
        - Validate no cycles exist in the DAG and report errors for invalid graphs
        - Support parameterized tasks with runtime variable substitution in configs
        pitfalls:
        - Cycle detection must run before execution starts
        - Topological sort doesn't handle failures - need separate tracking
        - Parallel execution needs thread-safe state management
        - Long chains of dependencies slow down pipeline
        concepts:
        - Directed acyclic graphs (DAGs) for task dependencies
        - Topological sorting for execution order
        - Graph cycle detection algorithms
        - Task state management and transitions
        skills:
        - DAG modeling
        - Task dependencies
        - Configuration
        deliverables:
        - Task definition with name, type, and configuration parameters
        - Dependency specification declaring upstream task requirements
        - DAG validation ensuring no cycles exist in task dependencies
        - DAG visualization rendering pipeline structure as a graph
        estimated_hours: '11'
      - id: etl-pipeline-m2
        name: Data Extraction & Loading
        description: Implement extractors and loaders for common data sources
        acceptance_criteria:
        - Extract data from databases using configurable SQL query connectors
        - Extract data from REST APIs using configurable HTTP client connectors
        - Load data to target destination with configurable batch size for efficiency
        - Handle schema mapping between source and destination column definitions
        pitfalls:
        - API pagination can change during extraction - use stable cursors
        - Watermark must be saved atomically with loaded data
        - Upsert needs proper conflict detection keys
        - Large batches can exhaust memory - stream when possible
        concepts:
        - Cursor-based pagination for stable data extraction
        - Change data capture (CDC) for incremental loads
        - Watermarking for tracking processed data
        - Stream processing vs batch processing tradeoffs
        skills:
        - Data connectors
        - Batch processing
        - Incremental loads
        deliverables:
        - Source connectors for databases, APIs, and file systems
        - Destination connectors for target storage and database systems
        - Incremental extraction tracking changed records since last run
        - Bulk loading with batched inserts for efficient data transfer
        estimated_hours: '11'
      - id: etl-pipeline-m3
        name: Data Transformations
        description: Implement transformation operations with schema validation
        acceptance_criteria:
        - Support SQL-based transformations for declarative data manipulation
        - Support Python UDF transformations for custom processing logic
        - Handle null values and type conversions without data loss or errors
        - Implement data validation rules that reject or flag invalid records
        pitfalls:
        - Type coercion can lose precision (float -> int)
        - Schema validation on every record is slow - sample or skip
        - Derived columns can fail - handle exceptions per record
        - Null handling differs between databases
        concepts:
        - Schema evolution and backward compatibility
        - Data type coercion and precision loss
        - Null value semantics across different systems
        - Lazy evaluation vs eager validation strategies
        skills:
        - Data mapping
        - Schema validation
        - Type conversion
        deliverables:
        - Transform functions for data manipulation and enrichment
        - Data cleaning operations for null handling and deduplication
        - Schema mapping for column renaming and type conversion
        - Aggregation operations for grouping and summarization
        estimated_hours: '11'
      - id: etl-pipeline-m4
        name: Pipeline Orchestration & Monitoring
        description: Implement pipeline execution with monitoring, alerting, and lineage tracking
        acceptance_criteria:
        - Schedule pipeline runs using cron expressions or event-based triggers
        - Track run status and timing for each task with per-task state reporting
        - Handle task failures with configurable alerts and automatic retry logic
        - Provide run history and log access for debugging and audit purposes
        pitfalls:
        - Retry delay should use exponential backoff
        - Parallel task execution needs careful state management
        - Lineage tracking adds overhead - make it optional
        - Cancelled pipelines need cleanup of partial data
        concepts:
        - Exponential backoff for retry strategies
        - Distributed state management and consistency
        - Data lineage and provenance tracking
        - Idempotent operations for safe retries
        skills:
        - Orchestration
        - Monitoring
        - Data lineage
        deliverables:
        - Scheduler integration for cron-based and event-driven pipeline triggers
        - Task execution tracking with status, timing, and dependency resolution
        - Failure handling with configurable retry and alerting policies
        - Monitoring dashboard displaying pipeline run history and performance
        estimated_hours: '11'
    - id: cdc-system
      name: Change Data Capture
      description: Database change streaming and replication
      difficulty: expert
      estimated_hours: '50'
      essence: Binary transaction log parsing (MySQL binlog, PostgreSQL WAL) to extract row-level change events, with exactly-once delivery semantics and distributed offset tracking to ensure ordered, idempotent replication across concurrent consumers.
      why_important: Building this teaches you low-level database internals, distributed systems patterns, and event-driven architecture‚Äîfoundational skills for data infrastructure engineering roles at companies building real-time data platforms.
      learning_outcomes:
      - Parse binary transaction logs (MySQL binlog, PostgreSQL WAL) to extract change events
      - Implement idempotent event delivery with exactly-once semantics using message deduplication
      - Design offset tracking and checkpointing for consumer failure recovery
      - Build schema registry integration to handle backward and forward compatibility
      - Handle log sequence number (LSN) tracking across database restarts and failovers
      - Implement partition-level ordering guarantees in distributed event streams
      - Debug replication lag and backpressure in streaming pipelines
      skills:
      - Transaction Log Parsing
      - Event Streaming Architecture
      - Exactly-Once Semantics
      - Schema Evolution
      - Distributed Systems
      - Idempotency Patterns
      - Offset Management
      - Binary Protocol Parsing
      tags:
      - cdc
      - change-capture
      - database
      - databases
      - debezium
      - events
      - expert
      - replication
      - streaming
      architecture_doc: architecture-docs/cdc-system/index.md
      languages:
        recommended:
        - Java
        - Go
        - Python
        also_possible: []
      resources:
      - name: Debezium Documentation
        url: https://debezium.io/documentation/reference/stable/index.html
        type: documentation
      - name: Kafka Delivery Semantics
        url: https://docs.confluent.io/kafka/design/delivery-semantics.html
        type: documentation
      - name: PostgreSQL WAL Internals
        url: https://www.postgresql.org/docs/current/wal-intro.html
        type: documentation
      - name: MySQL Binlog vs PostgreSQL WAL
        url: https://medium.com/data-science/understanding-change-data-capture-cdc-in-mysql-and-postgresql-binlog-vs-wal-logical-decoding-ac76adb0861f
        type: article
      - name: Change Data Capture Overview
        url: https://www.confluent.io/learn/change-data-capture/
        type: tutorial
      prerequisites:
      - type: project
        id: build-sqlite
      milestones:
      - id: cdc-system-m1
        name: Log Parsing & Change Events
        description: Parse database transaction logs to extract change events
        acceptance_criteria:
        - Parser connects to PostgreSQL WAL or MySQL binlog and reads transaction entries
        - INSERT, UPDATE, and DELETE operations are correctly extracted from log entries
        - Change events include before-image and after-image row values for UPDATE operations
        - Position tracking enables resuming log reading from last processed offset after restart
        pitfalls:
        - WAL parsing is database-specific - need adapter per DB
        - Trigger-based CDC adds write overhead to main tables
        - Binary log formats change between versions
        - Large transactions can create huge change events
        concepts:
        - Write-ahead log (WAL) structure and binary format
        - Change event types (INSERT, UPDATE, DELETE, DDL)
        - Transaction boundaries and commit ordering
        - Log sequence numbers (LSN) for position tracking
        - Event deduplication using primary keys
        skills:
        - WAL parsing
        - Binary protocols
        - Event modeling
        deliverables:
        - Database log connection reading WAL or binlog transaction entries
        - Log event parser extracting operation type, table, and row data from entries
        - Change event construction producing structured events with before and after values
        - Position and offset tracking recording current read position for resumption
        estimated_hours: '16.5'
      - id: cdc-system-m2
        name: Event Streaming & Delivery
        description: Stream change events to consumers with ordering and delivery guarantees
        acceptance_criteria:
        - Change events are published to Kafka topic partitioned by table and primary key
        - Consumer receives every change event at least once even after transient failures
        - Events for the same primary key are delivered in the same order they occurred
        - Consumer lag is monitored and alerts trigger when backlog exceeds configured threshold
        pitfalls:
        - Consumer rebalancing causes brief processing pause
        - Partition by pk, not just table, for better parallelism
        - Offset commit after processing prevents duplicates
        - Consumer heartbeat failure needs automatic reassignment
        concepts:
        - Message broker partitioning strategies
        - At-least-once vs exactly-once delivery semantics
        - Consumer offset management and checkpointing
        - Back-pressure handling in streaming pipelines
        - Partition key selection for ordering guarantees
        skills:
        - Event streaming
        - Ordering guarantees
        - Consumer groups
        deliverables:
        - Kafka or message queue integration publishing change events to topic partitions
        - At-least-once delivery guaranteeing every change event reaches consumers
        - Event ordering per table maintaining causal order for same-row changes
        - Backpressure handling slowing event production when consumers fall behind
        estimated_hours: '16.5'
      - id: cdc-system-m3
        name: Schema Evolution & Compatibility
        description: Handle schema changes without breaking consumers
        acceptance_criteria:
        - Schema registry stores versioned schema for each table and returns schema by version ID
        - Column additions with nullable or default values pass backward compatibility check
        - Column removal or type narrowing fails forward compatibility validation check
        - Schema change events notify consumers to update their deserialization logic
        pitfalls:
        - DDL events need special handling - may require resync
        - Schema registry is single point of failure - replicate it
        - Backward compatibility is usually most important
        - Type narrowing (bigint->int) can cause data loss
        concepts:
        - Schema versioning and compatibility modes (forward, backward, full)
        - Avro/Protobuf schema evolution rules
        - Schema registry for centralized schema management
        - Column additions vs deletions compatibility impact
        - Type coercion rules during schema migration
        skills:
        - Schema evolution
        - Compatibility checks
        - Migration
        deliverables:
        - Schema registry integration storing and versioning table schema definitions
        - Schema versioning tracking column additions, removals, and type changes over time
        - Compatibility checking validating schema changes against backward compatibility rules
        - Schema migration event generation notifying consumers of schema version changes
        estimated_hours: '16.5'
    - id: data-quality-framework
      name: Data Quality Framework
      description: Schema validation, anomaly detection, data profiling
      difficulty: intermediate
      estimated_hours: '45'
      essence: Rule-based expectation engines that validate data against declarative contracts, combined with statistical profiling methods (Z-scores, IQR, distribution analysis) and schema enforcement mechanisms that reject non-conforming writes while tracking drift across pipeline stages.
      why_important: Building this develops expertise in data reliability engineering‚Äîa critical skill as organizations increasingly treat data quality as code, using validation frameworks that prevent bad data from propagating through pipelines and ML systems.
      learning_outcomes:
      - Design data validation frameworks
      - Implement statistical data profiling
      - Build rule-based quality checks
      - Create data quality dashboards
      skills:
      - Schema validation
      - Statistical profiling
      - Anomaly detection
      - Rule engine
      - Data contracts
      - Alerting
      tags:
      - anomalies
      - expectations
      - framework
      - intermediate
      - profiling
      - validation
      architecture_doc: architecture-docs/data-quality-framework/index.md
      languages:
        recommended:
        - Python
        - Scala
        - Java
        also_possible: []
      resources:
      - name: Great Expectations Official Documentation
        url: https://docs.greatexpectations.io/
        type: documentation
      - name: Data Contracts for Schema Registry
        url: https://docs.confluent.io/platform/current/schema-registry/fundamentals/data-contracts.html
        type: documentation
      - name: Data Drift Detection Guide
        url: https://www.evidentlyai.com/ml-in-production/data-drift
        type: article
      - name: Great Expectations Tutorial
        url: https://www.datacamp.com/tutorial/great-expectations-tutorial
        type: tutorial
      - name: Schema Evolution Best Practices
        url: https://dataengineeracademy.com/module/best-practices-for-managing-schema-evolution-in-data-pipelines/
        type: article
      prerequisites:
      - type: skill
        name: SQL
      - type: skill
        name: Statistics basics
      - type: skill
        name: Python
      milestones:
      - id: data-quality-framework-m1
        name: Expectation Engine
        description: Define and run data quality expectations
        acceptance_criteria:
        - Define data quality expectations such as not_null, unique, and range checks
        - Evaluate expectations against datasets and report pass/fail per expectation
        - Support custom expectation definitions via user-provided validation functions
        - Return detailed validation results with row counts, failure counts, and percentages
        pitfalls:
        - Not handling null values consistently in expectations
        - Forgetting to collect context when expectations fail
        - Writing expectations that are too strict for real data
        - Not considering performance impact of complex validation chains
        - Tightly coupling expectations to specific data formats
        concepts:
        - Declarative validation rules and DSL design
        - Expectation result metadata and structured failures
        - Batch vs streaming validation execution models
        - Composable expectation chaining and logical operators
        skills:
        - Rule definition
        - Validation logic
        - Result collection
        deliverables:
        - Expectation base class with validation and serialization interface
        - Column expectations including not-null, unique, and type checks
        - Value expectations including range, regex, and allowed-values checks
        - Result objects with pass/fail status and detailed metrics
        - Expectation suite grouping related expectations together
        - JSON serialization for expectation definitions and results
        estimated_hours: '11'
      - id: data-quality-framework-m2
        name: Data Profiling
        description: Automatic statistical profiling of datasets
        acceptance_criteria:
        - Calculate column statistics including min, max, mean, and null counts accurately
        - Detect data types automatically from column values using heuristic inference
        - Generate distribution histograms showing value frequency for each column
        - Identify potential data quality issues such as outliers and inconsistent types
        pitfalls:
        - Computing statistics on entire dataset causing memory issues
        - Not handling highly skewed distributions properly
        - Ignoring temporal patterns in time-series data
        - Calculating percentiles without considering outlier effects
        - Profiling without sampling large datasets first
        concepts:
        - Summary statistics computation for numerical and categorical data
        - Histogram generation and binning strategies
        - Missing value patterns and data completeness metrics
        - Column correlation analysis and dependency detection
        skills:
        - Descriptive statistics
        - Distribution analysis
        - Cardinality
        deliverables:
        - Column statistics computation for mean, std, min, and max values
        - Null percentage calculation per column for data completeness assessment
        - Cardinality and uniqueness ratio analysis per column
        - Value distribution histogram generation for data understanding
        - Data type inference engine for automatic schema detection
        - Profile report generation in human-readable and machine-readable formats
        estimated_hours: '11'
      - id: data-quality-framework-m3
        name: Anomaly Detection
        description: Detect data anomalies and drift
        acceptance_criteria:
        - Detect statistical anomalies using Z-score and IQR methods on numerical data
        - Track metric trends over time and alert on significant deviations
        - Alert on sudden distribution changes that exceed configurable thresholds
        - Handle seasonality in time series data to reduce false positive alerts
        pitfalls:
        - Setting anomaly thresholds without baseline data
        - Not accounting for seasonal or cyclical patterns
        - Treating all outliers as anomalies without context
        - Using static thresholds on evolving data distributions
        - Detecting drift without sufficient historical window size
        concepts:
        - Statistical process control and control limits
        - Z-score and IQR methods for outlier detection
        - Distribution comparison techniques like KS test
        - Concept drift vs data drift differentiation
        skills:
        - Statistical tests
        - Drift detection
        - Outliers
        deliverables:
        - Z-score outlier detection for numerical columns
        - IQR method for robust outlier identification
        - Distribution drift detection using Kolmogorov-Smirnov test
        - Schema drift detection comparing current schema against baseline
        - Volume anomaly detection for unexpected row count changes
        - Freshness checks verifying data recency against expected schedule
        estimated_hours: '11'
      - id: data-quality-framework-m4
        name: Data Contracts
        description: Schema contracts and versioning
        acceptance_criteria:
        - Define schema contracts in YAML with field names, types, and constraints
        - Version contracts with semantic versioning for compatibility management
        - Validate incoming data against the active contract and report violations
        - Track contract violations over time with historical metrics and trends
        pitfalls:
        - Not validating schema changes against existing data
        - Allowing implicit schema evolution without versioning
        - Forgetting to handle optional vs required field transitions
        - Not documenting contract change rationale and impact
        - Tight coupling between contract versions and application code
        concepts:
        - Schema evolution strategies and backward compatibility
        - Semantic versioning for data contracts
        - Contract-first vs code-first design approaches
        - Breaking vs non-breaking schema changes
        skills:
        - Schema definition
        - Contract validation
        - Versioning
        deliverables:
        - Schema contract definition format in YAML with typed fields
        - Contract validation engine comparing data against contract specifications
        - Breaking change detection between contract versions automatically
        - Contract versioning with semantic versioning for compatibility tracking
        - Producer and consumer registration linking teams to their contracts
        - Contract testing framework for CI/CD pipeline integration
        estimated_hours: '11'
    - id: message-queue
      name: Message Queue
      description: Build an in-memory message queue with publish/subscribe, point-to-point delivery, and consumer groups. Learn async communication patterns fundamental to distributed systems.
      difficulty: intermediate
      estimated_hours: 25-35
      essence: Asynchronous message routing and delivery guarantees over TCP, implementing producer-consumer coordination, acknowledgment-based reliability semantics, and flow control mechanisms to handle variable throughput between message producers and consumers.
      why_important: Message queues are the backbone of async communication in modern distributed systems. Understanding how they work internally ‚Äî delivery guarantees, consumer coordination, backpressure ‚Äî is essential for designing reliable microservice architectures.
      learning_outcomes:
      - Implement publish/subscribe and point-to-point messaging patterns
      - Design consumer group coordination with partition assignment
      - Build message acknowledgment and redelivery mechanisms
      - Handle backpressure when consumers are slower than producers
      - Implement message persistence with write-ahead logging
      - Design wire protocol for producer/consumer communication
      skills:
      - Message Queue Design
      - Pub/Sub Patterns
      - Consumer Groups
      - Wire Protocols
      - Backpressure
      - Message Persistence
      tags:
      - messaging
      - pub/sub
      - distributed-systems
      - async
      architecture_doc: architecture-docs/message-queue/index.md
      languages:
        recommended:
        - Go
        - Python
        - Rust
        also_possible:
        - Java
        - JavaScript
      resources:
      - name: RabbitMQ Concepts
        url: https://www.rabbitmq.com/tutorials/amqp-concepts
        type: documentation
      - name: Building a Simple Message Broker
        url: https://www.confluent.io/blog/event-streaming-platform-1/
        type: article
      - name: Redis Pub/Sub Documentation
        url: https://redis.io/docs/latest/develop/interact/pubsub/
        type: documentation
      prerequisites:
      - type: skill
        name: TCP socket programming
      - type: skill
        name: Concurrency basics
      - type: skill
        name: Data structures (queues, hash maps)
      milestones:
      - id: message-queue-m1
        name: Core Queue & Wire Protocol
        description: Implement a TCP server with a custom binary protocol supporting PUBLISH, SUBSCRIBE, and ACK commands. Messages are stored in per-topic in-memory queues.
        acceptance_criteria:
        - TCP server accepts connections and parses binary protocol commands
        - PUBLISH command appends message to a named topic queue
        - SUBSCRIBE command registers a consumer for a topic
        - Messages are delivered to all subscribers (fan-out) in order
        pitfalls:
        - Partial TCP reads
        - Message ordering with concurrent publishers
        - Connection cleanup on disconnect
        concepts:
        - TCP server
        - binary protocol
        - message queues
        - fan-out delivery
        skills:
        - TCP networking
        - Protocol design
        - Concurrent data structures
        deliverables:
        - TCP message broker with basic pub/sub
        - Binary wire protocol specification document defining message framing and command structure
        - Topic-based message router supporting wildcard subscriptions and fanout delivery to multiple subscribers
        - Connection manager handling client lifecycle, heartbeats, and graceful shutdown
        estimated_hours: '8'
      - id: message-queue-m2
        name: Consumer Groups & Acknowledgment
        description: Implement consumer groups where messages are distributed across group members, with ACK/NACK and automatic redelivery for failed messages.
        acceptance_criteria:
        - Consumer groups distribute messages round-robin across group members
        - Messages require explicit ACK ‚Äî unacknowledged messages are redelivered after timeout
        - NACK causes immediate redelivery to another consumer in the group
        - Consumer group rebalancing when members join or leave
        pitfalls:
        - Duplicate delivery during rebalancing
        - Poison messages causing infinite redelivery loops
        - Head-of-line blocking
        concepts:
        - consumer groups
        - message acknowledgment
        - redelivery
        - rebalancing
        skills:
        - Consumer coordination
        - Delivery guarantees
        - Rebalancing
        deliverables:
        - Consumer group support with ACK/NACK and redelivery
        - Consumer group coordinator distributing messages across group members using round-robin or sticky assignment
        - Message acknowledgment protocol tracking pending messages and triggering redelivery on timeout
        - Rebalancing algorithm reassigning partitions when consumers join or leave the group
        estimated_hours: '8'
      - id: message-queue-m3
        name: Persistence & Backpressure
        description: Add message persistence using an append-only log file and implement backpressure to handle slow consumers.
        acceptance_criteria:
        - Messages are persisted to an append-only log file before delivery confirmation
        - On restart, broker recovers unacknowledged messages from the log
        - 'Backpressure: producers are throttled when consumer lag exceeds threshold'
        - Per-topic retention policy (time-based and size-based) with automatic cleanup
        pitfalls:
        - Log file growing unbounded without compaction
        - fsync performance vs durability tradeoff
        - Backpressure cascading to upstream services
        concepts:
        - append-only log
        - crash recovery
        - backpressure
        - retention policies
        skills:
        - Write-ahead logging
        - Crash recovery
        - Backpressure handling
        deliverables:
        - Persistent message queue with backpressure and retention
        - Append-only log file format storing messages with offset indexes for fast lookup
        - Crash recovery mechanism replaying uncommitted messages from WAL on broker restart
        - Backpressure handler pausing producers when consumer lag exceeds configurable threshold
        estimated_hours: '8'
      - id: message-queue-m4
        name: Dead Letter Queue & Monitoring
        description: Implement dead letter queues for poison messages and a monitoring API showing queue depths, consumer lag, and throughput.
        acceptance_criteria:
        - Messages exceeding max retry count are moved to a dead letter queue
        - DLQ messages can be inspected and replayed via API
        - Monitoring endpoint exposes queue depth, consumer lag, and message throughput per topic
        - Consumer heartbeat tracking with automatic removal of dead consumers
        pitfalls:
        - DLQ growing without alerting
        - Replayed messages processed out of order
        concepts:
        - dead letter queues
        - message replay
        - consumer heartbeat
        - observability
        skills:
        - Dead letter queues
        - Monitoring
        - Health checking
        deliverables:
        - DLQ handling and monitoring API
        - Dead letter queue capturing poison messages after exceeding max retry attempts
        - Message replay tool allowing reprocessing of DLQ messages after fixing consumer bugs
        - Monitoring API exposing queue depth, consumer lag, throughput, and error rates per topic
        estimated_hours: '6'
    advanced:
    - id: query-optimizer
      name: Query Optimizer
      description: Cost estimation
      difficulty: advanced
      estimated_hours: 20-35
      essence: Cost-based query plan generation through cardinality estimation, dynamic programming for join ordering, and selection between physical operators to minimize execution time and resource consumption.
      why_important: Building a query optimizer teaches you the algorithmic foundations of modern databases and how to reason about computational complexity in systems with multiple valid execution paths, skills critical for backend engineering and data infrastructure roles.
      learning_outcomes:
      - Implement tree-based query plan representations with logical and physical operators
      - Design cost models that estimate I/O, CPU, and memory usage for query execution
      - Build dynamic programming algorithms for optimal join ordering in multi-table queries
      - Apply selectivity estimation using histograms and statistics
      - Implement rule-based query rewriting and algebraic optimization
      - Compare cardinality estimation techniques and understand their impact on plan quality
      - Debug query performance issues by analyzing execution plans
      - Design heuristics for pruning the search space of possible query plans
      skills:
      - Cost-based optimization
      - Dynamic programming
      - Cardinality estimation
      - Query plan trees
      - Join algorithms
      - Statistics-based modeling
      - Algorithm complexity analysis
      - Performance profiling
      tags:
      - advanced
      - cost-estimation
      - databases
      - go
      - indexes
      - java
      - join-ordering
      - python
      architecture_doc: architecture-docs/query-optimizer/index.md
      languages:
        recommended:
        - Python
        - Java
        - Go
        also_possible:
        - Rust
        - C++
      resources:
      - name: CMU Database Course
        url: https://15445.courses.cs.cmu.edu/
        type: course
      - name: Query Optimization Survey
        url: https://www.vldb.org/pvldb/vol14/p3025-yang.pdf
        type: paper
      prerequisites:
      - type: skill
        name: SQL Parser
      - type: skill
        name: Database fundamentals
      - type: skill
        name: Algorithm complexity
      milestones:
      - id: query-optimizer-m1
        name: Query Plan Representation
        description: Define query plan tree structure.
        acceptance_criteria:
        - Represent plan as tree of operator nodes (Scan, Filter, Join, Project)
        - Support tree structure with parent-child operator relationships
        - Distinguish physical operators from logical operators
        - Pretty-print query plan as indented tree for debugging
        pitfalls:
        - Missing operators
        - Tree vs DAG
        - Operator semantics
        concepts:
        - Query plans
        - Operators
        - Plan trees
        skills:
        - Tree data structures
        - Operator pattern design
        - Abstract syntax trees
        - Recursive traversal algorithms
        deliverables:
        - Plan tree data structure with operator node hierarchy
        - Physical operator nodes for Scan, Filter, Join, and Project
        - Cost annotation attached to each plan node
        - Plan visualization printing indented tree representation
        estimated_hours: 3-4
      - id: query-optimizer-m2
        name: Cost Estimation
        description: Estimate the cost of query plans.
        acceptance_criteria:
        - Maintain table statistics including row count and distinct values per column
        - Estimate filter selectivity based on column statistics and operator type
        - Estimate join output cardinality using input sizes and selectivity
        - Compute total plan cost combining I/O pages and CPU operations
        pitfalls:
        - Statistics staleness
        - Correlation assumptions
        - Estimation errors
        concepts:
        - Cost models
        - Selectivity
        - Cardinality estimation
        skills:
        - Statistical analysis
        - Probability estimation
        - Cost function design
        - Histogram statistics
        - Mathematical modeling
        deliverables:
        - Statistics collection including row counts and column histograms
        - Selectivity estimation for filter predicates using statistics
        - I/O and CPU cost model with configurable weight parameters
        - Join cardinality estimation using column distinct value counts
        estimated_hours: 5-7
      - id: query-optimizer-m3
        name: Join Ordering
        description: Find optimal join order for multi-table queries.
        acceptance_criteria:
        - Enumerate all possible join orderings for multi-table queries
        - Use dynamic programming to find the minimum cost join order
        - Support both left-deep and bushy join tree plan shapes
        - Prune clearly suboptimal plans early to reduce search space
        pitfalls:
        - Exponential complexity
        - Cross joins
        - Missing predicates
        concepts:
        - Join ordering
        - Dynamic programming
        - Plan enumeration
        skills:
        - Dynamic programming
        - Graph algorithms
        - Combinatorial optimization
        - Memoization techniques
        deliverables:
        - Dynamic programming approach enumerating optimal join subsets
        - Cross product elimination pruning joins without predicates
        - Join cost estimation comparing different join pair orderings
        - Support for both bushy and left-deep plan tree shapes
        estimated_hours: 6-8
      - id: query-optimizer-m4
        name: Plan Selection
        description: Choose between physical operators and access methods.
        acceptance_criteria:
        - Select between sequential scan and index scan based on selectivity
        - Choose join algorithm (hash join vs nested loop) based on input sizes
        - Push filter predicates below joins to reduce intermediate result sizes
        - Generate final physical execution plan with all operators selected
        pitfalls:
        - Over-optimization
        - Plan caching
        - Statistics accuracy
        concepts:
        - Physical planning
        - Index selection
        - Predicate pushdown
        skills:
        - Index data structures
        - Heuristic algorithms
        - Performance profiling
        - Query execution strategies
        deliverables:
        - Dynamic programming for optimal join order selection
        - Heuristic pruning rules eliminating poor plan candidates
        - Best plan selection comparing costs across physical alternatives
        - Plan caching storing optimized plans for repeated queries
        estimated_hours: 5-7
    - id: wal-impl
      name: WAL Implementation
      description: Write-ahead logging
      difficulty: advanced
      estimated_hours: 15-25
      essence: Sequential log record append with LSN-tracked durability through fsync, followed by multi-phase crash recovery that reconstructs database state by replaying logged operations in analysis, redo, and undo phases to maintain ACID guarantees despite failures.
      why_important: Building this teaches you the fundamental mechanism behind ACID guarantees in production databases (PostgreSQL, MySQL, MongoDB), and how systems recover from crashes without data loss‚Äîcritical knowledge for any backend or infrastructure engineer.
      learning_outcomes:
      - Design binary log record formats with LSN tracking and record headers
      - Implement append-only file writes with fsync for durable persistence
      - Build ARIES recovery algorithm with analysis, redo, and undo phases
      - Implement compensation log records (CLRs) for idempotent crash recovery
      - Design checkpoint mechanisms to bound recovery time
      - Handle concurrent log writes with proper synchronization
      - Implement log segment rotation and space management
      - Debug crash scenarios and verify recovery correctness
      skills:
      - Write-Ahead Logging
      - Crash Recovery
      - Log-Structured Storage
      - ARIES Algorithm
      - File System Durability
      - Binary Protocol Design
      - Concurrency Control
      - System Reliability
      tags:
      - advanced
      - c
      - go
      - implementation
      - rust
      - storage
      architecture_doc: architecture-docs/wal-impl/index.md
      languages:
        recommended:
        - Rust
        - Go
        - C
        also_possible:
        - Python
        - Java
      resources:
      - name: ARIES Paper
        url: https://cs.stanford.edu/people/chr101/cs345/aries.pdf
        type: paper
      - name: SQLite WAL
        url: https://sqlite.org/wal.html
        type: documentation
      prerequisites:
      - type: skill
        name: File I/O
      - type: skill
        name: Database basics
      - type: skill
        name: Crash recovery concepts
      milestones:
      - id: wal-impl-m1
        name: Log Record Format
        description: Design and implement log record structure.
        acceptance_criteria:
        - LSN (Log Sequence Number) is monotonically increasing and unique per record
        - Transaction ID field links each log record to its originating transaction
        - Operation type field distinguishes INSERT, UPDATE, and DELETE log entries
        - Before and after images stored for undo and redo operations respectively
        pitfalls:
        - Variable-length fields
        - Endianness
        - Corruption detection
        concepts:
        - Log records
        - LSN
        - Undo/redo logging
        skills:
        - Binary serialization and deserialization
        - CRC checksums and data integrity validation
        - Memory-efficient data structure design
        - Variable-length encoding techniques
        deliverables:
        - Record header (LSN, type, length)
        - Record types (redo, undo, checkpoint)
        - CRC integrity checksum appended to each record for corruption detection
        - Record serialization converting structured log entries to binary byte format
        estimated_hours: 3-4
      - id: wal-impl-m2
        name: Log Writer
        description: Implement append-only log file with fsync.
        acceptance_criteria:
        - Append records atomically so partial writes are never visible after crash
        - Force log to disk via fsync before reporting commit to the client
        - Handle concurrent writers using locking or lock-free buffer for serialized append
        - Log file rotation creates new segment files at configurable size thresholds
        pitfalls:
        - Partial writes
        - fsync semantics
        - Torn pages
        concepts:
        - Durability
        - fsync
        - Group commit
        skills:
        - File I/O and system call optimization
        - Atomic write operations and fsync usage
        - Buffered I/O and write batching
        - Error handling for I/O failures
        deliverables:
        - Sequential log writer appending records to the end of the active segment file
        - Force write mechanism calling fsync to ensure durability before acknowledging
        - Buffer management batching multiple log records before flushing to reduce I/O
        - Log segment rotation creating a new file when the current segment reaches size limit
        estimated_hours: 4-5
      - id: wal-impl-m3
        name: Crash Recovery
        description: Implement ARIES-style recovery (redo then undo).
        acceptance_criteria:
        - Scan log from last checkpoint to identify committed and active transactions
        - Redo all committed changes restoring their effects to the database state
        - Undo incomplete transactions reverting their partial changes from the database
        - Track active transactions at crash time using the transaction table from the log
        pitfalls:
        - Idempotent operations
        - Log corruption
        - Partial recovery
        concepts:
        - ARIES recovery
        - Redo/undo
        - Crash consistency
        skills:
        - Transaction recovery algorithms implementation
        - Log scanning and parsing
        - State reconstruction from logs
        - Idempotent operation design
        - Crash simulation and testing
        deliverables:
        - Log scanning module reading records from the last checkpoint forward
        - Redo pass replaying all committed transaction changes to restore database state
        - Undo pass rolling back changes from transactions that were active at crash time
        - Recovery completion signal marking the database as consistent and ready for new transactions
        estimated_hours: 5-7
      - id: wal-impl-m4
        name: Checkpointing
        description: Implement checkpoints to speed up recovery.
        acceptance_criteria:
        - Fuzzy checkpoints allow concurrent transactions to continue without blocking
        - Record dirty pages and active transactions in the checkpoint record
        - Truncate old log entries before the checkpoint to reclaim disk space
        - Restart from checkpoint reduces recovery time by limiting log scan range
        pitfalls:
        - Checkpoint consistency
        - Log truncation timing
        - Master record
        concepts:
        - Checkpointing
        - Fuzzy checkpoints
        - Log truncation
        skills:
        - Background task scheduling and coordination
        - Snapshot consistency mechanisms
        - Log file management and rotation
        - Distributed checkpoint protocols
        deliverables:
        - Checkpoint record written to the log capturing current recovery state information
        - Active transaction list snapshot recorded at checkpoint for recovery starting point
        - Dirty page tracking recording which pages have uncommitted modifications in memory
        - Checkpoint coordination mechanism ensuring consistent state without blocking normal operations
        estimated_hours: 4-5
    - id: search-engine
      name: Search Engine
      description: Full-text search with inverted index, ranking
      difficulty: expert
      estimated_hours: '55'
      essence: Inverted index data structures mapping terms to document locations, combined with probabilistic ranking algorithms (TF-IDF/BM25) for relevance scoring, and approximate string matching via dynamic programming (Levenshtein distance) to enable fast, typo-tolerant full-text retrieval across large corpora.
      why_important: Building a full-text search engine teaches core information retrieval algorithms used by production systems like Elasticsearch and Lucene, while developing expertise in performance-critical systems programming with Rust/Go/C that handle high-throughput text processing and memory-efficient data structures.
      learning_outcomes:
      - Implement inverted index data structures with tokenization, normalization, and posting lists
      - Design and optimize TF-IDF and BM25 probabilistic ranking algorithms with document length normalization
      - Build fuzzy matching with Levenshtein distance algorithm and optimize using tries or BK-trees
      - Implement prefix-based autocomplete with finite state transducers or trie structures
      - Develop query parsers supporting boolean operators, phrase queries, and field-specific filters
      - Optimize memory-mapped file I/O for large-scale index storage and retrieval
      - Profile and benchmark search performance under high query throughput
      - Design skip list indexing for fast term lookups in sorted posting lists
      skills:
      - Inverted Index Design
      - Information Retrieval Algorithms
      - TF-IDF and BM25 Ranking
      - Fuzzy String Matching
      - Tokenization and Text Normalization
      - Memory-Mapped I/O
      - Query Language Parsing
      - Performance Optimization
      tags:
      - expert
      - framework
      - game-dev
      - indexing
      - information-retrieval
      - inverted-index
      - nlp
      - ranking
      - search
      - tokenization
      architecture_doc: architecture-docs/search-engine/index.md
      languages:
        recommended:
        - Rust
        - Go
        - C
        also_possible: []
      resources:
      - name: Inverted Index Tutorial
        url: https://www.baeldung.com/cs/indexing-inverted-index
        type: tutorial
      - name: Elasticsearch BM25 Guide
        url: https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables
        type: article
      - name: Stanford IR Book - Inverted Index
        url: https://nlp.stanford.edu/IR-book/html/htmledition/a-first-take-at-building-an-inverted-index-1.html
        type: documentation
      - name: Levenshtein Distance Implementation
        url: https://www.digitalocean.com/community/tutorials/levenshtein-distance-python
        type: tutorial
      - name: Meilisearch Documentation
        url: https://www.meilisearch.com/docs/home
        type: documentation
      milestones:
      - id: search-engine-m1
        name: Inverted Index
        description: Implement an inverted index with tokenization and normalization
        acceptance_criteria:
        - Build inverted index from a corpus of text documents
        - Support index updates including document addition and deletion
        - Handle large vocabularies with efficient memory usage
        - Implement index compression reducing storage size on disk
        pitfalls:
        - Stemming can over-stem (running -> run -> r) - use proven algorithms
        - Stopwords list should be configurable per language
        - Unicode normalization crucial for international text
        - Index updates are expensive - use batch updates when possible
        concepts:
        - 'Posting lists: maps terms to document IDs with positions'
        - Case folding and Unicode normalization for case-insensitive search
        - Stemming algorithms (Porter, Snowball) reduce words to root forms
        - Stop word removal to reduce index size and noise
        skills:
        - Inverted indexes
        - Tokenization
        - Text normalization
        deliverables:
        - Term-to-document mapping built from tokenized document content
        - Posting list structure storing document IDs and term positions
        - Index construction pipeline processing documents into inverted index
        - Index persistence saving and loading index from disk storage
        estimated_hours: '14'
      - id: search-engine-m2
        name: TF-IDF & BM25 Ranking
        description: Implement relevance ranking with TF-IDF and BM25 algorithms
        acceptance_criteria:
        - Calculate term frequency for each term in each document
        - Calculate inverse document frequency across the full corpus
        - Implement BM25 scoring formula with saturation and length normalization
        - Rank search results in descending order by relevance score
        pitfalls:
        - BM25 k1 and b parameters need tuning for your data
        - Very short documents can get artificially high scores
        - Precompute IDF values for performance
        - Field boosting can be gamed - normalize carefully
        concepts:
        - Term frequency (TF) measures how often a term appears in a document
        - Inverse document frequency (IDF) weighs rare terms higher than common ones
        - BM25 extends TF-IDF with saturation and document length normalization
        - Field boosting assigns different weights to title, body, metadata fields
        skills:
        - TF-IDF
        - BM25
        - Relevance scoring
        deliverables:
        - Term frequency calculation counting occurrences per document
        - Document frequency tracking how many documents contain each term
        - TF-IDF scoring combining term frequency and inverse document frequency
        - BM25 scoring implementation with tunable k1 and b parameters
        estimated_hours: '14'
      - id: search-engine-m3
        name: Fuzzy Matching & Autocomplete
        description: Implement typo tolerance with Levenshtein distance and prefix-based autocomplete
        acceptance_criteria:
        - Implement Levenshtein distance matching for approximate string comparison
        - Support prefix-based autocomplete returning matching terms
        - Handle common typos with configurable tolerance threshold
        - Rank autocomplete suggestions by combination of frequency and relevance
        pitfalls:
        - Edit distance is O(n*m) - prefilter candidates first
        - Max 2 typos is usually enough; more causes too many false matches
        - Transpositions are common typos - Damerau-Levenshtein handles them
        - Short words need fewer allowed edits (1 typo in 'cat' is too much)
        concepts:
        - Levenshtein distance counts insertions, deletions, and substitutions between strings
        - Damerau-Levenshtein adds transposition operations for common typos
        - Trie (prefix tree) enables efficient prefix-based autocomplete searches
        - N-gram indexing breaks terms into character sequences for fuzzy matching
        skills:
        - Edit distance
        - Prefix trees
        - Fuzzy search
        deliverables:
        - Edit distance computation using Levenshtein algorithm
        - Fuzzy search returning results within configurable edit distance
        - Prefix matching for typeahead query completion
        - Autocomplete suggestions ranked by frequency and relevance
        estimated_hours: '14'
      - id: search-engine-m4
        name: Query Parser & Filters
        description: Implement query parsing with boolean operators, phrases, and field filters
        acceptance_criteria:
        - Parse boolean queries with AND, OR, and NOT operators
        - Support phrase queries matching exact word sequences in order
        - Handle field-specific filters like author:name or date:range
        - Support numeric range queries filtering by min and max values
        pitfalls:
        - Phrase search requires position tracking - expensive
        - NOT without other terms returns entire index
        - Deeply nested queries can stack overflow - limit depth
        - Wildcard at start is very expensive - requires full scan
        concepts:
        - Recursive descent parsing converts query strings into abstract syntax trees
        - Boolean operators (AND, OR, NOT) combine term results using set operations
        - Phrase queries require positional indexes to verify term adjacency
        - Range filters and numeric comparisons need specialized index structures
        skills:
        - Query parsing
        - Boolean logic
        - Filter expressions
        deliverables:
        - Query tokenization splitting input into individual terms
        - Boolean operator support for AND, OR, and NOT logic
        - Field-specific filtering restricting search to named fields
        - Phrase query support matching exact multi-word sequences
        estimated_hours: '14'
    - id: event-sourcing
      name: Event Sourcing System
      description: Event store with projections and snapshots
      difficulty: advanced
      estimated_hours: '45'
      essence: Append-only immutable event log serving as system-of-record with state materialization through event replay, optimistic concurrency control for write conflicts, and separate read model maintenance through asynchronous projection rebuilds from event streams.
      why_important: Event sourcing provides audit trails, temporal queries, and robust distributed systems. Used in banking, e-commerce, and enterprise systems.
      learning_outcomes:
      - Implement append-only event store
      - Build projections from event streams
      - Handle snapshots for performance
      - Apply CQRS for read/write separation
      skills:
      - Event Store Design
      - CQRS Pattern
      - Aggregate Reconstitution
      - Event-Driven Architecture
      - Optimistic Concurrency
      - Projection Building
      - Snapshot Management
      - Domain Modeling
      tags:
      - advanced
      - architecture
      - backend
      - cqrs
      - distributed-systems
      - event-store
      - projections
      - replay
      architecture_doc: architecture-docs/event-sourcing/index.md
      languages:
        recommended:
        - Java
        - Kotlin
        - C#
        also_possible: []
      resources:
      - name: Martin Fowler on Event Sourcing
        url: https://martinfowler.com/eaaDev/EventSourcing.html
        type: article
      - name: Microsoft CQRS Pattern Guide
        url: https://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs
        type: documentation
      - name: Axon Framework Documentation
        url: https://www.axoniq.io/framework
        type: documentation
      - name: EventStoreDB Official Docs
        url: https://developers.eventstore.com/
        type: documentation
      - name: Event Sourcing in .NET Tutorial
        url: https://github.com/oskardudycz/EventSourcing.NetCore
        type: tutorial
      prerequisites:
      - type: project
        id: rest-api-design
      milestones:
      - id: event-sourcing-m1
        name: Event Store
        description: Build an append-only event store with optimistic concurrency, stream versioning, and event metadata.
        acceptance_criteria:
        - Append events to a stream with guaranteed ordering and persistence
        - Read events for a specific stream in the correct chronological order
        - Support optimistic concurrency using expected version on append operations
        - Handle high write throughput with efficient append-only storage design
        pitfalls:
        - Gap in versions makes stream unreadable
        - Large events bloat storage without compression
        - Missing global ordering breaks cross-stream projections
        concepts:
        - Append-only log architecture
        - Optimistic concurrency control with version vectors
        - Event stream partitioning and ordering guarantees
        - Event metadata and causality tracking
        - Storage compaction and retention policies
        skills:
        - Designing immutable data structures
        - Implementing optimistic locking
        - Database transaction isolation
        - Event serialization and versioning
        - Stream-based storage systems
        deliverables:
        - Event append operation adding events to a stream atomically
        - Event stream per aggregate with ordered retrieval support
        - Event versioning with schema evolution for backward compatibility
        - Event serialization using JSON or binary format with type metadata
        estimated_hours: '11'
      - id: event-sourcing-m2
        name: Aggregate & Event Sourcing
        description: Implement domain aggregates that are reconstituted from event history with command handlers.
        acceptance_criteria:
        - Load aggregate state by replaying its event stream from the beginning
        - Apply events sequentially to rebuild the aggregate's current state
        - Emit new events from command handlers after validating business rules
        - Handle command validation by checking aggregate state before emitting events
        pitfalls:
        - Business logic in apply() instead of command handlers
        - Forgetting to clear changes after save causes duplicates
        - Mutable event data allows accidental modification
        concepts:
        - Domain-driven design aggregate boundaries
        - Event sourcing reconstitution from history
        - Command-query separation in aggregates
        - Domain event modeling and naming
        - Aggregate invariant enforcement
        skills:
        - Implementing command handlers
        - Event replay and state reconstruction
        - Separating commands from events
        - Domain modeling with aggregates
        - Managing aggregate lifecycle
        deliverables:
        - Aggregate base class with event application and command handling interface
        - Event application methods rebuilding aggregate state from event history
        - Command handling that validates and emits new domain events
        - Aggregate loading from event stream with full state reconstruction
        estimated_hours: '11'
      - id: event-sourcing-m3
        name: Projections
        description: Build read models (projections) that update in response to events for efficient querying.
        acceptance_criteria:
        - Build read models from events by processing each event type into view data
        - Support multiple projections per stream for different query use cases
        - Handle projection rebuild from scratch without affecting running projections
        - Track projection position in stream to resume processing after restart
        pitfalls:
        - Processing same event twice without idempotency corrupts data
        - Checkpoint saved before event processed loses events on crash
        - Slow projection blocks all others in single-threaded engine
        concepts:
        - Event-driven materialized views
        - Projection idempotency and at-least-once semantics
        - Checkpoint-based event processing
        - Eventually consistent read models
        - Parallel projection processing strategies
        skills:
        - Building denormalized read models
        - Implementing idempotent event handlers
        - Checkpoint persistence and recovery
        - Async event processing
        - Query optimization for projections
        deliverables:
        - Projection handlers subscribing to event streams for read model updates
        - Read model updates transforming events into queryable view data
        - Projection rebuilding from scratch for schema changes or corrections
        - Eventual consistency tracking between event store and read models
        estimated_hours: '11'
      - id: event-sourcing-m4
        name: Snapshots
        description: Implement aggregate snapshots to avoid replaying entire event history for performance.
        acceptance_criteria:
        - Create aggregate snapshots periodically to reduce event replay time on load
        - Load aggregate from snapshot plus only events since the snapshot version
        - Handle snapshot versioning for schema changes between snapshot formats
        - Automatic snapshot creation triggers after configurable N events threshold
        pitfalls:
        - Snapshot schema changes break deserialization
        - Creating snapshot on every save destroys performance
        - Snapshot without version causes event replay from beginning
        concepts:
        - Snapshot strategies and frequency tuning
        - Schema evolution for snapshots
        - Incremental vs full snapshots
        - Snapshot versioning and compatibility
        - Performance tradeoffs of snapshotting
        skills:
        - Implementing snapshot serialization
        - Schema migration strategies
        - Performance profiling and optimization
        - Versioned data structures
        - Cache invalidation patterns
        deliverables:
        - Snapshot creation serializing current aggregate state at a point in time
        - Snapshot storage persisting snapshots alongside event stream metadata
        - Loading with snapshots to skip replaying early events during reconstruction
        - Snapshot scheduling based on event count or time interval thresholds
        estimated_hours: '11'
    - id: workflow-orchestrator
      name: Workflow Orchestrator
      description: DAG-based task scheduling like Airflow with dependencies
      difficulty: advanced
      estimated_hours: '70'
      essence: Topological ordering of task dependencies in a DAG, distributed worker pool coordination with concurrent execution, and transactional state persistence for idempotent retry logic and failure recovery across unreliable execution environments.
      why_important: Building this teaches you distributed systems coordination, task scheduling algorithms, and fault-tolerant state management‚Äîcore skills for data engineering platforms, CI/CD systems, and any large-scale automation infrastructure.
      learning_outcomes:
      - Design DAG-based workflow systems
      - Implement task dependency resolution
      - Handle failures and retries in pipelines
      - Build workflow monitoring and alerting
      skills:
      - DAG scheduling
      - Task dependencies
      - State management
      - Distributed execution
      - Failure handling
      - Monitoring
      tags:
      - advanced
      - airflow
      - dags
      - dependencies
      - devops
      - execution
      - orchestration
      architecture_doc: architecture-docs/workflow-orchestrator/index.md
      languages:
        recommended:
        - Go
        - Python
        - Java
        also_possible: []
      resources:
      - name: Apache Airflow Documentation
        url: https://airflow.apache.org/docs/apache-airflow/stable/index.html
        type: documentation
      - name: DAG Concepts Guide
        url: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html
        type: documentation
      - name: Build an Orchestrator in Go (From Scratch)
        url: https://www.manning.com/books/build-an-orchestrator-in-go-from-scratch
        type: book
      - name: Building a Simple Python Workflow Orchestrator
        url: https://blog.pmunhoz.com/python/simple-python-orchestrator
        type: tutorial
      - name: Designing a DAG-Based Workflow Engine from Scratch
        url: https://bugfree.ai/knowledge-hub/designing-a-dag-based-workflow-engine-from-scratch
        type: article
      prerequisites:
      - type: skill
        name: Background jobs
      - type: skill
        name: Database
      - type: skill
        name: Process management
      milestones:
      - id: workflow-orchestrator-m1
        name: DAG Definition
        description: Define workflows as directed acyclic graphs
        acceptance_criteria:
        - Define tasks with upstream and downstream dependency declarations
        - Parse DAG from config file and construct the internal dependency graph
        - Validate DAG has no cycles using topological sort or depth-first search
        - Support task parameters passed at trigger time and accessible in task execution
        pitfalls:
        - Forgetting to validate for cycles before workflow execution
        - Not handling dynamic task dependencies that change at runtime
        - Missing validation for orphaned tasks with no upstream dependencies
        - Allowing self-referencing tasks that create implicit cycles
        - Not enforcing naming constraints leading to task ID collisions
        concepts:
        - Directed Acyclic Graph (DAG) representation and cycle detection algorithms
        - Domain-specific language (DSL) parsing and abstract syntax trees
        - Task dependency declaration and validation rules
        - Graph serialization formats (JSON, YAML) for workflow persistence
        - Compile-time vs runtime DAG validation strategies
        skills:
        - Graph structures
        - DSL design
        - Validation
        deliverables:
        - Task class definition encapsulating a callable unit of work with metadata
        - DAG class with dependency tracking
        - Operator types (Python, Bash, SQL)
        - DAG validation with cycle detection rejecting invalid dependency graphs
        - Task parameters and templating supporting runtime variable substitution in task configs
        - DAG file discovery scanning a configured directory for DAG definition files
        estimated_hours: '14'
      - id: workflow-orchestrator-m2
        name: Scheduler
        description: Schedule DAG runs based on cron or triggers
        acceptance_criteria:
        - Trigger DAG runs based on schedule
        - Support cron schedules with standard five-field cron expression syntax
        - Handle manual triggers creating a DAG run with the current time as execution date
        - Track scheduled vs actual run time
        pitfalls:
        - Not handling timezone conversions correctly across distributed systems
        - Forgetting to limit concurrent DAG runs causing resource exhaustion
        - Missing edge cases in cron parsing like Feb 29th
        - Not implementing idempotency for backfill operations
        - Ignoring scheduler drift and clock synchronization issues
        concepts:
        - Cron expression parsing and next execution time calculation
        - Time-based vs event-driven workflow triggering mechanisms
        - Backfill operations for historical data processing windows
        - Catchup behavior for missed scheduled runs
        - Timezone handling and daylight saving time considerations
        skills:
        - Cron parsing
        - Run scheduling
        - Backfill
        deliverables:
        - Cron-based scheduling engine triggering DAG runs at configured cron intervals
        - Manual trigger support allowing ad-hoc DAG runs via API or CLI command
        - DAG run creation instantiating a new execution record with status tracking
        - Execution date handling assigning logical dates to each scheduled DAG run
        - Backfill support creating runs for missed intervals within a historical range
        - Catchup behavior configuration controlling whether past missed runs are auto-triggered
        estimated_hours: '14'
      - id: workflow-orchestrator-m3
        name: Task Execution
        description: Execute tasks with dependency resolution
        acceptance_criteria:
        - Execute ready tasks only after all upstream dependencies have succeeded
        - Handle task success and failure by updating state and triggering downstream logic
        - Support task retries with configurable retry count and delay between attempts
        - Implement task timeout killing execution that exceeds the configured duration limit
        pitfalls:
        - Not handling partial failures in parallel task execution gracefully
        - Forgetting to implement proper task timeout mechanisms
        - Missing deadlock scenarios in complex dependency graphs
        - Not propagating upstream failures correctly through the DAG
        - Ignoring resource contention when scheduling parallel tasks
        concepts:
        - Topological sorting for task execution order determination
        - Task state transitions (queued, running, success, failed, skipped)
        - Parallel task execution with concurrency limits and thread pools
        - Dependency resolution with upstream_failed and trigger rules
        - Retry mechanisms with exponential backoff strategies
        skills:
        - Topological sort
        - Parallel execution
        - State management
        deliverables:
        - Task instance state machine tracking transitions from queued to running to success or failure
        - Dependency checking gate ensuring upstream tasks completed before downstream execution
        - Parallel task execution running independent tasks concurrently up to a configured limit
        - Task queuing dispatching ready task instances to the executor for processing
        - XCom cross-task communication passing small data payloads between task instances
        - Task timeout handling killing tasks that exceed their configured maximum runtime
        estimated_hours: '14'
      - id: workflow-orchestrator-m4
        name: Worker & Executor
        description: Distributed task execution across workers
        acceptance_criteria:
        - Distribute tasks to workers via the configured executor backend
        - Support different executors including local sequential and distributed container modes
        - Handle worker failures by detecting missing heartbeats and requeueing affected tasks
        - Track task assignments mapping each running task to its assigned worker node
        pitfalls:
        - Not handling worker crashes and orphaned task reassignment
        - Forgetting to implement task result persistence before worker shutdown
        - Missing proper cleanup of worker resources on failure
        - Not accounting for network partitions between scheduler and workers
        - Ignoring task priority and queue starvation issues
        concepts:
        - Message queue systems for task distribution across workers
        - Heartbeat mechanisms for worker health monitoring
        - Task serialization and deserialization across process boundaries
        - Load balancing strategies for task assignment to workers
        - Worker pool management with dynamic scaling capabilities
        skills:
        - Worker processes
        - Task distribution
        - Resource management
        deliverables:
        - Local executor running tasks sequentially in the scheduler process
        - Celery or Redis executor distributing tasks to remote worker processes via message queue
        - Worker heartbeat mechanism reporting liveness at regular intervals to the scheduler
        - Task result collection gathering output and status from completed worker tasks
        - Worker pool management maintaining a set of concurrent worker processes or threads
        - Resource slot management limiting concurrent task execution to available capacity
        estimated_hours: '14'
      - id: workflow-orchestrator-m5
        name: Web UI & Monitoring
        description: Dashboard for workflow monitoring and management
        acceptance_criteria:
        - Display DAG graph with nodes colored by task execution status
        - Show run history listing past executions with start time, duration, and result
        - View task logs displaying captured output for debugging failed task instances
        - Trigger manual DAG runs from the web interface with optional parameter overrides
        pitfalls:
        - Not implementing pagination for large workflow lists causing UI freezes
        - Forgetting to handle stale data in real-time monitoring dashboards
        - Missing proper error boundaries in UI component hierarchy
        - Not optimizing log streaming causing memory leaks
        - Ignoring authentication and authorization for sensitive operations
        concepts:
        - WebSocket or Server-Sent Events for real-time status updates
        - DAG visualization with interactive graph rendering libraries
        - Gantt charts for task execution timeline visualization
        - Log streaming and aggregation from distributed workers
        - RESTful API design for workflow management operations
        skills:
        - Web UI
        - Real-time updates
        - Logging
        deliverables:
        - DAG list view displaying all registered DAGs with schedule and status summary
        - DAG graph visualization rendering task dependencies as a directed acyclic graph diagram
        - Task logs viewing interface displaying stdout and stderr output for each task instance
        - Manual trigger and clear controls allowing users to start runs or reset task states
        - Run history timeline showing past DAG runs with duration and outcome status
        - Alerting on failures sending notifications via email or webhook when tasks fail
        estimated_hours: '14'
    - id: vector-database
      name: Vector Database
      description: Similarity search with HNSW/IVF indexes for embeddings
      difficulty: advanced
      estimated_hours: '80'
      essence: Multi-dimensional vector storage with graph-based approximate nearest neighbor search using hierarchical navigable small world graphs, probabilistic layer navigation, and distance metric computations trading exact accuracy for sub-linear query time complexity.
      why_important: Vector similarity search is fundamental to modern AI applications including semantic search, recommendation systems, and RAG pipelines, and building a database from scratch teaches you core data structures (graphs, skip lists), systems programming (memory management, persistence), and performance optimization critical for infrastructure engineering roles.
      learning_outcomes:
      - Understand vector similarity search algorithms
      - Implement HNSW for efficient ANN search
      - Design memory-efficient vector storage
      - Build production-ready vector search APIs
      skills:
      - Vector similarity
      - HNSW algorithm
      - Distance metrics
      - Approximate nearest neighbor
      - Memory-mapped storage
      - Index persistence
      tags:
      - advanced
      - ai-ml
      - databases
      - embeddings
      - indexing
      - search
      - similarity-search
      architecture_doc: architecture-docs/vector-database/index.md
      languages:
        recommended:
        - Rust
        - Go
        - C++
        also_possible: []
      resources:
      - name: HNSW Algorithm Paper
        url: https://arxiv.org/abs/1603.09320
        type: paper
      - name: Faiss Documentation
        url: https://faiss.ai/index.html
        type: documentation
      - name: hnswlib Implementation
        url: https://github.com/nmslib/hnswlib
        type: tool
      - name: Comprehensive ANN Algorithms Guide
        url: https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6
        type: article
      - name: Understanding HNSW
        url: https://www.pinecone.io/learn/series/faiss/hnsw/
        type: tutorial
      prerequisites:
      - type: skill
        name: Data structures
      - type: skill
        name: Linear algebra basics
      milestones:
      - id: vector-database-m1
        name: Vector Storage
        description: Efficient storage and retrieval of vectors
        acceptance_criteria:
        - Store vectors with associated metadata fields like labels or tags
        - Support multiple vector dimensions configurable at collection creation time
        - Handle vector normalization converting raw vectors to unit length on insert
        - Implement efficient storage layout with contiguous memory for fast sequential access
        pitfalls:
        - Not aligning vectors to cache line boundaries
        - Forgetting to handle partial writes during serialization
        - Memory mapping without proper error handling on disk full
        - Using dynamic allocation for fixed-size vector storage
        concepts:
        - Memory alignment for SIMD operations
        - Contiguous vs. chunked storage layouts
        - Copy-on-write semantics for concurrent reads
        - Memory-mapped file lifecycle management
        skills:
        - Memory layout
        - Serialization
        - Memory mapping
        deliverables:
        - Fixed-dimension vector storage engine persisting float arrays with uniform dimensionality
        - Vector ID mapping index associating unique identifiers with stored vector offsets
        - Memory-mapped file storage enabling efficient access to large vector datasets on disk
        - Batch insert operations adding multiple vectors in a single transactional write
        - Vector retrieval by ID returning the full vector and associated metadata
        - Storage compaction process reclaiming space from deleted vector entries
        estimated_hours: '16'
      - id: vector-database-m2
        name: Distance Metrics
        description: Implement various similarity measures
        acceptance_criteria:
        - Implement cosine similarity returning values in the range of -1 to 1
        - Support Euclidean distance returning non-negative distance values
        - Support dot product similarity returning correct projection values
        - Handle metric-specific optimizations such as skipping normalization for pre-normalized vectors
        pitfalls:
        - Overflow in dot product with high-dimensional vectors
        - Not normalizing vectors before computing cosine similarity
        - Comparing distances from different metrics directly
        - Ignoring precision loss in distance accumulation
        concepts:
        - Numerical stability in floating-point distance calculations
        - SIMD vectorization for distance computation
        - 'Distance metric properties: symmetry and triangle inequality'
        - Normalized vs. unnormalized vector comparisons
        skills:
        - Cosine similarity
        - Euclidean distance
        - Dot product
        deliverables:
        - Cosine similarity computation measuring angular closeness between two vectors
        - Euclidean L2 distance computation measuring geometric distance between vectors
        - Dot product similarity computation measuring projection magnitude between vectors
        - SIMD-optimized implementations using hardware vector instructions for batch distance math
        - Batch distance computation evaluating one query against many stored vectors efficiently
        - Normalized vector handling skipping normalization when vectors are already unit-length
        estimated_hours: '16'
      - id: vector-database-m3
        name: Brute Force Search
        description: Exact nearest neighbor search baseline
        acceptance_criteria:
        - Implement exact nearest neighbor search computing distance to every stored vector
        - Support top-k queries returning results ranked by distance
        - Handle large datasets efficiently using batched distance computation
        - Use as baseline for ANN accuracy
        pitfalls:
        - Using sorted insertion instead of heap for top-K
        - Not pre-filtering candidates before distance computation
        - Scanning vectors in non-sequential memory order
        - Copying vectors instead of using indices for results
        concepts:
        - Heap-based top-K selection algorithms
        - Early termination strategies for filtered searches
        - Cache-efficient scanning patterns
        - Parallelization with work stealing for large datasets
        skills:
        - Linear scan
        - Top-K selection
        - Filtering
        deliverables:
        - K-nearest neighbors search returning the top-k closest vectors to a query
        - Threshold-based search returning all vectors within a similarity score cutoff
        - Metadata filtering restricting search results to vectors matching field predicates
        - Batch query support executing multiple search queries in a single request
        - Search with exclusions omitting specified vector IDs from result candidates
        - Performance benchmarking suite measuring query latency and throughput baselines
        estimated_hours: '16'
      - id: vector-database-m4
        name: HNSW Index
        description: Hierarchical Navigable Small World graph for ANN
        acceptance_criteria:
        - Implement HNSW graph construction with configurable layer count and connectivity
        - Support configurable M and ef parameters
        - Handle incremental inserts adding new vectors without full index rebuild
        - Achieve sub-linear search complexity demonstrated by benchmark comparison with brute force
        pitfalls:
        - Inserting nodes without updating neighbor connections bidirectionally
        - Not limiting max connections per node leading to memory bloat
        - Searching only the top layer without descending
        - Building graph without diversity in neighbor selection
        concepts:
        - Probabilistic skip list structure for layers
        - Greedy best-first search with backtracking
        - Dynamic neighbor selection during graph construction
        - Layer probability distribution and connectivity patterns
        skills:
        - Graph construction
        - Layer navigation
        - Greedy search
        deliverables:
        - Multi-layer graph structure organizing vectors in a hierarchical navigable small world
        - Node insertion with level assignment
        - Greedy search within each layer traversing edges toward nearest neighbors
        - Layer traversal strategy descending from top layer to base for query routing
        - ef_construction parameter controlling index build quality and neighbor exploration count
        - Index serialization persisting the graph structure to disk for later reload
        estimated_hours: '16'
      - id: vector-database-m5
        name: Query API & Server
        description: REST/gRPC API for vector operations
        acceptance_criteria:
        - REST or gRPC API exposes vector insert, search, and delete operations
        - Support batch queries executing multiple searches in a single round trip
        - Handle metadata filtering in search queries restricting results by field predicates
        - Implement hybrid search (vector + keyword)
        pitfalls:
        - Blocking all reads during index updates
        - Not setting timeouts on long-running queries
        - Deserializing entire request bodies before validation
        - Holding locks while performing disk I/O operations
        concepts:
        - Thread-safe read-write locking for concurrent queries
        - gRPC streaming for batch vector operations
        - Connection pooling and request queuing strategies
        - Backpressure handling for high query loads
        skills:
        - API design
        - Batch operations
        - Concurrent access
        deliverables:
        - Insert and upsert vectors API accepting vector data with optional metadata
        - Search API with filter support combining vector similarity and metadata predicates
        - Delete vectors endpoint removing entries by ID and reclaiming index space
        - Collection management API supporting create, list, and delete collection operations
        - Concurrent read and write handling using locking or lock-free structures for thread safety
        - Batch operations endpoint processing multiple inserts or queries in one request
        estimated_hours: '16'
    expert:
    - id: build-redis
      name: Build Your Own Redis
      description: In-memory data store with RESP protocol
      difficulty: expert
      estimated_hours: 40-60
      essence: Binary protocol parsing over TCP streams, concurrent client session management, and implementing memory-efficient data structures with dual persistence strategies (snapshot and log-based) for durability.
      why_important: Building Redis teaches low-level systems programming fundamentals‚ÄîTCP servers, binary protocols, memory management, and persistence mechanisms‚Äîthat are essential for backend infrastructure and database engineering roles.
      learning_outcomes:
      - Implement a TCP server with concurrent client connection handling
      - Parse and generate RESP (Redis Serialization Protocol) wire format
      - Design in-memory data structures optimized for different access patterns
      - Build key expiration mechanisms using time-based eviction
      - Implement RDB snapshot persistence with fork-based background saves
      - Design append-only file (AOF) logging for write durability
      - Build publish/subscribe messaging with client subscription management
      - Implement consistent hashing and slot-based data sharding
      skills:
      - TCP/IP Networking
      - Binary Protocol Parsing
      - In-Memory Data Structures
      - Concurrency and Threading
      - Persistence Mechanisms
      - Event-Driven Architecture
      - Distributed Systems
      - Process Forking
      tags:
      - build-from-scratch
      - c
      - data-structures
      - databases
      - expert
      - go
      - in-memory
      - key-value
      - networking
      - persistence
      - protocols
      - rust
      architecture_doc: architecture-docs/build-redis/index.md
      languages:
        recommended:
        - Go
        - Rust
        - C
        also_possible:
        - Python
        - Java
        - TypeScript
      resources:
      - name: CodeCrafters Redis Challenge
        url: https://app.codecrafters.io/courses/redis/overview
        type: tool
      - name: Build Your Own Redis (Book)
        url: https://build-your-own.org/redis/
        type: book
      - name: Redis Protocol Specification (RESP)
        url: https://redis.io/docs/reference/protocol-spec/
        type: documentation
      prerequisites:
      - type: skill
        name: TCP/IP networking basics
      - type: skill
        name: Hash tables and data structures
      - type: skill
        name: Concurrency fundamentals
      - type: skill
        name: File I/O
      milestones:
      - id: build-redis-m1
        name: TCP Server + RESP Protocol
        description: Create a TCP server that listens on port 6379 and responds to PING with +PONG. Implement basic RESP parsing.
        acceptance_criteria:
        - TCP server listens on configurable port and accepts concurrent client connections
        - RESP parser correctly decodes all five RESP data types from byte stream
        - RESP serializer produces wire-format output compatible with redis-cli client
        - PING command returns PONG response to verify end-to-end protocol implementation
        - Server handles client disconnection without crashing or leaking resources
        pitfalls:
        - Forgetting \r\n line endings (CRLF, not just LF)
        - Not handling partial reads (TCP is a stream)
        - Blocking the main thread on single client
        concepts:
        - TCP sockets and server lifecycle
        - RESP protocol encoding/decoding
        - Basic network I/O
        skills:
        - TCP/IP socket programming
        - Protocol parsing and state machines
        - Concurrent connection handling
        - Binary protocol implementation
        deliverables:
        - TCP server accepting multiple concurrent client connections
        - RESP protocol parser decoding simple strings, errors, integers, bulk strings, and arrays
        - RESP serializer encoding server responses into wire format
        - Client connection lifecycle management with graceful disconnection
        estimated_hours: 3-4
      - id: build-redis-m2
        name: GET/SET/DEL Commands
        description: Implement basic key-value operations. Store data in an in-memory hash map.
        acceptance_criteria:
        - SET stores key-value pair accessible by subsequent GET commands
        - GET returns bulk string for existing keys and nil for missing keys
        - DEL removes specified keys and returns integer count of keys actually deleted
        - SET supports NX (only if not exists) and XX (only if exists) flags
        - Key-value store handles concurrent access from multiple clients safely
        pitfalls:
        - Not handling binary-safe strings
        - Forgetting null response for non-existent keys
        - Case sensitivity issues
        concepts:
        - Hash table implementation
        - RESP array parsing
        - Command dispatch pattern
        skills:
        - Thread-safe data structures
        - Command parsing and routing
        - Memory management for key-value storage
        - String handling and encoding
        deliverables:
        - In-memory key-value store supporting string values
        - SET command storing key-value pairs with optional flags
        - GET command retrieving values by key with nil for missing keys
        - DEL command removing one or more keys and returning count deleted
        estimated_hours: 2-3
      - id: build-redis-m3
        name: Expiration (TTL)
        description: Add key expiration support. Keys can be set with PX (milliseconds) or EX (seconds) options.
        acceptance_criteria:
        - EXPIRE sets TTL on existing key and returns 1 on success or 0 if key missing
        - Keys become inaccessible via GET after their TTL has elapsed
        - TTL command returns remaining seconds until key expiration
        - Lazy expiration removes expired key when client attempts to access it
        - Active expiration periodically scans and removes a sample of expired keys
        pitfalls:
        - Using relative time instead of absolute timestamp
        - Not handling clock drift
        - Memory leaks from never-accessed expired keys
        concepts:
        - TTL and expiration strategies
        - Lazy vs active deletion
        - Time handling and precision
        skills:
        - Time-based event scheduling
        - Background task management
        - Memory cleanup strategies
        - Timestamp arithmetic and precision
        deliverables:
        - EXPIRE command setting time-to-live in seconds on existing keys
        - TTL storage per key tracking absolute expiration timestamps
        - Lazy expiration checking and removing expired keys on access
        - Active expiration sampling and removing expired keys periodically
        estimated_hours: 2-3
      - id: build-redis-m4
        name: Data Structures (List, Set, Hash)
        description: Implement Redis data structure commands beyond simple strings.
        acceptance_criteria:
        - LPUSH and RPUSH prepend and append elements to list respectively
        - SADD adds unique members to set and returns count of newly added elements
        - HSET stores field-value pairs in hash and HGET retrieves by field name
        - Operations on wrong type return WRONGTYPE error without modifying data
        - LRANGE returns list elements within specified start and stop index range
        pitfalls:
        - Using array for list (O(n) insert at head)
        - Not handling type errors
        - LRANGE negative indices
        concepts:
        - Linked lists vs arrays
        - Set data structure
        - Skip lists (for sorted sets)
        skills:
        - Implementing complex data structures
        - Type system design
        - Algorithm complexity optimization
        - Range query handling
        deliverables:
        - List data type with LPUSH, RPUSH, LPOP, RPOP, and LRANGE operations
        - Set data type with SADD, SREM, SMEMBERS, and SISMEMBER operations
        - Hash data type with HSET, HGET, HDEL, and HGETALL operations
        - Type checking preventing wrong-type operations on existing keys
        estimated_hours: 4-6
      - id: build-redis-m5
        name: Persistence (RDB Snapshots)
        description: Implement point-in-time snapshots using RDB format. SAVE blocks, BGSAVE forks.
        acceptance_criteria:
        - RDB writer serializes all keys, values, and TTLs to binary file format
        - RDB reader restores complete database state from binary snapshot file on startup
        - BGSAVE command forks background process to create snapshot without blocking clients
        - Automatic saves trigger when configured number of changes occur within time window
        pitfalls:
        - Blocking main thread during save
        - Not using fork() for BGSAVE
        - Corrupted RDB from incomplete writes
        concepts:
        - Binary serialization
        - Process forking and copy-on-write
        - Atomic file operations
        skills:
        - Process forking and IPC
        - Binary file format design
        - Atomic filesystem operations
        - Serialization and deserialization
        deliverables:
        - RDB file format writer serializing entire database to binary snapshot
        - RDB file format reader deserializing snapshot to restore database state
        - Background save process creating snapshot without blocking main thread
        - Automatic save triggers based on configurable change count thresholds
        estimated_hours: 5-8
      - id: build-redis-m6
        name: Persistence (AOF)
        description: Implement Append-Only File logging for durability.
        acceptance_criteria:
        - Every write command is appended to AOF file in RESP wire format
        - AOF replay correctly reconstructs database state from command log on startup
        - AOF rewrite produces compact file equivalent to current database state
        - Fsync policy supports always, every-second, and OS-controlled modes
        - AOF and RDB can coexist with AOF taking priority for recovery
        pitfalls:
        - AOF growing unbounded without rewrite
        - Concurrent writes during BGREWRITEAOF
        - Data loss from buffered writes
        concepts:
        - Write-ahead logging
        - fsync and durability guarantees
        - Log compaction
        skills:
        - Write-ahead logging implementation
        - File I/O and buffering strategies
        - Background compaction processes
        - Crash recovery mechanisms
        deliverables:
        - Append-only file logging every write command in RESP format
        - AOF replay mechanism reconstructing database state from command log
        - AOF rewrite compacting log by generating minimal command sequence
        - Configurable fsync policy balancing durability against write performance
        estimated_hours: 4-6
      - id: build-redis-m7
        name: Pub/Sub
        description: Implement publish/subscribe messaging pattern.
        acceptance_criteria:
        - SUBSCRIBE registers client to channel and confirms subscription with message count
        - PUBLISH delivers message to all clients subscribed to the target channel
        - UNSUBSCRIBE removes subscription and notifies client of remaining subscription count
        - PSUBSCRIBE matches channels using glob patterns like 'news.*' for wildcard subscription
        pitfalls:
        - Not blocking other commands in subscribed state
        - Memory leaks from disconnected subscribers
        - Race conditions in publish
        concepts:
        - Pub/Sub pattern
        - Observer pattern
        - Connection state management
        skills:
        - Event-driven architecture
        - Subscription management
        - Broadcasting patterns
        - Stateful connection handling
        deliverables:
        - SUBSCRIBE command registering client to receive messages on named channels
        - PUBLISH command broadcasting message to all subscribers of a channel
        - UNSUBSCRIBE command removing client subscription from specified channels
        - Pattern-based subscription matching channels with glob-style patterns
        estimated_hours: 3-4
      - id: build-redis-m8
        name: Cluster Mode (Sharding)
        description: Implement horizontal scaling with hash slot based sharding.
        acceptance_criteria:
        - 16384 hash slots are distributed evenly across all cluster nodes
        - Key routing consistently maps same key to same hash slot using CRC16
        - MOVED response includes correct target node address for redirected key
        - Cluster nodes exchange topology information and detect failed peers via gossip
        pitfalls:
        - Not handling key migration
        - Cross-slot operations
        - Network partitions and split brain
        concepts:
        - Consistent hashing
        - Hash slots and key routing
        - Distributed systems fundamentals
        skills:
        - Distributed hash table design
        - Inter-node communication protocols
        - Cluster topology management
        - Data migration strategies
        deliverables:
        - Hash slot assignment mapping 16384 slots across cluster nodes
        - Key-to-slot mapping using CRC16 hash of key modulo 16384
        - MOVED redirect response directing clients to correct shard node
        - Cluster topology gossip protocol sharing node state information
        estimated_hours: 8-12
    - id: build-sqlite
      name: Build Your Own SQLite
      description: Embedded SQL database
      difficulty: expert
      estimated_hours: 60-100
      essence: Page-based storage management with B-tree indexing, SQL parsing and compilation to bytecode, and ACID transaction guarantees through write-ahead logging and lock-based concurrency control.
      why_important: Building a database from scratch teaches the fundamental data structures and algorithms underlying all modern databases, from production systems like PostgreSQL to embedded stores in mobile apps. These low-level skills‚Äîdisk I/O management, query optimization, and crash recovery‚Äîare directly applicable to backend engineering, distributed systems, and performance optimization roles.
      learning_outcomes:
      - Implement B-tree storage with node splitting, merging, and rebalancing for efficient indexing
      - Design a lexer and recursive-descent parser to convert SQL text into an abstract syntax tree
      - Build a bytecode virtual machine that executes compiled query plans
      - Implement page-based disk I/O with fixed-size blocks and efficient space management
      - Design a cost-based query optimizer that selects optimal execution plans using statistics
      - Implement ACID transactions with write-ahead logging for crash recovery and durability
      - Build row-level locking and isolation mechanisms to handle concurrent reads and writes
      - Debug low-level file format issues and understand binary data layout on disk
      skills:
      - B-tree Indexing
      - SQL Parsing
      - Query Optimization
      - Page-Based Storage
      - Write-Ahead Logging
      - Transaction Management
      - Binary File Formats
      - Bytecode Execution
      tags:
      - acid
      - btree
      - build-from-scratch
      - c
      - databases
      - expert
      - go
      - persistence
      - query-engine
      - rust
      - sql
      architecture_doc: architecture-docs/build-sqlite/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible:
        - Python
        - Java
      resources:
      - name: Let's Build a Simple Database
        url: https://cstack.github.io/db_tutorial/
        type: tutorial
      - name: CodeCrafters SQLite Challenge
        url: https://app.codecrafters.io/courses/sqlite/overview
        type: tool
      - name: SQLite File Format
        url: https://www.sqlite.org/fileformat2.html
        type: documentation
      - name: CMU 15-445 Database Systems
        url: https://15445.courses.cs.cmu.edu
        type: course
      prerequisites:
      - type: skill
        name: B-tree data structure
      - type: skill
        name: SQL basics
      - type: skill
        name: File I/O and binary formats
      - type: skill
        name: Basic compiler concepts
      milestones:
      - id: build-sqlite-m1
        name: SQL Tokenizer
        description: Build a lexer that converts SQL text into tokens.
        acceptance_criteria:
        - Tokenizer correctly identifies SQL keywords regardless of letter casing
        - String literals enclosed in single quotes are parsed including escape sequences
        - Numeric literals including integers and floating point values are recognized
        - Operators like =, <, >, !=, AND, OR are tokenized as distinct operator tokens
        pitfalls:
        - Not handling escaped quotes in strings ('it''s')
        - Case sensitivity
        - Unicode identifiers
        concepts:
        - Lexical analysis
        - Finite state machines
        - Token representation
        skills:
        - String parsing and manipulation
        - State machine implementation
        - Token type classification
        - Error reporting with source locations
        deliverables:
        - Lexer splitting SQL input into keyword, identifier, and literal tokens
        - Keyword recognition for SELECT, INSERT, CREATE, WHERE, and other SQL words
        - String and number literal parsing with proper escape handling
        - Operator and punctuation tokenization for comparison and grouping symbols
        estimated_hours: 3-4
      - id: build-sqlite-m2
        name: SQL Parser (AST)
        description: Build a parser that converts tokens into an Abstract Syntax Tree.
        acceptance_criteria:
        - SELECT parser produces AST node with column list, FROM table, and optional WHERE
        - INSERT parser produces AST with target table, column names, and value expressions
        - CREATE TABLE parser extracts column definitions with types and constraint annotations
        - Expression parser correctly handles operator precedence and parenthesized groups
        pitfalls:
        - Left recursion in grammar
        - Operator precedence (AND vs OR)
        - Not handling parentheses
        concepts:
        - Recursive descent parsing
        - AST design
        - Operator precedence
        skills:
        - Recursive function design
        - Tree data structure implementation
        - Grammar rule encoding
        - Expression evaluation ordering
        deliverables:
        - SELECT statement parser producing AST with columns, table, and where clause
        - INSERT statement parser producing AST with table, columns, and values
        - CREATE TABLE parser producing AST with column names, types, and constraints
        - Expression parser handling arithmetic, comparison, and boolean expressions
        estimated_hours: 5-8
      - id: build-sqlite-m3
        name: B-tree Page Format
        description: Implement the on-disk B-tree page structure.
        acceptance_criteria:
        - Page header contains page type, cell count, and free space offset fields
        - Leaf pages store sorted key-value cells accessible by binary search
        - Internal pages store separator keys and child page numbers for tree navigation
        - Pages serialize to and deserialize from fixed-size byte buffers correctly
        pitfalls:
        - Cell overflow (value too large for page)
        - Page fragmentation after deletions
        - Endianness
        concepts:
        - B-tree structure
        - Page-based storage
        - Variable-length records
        skills:
        - Binary file I/O
        - Memory layout and alignment
        - Byte-order handling
        - Page-based buffer management
        deliverables:
        - Page structure with header, cell pointers, and cell content areas
        - Leaf page storing key-value pairs sorted by key within page
        - Internal page storing keys and child page pointers for navigation
        - Page serialization writing structured data to fixed-size disk pages
        estimated_hours: 6-10
      - id: build-sqlite-m4
        name: Table Storage
        description: Store table rows in B-tree leaves with rowid as key.
        acceptance_criteria:
        - CREATE TABLE creates B-tree root page and records schema in system catalog
        - INSERT serializes row data and inserts into correct B-tree leaf page by key
        - Node splitting creates new leaf page and promotes separator key to parent
        - Full scan returns all rows in primary key order by traversing leaf pages
        pitfalls:
        - Not handling NULL values
        - Rowid gaps after deletes
        - Variable-length integer encoding
        concepts:
        - Row storage formats
        - Schema management
        - Type serialization
        skills:
        - Variable-length encoding schemes
        - Type system implementation
        - Schema serialization
        - Row-level data structures
        deliverables:
        - Table creation allocating root B-tree page and storing schema metadata
        - Row insertion serializing column values and inserting into B-tree leaf page
        - B-tree node splitting when leaf page exceeds capacity after insertion
        - Full table scan iterating all rows by traversing B-tree leaf pages in order
        estimated_hours: 4-6
      - id: build-sqlite-m5
        name: SELECT Execution (Table Scan)
        description: Execute SELECT queries by scanning all rows.
        acceptance_criteria:
        - SELECT * FROM table returns all rows
        - SELECT col1, col2 returns specific columns
        - Rows returned in rowid order
        - Handle non-existent tables with error
        pitfalls:
        - Column name case sensitivity
        - NULL handling in output
        - Memory management for large result sets
        concepts:
        - B-tree traversal
        - Projection operator
        - Cursor pattern
        skills:
        - Iterator pattern implementation
        - Tree traversal algorithms
        - Result set construction
        - Query result formatting
        deliverables:
        - Table scan operator
        - Row deserialization from binary page format to in-memory record structure with typed column values
        - Column projection that selects specified fields from result rows and constructs the output tuple
        - Result set building
        estimated_hours: 3-4
      - id: build-sqlite-m6
        name: INSERT/UPDATE/DELETE
        description: Implement data modification operations.
        acceptance_criteria:
        - INSERT adds row to B-tree and subsequent SELECT returns the inserted data
        - UPDATE modifies specified columns in rows matching WHERE condition
        - DELETE removes matched rows and subsequent SELECT no longer returns them
        - NOT NULL constraint rejects INSERT or UPDATE setting column to null value
        pitfalls:
        - B-tree rebalancing after delete
        - Updating primary key
        - Constraint violations
        concepts:
        - B-tree insertion and deletion
        - Record modification
        - Constraint checking
        skills:
        - Tree rebalancing algorithms
        - Data integrity enforcement
        - Concurrent modification handling
        - Constraint validation logic
        deliverables:
        - INSERT execution adding new rows to table B-tree storage
        - UPDATE execution modifying column values in existing rows matching condition
        - DELETE execution removing rows matching condition from B-tree storage
        - Constraint enforcement validating NOT NULL and UNIQUE during write operations
        estimated_hours: 5-8
      - id: build-sqlite-m7
        name: WHERE Clause and Indexes
        description: Implement filtering and secondary indexes.
        acceptance_criteria:
        - WHERE clause filters rows using comparison operators and boolean logic correctly
        - Secondary index stores column values sorted for efficient range and equality lookups
        - Index lookup retrieves matching rows without scanning entire table
        - CREATE INDEX builds B-tree index from existing table data on specified column
        pitfalls:
        - Index not used for non-equality predicates
        - Maintaining index on INSERT/UPDATE/DELETE
        - Choosing between index scan vs table scan
        concepts:
        - Secondary indexes
        - Query planning basics
        - Filter predicates
        skills:
        - Index structure design
        - Query optimization basics
        - Predicate evaluation
        - Cost estimation heuristics
        deliverables:
        - WHERE clause evaluator filtering rows by boolean expression during scan
        - Secondary index B-tree mapping indexed column values to primary keys
        - Index-based lookup using B-tree search instead of full table scan
        - CREATE INDEX statement building secondary index on specified column
        estimated_hours: 6-10
      - id: build-sqlite-m8
        name: Query Planner
        description: Implement cost-based query optimization.
        acceptance_criteria:
        - Planner considers both full table scan and available index scan for each query
        - Cost model estimates I/O pages read for each candidate execution plan
        - Planner selects index scan when estimated cost is lower than full table scan
        - EXPLAIN shows chosen plan including scan type, index used, and estimated rows
        pitfalls:
        - Stale statistics
        - Wrong cardinality estimates
        - Exponential plan space for many JOINs
        concepts:
        - Cost-based optimization
        - Cardinality estimation
        - Join algorithms
        skills:
        - Statistics collection and maintenance
        - Cardinality estimation techniques
        - Join algorithm selection
        - Plan comparison and ranking
        deliverables:
        - Plan enumeration generating candidate execution strategies for each query
        - Cost estimation model comparing full scan versus index scan costs
        - Index selection choosing cheapest available index for WHERE clause predicates
        - EXPLAIN command displaying chosen execution plan for a given query
        estimated_hours: 8-12
      - id: build-sqlite-m9
        name: Transactions (BEGIN/COMMIT/ROLLBACK)
        description: Implement ACID transactions.
        acceptance_criteria:
        - BEGIN command starts a new transaction and sets transaction state to active
        - COMMIT command makes all changes permanent and clears the transaction state
        - ROLLBACK undoes all changes since BEGIN
        - Changes not visible to other connections until commit
        - Crash recovery mechanism restores database to last consistent state using journal
        pitfalls:
        - Partial writes (torn pages)
        - Lock ordering deadlocks
        - Long-running transactions blocking others
        concepts:
        - ACID properties
        - Shadow paging vs WAL
        - Crash recovery
        skills:
        - Lock management
        - Atomic operations
        - Recovery protocol implementation
        - Isolation level enforcement
        deliverables:
        - Transaction state tracking with active, committed, and aborted states plus isolation level configuration
        - Rollback journal recording original page contents before modification for transaction undo support
        - Commit processing that flushes dirty pages to disk and removes the rollback journal atomically
        - ACID guarantees ensuring atomicity via rollback journal, consistency via constraints, isolation via locking, and durability via fsync
        estimated_hours: 8-12
      - id: build-sqlite-m10
        name: WAL Mode
        description: Implement Write-Ahead Logging for better concurrency.
        acceptance_criteria:
        - WAL mode writes committed pages to separate WAL file instead of main database
        - Readers check WAL first for most recent page version before reading main database
        - Checkpoint merges WAL pages into main database file and truncates WAL
        - Multiple readers can query database concurrently while a writer holds WAL lock
        pitfalls:
        - WAL growing unbounded without checkpoint
        - Readers pinning old WAL frames
        - WAL file corruption detection
        concepts:
        - Write-ahead logging
        - MVCC basics
        - Checkpointing
        skills:
        - Log-structured storage
        - Concurrent read/write handling
        - Checkpoint algorithm design
        - Log replay and recovery
        deliverables:
        - WAL file format appending committed page images to separate log file
        - WAL reader serving reads from WAL for recently committed pages
        - Checkpoint process copying WAL pages back into main database file
        - Concurrent reader support allowing reads during active write transactions
        estimated_hours: 8-12
    - id: build-kafka
      name: Build Your Own Kafka
      description: Distributed message queue
      difficulty: expert
      estimated_hours: 60-100
      essence: Append-only distributed log with deterministic partition routing, leader-follower replication consensus, and coordinated consumer group balancing for parallel ordered message processing across unreliable networks.
      why_important: Building this teaches you foundational distributed systems patterns used in production data pipelines at companies like LinkedIn, Uber, and Netflix‚Äîskills that directly translate to designing high-throughput, fault-tolerant backend systems.
      learning_outcomes:
      - Implement topic partitioning with deterministic message routing based on keys
      - Design leader-follower replication with in-sync replica sets for fault tolerance
      - Build producer batching with configurable acknowledgment levels (acks=0,1,all)
      - Implement consumer group coordination with partition assignment and rebalancing protocols
      - Design write-ahead logging for durable message persistence on disk
      - Build offset management for exactly-once and at-least-once delivery semantics
      - Implement backpressure handling and flow control in distributed message streams
      - Debug network partition scenarios and split-brain resolution in replicated systems
      skills:
      - Distributed consensus
      - Log-based storage
      - Partition replication
      - Consumer coordination
      - Pub/sub architecture
      - Offset management
      - Leader election
      - Binary protocols
      tags:
      - build-from-scratch
      - consumers
      - data-structures
      - distributed
      - expert
      - go
      - java
      - message-queue
      - partitions
      - producers
      - rust
      - streaming
      architecture_doc: architecture-docs/build-kafka/index.md
      languages:
        recommended:
        - Go
        - Java
        - Rust
        also_possible:
        - Scala
        - C++
      resources:
      - type: book
        name: 'Kafka: The Definitive Guide'
        url: https://www.confluent.io/resources/kafka-the-definitive-guide/
      - type: paper
        name: 'Kafka: a Distributed Messaging System'
        url: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf
      prerequisites:
      - type: skill
        name: Replicated log
      - type: skill
        name: Distributed systems
      - type: skill
        name: Binary protocols
      milestones:
      - id: build-kafka-m1
        name: Topic and Partitions
        description: Implement topic management with multiple partitions.
        acceptance_criteria:
        - Topics are created with a user-specified partition count and each partition is initialized as an empty log
        - Each partition functions as an append-only log where messages are assigned sequential offsets starting from zero
        - Message key hashing consistently routes messages with the same key to the same partition for ordering guarantees
        - Consumer offset tracking stores the last consumed offset per partition per consumer group for resume-on-restart
        pitfalls:
        - Partition assignment consistency
        - Offset gaps
        - Key null handling
        concepts:
        - Partitioning
        - Ordered logs
        - Horizontal scaling
        skills:
        - Partitioning algorithms
        - Log-structured storage
        - Distributed system design
        - Load balancing across partitions
        deliverables:
        - Topic creation API that registers a new topic with a specified name, partition count, and replication factor
        - Partition management module that represents each partition as an independent append-only log segment
        - Partition-to-broker assignment logic that distributes partition leadership across brokers for load balancing
        - Topic metadata storage that records topic configuration, partition list, and leader assignments for cluster discovery
        estimated_hours: 10-15
      - id: build-kafka-m2
        name: Producer
        description: Implement producer with batching and acknowledgments.
        acceptance_criteria:
        - Batch accumulation groups multiple messages and sends them in a single produce request for throughput
        - Configurable acks (0=fire-and-forget, 1=leader-ack, all=ISR-ack) control durability guarantees per produce request
        - Retry logic re-sends failed produce requests with exponential backoff up to a configurable maximum attempt count
        - Idempotent producer assigns sequence numbers to detect and discard duplicate retries at the broker (optional)
        pitfalls:
        - Batch timeout handling
        - Leader failover during send
        - Duplicate messages
        concepts:
        - Batching
        - Acknowledgments
        - At-least-once delivery
        skills:
        - Asynchronous I/O
        - Batching and buffering strategies
        - Network protocol design
        - Retry logic and idempotency
        - Concurrency control
        deliverables:
        - Message serialization that encodes key, value, headers, and timestamp into the record batch wire format
        - Partition selection logic using key hash, round-robin, or custom partitioner to choose the target partition
        - Batch accumulation buffer that groups multiple messages into a single produce request for network efficiency
        - Delivery acknowledgment handler that confirms successful writes based on the configured acks level
        estimated_hours: 12-18
      - id: build-kafka-m3
        name: Consumer Groups
        description: Implement consumer groups with partition assignment and rebalancing.
        acceptance_criteria:
        - Group membership protocol handles consumer join, heartbeat, and leave requests through the group coordinator
        - Partition assignment strategies (range, round-robin) distribute partitions evenly among active group members
        - Offset commits persist the consumer's position per partition so it can resume after restart without reprocessing
        - Rebalancing redistributes partitions when a consumer joins, leaves, or fails its heartbeat within the session timeout
        pitfalls:
        - Rebalance storms
        - Stuck rebalances
        - Duplicate processing during rebalance
        concepts:
        - Consumer groups
        - Partition assignment
        - Rebalancing
        skills:
        - Distributed coordination protocols
        - Leader election algorithms
        - State machine replication
        - Failure detection and recovery
        - Group membership management
        deliverables:
        - Consumer group membership protocol that registers consumers with the group coordinator broker
        - Partition assignment strategies (range and round-robin) that distribute partitions among group members
        - Offset commit and fetch operations that persist consumer progress and resume from the last committed position
        - Rebalancing protocol that triggers partition reassignment when consumers join or leave the group
        estimated_hours: 18-30
      - id: build-kafka-m4
        name: Replication
        description: Implement partition replication for fault tolerance.
        acceptance_criteria:
        - Leader-follower replication continuously copies new records from the partition leader to all assigned followers
        - ISR set is maintained by removing followers that fall behind the leader by more than the configured lag threshold
        - Leader election selects a new leader from the current ISR set within seconds when the existing leader becomes unavailable
        - High watermark advances only after all ISR members acknowledge the offset, ensuring consumers never read uncommitted data
        pitfalls:
        - ISR shrinking to empty
        - Unclean leader election
        - Data loss on failover
        concepts:
        - Replication
        - ISR
        - High watermark
        - Exactly-once semantics
        skills:
        - Consensus algorithms
        - Fault tolerance patterns
        - Data durability guarantees
        - Replication lag monitoring
        - Distributed transactions
        deliverables:
        - Leader election per partition that selects a new leader from the in-sync replica set when the current leader fails
        - In-sync replica (ISR) tracking that maintains the set of followers whose logs are fully caught up with the leader
        - Follower fetch protocol that replicates new records from the leader to keep follower logs synchronized
        - High watermark advancement that tracks the offset up to which all ISR members have replicated for safe consumer reads
        estimated_hours: 20-37
    - id: time-series-db
      name: Time-Series Database
      description: Optimized for time-stamped data with compression
      difficulty: advanced
      estimated_hours: '180'
      essence: LSM-tree storage with time-partitioned data blocks using specialized compression algorithms (delta-of-delta for timestamps, XOR for float values) combined with write-ahead logging and in-memory buffering to achieve high write throughput while enabling efficient time-range scans through columnar layout and downsampling aggregations.
      why_important: Building this teaches you production database engineering fundamentals including write-optimized storage structures, compression algorithm implementation, and query optimization‚Äîskills directly applicable to building high-performance backend systems and understanding how real-world databases like InfluxDB and TimescaleDB work internally.
      learning_outcomes:
      - Understand time-series data characteristics and access patterns
      - Implement columnar storage with compression algorithms
      - Build efficient time-range query execution
      - Design downsampling and aggregation pipelines
      - Create retention and compaction policies
      skills:
      - Time-series data modeling
      - Columnar storage
      - Delta encoding and compression
      - Write-ahead logging
      - Retention policies
      - Continuous queries
      tags:
      - advanced
      - aggregations
      - compression
      - databases
      - downsampling
      - retention
      architecture_doc: architecture-docs/time-series-db/index.md
      languages:
        recommended:
        - Go
        - Rust
        - C++
        also_possible: []
      resources:
      - name: Write a TSDB Engine from Scratch
        url: https://nakabonne.dev/posts/write-tsdb-from-scratch/
        type: tutorial
      - name: InfluxDB Documentation
        url: https://docs.influxdata.com/
        type: documentation
      - name: 'Gorilla: Facebook''s TSDB Paper'
        url: https://www.vldb.org/pvldb/vol8/p1816-teller.pdf
        type: paper
      - name: TimescaleDB Documentation
        url: https://docs.timescale.com/
        type: documentation
      - name: LSM Trees Explained
        url: https://medium.com/@dwivedi.ankit21/lsm-trees-the-go-to-data-structure-for-databases-search-engines-and-more-c3a48fa469d2
        type: article
      prerequisites:
      - type: project
        id: btree-impl
        name: B-tree Implementation
      - type: skill
        name: Database engine basics
      - type: project
        id: distributed-cache
        name: Distributed Cache
      milestones:
      - id: time-series-db-m1
        name: Storage Engine
        description: Time-series optimized storage with compression
        acceptance_criteria:
        - Design time series storage format with columnar layout for timestamps and values
        - Implement column-oriented storage separating timestamps, values, and tags
        - Handle high write throughput sustaining thousands of points per second
        - Support efficient range queries scanning only relevant time blocks
        pitfalls:
        - Not aligning block sizes with filesystem page sizes
        - Forgetting to handle clock skew in distributed systems
        - Over-compressing hot data that needs frequent access
        - Not leaving buffer space for block metadata
        - Choosing fixed compression without profiling actual data patterns
        concepts:
        - Block-based storage layout for temporal locality
        - Timestamp delta encoding for compression efficiency
        - Gorilla compression algorithm for floating-point values
        - Memory-mapped files for zero-copy data access
        - Block indexing with min/max timestamps
        skills:
        - Columnar storage
        - Delta encoding
        - Run-length encoding
        deliverables:
        - Time-structured merge tree (TSM) implementation
        - Delta-of-delta timestamp compression reducing storage for sequential timestamps
        - Gorilla float compression algorithm encoding similar float values efficiently
        - Dictionary encoding for tags and labels mapping strings to integer IDs
        - Block-based storage with an index mapping time ranges to data blocks
        - Memory-mapped file access for efficient random reads without user-space buffering
        estimated_hours: '36'
      - id: time-series-db-m2
        name: Write Path
        description: High-throughput ingestion with buffering
        acceptance_criteria:
        - Buffer writes in memory and persist via WAL before acknowledging
        - Batch writes flush to storage when memtable reaches size threshold
        - Handle out-of-order data by merging late points during compaction
        - Implement write acknowledgment confirming durability to the client
        pitfalls:
        - Not implementing proper WAL rotation and cleanup
        - Blocking writes when buffers fill instead of applying back-pressure
        - Ignoring out-of-order timestamps in write path
        - Not fsyncing WAL before acknowledging writes
        - Creating memory leaks with unbounded buffering
        concepts:
        - Lock-free ring buffers for concurrent writes
        - Write-ahead log for durability guarantees
        - Batching strategy to amortize disk I/O costs
        - Memory pool allocation to reduce GC pressure
        - Back-pressure mechanisms for overload protection
        skills:
        - Write batching
        - WAL
        - Memory management
        deliverables:
        - Write-ahead log ensuring durability before acknowledging writes
        - In-memory buffer (memtable) for writes
        - Batch point ingestion API accepting multiple data points in one call
        - Out-of-order write handling inserting late-arriving points into correct positions
        - Series cardinality tracking monitoring the number of unique time series
        - Backpressure mechanism throttling writes when the memtable is near capacity
        estimated_hours: '36'
      - id: time-series-db-m3
        name: Query Engine
        description: Efficient time-range queries and aggregations
        acceptance_criteria:
        - Support time range queries returning points within start and end bounds
        - Implement aggregation functions producing correct results over time windows
        - Support downsampling returning lower-resolution summaries of raw data
        - Handle complex filter expressions combining tag predicates with boolean logic
        pitfalls:
        - Not pushing filters down to storage layer early
        - Loading entire time ranges into memory before filtering
        - Forgetting to handle gaps in time-series data
        - Not optimizing for common aggregation patterns
        - Inefficient join strategies for multi-series queries
        concepts:
        - Time-based sharding for query parallelization
        - Skip list indexes for efficient range scanning
        - Late materialization to reduce memory during scans
        - Pushdown aggregation to minimize data movement
        - Query result streaming to handle large datasets
        skills:
        - Query planning
        - Aggregations
        - Downsampling
        deliverables:
        - Time-range predicate pushdown skipping blocks outside the query window
        - Tag-based filtering and indexing using inverted index for fast label lookups
        - Built-in aggregation functions (sum, avg, min, max, count)
        - Windowed aggregations supporting tumbling and sliding time intervals
        - GROUP BY time bucket queries partitioning results into fixed-width intervals
        - Last and first value query optimization using block-level metadata shortcuts
        estimated_hours: '36'
      - id: time-series-db-m4
        name: Retention & Compaction
        description: Automatic data lifecycle management
        acceptance_criteria:
        - Enforce data TTL by automatically deleting expired data blocks
        - Implement storage compaction merging overlapping blocks and reclaiming space
        - Support downsampling for old data reducing storage while preserving trends
        - Handle storage reclamation freeing disk space after compaction and expiry
        pitfalls:
        - Compacting too aggressively and blocking writes
        - Not preserving enough resolution during downsampling
        - Deleting data before compaction completes
        - Ignoring disk space monitoring during retention enforcement
        - Running compaction during peak query load
        concepts:
        - Tiered storage with hot/warm/cold data separation
        - Background compaction without blocking queries
        - Tombstone management for deleted data
        - Downsampling algorithms that preserve statistical properties
        - Resource throttling for background maintenance tasks
        skills:
        - Retention policies
        - Compaction
        - Downsampling
        deliverables:
        - TTL-based retention policies specifying maximum data age per measurement
        - Automatic data expiration and deletion
        - Background compaction process merging small TSM files into larger optimized ones
        - Level-based compaction strategy promoting data through compaction tiers
        - Continuous downsampling queries pre-aggregating old data into lower resolution
        - Rollup aggregation storage persisting pre-computed summaries for fast historical queries
        estimated_hours: '36'
      - id: time-series-db-m5
        name: Query Language & API
        description: Expressive query interface for time-series
        acceptance_criteria:
        - Implement time series query language parsing SELECT with time-range predicates
        - Support aggregation functions within query language expressions
        - Handle GROUP BY time intervals with configurable bucket widths
        - Expose REST and gRPC API endpoints for both writes and queries
        pitfalls:
        - Not validating time range boundaries in queries
        - Allowing unbounded queries that scan entire dataset
        - Poor error messages for malformed time expressions
        - Not handling timezone conversions correctly
        - Exposing internal storage details in query language
        concepts:
        - Recursive descent parsing for query language
        - Time duration literals and relative time expressions
        - Aggregation function pushdown optimization
        - Rate and derivative functions for time-series analysis
        - Query result caching with time-based invalidation
        skills:
        - Query parsing
        - API design
        - PromQL/InfluxQL
        deliverables:
        - SQL-like query language with time extensions
        - Flux-style functional query pipeline chaining filter, map, and aggregate operations
        - HTTP write API (Line Protocol compatible)
        - Query API with multiple output formats
        - Prometheus remote read and write API for integration with Prometheus ecosystem
        - Grafana data source compatibility endpoint serving queries in Grafana format
        estimated_hours: '36'
    - id: stream-processing-engine
      name: Stream Processing Engine
      description: Real-time data processing like Flink with windowing, state
      difficulty: expert
      estimated_hours: '100'
      essence: Distributed dataflow coordination with barrier-based checkpointing and watermark-driven event-time semantics, handling out-of-order events through time-window aggregation while maintaining transactional state consistency across parallel operators for exactly-once processing guarantees.
      why_important: Building this teaches you distributed systems fault tolerance, the complexities of time in distributed computing, and state consistency mechanisms that underpin production streaming platforms like Flink, Kafka Streams, and real-time analytics systems.
      learning_outcomes:
      - Understand stream processing fundamentals
      - Implement windowing and watermarks
      - Build fault-tolerant stateful processing
      - Design exactly-once delivery systems
      skills:
      - Event time processing
      - Windowing
      - State management
      - Checkpointing
      - Watermarks
      - Exactly-once semantics
      tags:
      - aggregation
      - expert
      - framework
      - game-dev
      - streaming
      - watermarks
      - windowing
      architecture_doc: architecture-docs/stream-processing-engine/index.md
      languages:
        recommended:
        - Java
        - Scala
        - Go
        also_possible: []
      resources:
      - name: Apache Flink Documentation
        url: https://nightlies.apache.org/flink/flink-docs-lts/
        type: documentation
      - name: Stateful Stream Processing Concepts
        url: https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/stateful-stream-processing/
        type: documentation
      - name: Streaming Systems Book - Watermarks
        url: https://www.oreilly.com/library/view/streaming-systems/9781491983867/ch03.html
        type: book
      - name: Exactly-Once Semantics in Kafka
        url: https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/
        type: article
      - name: Checkpointing Best Practices
        url: https://propelius.ai/blogs/checkpointing-in-stream-processing-best-practices
        type: article
      prerequisites:
      - type: skill
        name: Distributed systems
      - type: skill
        name: Message queues
      - type: skill
        name: State machines
      milestones:
      - id: stream-processing-engine-m1
        name: Stream Abstraction
        description: Core stream data structures and operators
        acceptance_criteria:
        - Define stream data structure with typed record schema and metadata
        - Support operators map, filter, and flatMap with correct element-wise semantics
        - Handle operator chaining producing a composed pipeline from sequential calls
        - Implement parallel processing across partitions for keyed streams
        pitfalls:
        - Materializing entire streams in memory instead of lazy processing
        - Not handling null values in transformation operators
        - Forgetting to implement proper equals/hashCode for custom data types
        - Creating stateful operations in supposedly stateless map functions
        concepts:
        - Functional transformations (map, filter, flatMap) on infinite data streams
        - Lazy evaluation and operator fusion for memory efficiency
        - Backpressure mechanisms to handle slow consumers
        - Partitioning strategies for parallel stream processing
        skills:
        - Stream API
        - Operators
        - Transformations
        deliverables:
        - DataStream class representing an unbounded sequence of typed records
        - Map, filter, and flatMap operators transforming stream elements
        - KeyBy partitioning operator grouping stream elements by extracted key
        - Operator chaining mechanism composing transformations into a processing pipeline
        - Source and sink interfaces for pluggable data ingestion and output
        - Execution graph builder that compiles operator chain into a runnable DAG
        estimated_hours: '20'
      - id: stream-processing-engine-m2
        name: Windowing
        description: Time-based windowing for aggregations
        acceptance_criteria:
        - Support tumbling windows with fixed time duration and correct boundary alignment
        - Support sliding windows with configurable window size and slide interval
        - Support session windows that merge when gap between events is below threshold
        - Handle window triggers firing computation at correct count or time thresholds
        pitfalls:
        - Confusing event time windows with processing time windows
        - Not accounting for window overlap in sliding windows
        - Triggering computation before window has sufficient data
        - Memory leaks from never-closing session windows
        concepts:
        - Tumbling, sliding, and session window semantics
        - Window triggers determine when computation executes
        - Window evictors remove elements from window state
        - Allowed lateness for handling late-arriving data
        skills:
        - Window types
        - Triggers
        - Evictors
        deliverables:
        - Tumbling window assigner grouping elements into fixed non-overlapping intervals
        - Sliding window assigner grouping elements into overlapping intervals with configurable slide
        - Session window assigner merging elements within an inactivity gap threshold
        - Window assigner interface abstracting window assignment from processing logic
        - Triggers for count-based and time-based window evaluation firing
        - Late data handler routing out-of-window elements to a side output
        estimated_hours: '20'
      - id: stream-processing-engine-m3
        name: Event Time & Watermarks
        description: Handle out-of-order events with watermarks
        acceptance_criteria:
        - Track event time vs processing time
        - Generate watermarks that monotonically advance based on observed event timestamps
        - Handle late events by routing them to a side output or late-data handler
        - Support allowed lateness so windows accept events arriving after watermark passes
        pitfalls:
        - Generating watermarks based on processing time instead of event time
        - Not handling out-of-order data with sufficient allowed lateness
        - Watermark stalls when one partition stops sending data
        - Dropping late data without logging or side output
        concepts:
        - Event time extracted from record payload timestamps
        - Watermarks indicate progress in event time domain
        - Watermark generation strategies (periodic vs punctuated)
        - Allowed lateness windows accept late events within bounds
        - Monotonically increasing watermark constraint
        skills:
        - Event time
        - Watermark generation
        - Lateness
        deliverables:
        - Event time extractor pulling timestamps from record payload fields
        - Watermark generation strategies including periodic and punctuated approaches
        - Bounded out-of-orderness watermark allowing configurable maximum event delay
        - Watermark propagation logic forwarding minimum watermark across parallel operators
        - Idle source detection advancing watermarks when a partition has no new data
        - Allowed lateness configuration specifying how long late events are accepted
        estimated_hours: '20'
      - id: stream-processing-engine-m4
        name: Stateful Processing
        description: Manage operator state with checkpointing
        acceptance_criteria:
        - Maintain operator state across records with correct per-key isolation
        - Implement checkpointing that captures globally consistent state snapshot
        - Support state recovery restoring exact pre-failure state from checkpoint
        - Handle state migration when operator parallelism changes during rescale
        pitfalls:
        - Not clearing state causing memory leaks over time
        - Checkpoint failures from blocking I/O in operator code
        - Mixing keyed and non-keyed state in same operator
        - Not configuring state backend for production workloads
        - Race conditions when accessing state from multiple threads
        concepts:
        - Keyed state scoped to individual keys in stream
        - State backends (memory, RocksDB, filesystem) for persistence
        - Asynchronous checkpointing for fault tolerance
        - Changelog streams for state reconstruction
        - State TTL to prevent unbounded growth
        skills:
        - State backends
        - Keyed state
        - Checkpointing
        deliverables:
        - ValueState, ListState, and MapState abstractions for keyed operator state
        - State backend interface abstracting storage behind memory or persistent backends
        - RocksDB state backend storing large keyed state on local disk with caching
        - Checkpoint barrier injection coordinating consistent snapshots across operators
        - Async checkpointing writing state snapshots without blocking record processing
        - State recovery loader restoring operator state from the latest checkpoint on restart
        estimated_hours: '20'
      - id: stream-processing-engine-m5
        name: Exactly-Once Semantics
        description: Guarantee exactly-once processing
        acceptance_criteria:
        - Implement idempotent sinks that produce the same output on replay
        - Support transactional writes that commit atomically with checkpoint completion
        - Handle checkpoint barriers aligning across parallel operator instances
        - Verify no duplicate output records appear after crash recovery and replay
        pitfalls:
        - Using at-least-once sinks without idempotency guarantees
        - Not implementing proper transaction rollback on failure
        - Checkpoint alignment stalls with slow operators
        - Forgetting to enable checkpointing for exactly-once mode
        - Sink transactions timing out during long checkpoint intervals
        concepts:
        - Two-phase commit protocol for transactional sinks
        - Idempotent operations enable safe retries
        - Distributed snapshots via Chandy-Lamport algorithm
        - Transaction coordinators manage commit/abort decisions
        - Barrier alignment ensures consistent checkpoints
        skills:
        - Two-phase commit
        - Idempotent sinks
        - Transaction coordination
        deliverables:
        - Transactional source connectors supporting offset commit on checkpoint completion
        - Two-phase commit sink connectors pre-committing output and finalizing on checkpoint
        - Checkpoint completion callbacks notifying sources and sinks of successful snapshots
        - Transaction coordinator managing distributed commit across all sink operators
        - Abort and recovery logic rolling back uncommitted transactions after failure
        - End-to-end exactly-once delivery guarantee from source through sink
        estimated_hours: '20'
    - id: capstone-database-engine
      name: 'Capstone: Complete Database Engine'
      description: Build a complete relational database engine from scratch ‚Äî SQL parser, query planner, B-tree storage engine, write-ahead log, MVCC transactions, and a client protocol. This capstone integrates all database projects into a working RDBMS.
      difficulty: expert
      estimated_hours: 100-140
      essence: SQL parsing and query optimization pipeline feeding into page-based B-tree storage with MVCC snapshot isolation, coordinated through write-ahead logging for crash recovery and exposed via PostgreSQL wire protocol compatibility.
      why_important: Databases are the most critical piece of infrastructure in any application. Building one from scratch gives you unmatched understanding of indexing, transactions, query optimization, and storage ‚Äî knowledge that directly improves your ability to design schemas, write efficient queries, and debug production database issues.
      learning_outcomes:
      - Implement a complete SQL pipeline from parsing to execution
      - Build a cost-based query optimizer with join ordering and index selection
      - Design a page-based storage engine with B-tree indexes
      - Implement MVCC transactions with snapshot isolation
      - Build write-ahead logging for crash recovery with ARIES protocol
      - Design a wire protocol compatible with existing database clients
      - Optimize query execution with vectorized processing
      skills:
      - SQL Parsing
      - Query Optimization
      - B-tree Indexing
      - MVCC
      - Write-Ahead Logging
      - Storage Engines
      - Transaction Processing
      tags:
      - databases
      - storage-engines
      - capstone
      - systems-programming
      architecture_doc: architecture-docs/capstone-database-engine/index.md
      languages:
        recommended:
        - Rust
        - C
        - Go
        also_possible:
        - Java
        - C++
      resources:
      - name: Architecture of a Database System
        url: https://dsf.berkeley.edu/papers/fntdb07-architecture.pdf
        type: paper
      - name: CMU Database Systems Course
        url: https://15445.courses.cs.cmu.edu/
        type: tutorial
      - name: SQLite Architecture
        url: https://www.sqlite.org/arch.html
        type: documentation
      prerequisites:
      - type: project
        id: sql-parser
        name: SQL Parser
      - type: project
        id: query-optimizer
        name: Query Optimizer
      - type: project
        id: btree-impl
        name: B-tree Implementation
      - type: project
        id: wal-impl
        name: WAL Implementation
      milestones:
      - id: capstone-database-engine-m1
        name: SQL Frontend & Catalog
        description: Implement SQL parser (SELECT, INSERT, UPDATE, DELETE, CREATE TABLE), query planner, and system catalog storing table schemas and index metadata.
        acceptance_criteria:
        - Parser handles SELECT with WHERE, JOIN, ORDER BY, GROUP BY, LIMIT
        - 'DDL: CREATE TABLE, DROP TABLE, CREATE INDEX'
        - System catalog stores table schemas, column types, and index metadata
        - Logical plan generation from parsed SQL with basic validation
        pitfalls:
        - Ambiguous column references in JOINs
        - NULL handling in WHERE clauses
        - Catalog concurrency
        concepts:
        - SQL grammar
        - logical plan
        - system catalog
        - type validation
        skills:
        - SQL parsing
        - Query planning
        - Catalog design
        deliverables:
        - SQL parser, logical planner, and system catalog
        - Recursive descent SQL parser generating abstract syntax trees for DDL and DML statements
        - Logical query planner transforming AST into logical plan with projection, selection, and join operators
        - System catalog storing table schemas, column types, constraints, and index definitions
        estimated_hours: '18'
      - id: capstone-database-engine-m2
        name: Storage Engine & B-tree Index
        description: Build a page-based storage engine with a buffer pool manager and B+ tree indexes supporting point lookups and range scans.
        acceptance_criteria:
        - Fixed-size pages (4KB) with slotted page format for variable-length records
        - Buffer pool with LRU eviction and dirty page tracking
        - B+ tree index supporting insert, delete, point lookup, and range scan
        - Table heap with sequential scan and index scan execution paths
        pitfalls:
        - Page splits causing tree rebalancing cascade
        - Buffer pool deadlocks
        - Overflow pages for large records
        concepts:
        - slotted pages
        - buffer pool
        - B+ tree
        - page splits
        - LRU eviction
        skills:
        - Storage engines
        - Buffer management
        - B+ tree implementation
        deliverables:
        - Page-based storage engine with buffer pool and B+ tree indexes
        - Slotted page layout supporting variable-length records with efficient space utilization
        - LRU buffer pool manager caching frequently accessed pages with dirty page tracking
        - B+ tree implementation supporting concurrent insertions, deletions, and range scans with page splitting
        estimated_hours: '22'
      - id: capstone-database-engine-m3
        name: Query Execution & Optimization
        description: Implement a volcano-style query executor with hash join, sort-merge join, and a cost-based optimizer that chooses join order and index usage.
        acceptance_criteria:
        - 'Volcano iterator model: each operator implements open/next/close'
        - Hash join and sort-merge join for multi-table queries
        - Cost-based optimizer estimates cardinality and chooses join order
        - 'Index selection: optimizer uses indexes when selectivity is high enough'
        pitfalls:
        - Cardinality estimation errors cascading through plan
        - Hash join memory overflow
        - Optimizer search space explosion
        concepts:
        - volcano model
        - hash join
        - sort-merge join
        - cardinality estimation
        - cost model
        skills:
        - Query execution
        - Join algorithms
        - Cost-based optimization
        deliverables:
        - Query executor with join operators and cost-based optimizer
        - Volcano-style iterator model executing operators with demand-driven pull semantics
        - Hash join and sort-merge join implementations with memory-aware spilling to disk
        - Cost-based optimizer using cardinality estimates and cost models to choose optimal join order
        estimated_hours: '22'
      - id: capstone-database-engine-m4
        name: Transactions & WAL
        description: Implement MVCC with snapshot isolation for concurrent transactions and write-ahead logging for crash recovery.
        acceptance_criteria:
        - 'MVCC: each transaction sees a consistent snapshot of the database'
        - Snapshot isolation prevents dirty reads and non-repeatable reads
        - 'WAL: all changes are logged before being applied to data pages'
        - 'Crash recovery: replay WAL to restore database to last committed state'
        pitfalls:
        - Write skew anomaly in snapshot isolation
        - WAL growing without checkpointing
        - Deadlock between concurrent transactions
        concepts:
        - MVCC
        - snapshot isolation
        - write-ahead logging
        - ARIES recovery
        - transaction IDs
        skills:
        - MVCC
        - Transaction isolation
        - Write-ahead logging
        - Crash recovery
        deliverables:
        - MVCC transactions and WAL-based crash recovery
        - MVCC implementation maintaining multiple row versions with transaction ID visibility checking
        - Snapshot isolation providing consistent reads without blocking concurrent writes
        - ARIES-style write-ahead logging with redo/undo phases for crash recovery
        estimated_hours: '22'
      - id: capstone-database-engine-m5
        name: Wire Protocol & Client Interface
        description: Implement a PostgreSQL-compatible wire protocol so standard database clients (psql, JDBC, database libraries) can connect to your database.
        acceptance_criteria:
        - Server accepts TCP connections and speaks PostgreSQL wire protocol (v3)
        - Clients can connect using psql or any PostgreSQL library
        - Query results are correctly formatted with column types and row data
        - Connection pooling support with concurrent client sessions
        pitfalls:
        - Protocol version negotiation failures
        - Type conversion mismatches
        - Connection state cleanup on disconnect
        concepts:
        - PostgreSQL wire protocol
        - message framing
        - type OIDs
        - connection lifecycle
        skills:
        - Wire protocol
        - PostgreSQL compatibility
        - Connection management
        deliverables:
        - PostgreSQL-compatible wire protocol server
        - Message parser handling PostgreSQL protocol startup, simple query, and extended query flows
        - Type OID mapping translating internal types to PostgreSQL-compatible type identifiers
        - Connection pool supporting multiple concurrent client connections with authentication and SSL
        estimated_hours: '12'
- id: distributed
  name: Distributed & Cloud
  icon: ‚òÅÔ∏è
  subdomains:
  - name: Distributed Systems
  - name: Cloud & DevOps
  - name: Consensus
  - name: Blockchain
  projects:
    beginner:
    - id: service-discovery
      name: Service Discovery
      description: Registry, health checks
      difficulty: beginner
      estimated_hours: 10-15
      essence: Dynamic service registration and lookup using in-memory registries or distributed key-value stores, implementing heartbeat-based health monitoring and TTL expiration to maintain an eventually-consistent directory of available service instances across a network.
      why_important: Building this teaches you the foundational infrastructure pattern behind production microservices platforms like Kubernetes and Consul, giving you practical experience with distributed systems challenges like network partitions, instance failure detection, and eventual consistency that are critical for backend and infrastructure engineering roles.
      learning_outcomes:
      - Implement service registration and deregistration with TTL-based leases
      - Design health check mechanisms using heartbeat intervals and failure thresholds
      - Build HTTP REST APIs for registry CRUD operations
      - Handle concurrent service updates using optimistic locking or versioning
      - Implement watch mechanisms for real-time service discovery updates
      - Debug network partition scenarios and stale service entry cleanup
      - Design consistent hashing or round-robin for client-side load balancing
      - Test failure scenarios including service crashes and network timeouts
      skills:
      - Distributed key-value stores
      - REST API design
      - Health check patterns
      - Service registry architecture
      - Concurrent data structures
      - Heartbeat protocols
      - HTTP server implementation
      - Failure detection algorithms
      tags:
      - beginner-friendly
      - consul
      - dns-based
      - go
      - health-checks
      - javascript
      - python
      - registry
      - service
      architecture_doc: architecture-docs/service-discovery/index.md
      languages:
        recommended:
        - Go
        - Python
        - JavaScript
        also_possible:
        - Java
        - Rust
      resources:
      - name: Service Discovery Patterns
        url: https://microservices.io/patterns/service-registry.html
        type: article
      - name: Consul Service Discovery - HashiCorp Docs
        url: https://developer.hashicorp.com/consul/docs/use-case/service-discovery
        type: documentation
      - name: Consul Service Discovery Beginners Guide
        url: https://devopscube.com/consul-service-discovery-beginners-guide/
        type: tutorial
      prerequisites:
      - type: skill
        name: HTTP basics
      - type: skill
        name: Networking concepts
      milestones:
      - id: service-discovery-m1
        name: Service Registry
        description: Build basic service registration and lookup.
        acceptance_criteria:
        - Register service instance with name, host, port, and metadata
        - Deregister service instance by unique instance identifier
        - List all healthy instances of a service by service name
        - Return complete list of all registered services and instances
        pitfalls:
        - Not handling duplicate registrations
        - Race conditions
        - Memory growth without cleanup
        concepts:
        - Service registry pattern
        - Instance identity
        - Lookup by name
        skills:
        - Service registration APIs
        - Concurrent data structure handling
        - In-memory data storage
        - Service instance modeling
        deliverables:
        - Registration API for services to announce their availability
        - Service metadata storage including name, host, port, and tags
        - TTL-based automatic deregistration after heartbeat timeout
        - Health check integration marking unhealthy instances as unavailable
        estimated_hours: 2-3
      - id: service-discovery-m2
        name: Health Checking
        description: Add health checks to detect failed services.
        acceptance_criteria:
        - Expose HTTP /health endpoint on each registered service
        - Poll health endpoints at configurable check intervals
        - Remove instances failing consecutive health checks from registry
        - Configure health check interval and failure threshold per service
        pitfalls:
        - Network partition false positives
        - Check timeout too short
        - Not handling transient failures
        concepts:
        - Health checking
        - Failure detection
        - Background tasks
        skills:
        - Background task scheduling
        - HTTP health endpoint implementation
        - Service state management
        - Timeout configuration
        - Periodic polling patterns
        deliverables:
        - Health check execution polling service endpoints periodically
        - Support for HTTP, TCP, and gRPC health check protocols
        - Unhealthy instance removal from active service registry
        - Health status API exposing current health of all instances
        estimated_hours: 3-4
      - id: service-discovery-m3
        name: HTTP API
        description: Expose registry operations via HTTP API.
        acceptance_criteria:
        - POST /services registers new service instance and returns ID
        - DELETE /services/{name}/{id} deregisters the specified service instance
        - GET /services/{name} returns list of healthy service instances
        - GET /services returns complete list of all registered services
        pitfalls:
        - Missing validation
        - Not thread-safe
        - Error handling
        concepts:
        - REST API design
        - HTTP methods
        - JSON serialization
        skills:
        - RESTful endpoint design
        - HTTP request/response handling
        - JSON marshaling and unmarshaling
        - API route definition
        - Input validation
        deliverables:
        - Service lookup endpoint returning instances by service name
        - Health status endpoint showing current state of all services
        - Service registration endpoint accepting new instance announcements
        - Query filtering supporting lookup by tags or metadata attributes
        estimated_hours: 2-3
    - id: rpc-basic
      name: RPC Framework (Basic)
      description: Remote procedure calls
      difficulty: beginner
      estimated_hours: 10-15
      essence: Structured data serialization, socket-based client-server communication, and transparent method invocation across process boundaries using request routing, network I/O handling, and remote call abstraction mechanisms.
      why_important: Building an RPC framework teaches foundational distributed systems concepts used in microservices, gRPC, and modern APIs, while developing crucial skills in network programming and data serialization that are essential for backend engineering roles.
      learning_outcomes:
      - Design binary message protocols for request/response serialization
      - Implement TCP socket programming for client-server communication
      - Build dynamic proxy mechanisms to abstract remote method calls
      - Handle network errors, timeouts, and connection management
      - Serialize and deserialize structured data efficiently
      - Implement method dispatching and routing on the server side
      - Design protocol specifications with versioning and compatibility
      - Debug distributed system issues across network boundaries
      skills:
      - TCP/IP Networking
      - Binary Serialization
      - Client-Server Architecture
      - Protocol Design
      - Socket Programming
      - Error Handling
      - Method Dispatching
      - Network Debugging
      tags:
      - beginner-friendly
      - go
      - java
      - marshalling
      - networking
      - protocols
      - python
      - serialization
      - stubs
      architecture_doc: architecture-docs/rpc-basic/index.md
      languages:
        recommended:
        - Python
        - Go
        - Java
        also_possible:
        - JavaScript
        - Rust
      resources:
      - name: gRPC Concepts
        url: https://grpc.io/docs/what-is-grpc/core-concepts/
        type: documentation
      - name: JSON-RPC Spec
        url: https://www.jsonrpc.org/specification
        type: specification
      prerequisites:
      - type: skill
        name: TCP sockets
      - type: skill
        name: JSON/serialization
      - type: skill
        name: Client-server architecture
      milestones:
      - id: rpc-basic-m1
        name: Message Protocol
        description: Define request/response message format.
        acceptance_criteria:
        - Request includes method name, parameter list, and unique request ID
        - Response includes result or error paired with matching request ID
        - Serialize and deserialize messages using JSON encoding
        - Handle different parameter types including strings, numbers, and objects
        pitfalls:
        - ID mismatch
        - Serialization errors
        - Missing error handling
        concepts:
        - RPC protocol
        - Message serialization
        - Request-response pattern
        skills:
        - Protocol design and specification
        - Binary serialization formats
        - Message framing and parsing
        - Schema versioning
        deliverables:
        - Request message format with method name, parameters, and ID
        - Response message format with result, error, and matching ID
        - Error format with code, message, and optional data fields
        - Serialization using JSON or msgpack for wire encoding
        estimated_hours: 2-3
      - id: rpc-basic-m2
        name: Server Implementation
        description: Build RPC server that handles method calls.
        acceptance_criteria:
        - Register callable functions by name in method registry
        - Parse incoming request messages extracting method and parameters
        - Execute registered method with provided parameters and return result
        - Handle method-not-found and execution errors with error responses
        pitfalls:
        - Method not found
        - Parameter count mismatch
        - Blocking calls
        concepts:
        - Method registry
        - Request dispatching
        - Error handling
        skills:
        - Socket programming and server loops
        - Dynamic method dispatch
        - Concurrent request handling
        - Exception propagation across network
        deliverables:
        - TCP server listening for incoming client connections
        - Method registry mapping function names to callable handlers
        - Request dispatch parsing message and invoking registered method
        - Response serialization and sending back to connected client
        estimated_hours: 3-4
      - id: rpc-basic-m3
        name: Client Implementation
        description: Build RPC client with method proxying.
        acceptance_criteria:
        - Connect to RPC server via TCP socket connection
        - Send request messages and wait for matching response by ID
        - Provide proxy object allowing natural method call syntax
        - Handle call timeouts raising exception after deadline expires
        pitfalls:
        - Connection management
        - Timeout handling
        - ID tracking
        concepts:
        - RPC client
        - Method proxying
        - Connection pooling
        skills:
        - Stub generation and dynamic proxies
        - Connection lifecycle management
        - Asynchronous call patterns
        - Request timeout implementation
        deliverables:
        - TCP client connecting to remote RPC server endpoint
        - Method call API sending request and blocking for response
        - Transparent response handling returning result or raising error
        - Configurable timeout aborting calls that take too long
        estimated_hours: 3-4
    intermediate:
    - id: load-balancer-basic
      name: Load Balancer (Basic)
      description: Round-robin
      difficulty: intermediate
      estimated_hours: 15-20
      essence: HTTP traffic distribution through reverse proxying with request forwarding, backend health monitoring, and algorithmic server selection to achieve fault-tolerant multi-server request routing.
      why_important: Building a load balancer teaches core distributed systems concepts that power production infrastructure at scale, from reverse proxy mechanics to failure detection‚Äîskills directly applicable to DevOps, backend engineering, and systems architecture roles.
      learning_outcomes:
      - Implement HTTP reverse proxy with request forwarding and response routing
      - Design round-robin request distribution algorithm with server pool rotation
      - Build active health check system with periodic backend probing
      - Handle backend server failures with automatic rotation out of service pool
      - Implement weighted load balancing algorithms for non-uniform server capacity
      - Manage connection pooling and upstream server connection lifecycle
      - Debug network issues including connection timeouts and backend failures
      - Configure health check intervals, retry logic, and failure thresholds
      skills:
      - Reverse Proxy Design
      - HTTP Protocol Handling
      - Health Check Monitoring
      - Request Routing Algorithms
      - Connection Pool Management
      - Failure Detection
      - Distributed Systems Concepts
      - Network Programming
      tags:
      - go
      - health-checks
      - intermediate
      - javascript
      - networking
      - python
      - round-robin
      architecture_doc: architecture-docs/load-balancer-basic/index.md
      languages:
        recommended:
        - Go
        - Python
        - JavaScript
        also_possible:
        - Rust
        - Java
      resources:
      - name: Build Your Own Load Balancer
        url: https://codingchallenges.fyi/challenges/challenge-load-balancer/
        type: tutorial
      - name: Load Balancer in Go
        url: https://kasvith.me/posts/lets-create-a-simple-lb-go/
        type: tutorial
      prerequisites:
      - type: skill
        name: HTTP protocol
      - type: skill
        name: TCP networking
      - type: skill
        name: Concurrency basics
      milestones:
      - id: load-balancer-basic-m1
        name: HTTP Proxy Foundation
        description: Build basic HTTP reverse proxy functionality.
        acceptance_criteria:
        - Incoming HTTP requests on the proxy port are accepted and parsed with method, path, and headers
        - Requests are forwarded to a single configured backend server preserving method, path, and body
        - Backend responses including status code, headers, and body are returned to the originating client
        - Connection errors to the backend return a 502 Bad Gateway response to the client
        - All proxied requests are logged with timestamp, method, path, backend, and response status
        pitfalls:
        - Not forwarding all headers
        - Not handling request body
        - Connection timeouts
        concepts:
        - Reverse proxy
        - HTTP forwarding
        - Request/response handling
        skills:
        - HTTP protocol handling
        - Network programming
        - Request/response transformation
        - Connection management
        deliverables:
        - Request forwarder that relays incoming HTTP requests to the configured backend server
        - Response forwarder that returns the backend server's response to the original client
        - Header manipulation module that adds X-Forwarded-For and X-Forwarded-Proto headers to proxied requests
        - Connection handler that manages keep-alive, timeouts, and connection pooling to backends
        estimated_hours: 3-4
      - id: load-balancer-basic-m2
        name: Round Robin Distribution
        description: Distribute requests across multiple backends.
        acceptance_criteria:
        - Backend server list is configurable via a configuration file or API with hot-reload support
        - Round-robin algorithm selects each backend in turn, cycling back to the first after the last
        - Requests are distributed evenly across all healthy backends with less than 5% skew over 1000 requests
        - Counter increment is thread-safe using atomic operations to prevent race conditions under concurrency
        - Backends marked as unhealthy are skipped and the next healthy backend is selected instead
        pitfalls:
        - Race condition in counter
        - Modulo with zero backends
        - Uneven distribution after backend changes
        concepts:
        - Round robin algorithm
        - Atomic operations
        - Backend pool
        skills:
        - Concurrent programming
        - Thread-safe data structures
        - State management
        - Algorithm implementation
        deliverables:
        - Backend server list manager that stores and validates the pool of available backend addresses
        - Round robin selector that cycles through backend servers in sequential order for each request
        - Request router that directs each incoming request to the backend chosen by the active algorithm
        - Backend cycling counter that wraps around to the first server after reaching the last one
        estimated_hours: 2-3
      - id: load-balancer-basic-m3
        name: Health Checks
        description: Implement active health checking of backends.
        acceptance_criteria:
        - Health checks run periodically at a configurable interval sending HTTP requests to each backend
        - Backends failing more than N consecutive health checks are marked unhealthy and removed from rotation
        - Unhealthy backends are excluded from the load balancing rotation until recovery is confirmed
        - Backends are restored to rotation after passing a configurable number of consecutive health checks
        - Health check interval, timeout, healthy threshold, and unhealthy threshold are all configurable
        pitfalls:
        - Health checks overwhelming backends
        - No healthy backends available
        - Thundering herd on recovery
        concepts:
        - Health checking
        - Failure detection
        - Graceful degradation
        skills:
        - Background task scheduling
        - Circuit breaker patterns
        - Monitoring and observability
        - Timeout handling
        deliverables:
        - Health check endpoint prober that sends HTTP GET requests to each backend's health path
        - Periodic health checker that runs checks at a configurable interval on all registered backends
        - Unhealthy backend remover that takes backends out of rotation after consecutive check failures
        - Recovery detector that restores backends to the rotation after consecutive successful health checks
        estimated_hours: 3-4
      - id: load-balancer-basic-m4
        name: Additional Algorithms
        description: Implement additional load balancing algorithms.
        acceptance_criteria:
        - Weighted round robin distributes requests proportionally to backend weights over many requests
        - Least connections tracks active connections per backend and routes to the one with the fewest
        - IP hash routes the same client IP to the same backend consistently enabling sticky sessions
        - Random selection distributes requests approximately evenly across backends over many requests
        - The active load balancing algorithm is configurable and can be switched at runtime without restart
        pitfalls:
        - Least connections not updating on response
        - IP hash inconsistent after backend changes
        - Weighted round robin integer overflow
        concepts:
        - Load balancing algorithms
        - Session affinity
        - Algorithm tradeoffs
        skills:
        - Hashing algorithms
        - Stateful session management
        - Performance optimization
        - Algorithm comparison and selection
        deliverables:
        - Least connections algorithm that routes each request to the backend with the fewest active connections
        - Weighted round robin algorithm that distributes requests proportionally to each backend's configured weight
        - IP hash algorithm that consistently maps a client IP address to the same backend server
        - Random selection algorithm that picks a backend uniformly at random for each request
        estimated_hours: 4-5
    - id: rate-limiter
      name: Rate Limiter
      description: Token bucket
      difficulty: intermediate
      estimated_hours: 10-15
      essence: Token-based quota management with atomic refill operations, concurrent request validation using locks and atomic counters, and cross-server state coordination through shared storage backends to enforce consistent rate limits in distributed deployments.
      why_important: Rate limiting is critical infrastructure in production systems to prevent resource exhaustion and abuse, and building one teaches you about concurrency control, distributed systems consistency, and performance-sensitive code that runs on every API request.
      learning_outcomes:
      - Implement token bucket algorithm with refill rate and burst capacity management
      - Handle concurrent request validation using atomic operations and locks
      - Design per-client tracking with efficient data structures and memory management
      - Build HTTP middleware that intercepts requests with minimal latency overhead
      - Implement distributed rate limiting using Redis with Lua scripts for atomicity
      - Handle race conditions in high-concurrency scenarios across multiple servers
      - Design sliding window counters as an alternative to token bucket
      - Debug and performance-tune code that processes thousands of requests per second
      skills:
      - Concurrency Control
      - Distributed Systems
      - Redis
      - Atomic Operations
      - HTTP Middleware
      - Algorithm Implementation
      - Race Condition Handling
      - Performance Optimization
      tags:
      - algorithms
      - go
      - intermediate
      - javascript
      - python
      - sliding-window
      - throttling
      - token-bucket
      architecture_doc: architecture-docs/rate-limiter/index.md
      languages:
        recommended:
        - Python
        - Go
        - JavaScript
        also_possible:
        - Java
        - Rust
      resources:
      - name: Token Bucket Algorithm
        url: https://en.wikipedia.org/wiki/Token_bucket
        type: documentation
      - name: Rate Limiting Strategies
        url: https://blog.bytebytego.com/p/rate-limiting-fundamentals
        type: article
      prerequisites:
      - type: skill
        name: Basic web server knowledge
      - type: skill
        name: Concurrency basics
      - type: skill
        name: Time handling
      milestones:
      - id: rate-limiter-m1
        name: Token Bucket Implementation
        description: Implement the core token bucket algorithm.
        acceptance_criteria:
        - Configure bucket with maximum capacity and tokens-per-second rate
        - Add tokens at configurable rate based on elapsed time since last refill
        - Consume specified number of tokens per incoming request
        - Return allow or deny decision based on token availability
        - Implement thread-safe access using locks or atomic operations
        pitfalls:
        - Race conditions without locking
        - Integer overflow in token calculation
        - Clock drift issues
        concepts:
        - Token bucket algorithm
        - Thread safety
        - Rate calculations
        skills:
        - Concurrent data structure implementation
        - Time-based algorithm design
        - Thread synchronization primitives
        - Numerical precision in calculations
        deliverables:
        - Configurable bucket capacity and refill rate parameters
        - Token consumption function deducting from available tokens
        - Token refill logic calculating additions based on elapsed time
        - Burst handling allowing short bursts up to bucket capacity
        estimated_hours: 3-4
      - id: rate-limiter-m2
        name: Per-Client Rate Limiting
        description: Track rate limits per client (IP or API key).
        acceptance_criteria:
        - Maintain separate rate limit bucket for each client
        - Identify clients by IP address or API key header
        - Clean up stale buckets not accessed within timeout period
        - Store buckets efficiently to handle large numbers of clients
        - Support configurable per-client limit overrides for premium tiers
        pitfalls:
        - Memory leak from never cleaning buckets
        - Lock contention under load
        - Client spoofing bypassing limits
        concepts:
        - Per-client tracking
        - Memory management
        - Background cleanup
        skills:
        - HashMap/dictionary operations
        - Memory profiling and optimization
        - Background task scheduling
        - Client identification strategies
        deliverables:
        - Client identification by IP address or API key
        - Separate token bucket instance per unique client
        - Memory-efficient storage with automatic stale bucket cleanup
        - Background cleanup task removing expired client buckets
        estimated_hours: 2-3
      - id: rate-limiter-m3
        name: HTTP Middleware Integration
        description: Integrate rate limiter as HTTP middleware.
        acceptance_criteria:
        - Middleware intercepts all HTTP requests and checks rate limit
        - Return HTTP 429 Too Many Requests when client exceeds limit
        - Include Retry-After header indicating seconds until next allowed request
        - Include X-RateLimit-Limit and X-RateLimit-Remaining headers in responses
        - Integrate with Express, Flask, or equivalent web framework
        pitfalls:
        - Not returning proper status code
        - Missing Retry-After header
        - Rate limit headers only on 429
        concepts:
        - HTTP middleware
        - Rate limit headers
        - 429 response
        skills:
        - HTTP middleware pattern implementation
        - Response header manipulation
        - HTTP status code handling
        - Request/response lifecycle management
        deliverables:
        - Rate limit middleware function intercepting HTTP requests
        - Standard rate limit headers in every HTTP response
        - HTTP 429 Too Many Requests response with error body
        - Configurable limit rules per endpoint or route pattern
        estimated_hours: 2-3
      - id: rate-limiter-m4
        name: Distributed Rate Limiting
        description: Scale rate limiter across multiple server instances.
        acceptance_criteria:
        - Share rate limit state across multiple server instances via Redis
        - Use Redis-backed storage for token bucket or sliding window state
        - Perform atomic read-modify-write operations using Lua scripts
        - Handle Redis connection failures gracefully with local fallback
        - Maintain consistency under high concurrent request load
        pitfalls:
        - Non-atomic read-modify-write
        - Redis connection failures
        - Clock sync between servers
        concepts:
        - Distributed state
        - Redis Lua scripts
        - Atomic operations
        skills:
        - Redis client integration
        - Lua scripting for atomicity
        - Distributed system design
        - Failure handling and fallbacks
        deliverables:
        - Redis-based token counter storage shared across instances
        - Atomic increment operations using Redis Lua scripts
        - Sliding window rate limit implementation in Redis
        - Cluster-wide rate limits consistent across all server instances
        estimated_hours: 3-4
    - id: leader-election
      name: Leader Election
      description: Bully algorithm, ring election
      difficulty: intermediate
      estimated_hours: 12-20
      essence: Coordinating multiple distributed nodes to agree on a single leader through asynchronous message-passing protocols, handling concurrent election requests, network delays, and node failures while ensuring exactly one coordinator emerges without centralized control.
      why_important: Leader election is fundamental to distributed systems like Kubernetes, Kafka, and etcd, teaching you how to build fault-tolerant coordination mechanisms that prevent split-brain scenarios and ensure consistent cluster state management.
      learning_outcomes:
      - Implement message-passing protocols for inter-node communication
      - Design election algorithms that handle simultaneous coordinator failures
      - Build timeout-based failure detection mechanisms
      - Handle network partitions and split-brain scenarios
      - Implement the bully algorithm with priority-based leader selection
      - Design ring-based election with token circulation
      - Debug race conditions in concurrent distributed elections
      - Test fault tolerance under simulated node failures and network delays
      skills:
      - Distributed Coordination
      - Failure Detection
      - Consensus Protocols
      - Network Partitioning
      - Process Communication
      - Fault Tolerance
      - Leader Election
      - Message Passing
      tags:
      - bully
      - coordination
      - distributed
      - go
      - intermediate
      - java
      - python
      - ring
      architecture_doc: architecture-docs/leader-election/index.md
      languages:
        recommended:
        - Go
        - Python
        - Java
        also_possible:
        - Rust
        - Erlang
      resources:
      - name: Bully Algorithm
        url: https://en.wikipedia.org/wiki/Bully_algorithm
        type: article
      - name: Ring Election
        url: https://www.cs.colostate.edu/~cs551/CourseNotes/Synchronization/LeijdElect.html
        type: article
      prerequisites:
      - type: skill
        name: Distributed systems basics
      - type: skill
        name: Network programming
      - type: skill
        name: Failure detection
      milestones:
      - id: leader-election-m1
        name: Node Communication
        description: Set up inter-node messaging.
        acceptance_criteria:
        - Each node has a unique numeric ID that is used for comparison during election algorithms
        - Point-to-point messaging delivers a message reliably from one specific node to another
        - Broadcast messaging delivers a message to all currently known live nodes in the cluster
        - Node failure is detected within a configurable timeout and triggers re-election of the leader
        pitfalls:
        - Message ordering
        - Partial failures
        - Network partitions
        concepts:
        - Node identity
        - Message passing
        - Failure detection
        skills:
        - Network socket programming
        - Error handling for unreliable networks
        - Heartbeat and timeout mechanisms
        - Message serialization and deserialization
        deliverables:
        - Node discovery mechanism that detects peer nodes via configuration or multicast announcement
        - Message passing layer that sends and receives typed messages between nodes over TCP or UDP
        - Network partition handler that detects and responds to unreachable peers with split-brain avoidance
        - Node failure detector that identifies unresponsive nodes using heartbeat timeouts and suspicion levels
        estimated_hours: 3-4
      - id: leader-election-m2
        name: Bully Algorithm
        description: Implement the bully election algorithm.
        acceptance_criteria:
        - The node with the highest ID among all responding nodes wins the election every time
        - ELECTION messages are sent only to nodes with IDs higher than the initiating node
        - An OK response from any higher-ID node causes the lower-ID initiator to stop its election
        - COORDINATOR announcement is broadcast to all nodes after the winner is determined
        pitfalls:
        - Split brain
        - Message loss
        - Concurrent elections
        concepts:
        - Bully algorithm
        - Leader election
        - Distributed coordination
        skills:
        - Process ID comparison and priority handling
        - Concurrent message handling
        - Election state machine implementation
        - Crash recovery procedures
        deliverables:
        - Election initiation logic that starts a new election when the current leader is detected as failed
        - Higher-ID responder that sends OK messages to lower-ID nodes attempting to claim leadership
        - Coordinator announcement broadcaster that notifies all nodes of the newly elected leader
        - Election timeout handler that declares victory when no higher-ID node responds within the deadline
        estimated_hours: 4-5
      - id: leader-election-m3
        name: Ring Election
        description: Implement ring-based election algorithm.
        acceptance_criteria:
        - Nodes are arranged in a logical ring where each node knows its successor in ID order
        - Election messages are forwarded around the ring collecting all live node IDs along the way
        - All live node IDs are collected in the election message as it traverses the full ring
        - The node with the highest ID in the collected set becomes the new leader
        pitfalls:
        - Ring breaks
        - Multiple elections
        - Node rejoins
        concepts:
        - Ring topology
        - Token passing
        - Distributed election
        skills:
        - Ring data structure maintenance
        - Token-based coordination
        - Node ordering and successor management
        - Ring reformation after failures
        deliverables:
        - Ring topology manager that arranges nodes in a logical ring ordered by their IDs
        - Election message forwarder that passes election tokens along the ring to the next live node
        - Coordinator selector that picks the highest-ID node from the collected set of live node IDs
        - Ring repair mechanism that skips failed nodes and reconnects the ring to maintain message flow
        estimated_hours: 4-5
    - id: replicated-log
      name: Replicated Log
      description: Append-only log, basic replication
      difficulty: intermediate
      estimated_hours: 15-25
      essence: Deterministic event ordering and state replication across network-partitioned nodes through append-only log synchronization, combining write-ahead logging for durability with primary-backup coordination protocols to maintain consistency despite crashes and network failures.
      why_important: Building this teaches you the core infrastructure patterns behind production databases (PostgreSQL replication), message queues (Kafka), and coordination services (etcd, ZooKeeper), skills directly applicable to distributed systems engineering roles.
      learning_outcomes:
      - Implement append-only log storage with segment-based indexing for efficient reads and compaction
      - Design primary-backup replication protocol with deterministic operation ordering and acknowledgment semantics
      - Build failure detection system using heartbeat protocols and adaptive timeout mechanisms
      - Handle network partitions and split-brain scenarios with proper quorum-based decision making
      - Implement write-ahead logging for crash recovery and state reconstruction
      - Design client libraries with automatic primary discovery, connection pooling, and transparent failover
      - Debug non-deterministic timing issues and race conditions in distributed state machines
      - Optimize replication throughput through batching and asynchronous acknowledgments
      skills:
      - Distributed Replication
      - Consensus Protocols
      - Failure Detection
      - Write-Ahead Logging
      - Network Protocol Design
      - Crash Recovery
      - State Machine Replication
      - Quorum Systems
      tags:
      - append-only
      - distributed
      - durability
      - go
      - intermediate
      - java
      - ordering
      - rust
      architecture_doc: architecture-docs/replicated-log/index.md
      languages:
        recommended:
        - Go
        - Rust
        - Java
        also_possible:
        - Python
        - C
      resources:
      - type: paper
        name: Viewstamped Replication
        url: https://pmg.csail.mit.edu/papers/vr-revisited.pdf
      - type: article
        name: Distributed Log 101
        url: https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying
      prerequisites:
      - type: skill
        name: RPC basics
      - type: skill
        name: Networking
      - type: skill
        name: Log-based storage
      milestones:
      - id: replicated-log-m1
        name: Log Storage
        description: Implement an append-only log with indexing.
        acceptance_criteria:
        - Append entries with monotonically increasing sequence numbers
        - Read entries by sequence index with O(1) lookup time
        - Persist all entries to disk surviving process restarts
        - Handle log compaction to reclaim disk space from old entries
        pitfalls:
        - Partial writes on crash
        - Index corruption
        - File handle limits
        concepts:
        - Append-only logs
        - Sequence numbers
        - Durability
        skills:
        - File I/O and durability guarantees
        - Sequential write optimization
        - Binary data serialization
        - Index data structures
        - Crash recovery mechanisms
        deliverables:
        - Append-only log structure with sequential write semantics
        - Log entry format with sequence number, length, and data payload
        - Durable log persistence with fsync to disk after each write
        - Log compaction removing old entries beyond retention window
        estimated_hours: 3-5
      - id: replicated-log-m2
        name: Replication Protocol
        description: Implement primary-backup replication with follower sync.
        acceptance_criteria:
        - Primary node accepts all write operations from clients
        - Replicate each new log entry to all follower nodes
        - Acknowledge writes only after quorum of replicas confirm
        - Handle follower lag by sending missing entries on reconnect
        pitfalls:
        - Split brain without leader election
        - Replication lag
        - Network partitions
        concepts:
        - Primary-backup
        - Quorum
        - Replication lag
        skills:
        - Network communication protocols
        - Asynchronous message handling
        - State synchronization
        - Consensus algorithms
        - Leader election implementation
        deliverables:
        - Leader election mechanism selecting primary from cluster nodes
        - Log replication sending entries from primary to follower nodes
        - Consistency guarantees ensuring all replicas converge to same state
        - Quorum write acknowledgment requiring majority before confirming
        estimated_hours: 5-8
      - id: replicated-log-m3
        name: Failure Detection
        description: Detect node failures and handle recovery.
        acceptance_criteria:
        - Send periodic heartbeat messages between cluster nodes
        - Detect node failure by heartbeat timeout expiration
        - Catch up follower after recovery by replaying missed entries
        - Handle primary failure by notifying remaining cluster members
        pitfalls:
        - False positives in detection
        - Thundering herd on recovery
        - Consistency after catch-up
        concepts:
        - Failure detection
        - Heartbeats
        - Recovery protocols
        skills:
        - Timeout-based failure detection
        - Health monitoring systems
        - State machine replication
        - Log reconciliation
        - Network partition handling
        deliverables:
        - Heartbeat mechanism with periodic liveness signals between nodes
        - Failure timeout triggering node suspected status after missed heartbeats
        - Leader takeover process promoting follower when primary fails
        - Split-brain prevention ensuring only one active primary at any time
        estimated_hours: 4-6
      - id: replicated-log-m4
        name: Client Interface
        description: Implement a client that handles primary discovery and failover.
        acceptance_criteria:
        - Discover and connect to current primary node automatically
        - Retry failed operations with automatic primary rediscovery
        - Optionally read from follower replicas for lower latency
        - Provide consistent reads by verifying data freshness against primary
        pitfalls:
        - Stale primary cache
        - Read-your-writes consistency
        - Infinite retry loops
        concepts:
        - Service discovery
        - Client-side failover
        - Consistency levels
        skills:
        - Client library design
        - Retry logic with backoff
        - Service discovery patterns
        - Load balancing strategies
        - Failover automation
        deliverables:
        - Append API for writing new entries to the replicated log
        - Read API for fetching entries by sequence number or range
        - Subscribe-to-updates API streaming new entries to consumers
        - Consistency options supporting strong and eventual read modes
        estimated_hours: 3-6
    - id: vector-clocks
      name: Vector Clocks
      description: Logical time, causality
      difficulty: intermediate
      estimated_hours: 8-12
      essence: Logical timestamp data structure maintaining per-process counters to establish partial ordering between distributed events, enabling causal relationship detection and concurrent operation identification without physical clock synchronization.
      why_important: Building this teaches you fundamental concepts required for distributed databases, concurrent systems, and eventually-consistent architectures used in production systems like Amazon DynamoDB, Cassandra, and Riak.
      learning_outcomes:
      - Implement vector clock data structures with increment, merge, and comparison operations
      - Design algorithms to detect concurrent versus causally-ordered events using partial ordering
      - Build conflict detection mechanisms for identifying divergent replicas in distributed storage
      - Implement vector clock pruning strategies to prevent unbounded memory growth
      - Debug causality violations and race conditions in distributed event ordering
      - Integrate logical time tracking into a working distributed key-value store with replication
      - Analyze trade-offs between vector clocks, version vectors, and dotted version vectors
      - Optimize vector clock storage and transmission overhead for large-scale systems
      skills:
      - Causality Tracking
      - Logical Timestamps
      - Conflict Resolution
      - Distributed Consensus
      - Partial Ordering
      - Event Synchronization
      - Replicated Data Structures
      - Version Control Algorithms
      tags:
      - causality
      - distributed
      - go
      - intermediate
      - java
      - ordering
      - python
      - timestamps
      - version-vectors
      architecture_doc: architecture-docs/vector-clocks/index.md
      languages:
        recommended:
        - Python
        - Go
        - Java
        also_possible:
        - Rust
        - JavaScript
      resources:
      - name: Vector Clocks Paper
        url: https://en.wikipedia.org/wiki/Vector_clock
        type: article
      - name: Understanding Vector Clocks
        url: https://sookocheff.com/post/time/vector-clocks/
        type: article
      - name: 'Princeton COS 418: Vector Clocks & Distributed Snapshots'
        url: https://www.cs.princeton.edu/courses/archive/fall18/cos418/docs/L4-vc.pdf
        type: paper
      prerequisites:
      - type: skill
        name: Distributed systems basics
      - type: skill
        name: Understanding of causality
      milestones:
      - id: vector-clocks-m1
        name: Basic Vector Clock
        description: Implement vector clock data structure and operations.
        acceptance_criteria:
        - Initialize clock for N nodes with all counters set to zero
        - Increment local clock component on each local event at that node
        - Merge clocks on message receive by taking element-wise maximum values
        - Compare clocks returning happens-before, happens-after, or concurrent result
        pitfalls:
        - Forgetting to increment on receive
        - Not copying clock on send
        - Comparison logic
        concepts:
        - Vector clocks
        - Causality
        - Partial ordering
        skills:
        - Implementing data structures with state management
        - Clock vector operations and comparisons
        - Deep copying vs shallow copying in distributed systems
        - Logical timestamp manipulation
        deliverables:
        - Clock initialization creating a zero-valued vector for each participating node
        - Increment operation advancing the local node's counter in the vector on each event
        - Merge operation computing the element-wise maximum of two vector clock values
        - Clock comparison function determining before, after, or concurrent ordering
        estimated_hours: 3-4
      - id: vector-clocks-m2
        name: Conflict Detection
        description: Use vector clocks to detect conflicting updates.
        acceptance_criteria:
        - Detect concurrent writes to same key
        - Store each version alongside its vector clock for later comparison
        - Return all concurrent versions on read
        - Last-writer-wins resolution picks the version with the highest wall-clock timestamp (optional)
        pitfalls:
        - Growing version lists without cleanup
        - Comparison direction
        - Lost updates
        concepts:
        - Conflict detection
        - Version vectors
        - Multi-value registers
        skills:
        - Concurrent version comparison algorithms
        - Multi-value state reconciliation
        - Conflict resolution strategies
        - Version history tracking
        deliverables:
        - Concurrent event detection identifying writes with incomparable vector clocks
        - Happens-before relationship check determining causal ordering between two events
        - Conflict marking flag attached to values with concurrent version histories
        - Conflict resolution strategy supporting last-writer-wins and application-level merge
        estimated_hours: 3-4
      - id: vector-clocks-m3
        name: Version Pruning
        description: Implement strategies to limit unbounded growth of version history.
        acceptance_criteria:
        - Configurable max versions per key limits the retained history count
        - Prune oldest versions when limit exceeded
        - Optional pruning of dominated versions removes entries superseded by all current versions
        - Garbage collection of unreferenced clocks frees memory for departed nodes
        - Metrics report current version count and pruning activity per key
        pitfalls:
        - Pruning too aggressively loses conflict info
        - Clock comparison must handle different lengths
        - Timestamp skew across nodes
        - Memory leaks from orphaned clocks
        concepts:
        - Version pruning strategies
        - Garbage collection
        - Dominated versions
        - 'Trade-off: consistency vs storage'
        skills:
        - Memory management in long-running distributed systems
        - Garbage collection strategies for versioned data
        - Balancing consistency guarantees with resource constraints
        - Dominance relationship algorithms
        deliverables:
        - Clock garbage collection removing entries for nodes that have left the cluster
        - Minimum clock tracking recording the globally known safe pruning threshold
        - Pruning algorithm removing dominated versions that are causally superseded
        - Storage optimization reducing memory usage by compacting version history
        estimated_hours: 2-3
      - id: vector-clocks-m4
        name: Distributed Key-Value Store
        description: Integrate vector clocks into a working distributed key-value store.
        acceptance_criteria:
        - Multi-node setup with three or more nodes exchanging messages
        - PUT and GET operations include vector clocks for version tracking
        - Read-repair resolves detected conflicts by writing the merged result back
        - Configurable conflict resolution (LWW, merge, manual)
        - Simple replication (all nodes store all keys)
        pitfalls:
        - Network partitions causing divergence
        - Replication lag during high write load
        - Clock synchronization across restarts
        - Handling node failures during writes
        concepts:
        - Distributed replication
        - Quorum reads/writes
        - Read repair
        - Eventual consistency
        skills:
        - Building distributed consensus protocols
        - Implementing quorum-based replication
        - Network partition handling
        - Asynchronous distributed system coordination
        - Eventual consistency patterns
        deliverables:
        - Versioned values stored alongside their vector clock metadata
        - Read operation returning the value together with its current vector clock
        - Write operation accepting a context vector clock for conflict detection on update
        - Conflict resolution on read merging or surfacing concurrent versions to the client
        estimated_hours: 4-6
    - id: feature-flags
      name: Feature Flag System
      description: Dynamic feature toggles with targeting rules
      difficulty: intermediate
      estimated_hours: '35'
      essence: Rule-based flag evaluation using deterministic hashing for consistent percentage-based targeting, server-sent event streams for push-based configuration updates, and local caching with fallback mechanisms to eliminate runtime dependencies on external flag services.
      why_important: Feature flags enable safe deployments, A/B testing, and gradual rollouts. Understanding flag systems helps design better release strategies.
      learning_outcomes:
      - Design flag evaluation with targeting rules
      - Implement percentage-based rollouts
      - Build real-time flag updates without restart
      - Handle flag dependencies and conflicts
      skills:
      - Flag evaluation logic
      - Targeting rules engine
      - SSE/WebSocket streaming
      - Local caching strategies
      - A/B test statistical analysis
      - Percentage-based rollouts
      - Real-time configuration updates
      - Event tracking systems
      tags:
      - architecture
      - backend
      - devops
      - intermediate
      - rollout
      - targeting
      - toggles
      architecture_doc: architecture-docs/feature-flags/index.md
      languages:
        recommended:
        - Go
        - Python
        - Java
        also_possible: []
      resources:
      - name: OpenFeature Specification
        url: https://openfeature.dev/
        type: documentation
      - name: Unleash Feature Flag Best Practices
        url: https://docs.getunleash.io/topics/feature-flags/feature-flag-best-practices
        type: documentation
      - name: A/B Testing with Feature Flags Tutorial
        url: https://docs.getunleash.io/feature-flag-tutorials/use-cases/a-b-testing
        type: tutorial
      - name: Flagsmith Real-Time Flag Updates
        url: https://docs.flagsmith.com/advanced-use/real-time-flags
        type: documentation
      - name: SSE vs WebSockets Comparison
        url: https://softwaremill.com/sse-vs-websockets-comparing-real-time-communication-protocols/
        type: article
      prerequisites:
      - type: project
        id: rest-api-design
      milestones:
      - id: feature-flags-m1
        name: Flag Evaluation Engine
        description: Build the core flag evaluation engine with support for boolean, string, number, and JSON flags.
        acceptance_criteria:
        - Evaluate flag rules against user context and return correct variant
        - Support percentage rollouts using consistent hashing for stable assignment
        - Handle complex rule conditions with AND/OR logic and multiple attributes
        - Return variant value with evaluation reason for debugging and audit
        pitfalls:
        - Inconsistent hashing causes users to flip-flop variations
        - Circular prerequisites cause infinite loop
        - Rule priority ties cause non-deterministic evaluation
        concepts:
        - Consistent hashing algorithms for deterministic user bucketing
        - Rule evaluation precedence and boolean expression parsing
        - Flag type systems and schema validation
        - Dependency graph cycle detection
        skills:
        - Hash function implementation
        - Rule engine design
        - Type-safe flag evaluation
        - Graph traversal algorithms
        deliverables:
        - Flag definition storage with rules and targeting configuration
        - Evaluation logic resolving flag value from rules and context
        - Context-based targeting using user attributes and segments
        - Default values returned when no targeting rules match
        estimated_hours: '11.5'
      - id: feature-flags-m2
        name: Real-time Flag Updates
        description: Implement real-time flag updates using SSE/WebSocket with local caching and fallback.
        acceptance_criteria:
        - Push flag changes to connected SDKs within seconds of modification
        - Support streaming via SSE or WebSocket for low-latency flag updates
        - Handle SDK reconnection after network interruption with state recovery
        - Ensure consistency across all SDK instances receiving the same flag values
        pitfalls:
        - SSE reconnection without backoff causes thundering herd
        - Stale cache served indefinitely when stream down
        - Large flag payloads slow down evaluation
        concepts:
        - Server-Sent Events vs WebSocket trade-offs
        - Exponential backoff and jitter for reconnection
        - Cache invalidation strategies and TTL management
        - Event stream protocol design
        skills:
        - Real-time communication protocols
        - Caching layer implementation
        - Connection resilience patterns
        - Event-driven architecture
        deliverables:
        - Server-sent events or polling mechanism for flag change delivery
        - Flag change notification broadcasting updates to connected SDKs
        - Client SDK update handler applying new flag values without restart
        - Cache invalidation ensuring stale flag values are replaced promptly
        estimated_hours: '11.5'
      - id: feature-flags-m3
        name: Flag Analytics & Experiments
        description: Track flag evaluations for analytics and support A/B testing with statistical significance.
        acceptance_criteria:
        - Track flag evaluations with user context for exposure analysis
        - Support A/B experiment metrics comparing variant performance over time
        - Calculate statistical significance using appropriate hypothesis testing methods
        - Generate experiment reports summarizing variant performance and confidence levels
        pitfalls:
        - Peeking at results early inflates false positive rate
        - Sample ratio mismatch invalidates experiment
        - Novelty effect skews early results
        concepts:
        - Statistical significance testing and p-values
        - Sequential testing and stopping rules
        - Sample ratio mismatch detection
        - Attribution windows and time-based biasing
        skills:
        - A/B testing statistics
        - Event tracking and analytics pipelines
        - Experimental design validation
        - Metric aggregation and reporting
        deliverables:
        - Flag exposure logging recording each flag evaluation with context
        - A/B test assignment linking users to experiment variants persistently
        - Metrics collection aggregating conversion and engagement by variant
        - Experiment analysis computing statistical significance of variant differences
        estimated_hours: '11.5'
    - id: job-scheduler
      name: Job Scheduler
      description: Distributed task scheduling with retries
      difficulty: advanced
      estimated_hours: '45'
      essence: Cron expression parsing for time-based trigger calculation, distributed consensus protocols for atomic job claim coordination across worker nodes, and fault-tolerant execution with leader election, heartbeat monitoring, and recovery mechanisms for failed tasks and workers.
      why_important: Background job processing is essential for async workloads. Understanding schedulers helps design reliable batch processing systems.
      learning_outcomes:
      - Parse and evaluate cron expressions
      - Implement distributed locking for job claims
      - Design retry strategies with backoff
      - Handle worker failures and job recovery
      skills:
      - Cron expression parsing
      - Distributed locking
      - Priority queue implementation
      - Leader election algorithms
      - Heartbeat mechanisms
      - Exponential backoff strategies
      - Cluster coordination
      - Job deduplication
      tags:
      - advanced
      - backend
      - cron
      - distributed
      - distributed-systems
      - queues
      - reliability
      - retries
      - scheduling
      architecture_doc: architecture-docs/job-scheduler/index.md
      languages:
        recommended: &id002
        - Go
        - Java
        - Python
        also_possible: []
      resources:
      - name: Quartz Scheduler Documentation
        url: https://www.quartz-scheduler.org/documentation/
        type: documentation
      - name: AWS Leader Election Guide
        url: https://aws.amazon.com/builders-library/leader-election-in-distributed-systems/
        type: article
      - name: etcd Distributed Coordination
        url: https://etcd.io/docs/v3.5/
        type: documentation
      - name: Distributed Job Scheduler System Design
        url: https://www.geeksforgeeks.org/system-design/design-distributed-job-scheduler-system-design/
        type: tutorial
      - name: Crontab Expression Reference
        url: https://crontab.guru/
        type: tool
      prerequisites:
      - type: skill
        name: Redis basics
      - type: skill
        name: Concurrency
      - type: skill
        name: Cron scheduling concepts
      milestones:
      - id: job-scheduler-m1
        name: Cron Expression Parser
        description: Parse cron expressions and calculate next execution times.
        acceptance_criteria:
        - Parser correctly interprets standard five-field cron expressions including wildcards, ranges, and step values
        - Next run time calculation returns the correct future timestamp for any valid cron expression
        - Extended cron syntax including seconds field and shorthand aliases like @daily and @hourly is supported
        - Timezone-aware scheduling converts cron times correctly across UTC and local timezones
        pitfalls:
        - Daylight saving time causes missed or duplicate runs
        - Day-of-month 31 skips months with fewer days
        - Infinite loop if no valid date exists
        concepts:
        - Cron expression syntax with five or six fields
        - Timezone handling and UTC normalization
        - Date arithmetic with month/year boundaries
        - Next occurrence calculation algorithms
        skills:
        - Time and date manipulation
        - Parsing domain-specific languages
        - Algorithm design for scheduling
        - Edge case handling
        deliverables:
        - Cron field parser that handles minutes, hours, day-of-month, month, and day-of-week ranges
        - Next run time calculator that computes the nearest future execution from a cron expression
        - Cron expression validator that rejects malformed or out-of-range field values
        - Human-readable schedule description generator that converts cron expressions into plain English
        estimated_hours: '15'
      - id: job-scheduler-m2
        name: Job Queue with Priorities
        description: Implement a job queue with priorities, delayed execution, and deduplication.
        acceptance_criteria:
        - Jobs enqueued with numeric priority levels are dequeued in strict priority order
        - Delayed jobs remain invisible until their scheduled time and then become eligible for dequeue
        - Duplicate job submissions with the same idempotency key are silently deduplicated
        - Jobs with an expired TTL are automatically removed from the queue before dequeue
        pitfalls:
        - Race condition between claim and processing
        - Visibility timeout too short causes duplicate processing
        - Deduplication key collision across different job types
        concepts:
        - Priority queue data structures
        - Atomic operations and compare-and-swap
        - Visibility timeout patterns
        - Hash-based deduplication
        skills:
        - Concurrent data structure design
        - Idempotency key generation
        - Database transactions
        - Queue management
        deliverables:
        - Priority queue implementation backed by a min-heap or sorted structure for job ordering
        - Job submission endpoint that accepts job payloads with priority level and scheduling metadata
        - Job dequeue mechanism that always returns the highest-priority pending job first
        - Delayed job support that holds jobs until their scheduled execution time arrives
        estimated_hours: '15'
      - id: job-scheduler-m3
        name: Worker Coordination
        description: Coordinate multiple workers with leader election, heartbeats, and job recovery.
        acceptance_criteria:
        - Jobs are distributed evenly to available workers based on current load and capacity
        - Job locking via lease mechanism prevents duplicate execution by multiple workers
        - Worker heartbeat failures within a configurable timeout trigger automatic job reassignment
        - Graceful worker shutdown completes in-progress jobs before deregistering the worker
        pitfalls:
        - Leader election split-brain with network partition
        - Job recovered while still processing causes duplicate
        - Worker heartbeat failure during long job
        concepts:
        - Consensus algorithms for leader election
        - Distributed locking mechanisms
        - Heartbeat and failure detection
        - Graceful degradation strategies
        skills:
        - Distributed systems coordination
        - Fault tolerance implementation
        - Network partition handling
        - State recovery mechanisms
        deliverables:
        - Worker registration service that tracks available workers with their capacity and capabilities
        - Job assignment logic that distributes jobs to workers based on availability and affinity
        - Worker heartbeat mechanism that periodically verifies each worker is alive and responsive
        - Failed worker handler that detects unresponsive workers and reassigns their in-progress jobs
        estimated_hours: '15'
    advanced:
    - id: distributed-cache
      name: Distributed Cache
      description: Consistent hashing
      difficulty: advanced
      estimated_hours: 25-40
      essence: Hash ring-based key partitioning across multiple cache nodes with eventual consistency guarantees, implementing O(1) eviction policies through dual data structures, and managing network partition scenarios where replication protocols must balance consistency versus availability trade-offs.
      why_important: Building a distributed cache teaches you the core challenges of distributed systems‚Äîdata partitioning, replication strategies, and consensus‚Äîwhich are fundamental to backend engineering at companies like Google, Amazon, and Netflix where systems must scale horizontally across thousands of nodes.
      learning_outcomes:
      - Implement consistent hashing with virtual nodes to minimize data rebalancing during cluster changes
      - Design and build LRU eviction policies using hash maps and doubly-linked lists for O(1) operations
      - Build inter-node communication protocols using TCP/IP for cache routing and data transfer
      - Implement replication strategies to handle node failures while maintaining data availability
      - Debug race conditions and network partition scenarios in distributed environments
      - Design trade-offs between consistency and availability based on CAP theorem constraints
      - Implement cache coherence protocols to synchronize state across multiple nodes
      - Build monitoring and health-check systems to detect and recover from node failures
      skills:
      - Consistent Hashing
      - LRU Cache Implementation
      - Distributed Systems Design
      - CAP Theorem Trade-offs
      - TCP/IP Networking
      - Replication Protocols
      - Fault Tolerance
      - Cache Coherence
      tags:
      - advanced
      - caching
      - distributed
      - eviction
      - go
      - java
      - python
      - replication
      - sharding
      architecture_doc: architecture-docs/distributed-cache/index.md
      languages:
        recommended:
        - Go
        - Java
        - Python
        also_possible:
        - Rust
        - JavaScript
      resources:
      - type: article
        name: Consistent Hashing
        url: https://www.toptal.com/big-data/consistent-hashing
      - type: paper
        name: Dynamo Paper
        url: https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf
      prerequisites:
      - type: skill
        name: Hash tables
      - type: skill
        name: Networking basics
      - type: skill
        name: Consistent hashing concept
      milestones:
      - id: distributed-cache-m1
        name: Consistent Hash Ring
        description: Implement consistent hashing for key distribution.
        acceptance_criteria:
        - Hash ring distributes keys evenly across available cache nodes
        - Virtual nodes reduce hotspot risk and improve distribution balance
        - Node addition or removal only remaps minimal subset of existing keys
        - Key lookup returns correct target node in constant time complexity
        pitfalls:
        - Poor hash distribution
        - Not enough virtual nodes
        - Race conditions
        concepts:
        - Consistent hashing
        - Load balancing
        - Ring topology
        skills:
        - Hash function implementation
        - Ring data structure design
        - Distributed key routing
        - Virtual node management
        deliverables:
        - Hash ring implementation mapping keys to node positions
        - Virtual nodes for balanced key distribution across nodes
        - Key to node mapping using consistent hashing algorithm
        - Ring rebalancing on node addition or removal events
        estimated_hours: 6-10
      - id: distributed-cache-m2
        name: Cache Node Implementation
        description: Implement a cache node with LRU eviction.
        acceptance_criteria:
        - Get, set, and delete operations function correctly under concurrent access
        - LRU eviction removes least recently used entries when memory limit is reached
        - TTL support expires keys automatically after their configured time-to-live
        - Memory limit enforced by evicting entries when total size exceeds threshold
        pitfalls:
        - Memory accounting errors
        - TTL cleanup overhead
        - Lock contention
        concepts:
        - LRU cache
        - Memory management
        - TTL expiration
        skills:
        - Doubly linked list manipulation
        - Hash map with eviction policies
        - Concurrent data structure design
        - Memory-efficient caching
        deliverables:
        - LRU cache implementation per node with configurable capacity
        - Get, set, and delete operations with proper concurrency handling
        - TTL support for automatic key expiration after timeout
        - Memory limit enforcement with eviction when capacity is reached
        estimated_hours: 6-10
      - id: distributed-cache-m3
        name: Cluster Communication
        description: Implement inter-node communication and routing.
        acceptance_criteria:
        - Node discovery detects new nodes joining the cluster automatically
        - Request routing forwards cache operations to the correct hash ring node
        - Health checks detect and mark unresponsive nodes within timeout period
        - Cluster membership updates propagate to all nodes via gossip protocol
        pitfalls:
        - Split brain scenarios
        - Network partition handling
        - Stale membership info
        concepts:
        - Distributed systems
        - Health checking
        - Service discovery
        skills:
        - gRPC or HTTP API design
        - Gossip protocol implementation
        - Heartbeat mechanism design
        - Network serialization and deserialization
        deliverables:
        - Node discovery mechanism for automatic cluster membership
        - Gossip protocol for propagating cluster state between nodes
        - Request routing forwarding operations to the correct node
        - Node health checking with configurable probe intervals
        estimated_hours: 8-12
      - id: distributed-cache-m4
        name: Replication & Consistency
        description: Add data replication for fault tolerance.
        acceptance_criteria:
        - Configurable replication factor stores copies on N successor nodes
        - Read and write quorums ensure desired consistency level per operation
        - Conflict resolution handles divergent values using configurable strategy
        - Anti-entropy process repairs inconsistent replicas in the background
        pitfalls:
        - Stale reads
        - Write conflicts
        - Replica divergence
        concepts:
        - Quorum systems
        - Eventual consistency
        - Conflict resolution
        skills:
        - Consensus algorithm implementation
        - Vector clock usage
        - Read/write quorum configuration
        - Anti-entropy mechanisms
        deliverables:
        - Replication factor configuration for data redundancy level
        - Read and write quorum settings for consistency control
        - Consistency level options from eventual to strong consistency
        - Conflict resolution using vector clocks or last-write-wins strategy
        estimated_hours: 8-12
    - id: gossip-protocol
      name: Gossip Protocol
      description: Epidemic broadcast
      difficulty: advanced
      estimated_hours: 15-25
      essence: Randomized peer-to-peer message dissemination using epidemic spreading models, with periodic state reconciliation through push/pull anti-entropy and probabilistic convergence guarantees despite node failures and network partitions.
      why_important: Gossip protocols power real-world distributed systems like Cassandra, Consul, and Riak, teaching you how to build scalable, fault-tolerant systems that sacrifice strong consistency for availability and partition tolerance in accordance with the CAP theorem.
      learning_outcomes:
      - Implement randomized peer selection algorithms with bounded fanout
      - Design push-based and pull-based anti-entropy mechanisms for state reconciliation
      - Build gossip-based failure detection using indirect probing and suspicion mechanisms
      - Implement infection-style message dissemination with probabilistic delivery guarantees
      - Debug convergence issues in eventually consistent systems
      - Measure and optimize gossip round complexity and bandwidth overhead
      - Implement SWIM-style failure detection with configurable timeout thresholds
      - Design membership protocols that achieve time-bounded failure detection
      skills:
      - Epidemic Algorithms
      - Eventual Consistency
      - Distributed Failure Detection
      - Peer-to-Peer Networking
      - Probabilistic Protocols
      - Anti-Entropy Mechanisms
      - Membership Management
      - Weakly-Consistent Systems
      tags:
      - advanced
      - dissemination
      - distributed
      - failure-detection
      - go
      - membership
      - protocols
      - python
      - rust
      architecture_doc: architecture-docs/gossip-protocol/index.md
      languages:
        recommended:
        - Go
        - Rust
        - Python
        also_possible:
        - Java
        - Erlang
      resources:
      - name: Gossip Protocol Explained
        url: https://www.cs.cornell.edu/home/rvr/papers/flowgossip.pdf
        type: paper
      - name: SWIM Paper
        url: https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf
        type: paper
      prerequisites:
      - type: skill
        name: Networking basics
      - type: skill
        name: Distributed systems concepts
      - type: skill
        name: Probability
      milestones:
      - id: gossip-protocol-m1
        name: Peer Management
        description: Manage cluster membership and peer list.
        acceptance_criteria:
        - Maintain list of known peers with address, port, and last-seen timestamp
        - Random peer selection picks k random alive peers for each gossip round
        - Handle peer join and leave events updating the membership list correctly
        - Periodic peer list exchange synchronizes membership across all cluster nodes
        pitfalls:
        - Not handling own address
        - Thread safety
        - Stale peer info
        concepts:
        - Membership
        - Random selection
        - Peer state
        skills:
        - Concurrent data structure design
        - Network peer discovery and management
        - Random sampling algorithms
        - Thread-safe state synchronization
        deliverables:
        - Peer list maintenance tracking known cluster members and their state
        - Peer discovery mechanism for finding new nodes in the cluster
        - Peer state tracking recording alive, suspected, and dead status
        - Peer removal cleaning up dead peers from the membership list
        estimated_hours: 3-4
      - id: gossip-protocol-m2
        name: Push Gossip
        description: Implement push-based gossip dissemination.
        acceptance_criteria:
        - Periodic gossip rounds execute at configurable interval sending updates to peers
        - Push updates containing state deltas to randomly selected peer nodes each round
        - Version or timestamp tracking prevents old data from overwriting newer values
        - Infection-style spreading ensures updates reach all nodes within O(log N) rounds
        pitfalls:
        - Version conflicts
        - Infinite propagation
        - Network floods
        concepts:
        - Push gossip
        - Versioning
        - Epidemic spread
        skills:
        - Asynchronous message broadcasting
        - Vector clock implementation
        - Network protocol design
        - Eventual consistency patterns
        deliverables:
        - Random peer selection for fanout-based message dissemination
        - State delta calculation identifying changes since last gossip round
        - Push message sending transmitting updates to selected peer nodes
        - Infection-style spread propagating updates through transitive gossip rounds
        estimated_hours: 4-5
      - id: gossip-protocol-m3
        name: Pull Gossip & Anti-Entropy
        description: Add pull-based reconciliation for consistency.
        acceptance_criteria:
        - Pull mechanism requests and receives missing data from selected peer nodes
        - Anti-entropy performs periodic full state synchronization between random peer pairs
        - Combine push and pull gossip for efficient bidirectional data dissemination
        - Handle network partitions gracefully by converging state after partition heals
        pitfalls:
        - Digest size
        - Sync storms
        - Stale data during partition
        concepts:
        - Pull gossip
        - Anti-entropy
        - State reconciliation
        skills:
        - State reconciliation algorithms
        - Merkle tree or digest construction
        - Pull-based synchronization patterns
        - Partition tolerance handling
        deliverables:
        - Pull request and response protocol for data reconciliation between peers
        - State reconciliation comparing digests and exchanging missing entries
        - Anti-entropy repair mechanism for periodic full state synchronization
        - Consistency convergence ensuring all nodes reach same state eventually
        estimated_hours: 4-5
      - id: gossip-protocol-m4
        name: Failure Detection
        description: Implement gossip-based failure detection (SWIM-style).
        acceptance_criteria:
        - Probe random peers periodically and mark responsive nodes as alive
        - Indirect probing through helper peers when direct probe fails to respond
        - Suspicion mechanism waits for confirmation before declaring peer as dead
        - Disseminate membership changes by piggybacking on regular gossip messages
        pitfalls:
        - False positives
        - Suspicion timeout tuning
        - Split brain
        concepts:
        - SWIM protocol
        - Failure detection
        - Suspicion
        skills:
        - Heartbeat mechanism implementation
        - Timeout-based failure detection
        - Indirect probing techniques
        - Distributed consensus fundamentals
        deliverables:
        - Heartbeat mechanism with configurable probe interval for liveness checking
        - Failure suspicion protocol using indirect probing before declaring dead
        - SWIM protocol basics implementing probe, probe-req, and suspicion phases
        - Failure dissemination broadcasting membership changes via gossip piggyback
        estimated_hours: 4-6
    - id: 2pc-impl
      name: Two-Phase Commit
      description: Distributed transactions
      difficulty: advanced
      estimated_hours: 15-25
      essence: Distributed atomic commitment protocol coordinating multiple participants through synchronized prepare and commit phases with durable write-ahead logging, blocking coordinator-based consensus, and failure recovery mechanisms to achieve all-or-nothing transaction semantics across unreliable networks.
      why_important: Building this teaches you the foundational consensus mechanism underlying distributed databases and microservices, giving you practical experience with coordinator-based protocols, failure recovery, and the trade-offs between consistency and availability that appear in every distributed system.
      learning_outcomes:
      - Implement durable write-ahead logging for transaction recovery
      - Design coordinator state machines with prepare and commit phases
      - Build participant voting logic with timeout handling
      - Handle coordinator failure recovery from persistent logs
      - Implement participant crash recovery and transaction rollback
      - Debug blocking scenarios and coordinator-participant deadlocks
      - Design idempotent commit/abort operations for retry safety
      - Test distributed failure scenarios including network partitions
      skills:
      - Atomic Commitment
      - Distributed Consensus
      - Write-Ahead Logging
      - Coordinator Recovery
      - Two-Phase Locking
      - Failure Handling
      - Transaction Isolation
      - State Machine Design
      tags:
      - advanced
      - atomic-commit
      - coordinator
      - distributed
      - go
      - implementation
      - java
      - python
      - recovery
      architecture_doc: architecture-docs/2pc-impl/index.md
      languages:
        recommended:
        - Go
        - Java
        - Python
        also_possible:
        - Rust
        - Erlang
      resources:
      - name: 2PC Paper
        url: https://www.cs.princeton.edu/courses/archive/fall16/cos418/papers/bernstein-ch7.pdf
        type: paper
      - name: Distributed Transactions
        url: https://www.the-paper-trail.org/post/2014-10-16-consensus-protocols-two-phase-commit/
        type: article
      prerequisites:
      - type: skill
        name: Distributed systems
      - type: skill
        name: Transaction concepts
      - type: skill
        name: Failure handling
      milestones:
      - id: 2pc-impl-m1
        name: Transaction Log
        description: Implement durable transaction log for recovery.
        acceptance_criteria:
        - Write-ahead log persists every state transition to disk before the coordinator acts on it
        - Log records capture PREPARE, COMMIT, and ABORT states with associated participant identifiers
        - Coordinator process survives kill-9 and recovers correct transaction state from the log on restart
        - Recovery procedure replays the log and resumes any in-progress two-phase commit protocol rounds
        pitfalls:
        - Incomplete writes
        - Recovery order
        - Log truncation
        concepts:
        - Write-ahead logging
        - Durability
        - Crash recovery
        skills:
        - WAL implementation
        - Fsync and durability guarantees
        - Log replay and recovery
        - Structured logging formats
        deliverables:
        - Write-ahead log with fsync durability guaranteeing persistence before acknowledgment
        - Log record format containing tx_id, state, and participant list for each transaction
        - Recovery function that replays the log on restart to restore last-known transaction states
        - Log compaction and truncation that removes committed entries while preserving incomplete ones
        estimated_hours: 3-4
      - id: 2pc-impl-m2
        name: Prepare Phase
        description: Implement the voting/prepare phase.
        acceptance_criteria:
        - Coordinator broadcasts PREPARE to every participant listed in the transaction record
        - Each participant votes YES only after successfully acquiring all required resource locks
        - Participant persists its vote to its local log before sending the vote response to the coordinator
        - Coordinator aborts the transaction if any participant vote is not received within the configured timeout
        pitfalls:
        - Logging before response
        - Vote timeout
        - Participant crash after voting
        concepts:
        - Voting protocol
        - Prepare phase
        - Participant state
        skills:
        - Distributed voting protocols
        - RPC and network communication
        - Timeout and retry handling
        - State machine design
        deliverables:
        - Coordinator sends PREPARE message to all registered participants for a given transaction
        - Participants acquire necessary locks and respond with VOTE_COMMIT or VOTE_ABORT
        - Timeout handling that aborts the transaction when a participant fails to respond within deadline
        - Prepare state persisted to the write-ahead log before any vote response is sent
        estimated_hours: 4-5
      - id: 2pc-impl-m3
        name: Commit Phase
        description: Implement the decision/commit phase.
        acceptance_criteria:
        - All YES votes result in COMMIT decision; any single NO vote results in global ABORT
        - Coordinator logs the final decision to stable storage before sending it to any participant
        - Coordinator sends the decision message to all participants and waits for acknowledgments
        - Transaction completes only after all participant acknowledgments are received or timed out
        pitfalls:
        - Decision before logging
        - Lost messages
        - Participant uncertainty
        concepts:
        - Commit protocol
        - Atomic commitment
        - Decision logging
        skills:
        - Atomic broadcast protocols
        - Distributed consensus implementation
        - Message ordering and delivery
        - Transaction finalization
        deliverables:
        - Coordinator decides COMMIT if all participants voted yes, ABORT otherwise
        - COMMIT or ABORT message broadcast reliably to all participants in the transaction
        - Participants apply committed changes and release all acquired locks upon receiving COMMIT
        - Acknowledgment collection from all participants confirming they have applied the decision
        estimated_hours: 4-5
      - id: 2pc-impl-m4
        name: Failure Recovery
        description: Handle coordinator and participant failures.
        acceptance_criteria:
        - Coordinator restarts after a crash and correctly re-drives any in-progress transactions to completion
        - Participant that times out waiting for a decision queries the coordinator or other participants for the outcome
        - Participant recovers after crash by consulting its local log and contacting the coordinator if needed
        - Blocking scenario is detected and handled when coordinator is down and participants are in uncertain state
        pitfalls:
        - In-doubt blocking
        - Coordinator single point of failure
        - Network partitions
        concepts:
        - Crash recovery
        - Blocking protocols
        - Failure handling
        skills:
        - Crash-recovery algorithms
        - Coordinator failover mechanisms
        - Distributed deadlock detection
        - Handling partial failures
        - Network partition tolerance
        deliverables:
        - Coordinator crash recovery that re-reads the log and resumes the protocol from last known state
        - Participant crash recovery that checks its local log and queries the coordinator for unknown outcomes
        - Blocking protocol handling that detects and manages scenarios where the coordinator is unreachable
        - Presumed abort optimization that reduces log writes by assuming abort for missing decisions
        estimated_hours: 4-6
    - id: service-mesh
      name: Service Mesh
      description: Sidecar proxy for service-to-service communication
      difficulty: advanced
      estimated_hours: '55'
      essence: Layer-7 traffic interception with iptables redirection, dynamic endpoint resolution through service registry integration, cryptographic identity establishment via mutual TLS handshakes with automated certificate lifecycle management, and stateful connection routing using hashing-based and connection-tracking load balancing algorithms.
      why_important: Service meshes like Istio and Linkerd are essential for microservices. Understanding the proxy layer helps with debugging and optimization.
      learning_outcomes:
      - Implement transparent traffic interception
      - Build service discovery integration
      - Handle mTLS between services
      - Implement advanced load balancing
      skills:
      - Network Traffic Interception
      - Service Discovery Integration
      - Mutual TLS (mTLS)
      - Certificate Management
      - Load Balancing Algorithms
      - Proxy Protocol Implementation
      - iptables Configuration
      - Consistent Hashing
      tags:
      - advanced
      - distributed-systems
      - envoy
      - istio
      - microservices
      - mtls
      - networking
      - service
      - sidecar
      architecture_doc: architecture-docs/service-mesh/index.md
      languages:
        recommended:
        - Go
        - Rust
        - C++
        also_possible: []
      resources:
      - name: Envoy Proxy Documentation
        url: https://www.envoyproxy.io/docs
        type: documentation
      - name: Linux Transparent Proxy Support
        url: https://docs.kernel.org/networking/tproxy.html
        type: documentation
      - name: Go and Proxy Servers Tutorial
        url: https://eli.thegreenplace.net/2022/go-and-proxy-servers-part-1-http-proxies/
        type: tutorial
      - name: Consistent Hashing Explained
        url: https://www.geeksforgeeks.org/system-design/consistent-hashing/
        type: article
      - name: Google Cloud Service Mesh mTLS Tutorial
        url: https://cloud.google.com/service-mesh/docs/tutorials/mtls
        type: tutorial
      prerequisites:
      - type: project
        id: api-gateway
      - type: project
        id: circuit-breaker
      milestones:
      - id: service-mesh-m1
        name: Traffic Interception
        description: Intercept inbound and outbound traffic transparently using iptables redirect rules.
        acceptance_criteria:
        - Intercept all inbound and outbound traffic via iptables redirect rules
        - Handle transparent proxying without requiring application code changes
        - Parse and identify HTTP and gRPC protocols from intercepted traffic
        - Support pass-through for unknown or unrecognized protocol traffic
        pitfalls:
        - Redirect loop if proxy traffic not excluded
        - SO_ORIGINAL_DST not available on all systems
        - IPv6 requires separate ip6tables rules
        concepts:
        - Transparent proxying with iptables REDIRECT and TPROXY targets
        - SO_ORIGINAL_DST socket option for destination address recovery
        - iptables OWNER module for excluding proxy process traffic
        - Network namespaces and traffic redirection chains
        skills:
        - iptables configuration and rule management
        - Socket programming with getsockopt
        - Linux networking and packet filtering
        - IPv4/IPv6 dual-stack networking
        deliverables:
        - Iptables rules redirecting pod traffic through sidecar proxy
        - Sidecar proxy process intercepting all inbound and outbound connections
        - Transparent proxying forwarding traffic without application changes
        - Protocol detection identifying HTTP, gRPC, and TCP traffic automatically
        estimated_hours: '14'
      - id: service-mesh-m2
        name: Service Discovery Integration
        description: Integrate with service discovery (Consul, Kubernetes) to resolve service names to endpoints.
        acceptance_criteria:
        - Integrate with Kubernetes or Consul for service endpoint discovery
        - Handle endpoint additions and removals as services scale
        - Support DNS-based service discovery as alternative to registry
        - Cache discovered service endpoints locally for fast lookup
        pitfalls:
        - Watch connection drops require reconnection logic
        - Stale cache serves dead endpoints
        - DNS caching conflicts with dynamic discovery
        concepts:
        - Service discovery watch APIs and long-polling mechanisms
        - DNS-based service discovery versus API-based discovery
        - Endpoint health checking and cache invalidation strategies
        - Consul/Kubernetes API clients and event streams
        skills:
        - REST API integration and polling
        - Event-driven programming with watches
        - Distributed system failure handling
        - Cache coherency and TTL management
        deliverables:
        - Endpoint discovery fetching available service instance addresses
        - Service registry synchronization keeping local state up to date
        - Health status tracking monitoring liveness of discovered endpoints
        - Client-side load balancing distributing requests across healthy instances
        estimated_hours: '14'
      - id: service-mesh-m3
        name: mTLS and Certificate Management
        description: Implement mutual TLS between services with automatic certificate rotation.
        acceptance_criteria:
        - Generate unique X.509 certificates for each service instance
        - Enforce mutual TLS authentication between all mesh services
        - Handle automatic certificate rotation before certificates expire
        - Verify service identity using certificate subject or SPIFFE ID
        pitfalls:
        - Certificate rotation during active requests causes failures
        - Clock skew makes valid certificates appear expired
        - Missing SAN in certificate breaks modern TLS verification
        concepts:
        - X.509 certificates and Subject Alternative Names
        - Certificate rotation without dropping active connections
        - TLS handshake and mutual authentication flow
        - Certificate signing requests and CA integration
        skills:
        - TLS/SSL certificate management
        - Cryptographic key generation and rotation
        - Time synchronization and clock skew handling
        - Certificate authority integration
        deliverables:
        - X.509 certificate generation for each service identity
        - Automatic certificate rotation before expiration deadline
        - Mutual TLS enforcement requiring both client and server certificates
        - SPIFFE identity integration providing verifiable service identities
        estimated_hours: '14'
      - id: service-mesh-m4
        name: Load Balancing Algorithms
        description: Implement advanced load balancing including round-robin, least connections, weighted, and consistent hashing.
        acceptance_criteria:
        - Implement round-robin distribution across healthy service instances
        - Support least-connections routing to minimize backend load
        - Handle weighted distribution based on configured instance capacity
        - Implement health-aware routing excluding failed instances from selection
        pitfalls:
        - Consistent hashing needs ring rebuild on endpoint change
        - Least connections can thundering herd to recovered endpoint
        - Weight=0 causes division by zero
        concepts:
        - Consistent hashing with virtual nodes and ring structure
        - Least connections tracking across concurrent requests
        - Weighted round-robin with dynamic weight adjustment
        - Load balancing state synchronization and health-aware routing
        skills:
        - Algorithm implementation and data structures
        - Concurrency control and atomic operations
        - Performance profiling and optimization
        - Statistical load distribution
        deliverables:
        - Round-robin algorithm distributing requests evenly across backends
        - Least-connections algorithm routing to least loaded backend
        - Weighted distribution algorithm proportioning traffic by server capacity
        - Locality-aware routing preferring backends in same availability zone
        estimated_hours: '14'
    - id: rate-limiter-distributed
      name: Distributed Rate Limiter
      description: Rate limiting across multiple nodes
      difficulty: intermediate
      estimated_hours: '25'
      essence: Atomic counter operations and timestamp tracking across distributed nodes using Redis Lua scripts to enforce accurate rate limits while handling clock skew, race conditions, and burst traffic patterns in a multi-server environment.
      why_important: Rate limiting protects services from abuse and ensures fair resource usage. Understanding algorithms helps choose the right approach.
      learning_outcomes:
      - Implement token bucket and sliding window algorithms
      - Design distributed rate limiting with Redis
      - Handle burst traffic gracefully
      - Build multi-tier rate limiting
      skills:
      - Token Bucket Algorithm
      - Sliding Window Techniques
      - Redis Lua Scripting
      - Distributed Synchronization
      - Atomic Operations
      - Multi-tier Policies
      - Clock Skew Handling
      tags:
      - backend
      - coordination
      - databases
      - distributed
      - distributed-systems
      - fairness
      - intermediate
      - redis
      - reliability
      architecture_doc: architecture-docs/rate-limiter-distributed/index.md
      languages:
        recommended:
        - Go
        - Java
        - Python
        also_possible: []
      resources:
      - name: Redis Rate Limiting Tutorial
        url: https://redis.io/tutorials/howtos/ratelimiting/
        type: tutorial
      - name: Rate Limiting Algorithms Explained
        url: https://blog.algomaster.io/p/rate-limiting-algorithms-explained-with-code
        type: article
      - name: Designing a Distributed Rate Limiter
        url: https://blog.algomaster.io/p/designing-a-distributed-rate-limiter
        type: article
      - name: Token Bucket with Redis and Java
        url: https://foojay.io/today/token-bucket-rate-limiter-redis-java/
        type: tutorial
      - name: Sliding Window Rate Limiter Implementation
        url: https://arpitbhayani.me/blogs/sliding-window-ratelimiter/
        type: article
      prerequisites:
      - type: project
        id: rate-limiter
        name: Rate Limiter
      - type: skill
        name: Redis basics
      - type: skill
        name: Distributed systems concepts
      milestones:
      - id: rate-limiter-distributed-m1
        name: Rate Limiting Algorithms
        description: Implement token bucket, sliding window log, and sliding window counter algorithms.
        acceptance_criteria:
        - Implement sliding window counter handling boundary transitions
        - Support token bucket algorithm with burst and refill rate
        - Handle window boundary transitions without count loss
        - Configure rate limit threshold and time window size
        pitfalls:
        - Token bucket allows burst above sustained rate
        - Sliding window log uses O(n) memory per key
        - Counter approximation can allow 2x limit at window boundary
        concepts:
        - Token bucket algorithm with refill rate and capacity
        - Sliding window log timestamp-based request tracking
        - Sliding window counter with weighted bucket aggregation
        - Time-based atomic operations in distributed systems
        - Memory vs accuracy tradeoffs in rate limiting
        skills:
        - Algorithm implementation and complexity analysis
        - Time-series data structure design
        - Atomic operations and race condition handling
        - Performance profiling and memory optimization
        deliverables:
        - Token bucket implementation with configurable rate and capacity
        - Sliding window counter implementation for fixed-window smoothing
        - Leaky bucket option providing steady outflow rate
        - Algorithm comparison benchmark measuring accuracy and performance
        estimated_hours: '12.5'
      - id: rate-limiter-distributed-m2
        name: Multi-tier Rate Limiting
        description: Implement hierarchical rate limiting with per-user, per-API, and global limits.
        acceptance_criteria:
        - Support both per-user and per-IP rate limit tiers simultaneously
        - Implement global rate limits aggregating across all clients
        - Handle burst allowance permitting short spikes above steady rate
        - Support different rate limit thresholds per API endpoint
        pitfalls:
        - Checking all tiers even after one fails wastes resources
        - Different tier windows cause confusing UX
        - Adaptive limiting oscillates under variable load
        concepts:
        - Hierarchical resource allocation and limit enforcement
        - Multi-dimensional rate limit key composition
        - Short-circuit evaluation for cascading checks
        - Adaptive rate limiting with feedback loops
        - Priority-based request handling in quota systems
        skills:
        - Multi-tier architecture design
        - Configuration hierarchy management
        - Performance optimization through early termination
        - Dynamic threshold adjustment algorithms
        deliverables:
        - Per-user rate limit tier with individual quotas
        - Per-IP rate limit tier for unauthenticated traffic
        - Global rate limit tier protecting overall system capacity
        - Quota management tracking usage across all tiers
        estimated_hours: '12.5'
      - id: rate-limiter-distributed-m3
        name: Redis Backend Integration
        description: Implement Redis-backed rate limiting with atomic Lua scripts for cluster-wide state sharing.
        acceptance_criteria:
        - Rate limit state is stored in Redis with atomic increment operations
        - Lua scripts ensure atomicity of check-and-update operations
        - Graceful degradation when Redis is unavailable (fallback to local limiting)
        - Connection pooling and retry logic for Redis operations
        pitfalls:
        - Race conditions without atomic operations
        - Redis single point of failure
        - Clock skew between nodes
        concepts:
        - Redis Lua scripting
        - atomic operations
        - connection pooling
        - graceful degradation
        skills:
        - Redis Lua scripting
        - Atomic operations
        - Failover patterns
        deliverables:
        - Redis-backed rate limiter with Lua scripts and failover
        - Atomic Lua script implementing token bucket algorithm with nanosecond precision timestamps
        - Connection pool manager with health checking and automatic failover to replica nodes
        - Local cache fallback providing degraded rate limiting when Redis is unavailable
        estimated_hours: '5'
      - id: rate-limiter-distributed-m4
        name: Consistent Hashing & Sharding
        description: Distribute rate limit counters across multiple Redis nodes using consistent hashing for horizontal scaling.
        acceptance_criteria:
        - Rate limit keys are distributed across Redis nodes via consistent hashing
        - Adding/removing nodes causes minimal key redistribution
        - Hot key detection and automatic rebalancing
        - Health checking and automatic failover for Redis nodes
        pitfalls:
        - Uneven distribution without virtual nodes
        - Split-brain during network partitions
        concepts:
        - consistent hashing
        - virtual nodes
        - hot key detection
        - rebalancing
        skills:
        - Consistent hashing
        - Sharding
        - Health checking
        deliverables:
        - Sharded rate limiter with consistent hashing and node management
        - Consistent hash ring implementation with virtual nodes for balanced distribution
        - Hot key detection monitoring skewed access patterns and triggering replication
        - Node rebalancing tool migrating rate limit state during cluster topology changes
        estimated_hours: '5'
      - id: rate-limiter-distributed-m5
        name: Rate Limit API & Dashboard
        description: Build REST API for dynamic rate limit configuration and a real-time dashboard showing usage metrics.
        acceptance_criteria:
        - REST API for CRUD operations on rate limit rules
        - Dynamic rule updates without service restart
        - Real-time dashboard showing current usage vs limits per key
        - Rate limit headers in responses (X-RateLimit-Limit, X-RateLimit-Remaining, Retry-After)
        pitfalls:
        - Configuration propagation delay
        - Dashboard overwhelming the rate limiter with queries
        concepts:
        - dynamic configuration
        - rate limit headers
        - real-time metrics
        skills:
        - REST API design
        - Dynamic configuration
        - Real-time monitoring
        deliverables:
        - Rate limit management API and monitoring dashboard
        - REST API endpoints for CRUD operations on rate limit rules with validation
        - Real-time metrics dashboard displaying current usage percentages and remaining quota per client
        - HTTP response headers (X-RateLimit-Limit, X-RateLimit-Remaining) conforming to RFC standards
        estimated_hours: '5'
    - id: saga-orchestrator
      name: Saga Orchestrator
      description: Distributed transactions with compensating actions
      difficulty: advanced
      estimated_hours: '40'
      essence: State machine-based orchestration of distributed multi-service transactions with compensating rollback logic, persistent execution state for failure recovery, and idempotent step execution to achieve eventual consistency without two-phase commit or distributed locks.
      why_important: Sagas solve the distributed transaction problem in microservices. Understanding saga patterns helps design reliable distributed workflows.
      learning_outcomes:
      - Design saga step definitions with compensations
      - Implement orchestration vs choreography patterns
      - Handle partial failures and rollback
      - Build saga state machine with persistence
      skills:
      - Distributed Transaction Patterns
      - State Machine Design
      - Compensation Logic
      - Event-Driven Architecture
      - Idempotent Operations
      - Workflow Orchestration
      - Failure Recovery
      - Transaction Isolation
      tags:
      - advanced
      - compensation
      - distributed
      - distributed-systems
      - long-running
      - microservices
      - reliability
      - rollback
      architecture_doc: architecture-docs/saga-orchestrator/index.md
      languages:
        recommended:
        - Java
        - Go
        - Python
        also_possible: []
      resources:
      - name: Saga Pattern - Microservices.io
        url: https://microservices.io/patterns/data/saga.html
        type: documentation
      - name: Azure Saga Pattern Guide
        url: https://learn.microsoft.com/en-us/azure/architecture/patterns/saga
        type: documentation
      - name: Orchestration-Based Sagas Tutorial
        url: https://microservices.io/post/sagas/2019/12/12/developing-sagas-part-4.html
        type: tutorial
      - name: Baeldung Saga Pattern Guide
        url: https://www.baeldung.com/cs/saga-pattern-microservices
        type: tutorial
      - name: Temporal Saga Pattern Explained
        url: https://temporal.io/blog/saga-pattern-made-easy
        type: article
      prerequisites:
      - type: project
        id: job-scheduler
      - type: project
        id: event-sourcing
      milestones:
      - id: saga-orchestrator-m1
        name: Saga Definition
        description: Define saga steps with forward actions and compensation handlers.
        acceptance_criteria:
        - Define saga steps each with a compensating rollback action
        - Support step dependency ordering for sequential execution
        - Handle per-step timeout configuration with sensible defaults
        - Store saga definitions for reuse across multiple executions
        pitfalls:
        - Compensation that fails leaves inconsistent state
        - Parallel steps with shared state cause race conditions
        - Timeout too short fails legitimate slow operations
        concepts:
        - Saga pattern for distributed transaction management
        - Compensation logic and rollback semantics
        - Forward action execution ordering and dependencies
        - Idempotency in distributed operations
        skills:
        - Designing compensating transactions
        - State machine modeling
        - Error handling in distributed systems
        - API design for saga steps
        deliverables:
        - Step definition specifying action function and step metadata
        - Compensation definition pairing each step with its rollback action
        - Step ordering defining execution sequence and dependencies
        - Saga metadata including name, timeout, and retry configuration
        estimated_hours: '13.5'
      - id: saga-orchestrator-m2
        name: Saga Orchestrator Engine
        description: Build the orchestrator that executes sagas, tracks state, and handles failures.
        acceptance_criteria:
        - Execute saga steps in defined sequence until completion or failure
        - Track completion status of each individual step
        - Trigger compensation chain on any step failure running rollbacks in reverse
        - Handle step execution timeouts killing or retrying long-running steps
        pitfalls:
        - Resuming saga loses in-memory step results
        - Parallel compensation can conflict
        - Network partition causes duplicate saga execution
        concepts:
        - State machine implementation and transitions
        - Concurrent execution coordination and synchronization
        - Failure detection and recovery mechanisms
        - Saga execution context and metadata tracking
        - Event-driven orchestration patterns
        skills:
        - Building workflow engines
        - Managing distributed state
        - Implementing retry and timeout logic
        - Parallel execution handling
        - Saga lifecycle management
        deliverables:
        - Saga execution engine running steps in defined sequence
        - Step invocation calling action function with input parameters
        - Failure handling detecting step errors and triggering compensation
        - Compensation execution running rollback actions in reverse order
        estimated_hours: '13.5'
      - id: saga-orchestrator-m3
        name: Saga State Persistence
        description: Persist saga state for recovery and implement idempotent step execution.
        acceptance_criteria:
        - Persist saga execution state to durable storage after each step
        - Support saga recovery after orchestrator process crash or restart
        - Track compensation progress separately from forward execution state
        - Handle idempotent step execution preventing duplicate side effects
        pitfalls:
        - JSON serialization loses datetime precision
        - Lock timeout shorter than step timeout causes issues
        - Recovery loop processes same saga repeatedly
        concepts:
        - Durable state persistence and recovery
        - Idempotency keys and duplicate detection
        - Optimistic and pessimistic locking strategies
        - Transaction isolation in distributed systems
        - Write-ahead logging for saga steps
        skills:
        - Database transaction management
        - Implementing idempotent operations
        - Distributed locking mechanisms
        - State serialization and deserialization
        - Recovery protocol implementation
        deliverables:
        - Saga instance state record storing current execution progress
        - Step completion tracking recording success or failure per step
        - Crash recovery logic resuming saga execution from last known state
        - Idempotency mechanism preventing duplicate step execution on retry
        estimated_hours: '13.5'
      - id: saga-orchestrator-m4
        name: Timeout Handling & Dead Letter Queue
        description: Handle step timeouts, stuck sagas, and failed compensations with dead letter queue and manual resolution.
        acceptance_criteria:
        - Per-step timeout configuration with automatic cancellation
        - Stuck saga detection via heartbeat timeout
        - Dead letter queue for sagas that exhaust retry attempts
        - Admin API for manual saga resolution (force complete, force compensate, skip step)
        pitfalls:
        - Timeout too short causes false positives
        - DLQ growing without monitoring
        - Compensation that itself fails
        concepts:
        - timeout patterns
        - dead letter queues
        - manual intervention
        - circuit breaker for steps
        skills:
        - Timeout patterns
        - Dead letter queues
        - Admin tooling
        deliverables:
        - Timeout handling, stuck detection, DLQ, and admin resolution API
        - Timeout watchdog detecting steps exceeding configured duration and triggering compensation
        - Dead letter queue storing failed sagas with error context for manual investigation
        - Admin API allowing manual retry, skip, or force-complete operations for stuck sagas
        estimated_hours: '8'
      - id: saga-orchestrator-m5
        name: Saga Observability & Testing
        description: Add distributed tracing integration, saga visualization, and chaos testing for saga workflows.
        acceptance_criteria:
        - Each saga execution emits trace spans for every step (forward and compensation)
        - Visual timeline showing saga progress, failures, and compensations
        - Chaos testing injects random step failures to verify compensation correctness
        - 'Metrics: saga success rate, average duration, compensation frequency per step'
        pitfalls:
        - Tracing overhead in hot path
        - Chaos tests that corrupt real data
        concepts:
        - distributed tracing
        - saga visualization
        - chaos testing
        - compensation verification
        skills:
        - Distributed tracing
        - Visualization
        - Chaos testing
        deliverables:
        - Saga observability dashboard and chaos testing suite
        - Distributed tracing integration propagating trace context through all saga steps
        - Saga visualization UI showing step dependencies, current state, and execution timeline
        - Chaos testing framework injecting random failures into saga steps to verify compensation logic
        estimated_hours: '7'
    - id: chaos-engineering
      name: Chaos Engineering Framework
      description: Fault injection and resilience testing
      difficulty: expert
      estimated_hours: '50'
      essence: Controlled failure injection into distributed systems with automated steady-state hypothesis validation, experiment orchestration with safety boundaries, and real-time system behavior measurement to systematically discover resilience weaknesses without triggering cascading production outages.
      why_important: Building this develops critical skills for designing highly available distributed systems and teaches you to think probabilistically about failure modes, which is essential for senior engineering roles in infrastructure, SRE, and platform engineering.
      learning_outcomes:
      - Implement fault injection primitives for network latency, service errors, and resource exhaustion
      - Design experiment orchestration with steady-state hypothesis validation and automated rollback
      - Build blast radius controls to limit experiment scope and prevent cascading failures
      - Develop safety mechanisms including circuit breakers and graceful degradation patterns
      - Create experiment scheduling systems with GameDay automation and runbook integration
      - Implement observability instrumentation to detect and measure system resilience metrics
      - Debug distributed system failures through controlled chaos experiments and failure analysis
      - Design automated incident response workflows triggered by chaos experiment results
      skills:
      - Distributed Systems Resilience
      - Fault Injection Techniques
      - Chaos Engineering Principles
      - Steady-State Hypothesis Testing
      - Circuit Breaker Patterns
      - Observability and Monitoring
      - Blast Radius Management
      - Incident Response Automation
      tags:
      - chaos
      - experiments
      - expert
      - fault-injection
      - framework
      - reliability
      - resilience
      - testing
      architecture_doc: architecture-docs/chaos-engineering/index.md
      languages:
        recommended:
        - Go
        - Python
        - Java
        also_possible: []
      resources:
      - name: Principles of Chaos Engineering
        url: https://principlesofchaos.org/
        type: documentation
      - name: Chaos Monkey by Netflix
        url: https://netflix.github.io/chaosmonkey/
        type: tool
      - name: Microsoft Fault Injection Testing Guide
        url: https://microsoft.github.io/code-with-engineering-playbook/automated-testing/fault-injection-testing/
        type: tutorial
      - name: Google Cloud Chaos Engineering Guide
        url: https://cloud.google.com/blog/products/devops-sre/getting-started-with-chaos-engineering
        type: article
      - name: Go Fault Injection Library
        url: https://github.com/lingrino/go-fault
        type: tool
      prerequisites:
      - type: skill
        name: HTTP server basics
      - type: project
        id: container-runtime
        name: Container Runtime
      milestones:
      - id: chaos-engineering-m1
        name: Fault Injection Framework
        description: Implement fault injection primitives (latency, errors, resource exhaustion)
        acceptance_criteria:
        - Network latency injection adds configurable delay to targeted service traffic
        - Packet loss simulation drops configurable percentage of network packets
        - Process kill fault terminates specified process and observes recovery behavior
        - CPU and memory stress faults exhaust resources to configurable utilization levels
        pitfalls:
        - tc commands require root/CAP_NET_ADMIN
        - CPU stress can affect chaos tool itself - isolate
        - Process kill needs restart mechanism or test fails
        - Probability <1.0 creates intermittent failures (realistic)
        concepts:
        - Network fault injection via traffic control (tc) and iptables
        - Resource exhaustion through cgroup limits and stress testing
        - 'Failure injection patterns: latency, packet loss, CPU/memory saturation'
        - Chaos proxy implementation for HTTP/gRPC request interception
        - Blast radius control and gradual rollout strategies
        skills:
        - Fault injection
        - Proxy interception
        - Resource limits
        deliverables:
        - Fault type definitions for latency, error, packet loss, and resource exhaustion
        - Target selection mechanism specifying affected pods, services, or network paths
        - Fault scheduling controlling when and how long faults are active
        - Fault cleanup and rollback restoring normal operation after experiment ends
        estimated_hours: '16.5'
      - id: chaos-engineering-m2
        name: Experiment Orchestration
        description: Implement experiment definition, scheduling, and safety controls
        acceptance_criteria:
        - Experiments define clear hypothesis about expected system behavior under fault conditions
        - Steady state is verified before fault injection to establish valid baseline
        - Safety abort conditions automatically stop experiment when critical thresholds are breached
        - Experiment results report whether steady state was maintained during fault injection
        pitfalls:
        - Always verify steady state BEFORE injecting faults
        - Automatic rollback is critical - manual cleanup is error-prone
        - Abort conditions should check error rates, not just availability
        - Run during business hours initially - easier to respond to issues
        concepts:
        - Steady-state hypothesis validation before fault injection
        - 'Experiment state machines: baseline, injection, recovery, validation'
        - Circuit breaker patterns for automatic experiment abort
        - 'Observability correlation: metrics, logs, traces during chaos'
        - Time-based vs. condition-based rollback strategies
        skills:
        - Experiment design
        - Safety controls
        - Rollback
        deliverables:
        - Experiment definition specifying hypothesis, faults, duration, and blast radius
        - Steady state hypothesis verification checking system health before and after faults
        - Experiment execution engine injecting faults and monitoring system behavior
        - Result analysis comparing pre-fault and post-fault steady state measurements
        estimated_hours: '16.5'
      - id: chaos-engineering-m3
        name: GameDay Automation
        description: Implement scheduled chaos experiments with runbooks and incident response
        acceptance_criteria:
        - GameDay runs multiple experiments in sequence with pauses between each
        - System metrics are monitored continuously during experiment and compared to baseline
        - Auto-rollback triggers when safety threshold breach is detected during any experiment
        - Final report summarizes each experiment outcome with pass/fail and observations
        pitfalls:
        - GameDays need preparation - brief observers on experiments
        - Manual runbook steps may be needed for complex recovery
        - Record everything - observations are valuable learning
        - Schedule GameDays when team is available to respond
        concepts:
        - 'GameDay scenarios: dependency failure, latency spikes, resource depletion'
        - Runbook automation with manual approval gates
        - Post-incident analysis and chaos experiment refinement
        - Observer briefing and real-time communication protocols
        - 'Chaos maturity progression: manual to scheduled to continuous'
        skills:
        - GameDay planning
        - Runbooks
        - Incident response
        deliverables:
        - Scenario scripting defining multi-experiment sequences for comprehensive testing
        - Automated health checks monitoring system metrics throughout experiment duration
        - Incident simulation replicating real-world failure scenarios in controlled environment
        - Recovery validation confirming system returns to normal after faults are removed
        estimated_hours: '16.5'
      - id: chaos-engineering-m4
        name: Steady-State Hypothesis & Metrics Validation
        description: Implement steady-state hypothesis definition and automated validation using system metrics during experiments.
        acceptance_criteria:
        - Experiments define steady-state hypotheses with metric thresholds (latency < 200ms, error rate < 1%)
        - Metrics are collected from the system during experiment execution
        - Automatic experiment abort if metrics breach safety thresholds
        - Post-experiment report comparing pre/during/post metrics with statistical significance
        pitfalls:
        - Metrics lag causing late abort
        - Confounding variables in results
        concepts:
        - steady-state hypothesis
        - blast radius
        - experiment safety
        - statistical validation
        skills:
        - Hypothesis testing
        - Metrics collection
        - Statistical validation
        deliverables:
        - Hypothesis-driven experiment engine with metrics validation
        - Steady-state hypothesis DSL defining expected system behavior using metrics thresholds
        - Metrics validator automatically comparing experiment and control groups using statistical tests
        - Blast radius limiter constraining experiment scope to percentage of traffic or specific services
        estimated_hours: '10'
      - id: chaos-engineering-m5
        name: Network Chaos & Infrastructure Faults
        description: Implement network-level chaos (partition, packet loss, DNS failure) and infrastructure faults (disk full, OOM).
        acceptance_criteria:
        - Network partition simulation between specified service pairs
        - Configurable packet loss, latency injection, and bandwidth throttling
        - DNS failure injection (NXDOMAIN, timeout, wrong IP)
        - 'Infrastructure faults: disk full simulation, memory pressure, CPU saturation'
        pitfalls:
        - Network chaos affecting the chaos tool itself
        - Irreversible infrastructure changes
        concepts:
        - network partitioning
        - tc/iptables
        - DNS poisoning
        - cgroups resource limits
        skills:
        - Network simulation
        - Linux networking (tc/iptables)
        - Resource limits
        deliverables:
        - Network chaos and infrastructure fault injection toolkit
        - Network partition simulator using iptables rules to isolate services or data centers
        - Packet loss and latency injector using tc netem for simulating degraded network conditions
        - Resource exhaustion tool using cgroups to simulate disk full, OOM, and CPU throttling scenarios
        estimated_hours: '10'
    - id: container-runtime
      name: Container Runtime
      description: OCI-compliant container execution
      difficulty: advanced
      estimated_hours: '50'
      essence: Kernel-level process isolation through namespace manipulation, hierarchical resource constraint enforcement via cgroup controllers, and union mount filesystem operations for layered image storage with copy-on-write semantics.
      why_important: Understanding containers at the kernel level helps debug container issues, optimize performance, and secure deployments.
      learning_outcomes:
      - Use Linux namespaces for isolation
      - Implement cgroups for resource limits
      - Build overlay filesystem for images
      - Handle container networking
      skills:
      - Linux Namespaces
      - Cgroups v2
      - Overlay Filesystem
      - System Calls
      - Process Isolation
      - Resource Limiting
      - Container Networking
      - Low-Level Runtime Design
      tags:
      - advanced
      - containers
      - images
      - layers
      - linux
      - multi-tenancy
      - oci
      - runc
      - systems
      architecture_doc: architecture-docs/container-runtime/index.md
      languages:
        recommended: *id002
        also_possible: []
      resources:
      - name: Linux Namespaces Manual
        url: https://man7.org/linux/man-pages/man7/namespaces.7.html
        type: documentation
      - name: Overlay Filesystem Documentation
        url: https://docs.kernel.org/filesystems/overlayfs.html
        type: documentation
      - name: Build Your Own Container (Go)
        url: https://www.infoq.com/articles/build-a-container-golang/
        type: tutorial
      - name: Container Runtime from Scratch
        url: https://nixpig.dev/posts/container-runtime-introduction/
        type: tutorial
      - name: Cgroups Manual Page
        url: https://man7.org/linux/man-pages/man7/cgroups.7.html
        type: documentation
      prerequisites:
      - type: project
        id: container-basic
        name: Container (Basic)
      - type: skill
        name: Linux namespaces/cgroups
      milestones:
      - id: container-runtime-m1
        name: Process Isolation with Namespaces
        description: Create isolated process environment using PID, mount, network, UTS, and user namespaces.
        acceptance_criteria:
        - Create PID namespace where container process sees itself as PID 1
        - Create mount namespace providing isolated filesystem view for container
        - Create network namespace providing isolated network interfaces and routing
        - Create UTS namespace allowing container to have its own hostname
        pitfalls:
        - pivot_root fails if new root is not a mount point
        - Forgetting to mount /proc breaks process tools
        - User namespace UID mapping required for unprivileged containers
        concepts:
        - Linux namespace types and isolation boundaries
        - System call interfaces for namespace creation and entry
        - Filesystem pivot operations and mount propagation
        - User namespace UID/GID mapping and capabilities
        skills:
        - Linux system programming
        - Process isolation techniques
        - Namespace management
        - Privilege escalation mitigation
        deliverables:
        - PID namespace setup for process ID isolation
        - Mount namespace setup for filesystem view isolation
        - Network namespace setup for network stack isolation
        - UTS and IPC namespaces for hostname and IPC isolation
        estimated_hours: '12.5'
      - id: container-runtime-m2
        name: Resource Limits with Cgroups
        description: Implement CPU, memory, and I/O limits using cgroups v2 for container resource control.
        acceptance_criteria:
        - Set memory limits using cgroups v2 memory.max controller correctly
        - Set CPU limits using cpu.max quota and period parameters
        - Monitor resource usage and report current consumption statistics
        - Handle OOM situations gracefully with configurable OOM behavior
        pitfalls:
        - Cgroups v1 vs v2 have different interfaces
        - Can't remove cgroup with active processes
        - Memory limit without swap limit allows OOM escape
        concepts:
        - Cgroup hierarchy and controller delegation
        - Resource accounting and enforcement mechanisms
        - CPU scheduling classes and quota management
        - Memory pressure handling and OOM killer behavior
        skills:
        - Resource limit configuration
        - Cgroup filesystem manipulation
        - Performance constraint implementation
        - System resource monitoring
        deliverables:
        - Cgroup controller setup for v2 unified hierarchy
        - Memory limits with hard cap and soft limit configuration
        - CPU limits using cpu.max quota and period values
        - Device access control restricting container device visibility
        estimated_hours: '12.5'
      - id: container-runtime-m3
        name: Overlay Filesystem
        description: Implement image layering using overlayfs for copy-on-write filesystem support.
        acceptance_criteria:
        - Mount overlay filesystem for container root combining image layers
        - Handle multiple image layers stacked in correct order for overlayFS
        - Support copy-on-write semantics so file changes stay in upper layer
        - Clean up overlay layers and mount points on container removal
        pitfalls:
        - Overlayfs requires specific kernel version (3.18+)
        - Work directory must be empty and same filesystem as upper
        - Hardlinks across layers cause unexpected behavior
        concepts:
        - Copy-on-write filesystem semantics
        - Layered filesystem architecture
        - Mount options and directory structure requirements
        - Inode handling across filesystem layers
        skills:
        - Filesystem layering implementation
        - Storage driver configuration
        - Image management systems
        - Kernel filesystem features
        deliverables:
        - OverlayFS mount combining multiple read-only layers with writable layer
        - Layer management for image layer stacking and ordering
        - Copy-on-write semantics for efficient file modifications
        - Layer caching to avoid redundant downloads and extraction
        estimated_hours: '12.5'
      - id: container-runtime-m4
        name: Container Networking
        description: Implement bridge networking for containers with port mapping and inter-container communication.
        acceptance_criteria:
        - Create veth pair for container and attach one end to bridge
        - Set up bridge networking connecting multiple containers on same network
        - Implement port mapping using NAT rules for host-to-container access
        - Handle container DNS resolution using configurable nameserver settings
        pitfalls:
        - Container loses network if host veth deleted before container end
        - iptables rules persist after container death
        - MTU mismatch causes packet fragmentation issues
        concepts:
        - Virtual ethernet pairs and network namespaces
        - Bridge networking and packet forwarding
        - NAT rules and port forwarding configuration
        - Network interface creation and IP assignment
        skills:
        - Virtual network configuration
        - iptables rule management
        - Bridge network setup
        - Container network isolation
        deliverables:
        - Veth pair creation connecting container to host network namespace
        - Bridge setup for connecting multiple container network interfaces
        - IP address assignment from configurable subnet pool
        - Port forwarding using NAT rules for external access
        estimated_hours: '12.5'
    - id: kubernetes-operator
      name: Kubernetes Operator
      description: Custom controller with CRDs for automated app management
      difficulty: advanced
      estimated_hours: '60'
      essence: Event-driven reconciliation of cluster state through controllers that continuously watch custom resources, compare actual state against declared specifications, and execute idempotent operations to eliminate configuration drift while handling concurrent modifications and failure scenarios.
      why_important: Operators automate complex, stateful application lifecycle management at scale, teaching you the controller pattern that powers production Kubernetes infrastructure and is essential for platform engineering roles.
      learning_outcomes:
      - Understand Kubernetes controller patterns
      - Implement CRDs and controllers
      - Handle reconciliation and state management
      - Deploy and operate custom operators
      skills:
      - Kubernetes API
      - Custom Resources
      - Controller pattern
      - Reconciliation loop
      - Leader election
      - Webhook validation
      tags:
      - advanced
      - controllers
      - crds
      - devops
      - reconciliation
      - watchers
      architecture_doc: architecture-docs/kubernetes-operator/index.md
      languages:
        recommended: *id002
        also_possible: []
      resources:
      - name: Kubernetes Operator Pattern
        url: https://kubernetes.io/docs/concepts/extend-kubernetes/operator/
        type: documentation
      - name: Custom Resources Documentation
        url: https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/
        type: documentation
      - name: Kubebuilder Book
        url: https://book.kubebuilder.io/
        type: tutorial
      - name: Operator SDK Documentation
        url: https://sdk.operatorframework.io/
        type: documentation
      - name: Operator Framework
        url: https://operatorframework.io/
        type: documentation
      prerequisites:
      - type: skill
        name: Kubernetes basics
      - type: skill
        name: Go or Python
      milestones:
      - id: kubernetes-operator-m1
        name: Custom Resource Definition
        description: Define custom resources for your domain
        acceptance_criteria:
        - CRD schema with OpenAPI validation rejects resources with missing required fields or wrong types
        - Spec and status subresources are independently updatable without overwriting each other
        - Custom printer columns display relevant fields like status and age in kubectl get output
        - CRD versioning supports multiple API versions with a storage version and conversion strategy
        pitfalls:
        - Not marking fields as required leading to nil pointer panics
        - Forgetting to enable status subresource causing update conflicts
        - Using deprecated API versions without conversion strategy
        - Omitting validation rules allowing invalid resource states
        - Not setting storage version correctly for multi-version CRDs
        concepts:
        - OpenAPI v3 schema validation in CRD spec
        - Strategic merge patch and JSON patch semantics
        - CRD versioning with conversion webhooks
        - Structural schema requirements for pruning
        - Status subresource for decoupling spec and status updates
        skills:
        - CRD schema
        - OpenAPI validation
        - Versioning
        deliverables:
        - CRD YAML manifest with OpenAPI v3 schema validation for spec fields
        - Status subresource definition enabling independent spec and status updates
        - Printer columns configuration that displays key fields in kubectl get output
        - Validation rules using CEL expressions to enforce field constraints beyond schema
        - Default values specification that auto-populates omitted optional fields
        - Multiple CRD versions with conversion webhooks for version migration
        estimated_hours: '12'
      - id: kubernetes-operator-m2
        name: Controller Setup
        description: Set up controller with client and informers
        acceptance_criteria:
        - Controller watches custom resource events and enqueues reconcile requests for each change
        - Informers provide a local cache that reduces API server load for list and watch operations
        - Connection errors and API server restarts trigger automatic reconnection with backoff
        - Leader election ensures only one controller replica actively reconciles at any time
        pitfalls:
        - Not handling informer cache sync before processing events
        - Forgetting to implement proper queue shutdown causing goroutine leaks
        - Using direct API calls instead of cached informer lookups
        - Not setting proper RBAC permissions for watched resources
        - Blocking the workqueue with long-running operations
        concepts:
        - SharedIndexInformer with resync period for cache consistency
        - Workqueue rate limiting and exponential backoff
        - Leader election for high availability deployments
        - Client-go dynamic client vs typed client tradeoffs
        - Event recording for debugging and observability
        skills:
        - Client-go
        - Informers
        - Work queue
        deliverables:
        - Kubernetes client configuration that authenticates via in-cluster or kubeconfig credentials
        - Shared informer for the custom resource that caches and watches resource events efficiently
        - Rate-limited work queue that buffers reconcile requests with exponential backoff on failure
        - Event handler functions that enqueue add, update, and delete events into the work queue
        - Controller struct that holds references to the client, informer, queue, and recorder
        - Run loop that starts informer sync and launches worker goroutines to process the queue
        estimated_hours: '12'
      - id: kubernetes-operator-m3
        name: Reconciliation Loop
        description: Implement the core reconciliation logic
        acceptance_criteria:
        - Reconciler compares the desired spec state with the actual state of all owned resources
        - Owned resources are created, updated, or deleted to converge toward the desired state
        - Status subresource is updated with current conditions, ready replicas, and last reconciliation time
        - Reconciliation errors trigger automatic requeue with exponential backoff and jitter
        pitfalls:
        - Not implementing idempotent operations causing duplicate resources
        - Forgetting to update resource status after reconciliation
        - Missing finalizer removal causing stuck deletion
        - Not handling partial failures in multi-step reconciliation
        - Infinite reconciliation loops from not checking current state
        concepts:
        - Declarative reconciliation using diff between desired and actual state
        - Finalizers for cleanup before resource deletion
        - Owner references for garbage collection and cascade deletion
        - Requeue strategies for transient errors vs permanent failures
        - Condition types for detailed status reporting
        skills:
        - Desired vs actual state
        - Idempotency
        - Error handling
        deliverables:
        - Current state fetcher that reads the live state of owned resources from the API server
        - Desired state comparator that diffs the spec against the actual state of owned resources
        - Resource mutator that creates, updates, or deletes owned resources to match the desired state
        - Status updater that writes the current reconciliation state back to the custom resource status
        - Requeue-on-error handler that requeues the resource with exponential backoff after failures
        - Idempotent operation design ensuring repeated reconciliation produces the same result
        estimated_hours: '12'
      - id: kubernetes-operator-m4
        name: Webhooks
        description: Admission webhooks for validation and mutation
        acceptance_criteria:
        - Validating webhook rejects resources with invalid field combinations and returns clear error messages
        - Mutating webhook injects default values into resource spec when optional fields are omitted
        - Webhook TLS certificates are provisioned and rotated automatically by cert-manager
        - Admission review responses include human-readable reason strings for validation failures
        pitfalls:
        - Self-signed certificates expiring without rotation strategy
        - Not handling webhook service unavailability gracefully
        - Validating webhook rejecting all requests during deployment
        - Missing namespace selector causing webhook to intercept system resources
        - Not implementing proper error responses for admission denials
        concepts:
        - Admission review request and response structure
        - Certificate management for webhook TLS endpoints
        - Webhook failure policies and timeout configuration
        - Defaulting vs validation webhook execution order
        - Dynamic admission control and namespace selectors
        skills:
        - Validating webhook
        - Mutating webhook
        - TLS
        deliverables:
        - HTTPS webhook server that listens for admission review requests from the API server
        - Validating admission webhook that rejects resources failing custom business rule checks
        - Mutating admission webhook that injects default values into newly created resources
        - TLS certificate setup using cert-manager or self-signed certificates for webhook HTTPS
        - Webhook configuration manifests that register the webhooks with the API server
        - Error response handler that returns structured admission review rejection messages
        estimated_hours: '12'
      - id: kubernetes-operator-m5
        name: Testing & Deployment
        description: Test and deploy the operator
        acceptance_criteria:
        - Unit tests with fake client cover create, update, delete, and error reconciliation paths
        - Integration tests with envtest verify end-to-end reconciliation against a real API server
        - Container image is built and tagged with the operator binary and all required dependencies
        - Deployment manifests apply RBAC permissions following the principle of least privilege
        pitfalls:
        - Not testing reconciliation with multiple concurrent updates
        - Missing RBAC permissions discovered only in production
        - Helm chart not handling upgrade scenarios correctly
        - Not testing operator behavior during API server unavailability
        - Forgetting to test finalizer cleanup in deletion scenarios
        concepts:
        - Envtest for integration testing with real API server
        - Controller runtime fake client for unit testing
        - Helm chart templating with values for configurability
        - RBAC ClusterRole vs Role scope decisions
        - Namespace-scoped vs cluster-scoped operator deployment
        skills:
        - Envtest
        - Helm charts
        - RBAC
        deliverables:
        - Unit tests using a fake Kubernetes client that verify reconciliation logic without a cluster
        - Integration tests using envtest that run the controller against a real API server in-process
        - RBAC role and role-binding manifests scoped to the minimum required permissions
        - Deployment manifests including the controller Deployment, ServiceAccount, and Namespace
        - Helm chart with configurable values for image, replicas, resources, and RBAC settings
        - Leader election configuration that prevents multiple controller replicas from reconciling simultaneously
        estimated_hours: '12'
    - id: gitops-deployment
      name: GitOps Deployment System
      description: Git-driven deployments like ArgoCD with sync, rollback
      difficulty: advanced
      estimated_hours: '55'
      essence: Continuous state reconciliation between Git-declared manifests and live Kubernetes clusters through automated control loops that perform three-way diffs (Git source, cached state, actual cluster resources), detecting configuration drift and applying corrective actions to maintain eventual consistency across distributed infrastructure.
      why_important: Building this teaches you production-grade infrastructure automation patterns used at scale, combining distributed systems concepts (reconciliation loops, eventual consistency) with practical Kubernetes API programming and Git-driven declarative configuration management that's becoming the industry standard for cloud-native deployments.
      learning_outcomes:
      - Understand GitOps principles
      - Implement Git-driven deployments
      - Build sync and health monitoring
      - Handle rollback and recovery
      skills:
      - Git operations
      - Kubernetes API
      - Reconciliation
      - Diff detection
      - Rollback
      - Health checks
      tags:
      - advanced
      - argocd
      - declarative
      - devops
      - flux
      - sync
      architecture_doc: architecture-docs/gitops-deployment/index.md
      languages:
        recommended:
        - Python
        - Go
        - YAML
        also_possible: []
      resources:
      - name: Argo CD Documentation
        url: https://argo-cd.readthedocs.io/en/stable/
        type: documentation
      - name: OpenGitOps Principles
        url: https://opengitops.dev/
        type: documentation
      - name: Kubernetes API Overview
        url: https://kubernetes.io/docs/reference/using-api/
        type: documentation
      - name: Google Cloud GitOps Tutorial
        url: https://cloud.google.com/kubernetes-engine/docs/tutorials/gitops-cloud-build
        type: tutorial
      - name: Kubernetes client-go Library
        url: https://github.com/kubernetes/client-go
        type: tool
      prerequisites:
      - type: skill
        name: Kubernetes basics
      - type: skill
        name: Git
      - type: skill
        name: YAML/Helm
      milestones:
      - id: gitops-deployment-m1
        name: Git Repository Sync
        description: Clone and sync Git repositories
        acceptance_criteria:
        - Clone Git repository from remote URL with correct branch checked out
        - Poll for changes periodically and detect new commits since last sync
        - Support webhook triggers for immediate sync without polling delay
        - Track current synced commit SHA for comparison with remote HEAD
        pitfalls:
        - Not handling repository authentication failures gracefully
        - Cloning entire history instead of shallow clones
        - Missing webhook signature verification allowing unauthorized triggers
        - Hardcoding credentials instead of using secret management
        - Not implementing exponential backoff for polling failures
        concepts:
        - Shallow vs deep cloning for repository efficiency
        - Git authentication methods (SSH keys, tokens, deploy keys)
        - Webhook payload validation and signature verification
        - Repository polling intervals and rate limiting strategies
        - Detached HEAD state and branch tracking in automation
        skills:
        - Git operations
        - Polling/webhooks
        - Credential management
        deliverables:
        - Repository cloning from remote URL with branch selection
        - Branch and tag tracking for monitoring multiple ref targets
        - Polling for changes with configurable interval and commit comparison
        - Webhook receiver for immediate sync on push event notification
        - SSH and HTTPS credential management for private repository access
        - Repository caching to avoid redundant full clones on each sync
        estimated_hours: '11'
      - id: gitops-deployment-m2
        name: Manifest Generation
        description: Generate Kubernetes manifests from source
        acceptance_criteria:
        - Support plain Kubernetes YAML manifest files read from repository
        - Support Kustomize overlays generating manifests from base and patches
        - Support Helm chart rendering with configurable values and release names
        - Validate generated manifests against Kubernetes schema before applying
        pitfalls:
        - Not validating generated YAML syntax before applying
        - Hardcoding environment-specific values in base manifests
        - Missing namespace declarations in generated resources
        - Circular dependencies in Helm chart value overrides
        - Not handling missing or invalid template variables
        concepts:
        - Helm template rendering and value merging hierarchy
        - Kustomize overlay patches and strategic merge patches
        - YAML anchors and references for DRY manifests
        - Manifest templating vs parameterization trade-offs
        - Directory structure conventions for manifest organization
        skills:
        - YAML processing
        - Helm
        - Kustomize
        deliverables:
        - Plain YAML reading and parsing for raw Kubernetes manifests
        - Helm template rendering from chart with values file overrides
        - Kustomize build processing overlays and patches into final manifests
        - Parameter overrides for environment-specific value customization
        - Environment-specific values supporting dev, staging, and production configs
        - Manifest validation checking resource schema before deployment
        estimated_hours: '11'
      - id: gitops-deployment-m3
        name: Sync & Reconciliation
        description: Apply manifests and reconcile state
        acceptance_criteria:
        - Compare desired state from git with actual state from cluster accurately
        - Apply diffs to reach desired state using server-side apply mechanism
        - Handle resource creation, update, and deletion during reconciliation cycle
        - Support sync hooks for pre-sync and post-sync custom resource execution
        pitfalls:
        - Applying manifests without dry-run validation first
        - Not implementing proper resource ordering causing failures
        - Pruning resources still in use by other applications
        - Missing rollback plan when apply operations fail
        - Not handling API server rate limiting during bulk applies
        concepts:
        - Three-way merge for declarative state reconciliation
        - Kubernetes server-side apply vs client-side apply
        - Resource pruning strategies and orphan detection
        - Dry-run mode for validation before actual changes
        - Resource ordering and dependency management during apply
        skills:
        - K8s apply
        - Diff detection
        - Pruning
        deliverables:
        - Manifest diff calculation comparing desired state against live cluster
        - Apply with server-side apply for declarative resource management
        - Resource pruning removing orphaned resources not in desired state
        - Sync waves and hooks for ordered multi-phase deployment execution
        - Selective sync allowing targeted resource-level sync operations
        - Dry-run mode previewing changes without modifying cluster state
        estimated_hours: '11'
      - id: gitops-deployment-m4
        name: Health Assessment
        description: Monitor application and resource health
        acceptance_criteria:
        - Monitor deployed resource health using kind-specific status field inspection
        - Detect degraded, progressing, and healthy states based on resource status
        - Track sync errors and warnings with detailed messages for troubleshooting
        - Support custom health checks using user-defined evaluation scripts or rules
        pitfalls:
        - Relying only on Pod phase without checking conditions
        - Not distinguishing between progressing and degraded states
        - Setting health check timeouts too short for startup
        - Missing health checks for custom resource definitions
        - Not monitoring dependent resources like ConfigMaps or Secrets
        concepts:
        - Kubernetes resource status conditions and phases
        - Custom health check logic beyond readiness probes
        - Aggregating health status across resource dependencies
        - Pod restart policies and backoff strategies
        - Health check timeout and retry configuration
        skills:
        - Health checks
        - Status aggregation
        - Custom health
        deliverables:
        - Built-in health checks per Kubernetes resource kind (Deployment, Service, etc.)
        - Deployment rollout status monitoring tracking replica readiness
        - Custom health scripts for application-specific health evaluation
        - Application health aggregation combining individual resource health status
        - Degraded vs healthy vs progressing state classification for resources
        - Health history tracking status changes over time for trend analysis
        estimated_hours: '11'
      - id: gitops-deployment-m5
        name: Rollback & History
        description: Track history and enable rollback
        acceptance_criteria:
        - Store deployment history with commit SHA, timestamp, and sync result
        - Support rollback to any previous revision by re-syncing its manifest set
        - Show diff between any two revisions for change review and comparison
        - Track who deployed what and when for audit and compliance requirements
        pitfalls:
        - Not preserving enough history for meaningful rollbacks
        - Losing audit trail when resources are deleted
        - Implementing rollback without validating target revision exists
        - Missing timestamps and user attribution in audit logs
        - Not handling partial rollbacks when only some resources fail
        concepts:
        - Git commit SHA as immutable deployment identifier
        - Revision history storage in Kubernetes annotations or CRDs
        - Rollback as reapplying previous manifest versions
        - Audit logging for compliance and debugging purposes
        - Comparing live state with desired state for drift detection
        skills:
        - Version tracking
        - Rollback
        - Audit
        deliverables:
        - Revision history storage recording deployed commits and manifest hashes
        - Rollback to specific revision re-deploying previous manifest set
        - Auto-rollback on failure reverting to last known good deployment
        - Deployment annotations recording sync metadata on deployed resources
        - Audit logging tracking who deployed what and when for compliance
        - Sync status tracking showing current and historical sync outcomes
        estimated_hours: '11'
    expert:
    - id: build-raft
      name: Build Your Own Raft
      description: Consensus algorithm
      difficulty: expert
      estimated_hours: 60-100
      essence: Leader-based replicated state machine using term-numbered log entries, heartbeat timeouts, and majority quorums to achieve linearizable consensus across crash-recovery failures and network partitions.
      why_important: Building Raft teaches foundational distributed systems principles used in production databases (etcd, CockroachDB), key-value stores (Consul), and any system requiring fault-tolerant coordination, making it essential knowledge for infrastructure and backend engineering roles.
      learning_outcomes:
      - Implement leader election with randomized timeouts and term-based voting
      - Design log replication with AppendEntries RPC and commit index tracking
      - Build persistent state recovery after node crashes and restarts
      - Debug network partition scenarios and split-brain prevention
      - Implement snapshot mechanisms for log compaction
      - Handle edge cases in distributed timing and failure detection
      - Design RPC communication between cluster nodes
      - Verify safety properties through invariant testing
      skills:
      - Distributed Consensus
      - Leader Election Algorithms
      - Log Replication
      - Fault Tolerance
      - RPC Communication
      - State Machine Replication
      - Cluster Coordination
      - Network Partition Handling
      tags:
      - algorithms
      - build-from-scratch
      - consensus
      - distributed
      - expert
      - go
      - java
      - leader-election
      - log-replication
      - rust
      - state-machine
      architecture_doc: architecture-docs/build-raft/index.md
      languages:
        recommended:
        - Go
        - Rust
        - Java
        also_possible: []
      resources:
      - type: paper
        name: In Search of an Understandable Consensus Algorithm
        url: https://raft.github.io/raft.pdf
      - type: tool
        name: Raft Visualization
        url: https://raft.github.io/
      prerequisites:
      - type: skill
        name: Replicated log
      - type: skill
        name: Leader election
      - type: skill
        name: Distributed systems basics
      milestones:
      - id: build-raft-m1
        name: Leader Election
        description: Implement Raft leader election with terms and voting.
        acceptance_criteria:
        - Nodes increment term and transition to candidate after election timeout expires
        - Election timeouts are randomized within configured range to reduce split votes
        - RequestVote RPC includes candidate term, log index, and log term fields
        - Nodes grant vote only if candidate log is at least as up-to-date as own
        pitfalls:
        - Split vote scenarios
        - Term number handling
        - Log up-to-date check
        concepts:
        - Leader election
        - Distributed voting
        - Failure detection
        skills:
        - Implementing distributed state machines
        - Managing election timeouts and heartbeats
        - Handling concurrent vote requests
        - Testing fault-tolerant distributed systems
        deliverables:
        - Term management tracking current term and voted-for state
        - Election timeout randomization preventing simultaneous candidate elections
        - RequestVote RPC requesting votes from peer nodes in cluster
        - Vote granting rules ensuring at most one leader per term
        estimated_hours: 12-18
      - id: build-raft-m2
        name: Log Replication
        description: Implement log replication from leader to followers.
        acceptance_criteria:
        - AppendEntries RPC sends log entries with prevLogIndex and prevLogTerm for consistency
        - Followers reject AppendEntries if their log does not match at prevLogIndex
        - Leader advances commit index when entry is replicated to majority of cluster
        - Committed entries are applied to state machine in log order without gaps
        pitfalls:
        - Log index off-by-one
        - Commit index from previous term
        - Handling gaps
        concepts:
        - Log replication
        - Consistency
        - Quorum
        skills:
        - Implementing append-only log structures
        - Managing consistency across distributed nodes
        - Handling RPC retries and idempotency
        - Tracking commit and apply indices
        deliverables:
        - AppendEntries RPC replicating log entries from leader to followers
        - Log consistency check ensuring follower log matches leader prefix
        - Commit index advancement after majority replication confirmation
        - Log entry application to state machine after commit
        estimated_hours: 15-25
      - id: build-raft-m3
        name: Safety Properties
        description: Ensure Raft safety guarantees are maintained.
        acceptance_criteria:
        - Split vote scenario resolves correctly with no two leaders in same term
        - Newly elected leader contains all previously committed log entries
        - If two logs contain an entry with same index and term the logs are identical up to that index
        - All non-failed nodes apply exactly the same sequence of state machine commands
        pitfalls:
        - Committing old term entries
        - Figure 8 scenario
        - Network partition handling
        concepts:
        - Safety properties
        - Formal verification
        - Invariants
        skills:
        - Proving correctness in distributed systems
        - Implementing linearizable operations
        - Testing race conditions and edge cases
        - Handling network partitions and failures
        deliverables:
        - Election safety ensuring at most one leader per term
        - Leader completeness guaranteeing committed entries survive leader changes
        - Log matching property verifying consistent logs across nodes
        - State machine safety ensuring all nodes apply same entries in same order
        estimated_hours: 15-25
      - id: build-raft-m4
        name: Cluster Membership Changes
        description: Implement dynamic cluster membership changes.
        acceptance_criteria:
        - Joint consensus requires majority of both old and new configurations for decisions
        - Configuration changes are committed as special log entries through Raft protocol
        - New nodes catch up on log before being counted in majority calculations
        - Removed nodes stop participating in elections after configuration change commits
        pitfalls:
        - Disjoint majorities
        - Leader in transition
        - Stuck configurations
        concepts:
        - Membership changes
        - Joint consensus
        - Availability during changes
        skills:
        - Implementing two-phase configuration changes
        - Managing quorum transitions safely
        - Handling leader transfer during reconfigurations
        - Testing cluster membership edge cases
        deliverables:
        - Joint consensus protocol transitioning between old and new configurations
        - Configuration change log entries replicated through Raft protocol
        - Node addition procedure safely introducing new members to cluster
        - Node removal procedure safely removing members without losing availability
        estimated_hours: 18-32
    - id: build-distributed-kv
      name: Build Your Own Distributed KV Store
      description: Partitioning, replication
      difficulty: expert
      estimated_hours: 70-120
      essence: Distributed data partitioning through consistent hashing for load balancing, quorum-based replication with tunable consistency guarantees, and coordinated transaction commits across network-partitioned nodes handling concurrent operations under partial failures.
      why_important: Building this teaches you the core techniques powering production databases like DynamoDB and Cassandra, giving you deep expertise in distributed systems that's highly valued for backend infrastructure roles.
      learning_outcomes:
      - Implement consistent hashing with virtual nodes for even data distribution across cluster nodes
      - Design and build quorum-based replication with tunable consistency levels
      - Implement gossip protocols for cluster membership and failure detection
      - Build two-phase commit protocol for distributed transaction coordination
      - Design partition tolerance strategies handling network splits and node failures
      - Implement vector clocks or hybrid logical clocks for conflict resolution
      - Build client routing logic using partition maps and node discovery
      - Debug race conditions and consistency anomalies in concurrent distributed operations
      skills:
      - Consistent Hashing
      - Distributed Consensus
      - Quorum Replication
      - Failure Detection
      - Vector Clocks
      - Two-Phase Commit
      - Gossip Protocols
      - Network Partitioning
      tags:
      - build-from-scratch
      - consistency
      - distributed
      - expert
      - go
      - java
      - partitioning
      - replication
      - rust
      architecture_doc: architecture-docs/build-distributed-kv/index.md
      languages:
        recommended:
        - Go
        - Rust
        - Java
        also_possible:
        - C++
        - Scala
      resources:
      - type: paper
        name: 'Dynamo: Amazon''s Key-Value Store'
        url: https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf
      - type: course
        name: MIT 6.824 Distributed Systems
        url: https://pdos.csail.mit.edu/6.824/
      prerequisites:
      - type: skill
        name: Build Raft or 2PC
      - type: skill
        name: Consistent hashing
      - type: skill
        name: RPC framework
      milestones:
      - id: build-distributed-kv-m1
        name: Consistent Hashing
        description: Implement consistent hashing for key distribution across nodes.
        acceptance_criteria:
        - Hash ring with virtual nodes distributes keys uniformly across nodes with less than 10% standard deviation
        - Key lookup returns the correct responsible node by walking clockwise to the next token on the ring
        - Adding a node causes only keys between the new node and its predecessor to be redistributed
        - Removing a node causes only that node's keys to be redistributed to the next node on the ring
        pitfalls:
        - Hot spots with few nodes
        - Virtual node count tuning
        - Rebalancing overhead
        concepts:
        - Consistent hashing
        - Load balancing
        - Data partitioning
        skills:
        - Hash ring implementation
        - Virtual node management
        - Distributed system design
        - Key routing algorithms
        deliverables:
        - Hash ring implementation that maps keys to nodes using consistent hashing on a circular key space
        - Virtual node support that places multiple hash ring tokens per physical node for balanced load distribution
        - Node addition and removal with minimal key rehashing, only moving keys between adjacent ring positions
        - Key-to-node mapping function that locates the responsible node by walking clockwise from the key's hash position
        estimated_hours: 10-15
      - id: build-distributed-kv-m2
        name: Replication
        description: Implement replication with configurable consistency levels.
        acceptance_criteria:
        - Each key is replicated to N nodes, where N is the configurable replication factor per keyspace
        - Quorum operations enforce R + W > N so that reads and writes overlap on at least one up-to-date replica
        - Sloppy quorum and hinted handoff allow writes to proceed when a preferred replica is temporarily unavailable
        - Conflict resolution using vector clocks or last-write-wins timestamps handles concurrent writes to the same key
        pitfalls:
        - Quorum calculation
        - Read repair races
        - Hint delivery ordering
        concepts:
        - Quorum consensus
        - Eventual consistency
        - Hinted handoff
        skills:
        - Quorum-based consensus
        - Vector clock implementation
        - Conflict resolution strategies
        - Read repair mechanisms
        - Asynchronous replication
        deliverables:
        - Configurable replication factor N that stores each key-value pair on N consecutive ring nodes
        - Quorum read and write operations where reads require R replies and writes require W acks (R + W > N)
        - Replica synchronization protocol that propagates updates to all replicas eventually
        - Anti-entropy repair using Merkle tree comparison to detect and fix divergent replicas
        estimated_hours: 15-25
      - id: build-distributed-kv-m3
        name: Cluster Management
        description: Implement cluster membership and failure detection.
        acceptance_criteria:
        - Gossip protocol disseminates membership changes to all nodes within a bounded number of communication rounds
        - Failure detection identifies unresponsive nodes within a configurable timeout and marks them as suspect or down
        - Automatic rebalancing redistributes key ownership when nodes join or leave the cluster
        - Cluster state converges to a consistent view across all nodes within a bounded time after any membership change
        pitfalls:
        - Gossip message size
        - Split brain
        - Cascade failures
        concepts:
        - Gossip protocols
        - Failure detection
        - Cluster membership
        skills:
        - Gossip protocol implementation
        - Heartbeat monitoring
        - Membership state management
        - Network partition handling
        deliverables:
        - Node discovery and membership service that maintains the list of active cluster members
        - Gossip protocol that propagates cluster state changes (joins, leaves, failures) across all nodes
        - Failure detection using periodic heartbeats and phi-accrual or timeout-based suspicion mechanisms
        - Hinted handoff for failed nodes that temporarily stores writes and replays them when the node recovers
        estimated_hours: 18-30
      - id: build-distributed-kv-m4
        name: Transactions
        description: Implement distributed transactions across shards.
        acceptance_criteria:
        - Single-key operations provide linearizability so reads always return the latest committed write
        - Multi-key transactions using two-phase commit atomically commit or abort across all involved keys and nodes
        - Optimistic concurrency control detects conflicting writes at commit time and aborts the losing transaction
        - Deadlock detection or prevention mechanism avoids indefinite blocking when two transactions wait on each other
        pitfalls:
        - Coordinator failure during 2PC
        - Lock timeout tuning
        - Phantom reads
        concepts:
        - Distributed transactions
        - 2PC
        - Serializability
        skills:
        - Two-phase commit protocol
        - Distributed locking
        - Transaction coordinator design
        - Deadlock detection and resolution
        - Isolation level implementation
        deliverables:
        - Single-key transactions using compare-and-swap (CAS) for atomic read-modify-write operations
        - Multi-key transactions using two-phase commit or Percolator-style protocol for cross-key atomicity
        - Multi-version concurrency control (MVCC) that maintains versioned values for snapshot isolation reads
        - Conflict resolution strategy supporting last-write-wins timestamps or CRDT-based automatic merge
        estimated_hours: 27-50
    - id: infrastructure-as-code
      name: Infrastructure as Code Engine
      description: Terraform-like resource provisioning
      difficulty: advanced
      estimated_hours: '50'
      essence: Declarative configuration parsing into an AST, state reconciliation through three-way merge diffing (desired vs. current vs. stored state), and topological ordering of resource dependencies to orchestrate parallel execution of infrastructure mutations while maintaining referential integrity across provider APIs.
      why_important: Understanding IaC internals (like Terraform) helps debug state issues, write better modules, and build custom providers.
      learning_outcomes:
      - Design declarative configuration language
      - Implement state management and locking
      - Build dependency graph for resource ordering
      - Handle provider abstraction for multi-cloud
      skills:
      - HCL/DSL Parsing
      - State File Management
      - Directed Acyclic Graphs
      - Topological Sorting
      - Resource Lifecycle Management
      - Distributed Locking
      - Provider Plugin Architecture
      - Diff Algorithm Implementation
      tags:
      - advanced
      - automation
      - declarative
      - devops
      - game-dev
      - infrastructure
      - providers
      - state
      - terraform
      architecture_doc: architecture-docs/infrastructure-as-code/index.md
      languages:
        recommended:
        - Python
        - Go
        - HCL
        also_possible: []
      resources:
      - name: Terraform Language Documentation
        url: https://developer.hashicorp.com/terraform/language
        type: documentation
      - name: HCL Syntax Specification
        url: https://github.com/hashicorp/hcl/blob/main/hclsyntax/spec.md
        type: documentation
      - name: Terraform State Management
        url: https://developer.hashicorp.com/terraform/language/state
        type: documentation
      - name: Custom Terraform Provider Tutorial
        url: https://developer.hashicorp.com/terraform/tutorials/providers-plugin-framework/providers-plugin-framework-provider
        type: tutorial
      - name: Dependency Graph Resolution Algorithm
        url: https://www.electricmonk.nl/docs/dependency_resolving_algorithm/dependency_resolving_algorithm.html
        type: article
      prerequisites:
      - type: project
        id: ci-cd-pipeline
      milestones:
      - id: infrastructure-as-code-m1
        name: Configuration Parser
        description: Parse HCL-like configuration files with resources, variables, outputs, and module references.
        acceptance_criteria:
        - Parse HCL or YAML configuration files into structured resource definitions
        - Support variable interpolation resolving ${var.name} references in values
        - Handle includes and modules importing external configuration definitions
        - Validate configuration syntax and report meaningful errors for invalid input
        pitfalls:
        - Circular references cause infinite resolution loop
        - Interpolation in count creates chicken-egg problem
        - Module source paths need normalization
        concepts:
        - Recursive descent parsing for HCL syntax
        - Variable interpolation and expression evaluation
        - Abstract syntax tree construction and validation
        - Module resolution with version constraints
        - Schema validation for resource blocks
        skills:
        - Lexical analysis and tokenization
        - Recursive data structure traversal
        - Configuration validation patterns
        - Path normalization and resolution
        deliverables:
        - HCL or YAML parsing converting configuration files into structured data
        - Resource block extraction identifying resource type, name, and attributes
        - Variable interpolation resolving ${var.name} references in attribute values
        - Module support for importing and composing reusable configuration blocks
        estimated_hours: '12.5'
      - id: infrastructure-as-code-m2
        name: State Management
        description: Implement state file tracking of deployed resources with locking for concurrent access.
        acceptance_criteria:
        - Store current infrastructure state including resource IDs and attribute values
        - Lock state during apply operations to prevent concurrent modification conflicts
        - Support state backends including local filesystem and remote cloud storage
        - Handle state corruption recovery with backup and restore mechanisms
        pitfalls:
        - State corruption on partial write (use atomic rename)
        - Stale lock from crashed process blocks everyone
        - Remote state race condition between read and write
        concepts:
        - File-based distributed locking mechanisms
        - Atomic file operations with rename semantics
        - State serialization with version compatibility
        - Lease-based lock expiration and renewal
        - Optimistic concurrency control with checksums
        skills:
        - Concurrent access synchronization
        - Crash recovery and cleanup
        - Atomic file system operations
        - State migration and versioning
        deliverables:
        - State file format storing resource IDs, attributes, and dependency metadata
        - State locking preventing concurrent modifications during apply operations
        - State diff computation comparing current state against desired configuration
        - Remote state backend supporting S3 or similar cloud storage for team access
        estimated_hours: '12.5'
      - id: infrastructure-as-code-m3
        name: Dependency Graph & Planning
        description: Build resource dependency graph and generate execution plan with create, update, delete operations.
        acceptance_criteria:
        - Build resource dependency graph from explicit depends_on and implicit attribute references
        - Generate execution plan with create, update, and delete actions for each resource
        - Show plan diff before apply so operator can review proposed infrastructure changes
        - Support targeted apply executing changes only for specified resource subset
        pitfalls:
        - Cycle in dependencies causes topological sort failure
        - Implicit dependency detection misses some patterns
        - Replace action must delete before create if unique constraint
        concepts:
        - Directed acyclic graph construction and validation
        - Topological sort for execution ordering
        - Diff algorithms for resource state comparison
        - Dependency inference from resource references
        - Graph cycle detection algorithms
        skills:
        - Graph algorithm implementation
        - Parallel execution planning
        - Change set calculation
        - Resource lifecycle management
        deliverables:
        - Resource dependency extraction from explicit and implicit references
        - DAG construction building directed acyclic graph from resource dependencies
        - Plan generation computing create, update, and delete actions from state diff
        - Plan preview displaying proposed changes before applying them to infrastructure
        estimated_hours: '12.5'
      - id: infrastructure-as-code-m4
        name: Provider Abstraction
        description: Build provider interface for abstracting cloud APIs with resource CRUD operations.
        acceptance_criteria:
        - Define provider interface with standard create, read, update, and delete methods
        - Implement CRUD operations for at least one cloud resource type end-to-end
        - Handle provider authentication using configured credentials and access tokens
        - Support provider state refresh reading current resource attributes from cloud API
        pitfalls:
        - API rate limits require retry with backoff
        - Eventual consistency means read after create may fail
        - Resource stuck in pending state needs timeout handling
        concepts:
        - Interface-based abstraction for polymorphism
        - Exponential backoff with jitter for retries
        - Idempotency in distributed operations
        - Async polling with timeout mechanisms
        - Circuit breaker pattern for API failures
        skills:
        - REST API integration patterns
        - Retry logic with exponential backoff
        - Timeout and deadline handling
        - Error classification and recovery
        - Plugin architecture design
        deliverables:
        - Provider interface defining create, read, update, and delete operations
        - CRUD operations implementing resource lifecycle for each provider type
        - Provider configuration handling credentials and region settings
        - Provider plugins enabling extensible support for additional cloud platforms
        estimated_hours: '12.5'
    - id: serverless-runtime
      name: Serverless Function Runtime
      description: Function-as-a-Service with cold start optimization, scaling
      difficulty: expert
      estimated_hours: '80'
      essence: Function execution lifecycle orchestration through process isolation mechanisms (containers, microVMs, or WebAssembly sandboxes), implementing cold start mitigation via snapshot restoration and pre-warming pools, while dynamically managing instance capacity through event-driven auto-scaling based on request queue depth and resource utilization metrics.
      why_important: Building this teaches you production-grade isolation techniques (containers, microVMs), performance optimization under strict latency constraints, and distributed systems patterns for auto-scaling and load balancing‚Äîskills directly applicable to cloud infrastructure and platform engineering roles.
      learning_outcomes:
      - Understand serverless architecture
      - Implement function isolation and sandboxing
      - Build efficient cold start mechanisms
      - Design auto-scaling systems
      skills:
      - Container isolation
      - Cold start optimization
      - Auto-scaling
      - Request routing
      - Resource limits
      - Function lifecycle
      tags:
      - cold-start
      - devops
      - expert
      - functions
      - multi-tenancy
      - scaling
      - service
      architecture_doc: architecture-docs/serverless-runtime/index.md
      languages:
        recommended: *id002
        also_possible: []
      resources:
      - name: AWS Lambda Cold Starts
        url: https://aws.amazon.com/blogs/compute/understanding-and-remediating-cold-starts-an-aws-lambda-perspective/
        type: article
      - name: Firecracker MicroVM GitHub
        url: https://github.com/firecracker-microvm/firecracker
        type: documentation
      - name: OpenFaaS Documentation
        url: https://docs.openfaas.com/
        type: documentation
      - name: Blending Containers and VMs Research Paper
        url: https://pages.cs.wisc.edu/~swift/papers/vee20-isolation.pdf
        type: paper
      - name: Serverless Framework Tutorial
        url: https://www.serverless.com/framework/docs/tutorial
        type: tutorial
      prerequisites:
      - type: skill
        name: Containers
      - type: skill
        name: HTTP servers
      - type: skill
        name: Process management
      milestones:
      - id: serverless-runtime-m1
        name: Function Packaging
        description: Package and store function code
        acceptance_criteria:
        - Package function source code together with declared dependencies
        - Store packaged function artifacts in durable backend storage
        - Support multiple language runtimes including Python, Node, and Go
        - Handle function versioning with immutable version identifiers
        pitfalls:
        - Not handling transitive dependency conflicts correctly
        - Failing to validate function package integrity before storage
        - Including unnecessary files that bloat package size
        - Not supporting incremental updates for large dependencies
        concepts:
        - Dependency tree resolution and conflict handling
        - Artifact compression and deduplication strategies
        - Content-addressable storage for versioning
        - Language-specific runtime bundling (Go binaries, Java JARs, Python wheels)
        skills:
        - Code packaging
        - Dependency resolution
        - Storage
        deliverables:
        - Function definition format specifying handler, runtime, and configuration
        - Code upload API accepting function source archive
        - Dependency bundling packaging function code with required libraries
        - Runtime selection supporting Python, Node.js, and Go environments
        - Version management tracking deployed function code revisions
        - Code storage backend persisting function packages to S3 or local disk
        estimated_hours: '16'
      - id: serverless-runtime-m2
        name: Execution Environment
        description: Isolated function execution with resource limits
        acceptance_criteria:
        - Create isolated execution environment for each function invocation
        - Inject function handler code into prepared runtime container
        - Enforce memory and CPU resource limits on running functions
        - Terminate function execution after configurable timeout period
        pitfalls:
        - Setting resource limits too low causing premature termination
        - Not properly cleaning up containers after function execution
        - Allowing functions to escape sandbox through privilege escalation
        - Sharing mutable state between concurrent function invocations
        concepts:
        - Namespace isolation using cgroups and namespaces
        - Memory and CPU quota enforcement mechanisms
        - Filesystem overlay layers for read-only base images
        - Process sandboxing with seccomp and AppArmor
        skills:
        - Container runtime
        - Resource limits
        - Isolation
        deliverables:
        - Container-based isolation running each function in separate environment
        - Memory limit enforcement capping function RAM consumption
        - CPU limit enforcement throttling function processor usage
        - Network isolation restricting function outbound connectivity
        - Filesystem isolation providing clean writable /tmp per invocation
        - Execution timeout killing function after configurable deadline
        estimated_hours: '16'
      - id: serverless-runtime-m3
        name: Cold Start Optimization
        description: Minimize function startup latency
        acceptance_criteria:
        - Pre-warm execution environments to reduce cold start latency
        - Implement container snapshot and restore for sub-100ms cold starts
        - Cache warm container instances for reuse by subsequent invocations
        - Track cold start frequency and duration in metrics dashboard
        pitfalls:
        - Over-allocating warm pool instances wasting resources
        - Not considering memory footprint when keeping instances warm
        - Failing to refresh stale warm instances periodically
        - Ignoring language-specific initialization costs like JVM warmup
        concepts:
        - Container checkpoint/restore with CRIU for instant startup
        - Just-in-time compilation caching for interpreted languages
        - Keep-alive connection pools to downstream services
        - Pre-initialized runtime environments with lazy loading
        - Predictive pre-warming based on invocation patterns
        skills:
        - Warm pools
        - Snapshotting
        - Pre-warming
        deliverables:
        - Warm container pool maintaining pre-initialized environments ready for use
        - Container reuse assigning returning requests to previously used environments
        - Pre-initialization loading runtime and dependencies before first request
        - Snapshot and restore using CRIU for instant environment restoration
        - Predictive warming pre-creating containers based on traffic patterns
        - Shared layer caching reusing common runtime and dependency layers
        estimated_hours: '16'
      - id: serverless-runtime-m4
        name: Request Routing
        description: Route requests to function instances
        acceptance_criteria:
        - Route incoming HTTP requests to correct function instance
        - Handle multiple concurrent requests with per-function limits
        - Implement request queuing when all instances are busy
        - Support both synchronous and asynchronous invocation modes
        pitfalls:
        - Not implementing proper request timeout handling
        - Routing to cold instances when warm ones available
        - Allowing unbounded request queues causing memory exhaustion
        - Failing to drain in-flight requests during scale-down
        concepts:
        - Consistent hashing for sticky routing to instances
        - Queue backpressure mechanisms to prevent overload
        - Per-instance concurrency limits and semaphores
        - Circuit breaker patterns for unhealthy instances
        skills:
        - Load balancing
        - Request queuing
        - Concurrency
        deliverables:
        - HTTP gateway accepting inbound function invocation requests
        - Per-function request queue buffering incoming invocations
        - Concurrency limit enforcement capping parallel executions per function
        - Load balancing distributing requests across available instances
        - Request timeout returning error if function does not respond in time
        - Asynchronous invocation mode queuing request for background execution
        estimated_hours: '16'
      - id: serverless-runtime-m5
        name: Auto-Scaling
        description: Scale function instances based on demand
        acceptance_criteria:
        - Scale function instances up based on incoming request rate
        - Implement scale-to-zero removing all instances after idle timeout
        - Enforce minimum and maximum scaling limits per function
        - Track scaling events and instance utilization in metrics
        pitfalls:
        - Scaling too aggressively causing rapid instance churn
        - Not accounting for cold start latency in scaling decisions
        - Setting scale-to-zero timeout too short for intermittent workloads
        - Ignoring cost implications of keeping minimum instance counts
        concepts:
        - Reactive vs predictive scaling strategies
        - Rate of change metrics to prevent oscillation
        - Scale-down delay windows to handle bursty traffic
        - Target concurrency and utilization thresholds
        skills:
        - Metrics collection
        - Scaling algorithms
        - Scale to zero
        deliverables:
        - Request rate metrics tracking invocations per second per function
        - Concurrent execution tracking counting active running instances
        - Scale-up trigger adding instances when utilization exceeds threshold
        - Scale-down with cooldown removing idle instances after quiet period
        - Scale to zero terminating all instances after prolonged inactivity
        - Provisioned concurrency keeping minimum instances always warm
        estimated_hours: '16'
    - id: capstone-microservices-platform
      name: 'Capstone: Production Microservices Platform'
      description: Build a complete microservices platform with API gateway, service discovery, circuit breaking, distributed tracing, rate limiting, and CI/CD ‚Äî all integrated into a working e-commerce system with 4+ services.
      difficulty: expert
      estimated_hours: 100-150
      essence: Orchestrating independent services through dynamic discovery, API gateway routing, distributed tracing correlation, circuit breaking, saga-based transaction compensation, and automated deployment pipelines while maintaining system-wide observability and resilience.
      why_important: 'This is how production systems are actually built at scale. Integrating all the patterns you learned individually into a working platform teaches you the hardest part of distributed systems: making everything work together reliably.'
      learning_outcomes:
      - Design and decompose a monolith into well-bounded microservices
      - Integrate API gateway with service discovery for dynamic routing
      - Implement end-to-end distributed tracing across all services
      - Apply circuit breaker and rate limiting patterns at the gateway layer
      - Build a CI/CD pipeline that deploys services independently with blue-green deployment
      - Handle distributed data with saga pattern for cross-service transactions
      - Monitor system health with centralized logging, metrics, and alerting
      skills:
      - Microservices Architecture
      - API Gateway
      - Service Mesh
      - Distributed Tracing
      - CI/CD
      - Observability
      - Saga Pattern
      tags:
      - microservices
      - distributed-systems
      - capstone
      - production-ready
      architecture_doc: architecture-docs/capstone-microservices-platform/index.md
      languages:
        recommended:
        - Go
        - Java
        - Python
        also_possible:
        - Rust
        - JavaScript
      resources:
      - name: Microservices Patterns
        url: https://microservices.io/patterns/
        type: documentation
      - name: Building Microservices (Sam Newman)
        url: https://samnewman.io/books/building_microservices_2nd_edition/
        type: book
      - name: The Twelve-Factor App
        url: https://12factor.net/
        type: article
      prerequisites:
      - type: project
        id: api-gateway
        name: API Gateway
      - type: project
        id: service-discovery
        name: Service Discovery
      - type: project
        id: circuit-breaker
        name: Circuit Breaker Pattern
      - type: project
        id: rate-limiter
        name: Rate Limiter
      - type: project
        id: distributed-tracing
        name: Distributed Tracing System
      - type: project
        id: ci-cd-pipeline
        name: CI/CD Pipeline Builder
      milestones:
      - id: capstone-microservices-platform-m1
        name: Service Decomposition & Discovery
        description: Decompose an e-commerce domain into 4 services (Users, Products, Orders, Payments). Each service registers with the service discovery registry and communicates via gRPC.
        acceptance_criteria:
        - Four independent services with separate databases (database-per-service)
        - All services register with service discovery on startup and deregister on shutdown
        - Inter-service communication via gRPC with automatic service lookup
        - Health check endpoints with liveness and readiness probes
        pitfalls:
        - Distributed monolith (services too tightly coupled)
        - Service discovery stale entries
        - Shared database anti-pattern
        concepts:
        - service decomposition
        - bounded contexts
        - service discovery
        - gRPC
        - health checks
        skills:
        - Service decomposition
        - gRPC
        - Service discovery
        deliverables:
        - 4 microservices with service discovery and gRPC communication
        - Domain-driven service boundary definitions with documented bounded contexts and data ownership
        - Service registry implementation with heartbeat-based health checking and automatic deregistration
        - gRPC service definitions (.proto files) with versioned APIs and backward compatibility guarantees
        estimated_hours: '20'
      - id: capstone-microservices-platform-m2
        name: API Gateway & Resilience
        description: Build the API gateway that routes requests to services via discovery, applies rate limiting per client, and wraps calls in circuit breakers.
        acceptance_criteria:
        - Gateway routes HTTP requests to appropriate services using discovery lookup
        - 'Per-client rate limiting with configurable tiers (free: 100 req/min, pro: 1000 req/min)'
        - Circuit breaker per downstream service with configurable thresholds
        - Request/response transformation (REST ‚Üí gRPC) at the gateway
        pitfalls:
        - Gateway becoming a bottleneck
        - Circuit breaker masking real issues
        - Rate limit bypass via distributed clients
        concepts:
        - API gateway pattern
        - rate limiting tiers
        - circuit breaker states
        - protocol translation
        skills:
        - API Gateway
        - Rate limiting
        - Circuit breaker
        deliverables:
        - API gateway with rate limiting, circuit breaker, and protocol translation
        - Request routing layer mapping HTTP endpoints to gRPC service calls via discovery lookup
        - Per-client rate limiting enforcing tiered quotas based on API key or authentication token
        - Circuit breaker implementation tracking failure rates and opening circuits after threshold breaches
        estimated_hours: '20'
      - id: capstone-microservices-platform-m3
        name: Distributed Transactions & Saga
        description: 'Implement the order flow as a saga: create order ‚Üí reserve inventory ‚Üí process payment ‚Üí confirm order, with compensating actions on failure.'
        acceptance_criteria:
        - Order creation saga spans 3 services with compensation on any step failure
        - Payment failure triggers inventory release and order cancellation
        - Saga state persisted for crash recovery
        - Idempotent operations prevent duplicate charges on retry
        pitfalls:
        - Compensation failure leaving inconsistent state
        - Race conditions between saga steps
        - Long-running sagas blocking resources
        concepts:
        - saga pattern
        - compensating transactions
        - idempotency
        - eventual consistency
        skills:
        - Saga pattern
        - Distributed transactions
        - Idempotency
        deliverables:
        - Saga-based order flow with compensation and idempotency
        - Saga orchestrator coordinating order creation, inventory reservation, payment, and order confirmation steps
        - Compensation handlers rolling back inventory and payment on failure with retry logic
        - Idempotency layer using request IDs to prevent duplicate operations during retries
        estimated_hours: '20'
      - id: capstone-microservices-platform-m4
        name: Observability Stack
        description: Integrate distributed tracing (W3C Trace Context), centralized logging, and metrics collection across all services. Build a dashboard showing service health and request flows.
        acceptance_criteria:
        - Every request gets a trace ID propagated across all service calls
        - Centralized log aggregation with trace ID correlation
        - Metrics (latency p95, error rate, throughput) collected per service
        - Dashboard showing service dependency map, request flow traces, and health status
        pitfalls:
        - Trace context lost in async operations
        - Log volume explosion
        - Dashboard too slow for real-time monitoring
        concepts:
        - W3C Trace Context
        - log correlation
        - RED metrics
        - service dependency map
        skills:
        - Distributed tracing
        - Log aggregation
        - Metrics
        - Dashboarding
        deliverables:
        - Full observability stack with tracing, logging, metrics, and dashboard
        - W3C Trace Context propagation across all service boundaries with parent-child span relationships
        - Centralized logging with correlation IDs linking logs to distributed traces
        - Service dependency map visualization showing call graphs and latency percentiles between services
        estimated_hours: '18'
      - id: capstone-microservices-platform-m5
        name: CI/CD & Deployment
        description: Build a CI/CD pipeline that independently builds, tests, and deploys each service with blue-green deployment, canary releases, and automatic rollback.
        acceptance_criteria:
        - Each service has independent CI pipeline (build ‚Üí test ‚Üí deploy)
        - Blue-green deployment with zero-downtime switchover
        - Canary releases route 5% traffic to new version before full rollout
        - Automatic rollback triggered by error rate spike in canary
        pitfalls:
        - Database migration coordination across services
        - Canary with incompatible API versions
        - Rollback not reversing database changes
        concepts:
        - independent deployability
        - blue-green deployment
        - canary releases
        - automatic rollback
        skills:
        - CI/CD
        - Blue-green deployment
        - Canary releases
        deliverables:
        - CI/CD pipeline with blue-green and canary deployment
        - Independent build and test pipelines per service with container image publishing
        - Blue-green deployment script atomically switching traffic between environments with health validation
        - Canary release controller gradually shifting traffic percentages while monitoring error rates and latency
        estimated_hours: '15'
- id: ai-ml
  name: AI & Machine Learning
  icon: üß†
  subdomains:
  - name: Classical ML
  - name: Deep Learning
  - name: NLP
  projects:
    beginner:
    - id: linear-regression
      name: Linear Regression
      description: Gradient descent
      difficulty: beginner
      estimated_hours: 8-12
      essence: Convex optimization of a parametric model using calculus-based iterative weight updates to minimize mean squared error between predicted and actual values across training data.
      why_important: Linear regression is the foundation of machine learning optimization ‚Äî mastering gradient descent here builds intuition for training neural networks and all gradient-based algorithms.
      learning_outcomes:
      - Implement cost function computation using mean squared error
      - Derive partial derivatives for gradient descent parameter updates
      - Build batch gradient descent optimization from scratch
      - Implement vectorized operations with NumPy for efficient computation
      - Extend single-variable regression to multiple features using matrix operations
      - Debug convergence issues by tuning learning rates
      - Validate model performance using train-test splits and metrics
      skills:
      - Gradient Descent Optimization
      - Loss Function Design
      - NumPy Vectorization
      - Mathematical Derivation
      - Model Evaluation
      - Feature Normalization
      tags:
      - ai-ml
      - beginner-friendly
      - features
      - gradient-descent
      - loss-function
      - python
      architecture_doc: architecture-docs/linear-regression/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - JavaScript
        - Julia
      resources:
      - name: Linear Regression Tutorial
        url: https://scikit-learn.org/stable/modules/linear_model.html
        type: documentation
      - name: Gradient Descent Explained
        url: https://www.youtube.com/watch?v=sDv4f4s2SB8
        type: video
      prerequisites:
      - type: skill
        name: Basic Python/NumPy
      - type: skill
        name: High school math (derivatives)
      milestones:
      - id: linear-regression-m1
        name: Simple Linear Regression
        description: Implement single-variable linear regression.
        acceptance_criteria:
        - Model fits the equation y = mx + b using the ordinary least squares closed-form formula
        - Closed-form solution computes slope and intercept directly from the normal equation
        - Predictions for new input values are calculated correctly using the fitted slope and intercept
        - R-squared score is computed and falls between 0 and 1 for well-specified models
        pitfalls:
        - Division by zero with constant X
        - Data not being proper arrays
        - Using float instead of numpy
        concepts:
        - Linear relationship
        - Least squares
        - Model evaluation
        skills:
        - Implementing mathematical models in code
        - Computing statistical metrics
        - Basic data visualization
        - NumPy array operations
        deliverables:
        - Data loading module that reads feature and target columns from CSV or array input
        - Closed-form solution implementing the ordinary least squares formula for slope and intercept
        - Prediction function that computes y-hat given new x values using the fitted parameters
        - R-squared calculator that measures the proportion of variance explained by the model
        estimated_hours: 2-3
      - id: linear-regression-m2
        name: Gradient Descent
        description: Implement gradient descent optimization.
        acceptance_criteria:
        - Cost function computes mean squared error as the average of squared prediction residuals
        - Gradient calculation returns the correct partial derivatives for slope and intercept parameters
        - Iterative parameter updates reduce the cost function monotonically with an appropriate learning rate
        - Convergence is detected when the change in cost between consecutive iterations falls below epsilon
        pitfalls:
        - Learning rate too high (divergence)
        - Not normalizing features
        - Stopping too early
        concepts:
        - Gradient descent
        - Learning rate
        - Cost function
        skills:
        - Iterative optimization algorithms
        - Hyperparameter tuning
        - Convergence analysis
        - Cost function implementation
        deliverables:
        - Cost function implementing mean squared error over the entire training dataset
        - Gradient computation module that calculates partial derivatives of cost with respect to each parameter
        - Parameter update rule that subtracts the learning rate times gradient from current parameters
        - Convergence checker that stops training when cost improvement falls below a threshold
        estimated_hours: 2-3
      - id: linear-regression-m3
        name: Multiple Linear Regression
        description: Extend to multiple input features.
        acceptance_criteria:
        - Model handles multiple input features by fitting a weight vector instead of a single slope
        - Matrix form y = Xw is used to express predictions as a matrix-vector product
        - Vectorized gradient descent updates all weights in a single matrix operation per iteration
        - Feature normalization scales each feature to zero mean and unit variance before fitting
        pitfalls:
        - Features on different scales
        - Matrix dimension mismatches
        - Forgetting bias term
        concepts:
        - Matrix operations
        - Feature scaling
        - Vectorization
        skills:
        - Matrix multiplication and linear algebra
        - Feature engineering and preprocessing
        - Multi-dimensional data handling
        - Vectorized computations
        deliverables:
        - Feature matrix constructor that builds the design matrix X with an intercept column of ones
        - Multi-variable gradient descent that updates all weight parameters simultaneously each iteration
        - Feature normalization using z-score standardization to ensure all features have comparable scales
        - L2 regularization (Ridge) option that adds a penalty term to the cost function to prevent overfitting
        estimated_hours: 2-3
    - id: knn
      name: KNN Classifier
      description: Distance metrics
      difficulty: beginner
      estimated_hours: 6-10
      essence: Instance-based classification through distance metric computation and majority voting among k-nearest training samples, implementing lazy learning without explicit model training or parameter optimization.
      why_important: Building KNN from scratch teaches fundamental machine learning concepts like distance metrics, vector operations, and the bias-variance tradeoff without the complexity of gradient descent or neural networks, making it an ideal first ML algorithm to implement.
      learning_outcomes:
      - Implement multiple distance metrics (Euclidean, Manhattan, Minkowski) using NumPy vectorization
      - Build efficient neighbor search algorithms with computational complexity analysis
      - Design weighted voting schemes for classification and regression tasks
      - Optimize k-parameter selection through cross-validation and performance metrics
      - Handle high-dimensional data and understand the curse of dimensionality
      - Implement tie-breaking strategies and boundary case handling
      - Evaluate classifier performance using confusion matrices, precision, recall, and F1-score
      - Compare lazy learning approaches with parametric model training
      skills:
      - Distance Metrics
      - Vectorized Computing
      - Non-parametric Models
      - Cross-validation
      - NumPy Array Operations
      - Classification Evaluation
      - Hyperparameter Tuning
      - Lazy Learning
      tags:
      - ai-ml
      - algorithms
      - beginner-friendly
      - classification
      - distance-metrics
      - neighbors
      - python
      architecture_doc: architecture-docs/knn/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - JavaScript
        - Julia
      resources:
      - name: KNN Explained
        url: https://scikit-learn.org/stable/modules/neighbors.html
        type: documentation
      - name: KNN from Scratch
        url: https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/
        type: tutorial
      prerequisites:
      - type: skill
        name: Basic Python/NumPy
      - type: skill
        name: Distance formulas
      milestones:
      - id: knn-m1
        name: Distance Calculation
        description: Implement distance metrics for KNN.
        acceptance_criteria:
        - Euclidean distance correctly returns the square root of the sum of squared differences
        - Manhattan distance correctly returns the sum of absolute differences across all dimensions
        - Distance to all training points is computed and returned as a sorted array of distances
        - Distance computation runs efficiently using vectorized operations to avoid Python-level loops
        pitfalls:
        - Negative under square root
        - Feature scaling differences
        - Integer vs float types
        concepts:
        - Distance metrics
        - Vector operations
        - NumPy arrays
        skills:
        - NumPy array manipulation
        - Mathematical distance calculations
        - Feature normalization techniques
        deliverables:
        - Euclidean distance function that computes the L2 norm between two feature vectors
        - Manhattan distance function that computes the L1 norm (sum of absolute differences) between vectors
        - Cosine similarity function that measures the angular similarity between two feature vectors
        - Distance matrix builder that precomputes pairwise distances between all training samples
        estimated_hours: 1-2
      - id: knn-m2
        name: K-Nearest Neighbors Classification
        description: Implement the full KNN classifier.
        acceptance_criteria:
        - Find-k-nearest-neighbors returns exactly K neighbors sorted by ascending distance
        - Majority voting assigns the class that appears most frequently among the K neighbors
        - Prediction on new unseen data points returns the correct class label from the trained model
        - Classification accuracy is calculated as the ratio of correct predictions to total predictions
        pitfalls:
        - K larger than dataset
        - Ties in voting
        - Empty neighbors
        concepts:
        - Classification
        - Majority voting
        - Accuracy metric
        skills:
        - Algorithm implementation
        - Data structure operations
        - Classification metrics evaluation
        deliverables:
        - K selection parameter that allows configuring the number of neighbors to consider
        - Neighbor finder that returns the K training samples closest to the query point
        - Majority voting classifier that assigns the most common class among K neighbors
        - Weighted voting classifier that weighs each neighbor's vote by the inverse of its distance
        estimated_hours: 2-3
      - id: knn-m3
        name: Improvements & Evaluation
        description: Add improvements and proper evaluation.
        acceptance_criteria:
        - Weighted voting by inverse distance gives closer neighbors proportionally more influence
        - K-fold cross-validation partitions data into K folds and averages metrics across all folds
        - Confusion matrix correctly shows true positives, false positives, true negatives, and false negatives per class
        - Optimal K is found by evaluating multiple values and selecting the one with highest cross-validated accuracy
        pitfalls:
        - Overfitting with small k
        - Underfitting with large k
        - Computational cost
        concepts:
        - Weighted voting
        - Cross-validation
        - Hyperparameter tuning
        skills:
        - Model optimization
        - Cross-validation implementation
        - Performance tuning strategies
        - Hyperparameter search
        deliverables:
        - Cross-validation module that evaluates model performance using K-fold data splitting
        - K optimization routine that tests multiple K values and selects the best by validation accuracy
        - KD-tree spatial index that accelerates neighbor lookup from O(n) to O(log n) per query
        - Accuracy metrics module that computes precision, recall, F1-score, and confusion matrix
        estimated_hours: 2-3
    - id: chatbot-intent
      name: Intent-Based Chatbot
      description: Build a chatbot with intent classification and entity extraction. Foundation for conversational AI without LLMs.
      difficulty: beginner
      estimated_hours: 15-25
      essence: Text classification using TF-IDF vectorization and supervised learning to map natural language inputs to predefined intent categories, combined with rule-based pattern matching and machine learning models to extract structured entity information (dates, names, locations) from unstructured text, while maintaining conversation state through finite state machines that track context across dialog turns.
      why_important: Building this teaches foundational NLP techniques that underpin all conversational AI systems, from customer service bots to voice assistants, while providing hands-on experience with supervised learning and state management that transfers to production chatbot development.
      learning_outcomes:
      - Implement intent classification using TF-IDF vectorization and supervised learning algorithms
      - Build pattern-based and machine learning entity extractors for dates, names, and custom entity types
      - Design finite state machines to track dialog context and manage multi-turn conversation flows
      - Create template-based response generation with variable slot-filling from extracted entities
      - Train and evaluate text classifiers using scikit-learn with precision, recall, and F1 metrics
      - Handle ambiguous user inputs and implement fallback strategies for out-of-scope requests
      - Build a complete dialog pipeline from raw text input to contextualized natural language responses
      skills:
      - Text Classification
      - Named Entity Recognition
      - State Machine Design
      - TF-IDF Vectorization
      - Dialog State Tracking
      - Pattern Matching
      - Conversational AI
      tags:
      - beginner-friendly
      - classification
      - conversational
      - entities
      - intents
      - nlp
      - python
      architecture_doc: architecture-docs/chatbot-intent/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - JavaScript
        - Go
      resources:
      - name: Rasa Open Source
        url: https://rasa.com/docs/rasa/
        type: documentation
      - name: Intent Classification Tutorial
        url: https://www.tensorflow.org/tutorials/text/text_classification_rnn
        type: tutorial
      prerequisites:
      - type: skill
        name: Python basics
      - type: skill
        name: Text processing
      - type: skill
        name: Basic ML concepts
      milestones:
      - id: chatbot-intent-m1
        name: Intent Classification
        description: Build intent classifier from training examples.
        acceptance_criteria:
        - Training data contains at least 10 examples per intent for adequate coverage
        - Classifier vectorizes text using TF-IDF and trains supervised classification model
        - Classifier predicts intent with probability score for each known intent category
        - Low-confidence predictions below threshold are reported as unknown intent
        - Classifier achieves at least 80% accuracy on held-out test set of labeled examples
        pitfalls:
        - Too few training examples per intent
        - Imbalanced classes skew predictions
        - Not handling out-of-domain inputs
        - Overfitting on small datasets
        concepts:
        - Text classification
        - TF-IDF
        - Naive Bayes
        - Confidence thresholds
        skills:
        - Text preprocessing and tokenization
        - Machine learning classification
        - Handling imbalanced datasets
        - Model evaluation and confidence scoring
        deliverables:
        - Training data preparation organizing labeled examples by intent category
        - Intent classifier model using TF-IDF vectorization and Naive Bayes or SVM
        - Multi-intent handling detecting when input matches multiple intent categories
        - Confidence threshold filtering rejecting low-confidence classifications as unknown
        estimated_hours: 4-6
      - id: chatbot-intent-m2
        name: Entity Extraction
        description: Extract entities (names, dates, numbers) from user input.
        acceptance_criteria:
        - NER extracts standard entities like person names, dates, and locations from text
        - Rule-based patterns extract structured entities like times, durations, and numbers
        - Slot filler maps extracted entities to required parameters for detected intent
        - Missing required slots are identified so system can prompt user for more information
        - Multiple entities in single message are extracted and assigned to correct slots
        pitfalls:
        - Regex too greedy matches wrong text
        - spaCy model not loaded (need python -m spacy download)
        - Date parsing ambiguity (01/02 = Jan 2 or Feb 1?)
        - Overlapping entity matches
        concepts:
        - Named Entity Recognition
        - Regex patterns
        - Slot filling
        - spaCy
        skills:
        - Pattern matching with regular expressions
        - Natural language processing pipelines
        - Working with pre-trained NLP models
        - Entity disambiguation and validation
        deliverables:
        - Named entity recognition extracting people, places, dates, and custom entities
        - Slot filling mapping extracted entities to intent-specific parameter slots
        - Entity normalization converting extracted text to canonical data formats
        - Custom entity type definitions for domain-specific extraction patterns
        estimated_hours: 4-6
      - id: chatbot-intent-m3
        name: Dialog Management
        description: Manage multi-turn conversations with context.
        acceptance_criteria:
        - Dialog state persists across turns tracking current intent and filled slots
        - Follow-up messages add to existing context without requiring full intent restatement
        - Missing slot prompts ask user specific questions to collect required information
        - Conversation reset command clears context and returns to initial state
        - Context expires after configurable inactivity timeout to prevent stale sessions
        pitfalls:
        - Memory leak from never-expiring sessions
        - Losing context on intent change
        - Infinite loops in dialog flow
        - Not handling 'cancel' or 'start over'
        concepts:
        - State machines
        - Session management
        - Multi-turn dialog
        - Context tracking
        skills:
        - State Machine Design
        - Context Window Management
        - Multi-turn Conversation Handling
        - Session Persistence
        deliverables:
        - Dialog state tracking maintaining conversation context across multiple turns
        - Context management preserving and updating slot values throughout conversation
        - Multi-turn conversation support handling follow-up questions and clarifications
        - Slot confirmation prompting user to verify extracted values before executing action
        estimated_hours: 5-8
      - id: chatbot-intent-m4
        name: Response Generation
        description: Generate natural language responses with templates.
        acceptance_criteria:
        - Response templates include placeholders replaced with context-specific entity values
        - Variable substitution correctly inserts extracted entities into response text
        - Multiple response variations exist per intent-state combination for natural conversation
        - Fallback responses handle unrecognized input with helpful clarification prompts
        - Response tone remains consistent across all generated responses for the chatbot persona
        pitfalls:
        - Templates not escaping user input (XSS)
        - Missing template variables cause errors
        - Too few variations feel robotic
        - Inconsistent tone across responses
        concepts:
        - Template engines
        - NLG basics
        - Response variation
        - Personality design
        skills:
        - Template-based text generation
        - Variable interpolation and formatting
        - Randomization for natural responses
        - User input sanitization
        deliverables:
        - Template-based response system with variable substitution for dynamic content
        - Dynamic content insertion filling response templates with context-specific values
        - Fallback handling providing graceful responses for unrecognized or ambiguous input
        - Response variation randomly selecting from alternative phrasings to avoid repetition
        estimated_hours: 3-5
    intermediate:
    - id: neural-network-basic
      name: Neural Network (micrograd)
      description: Forward/backward pass
      difficulty: intermediate
      estimated_hours: 15-25
      essence: Scalar-valued automatic differentiation through dynamic computational graph construction, topological sorting for reverse-mode gradient propagation, and operator overloading to track dependencies between mathematical operations for implementing the chain rule mechanically.
      why_important: Building this demystifies how modern deep learning frameworks like PyTorch and TensorFlow work under the hood, revealing that the core differentiation engine is surprisingly simple (~100 lines) while teaching the mathematical foundations essential for debugging and optimizing neural networks.
      learning_outcomes:
      - Implement reverse-mode automatic differentiation using computational graphs
      - Build a scalar-based autograd engine with operator overloading
      - Design backpropagation through arbitrary computation graphs using topological sorting
      - Implement neural network primitives (neurons, layers, MLP) from first principles
      - Debug gradient flow and vanishing/exploding gradient problems
      - Apply chain rule recursively for multi-layer gradient computation
      - Construct a training loop with forward pass, backward pass, and parameter updates
      - Visualize computational graphs to trace derivative calculations
      skills:
      - Automatic Differentiation
      - Computational Graphs
      - Backpropagation
      - Gradient Descent
      - Neural Network Architecture
      - Chain Rule Implementation
      - Graph Algorithms
      - Operator Overloading
      tags:
      - activation
      - ai-ml
      - backprop
      - feedforward
      - intermediate
      - networking
      - python
      architecture_doc: architecture-docs/neural-network-basic/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - Julia
        - JavaScript
      resources:
      - name: Micrograd Repository
        url: https://github.com/karpathy/micrograd
        type: repository
      - name: Micrograd Video Tutorial
        url: https://www.youtube.com/watch?v=VMj-3S1tku0
        type: video
      prerequisites:
      - type: skill
        name: Calculus (derivatives)
      - type: skill
        name: Python basics
      - type: skill
        name: Linear algebra basics
      milestones:
      - id: neural-network-basic-m1
        name: Value Class with Autograd
        description: Create Value class that tracks computation graph for automatic differentiation.
        acceptance_criteria:
        - Value class wraps a scalar float number and exposes it for arithmetic operations
        - Each Value tracks its child operands and the operation that produced it in the computational graph
        - Each Value tracks the operation type (add, multiply, etc.) that created it for backward dispatch
        - Gradient field is initialized to zero and accumulates derivatives during the backward pass
        - Value supports addition, multiplication, subtraction, division, and power operator overloads
        pitfalls:
        - Forgetting += for gradients (accumulation)
        - Not handling scalar + Value
        - Backward called before forward
        concepts:
        - Computational graphs
        - Chain rule
        - Gradient accumulation
        skills:
        - Operator overloading in Python
        - Automatic differentiation implementation
        - Computational graph construction
        - Gradient tracking data structures
        deliverables:
        - Value wrapper class that holds a scalar number along with its gradient and computational graph references
        - Operation tracker that records which operation and operands produced each Value for backpropagation
        - Gradient storage field on each Value that accumulates the derivative of the loss with respect to that Value
        - Parent reference links that connect each Value to the Values used as inputs in the operation that created it
        estimated_hours: 4-6
      - id: neural-network-basic-m2
        name: Backward Pass
        description: Implement backpropagation through the computation graph.
        acceptance_criteria:
        - Topological sort produces a valid ordering where all inputs appear before the operations that use them
        - Backward pass iterates in reverse topological order computing gradients from output toward inputs
        - Gradients are zeroed before each backward pass to prevent accumulation from previous iterations
        - Values used as input to multiple operations correctly accumulate gradients from all downstream paths
        pitfalls:
        - Not zeroing gradients
        - Wrong topological order
        - Gradient through constants
        concepts:
        - Backpropagation
        - Topological sort
        - Reverse-mode AD
        skills:
        - Graph traversal algorithms
        - Recursive function implementation
        - Gradient computation for basic operations
        - Memory management in backprop
        deliverables:
        - Topological sort that orders all Values in the computational graph from inputs to output
        - Backward propagation engine that applies the chain rule in reverse topological order to compute gradients
        - Chain rule applicator that multiplies the upstream gradient by the local derivative at each operation node
        - Gradient accumulator that sums gradient contributions when a Value is used as input to multiple operations
        estimated_hours: 2-3
      - id: neural-network-basic-m3
        name: Neuron and Layer
        description: Build neural network components using Value.
        acceptance_criteria:
        - Each neuron has a weight vector and bias term that are initialized randomly and are trainable Value objects
        - Activation function applies tanh or ReLU nonlinearity to the weighted sum producing a bounded output
        - Layer computes outputs for all its neurons producing a vector of activation values for each input
        - MLP (Multi-Layer Perceptron) chains multiple layers and propagates data through all of them sequentially
        pitfalls:
        - Not initializing weights properly
        - Missing activation on last layer
        - Parameter collection incomplete
        concepts:
        - Neural network architecture
        - Activation functions
        - Parameter management
        skills:
        - Object-oriented design for ML components
        - Random weight initialization techniques
        - Forward pass implementation
        - Parameter collection and management
        deliverables:
        - Neuron class that computes a weighted sum of inputs plus bias and applies a nonlinear activation function
        - Layer class that groups multiple neurons and computes all their outputs for a given input vector
        - Forward pass method that propagates input data through the neuron computing activation output
        - Parameter collection method that returns all trainable weights and biases from the network hierarchy
        estimated_hours: 3-4
      - id: neural-network-basic-m4
        name: Training Loop
        description: Train the neural network with gradient descent.
        acceptance_criteria:
        - Forward pass computes predictions by propagating inputs through all layers of the network
        - MSE loss is computed as the mean of squared differences between predictions and target values
        - Backward pass computes gradients of the loss with respect to all trainable parameters
        - Gradient descent update subtracts learning rate times gradient from each parameter reducing the loss
        - Training over multiple epochs progressively reduces the loss demonstrating network learning
        pitfalls:
        - Forgetting to zero gradients
        - Learning rate too high
        - Not enough epochs
        concepts:
        - Training loop
        - Gradient descent
        - Loss functions
        skills:
        - Optimization loop implementation
        - Loss function design
        - Hyperparameter tuning basics
        - Training convergence monitoring
        deliverables:
        - Loss function that computes mean squared error between predicted and target output values
        - Gradient zeroing routine that resets all parameter gradients to zero before each backward pass
        - Parameter update step that adjusts each weight by subtracting learning rate times its gradient
        - Training iteration loop that repeats forward pass, loss, backward, and update for multiple epochs
        estimated_hours: 3-4
    - id: word2vec
      name: Word Embeddings
      description: Skip-gram, CBOW
      difficulty: intermediate
      estimated_hours: 15-25
      essence: Training a shallow neural network to predict context words from target words (skip-gram) or target words from context (CBOW), producing dense vector representations where geometric relationships encode semantic meaning, with negative sampling replacing expensive full-vocabulary softmax computation.
      why_important: Word embeddings are foundational to modern NLP systems, and implementing Word2Vec from scratch teaches you how neural networks learn distributed representations, efficient training techniques for large vocabularies, and the mathematical principles behind semantic vector spaces used in production systems.
      learning_outcomes:
      - Implement skip-gram or CBOW neural network architectures for word prediction
      - Design efficient training procedures using negative sampling to avoid computing full softmax
      - Build text preprocessing pipelines including tokenization, vocabulary construction, and context window generation
      - Optimize high-dimensional embedding spaces using gradient descent and backpropagation
      - Evaluate semantic quality of embeddings using analogy tasks and cosine similarity
      - Visualize high-dimensional word vectors using dimensionality reduction techniques like t-SNE
      - Debug convergence issues in neural embedding models and tune hyperparameters
      - Implement subsampling of frequent words and unigram distribution for negative sample selection
      skills:
      - Neural Network Training
      - Embedding Representations
      - Negative Sampling
      - Text Preprocessing
      - Vector Space Models
      - Gradient Descent Optimization
      - Dimensionality Reduction
      - Semantic Similarity
      tags:
      - cbow
      - embeddings
      - intermediate
      - nlp
      - python
      - skip-gram
      - word-vectors
      architecture_doc: architecture-docs/word2vec/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - Julia
        - C++
      resources:
      - name: Word2Vec Paper
        url: https://arxiv.org/abs/1301.3781
        type: paper
      - name: Word2Vec Tutorial
        url: https://www.tensorflow.org/tutorials/text/word2vec
        type: tutorial
      prerequisites:
      - type: skill
        name: Neural network basics
      - type: skill
        name: Linear algebra
      - type: skill
        name: Python/NumPy
      milestones:
      - id: word2vec-m1
        name: Data Preprocessing
        description: Prepare text corpus for training.
        acceptance_criteria:
        - Tokenize text into words with lowercasing and punctuation removal
        - Build vocabulary with word-to-index mapping and minimum frequency threshold
        - Create training pairs of context and target words from sliding window over corpus
        - Implement subsampling of frequent words reducing their occurrence probability in training
        pitfalls:
        - Vocabulary too large
        - Not removing rare words
        - Memory issues with large corpus
        concepts:
        - Tokenization
        - Vocabulary building
        - Skip-gram pairs
        skills:
        - Text preprocessing and tokenization
        - Vocabulary management and indexing
        - Context window extraction
        - Handling large text datasets
        deliverables:
        - Text tokenization splitting raw input into individual word tokens with normalization
        - Vocabulary building collecting unique words with frequency counts above a threshold
        - Word to index mapping creating bidirectional lookup between words and integer IDs
        - Subsampling frequent words randomly discarding high-frequency tokens during training
        estimated_hours: 3-4
      - id: word2vec-m2
        name: Skip-gram Model
        description: Implement the Skip-gram neural network.
        acceptance_criteria:
        - Embedding layer maps each word index to a dense vector of configurable dimension
        - Output layer predicts context words given the target word embedding
        - Softmax over vocabulary computes probability distribution for context word prediction
        - Forward pass implementation computes prediction scores for all context candidates
        pitfalls:
        - Softmax numerical stability
        - Wrong matrix dimensions
        - Embedding lookup indexing
        concepts:
        - Word embeddings
        - Softmax classification
        - Neural network layers
        skills:
        - Neural network architecture design
        - Matrix operations and tensor manipulation
        - Forward propagation implementation
        - Embedding layer construction
        deliverables:
        - Embedding layer mapping word indices to dense trainable vector representations
        - Context window parameter defining the radius of surrounding words for pair generation
        - Training pair generation producing target-context word pairs from the sliding window
        - Forward pass computing dot product between target and context embedding vectors
        estimated_hours: 3-4
      - id: word2vec-m3
        name: Training with Negative Sampling
        description: Implement efficient training with negative sampling.
        acceptance_criteria:
        - Cross-entropy loss computes correct gradients for positive and negative sample pairs
        - Negative sampling instead of full softmax
        - Gradient computation correctly updates both input and output embedding matrices
        - SGD parameter updates reduce loss over successive training iterations
        pitfalls:
        - Sampling distribution
        - Gradient computation errors
        - Learning rate too high
        concepts:
        - Negative sampling
        - Cross-entropy loss
        - Stochastic gradient descent
        skills:
        - Optimization algorithm implementation
        - Loss function design and debugging
        - Sampling strategies for efficiency
        - Gradient descent tuning
        deliverables:
        - Negative sample selection drawing non-context words weighted by frequency distribution
        - Loss function computing binary cross-entropy between positive and negative samples
        - Gradient computation deriving parameter updates from the negative sampling loss
        - Parameter update step applying stochastic gradient descent to embedding weights
        estimated_hours: 4-6
      - id: word2vec-m4
        name: Evaluation & Visualization
        description: Evaluate embeddings and visualize results.
        acceptance_criteria:
        - Find similar words by cosine similarity returning ranked nearest neighbors
        - 'Analogy task: king - man + woman = queen'
        - Visualize embedding clusters with t-SNE or PCA in a 2D scatter plot
        - Save and load embeddings to and from disk in a portable format
        pitfalls:
        - Normalizing vectors
        - Including query word in results
        - Memory for large vocab
        concepts:
        - Cosine similarity
        - Word analogies
        - Dimensionality reduction
        skills:
        - Vector similarity metrics
        - Dimensionality reduction techniques
        - Model evaluation and validation
        - Data visualization for high-dimensional spaces
        deliverables:
        - Word similarity function finding nearest neighbors by cosine similarity in embedding space
        - Analogy test evaluator computing vector arithmetic to solve word analogy tasks
        - t-SNE visualization projecting high-dimensional embeddings to 2D for visual inspection
        - Embedding export module saving trained vectors in standard text or binary format
        estimated_hours: 3-4
    - id: rag-system
      name: RAG System (Retrieval Augmented Generation)
      description: 'Build a production RAG pipeline: document ingestion, chunking, embedding, vector search, and LLM generation with retrieved context.'
      difficulty: intermediate
      estimated_hours: 30-50
      essence: 'Semantic similarity search through high-dimensional vector space: transforming unstructured text into dense embeddings, performing approximate nearest neighbor retrieval with sub-linear time complexity, and synthesizing grounded responses by injecting retrieved context into LLM prompts while managing the tradeoff between retrieval precision and context window constraints.'
      why_important: Building RAG systems teaches production AI pipeline engineering‚Äîcombining embedding models, vector databases, and LLMs‚Äîwhich is the foundation for most real-world AI applications in 2026, from chatbots to enterprise knowledge bases.
      learning_outcomes:
      - Implement document chunking strategies with overlap and semantic boundaries
      - Design and optimize vector embeddings using transformer models
      - Build efficient similarity search with approximate nearest neighbor algorithms
      - Integrate retrieval context into LLM prompts with proper formatting
      - Evaluate RAG quality using context precision, recall, and answer relevance metrics
      - Debug retrieval failures through query augmentation and reranking
      - Optimize chunk size and embedding dimensions for accuracy-performance tradeoffs
      - Implement hybrid search combining dense vectors and keyword matching
      skills:
      - Vector Embeddings
      - Semantic Search
      - Document Processing
      - LLM Prompting
      - Vector Databases
      - Information Retrieval
      - Pipeline Orchestration
      - RAG Evaluation
      tags:
      - ai-ml
      - chunks
      - context
      - embeddings
      - generation
      - intermediate
      - python
      - retrieval
      - search
      - streaming
      architecture_doc: architecture-docs/rag-system/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - TypeScript
        - Go
      resources:
      - name: LangChain RAG Tutorial
        url: https://python.langchain.com/docs/tutorials/rag/
        type: tutorial
      - name: OpenAI Embeddings
        url: https://platform.openai.com/docs/guides/embeddings
        type: documentation
      - name: Pinecone Vector DB
        url: https://docs.pinecone.io/
        type: documentation
      prerequisites:
      - type: skill
        name: Python
      - type: skill
        name: Basic ML concepts
      - type: skill
        name: REST APIs
      - type: skill
        name: Database basics
      milestones:
      - id: rag-system-m1
        name: Document Ingestion & Chunking
        description: Load documents and split into optimal chunks for retrieval.
        acceptance_criteria:
        - Load documents from PDF, Markdown, and HTML file formats
        - Extract text content while preserving document metadata
        - Implement multiple chunking strategies (fixed, semantic, recursive)
        - Handle chunk overlap to maintain context continuity across boundaries
        pitfalls:
        - Chunks too small lose context
        - Chunks too large exceed LLM context window
        - PDF extraction loses formatting/tables
        - Not handling encoding issues (UTF-8)
        concepts:
        - Document parsing
        - Text chunking
        - Tokenization
        - Metadata tracking
        skills:
        - File I/O and parsing (PDF, TXT, JSON)
        - Text preprocessing and cleaning
        - Working with chunking strategies (fixed, recursive, semantic)
        - String manipulation and encoding handling
        deliverables:
        - Document loader supporting PDF, Markdown, and plain text formats
        - Text chunking strategies including fixed-size and semantic splitting
        - Chunk overlap handling to preserve cross-boundary context
        - Metadata extraction tracking source document and position
        estimated_hours: 6-10
      - id: rag-system-m2
        name: Embedding Generation
        description: Convert text chunks to vector embeddings for semantic search.
        acceptance_criteria:
        - Generate vector embeddings using OpenAI or local sentence-transformer model
        - Process embeddings in batches for efficiency and rate limit compliance
        - Handle API rate limits with exponential backoff retry logic
        - Cache computed embeddings to disk to avoid redundant API calls
        pitfalls:
        - Rate limits cause failures without retry logic
        - Large batches exceed API limits
        - Not normalizing embeddings for cosine similarity
        - Embedding dimension mismatch between models
        concepts:
        - Text embeddings
        - API rate limiting
        - Caching strategies
        - Batch processing
        skills:
        - Using embedding APIs (OpenAI, HuggingFace)
        - Implementing retry logic and exponential backoff
        - Batch processing and parallelization
        - Caching mechanisms for API responses
        - Vector normalization and dimension handling
        deliverables:
        - Embedding model integration with OpenAI or local transformer
        - Batch embedding processing for efficient API usage
        - Embedding cache storing computed vectors to avoid recomputation
        - Model selection supporting multiple embedding providers
        estimated_hours: 4-6
      - id: rag-system-m3
        name: Vector Store & Retrieval
        description: Store embeddings and perform similarity search.
        acceptance_criteria:
        - Store embedding vectors alongside chunk text and metadata
        - Perform K-nearest neighbor search returning most similar chunks
        - Support hybrid search combining vector similarity and keyword BM25
        - Filter search results by metadata attributes like source or date
        pitfalls:
        - Using wrong distance metric (cosine vs L2)
        - Not normalizing scores for hybrid search
        - Memory issues with large collections
        - Stale BM25 index after updates
        concepts:
        - Vector databases
        - ANN search
        - BM25
        - Hybrid retrieval
        skills:
        - Vector database operations (FAISS, Pinecone, ChromaDB)
        - Implementing similarity search algorithms
        - Combining sparse and dense retrieval methods
        - Index management and optimization
        - Distance metric selection and configuration
        deliverables:
        - Vector database integration with Chroma, Pinecone, or pgvector
        - K-nearest neighbor similarity search over stored embeddings
        - Metadata-based filtering narrowing search to relevant subsets
        - Re-ranking retrieved results using cross-encoder model
        estimated_hours: 6-8
      - id: rag-system-m4
        name: LLM Integration & Prompting
        description: Generate answers using retrieved context with LLM.
        acceptance_criteria:
        - Build effective RAG prompt template with retrieved context
        - Handle context window limits by truncating excess chunks
        - Stream LLM responses token by token to the client
        - Handle LLM API errors and timeouts with graceful fallback
        pitfalls:
        - Exceeding context window with too many chunks
        - LLM hallucinating despite 'only use context' instruction
        - Not handling streaming errors mid-response
        - Prompt injection from retrieved content
        concepts:
        - Prompt engineering
        - Context window management
        - Streaming responses
        - LLM providers
        skills:
        - LLM API integration (OpenAI, Anthropic)
        - Prompt template design and engineering
        - Managing token budgets and context windows
        - Streaming response handling
        - Input sanitization and validation
        deliverables:
        - RAG prompt template constructing context-augmented queries
        - Prompt template with citation and grounding instructions
        - LLM API call wrapper supporting streaming responses
        - Response parsing extracting answer text and source citations
        estimated_hours: 6-8
      - id: rag-system-m5
        name: Evaluation & Optimization
        description: Measure and improve RAG quality.
        acceptance_criteria:
        - Implement retrieval metrics including recall at K and MRR
        - Implement generation quality metrics for faithfulness and relevance
        - Create labeled evaluation dataset with questions and expected answers
        - A/B test different configurations and measure quality differences
        pitfalls:
        - Evaluation set too small to be statistically significant
        - LLM judge bias toward verbose responses
        - Not versioning evaluation datasets
        - Overfitting to evaluation set
        concepts:
        - Retrieval metrics
        - Generation evaluation
        - LLM-as-judge
        - Cross-encoder reranking
        skills:
        - Computing precision, recall, and F1 metrics
        - Implementing automated evaluation pipelines
        - Using LLMs for quality assessment
        - Re-ranking retrieved results with cross-encoders
        - A/B testing and experiment tracking
        deliverables:
        - Retrieval metrics measuring recall at K and mean reciprocal rank
        - Answer quality metrics scoring faithfulness and relevance
        - Chunk size optimization comparing different chunking configurations
        - Prompt optimization testing different template variations
        estimated_hours: 8-12
    - id: semantic-search
      name: Semantic Search Engine
      description: Build a semantic search engine that understands meaning, not just keywords. Core technology behind modern search.
      difficulty: intermediate
      estimated_hours: 25-40
      essence: Transform text into dense vector embeddings using transformer models, implement approximate nearest neighbor search with spatial indexing structures (FAISS, HNSW), and fuse keyword-based ranking (BM25) with cosine similarity scoring to retrieve semantically relevant results beyond exact string matches.
      why_important: Semantic search powers modern information retrieval systems from Google to ChatGPT's RAG pipelines. Building this teaches the complete ML-powered search stack‚Äîfrom embedding models and vector databases to hybrid ranking algorithms‚Äîskills directly applicable to LLM applications, recommendation systems, and AI-augmented products.
      learning_outcomes:
      - Implement vector embeddings using transformer models like Sentence-BERT to capture semantic meaning
      - Build approximate nearest neighbor indexes with FAISS for sub-linear similarity search at scale
      - Design hybrid ranking systems combining BM25 keyword scoring with cosine similarity for optimal relevance
      - Optimize index structures and query performance through dimensionality reduction and quantization techniques
      - Implement metadata filtering alongside vector similarity for precision retrieval
      - Build production-ready search APIs with sub-100ms latency using efficient batching and caching
      - Debug embedding quality issues through visualization and similarity score analysis
      skills:
      - Vector Embeddings
      - Approximate Nearest Neighbor Search
      - Transformer Models (BERT/SBERT)
      - FAISS Indexing
      - Hybrid Search Ranking
      - High-Dimensional Vector Math
      - Information Retrieval Systems
      tags:
      - embeddings
      - game-dev
      - go
      - intermediate
      - python
      - search
      - sentence-transformers
      - similarity
      - vector-search
      architecture_doc: architecture-docs/semantic-search/index.md
      languages:
        recommended:
        - Python
        - Go
        also_possible:
        - TypeScript
        - Rust
      resources:
      - name: Sentence Transformers
        url: https://www.sbert.net/
        type: documentation
      - name: FAISS Library
        url: https://github.com/facebookresearch/faiss
        type: documentation
      prerequisites:
      - type: skill
        name: Python
      - type: skill
        name: Embeddings basics
      - type: skill
        name: Database fundamentals
      milestones:
      - id: semantic-search-m1
        name: Embedding Index
        description: Build efficient vector index for similarity search.
        acceptance_criteria:
        - Index millions of document vectors with efficient memory usage
        - Achieve sub-second query latency for nearest neighbor search
        - Support incremental index updates without full reconstruction
        - Implement HNSW or IVF approximate nearest neighbor index
        - Support memory-mapped access for datasets exceeding available RAM
        pitfalls:
        - Not normalizing vectors for cosine similarity
        - IVF requires training before adding
        - Memory explosion with large HNSW M parameter
        - ID mapping gets out of sync
        concepts:
        - HNSW algorithm
        - IVF indexing
        - Approximate nearest neighbors
        - Index persistence
        skills:
        - Vector database operations
        - Approximate nearest neighbor search
        - Index optimization and tuning
        - Binary serialization formats
        - Memory-mapped file handling
        deliverables:
        - Document embedding pipeline converting text to vector representations
        - Vector index construction using HNSW or IVF algorithm
        - Index persistence saving and loading trained index from disk
        - Incremental update support adding new vectors without full rebuild
        estimated_hours: 6-10
      - id: semantic-search-m2
        name: Query Processing
        description: Parse and enhance queries for better results.
        acceptance_criteria:
        - Expand queries with synonyms and related terms for broader recall
        - Understand query intent by extracting entities and semantic meaning
        - Support multi-vector queries combining multiple query aspects
        - Handle negative query terms subtracting unwanted concepts from results
        - Cache frequent query embeddings for faster repeated lookups
        pitfalls:
        - Over-expansion dilutes original intent
        - Synonym quality varies by domain
        - Negative subtraction can flip meaning
        - Cache invalidation on embedding model change
        concepts:
        - Query expansion
        - Semantic understanding
        - Vector arithmetic
        - Query caching
        skills:
        - Query parsing and tokenization
        - Semantic similarity computation
        - Vector operations and arithmetic
        - Cache implementation strategies
        - Query optimization techniques
        deliverables:
        - Query text embedding converting search query to vector
        - Nearest neighbor similarity search over indexed vectors
        - Top-K result retrieval returning most similar documents
        - Query expansion generating synonym and related term variants
        estimated_hours: 5-8
      - id: semantic-search-m3
        name: Ranking & Relevance
        description: Combine signals for optimal result ranking.
        acceptance_criteria:
        - Implement multi-stage ranking with fast retrieval then precise reranking
        - Combine semantic vector scores with lexical BM25 keyword scores
        - Apply personalization signals boosting results matching user preferences
        - Boost recent documents with freshness score decay function
        - Learn from click-through rate data to improve future ranking quality
        pitfalls:
        - Cross-encoder too slow for all results
        - Weight tuning is data-dependent
        - Popularity bias creates filter bubbles
        - Freshness can demote evergreen content
        concepts:
        - Multi-stage ranking
        - Learning to rank
        - Feature engineering
        - Personalization
        skills:
        - Multi-stage ranking pipelines
        - Feature extraction and engineering
        - Model serving and inference
        - Relevance scoring algorithms
        - A/B testing and evaluation
        deliverables:
        - Semantic similarity scoring measuring vector distance between query and documents
        - Hybrid search combining keyword BM25 and semantic vector scores
        - Cross-encoder re-ranking refining top candidates with pairwise model
        - Relevance tuning adjusting score weights for different signal types
        estimated_hours: 6-10
      - id: semantic-search-m4
        name: Search API & UI
        description: Build production search API with instant results.
        acceptance_criteria:
        - Serve RESTful search API with JSON request and response format
        - Provide typeahead autocomplete suggestions with sub-100ms latency
        - Support faceted search with filter counts per category
        - Highlight matching query terms in result text snippets
        - Collect and display search analytics including zero-result queries
        pitfalls:
        - Autocomplete latency > 100ms feels slow
        - Facet counts expensive to compute
        - Highlighting breaks on HTML entities
        - Not tracking null results for improvement
        concepts:
        - Search API design
        - Autocomplete
        - Faceted navigation
        - Result highlighting
        skills:
        - REST API design patterns
        - Real-time search optimization
        - Frontend autocomplete implementation
        - Response serialization and streaming
        - Search analytics and logging
        deliverables:
        - RESTful search endpoint accepting query and filter parameters
        - Result formatting with title, snippet, and relevance score
        - Query term highlighting marking matched words in result snippets
        - Search analytics dashboard tracking query volume and result quality
        estimated_hours: 8-12
    - id: recommendation-engine
      name: Recommendation Engine
      description: Build a production recommendation system with collaborative filtering, content-based, and hybrid approaches.
      difficulty: intermediate
      estimated_hours: 35-50
      essence: Sparse matrix decomposition and similarity computation across millions of user-item interactions, with multi-stage retrieval and ranking pipelines to achieve sub-200ms recommendation latency at scale.
      why_important: Building this teaches you production machine learning system design‚Äîfrom algorithm implementation (SVD, ALS, collaborative filtering) to real-world deployment challenges like cold-start problems, scaling to millions of users, and balancing model complexity with inference latency.
      learning_outcomes:
      - Implement user-based and item-based collaborative filtering with similarity metrics
      - Build matrix factorization models using SVD and ALS for latent factor discovery
      - Design content-based filtering with feature extraction and preference modeling
      - Handle cold-start problems for new users and items without interaction history
      - Implement multi-stage retrieval and ranking pipelines for low-latency recommendations
      - Build real-time recommendation APIs with caching and model serving infrastructure
      - Evaluate recommendation quality using precision@k, recall@k, and NDCG metrics
      - Deploy hybrid recommendation systems combining multiple algorithmic approaches
      skills:
      - Matrix Factorization
      - Collaborative Filtering
      - Sparse Matrix Operations
      - Feature Engineering
      - Model Serving
      - Latency Optimization
      - A/B Testing
      - Hybrid Systems
      tags:
      - collaborative-filtering
      - content-based
      - framework
      - game-dev
      - intermediate
      - python
      - ranking
      architecture_doc: architecture-docs/recommendation-engine/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - Go
        - Scala
      resources:
      - name: Surprise Library
        url: https://surpriselib.com/
        type: documentation
      - name: Netflix Recommendation Paper
        url: https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-BellKor-Solution.pdf
        type: paper
      prerequisites:
      - type: skill
        name: Python
      - type: skill
        name: Linear algebra basics
      - type: skill
        name: Database fundamentals
      milestones:
      - id: recommendation-engine-m1
        name: Collaborative Filtering
        description: Implement user-based and item-based collaborative filtering.
        acceptance_criteria:
        - Build user-item rating matrix from interaction history data
        - Calculate user-to-user similarity using cosine or Pearson metric
        - Calculate item-to-item similarity from co-rating patterns
        - Predict ratings using K-nearest neighbors weighted average method
        - Handle cold start problem for new users and new items
        pitfalls:
        - Sparse matrix operations are memory-intensive
        - Precomputing all similarities doesn't scale
        - 'Cold start: new users/items have no data'
        - Popularity bias in recommendations
        concepts:
        - Collaborative filtering
        - Similarity metrics
        - K-NN
        - Sparse matrices
        skills:
        - Sparse matrix manipulation with scipy
        - Computing cosine and Pearson similarity
        - Nearest neighbor search and ranking
        - Handling cold start scenarios
        deliverables:
        - User-item interaction matrix built from rating data
        - User similarity computation using cosine or Pearson correlation
        - Item similarity computation using co-occurrence patterns
        - Rating prediction using K-nearest neighbor weighted average
        estimated_hours: 8-12
      - id: recommendation-engine-m2
        name: Matrix Factorization
        description: Implement SVD and ALS for latent factor models.
        acceptance_criteria:
        - Perform SVD decomposition of the user-item rating matrix
        - Implement ALS (Alternating Least Squares) as alternative factorization
        - Apply regularization to prevent overfitting on sparse data
        - Handle implicit feedback signals like views and clicks
        - Tune hyperparameters for latent dimension count and learning rate
        pitfalls:
        - Learning rate too high causes divergence
        - Overfitting without regularization
        - Implicit data needs different loss function
        - Factor initialization affects convergence
        concepts:
        - Matrix factorization
        - SVD
        - Stochastic gradient descent
        - Regularization
        skills:
        - Implementing gradient descent optimization
        - Matrix decomposition techniques
        - Regularization to prevent overfitting
        - Training recommendation models with implicit feedback
        deliverables:
        - User-item matrix construction from sparse rating data
        - SVD or ALS matrix decomposition into latent factors
        - Latent factor learning with gradient descent optimization
        - Rating prediction by dot product of user and item factors
        estimated_hours: 8-12
      - id: recommendation-engine-m3
        name: Content-Based Filtering
        description: Recommend based on item features and user preferences.
        acceptance_criteria:
        - Extract item features from text descriptions, categories, and tags
        - Build user preference profile from liked and interacted items
        - Compute content similarity matching user preferences to item features
        - Combine content-based scores with collaborative filtering signals
        - Generate explainable recommendations showing why items were recommended
        pitfalls:
        - Feature extraction misses important signals
        - User profile dominated by few items
        - 'Filter bubble: only recommend similar items'
        - Explanations don't match user perception
        concepts:
        - Content-based filtering
        - TF-IDF
        - User profiling
        - Explainability
        skills:
        - Text feature extraction with TF-IDF
        - Building and updating user preference profiles
        - Content similarity computation
        - Generating explainable recommendations
        deliverables:
        - Item feature extraction from text, categories, and tags
        - User preference profile built from interaction history
        - Content similarity computation between user profile and items
        - Hybrid scoring combining collaborative and content-based signals
        estimated_hours: 6-10
      - id: recommendation-engine-m4
        name: Production System
        description: Build production-ready recommendation service.
        acceptance_criteria:
        - Serve real-time recommendation API with sub-second latency
        - Support batch candidate generation for offline precomputation
        - Implement A/B testing framework for comparing algorithm variants
        - Expose recommendation metrics and monitoring dashboards
        - Support model versioning with rollback capability
        pitfalls:
        - Cold cache causes latency spikes
        - A/B test assignment not deterministic
        - Metrics delayed lose temporal correlation
        - Model drift not detected
        concepts:
        - Two-stage recommendation
        - A/B testing
        - Metrics tracking
        - Model serving
        skills:
        - Designing multi-stage recommendation pipelines
        - Implementing A/B testing frameworks
        - Real-time model serving and caching
        - Monitoring recommendation system metrics
        deliverables:
        - Recommendation API endpoint returning ranked item suggestions
        - Real-time scoring pipeline computing recommendations on demand
        - A/B testing support for comparing recommendation algorithms
        - User feedback integration loop updating models from interactions
        estimated_hours: 12-16
    - id: ml-model-serving
      name: ML Model Serving API
      description: Model inference service with batching, versioning, A/B testing
      difficulty: intermediate
      estimated_hours: '45'
      essence: Dynamic request batching for inference throughput optimization, multi-version model deployment with traffic routing for A/B testing, and real-time drift detection through metrics aggregation across prediction streams.
      why_important: Building this teaches you production ML system design‚Äîhandling inference optimization, traffic splitting strategies, and drift detection that are critical for any ML engineering role.
      learning_outcomes:
      - Design scalable model serving architectures
      - Implement efficient batching for throughput
      - Handle model versioning and rollback
      - Monitor model performance in production
      skills:
      - Model loading
      - Request batching
      - Model versioning
      - A/B testing
      - Latency optimization
      - Model monitoring
      tags:
      - ai-ml
      - api
      - batching
      - devops
      - inference
      - intermediate
      - optimization
      architecture_doc: architecture-docs/ml-model-serving/index.md
      languages:
        recommended:
        - Python
        - Go
        also_possible: []
      resources:
      - name: TensorFlow Serving Guide
        url: https://www.tensorflow.org/tfx/guide/serving
        type: documentation
      - name: PyTorch Serve Performance Checklist
        url: https://docs.pytorch.org/serve/performance_checklist.html
        type: documentation
      - name: Ray Batch Inference Tutorial
        url: https://docs.ray.io/en/latest/data/batch_inference.html
        type: tutorial
      - name: A/B Testing ML Models Guide
        url: https://mlinproduction.com/ab-test-ml-models-deployment-series-08/
        type: article
      - name: Grafana ML Monitoring with Prometheus
        url: https://grafana.com/blog/2021/08/02/how-basisai-uses-grafana-and-prometheus-to-monitor-model-drift-in-machine-learning-workloads/
        type: article
      prerequisites:
      - type: skill
        name: REST API
      - type: skill
        name: Basic ML
      - type: skill
        name: Docker
      milestones:
      - id: ml-model-serving-m1
        name: Model Loading & Inference
        description: Load models and serve predictions
        acceptance_criteria:
        - Models from PyTorch, TensorFlow, and ONNX frameworks are loaded and ready for inference at startup
        - Inference endpoint validates input shape and dtype before running the model and returns structured results
        - Model warmup sends configurable dummy requests at startup to eliminate cold-start inference latency
        - GPU inference is used when a CUDA device is available and falls back to CPU when it is not
        pitfalls:
        - Loading entire model into memory without streaming for large models
        - Not handling model file corruption or version mismatches
        - Forgetting to set model to evaluation mode before inference
        - Blocking main thread during model loading causing timeout errors
        - Not preallocating tensors leading to repeated memory allocations during inference
        concepts:
        - Model serialization formats (ONNX, TensorFlow SavedModel, PyTorch JIT)
        - Memory-mapped file loading for large models
        - Inference optimization techniques (quantization, pruning, ONNX Runtime)
        - Thread-safe model initialization and singleton patterns
        - GPU memory allocation and CUDA context management
        skills:
        - Model formats
        - Memory management
        - Inference
        deliverables:
        - Model loader that imports models from PyTorch, TensorFlow SavedModel, and ONNX file formats
        - Prediction HTTP endpoint that accepts input data and returns model inference results as JSON
        - Input validator that checks request payloads against the model's expected input schema and dimensions
        - Output formatter that converts raw model tensors into structured JSON response payloads
        - GPU and CPU device handler that places models on the appropriate compute device based on availability
        - Warm-up request runner that sends initial inference requests at startup to pre-compile model graphs
        estimated_hours: '9'
      - id: ml-model-serving-m2
        name: Request Batching
        description: Batch requests for better throughput
        acceptance_criteria:
        - Incoming requests are batched together to improve GPU utilization and inference throughput
        - Maximum batch size and maximum wait timeout are configurable to balance latency and throughput
        - Partial batches are flushed and executed when the timeout expires even if the batch is not full
        - Batching metrics including average batch size, queue depth, and wait time are exposed for monitoring
        pitfalls:
        - Waiting too long to form batches causing high latency
        - Not handling timeout edge cases when batch isn't full
        - Forgetting to unbatch results in correct order after inference
        - Memory leaks from not clearing completed batches from queue
        - Ignoring maximum batch size limits leading to OOM errors
        concepts:
        - Dynamic batching with adaptive timeout windows
        - Asynchronous queue management for concurrent request handling
        - Batch size optimization based on GPU memory and latency requirements
        - Request padding and masking for variable-length inputs
        - Backpressure mechanisms to prevent queue overflow
        skills:
        - Dynamic batching
        - Async processing
        - Timeout handling
        deliverables:
        - Request queue that buffers incoming inference requests for batch formation before model execution
        - Dynamic batch former that groups queued requests into batches up to a configurable maximum size
        - Batch size limiter that caps the number of requests per batch to avoid GPU out-of-memory errors
        - Timeout handler that flushes a partial batch for execution when the maximum wait time is reached
        - Response router that splits batch inference results and returns each result to its original requester
        - Throughput metrics collector that tracks requests per second, batch sizes, and queue wait times
        estimated_hours: '9'
      - id: ml-model-serving-m3
        name: Model Versioning
        description: Manage multiple model versions
        acceptance_criteria:
        - Multiple versions of the same model are stored in the registry and available for inference simultaneously
        - Version-specific inference routes requests to the exact model version specified in the request
        - Hot model swapping loads a new version into memory and switches traffic without dropping requests
        - Version metadata including accuracy, training date, and data hash is tracked and queryable
        pitfalls:
        - Not draining request queues before swapping model versions
        - Breaking API contracts when updating model input/output schemas
        - Race conditions between concurrent model loads and serving
        - Not preserving performance baselines when rolling back versions
        - Insufficient validation before promoting new model version to production
        concepts:
        - Shadow traffic routing for zero-downtime version switching
        - Model registry and metadata storage for version tracking
        - Graceful shutdown patterns to drain in-flight requests
        - Atomic file operations for model replacement on disk
        - Version-specific feature flags and compatibility layers
        skills:
        - Version management
        - Hot reload
        - Rollback
        deliverables:
        - Model registry that stores and catalogs multiple models with their version numbers and metadata
        - Version metadata store that records model accuracy, training date, and framework for each version
        - Hot model reload mechanism that swaps a running model to a new version without server restart
        - Rollback mechanism that reverts to the previous model version if the new version performs poorly
        - Default version selector that routes requests to the latest stable version when no version is specified
        - Version-specific endpoint router that directs requests to a particular model version by URL path or header
        estimated_hours: '9'
      - id: ml-model-serving-m4
        name: A/B Testing & Canary
        description: Traffic splitting for model comparison
        acceptance_criteria:
        - Traffic is split between model versions according to configured percentage weights with minimal skew
        - Traffic percentage weights are configurable per experiment and can be adjusted without server restart
        - Per-version metrics including latency, error rate, and prediction distribution are tracked and compared
        - Gradual rollout increases traffic to the new version in configurable increments over time
        pitfalls:
        - Insufficient sample size leading to inconclusive A/B test results
        - Not accounting for selection bias in traffic routing
        - Forgetting to log experiment metadata with prediction results
        - Changing multiple variables simultaneously making results uninterpretable
        - Not handling model failures gracefully during canary deployments
        concepts:
        - Consistent hashing for deterministic traffic splitting
        - Multi-armed bandit algorithms for exploration vs exploitation
        - Statistical significance testing (t-test, chi-square) for experiment results
        - Shadow mode inference to compare models without affecting users
        - Feature flags with gradual rollout percentages
        skills:
        - Traffic routing
        - Experiment tracking
        - Statistical analysis
        deliverables:
        - Traffic split configuration that defines percentage weights for routing requests between model versions
        - Consistent user routing that hashes a user identifier to always send the same user to the same model version
        - Experiment metrics tracker that records per-version prediction outcomes and business metrics
        - Statistical significance calculator that determines when enough data has been collected to declare a winner
        - Gradual rollout controller that incrementally increases traffic to a new version over a configured schedule
        - Automatic rollback trigger that reverts traffic to the previous version if error rates exceed a threshold
        estimated_hours: '9'
      - id: ml-model-serving-m5
        name: Monitoring & Observability
        description: Monitor model performance and drift
        acceptance_criteria:
        - Inference latency percentiles p50, p90, p95, and p99 are tracked and exposed as metrics per model version
        - Prediction output distribution is monitored to detect shifts in model behavior over time
        - Data drift is detected by comparing live input feature distributions against the training data baseline
        - Alerts fire when monitored metrics exceed configured thresholds and are delivered via configured channels
        pitfalls:
        - Logging too much data causing storage and performance issues
        - Not monitoring actual prediction quality, only latency metrics
        - Missing drift in feature distributions that affect model accuracy
        - Alert fatigue from poorly tuned threshold values
        - Not correlating infrastructure metrics with model performance degradation
        concepts:
        - Prediction latency percentiles (p50, p95, p99) tracking
        - Input distribution drift detection using KL divergence or KS test
        - Prometheus metrics and Grafana dashboards for real-time monitoring
        - Structured logging with correlation IDs for request tracing
        - Model performance degradation alerts and automatic rollback triggers
        skills:
        - Metrics
        - Logging
        - Drift detection
        deliverables:
        - Latency metrics tracker recording p50, p90, p95, and p99 inference times per model and version
        - Throughput metrics tracker recording requests per second and successful versus failed inference counts
        - Input and output logger that samples and stores request payloads and model predictions for debugging
        - Data drift detector that monitors input feature distributions for significant shifts from the training baseline
        - Model accuracy monitor that compares predictions against delayed ground truth labels when available
        - Alerting rule engine that fires notifications when latency, error rate, or drift metrics exceed thresholds
        estimated_hours: '9'
    advanced:
    - id: transformer-scratch
      name: Transformer from Scratch
      description: Attention mechanism
      difficulty: advanced
      estimated_hours: 30-50
      essence: Scaled dot-product attention with query-key-value matrix operations, multi-head parallel processing, positional encoding to inject sequence order, and stacked encoder-decoder layers with residual connections and layer normalization for parallelizable sequence transduction without recurrence.
      why_important: Transformers power modern LLMs (GPT, BERT) and understanding their architecture from first principles‚Äîattention mechanisms, positional encodings, and encoder-decoder design‚Äîis essential for any ML engineer working with NLP, whether fine-tuning models, optimizing inference, or researching new architectures.
      learning_outcomes:
      - Implement scaled dot-product attention with query-key-value matrix operations
      - Build multi-head attention with parallel head projections and concatenation
      - Design positional encodings using sinusoidal functions to inject sequence order
      - Construct encoder and decoder layers with residual connections and layer normalization
      - Implement masked self-attention for autoregressive decoding
      - Debug gradient flow and numerical stability issues in deep attention networks
      - Train a transformer model on sequence-to-sequence tasks like translation
      - Optimize attention computation complexity and memory usage patterns
      skills:
      - Self-Attention Mechanisms
      - Matrix Operations & Linear Algebra
      - Neural Architecture Design
      - Sequence Modeling
      - Positional Encoding
      - Gradient Debugging
      - PyTorch Implementation
      - Training Deep Networks
      tags:
      - advanced
      - ai-ml
      - attention
      - embeddings
      - positional-encoding
      - python
      - self-attention
      architecture_doc: architecture-docs/transformer-scratch/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - Julia
      resources:
      - name: Attention Is All You Need Paper
        url: https://arxiv.org/abs/1706.03762
        type: paper
      - name: The Illustrated Transformer
        url: https://jalammar.github.io/illustrated-transformer/
        type: article
      - name: Annotated Transformer
        url: https://nlp.seas.harvard.edu/2018/04/03/attention.html
        type: tutorial
      prerequisites:
      - type: skill
        name: Neural networks
      - type: skill
        name: Linear algebra
      - type: skill
        name: Python/PyTorch basics
      milestones:
      - id: transformer-scratch-m1
        name: Scaled Dot-Product Attention
        description: Implement the core attention mechanism.
        acceptance_criteria:
        - Compute Q, K, V from input
        - Scaled dot-product computes softmax(QK^T / sqrt(d_k))V with correct dimensions
        - Handle attention mask for padding tokens and causal autoregressive decoding
        - Vectorized implementation processes full batch without explicit Python loops
        pitfalls:
        - Forgetting to scale by sqrt(d_k)
        - Wrong mask application
        - Dimension mismatches
        concepts:
        - Attention mechanism
        - Query-Key-Value
        - Masking
        skills:
        - Matrix multiplication and broadcasting
        - Softmax implementation
        - Attention weight computation
        - Numerical stability techniques
        deliverables:
        - Q, K, V matrix computation from input embeddings via learned linear projections
        - Attention score calculation using dot product between query and key matrices
        - Softmax normalization converting raw scores into a probability distribution
        - Weighted sum computation combining value vectors using attention probabilities
        estimated_hours: 4-6
      - id: transformer-scratch-m2
        name: Multi-Head Attention
        description: Implement multi-head attention with multiple parallel heads.
        acceptance_criteria:
        - Split input into h attention heads with d_k = d_model / h dimensions each
        - Project Q, K, V for each head
        - Parallel attention computation processes all heads in a single batched operation
        - Concatenate head outputs and project through output linear layer
        pitfalls:
        - Reshape vs view errors
        - Transpose dimensions wrong
        - Not using contiguous()
        concepts:
        - Multi-head attention
        - Parallel computation
        - Linear projections
        skills:
        - Tensor reshaping and dimension management
        - Parallel computation patterns
        - Linear transformation design
        - Multi-dimensional tensor operations
        deliverables:
        - Head splitting logic partitioning Q, K, V into h parallel attention heads
        - Parallel attention computation running scaled dot-product on each head independently
        - Head concatenation merging outputs from all attention heads into a single tensor
        - Output projection layer applying a linear transformation to concatenated head outputs
        estimated_hours: 4-6
      - id: transformer-scratch-m3
        name: Position-wise Feed-Forward & Embeddings
        description: Implement FFN layer and positional embeddings.
        acceptance_criteria:
        - Two-layer FFN with ReLU activation and configurable hidden dimension
        - Positional encoding uses sinusoidal functions with period varying by dimension
        - Token embeddings map each vocabulary index to a learned dense vector
        - Embedding scaling multiplies embeddings by sqrt(d_model) before adding positional encoding
        pitfalls:
        - Positional encoding dimensions
        - Forgetting dropout
        - Embedding scale factor
        concepts:
        - Positional encoding
        - Feed-forward networks
        - Residual connections
        skills:
        - Sinusoidal encoding implementation
        - Fully connected layer design
        - Embedding layer usage
        - Dropout regularization
        deliverables:
        - Feed-forward network with two linear layers and a ReLU activation between them
        - Token embedding lookup table mapping vocabulary indices to dense vectors
        - Positional encoding using sinusoidal functions at different frequencies per dimension
        - Embedding combination layer adding token embeddings and positional encodings element-wise
        estimated_hours: 4-6
      - id: transformer-scratch-m4
        name: Encoder & Decoder Layers
        description: Combine components into encoder/decoder layers.
        acceptance_criteria:
        - Encoder layer applies self-attention followed by feed-forward with residual connections
        - 'Decoder: masked self-attn + cross-attn + FFN'
        - Layer normalization is applied before or after each sublayer per configuration
        - Residual connections add sublayer input to its output before normalization
        pitfalls:
        - Pre-norm vs post-norm
        - Causal mask in decoder
        - Cross-attention key/value source
        concepts:
        - Encoder-decoder architecture
        - Layer normalization
        - Residual connections
        skills:
        - Layer composition and stacking
        - Normalization layer integration
        - Masked attention implementation
        - Cross-attention mechanics
        deliverables:
        - Encoder layer combining self-attention and feed-forward with residual connections
        - Decoder layer combining masked self-attention, cross-attention, and feed-forward sublayers
        - Layer stacking mechanism composing N identical layers into encoder and decoder stacks
        - Masked attention in decoder preventing positions from attending to future tokens
        estimated_hours: 5-8
      - id: transformer-scratch-m5
        name: Full Transformer & Training
        description: Assemble full transformer and train on a task.
        acceptance_criteria:
        - Stack N encoder and decoder layers with shared or separate parameters
        - Output projection maps decoder output to vocabulary-sized logits for prediction
        - Label smoothing regularization distributes probability mass to non-target tokens (optional)
        - Train on translation or LM task
        pitfalls:
        - Mask generation
        - Teacher forcing
        - Learning rate schedule
        concepts:
        - Full transformer
        - Training loop
        - Inference/generation
        skills:
        - End-to-end model assembly
        - Training loop implementation
        - Autoregressive generation
        - Loss function design
        - Learning rate scheduling
        deliverables:
        - Encoder-decoder architecture wiring together N-layer encoder and decoder stacks
        - Training loop with forward pass, loss computation, and backpropagation steps
        - Cross-entropy loss function computed over the output vocabulary logits
        - Inference module performing autoregressive decoding with greedy or beam search
        estimated_hours: 8-12
    - id: ai-agent-framework
      name: AI Agent Framework
      description: Build a framework for autonomous AI agents that can use tools, plan, and execute multi-step tasks.
      difficulty: advanced
      estimated_hours: 50-80
      essence: LLM-orchestrated control flow that alternates between generating reasoning traces and executing tool calls, with state management across multi-step execution plans that dynamically adapt based on observation feedback and task decomposition into acyclic dependency graphs.
      why_important: Building agent frameworks teaches the architecture of autonomous AI systems ‚Äî a rapidly growing category where over 60% of enterprise AI applications are expected to include agentic components by 2026, requiring skills in prompt engineering, tool orchestration, and stateful workflow design.
      learning_outcomes:
      - Implement tool calling systems with function schemas and dynamic invocation
      - Design ReAct loops that interleave reasoning traces with action execution
      - Build task decomposition engines that break complex goals into DAG-based execution plans
      - Implement short-term and long-term memory systems using vector stores and conversation buffers
      - Design multi-agent coordination protocols with message passing and shared state
      - Handle error recovery and plan re-evaluation when tool calls fail or contexts change
      - Implement function calling with LLM APIs using structured output schemas
      - Build reflection mechanisms that allow agents to critique and revise their own plans
      skills:
      - Tool Orchestration
      - ReAct Pattern Implementation
      - Task Decomposition
      - LLM Function Calling
      - Stateful Workflow Design
      - Vector Memory Systems
      - Multi-Agent Coordination
      - Plan-and-Execute Architecture
      tags:
      - advanced
      - ai-ml
      - framework
      - langchain
      - memory
      - planning
      - python
      - reasoning
      - tools
      architecture_doc: architecture-docs/ai-agent-framework/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - TypeScript
      resources:
      - name: LangChain Agents
        url: https://python.langchain.com/docs/modules/agents/
        type: documentation
      - name: AutoGPT
        url: https://github.com/Significant-Gravitas/AutoGPT
        type: reference
      - name: ReAct Paper
        url: https://arxiv.org/abs/2210.03629
        type: paper
      prerequisites:
      - type: skill
        name: LLM APIs
      - type: skill
        name: Python async
      - type: skill
        name: System design
      milestones:
      - id: ai-agent-framework-m1
        name: Tool System
        description: Build extensible tool system for agent capabilities.
        acceptance_criteria:
        - Tool interface defines name, description, JSON-schema parameters, and an execute callback
        - Tool discovery and registration allows adding new tools at runtime without restarting the agent
        - Parameter validation rejects malformed inputs and returns descriptive error messages before execution
        - Error handling catches tool failures, retries transient errors, and surfaces permanent failures to the agent
        - Tool result formatting converts raw output into a structured text format consumable by the LLM
        pitfalls:
        - Tool descriptions too vague for LLM to choose correctly
        - Missing parameter validation causes runtime errors
        - Tool execution without timeout hangs forever
        - Sensitive tools without permission checks
        concepts:
        - Tool abstraction
        - JSON Schema
        - Sandboxed execution
        - Registry pattern
        skills:
        - Python decorator design patterns
        - JSON Schema validation
        - Subprocess sandboxing and security
        - Plugin architecture implementation
        deliverables:
        - Tool interface definition specifying name, description, parameter schema, and execute method
        - Tool registry that allows dynamic registration, lookup, and discovery of available tools
        - Tool execution engine that validates inputs, runs the tool, and captures structured output
        - Built-in tools including web search, calculator, and sandboxed code execution with result capture
        estimated_hours: 8-12
      - id: ai-agent-framework-m2
        name: ReAct Loop (Reasoning + Acting)
        description: 'Implement the core agent loop: think, act, observe, repeat.'
        acceptance_criteria:
        - Thought-Action-Observation cycle repeats until the agent produces a final answer or hits a limit
        - Parser correctly extracts the tool name and JSON arguments from the LLM's action output
        - Action failures are caught and fed back as error observations so the agent can adjust its plan
        - Maximum iteration limit prevents infinite loops and triggers a graceful fallback response
        - Structured output parsing handles both JSON and freeform action formats from the LLM
        pitfalls:
        - Infinite loops when agent doesn't know when to stop
        - LLM hallucinates non-existent tools
        - Action parsing fails on malformed output
        - Not handling tool timeouts
        concepts:
        - ReAct framework
        - Function calling
        - Agent loops
        - Structured outputs
        skills:
        - LLM function calling APIs
        - Prompt engineering for structured outputs
        - State machine design
        - Error recovery and retry logic
        deliverables:
        - Thought-Action-Observation cycle that alternates between LLM reasoning and tool execution
        - LLM prompt template that elicits structured reasoning before each action selection
        - Action parser that extracts tool name and arguments from the LLM's textual output
        - Observation formatter that feeds tool results back into the LLM context for the next reasoning step
        estimated_hours: 10-15
      - id: ai-agent-framework-m3
        name: Planning & Task Decomposition
        description: Add planning layer for complex multi-step tasks.
        acceptance_criteria:
        - Agent decomposes a complex multi-step task into a list of concrete, actionable subtasks
        - Execution plan respects the declared ordering and runs subtasks in their dependency sequence
        - Dependencies between steps are tracked so that a blocked step waits for its prerequisite to complete
        - Agent re-plans when a subtask fails, generating an alternative path toward the overall goal
        - Independent subtasks are identified and executed in parallel where possible to reduce latency
        pitfalls:
        - Circular dependencies in task graph
        - Over-decomposition creates too many steps
        - Context lost between subtasks
        - Replanning loops infinitely
        concepts:
        - Task decomposition
        - DAG execution
        - Async parallel execution
        - Replanning
        skills:
        - Graph data structures and topological sorting
        - Async/await and concurrent execution
        - Dynamic replanning algorithms
        - Task dependency management
        deliverables:
        - Task decomposition prompting that instructs the LLM to break complex goals into ordered subtasks
        - Sub-task tracking data structure that records status, dependencies, and results for each subtask
        - Plan execution engine that runs subtasks in order, re-planning when a step fails or context changes
        - Goal completion detection that evaluates whether all subtasks are satisfied and the overall objective is met
        estimated_hours: 12-18
      - id: ai-agent-framework-m4
        name: Memory & Context Management
        description: Give agents short-term and long-term memory.
        acceptance_criteria:
        - Short-term conversation history is maintained in order and accessible for the current task session
        - Working memory holds the current task's intermediate state, goals, and partial results
        - Long-term vector store indexes past interactions and retrieves the top-k most relevant entries by similarity
        - Memory retrieval injects relevant context into the LLM prompt without exceeding the token budget
        - Memory summarization compresses older conversation turns into concise summaries to free context space
        pitfalls:
        - Memory grows unbounded without cleanup
        - Retrieved memories not relevant to task
        - Summarization loses critical details
        - Importance scoring is subjective
        concepts:
        - Working vs long-term memory
        - Memory retrieval
        - Context compression
        - Reflection
        skills:
        - Vector embeddings and semantic search
        - Token counting and context window management
        - Summary generation techniques
        - Priority queue and LRU cache implementation
        deliverables:
        - Conversation memory storage that persists the full dialogue history for the current session
        - Long-term memory backed by a vector store for semantic retrieval of past interactions and facts
        - Context window management that truncates or summarizes history to fit within the LLM token limit
        - Memory retrieval module that fetches relevant past context to augment the current LLM prompt
        estimated_hours: 10-15
      - id: ai-agent-framework-m5
        name: Multi-Agent Collaboration
        description: Build systems where multiple agents work together.
        acceptance_criteria:
        - Each agent has a defined role and advertised set of capabilities used for task routing
        - Message passing between agents supports request-response and broadcast communication patterns
        - A coordinator or orchestrator agent distributes work and monitors progress across subordinate agents
        - Shared context and state are accessible to all agents working on the same task without conflicts
        - Conflict resolution handles contradictory outputs from agents by applying a defined merge strategy
        pitfalls:
        - Agents talk past each other without shared context
        - Delegation loops (A delegates to B delegates to A)
        - Orchestrator becomes bottleneck
        - No timeout for unresponsive agents
        concepts:
        - Multi-agent systems
        - Message passing
        - Orchestration patterns
        - Role specialization
        skills:
        - Message queue patterns
        - Agent orchestration strategies
        - Inter-process communication
        - Load balancing across agents
        deliverables:
        - Agent-to-agent communication protocol enabling structured message passing between agents
        - Role assignment module that assigns specialized roles and capabilities to each agent instance
        - Task delegation mechanism that routes subtasks to the most capable agent based on role matching
        - Result aggregation layer that collects outputs from multiple agents and merges them into a coherent response
        estimated_hours: 15-20
    - id: llm-eval-framework
      name: LLM Evaluation Framework
      description: Build comprehensive evaluation system for LLM applications. Critical for production AI.
      difficulty: advanced
      estimated_hours: 40-60
      essence: Automated quality measurement of generative model outputs through statistical overlap metrics (BLEU, ROUGE), embedding similarity, and LLM-as-judge patterns, combined with dataset versioning, parallel execution infrastructure, and statistical significance testing to detect regression in production systems.
      why_important: Production LLM systems degrade silently through prompt drift, model updates, and data shifts‚Äîbuilding evaluation infrastructure teaches systematic quality measurement and regression detection essential for deploying AI reliably.
      learning_outcomes:
      - Implement reference-based metrics (BLEU, ROUGE) and reference-free evaluation patterns
      - Design LLM-as-judge evaluation pipelines with calibration and consistency checking
      - Build versioned evaluation dataset management with stratified sampling
      - Create parallel evaluation runners with result caching and incremental re-evaluation
      - Develop statistical analysis pipelines for metric aggregation and significance testing
      - Implement faithfulness and hallucination detection for RAG systems
      - Design experiment tracking systems comparing prompt variations and model versions
      - Build automated regression detection triggering on metric degradation
      skills:
      - LLM-as-judge patterns
      - Evaluation metrics design
      - Dataset versioning
      - Statistical analysis
      - Parallel processing
      - RAG evaluation
      - A/B testing infrastructure
      - Regression detection
      tags:
      - advanced
      - ai-ml
      - benchmarks
      - evaluation
      - framework
      - metrics
      - prompts
      - python
      architecture_doc: architecture-docs/llm-eval-framework/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - TypeScript
      resources:
      - name: Anthropic Eval Best Practices
        url: https://docs.anthropic.com/en/docs/build-with-claude/develop-tests
        type: documentation
      - name: LangSmith
        url: https://docs.smith.langchain.com/
        type: documentation
      - name: Braintrust
        url: https://www.braintrustdata.com/docs
        type: documentation
      prerequisites:
      - type: skill
        name: LLM APIs
      - type: skill
        name: Statistics
      - type: skill
        name: Python
      milestones:
      - id: llm-eval-framework-m1
        name: Dataset Management
        description: Create and version evaluation datasets.
        acceptance_criteria:
        - Test case schema validates that each case has a prompt, expected output, and at least one tag
        - Datasets are versioned with git-like tracking so changes can be diffed and rolled back
        - Import from CSV and JSON files maps columns to the test case schema automatically
        - Dataset splits into train, test, and validation subsets with configurable split ratios
        - Golden examples are marked as high-quality reference cases for calibrating LLM-as-judge evaluators
        pitfalls:
        - Test set leaking into training data
        - Version conflicts with concurrent edits
        - Large datasets slow to load
        - Not tracking dataset provenance
        concepts:
        - Dataset versioning
        - Train/test splits
        - Golden examples
        - Data lineage
        skills:
        - Data versioning with DVC or similar tools
        - Python data serialization (JSON, Parquet, pickle)
        - Dataset validation and schema enforcement
        - File-based data management patterns
        deliverables:
        - Dataset loader supporting CSV, JSON, and JSONL input formats with schema validation
        - Test case structure defining input prompt, expected output, metadata tags, and difficulty level
        - Ground truth storage that associates each test case with its reference answer and scoring rubric
        - Dataset versioning system that tracks changes to test cases over time with diff history
        estimated_hours: 6-10
      - id: llm-eval-framework-m2
        name: Evaluation Metrics
        description: Implement metrics for different evaluation types.
        acceptance_criteria:
        - Exact match and fuzzy match modes handle whitespace normalization and case-insensitive comparison
        - Semantic similarity computes cosine similarity between embeddings of the output and reference text
        - LLM-as-judge grading sends output and reference to a judge model and parses its score
        - Custom metric functions are registered as plugins and called automatically during evaluation
        - Aggregate scoring computes mean, median, and percentile scores across the full evaluation dataset
        pitfalls:
        - Semantic similarity not calibrated (what's 'good' score?)
        - LLM judge inconsistent across runs
        - Custom metrics not between 0-1
        - Not handling edge cases (empty strings, None)
        concepts:
        - Evaluation metrics
        - Fuzzy matching
        - Semantic similarity
        - LLM-as-judge
        skills:
        - String similarity algorithms (Levenshtein, fuzzy matching)
        - Embedding-based semantic comparison
        - LLM API integration for evaluation
        - Statistical metric design and normalization
        deliverables:
        - Exact match scorer that compares model output character-by-character against the reference answer
        - BLEU and ROUGE score calculators that measure n-gram overlap with reference text
        - Semantic similarity scorer using embedding cosine distance to compare meaning rather than surface text
        - Custom metric plugin system that allows users to register their own scoring functions
        estimated_hours: 8-12
      - id: llm-eval-framework-m3
        name: Evaluation Runner
        description: Run evaluations efficiently with caching and parallelism.
        acceptance_criteria:
        - Batch evaluation processes all test cases and records each model response with its score
        - Parallel LLM calls execute up to a configurable concurrency limit to maximize throughput
        - Result caching stores responses so re-running the same prompt skips the API call and uses cached output
        - Progress tracking displays a live counter of completed, pending, and failed evaluations
        - Evaluation resumes from the last successful test case after a crash or API failure
        pitfalls:
        - Rate limiting without backoff crashes run
        - Cache invalidation when prompt changes
        - Memory issues with large result sets
        - Lost progress on crash without checkpoints
        concepts:
        - Async evaluation
        - Result caching
        - Checkpointing
        - Progress tracking
        skills:
        - Python asyncio and concurrent execution
        - Redis or file-based caching strategies
        - Rate limiting and exponential backoff
        - Batch processing and memory management
        deliverables:
        - Model API integration layer that supports OpenAI, Anthropic, and local model inference backends
        - Batch evaluation engine that processes all test cases in configurable batch sizes
        - Progress tracker that displays completion percentage, estimated time remaining, and error counts
        - Result cache that stores model responses keyed by prompt hash to avoid redundant API calls
        estimated_hours: 10-15
      - id: llm-eval-framework-m4
        name: Reporting & Analysis
        description: Generate insights from evaluation results.
        acceptance_criteria:
        - Score breakdown groups results by tags and categories showing per-group mean and variance
        - Regression detection flags metrics that have degraded compared to a stored baseline evaluation
        - Failure analysis identifies the most common error patterns and groups them by similarity
        - Reports are exported as self-contained HTML or PDF files with charts and tables
        - CI/CD integration returns a pass or fail exit code based on configurable score thresholds
        pitfalls:
        - Small sample sizes give unreliable comparisons
        - Multiple comparisons without correction
        - Ignoring variance in aggregate scores
        - Report generation fails silently
        concepts:
        - Statistical testing
        - Regression detection
        - Failure analysis
        - Reporting
        skills:
        - Statistical hypothesis testing in Python
        - Data visualization with matplotlib or plotly
        - Pandas dataframe analysis and aggregation
        - Automated report generation from templates
        deliverables:
        - Score aggregation module that computes per-tag, per-category, and overall evaluation summaries
        - Error analysis tool that clusters common failure patterns and highlights weak areas
        - Comparison report generator that shows side-by-side metric diffs between two evaluation runs
        - Visualization dashboard that renders score distributions, tag breakdowns, and trend charts
        estimated_hours: 8-12
    - id: llm-finetuning-pipeline
      name: LLM Fine-tuning Pipeline
      description: LoRA/QLoRA fine-tuning with dataset preparation, evaluation
      difficulty: advanced
      estimated_hours: '55'
      essence: Low-rank matrix decomposition for parameter-efficient adapter training on frozen base models, combined with 4-bit NormalFloat quantization and double quantization for memory-constrained optimization of billion-parameter transformers.
      why_important: Building this teaches you production-ready techniques for customizing LLMs on consumer hardware, bridging the gap between research models and domain-specific applications without requiring enterprise-scale infrastructure.
      learning_outcomes:
      - Understand parameter-efficient fine-tuning methods
      - Prepare and format datasets for instruction tuning
      - Implement training with memory optimization
      - Evaluate and deploy fine-tuned models
      skills:
      - LoRA/QLoRA
      - Dataset preparation
      - Training loop
      - Evaluation metrics
      - Model merging
      - Quantization
      tags:
      - advanced
      - ai-ml
      - datasets
      - lora
      - streaming
      - training
      architecture_doc: architecture-docs/llm-finetuning-pipeline/index.md
      languages:
        recommended:
        - Python
        also_possible: []
      resources:
      - name: PEFT Documentation
        url: https://huggingface.co/docs/peft/en/index
        type: documentation
      - name: QLoRA Paper
        url: https://arxiv.org/abs/2305.14314
        type: paper
      - name: 4-bit Quantization with bitsandbytes
        url: https://huggingface.co/blog/4bit-transformers-bitsandbytes
        type: article
      - name: Instruction Tuning Data Overview
        url: https://www.ruder.io/an-overview-of-instruction-tuning-data/
        type: article
      - name: LLM Evaluation Metrics Guide
        url: https://www.evidentlyai.com/llm-guide/llm-benchmarks
        type: tutorial
      prerequisites:
      - type: skill
        name: PyTorch basics
      - type: skill
        name: Transformers library
      - type: skill
        name: GPU training
      milestones:
      - id: llm-finetuning-pipeline-m1
        name: Dataset Preparation
        description: Prepare and format training data
        acceptance_criteria:
        - Training data is loaded and validated for required fields like instruction and response
        - Raw data is converted to the instruction-response format expected by the fine-tuning trainer
        - Data is split into training and validation sets with configurable ratios and optional stratification
        - Data augmentation techniques like paraphrasing or back-translation are supported as optional steps
        pitfalls:
        - Not adding EOS tokens between conversation turns
        - Mixing incompatible chat templates from different model families
        - Truncating prompts mid-sentence without proper attention masks
        - Creating validation set from same distribution as training data
        - Forgetting to handle multi-turn conversations with proper role markers
        concepts:
        - Tokenizer special tokens and chat templates for instruction tuning
        - Dataset formatting patterns (instruction-response, conversations, system prompts)
        - Token length constraints and truncation strategies
        - Train-validation split strategies for LLM datasets
        - Data quality metrics and filtering techniques
        skills:
        - Data formatting
        - Tokenization
        - Chat templates
        deliverables:
        - Data loader that reads training data from JSON, JSONL, CSV, and Parquet formats
        - Instruction-response formatter that converts raw data into the model's expected prompt template
        - Chat template applicator that wraps instruction-response pairs in the model's chat format tokens
        - Tokenizer pipeline that encodes text with proper padding, truncation, and attention masks
        - Train-validation splitter that partitions data into training and validation sets with stratification
        - Data quality filter that removes duplicates, short entries, and samples exceeding max token length
        estimated_hours: '11'
      - id: llm-finetuning-pipeline-m2
        name: LoRA Configuration
        description: Set up LoRA adapters for efficient fine-tuning
        acceptance_criteria:
        - Target modules for LoRA injection are identified automatically based on model architecture
        - Rank and alpha parameters are configurable and affect the capacity of the low-rank adapters
        - Trainable LoRA adapters are initialized and injected into the frozen base model layers
        - Parameter count reduction is verified to be less than 1% of the full model parameter count
        pitfalls:
        - Setting rank too low causing underfitting on domain tasks
        - Targeting only attention layers and missing FFN contributions
        - Using default alpha without adjusting for chosen rank value
        - Forgetting to enable gradient checkpointing with LoRA adapters
        - Not freezing base model weights during adapter training
        concepts:
        - Low-rank matrix decomposition for parameter-efficient fine-tuning
        - Rank selection trade-offs between accuracy and memory efficiency
        - Target module selection (Q, K, V, O projections vs FFN layers)
        - LoRA alpha scaling parameter and effective learning rate
        - Merge strategies for combining LoRA weights with base model
        skills:
        - PEFT library
        - LoRA math
        - Target modules
        deliverables:
        - LoRA configuration object specifying rank, alpha, dropout, and target module names
        - Target module identifier that auto-detects attention and MLP layers eligible for LoRA adapters
        - Rank and alpha tuning utilities that help select appropriate hyperparameters for the task
        - Adapter initializer that injects low-rank matrices into the selected model layers
        - Trainable parameter counter that reports the total and percentage of parameters being fine-tuned
        - Memory estimator that predicts GPU VRAM usage based on model size and LoRA configuration
        estimated_hours: '11'
      - id: llm-finetuning-pipeline-m3
        name: QLoRA & Quantization
        description: 4-bit quantization for memory efficiency
        acceptance_criteria:
        - Base model loads in 4-bit precision using NF4 quantization with acceptable quality loss
        - Quantization parameters including quant type and compute dtype are configurable via a config object
        - Mixed precision training runs forward pass in float16 or bfloat16 while keeping weights in 4-bit
        - VRAM usage is monitored and reported before training, during training, and after model loading
        pitfalls:
        - Running out of CUDA memory despite quantization due to optimizer states
        - Using incompatible CUDA versions with bitsandbytes library
        - Not setting bnb_4bit_compute_dtype to match training precision
        - Attempting to quantize already-quantized model checkpoints
        - Forgetting nested quantization flag for maximum memory savings
        concepts:
        - 4-bit NormalFloat (NF4) quantization optimized for normal weight distributions
        - Double quantization for quantizing quantization constants themselves
        - Compute dtype versus storage dtype in mixed-precision training
        - Page-locked memory and CUDA kernel optimizations for quantized ops
        - Dequantization overhead during forward and backward passes
        skills:
        - BitsAndBytes
        - 4-bit quantization
        - NF4
        deliverables:
        - 4-bit model loader that loads the base model quantized to 4-bit NormalFloat precision
        - NF4 quantization configuration specifying quantization type, compute dtype, and nested quantization
        - Compute dtype selector that sets the dtype for forward pass calculations during training
        - Double quantization option that quantizes the quantization constants for additional memory savings
        - Memory benchmarking tool that measures actual VRAM usage before and after quantization
        - Quality-versus-memory tradeoff analysis comparing perplexity at different quantization levels
        estimated_hours: '11'
      - id: llm-finetuning-pipeline-m4
        name: Training Loop
        description: Implement training with optimization
        acceptance_criteria:
        - Training loop runs with gradient accumulation across the configured number of micro-batches
        - Training loss is tracked and logged at every step to monitor convergence progress
        - Checkpoints are saved at regular intervals and the best checkpoint is tracked by validation loss
        - Early stopping halts training when validation loss fails to improve for a configurable number of evaluations
        pitfalls:
        - Setting gradient accumulation steps without adjusting learning rate
        - Using too high learning rate causing catastrophic forgetting
        - Not monitoring loss curves for overfitting early signs
        - Forgetting to clear optimizer states when resuming from checkpoint
        - Running full evaluation too frequently slowing down training
        concepts:
        - Gradient accumulation for simulating larger effective batch sizes
        - Learning rate scheduling (warmup, cosine decay, constant with cooldown)
        - Mixed-precision training with automatic loss scaling
        - Gradient clipping to prevent training instability
        - Checkpoint saving strategies (best model, interval-based, last-k)
        skills:
        - Trainer API
        - Gradient accumulation
        - Checkpointing
        deliverables:
        - Training arguments configuration covering learning rate, batch size, warmup steps, and max epochs
        - Gradient accumulation handler that simulates larger batch sizes across multiple forward passes
        - Learning rate scheduler supporting linear warmup with cosine or linear decay over training steps
        - Checkpoint manager that saves model weights and optimizer state at configurable step intervals
        - Logging integration that streams training metrics to WandB or TensorBoard in real time
        - Early stopping callback that halts training when validation loss stops improving for N evaluations
        estimated_hours: '11'
      - id: llm-finetuning-pipeline-m5
        name: Evaluation & Merging
        description: Evaluate model and merge adapters
        acceptance_criteria:
        - Perplexity is evaluated on the held-out validation set and compared against the base model
        - Task-specific metrics show measurable improvement comparing before and after fine-tuning scores
        - LoRA adapter weights are merged into the base model producing a single standalone model file
        - Merged model is exported in GGUF format and loads correctly in llama.cpp or similar inference runtime
        pitfalls:
        - Evaluating only on training distribution without out-of-domain tests
        - Merging adapters without verifying merged weights match expected behavior
        - Converting to GGUF without testing inference quality degradation
        - Not comparing fine-tuned model against base model baseline
        - Forgetting to disable dropout and set model to eval mode
        concepts:
        - Perplexity and cross-entropy loss for language modeling evaluation
        - Task-specific metrics (BLEU, ROUGE, exact match, accuracy)
        - Adapter weight merging versus keeping separate LoRA modules
        - GGUF quantization formats for efficient inference deployment
        - Inference optimization techniques (Flash Attention, KV cache)
        skills:
        - Evaluation metrics
        - Model merging
        - GGUF export
        deliverables:
        - Perplexity calculator that evaluates the fine-tuned model on the held-out validation set
        - Task-specific evaluation runner that measures accuracy on domain-relevant benchmark tasks
        - Adapter merger that fuses LoRA weights back into the base model for standalone inference
        - Model exporter that converts the merged model to GGUF format for local inference engines
        - Inference comparison tool that runs the same prompts on base and fine-tuned models side by side
        - Quality benchmark suite that scores the fine-tuned model against standard LLM evaluation benchmarks
        estimated_hours: '11'
    - id: mlops-platform
      name: MLOps Platform
      description: 'End-to-end ML lifecycle: training, versioning, deployment, monitoring'
      difficulty: advanced
      estimated_hours: '70'
      essence: Orchestrating distributed experiment tracking and metadata management, implementing versioned artifact storage with lineage tracking, building containerized training pipelines with resource scheduling, deploying models as scalable inference endpoints, and detecting statistical drift in production data distributions to trigger automated retraining workflows.
      why_important: Building an MLOps platform teaches production-grade ML engineering skills that are essential for deploying and maintaining models in real-world systems, covering the full model lifecycle from experimentation to production monitoring.
      learning_outcomes:
      - Design ML lifecycle management systems
      - Implement experiment tracking and versioning
      - Build automated training pipelines
      - Deploy and monitor models in production
      skills:
      - Experiment tracking
      - Model versioning
      - Pipeline orchestration
      - Model deployment
      - Feature store
      - Model monitoring
      tags:
      - advanced
      - devops
      - experiments
      - feature-store
      - framework
      - model-registry
      - pipelines
      architecture_doc: architecture-docs/mlops-platform/index.md
      languages:
        recommended:
        - Python
        - Go
        also_possible: []
      resources:
      - name: MLflow Documentation
        url: https://mlflow.org/docs/latest/
        type: documentation
      - name: Kubeflow Documentation
        url: https://www.kubeflow.org/docs/
        type: documentation
      - name: Google Cloud MLOps Guide
        url: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning
        type: article
      - name: ML Model Monitoring Best Practices
        url: https://www.datadoghq.com/blog/ml-model-monitoring-in-production-best-practices/
        type: article
      - name: Kubeflow Pipelines Tutorial
        url: https://www.datacamp.com/tutorial/kubeflow-tutorial-building-and-deploying-machine-learning-pipelines
        type: tutorial
      prerequisites:
      - type: skill
        name: ML basics
      - type: skill
        name: Docker
      - type: skill
        name: REST APIs
      milestones:
      - id: mlops-platform-m1
        name: Experiment Tracking
        description: Track experiments, parameters, and metrics
        acceptance_criteria:
        - Experiment parameters are logged as key-value pairs and associated with the specific run that produced them
        - Training metrics are tracked at each step creating a time series that shows learning progress
        - Artifacts including model files, plots, and configuration files are stored and downloadable per run
        - Multiple experiment runs can be compared side-by-side on metrics and parameters in a table or chart view
        pitfalls:
        - Not normalizing metric names leading to duplicate entries
        - Logging too frequently causing storage bloat and performance issues
        - Storing large artifacts directly in database instead of object storage
        - Not implementing pagination for experiment queries causing memory issues
        - Failing to validate parameter types before storage causing query failures
        concepts:
        - Structured logging with correlation IDs for experiment traceability
        - Hierarchical parameter storage with nested configurations
        - Time-series metric aggregation and statistical comparisons
        - Artifact hashing and deduplication for efficient storage
        - Query optimization for experiment metadata retrieval
        skills:
        - Logging
        - Metadata storage
        - Visualization
        deliverables:
        - Experiment and run abstraction layer that organizes training runs into named experiment groups
        - Parameter logging API that records hyperparameter key-value pairs for each training run
        - Metric logging API that records named metrics at each training step with step number and timestamp
        - Artifact storage system that saves models, plots, and data files associated with each run
        - Run comparison view that displays side-by-side metrics and parameters for selected runs
        - Experiment dashboard UI that lists runs with sortable columns for key metrics and parameters
        estimated_hours: '14'
      - id: mlops-platform-m2
        name: Model Registry
        description: Version and manage trained models
        acceptance_criteria:
        - Trained models are registered with metadata including name, version, accuracy, and source run ID
        - Model versions transition through defined stages with approval gates between staging and production
        - Model lineage traces back to the training data version, code commit, and experiment run that produced it
        - Model search and discovery queries filter and sort models by name, stage, metrics, and tags
        pitfalls:
        - Not validating model compatibility during version upgrades
        - Storing entire model files instead of using diff-based versioning
        - Missing immutability guarantees allowing production model corruption
        - Not tracking model dependencies causing deployment failures
        - Inadequate access control allowing unauthorized model modifications
        concepts:
        - Content-addressable storage using model checksums for deduplication
        - Semantic versioning with major minor patch for model releases
        - Model lineage tracking linking models to training experiments
        - Stage promotion workflows with approval gates and rollback capability
        - Model signature validation ensuring input output schema compatibility
        skills:
        - Model serialization
        - Versioning
        - Stage management
        deliverables:
        - Model registration API that creates a named model entry with version and link to the source run
        - Version management system that auto-increments version numbers and tracks version history
        - Stage transition workflow that moves model versions through staging, production, and archived stages
        - Model metadata store that records framework, accuracy, data hash, and custom tags for each version
        - Model lineage tracker that links each model version back to its training data, code commit, and run
        - Model download API that serves model artifacts for a specific version from the registry storage
        estimated_hours: '14'
      - id: mlops-platform-m3
        name: Training Pipeline
        description: Orchestrate training workflows
        acceptance_criteria:
        - Training pipeline steps are defined in a DAG with explicit data dependencies between steps
        - Steps execute on configured compute infrastructure with specified CPU, memory, and GPU resources
        - Resource usage for each step including CPU time, memory peak, and GPU utilization is tracked and logged
        - Distributed training across multiple GPUs or nodes is supported for steps that require it
        pitfalls:
        - Not handling partial failures causing entire pipeline restarts
        - Hardcoding resource requirements instead of dynamic allocation
        - Missing data validation steps causing silent training failures
        - Not implementing idempotent steps leading to duplicate processing
        - Inadequate logging making pipeline debugging extremely difficult
        concepts:
        - Directed acyclic graphs for dependency-based task execution
        - Resource allocation and scheduling across compute clusters
        - Checkpoint restart mechanisms for fault-tolerant training
        - Parameter server architecture for distributed gradient aggregation
        - Container orchestration with resource limits and isolation
        skills:
        - DAG orchestration
        - Containerized training
        - Distributed training
        deliverables:
        - Pipeline definition DSL that describes training steps as a directed acyclic graph of operations
        - Step executor that runs each pipeline step with configured compute resources and container image
        - Data passing mechanism that transfers outputs from upstream steps to downstream step inputs
        - Conditional execution logic that skips or includes pipeline steps based on runtime conditions
        - Parallel step scheduler that executes independent steps concurrently to reduce total pipeline time
        - Pipeline versioning system that tracks changes to pipeline definitions for reproducibility
        estimated_hours: '14'
      - id: mlops-platform-m4
        name: Model Deployment
        description: Deploy models to production
        acceptance_criteria:
        - A registered model version is deployed as an HTTP API endpoint accepting inference requests
        - Canary deployments route a configurable percentage of traffic to the new version for validation
        - Auto-scaling adjusts replica count based on request rate and p99 latency within configured bounds
        - Deployment integrates with inference servers like Triton, TorchServe, or TensorFlow Serving
        pitfalls:
        - Not warming up models before routing production traffic
        - Missing graceful degradation when model serving fails
        - Inadequate load testing leading to production bottlenecks
        - Not versioning serving infrastructure alongside models
        - Forgetting to implement request timeouts causing resource exhaustion
        concepts:
        - Model serving optimization through batching and caching strategies
        - Blue-green deployment for zero-downtime model updates
        - Traffic splitting and gradual rollout for risk mitigation
        - Model serialization formats balancing size and inference speed
        - Auto-scaling based on request latency and throughput metrics
        skills:
        - Serving infrastructure
        - Canary deployment
        - A/B testing
        deliverables:
        - Model serving endpoint generator that wraps a registered model version behind an HTTP inference API
        - Blue-green deployment controller that runs two versions simultaneously and switches traffic atomically
        - Canary rollout manager that gradually shifts traffic from the current version to the new version
        - A/B testing framework that splits traffic between model versions and tracks per-version metrics
        - Auto-scaler that adjusts the number of serving replicas based on request load and latency targets
        - Deployment rollback mechanism that reverts to the previous model version on error rate spike
        estimated_hours: '14'
      - id: mlops-platform-m5
        name: Model Monitoring
        description: Monitor model performance in production
        acceptance_criteria:
        - Prediction metrics including accuracy, latency percentiles, and throughput are tracked in production
        - Data drift is detected by computing statistical distance between live and training feature distributions
        - Alerts fire when model accuracy drops or prediction distribution shifts beyond configured thresholds
        - A/B testing analysis computes statistical significance between model versions and recommends a winner
        pitfalls:
        - Setting static thresholds that don't adapt to data patterns
        - Not monitoring upstream data pipeline failures affecting model inputs
        - Missing business metric correlation with technical metrics
        - Inadequate retention policies causing metric storage explosion
        - Not establishing baseline metrics before deploying monitoring
        concepts:
        - Statistical drift detection using KL divergence and PSI metrics
        - Feature distribution monitoring for input data validation
        - Prediction latency tracking across percentiles and endpoints
        - Model performance degradation alerts with configurable thresholds
        - Explainability tracking to monitor feature importance shifts
        skills:
        - Metrics collection
        - Drift detection
        - Alerting
        deliverables:
        - Prediction logger that records model inputs, outputs, and timestamps for every inference request
        - Performance metrics tracker that computes accuracy, latency, and throughput for the deployed model
        - Data drift detector that compares incoming feature distributions against the training data baseline
        - Model drift detector that monitors prediction distribution changes indicating concept drift
        - Alerting rule engine that fires notifications when monitored metrics cross configured thresholds
        - Monitoring dashboard that visualizes prediction metrics, drift scores, and alert history over time
        estimated_hours: '14'
    expert:
    - id: build-nn-framework
      name: Build Your Own Neural Network Framework
      description: PyTorch/TensorFlow clone
      difficulty: expert
      estimated_hours: 60-100
      essence: Computational graph construction with reverse-mode automatic differentiation (autograd), implementing the chain rule through dynamic graph traversal to compute gradients for arbitrary tensor operations.
      why_important: Building a deep learning framework from scratch demystifies how PyTorch and TensorFlow work internally, teaching you the mathematical foundations of backpropagation, memory-efficient gradient computation, and production-grade numerical computing that underlies all modern AI systems.
      learning_outcomes:
      - Implement dynamic computational graphs with topological sorting for gradient flow
      - Design reverse-mode automatic differentiation using the chain rule and Jacobian-vector products
      - Build memory-efficient tensor storage with broadcasting and view semantics
      - Implement gradient descent optimizers with momentum and adaptive learning rates
      - Debug numerical stability issues in backpropagation and vanishing gradients
      - Optimize matrix operations using BLAS libraries or GPU acceleration
      - Design modular neural network APIs with parameter management and state serialization
      - Profile and benchmark computational performance against production frameworks
      skills:
      - Automatic Differentiation
      - Computational Graphs
      - Numerical Computing
      - Gradient Optimization
      - Memory Management
      - BLAS/Linear Algebra
      - Dynamic Typing Systems
      - Performance Profiling
      tags:
      - ai-ml
      - autograd
      - backpropagation
      - build-from-scratch
      - c++
      - expert
      - framework
      - layers
      - networking
      - python
      - rust
      - tensors
      architecture_doc: architecture-docs/build-nn-framework/index.md
      languages:
        recommended:
        - Python
        - Rust
        - C++
        also_possible:
        - Julia
      resources:
      - type: video
        name: Micrograd by Karpathy
        url: https://www.youtube.com/watch?v=VMj-3S1tku0
      - type: article
        name: Autodiff from Scratch
        url: https://sidsite.com/posts/autodiff/
      - type: repository
        name: Tinygrad
        url: https://github.com/tinygrad/tinygrad
      prerequisites:
      - type: skill
        name: Linear algebra
      - type: skill
        name: Calculus (chain rule)
      - type: skill
        name: Python/NumPy
      - type: skill
        name: Basic neural networks
      milestones:
      - id: build-nn-framework-m1
        name: Tensor & Operations
        description: Implement tensor data structure with basic operations.
        acceptance_criteria:
        - Tensor stores N-dimensional array data with shape and requires_grad attributes
        - Element-wise add, subtract, multiply, and divide produce correct results
        - Matrix multiplication yields correct output for 2D and batched inputs
        - Broadcasting automatically expands dimensions following NumPy rules
        - Optional GPU support offloads computation to CUDA device
        pitfalls:
        - Broadcasting gradient shapes
        - In-place operations breaking grad
        - Memory management
        concepts:
        - Tensor operations
        - Broadcasting
        - Memory layout
        skills:
        - Multidimensional array manipulation
        - Memory-efficient data structures
        - Numerical computing optimization
        - Broadcasting semantics implementation
        deliverables:
        - N-dimensional tensor class with shape and dtype tracking
        - Element-wise arithmetic operations with operator overloading
        - Matrix multiplication supporting batched dimensions
        - NumPy-style broadcasting for mismatched tensor shapes
        estimated_hours: 12-18
      - id: build-nn-framework-m2
        name: Automatic Differentiation
        description: Implement reverse-mode autodiff (backpropagation).
        acceptance_criteria:
        - Computation graph correctly tracks parent-child operation relationships
        - Reverse-mode autodiff computes gradients matching numerical differentiation
        - Gradient accumulation sums contributions when tensor used multiple times
        - Topological sort ensures correct backward pass ordering without missing nodes
        pitfalls:
        - Gradient not accumulated
        - Wrong topological order
        - Vanishing/exploding gradients
        concepts:
        - Computational graphs
        - Chain rule
        - Backpropagation
        skills:
        - Computational graph construction
        - Reverse-mode differentiation implementation
        - Gradient accumulation and management
        - Dynamic graph traversal algorithms
        deliverables:
        - Computation graph constructed during forward pass operations
        - Backward pass traversal using topological sort ordering
        - Gradient accumulation across multiple backward contributions
        - Gradient tape implementation recording operation history
        estimated_hours: 15-20
      - id: build-nn-framework-m3
        name: Layers & Modules
        description: Implement neural network layers and module system.
        acceptance_criteria:
        - Linear layer produces correct output shape for given input and weight dimensions
        - Activation functions apply element-wise and propagate gradients correctly
        - Module.parameters() recursively collects all trainable tensors from submodules
        - Sequential container chains layers and forwards input through each in order
        pitfalls:
        - Parameter not tracked
        - In-place modifications
        - Wrong initialization
        concepts:
        - Module pattern
        - Parameter management
        - Initialization
        skills:
        - Object-oriented framework design
        - Parameter initialization strategies
        - Module composition patterns
        - State management in neural networks
        deliverables:
        - Linear layer implementing y = Wx + b with weight initialization
        - Activation functions including ReLU, sigmoid, and tanh
        - Module base class with recursive parameter collection
        - Parameter registration system tracking all trainable tensors
        estimated_hours: 12-18
      - id: build-nn-framework-m4
        name: Optimizers & Training
        description: Implement optimizers and training loop.
        acceptance_criteria:
        - SGD updates parameters by subtracting learning rate times gradient
        - Adam optimizer converges faster than SGD on non-convex loss landscapes
        - Learning rate scheduling reduces rate according to configurable strategy
        - Mini-batch training shuffles data and processes fixed-size batches per epoch
        pitfalls:
        - Bias correction in Adam
        - Learning rate too high/low
        - Not shuffling data
        concepts:
        - Optimization algorithms
        - Momentum
        - Adaptive learning rates
        skills:
        - Gradient descent optimization variants
        - Mini-batch training implementation
        - Learning rate scheduling strategies
        - Training loop design patterns
        deliverables:
        - SGD optimizer with configurable momentum and learning rate
        - Adam optimizer with bias-corrected first and second moment estimates
        - Training step abstraction managing forward, backward, and update
        - Loss functions including cross-entropy and mean squared error
        estimated_hours: 15-20
    - id: build-transformer
      name: Build Your Own Transformer
      description: Full GPT implementation
      difficulty: expert
      estimated_hours: 50-80
      essence: Self-attention mechanisms computing query-key-value transformations over token embeddings, with multi-head parallelization and positional encodings to capture sequence order, trained through next-token prediction to learn probabilistic language modeling.
      why_important: Building a transformer from scratch demystifies the architecture powering modern LLMs like GPT and BERT, teaching core deep learning patterns (attention, residual connections, layer normalization) essential for ML engineering roles.
      learning_outcomes:
      - Implement scaled dot-product attention with query-key-value matrices
      - Build multi-head attention by concatenating parallel attention heads
      - Design positional encodings to inject sequence order information
      - Implement feed-forward networks with residual connections and layer normalization
      - Debug gradient flow through deep networks with multiple skip connections
      - Implement autoregressive text generation with temperature sampling
      - Train language models using cross-entropy loss and next-token prediction
      - Optimize memory usage with key-value caching during inference
      skills:
      - Self-Attention Mechanisms
      - Neural Architecture Design
      - Autoregressive Generation
      - Positional Encoding
      - Deep Learning Optimization
      - PyTorch Implementation
      - Language Modeling
      - Gradient Debugging
      tags:
      - ai-ml
      - attention
      - build-from-scratch
      - embeddings
      - expert
      - nlp
      - positional-encoding
      - python
      architecture_doc: architecture-docs/build-transformer/index.md
      languages:
        recommended:
        - Python
        also_possible:
        - Julia
        - Rust
      resources:
      - type: paper
        name: Attention Is All You Need
        url: https://arxiv.org/abs/1706.03762
      - type: video
        name: Let's build GPT by Karpathy
        url: https://www.youtube.com/watch?v=kCc8FmEb1nY
      - type: repository
        name: minGPT
        url: https://github.com/karpathy/minGPT
      prerequisites:
      - type: skill
        name: Neural networks
      - type: skill
        name: Attention mechanism basics
      - type: skill
        name: NLP fundamentals
      - type: skill
        name: PyTorch/TensorFlow
      milestones:
      - id: build-transformer-m1
        name: Self-Attention
        description: Implement scaled dot-product attention.
        acceptance_criteria:
        - Q, K, V projections produce matrices of correct dimensions for each attention head
        - Attention scores are scaled by square root of key dimension before softmax
        - Multi-head attention concatenates outputs from all heads and projects to output dimension
        - Causal attention mask sets future position scores to negative infinity before softmax
        pitfalls:
        - Wrong attention dimension
        - Forgetting to scale
        - Mask applied after softmax
        concepts:
        - Attention mechanism
        - Multi-head attention
        - Masking
        skills:
        - Matrix multiplication optimization
        - Broadcasting operations in NumPy/PyTorch
        - Implementing attention masks
        - Softmax numerical stability
        deliverables:
        - Query, key, value projection matrices transforming input embeddings
        - Scaled dot-product attention computing weighted sum of value vectors
        - Multi-head attention splitting computation across parallel attention heads
        - Attention mask preventing positions from attending to future tokens
        estimated_hours: 10-15
      - id: build-transformer-m2
        name: Transformer Block
        description: Build a complete transformer block with FFN and normalization.
        acceptance_criteria:
        - Feed-forward network expands to 4x hidden dimension then projects back to original
        - Layer normalization normalizes activations to zero mean and unit variance per token
        - Residual connections preserve gradient flow by adding sub-layer input to its output
        - Dropout randomly zeros elements with configurable probability during training only
        pitfalls:
        - Pre-norm vs post-norm confusion
        - Missing residual connections
        - Wrong FFN expansion ratio
        concepts:
        - Transformer architecture
        - Layer normalization
        - Residual learning
        skills:
        - Layer normalization implementation
        - Residual connection patterns
        - Feed-forward network design
        - Module composition in PyTorch/TensorFlow
        deliverables:
        - Feed-forward network with two linear layers and activation between them
        - Layer normalization stabilizing activations with learned scale and shift
        - Residual connections adding input to output of each sub-layer
        - Dropout layer randomly zeroing activations during training for regularization
        estimated_hours: 12-18
      - id: build-transformer-m3
        name: Training Pipeline
        description: Implement training with language modeling objective.
        acceptance_criteria:
        - Tokenizer splits text into subword or character tokens and maps to integer IDs
        - Data loader yields batches of fixed-length token sequences with next-token targets
        - Cross-entropy loss decreases consistently over training epochs on training data
        - Training loop logs loss at regular intervals showing convergence progress
        pitfalls:
        - Label shifting wrong
        - No gradient clipping causing instability
        - Wrong loss computation
        concepts:
        - Language modeling
        - Learning rate scheduling
        - Mixed precision
        skills:
        - Cross-entropy loss for sequences
        - Gradient clipping techniques
        - Learning rate warmup schedules
        - Mixed precision training with autocast
        - Checkpoint saving and loading
        deliverables:
        - Tokenizer converting text strings into integer token sequences
        - Data loader producing batched token sequences for training iteration
        - Cross-entropy loss computing prediction error against target tokens
        - Training loop with gradient computation, optimizer step, and loss logging
        estimated_hours: 12-18
      - id: build-transformer-m4
        name: Text Generation
        description: Implement autoregressive text generation.
        acceptance_criteria:
        - Greedy decoding produces deterministic output by always selecting most likely token
        - Temperature parameter scales logits so lower values produce more focused text
        - Top-k sampling restricts candidates to k highest-probability tokens before sampling
        - Generated text is coherent at short lengths after training on sufficient data
        pitfalls:
        - Repetitive text without sampling
        - KV cache dimension mismatch
        - Temperature of 0 causing division
        concepts:
        - Autoregressive generation
        - Sampling strategies
        - KV caching
        skills:
        - Temperature-based sampling
        - Top-k and nucleus sampling
        - Efficient inference with KV caching
        - Handling end-of-sequence tokens
        deliverables:
        - Greedy decoding selecting highest-probability token at each generation step
        - Temperature-based sampling controlling randomness of generated text
        - Top-k and top-p sampling filtering token candidates before sampling
        - Text generation loop producing sequences from given prompt string
        estimated_hours: 12-18
- id: game-dev
  name: Game Development
  icon: üéÆ
  subdomains:
  - name: Game Programming
  - name: Graphics
  - name: Engine Development
  projects:
    beginner:
    - id: tetris
      name: Tetris
      description: Rotation, line clearing
      difficulty: beginner
      estimated_hours: 15-20
      essence: Grid-based collision detection, matrix transformations for piece rotation with wall-kick algorithms, and real-time game state updates managing falling pieces, line clearing, and score calculation.
      why_important: Tetris introduces fundamental game development patterns including fixed-timestep game loops, collision detection in constrained spaces, and state machines for managing game progression, all of which translate directly to more complex game systems and real-time applications.
      learning_outcomes:
      - Implement grid-based collision detection for moving game pieces
      - Design rotation matrices with wall-kick algorithms for boundary handling
      - Build a fixed-timestep game loop with frame-independent updates
      - Manage complex game state across multiple concurrent events
      - Implement efficient line-clearing algorithms with 2D array manipulation
      - Create input buffering and key repeat handling for responsive controls
      - Debug timing-dependent behavior and race conditions in real-time systems
      - Optimize render cycles to separate game logic from visual updates
      skills:
      - Grid-based Collision Detection
      - Matrix Transformations
      - Game Loop Architecture
      - State Machine Design
      - 2D Array Manipulation
      - Input Handling
      - Frame-Independent Timing
      - Real-time Rendering
      tags:
      - beginner-friendly
      - c#
      - collision
      - game-dev
      - game-loop
      - input-handling
      - javascript
      - python
      - sprites
      architecture_doc: architecture-docs/tetris/index.md
      languages:
        recommended:
        - JavaScript
        - Python
        - C#
        also_possible:
        - Rust
        - Go
        - C++
      resources:
      - name: Tetris with JavaScript
        url: https://www.youtube.com/watch?v=rAUn1Lom6dw
        type: video
      - name: Coding Challenges Tetris
        url: https://codingchallenges.fyi/challenges/challenge-tetris/
        type: tutorial
      prerequisites:
      - type: skill
        name: Basic programming
      - type: skill
        name: 2D arrays
      - type: skill
        name: HTML5 Canvas or similar
      milestones:
      - id: tetris-m1
        name: Board & Tetrominoes
        description: Create the game board and define tetromino shapes.
        acceptance_criteria:
        - 10x20 game board is initialized with all cells empty
        - 7 standard tetromino shapes (I, O, T, S, Z, J, L)
        - Each shape has a distinct color visible on the board
        - Shapes are defined as 2D arrays with rotation offset data
        - Board renders the empty grid with visible cell boundaries
        pitfalls:
        - Piece definition orientation inconsistent
        - Board dimensions swapped
        - Off-by-one in grid rendering
        concepts:
        - 2D arrays
        - Piece representation
        - Grid rendering
        skills:
        - Grid-based rendering
        - Data structure design for game states
        - Coordinate system management
        - Shape representation in code
        deliverables:
        - Game board grid data structure representing a 10-column by 20-row playing field
        - Tetromino shape definitions for all seven standard piece types
        - Tetromino rotation data storing four orientation states for each piece type
        - Color mapping table assigning a distinct color to each tetromino type
        estimated_hours: 2-3
      - id: tetris-m2
        name: Piece Falling & Controls
        description: Implement falling pieces and player controls.
        acceptance_criteria:
        - Current piece falls automatically at a rate determined by the level
        - Left and right arrow keys move piece one cell horizontally
        - Down arrow accelerates the piece fall speed for soft drop
        - Space bar performs hard drop placing piece instantly at lowest valid position
        - Cannot move piece outside board boundaries or into occupied cells
        pitfalls:
        - Negative Y during spawn
        - Piece moving into placed blocks
        - Hard drop going through floor
        concepts:
        - Position validation
        - Input handling
        - Collision detection
        skills:
        - Game loop implementation
        - Keyboard input processing
        - Boundary collision detection
        - Timer-based movement
        - State update patterns
        deliverables:
        - Piece spawning logic placing new tetrominoes at the top center of the board
        - Gravity timer that automatically drops the current piece at interval ticks
        - Left and right movement handlers shifting the piece horizontally on key press
        - Soft drop and hard drop controls for accelerated and instant piece placement
        estimated_hours: 3-4
      - id: tetris-m3
        name: Piece Rotation
        description: Implement piece rotation with wall kicks.
        acceptance_criteria:
        - Up arrow rotates the current piece clockwise by one state
        - Rotation works correctly for all seven piece types in all orientations
        - Cannot rotate into walls or pieces
        - 'Wall kick: try offset positions if direct rotation fails'
        - O piece does not rotate and remains in its single orientation
        pitfalls:
        - O piece rotation changing position
        - I piece wall kick needs special handling
        - Rotating into ceiling
        concepts:
        - Matrix rotation
        - Wall kicks
        - SRS (Super Rotation System)
        skills:
        - Matrix transformation operations
        - Complex collision handling
        - Algorithm implementation from specification
        - Edge case testing
        deliverables:
        - Rotation state machine cycling through four orientations per piece type
        - Wall kick test offsets attempted when rotation collides with boundaries
        - SRS (Super Rotation System) implementing the official rotation and kick tables
        - Collision checking verifying rotated piece fits within the board without overlap
        estimated_hours: 3-4
      - id: tetris-m4
        name: Line Clearing & Scoring
        description: Implement line clearing and scoring system.
        acceptance_criteria:
        - Filled rows are detected immediately after a piece locks into place
        - Filled rows are removed and visually cleared from the board
        - Rows above cleared lines fall down to fill the gap
        - Score based on lines cleared (1/2/3/4 = 100/300/500/800)
        - Level increases after every ten lines cleared
        - Speed increases with each level reducing the gravity timer interval
        pitfalls:
        - Iterating wrong direction when removing
        - Not re-checking row after splice
        - Tetris (4 lines) scoring wrong
        concepts:
        - Row clearing
        - Score systems
        - Difficulty progression
        skills:
        - Array manipulation and filtering
        - Cascading effects handling
        - Game balance and progression design
        - Scoring algorithm implementation
        deliverables:
        - Full line detection scanning each row for completely filled cells
        - Line removal logic clearing filled rows and shifting rows above downward
        - Scoring system awarding points based on number of lines cleared simultaneously
        - Level progression system increasing difficulty after a set number of lines
        estimated_hours: 3-4
    intermediate:
    - id: platformer
      name: Platformer
      description: Gravity, jumping
      difficulty: intermediate
      estimated_hours: 20-30
      essence: Frame-rate independent physics simulation with variable timestep integration, axis-aligned bounding box collision detection, and tile-based spatial partitioning for efficient level interaction.
      why_important: Building this teaches you fundamental game physics algorithms and real-time simulation techniques that apply to any interactive application requiring smooth movement and collision handling, from games to robotics simulations.
      learning_outcomes:
      - Implement gravity and velocity-based movement with delta time scaling
      - Design variable-height jump mechanics using input buffering
      - Build AABB collision detection with swept collision testing
      - Create tile-based spatial partitioning for efficient collision queries
      - Implement collision response with separation vectors and surface detection
      - Debug floating-point precision issues in physics calculations
      - Design state machines for character animation and movement states
      - Optimize collision detection using grid-based broad-phase culling
      skills:
      - 2D Physics Simulation
      - AABB Collision Detection
      - Tile-based Level Design
      - State Machine Architecture
      - Delta Time Integration
      - Spatial Partitioning
      - Input Buffering
      - Animation Systems
      tags:
      - c#
      - collision
      - framework
      - game-dev
      - intermediate
      - javascript
      - python
      - sprites
      - tiles
      architecture_doc: architecture-docs/platformer/index.md
      languages:
        recommended:
        - JavaScript
        - Python
        - C#
        also_possible:
        - C++
        - Lua
      resources:
      - name: 2D Platformer Tutorial
        url: https://www.youtube.com/results?search_query=2d+platformer+tutorial
        type: video
      - name: 2D Platformer Physics
        url: https://jobtalle.com/2d_platformer_physics.html
        type: tutorial
      - name: 2D Collision Detection - MDN Web Docs
        url: https://developer.mozilla.org/en-US/docs/Games/Techniques/2D_collision_detection
        type: documentation
      - name: Game Programming Patterns
        url: https://gameprogrammingpatterns.com/
        type: book
      prerequisites:
      - type: skill
        name: Basic game loop
      - type: skill
        name: 2D graphics
      - type: skill
        name: Basic physics
      milestones:
      - id: platformer-m1
        name: Basic Movement and Gravity
        description: Implement player movement with gravity.
        acceptance_criteria:
        - Move player left and right with arrow key input
        - Apply downward gravity acceleration each physics frame
        - Detect ground collision and stop player from falling through
        - Use velocity-based movement scaled by delta time
        pitfalls:
        - Gravity too strong or weak
        - Not using delta time
        - Ground clipping
        concepts:
        - Physics simulation
        - Velocity and acceleration
        - Delta time
        skills:
        - Implementing gravity simulation
        - Working with delta time for frame-independent movement
        - Applying velocity and acceleration to game objects
        - Handling player input for horizontal movement
        deliverables:
        - Player entity with position, velocity, and dimensions
        - Horizontal movement responding to left and right input
        - Gravity acceleration applied each frame using delta time
        - Terminal velocity cap preventing infinite falling speed
        estimated_hours: 3-4
      - id: platformer-m2
        name: Jumping
        description: Implement jump mechanics with variable height.
        acceptance_criteria:
        - Allow jump only when player is standing on ground
        - Support variable jump height by holding the jump button longer
        - Implement coyote time permitting jump within 100ms of leaving edge
        - Implement jump buffer accepting input within 100ms before landing
        pitfalls:
        - Double jump without intending
        - Jump feels floaty
        - Coyote time too generous
        concepts:
        - Jump mechanics
        - Coyote time
        - Input buffering
        skills:
        - Implementing responsive jump controls
        - Managing player state machines
        - Creating coyote time for better game feel
        - Implementing input buffering systems
        - Variable jump height based on button hold duration
        deliverables:
        - Jump initiation applying upward velocity impulse
        - Variable jump height based on button hold duration
        - Coyote time allowing jump shortly after leaving edge
        - Jump buffering registering input just before landing
        estimated_hours: 3-4
      - id: platformer-m3
        name: Tile-based Collision
        description: Implement collision with tile-based level.
        acceptance_criteria:
        - Load and render tile map from level data file
        - Detect collision between player and solid tiles accurately
        - Resolve X and Y collisions separately to handle corners
        - Optionally support slope tiles for angled surfaces
        pitfalls:
        - Corner clipping
        - Tunneling at high speeds
        - Off-by-one in tile lookup
        concepts:
        - AABB collision
        - Collision resolution
        - Tile maps
        skills:
        - Working with 2D tile-based collision systems
        - Implementing AABB collision detection
        - Resolving collision responses correctly
        - Converting world coordinates to tile coordinates
        - Handling edge cases in collision detection
        deliverables:
        - Tilemap data structure representing level geometry
        - AABB collision detection against solid tile boundaries
        - Separate X and Y axis collision resolution logic
        - One-way platform support allowing upward pass-through
        estimated_hours: 5-6
      - id: platformer-m4
        name: Enemies and Hazards
        description: Add enemies and death/respawn system.
        acceptance_criteria:
        - Create enemies with simple left-right patrol behavior
        - Kill player on contact with enemy from the side
        - Respawn player at last activated checkpoint on death
        - Optionally defeat enemies by stomping from above
        pitfalls:
        - Stomp detection window
        - Death during invincibility
        - Enemy stuck at patrol bounds
        concepts:
        - AI patrol behavior
        - Health systems
        - Collision types
        skills:
        - Implementing AI patrol patterns
        - Creating health and damage systems
        - Handling player death and respawn mechanics
        - Implementing invincibility frames
        - Detecting collision types for different interactions
        deliverables:
        - Enemy entity class with position and movement behavior
        - Basic patrol AI walking between defined waypoints
        - Player-enemy collision detection and response system
        - Death and respawn system returning player to checkpoint
        estimated_hours: 4-6
    - id: topdown-shooter
      name: Top-down Shooter
      description: Enemies, projectiles
      difficulty: intermediate
      estimated_hours: 20-30
      essence: Real-time collision detection between dynamic entities, AI behavior trees for enemy movement patterns, and delta-time-based physics simulation in a constrained 2D coordinate system.
      why_important: Building this teaches you core game programming fundamentals‚Äîspatial partitioning, frame-independent movement, and state machines‚Äîthat translate directly to professional game development and real-time interactive systems.
      learning_outcomes:
      - Implement AABB collision detection with spatial optimization
      - Design enemy AI using state machines and behavior patterns
      - Build projectile pooling system for efficient memory management
      - Implement delta-time physics for frame-independent movement
      - Create wave spawning system with difficulty progression
      - Debug real-time systems with visual feedback and logging
      - Optimize game loop performance with profiling tools
      - Handle input buffering and mouse-to-world coordinate transformation
      skills:
      - Collision Detection
      - Game AI & State Machines
      - Object Pooling
      - Delta-Time Physics
      - Spatial Partitioning
      - Game Loop Architecture
      - Vector Mathematics
      - Input Handling
      tags:
      - c#
      - collision
      - enemies
      - game-dev
      - intermediate
      - javascript
      - projectiles
      - python
      - sprites
      architecture_doc: architecture-docs/topdown-shooter/index.md
      languages:
        recommended:
        - JavaScript
        - Python
        - C#
        also_possible:
        - C++
        - Lua
      resources:
      - name: Game Programming Patterns
        url: https://gameprogrammingpatterns.com/
        type: book
      - name: Creating a Top-Down Shooter in Godot
        url: https://www.sharpcoderblog.com/blog/creating-a-top-down-shooter-game-in-godot
        type: tutorial
      - name: 2D Collision Detection - MDN Web Docs
        url: https://developer.mozilla.org/en-US/docs/Games/Techniques/2D_collision_detection
        type: documentation
      prerequisites:
      - type: skill
        name: Basic game loop
      - type: skill
        name: 2D graphics
      - type: skill
        name: Vector math
      milestones:
      - id: topdown-shooter-m1
        name: Player Movement & Aiming
        description: Implement player with 8-directional movement and mouse aiming.
        acceptance_criteria:
        - WASD movement supports eight directions including diagonal combinations
        - Mouse aiming rotates the player sprite to face the cursor in real time
        - Smooth movement with acceleration ramps up speed gradually on key hold
        - Screen bounds checking prevents the player from moving outside visible area
        pitfalls:
        - Diagonal speed boost
        - Angle in wrong units
        - Jittery movement
        concepts:
        - 8-directional movement
        - Mouse aiming
        - Vector normalization
        skills:
        - 2D Vector Mathematics
        - Input Handling & Event Systems
        - Game Loop Architecture
        - Player Controller Implementation
        deliverables:
        - WASD movement handler translating key presses into velocity vectors
        - Mouse aiming system rotating the player sprite toward the cursor position
        - Player sprite rotation rendering the character facing the aim direction
        - Movement speed configuration with acceleration and deceleration curves
        estimated_hours: 2-3
      - id: topdown-shooter-m2
        name: Shooting & Projectiles
        description: Implement projectile system with firing and collision.
        acceptance_criteria:
        - Click to fire a projectile from the player's current position
        - Projectile travels in the aimed direction at a constant speed
        - Fire rate limiting enforces a cooldown period between consecutive shots
        - Projectile-enemy collision deals damage and destroys the projectile on hit
        - Remove off-screen projectiles to free memory and maintain performance
        pitfalls:
        - Projectile spawn position
        - Fire rate timing
        - Memory from many projectiles
        concepts:
        - Projectile physics
        - Fire rate
        - Object pooling
        skills:
        - Object Pooling & Memory Management
        - Collision Detection Systems
        - Time-based Systems & Deltatime
        - Sprite Rendering & Transformations
        deliverables:
        - Projectile spawning system creating bullets at the player's weapon position
        - Projectile movement controller updating bullet positions each frame along their trajectory
        - Projectile collision detection checking hits against enemy bounding boxes
        - Fire rate limiter enforcing minimum cooldown between consecutive shots
        estimated_hours: 3-4
      - id: topdown-shooter-m3
        name: Enemies & AI
        description: Add enemies with simple AI behaviors.
        acceptance_criteria:
        - Enemy chases the player using direct line-of-sight movement each frame
        - Different enemy types have varying speed, health, and damage attributes
        - Health and damage system correctly reduces HP on hit and triggers death at zero
        - Death removes the enemy and spawning places new enemies at wave intervals
        pitfalls:
        - Division by zero in normalize
        - Enemy stacking
        - Instant kill on spawn
        concepts:
        - Enemy AI
        - Health systems
        - Enemy types
        skills:
        - Pathfinding & Navigation
        - State Machine Implementation
        - Entity Component Systems
        - Game Balance & Difficulty Tuning
        deliverables:
        - Enemy spawning system placing new enemies at random positions outside the screen
        - Chase behavior AI moving enemies toward the player's current position each frame
        - Enemy health system tracking hit points and triggering death on zero HP
        - Enemy-player collision handler dealing contact damage when enemies reach the player
        estimated_hours: 4-5
      - id: topdown-shooter-m4
        name: Waves & Scoring
        description: Implement wave system and scoring.
        acceptance_criteria:
        - Wave-based spawning releases enemies in defined groups with rest periods between
        - Difficulty increases each wave with more and tougher enemies
        - Score increments for each enemy kill based on enemy type value
        - High score tracking persists the best score across game sessions
        - Wave clear bonus awards extra points when all enemies in a wave are defeated
        pitfalls:
        - Infinite spawn loop
        - No break between waves
        - Unfair difficulty spike
        concepts:
        - Wave systems
        - Difficulty scaling
        - Score systems
        skills:
        - Game State Management
        - Progression Systems Design
        - UI Implementation & HUD
        - Performance Optimization
        deliverables:
        - Wave system spawning groups of enemies with increasing count per wave
        - Score tracking system awarding points for each enemy kill and wave completion
        - Difficulty scaling increasing enemy count, speed, and health each wave
        - Game over condition triggered when the player's health reaches zero
        estimated_hours: 3-4
    advanced:
    - id: software-3d
      name: Software 3D Renderer
      description: No GPU, pure math
      difficulty: advanced
      estimated_hours: 30-50
      essence: Scanline-based rasterization of geometric primitives through matrix-driven coordinate transformations, z-buffer occlusion testing, and perspective-correct attribute interpolation‚Äîimplementing the core algorithms that underlie GPU rendering pipelines entirely in software.
      why_important: Building this demystifies the mathematics and algorithms underlying modern GPU pipelines, teaching fundamental 3D graphics concepts that apply to game engines, visualization tools, and graphics API debugging.
      learning_outcomes:
      - Implement Bresenham's line algorithm for pixel-perfect rasterization
      - Design and apply model-view-projection matrix transformations
      - Build scanline or barycentric coordinate-based triangle rasterization
      - Implement z-buffer algorithm for depth-based visibility culling
      - Develop perspective-correct attribute interpolation across polygons
      - Debug floating-point precision issues in homogeneous coordinates
      - Optimize rendering pipelines through spatial data structures
      - Implement Phong or Gouraud shading with normal interpolation
      skills:
      - Linear Algebra
      - Matrix Transformations
      - Rasterization Algorithms
      - Depth Buffering
      - Homogeneous Coordinates
      - Geometric Primitives
      - Memory Management
      - Fixed-Point Math
      tags:
      - 3d-graphics
      - advanced
      - c
      - c++
      - matrices
      - rasterization
      - rendering
      - rust
      architecture_doc: architecture-docs/software-3d/index.md
      languages:
        recommended:
        - C
        - C++
        - Rust
        also_possible:
        - Python
        - JavaScript
      resources:
      - name: tinyrenderer
        url: https://github.com/ssloy/tinyrenderer/wiki
        type: tutorial
      - name: 3D Math Primer
        url: https://gamemath.com/
        type: book
      prerequisites:
      - type: skill
        name: Linear algebra (matrices, vectors)
      - type: skill
        name: Basic trigonometry
      - type: skill
        name: 2D graphics basics
      milestones:
      - id: software-3d-m1
        name: Line Drawing
        description: Implement Bresenham's line algorithm.
        acceptance_criteria:
        - Draw lines between any two points
        - Handle all octants correctly including horizontal and vertical lines
        - Optimize for integer-only math with no floating-point operations
        - Draw pixels to a memory buffer that can be displayed on screen
        pitfalls:
        - Integer overflow
        - Division by zero
        - Missing octants
        concepts:
        - Rasterization
        - Bresenham's algorithm
        - Pixel buffers
        skills:
        - Integer arithmetic and bitwise operations
        - Pixel coordinate systems and screen space
        - Algorithmic optimization and loop invariants
        deliverables:
        - Bresenham's line algorithm implementation supporting all slope octants
        - Cohen-Sutherland line clipping against viewport rectangle boundaries
        - Anti-aliased line drawing using Xiaolin Wu's algorithm for smooth edges
        - Framebuffer write interface that sets pixel color at x,y coordinates
        estimated_hours: 3-4
      - id: software-3d-m2
        name: Triangle Rasterization
        description: Fill triangles using scanline or barycentric methods.
        acceptance_criteria:
        - Fill solid-colored triangles with correct pixel coverage
        - Handle edge cases including flat-top, flat-bottom, and degenerate triangles
        - Implement barycentric coordinates for smooth attribute interpolation across triangles
        - Anti-aliasing smooths triangle edges using sub-pixel sampling (optional)
        pitfalls:
        - Winding order
        - Gaps between triangles
        - Subpixel precision
        concepts:
        - Barycentric coordinates
        - Rasterization
        - Fill rules
        skills:
        - 2D geometry and edge equations
        - Scanline algorithms and interpolation
        - Memory-efficient buffer access patterns
        deliverables:
        - Edge function evaluator determining if a point is inside a triangle
        - Scanline rasterization filling triangles row by row between edges
        - Barycentric coordinate interpolation for vertex attribute blending
        - Texture coordinate interpolation mapping UV values across triangle surfaces
        estimated_hours: 5-7
      - id: software-3d-m3
        name: 3D Transformations
        description: Implement model, view, and projection matrices.
        acceptance_criteria:
        - 4x4 matrix multiplication produces correct composite transformation results
        - Translation, rotation, and scaling transforms apply correctly to vertex positions
        - Look-at camera matrix orients the view toward a target point from eye position
        - Perspective projection produces correct foreshortening with near and far planes
        pitfalls:
        - Matrix multiplication order
        - Homogeneous coordinates
        - Y-axis flip
        concepts:
        - Linear transformations
        - Projection
        - Camera space
        skills:
        - Matrix mathematics and linear algebra
        - Coordinate system transformations
        - Perspective projection and viewport mapping
        deliverables:
        - Model matrix supporting translation, rotation, and scaling of objects
        - View matrix implementing camera positioning with look-at computation
        - Projection matrix implementing perspective and orthographic projections
        - Vertex transformation pipeline applying model-view-projection to each vertex
        estimated_hours: 6-8
      - id: software-3d-m4
        name: Depth Buffer & Lighting
        description: Add z-buffering and basic shading.
        acceptance_criteria:
        - Z-buffer correctly resolves hidden surfaces in overlapping geometry
        - Flat shading computes lighting from surface normals for each triangle face
        - Gouraud shading interpolates per-vertex lighting across triangle for smooth gradients
        - Basic diffuse lighting computes intensity from dot product of normal and light direction
        pitfalls:
        - Z-fighting
        - Normal direction
        - Interpolation artifacts
        concepts:
        - Z-buffering
        - Surface normals
        - Lighting models
        skills:
        - Per-pixel depth comparison and sorting
        - Vector math for lighting calculations
        - Attribute interpolation across triangles
        - Fragment shading and color computation
        deliverables:
        - Z-buffer implementation storing per-pixel depth values for occlusion testing
        - Depth testing logic rejecting fragments behind already-rendered surfaces
        - Flat shading computing one normal per triangle for uniform face lighting
        - Gouraud shading interpolating vertex colors across the triangle surface
        estimated_hours: 8-12
    - id: ecs-arch
      name: ECS Architecture
      description: Entity-component-system
      difficulty: advanced
      estimated_hours: 20-35
      essence: Contiguous component array storage with cache-aligned memory layouts and archetype-based entity grouping that enables SIMD-friendly iteration patterns while maintaining O(1) component access through sparse sets or entity-to-archetype mapping tables.
      why_important: ECS is the dominant architecture in modern game engines and high-performance simulations, teaching data-oriented design principles that apply to any performance-critical system beyond just games.
      learning_outcomes:
      - Implement contiguous memory storage for components using structure-of-arrays layout
      - Design sparse set or archetype-based entity lookup with O(1) component access
      - Build a type-safe system scheduler that iterates only over entities with required components
      - Optimize memory layout for CPU cache line alignment and prefetching
      - Debug performance bottlenecks using cache miss analysis and memory profilers
      - Implement archetype transitions when components are added or removed from entities
      - Design query interfaces that filter entities by component presence or absence
      - Measure and compare performance between object-oriented and data-oriented approaches
      skills:
      - Data-Oriented Design
      - Cache Optimization
      - Memory Layout Patterns
      - System Architecture
      - Performance Profiling
      - SIMD Vectorization
      - Type System Design
      - Sparse Data Structures
      tags:
      - advanced
      - c
      - c++
      - components
      - entities
      - frontend
      - game-dev
      - rust
      - systems
      architecture_doc: architecture-docs/ecs-arch/index.md
      languages:
        recommended:
        - C++
        - Rust
        - C
        also_possible:
        - C#
        - Go
      resources:
      - name: ECS FAQ
        url: https://github.com/SanderMertens/ecs-faq
        type: article
      - name: Overwatch GDC Talk
        url: https://www.youtube.com/watch?v=W3aieHjyNvw
        type: video
      prerequisites:
      - type: skill
        name: Game programming basics
      - type: skill
        name: Data structures
      - type: skill
        name: Performance concepts
      milestones:
      - id: ecs-arch-m1
        name: Entity Manager
        description: Create entity ID management system.
        acceptance_criteria:
        - Generate unique entity IDs that are never duplicated during runtime
        - Track alive and dead entities with efficient status lookup support
        - Recycle deleted entity IDs for memory-efficient long-running systems
        - Generation counter prevents stale entity ID references from resolving incorrectly
        pitfalls:
        - Stale entity references
        - Index overflow
        - Not recycling IDs
        concepts:
        - Entity IDs
        - Generation counters
        - ID recycling
        skills:
        - Memory management with generation counters
        - ID pool and free list implementation
        - Handle invalidation detection
        - Bitset or sparse set manipulation
        deliverables:
        - Entity ID generation with unique identifier assignment
        - Entity creation and destruction with lifecycle tracking
        - Entity lookup by ID with constant-time access
        - Entity iteration across all alive entities efficiently
        estimated_hours: 3-4
      - id: ecs-arch-m2
        name: Component Storage
        description: Implement cache-friendly component storage.
        acceptance_criteria:
        - Contiguous array storage enables cache-friendly iteration over component data
        - Sparse set maps entity IDs to component indices for constant-time lookup
        - Add and remove components dynamically without affecting other entity components
        - Type-safe component access prevents runtime errors from mismatched types
        pitfalls:
        - Swap-remove ordering
        - Sparse array growth
        - Invalid index sentinel
        concepts:
        - Sparse sets
        - Data-oriented design
        - Cache efficiency
        skills:
        - Cache-aware data structure design
        - Contiguous memory layout optimization
        - Sparse set implementation
        - Component packing and alignment
        - SIMD-friendly data organization
        deliverables:
        - Component type registry mapping types to storage arrays
        - Sparse set storage for efficient entity-to-component mapping
        - Component add and remove operations with dynamic attachment
        - Component lookup by entity ID with type-safe access interface
        estimated_hours: 5-7
      - id: ecs-arch-m3
        name: System Interface
        description: Create system execution framework.
        acceptance_criteria:
        - Systems operate on entities matching their required component set each frame
        - Query entities having specific component combinations with efficient iteration
        - System registration and ordering ensures correct execution sequence per frame
        - Delta time passing provides frame time to systems for time-based updates
        pitfalls:
        - Component iteration invalidation
        - System ordering dependencies
        - Thread safety
        concepts:
        - System execution
        - Component queries
        - Iteration patterns
        skills:
        - System scheduling and dependency resolution
        - Component query pattern matching
        - Iterator invalidation prevention
        - Read/write access tracking
        - Lock-free or mutex-based synchronization
        deliverables:
        - System base class defining the update and query interface
        - Component queries selecting entities matching required component sets
        - System execution order with configurable priority scheduling
        - System dependency declaration for correct ordering guarantees
        estimated_hours: 5-7
      - id: ecs-arch-m4
        name: Archetypes (Optional Advanced)
        description: Implement archetype-based storage for better performance.
        acceptance_criteria:
        - Group entities by their component combination into shared archetype tables
        - Move entities between archetypes when components are added or removed
        - Archetype graph enables fast lookup of matching archetypes for queries
        - Chunk-based storage within archetypes enables cache-efficient linear iteration
        pitfalls:
        - Archetype explosion
        - Move overhead
        - Complex implementation
        concepts:
        - Archetypes
        - Data locality
        - Component masks
        skills:
        - Graph-based archetype transitions
        - Memory pool allocation strategies
        - Bitmasking for component sets
        - Structural change batching
        - Archetype edge caching
        deliverables:
        - Archetype identification grouping entities by component combination
        - Archetype-based storage co-locating components of same archetype
        - Archetype transitions moving entities between groups on component changes
        - Cache-friendly iteration over archetype storage for performance
        estimated_hours: 6-10
    expert:
    - id: build-game-engine
      name: Build Your Own Game Engine
      description: Full 2D/3D engine
      difficulty: expert
      estimated_hours: 100-200
      essence: Real-time rendering pipeline orchestration with GPU command buffer submission, cache-coherent memory layouts for component iteration at 60+ FPS, and continuous collision detection with impulse-based constraint solving across dynamic rigid bodies.
      why_important: Building a game engine exposes you to systems programming fundamentals that underpin modern game development and real-time applications, from graphics pipeline management to cache-friendly data layout patterns used in AAA studios.
      learning_outcomes:
      - Implement window creation and OpenGL/Vulkan rendering pipeline with shader compilation
      - Design cache-coherent Entity Component System architecture with component arrays
      - Build spatial partitioning structures for broadphase collision detection
      - Implement impulse-based physics with constraint resolution and integration
      - Develop asset loading pipeline with custom binary formats and memory management
      - Create scene graph hierarchy with transform propagation and culling
      - Debug graphics API validation layers and memory allocators
      - Optimize frame timing and multithreaded rendering submission
      skills:
      - Graphics API Integration
      - Memory Layout Optimization
      - Spatial Partitioning
      - Physics Simulation
      - Component-Based Architecture
      - Asset Pipeline Design
      - Performance Profiling
      - Systems Programming
      tags:
      - build-from-scratch
      - c
      - c++
      - ecs
      - expert
      - framework
      - game-dev
      - game-loop
      - physics
      - rendering
      - rust
      architecture_doc: architecture-docs/build-game-engine/index.md
      languages:
        recommended:
        - C++
        - Rust
        - C
        also_possible:
        - Zig
      resources:
      - type: book
        name: Game Engine Architecture
        url: https://www.gameenginebook.com/
      - type: video
        name: Handmade Hero
        url: https://handmadehero.org/
      - type: tutorial
        name: Learn OpenGL
        url: https://learnopengl.com/
      prerequisites:
      - type: skill
        name: Graphics programming basics
      - type: skill
        name: Linear algebra
      - type: skill
        name: C/C++ or Rust
      - type: skill
        name: Game architecture patterns
      milestones:
      - id: build-game-engine-m1
        name: Window & Rendering Foundation
        description: Create window and basic rendering pipeline.
        acceptance_criteria:
        - Window is created with the requested resolution and title, and processes input events from the OS
        - OpenGL or Vulkan context is initialized and can clear the screen to a solid color each frame
        - Basic 2D sprite rendering draws textured rectangles at arbitrary positions, scales, and rotations
        - Texture loading reads PNG or JPG images from disk and uploads them to GPU texture objects
        - Shader system compiles vertex and fragment shaders from source and links them into a usable program
        pitfalls:
        - OpenGL state leaks
        - Shader compilation errors not checked
        - Wrong vertex attribute setup
        concepts:
        - Graphics APIs
        - Shader programming
        - Batch rendering
        skills:
        - OpenGL/Vulkan/DirectX API usage
        - GLSL shader development
        - Window management and event handling
        - Vertex buffer and index buffer management
        deliverables:
        - Window creation and event loop using SDL2 or GLFW with configurable resolution and title
        - OpenGL or Vulkan graphics context setup with swap chain and basic render pipeline initialization
        - Game render loop with delta time calculation for frame-rate-independent updates and rendering
        - Basic sprite and mesh rendering pipeline that draws textured quads or 3D meshes to the screen
        estimated_hours: 20-30
      - id: build-game-engine-m2
        name: Entity Component System
        description: Implement ECS architecture for game objects.
        acceptance_criteria:
        - Entity creation returns a unique ID and entity destruction recycles the ID for future reuse
        - Component storage supports adding, removing, and retrieving components by entity ID in amortized O(1) time
        - System execution iterates over all entities with the required components and calls the system's update function
        - Component queries efficiently select entities matching a multi-component filter without scanning all entities
        - Entity destruction removes all associated components and marks the entity ID as available for recycling
        pitfalls:
        - Component iteration invalidation
        - Memory fragmentation
        - System ordering dependencies
        concepts:
        - ECS architecture
        - Data-oriented design
        - Cache efficiency
        skills:
        - Component-based architecture design
        - Memory-efficient data structures
        - Cache-aware programming
        - System design and decoupling
        deliverables:
        - Entity ID manager that creates, tracks, and recycles integer entity identifiers with generation counters
        - Component storage using dense arrays (struct-of-arrays) for cache-friendly iteration over component data
        - System execution framework that iterates over entities matching a component signature and applies logic
        - Entity query API that selects entities possessing a specific combination of component types for processing
        estimated_hours: 20-30
      - id: build-game-engine-m3
        name: Physics & Collision
        description: Implement 2D physics and collision detection.
        acceptance_criteria:
        - Rigid body dynamics correctly apply gravity, velocity, and acceleration to update entity positions each frame
        - Collision detection identifies overlapping AABB and circle collider pairs in the broad phase and narrow phase
        - Collision response applies impulse forces and position corrections to separate overlapping bodies realistically
        - Spatial partitioning reduces collision pair checks from O(n^2) to O(n log n) or better for large entity counts
        - Physics timestep uses a fixed delta time with accumulator to ensure deterministic simulation independent of frame rate
        pitfalls:
        - Variable timestep physics
        - Tunneling (fast objects passing through)
        - Collision jitter
        concepts:
        - Physics simulation
        - Collision detection
        - Impulse resolution
        skills:
        - Numerical integration methods
        - Spatial partitioning algorithms
        - Vector mathematics and geometry
        - Fixed timestep implementation
        deliverables:
        - AABB (axis-aligned bounding box) collision detection that tests overlap between rectangular bounds
        - Spatial partitioning structure (grid or quadtree) that reduces collision checks to nearby entity pairs
        - Rigid body physics integration using semi-implicit Euler for velocity and position updates each frame
        - Collision response and resolution that applies impulses and separates overlapping bodies after detection
        estimated_hours: 25-40
      - id: build-game-engine-m4
        name: Resource & Scene Management
        description: Implement asset loading and scene system.
        acceptance_criteria:
        - Asset loader reads textures, audio clips, and data files from disk and returns typed resource handles
        - Resource cache returns the same handle for duplicate load requests and tracks reference counts for cleanup
        - Scene serialization writes the current entity-component state to a file that can be loaded to restore the scene
        - Scene transitions unload the current scene's resources and load the new scene's assets without memory leaks
        - Game loop runs update and render phases in sequence at a stable frame rate with delta-time propagation
        pitfalls:
        - Resource leaks
        - Scene state corruption during transition
        - Serialization version incompatibility
        concepts:
        - Resource management
        - Scene graphs
        - Game architecture
        skills:
        - Asset pipeline development
        - Memory management and pooling
        - Serialization and deserialization
        - State machine implementation
        deliverables:
        - Asset loading system that reads textures, models, audio files, and data from disk into memory
        - Resource caching with reference counting that shares loaded assets and frees them when no longer referenced
        - Scene graph or level structure that organizes entities into hierarchical groups with transform inheritance
        - Scene serialization and deserialization that saves and loads scene state to and from JSON or binary files
        estimated_hours: 20-30
    - id: build-raytracer
      name: Build Your Own Ray Tracer
      description: Path tracing renderer
      difficulty: advanced
      estimated_hours: 20-40
      essence: Simulating photon trajectories through recursive ray casting and intersection testing, computing surface radiance by stochastic sampling of reflection equations to approximate the integral of incoming light across hemisphere directions.
      why_important: Building a ray tracer teaches fundamental computer graphics algorithms (vector math, geometric intersection tests, physically-based light transport) that underpin modern rendering engines, game development, and visual effects pipelines, while developing skills in numerical methods and performance optimization of computationally intensive algorithms.
      learning_outcomes:
      - Implement ray-primitive intersection tests using quadratic equation solving for spheres
      - Design a recursive ray tracing engine with material abstraction and hit record management
      - Implement Monte Carlo sampling for antialiasing and diffuse light scattering
      - Build physically-based material models including Lambertian reflection, Fresnel equations, and Schlick's approximation
      - Debug numerical precision issues in floating-point ray-geometry calculations
      - Optimize rendering performance through bounding volume hierarchies and early ray termination
      - Implement camera models with adjustable field of view, focus distance, and aperture for depth of field
      - Apply gamma correction and tone mapping for accurate image output
      skills:
      - Vector Mathematics
      - Ray-Geometry Intersection
      - Monte Carlo Integration
      - Physically-Based Rendering
      - Recursive Algorithms
      - Performance Optimization
      - Linear Algebra
      - Image Format Handling
      tags:
      - advanced
      - build-from-scratch
      - c++
      - game-dev
      - go
      - lighting
      - ray-intersection
      - rendering
      - rust
      - shading
      architecture_doc: architecture-docs/build-raytracer/index.md
      languages:
        recommended:
        - C++
        - Rust
        - Go
        also_possible:
        - Python
        - JavaScript
      resources:
      - name: Ray Tracing in One Weekend
        url: https://raytracing.github.io/books/RayTracingInOneWeekend.html
        type: book
      - name: 'Ray Tracing: The Next Week'
        url: https://raytracing.github.io/books/RayTracingTheNextWeek.html
        type: book
      - name: Physically Based Rendering (PBRT)
        url: https://pbr-book.org/
        type: reference
      prerequisites:
      - type: skill
        name: Linear algebra (vectors, matrices)
      - type: skill
        name: Basic geometry
      - type: skill
        name: Some physics (light, optics)
      milestones:
      - id: build-raytracer-m1
        name: Output an Image
        description: Generate a simple PPM image with gradient colors.
        acceptance_criteria:
        - PPM writer outputs valid P3 format file openable by image viewers
        - Color class stores red, green, blue components as floating point values
        - Image buffer correctly maps x,y coordinates to linear pixel array
        - Gradient image shows smooth color transition from top to bottom
        pitfalls:
        - RGB values must be clamped to 0-255 range
        - Forgetting newline between pixel values in PPM
        - Integer overflow when color values exceed 1.0
        - 'Y-axis direction: most formats have origin at top-left, not bottom-left'
        concepts:
        - Image file formats
        - Color representation
        - Coordinate systems
        skills:
        - File I/O operations
        - RGB color manipulation
        - 2D coordinate system implementation
        - Basic graphics pipeline
        deliverables:
        - PPM image file writer outputting RGB pixel data
        - Color class representing RGB values with clamping support
        - Image buffer storing pixel data for width by height grid
        - Gradient rendering demonstrating color interpolation across image
        estimated_hours: '1'
      - id: build-raytracer-m2
        name: Ray Class and Background
        description: Define ray class and render background gradient.
        acceptance_criteria:
        - Ray evaluates point at parameter t as origin plus t times direction
        - Camera generates rays from eye through each viewport pixel position
        - Background returns blue-to-white gradient based on ray y-direction component
        - Vector operations produce mathematically correct results for unit tests
        pitfalls:
        - Rays with zero-length direction vectors cause NaN
        - Forgetting to normalize direction vector affects calculations
        - Using int instead of float for coordinates loses precision
        - Background color calculation sensitive to ray direction normalization
        concepts:
        - Ray representation
        - Linear interpolation (lerp)
        - Camera model basics
        skills:
        - Vector mathematics
        - 3D coordinate transformations
        - Parametric ray equations
        - Gradient interpolation
        deliverables:
        - Ray class storing origin point and direction vector
        - Camera model defining viewport and ray generation per pixel
        - Background color function interpolating based on ray direction
        - Vector utility functions for dot product, cross product, and normalization
        estimated_hours: 1-2
      - id: build-raytracer-m3
        name: Sphere Intersection
        description: Add a sphere and test ray-sphere intersection.
        acceptance_criteria:
        - Ray-sphere intersection correctly solves quadratic equation for hit points
        - Hit record contains intersection point, surface normal, and parameter t
        - Negative t values are rejected to avoid intersections behind camera
        - Rendered sphere appears as colored circle centered in the output image
        pitfalls:
        - Choosing correct root (closest positive)
        - Numeric precision issues
        - Sphere behind camera
        concepts:
        - Ray-sphere intersection
        - Quadratic formula
        - Hit detection
        skills:
        - Solving quadratic equations programmatically
        - Geometric intersection algorithms
        - Root selection logic
        - Handling edge cases in collision detection
        deliverables:
        - Sphere geometry class with center position and radius
        - Ray-sphere intersection test using quadratic discriminant formula
        - Hit record storing intersection point, normal, and distance parameter
        - Scene rendering showing red sphere against gradient background
        estimated_hours: 1-2
      - id: build-raytracer-m4
        name: Surface Normals and Multiple Objects
        description: Compute normals and support multiple objects with closest hit.
        acceptance_criteria:
        - Surface normals point outward and are unit length at intersection points
        - Normal-to-color mapping produces visually distinct shading across sphere surface
        - Hittable list iterates all objects and selects closest valid intersection
        - Scene with ground plane and sphere renders both objects with correct occlusion
        pitfalls:
        - Normal must point outward from surface (flip if inside)
        - t_min should be small positive (0.001) not zero to avoid self-intersection
        - 'Floating point precision: comparing t == 0 fails'
        - Forgetting to return closest hit, not first hit
        concepts:
        - Surface normals
        - Object-oriented design
        - Closest hit algorithm
        skills:
        - Vector normalization
        - Polymorphic object design
        - Spatial data structure traversal
        - Normal vector calculation
        deliverables:
        - Surface normal computation at sphere intersection points
        - Normal-based coloring mapping normal direction to RGB values
        - Hittable list managing collection of scene objects
        - Closest hit selection returning nearest intersection from all objects
        estimated_hours: 2-3
      - id: build-raytracer-m5
        name: Antialiasing
        description: Add antialiasing by shooting multiple rays per pixel.
        acceptance_criteria:
        - Multiple samples per pixel are cast with random sub-pixel offsets
        - Averaged samples produce smoother edges compared to single-sample rendering
        - Sample count is configurable and higher counts produce visibly smoother results
        - Random jitter is uniformly distributed within each pixel boundary
        pitfalls:
        - Not using random offsets within pixel causes visible patterns
        - Too few samples causes noisy images
        - Forgetting to average samples produces overly bright images
        - Random number generator state affects reproducibility
        concepts:
        - Antialiasing
        - Monte Carlo sampling
        - Random number generation
        skills:
        - Monte Carlo integration techniques
        - Random sampling strategies
        - Statistical averaging
        - Noise reduction algorithms
        deliverables:
        - Multi-sample per pixel casting multiple rays with random jitter
        - Random number generator producing uniform samples in unit square
        - Sample averaging combining multiple ray colors into final pixel color
        - Configurable samples-per-pixel controlling quality versus render time
        estimated_hours: 1-2
      - id: build-raytracer-m6
        name: Diffuse Materials
        description: Implement Lambertian (diffuse) material.
        acceptance_criteria:
        - Diffuse surfaces scatter rays randomly with cosine-weighted distribution
        - Recursive tracing terminates at configurable maximum bounce depth limit
        - Shadow regions appear naturally where bounced rays receive less light
        - Gamma correction with power 1/2.2 produces visually correct brightness levels
        pitfalls:
        - Infinite recursion without depth limit
        - Shadow acne (self-intersection)
        - Correct hemisphere sampling
        concepts:
        - Lambertian reflection
        - Recursive ray tracing
        - Material scattering
        skills:
        - Recursive algorithm implementation
        - Ray bounce calculations
        - Hemisphere sampling methods
        - Termination condition design
        deliverables:
        - Lambertian material scattering rays in random hemisphere directions
        - Random point generation on unit sphere for diffuse bounce direction
        - Recursive ray tracing following scattered rays up to depth limit
        - Gamma correction transforming linear color space to display color space
        estimated_hours: 2-3
      - id: build-raytracer-m7
        name: Metal and Reflections
        description: Implement metallic (reflective) materials with fuzz.
        acceptance_criteria:
        - Metal surfaces produce mirror-like reflections of surrounding scene objects
        - Reflection formula computes v - 2*dot(v,n)*n for incident vector v and normal n
        - Fuzz parameter adds random perturbation making reflections appear rougher
        - Different materials can be assigned to different objects in the same scene
        pitfalls:
        - Reflected ray pointing into surface (dot product check)
        - Infinite recursion without max depth limit
        - Fuzz parameter > 1 causes rays to pass through surface
        - Not attenuating color with each bounce produces unrealistic brightness
        concepts:
        - Specular reflection
        - Roughness/fuzz
        - Material system
        skills:
        - Reflection vector computation
        - Material property encoding
        - Ray attenuation handling
        - Perturbed vector generation
        deliverables:
        - Metal material reflecting rays about surface normal vector
        - Reflection vector computation using incident ray and normal
        - Fuzz parameter controlling roughness of metallic reflections
        - Material assignment system associating materials with scene objects
        estimated_hours: 2-3
      - id: build-raytracer-m8
        name: Dielectrics (Glass)
        description: Implement glass with refraction and Fresnel effects.
        acceptance_criteria:
        - Glass spheres both refract and reflect light based on angle of incidence
        - Snell's law correctly computes refracted direction given refractive index ratio
        - Schlick approximation produces higher reflectance at shallow viewing angles
        - Total internal reflection occurs when light exits dense medium at steep angles
        pitfalls:
        - Getting the IOR ratio right
        - Hollow glass spheres (negative radius trick)
        - Total internal reflection check
        concepts:
        - Refraction (Snell's law)
        - Total internal reflection
        - Fresnel equations
        skills:
        - Refraction physics implementation
        - Index of refraction calculations
        - Conditional ray behavior
        - Probabilistic ray splitting
        deliverables:
        - Dielectric material implementing both refraction and reflection
        - Snell's law computation determining refracted ray direction
        - Schlick approximation estimating reflectance at grazing angles
        - Total internal reflection handling when refraction is impossible
        estimated_hours: 3-4
      - id: build-raytracer-m9
        name: Positionable Camera
        description: Implement camera with position, look-at, and field of view.
        acceptance_criteria:
        - Camera can be positioned at arbitrary location pointing at specified target
        - Field of view parameter visibly changes the zoom level of rendered scene
        - Camera up vector determines roll orientation of the rendered view
        - Aspect ratio matches image width-to-height ratio without distortion
        pitfalls:
        - Field of view in radians vs degrees confusion
        - Up vector parallel to look direction crashes
        - Aspect ratio calculation wrong leads to stretched images
        - Viewport height/width off by one pixel
        concepts:
        - Camera coordinate system
        - Field of view
        - Look-at transformation
        skills:
        - Camera transformation matrices
        - Trigonometric angle conversions
        - View frustum construction
        - Orthonormal basis calculation
        deliverables:
        - Camera positioning with lookfrom and lookat point parameters
        - Field of view parameter controlling viewport angular width
        - Camera orientation basis vectors computed from view direction and up vector
        - Aspect ratio configuration matching output image dimensions
        estimated_hours: 2-3
      - id: build-raytracer-m10
        name: Depth of Field
        description: Add defocus blur (depth of field) effect.
        acceptance_criteria:
        - Objects at focus distance appear sharp while others appear blurred
        - Larger aperture produces more pronounced depth-of-field blur effect
        - Ray origins are randomly distributed within circular lens aperture disk
        - Final scene demonstrates multiple materials, camera positioning, and defocus blur
        pitfalls:
        - Aperture too large causes everything to be blurry
        - Focus distance wrong makes subject blurry
        - Random disk sampling bias causes visible artifacts
        - Thin lens approximation breaks at extreme apertures
        concepts:
        - Thin lens model
        - Depth of field
        - Aperture and focus
        skills:
        - Lens simulation techniques
        - Random disk point generation
        - Focal plane geometry
        - Depth-based blurring effects
        deliverables:
        - Thin lens model with configurable aperture size parameter
        - Focus distance parameter controlling plane of sharp focus
        - Random ray origin offset within lens disk for defocus blur
        - Final scene rendering combining all materials and camera effects
        estimated_hours: 2-3
- id: compilers
  name: Languages & Compilers
  icon: üìù
  subdomains:
  - name: Parsing & Lexing
  - name: Interpreters
  - name: Compilers
  - name: Runtime Systems
  projects:
    beginner:
    - id: calculator-parser
      name: Calculator Parser
      description: Arithmetic expressions
      difficulty: beginner
      estimated_hours: 6-10
      essence: Tokenization of character streams into structured tokens, transformation of infix notation into abstract syntax trees through recursive descent parsing or the Shunting Yard algorithm, and evaluation via postorder tree traversal while correctly handling operator precedence and associativity rules.
      why_important: Building this teaches you fundamental compiler construction techniques used in every programming language, interpreter, and query engine, while mastering data structures like trees and stacks that appear throughout software engineering.
      learning_outcomes:
      - Implement tokenization to convert character streams into structured tokens
      - Build recursive descent parsers that transform grammar rules into executable code
      - Design operator precedence systems using precedence climbing or Pratt parsing
      - Construct and traverse abstract syntax trees using postorder traversal
      - Implement the Shunting Yard algorithm to convert infix to postfix notation
      - Handle unary operators and operator associativity rules correctly
      - Debug ambiguous grammars and resolve shift-reduce conflicts
      - Extend parsers with new operators and functions through modular design
      skills:
      - Recursive Descent Parsing
      - Operator Precedence
      - Abstract Syntax Trees
      - Tokenization
      - Context-Free Grammars
      - Stack-Based Evaluation
      - Expression Evaluation
      - Parser Design Patterns
      tags:
      - beginner-friendly
      - c
      - compilers
      - evaluation
      - expression
      - javascript
      - parsing
      - precedence
      - python
      architecture_doc: architecture-docs/calculator-parser/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - C
        also_possible:
        - Go
        - Rust
      resources:
      - name: Recursive Descent Parsing
        url: https://craftinginterpreters.com/parsing-expressions.html
        type: book
      - name: Pratt Parsing
        url: https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html
        type: article
      prerequisites:
      - type: skill
        name: Basic programming
      - type: skill
        name: Understanding of precedence
      milestones:
      - id: calculator-parser-m1
        name: Basic Arithmetic
        description: Evaluate simple arithmetic expressions.
        acceptance_criteria:
        - Parser reads integer and floating-point numbers including negative values
        - Addition and subtraction evaluate left-to-right at lowest precedence level
        - Multiplication and division bind tighter than addition and subtraction
        - Parenthesized expressions are evaluated first regardless of surrounding operators
        pitfalls:
        - Left vs right associativity
        - Division by zero
        - Whitespace handling
        concepts:
        - Recursive descent
        - Operator precedence
        - Expression trees
        skills:
        - String parsing and tokenization
        - Implementing basic arithmetic operations
        - Handling operator precedence rules
        - Building recursive parsers
        deliverables:
        - Number parser reading integer and decimal numeric literals from input
        - Addition and subtraction operators handling left-associative expressions
        - Multiplication and division operators with higher precedence than addition
        - Parenthesized expression grouping overriding default operator precedence
        estimated_hours: 2-3
      - id: calculator-parser-m2
        name: Unary and Power
        description: Add unary operators and exponentiation.
        acceptance_criteria:
        - Unary minus correctly negates values including nested unary like --5 evaluating to 5
        - Exponentiation computes base raised to power for integer and floating-point operands
        - Right associativity evaluates 2^3^2 as 2^(3^2) equaling 512 not 64
        - Power binds tighter than unary so -2^2 evaluates to -(2^2) equaling -4
        pitfalls:
        - --5 (double negative)
        - Power precedence vs unary
        - Right associativity
        concepts:
        - Unary operators
        - Associativity
        - Precedence climbing
        skills:
        - Implementing unary operator handling
        - Managing right-associative operators
        - Extending parser with new operators
        - Handling operator precedence conflicts
        deliverables:
        - Unary minus operator negating the following numeric expression
        - Exponentiation operator raising base to power with right associativity
        - Right-associative evaluation for power operator so 2^3^2 equals 2^9
        - Operator precedence hierarchy placing power above unary above multiplication
        estimated_hours: 2-3
      - id: calculator-parser-m3
        name: Variables and Functions
        description: Add variables and built-in functions.
        acceptance_criteria:
        - Variable assignment with x = 5 stores value and subsequent x + 2 evaluates to 7
        - Undefined variable reference produces descriptive error message with variable name
        - Built-in sin, cos, sqrt functions compute correct mathematical results
        - Function calls like sqrt(16) parse identifier, open paren, argument, and close paren
        pitfalls:
        - Function vs variable ambiguity
        - Assignment in expression
        - Scope issues
        concepts:
        - Symbol tables
        - Function calls
        - Variable binding
        skills:
        - Implementing symbol table data structures
        - Managing variable scope and binding
        - Parsing function call syntax
        - Extending language with new features
        deliverables:
        - Variable assignment storing named values in environment dictionary
        - Variable lookup resolving identifier names to stored numeric values
        - Built-in function library providing sin, cos, sqrt, and abs functions
        - Function call parsing detecting identifier followed by parenthesized arguments
        estimated_hours: 2-3
    - id: json-parser
      name: JSON Parser
      description: Recursive descent
      difficulty: beginner
      estimated_hours: 8-12
      essence: Lexical analysis through finite state machines to tokenize character streams, followed by recursive descent parsing to validate context-free grammar rules and construct hierarchical data structures from linear text input.
      why_important: Building this provides foundational knowledge for compiler construction, data format processing, and language design‚Äîskills directly applicable to building interpreters, DSLs, configuration parsers, and understanding how programming languages work internally.
      learning_outcomes:
      - Implement a lexical analyzer that tokenizes JSON strings using finite state machines
      - Design and build a recursive descent parser that validates grammar rules
      - Construct Abstract Syntax Trees (ASTs) from token streams
      - Handle lookahead and backtracking in top-down parsing
      - Debug parsing errors and implement meaningful error messages with position tracking
      - Implement escape sequence handling and Unicode support in string parsing
      - Design grammar rules for nested data structures (objects and arrays)
      - Validate syntactic correctness while building native data structures
      skills:
      - Recursive Descent Parsing
      - Tokenization & Lexical Analysis
      - Abstract Syntax Trees
      - Context-Free Grammars
      - Error Recovery
      - Finite State Machines
      - String Processing
      - Syntax Validation
      tags:
      - beginner-friendly
      - c
      - compilers
      - configuration
      - javascript
      - parsing
      - python
      - recursive-descent
      - tokenization
      - validation
      architecture_doc: architecture-docs/json-parser/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - C
        also_possible:
        - Rust
        - Go
      resources:
      - name: JSON Specification
        url: https://www.json.org/json-en.html
        type: specification
      - name: Crafting Interpreters
        url: https://craftinginterpreters.com/
        type: book
      prerequisites:
      - type: skill
        name: Basic programming
      - type: skill
        name: Understanding of JSON format
      milestones:
      - id: json-parser-m1
        name: Tokenizer
        description: Build a lexer that converts JSON string into tokens.
        acceptance_criteria:
        - Tokenizer correctly identifies and categorizes strings, numbers, booleans, and null literals
        - Punctuation tokens for braces, brackets, colons, and commas are emitted individually
        - Whitespace characters including spaces, tabs, newlines, and carriage returns are skipped between tokens
        - Escape sequences including backslash-n, backslash-t, backslash-u hex, and backslash-quote are handled in strings
        pitfalls:
        - Escape sequences in strings
        - Negative numbers
        - Scientific notation
        concepts:
        - Lexical analysis
        - Token types
        - State machine
        skills:
        - String parsing and character-by-character processing
        - Finite state machine design
        - Regular expression alternatives for tokenization
        - Character encoding and escape sequence handling
        deliverables:
        - Token type definitions for string, number, boolean, null, and structural characters
        - String tokenizer that correctly handles escape sequences including unicode escapes
        - Number parser that recognizes integers, floats, and scientific notation formats
        - Delimiter token recognizer for braces, brackets, colons, and commas
        estimated_hours: 2-3
      - id: json-parser-m2
        name: Parser
        description: Parse tokens into native data structures.
        acceptance_criteria:
        - Parser correctly builds native data structures from object and array token sequences
        - Deeply nested structures with mixed objects and arrays parse without stack overflow up to configurable depth
        - Parsed values are returned as native language types (dict, list, string, number, bool, null)
        - Syntax errors produce descriptive error messages indicating the unexpected token and its position
        pitfalls:
        - Trailing comma handling
        - Deep nesting stack overflow
        - Empty object/array edge case
        concepts:
        - Recursive descent
        - Grammar rules
        - AST construction
        skills:
        - Recursive function design
        - Grammar-based parsing
        - Abstract syntax tree construction
        - Stack management for nested structures
        deliverables:
        - Recursive descent parser that consumes tokens and builds a structured document tree
        - Object parser that reads key-value pairs delimited by colons and separated by commas
        - Array parser that reads ordered sequences of values within bracket delimiters
        - Nested structure handler that recursively processes objects and arrays to arbitrary depth
        estimated_hours: 3-4
      - id: json-parser-m3
        name: Error Handling & Edge Cases
        description: Add robust error handling and edge case support.
        acceptance_criteria:
        - Error messages include line number, column offset, and a description of the expected versus found token
        - All JSON value types including nested empty objects and empty arrays parse correctly
        - Number format validation rejects leading zeros, lone minus signs, and trailing decimal points
        - Unicode escape sequences in strings including surrogate pairs are decoded to proper characters
        pitfalls:
        - Numbers with leading zeros (invalid in JSON)
        - Unicode escapes \u0000
        - Duplicate keys in objects
        concepts:
        - Error recovery
        - JSON specification compliance
        - Unicode handling
        skills:
        - Specification compliance and validation
        - Unicode character handling
        - Error message formatting for user feedback
        - Boundary condition testing
        deliverables:
        - Syntax error reporter that outputs line number, column, and description of the parsing failure
        - Trailing comma handler that detects and reports trailing commas in objects and arrays
        - Unicode escape sequence processor that converts backslash-u hex sequences to characters
        - Nested depth limiter that raises an error when document nesting exceeds a configurable maximum
        estimated_hours: 2-3
    - id: tokenizer
      name: Tokenizer/Lexer
      description: Token types, state machine
      difficulty: beginner
      estimated_hours: 8-15
      essence: Character-by-character state machine traversal using finite automata to recognize lexemes through maximal munch and lookahead, transforming linear character streams into categorized token sequences while handling whitespace, delimiters, and lexical error boundaries.
      why_important: Building a lexer demystifies how programming languages work at the lowest level and provides the foundation for understanding parsers, compilers, and interpreters‚Äîskills directly applicable to developer tools, DSLs, and language server implementations.
      learning_outcomes:
      - Implement finite state machines for pattern recognition
      - Design token type systems with proper categorization
      - Build character-by-character scanning with lookahead
      - Handle edge cases like escape sequences and nested structures
      - Implement error detection and recovery at the lexical level
      - Optimize scanning performance with buffering strategies
      - Construct symbol tables for identifier management
      - Debug state transitions using trace outputs
      skills:
      - Finite State Machines
      - Pattern Matching
      - String Processing
      - Regular Expressions
      - Error Handling
      - Compiler Theory
      - Character Encoding
      tags:
      - beginner-friendly
      - compilers
      - go
      - javascript
      - parsing
      - python
      architecture_doc: architecture-docs/tokenizer/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Go
        also_possible:
        - Rust
        - C
      resources:
      - name: Crafting Interpreters - Scanning
        url: https://craftinginterpreters.com/scanning.html
        type: book
      - name: Let's Build a Compiler
        url: https://compilers.iecc.com/crenshaw/
        type: tutorial
      prerequisites:
      - type: skill
        name: Regular expressions basics
      - type: skill
        name: String manipulation
      milestones:
      - id: tokenizer-m1
        name: Basic Token Types
        description: Define token types and basic structure.
        acceptance_criteria:
        - Token class with type and value
        - Support numbers and identifiers as distinct token types with correct classification
        - Support operators (+, -, *, /)
        - Track position with line and column numbers for each emitted token
        pitfalls:
        - Forgetting EOF token
        - Not tracking position
        - Inconsistent naming
        concepts:
        - Tokens
        - Lexemes
        - Token types
        skills:
        - Enum and constant definitions
        - Data structure design for tokens
        - Position tracking in source code
        deliverables:
        - Token type enumeration defining all recognized token categories
        - Token structure holding type, lexeme value, and source position
        - Position tracking recording line number and column for each token
        - Token list output producing a complete ordered sequence of recognized tokens
        estimated_hours: 1-2
      - id: tokenizer-m2
        name: Scanning Logic
        description: Implement the main scanning loop.
        acceptance_criteria:
        - Iterate through source code character by character producing tokens
        - Match single-character tokens and emit them with correct type
        - Handle whitespace and newlines by skipping without emitting tokens
        - Report lexical errors with line and column position of the offending character
        pitfalls:
        - Off-by-one errors
        - Not handling newlines
        - Consuming too much
        concepts:
        - Scanning
        - Character stream
        - Position tracking
        skills:
        - Loop control and state management
        - Character-by-character input processing
        - Conditional logic for token recognition
        deliverables:
        - Character consumption function advancing the read position in source text
        - Peek and advance methods inspecting upcoming characters without consuming them
        - Whitespace handling logic skipping spaces, tabs, and newline characters
        - EOF handling logic emitting an end-of-file token when input is exhausted
        estimated_hours: 2-3
      - id: tokenizer-m3
        name: Multi-character Tokens
        description: Handle numbers, identifiers, strings, and multi-char operators.
        acceptance_criteria:
        - Scan integers and floats including decimal point and digit sequences
        - Scan identifiers starting with a letter or underscore followed by alphanumeric characters
        - Distinguish keywords from identifiers using a reserved word lookup table
        - Handle ==, !=, <=, >= operators
        pitfalls:
        - Not handling '.' in floats correctly
        - Keywords vs identifiers
        - Multi-char operators
        concepts:
        - Maximal munch
        - Keyword tables
        - Lookahead
        skills:
        - String parsing and validation
        - Lookahead buffer implementation
        - Hash table or map usage for keywords
        - Number parsing with decimal points
        deliverables:
        - Multi-character operator scanning for ==, !=, <=, and >= tokens
        - Identifier scanner consuming alphanumeric characters and underscores into a single token
        - Keyword recognizer distinguishing reserved words from user-defined identifiers
        - Number literal scanner consuming integer and floating-point digit sequences
        estimated_hours: 3-4
      - id: tokenizer-m4
        name: Strings and Comments
        description: Handle string literals and comments.
        acceptance_criteria:
        - String literals parse correctly including escape sequences like \n and \t
        - Single-line comments starting with // are skipped until end of line
        - Multi-line comments between /* and */ are skipped including nested newlines
        - Handle unterminated strings and comments by reporting an error with position
        pitfalls:
        - Unterminated strings across lines
        - Nested comments
        - Escape sequences
        concepts:
        - String literals
        - Comments
        - Escape sequences
        skills:
        - String escape sequence handling
        - Multi-line input handling
        - Comment filtering logic
        - State machine for quote matching
        deliverables:
        - String literal scanner consuming characters between matching quote delimiters
        - Escape sequence handler interpreting backslash-prefixed special characters in strings
        - Single-line comment scanner skipping characters from // to end of line
        - Multi-line comment scanner skipping characters between /* and */ delimiters
        estimated_hours: 2-3
    intermediate:
    - id: lisp-interp
      name: Lisp Interpreter
      description: S-expressions, eval
      difficulty: intermediate
      estimated_hours: 15-25
      essence: Recursive evaluation of symbolic expressions through environment chaining, where tokenized S-expressions are parsed into nested list structures and evaluated by applying operators to operands with lexical scope resolution through linked environment frames capturing variable bindings and closures.
      why_important: Building a Lisp interpreter demystifies how programming languages work at a fundamental level, teaching you about parsing, evaluation strategies, closure implementation, and metaprogramming‚Äîskills that transfer directly to understanding any language runtime or building domain-specific languages.
      learning_outcomes:
      - Implement recursive descent parsing for S-expressions with proper tokenization
      - Design environment data structures supporting lexical scoping and closure capture
      - Build an evaluation loop handling special forms versus function application
      - Implement first-class functions with lambda expressions and proper variable capture
      - Debug scope chain resolution and variable binding in nested environments
      - Construct tail-call optimization to prevent stack overflow in recursive functions
      - Handle dynamic typing and runtime type checking for primitive operations
      - Implement list manipulation primitives and higher-order function support
      skills:
      - Recursive Descent Parsing
      - Environment-based Evaluation
      - Lexical Scoping
      - First-class Functions
      - Abstract Syntax Trees
      - Metaprogramming Concepts
      - Dynamic Typing
      - Functional Programming
      tags:
      - compilers
      - environment
      - intermediate
      - javascript
      - macros
      - python
      - ruby
      - s-expressions
      architecture_doc: architecture-docs/lisp-interp/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Ruby
        also_possible:
        - Go
        - Rust
        - C
      resources:
      - name: Make a Lisp
        url: https://github.com/kanaka/mal
        type: tutorial
      - name: SICP
        url: https://mitpress.mit.edu/sites/default/files/sicp/index.html
        type: book
      prerequisites:
      - type: skill
        name: Recursion
      - type: skill
        name: Basic parsing
      - type: skill
        name: Functional programming concepts
      milestones:
      - id: lisp-interp-m1
        name: S-Expression Parser
        description: Parse Lisp S-expressions into data structures.
        acceptance_criteria:
        - Atoms including numbers, symbols, and string literals are parsed into their correct types
        - Nested parenthesized lists are parsed into nested list data structures at arbitrary depth
        - Whitespace and semicolon-initiated line comments are correctly ignored during tokenization
        - Parser returns native data structures (numbers as numbers, symbols as strings, lists as arrays)
        pitfalls:
        - Unbalanced parentheses
        - Quote syntax
        - Negative numbers
        concepts:
        - S-expressions
        - Tokenization
        - Recursive parsing
        skills:
        - String manipulation and tokenization
        - Recursive data structure parsing
        - Abstract syntax tree construction
        deliverables:
        - Tokenizer that splits input into parentheses, atoms, string literals, and whitespace tokens
        - Recursive descent parser that builds nested list structures from the token stream
        - List constructor that creates the internal representation for parenthesized sequences of expressions
        - Quote handler that transforms 'expr shorthand into the equivalent (quote expr) list form
        estimated_hours: 2-4
      - id: lisp-interp-m2
        name: Basic Evaluation
        description: Evaluate arithmetic and conditionals.
        acceptance_criteria:
        - Number literals evaluate to themselves without any environment lookup or transformation
        - Arithmetic operators +, -, *, / return correct results for integers and floating-point operands
        - Comparison operators <, >, =, <=, >= return boolean results for numeric comparisons
        - Conditional if-expressions evaluate the consequent or alternative branch based on the test value
        pitfalls:
        - Special forms vs functions
        - Short-circuit evaluation
        - Environment lookup
        concepts:
        - Evaluation rules
        - Environments
        - Special forms
        skills:
        - Expression evaluation engines
        - Environment and scope management
        - Conditional logic implementation
        - Pattern matching for special forms
        deliverables:
        - Number evaluator that returns numeric atoms as their literal value without transformation
        - Arithmetic primitives implementing addition, subtraction, multiplication, and division operators
        - Comparison primitives implementing less-than, greater-than, equal, and their combinations
        - Boolean primitives implementing logical and, or, and not operations on truth values
        estimated_hours: 3-4
      - id: lisp-interp-m3
        name: Variables and Functions
        description: Add define, lambda, and lexical scope.
        acceptance_criteria:
        - Define form binds a variable name to the evaluated value in the current environment
        - Lambda form creates a first-class function value that captures its lexical environment
        - Lexical scoping ensures closures access variables from their definition scope, not the call site
        - Let form creates local bindings that are visible only within the let body expressions
        pitfalls:
        - Mutation vs shadowing
        - Closure capturing
        - Let vs let*
        concepts:
        - Closures
        - Lexical scope
        - Variable binding
        skills:
        - Function closure implementation
        - Lexical scoping systems
        - Variable binding and shadowing
        - First-class function support
        deliverables:
        - Environment data structure mapping symbol names to their bound values with parent scope chain
        - Define special form that binds a name to a value in the current environment scope
        - Lambda special form that creates a closure capturing the defining environment and parameter list
        - Function application logic that evaluates arguments and calls the function with a new scope
        estimated_hours: 4-6
      - id: lisp-interp-m4
        name: List Operations & Recursion
        description: Add list primitives and support recursion.
        acceptance_criteria:
        - Cons creates a pair, car returns the first element, and cdr returns the rest of the list
        - List constructor builds a proper nil-terminated list from its arguments in order
        - Null? predicate returns true for the empty list and false for all other values
        - Recursive functions execute correctly without stack overflow for reasonable recursion depths
        pitfalls:
        - Proper vs improper lists
        - Stack overflow on deep recursion
        - Empty list handling
        concepts:
        - List processing
        - Recursion
        - Higher-order functions
        skills:
        - Recursive algorithm design
        - List data structure operations
        - Higher-order function implementation
        - Tail call optimization techniques
        deliverables:
        - Car, cdr, and cons primitives that access the head, tail, and construct new pairs respectively
        - List construction function that builds a proper list from a variable number of arguments
        - Recursive function support allowing functions to call themselves by name within their body
        - Tail call optimization that reuses the current stack frame for calls in tail position
        estimated_hours: 3-4
    - id: bytecode-vm
      name: Bytecode VM
      description: Stack-based
      difficulty: intermediate
      estimated_hours: 15-25
      essence: Stack-based instruction dispatch and execution engine that translates custom bytecode into runtime behavior through fetch-decode-execute cycles, managing operand stacks, call frames, and control flow primitives.
      why_important: Building a VM reveals how high-level languages actually execute under the hood‚Äîfrom Python's CPython to the JVM‚Äîteaching you the runtime foundations that power every interpreted language and preparing you for systems programming, compiler backends, and language implementation work.
      learning_outcomes:
      - Design a compact instruction set with opcodes for arithmetic, stack manipulation, and control flow
      - Implement a fetch-decode-execute loop with instruction dispatching
      - Build a value stack for operand management and expression evaluation
      - Create call frames and activation records for function invocation
      - Implement jump instructions and conditional branching for control flow
      - Debug bytecode execution with instruction tracing and stack inspection
      - Optimize bytecode interpretation with computed gotos or threaded code
      - Handle local variables through stack offsets and frame pointers
      skills:
      - Instruction Set Architecture
      - Stack-Based Execution
      - Bytecode Interpretation
      - Memory Management
      - Control Flow Implementation
      - Low-Level Debugging
      - Systems Programming
      tags:
      - c
      - compilers
      - data-structures
      - execution
      - go
      - intermediate
      - opcodes
      - rust
      - stack-machine
      - vm
      architecture_doc: architecture-docs/bytecode-vm/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible:
        - Python
        - Java
      resources:
      - name: Crafting Interpreters - Bytecode VM
        url: https://craftinginterpreters.com/a-bytecode-virtual-machine.html
        type: book
      - name: Writing a Simple VM
        url: https://felix.engineer/blogs/virtual-machine-in-c
        type: article
      prerequisites:
      - type: skill
        name: Basic assembly concepts
      - type: skill
        name: Stack data structure
      - type: skill
        name: Binary representation
      milestones:
      - id: bytecode-vm-m1
        name: Instruction Set Design
        description: Define opcodes and bytecode format.
        acceptance_criteria:
        - Opcode enum covers arithmetic, comparison, load, store, jump, and call operations
        - Instruction format encodes opcode in first byte with operand bytes following
        - Bytecode chunk stores instruction stream alongside indexed constant pool
        - Disassembler outputs instruction name, operands, and offset for each bytecode instruction
        pitfalls:
        - Too complex instruction set
        - Forgetting HALT
        - Operand encoding issues
        concepts:
        - Opcodes
        - Instruction encoding
        - Bytecode format
        skills:
        - Instruction set architecture design
        - Binary encoding and bit manipulation
        - Opcode specification and documentation
        - Assembly language fundamentals
        deliverables:
        - Opcode enumeration defining all supported bytecode instruction types
        - Instruction encoding format specifying opcode byte and operand bytes layout
        - Bytecode chunk structure holding instruction bytes and constant pool together
        - Disassembler printing human-readable representation of bytecode instructions
        estimated_hours: 2-3
      - id: bytecode-vm-m2
        name: Stack-Based Execution
        description: Implement the execution loop with value stack.
        acceptance_criteria:
        - Operand stack correctly pushes and pops values without overflow or underflow errors
        - ADD instruction pops two values, computes sum, and pushes result onto stack
        - Comparison instructions pop two values and push boolean result for conditional use
        - Instruction pointer advances sequentially and dispatches correct handler per opcode
        pitfalls:
        - Stack over/underflow
        - Off-by-one in IP
        - Order of operands for SUB/DIV
        concepts:
        - Stack machines
        - Instruction pointer
        - Fetch-decode-execute
        skills:
        - Stack data structure implementation
        - Program counter management
        - Bytecode interpretation loops
        - Runtime error detection
        deliverables:
        - Operand stack pushing and popping values during instruction execution
        - Arithmetic instruction execution performing operations on stack top values
        - Comparison instruction execution pushing boolean result onto stack
        - Instruction pointer advancing through bytecode and dispatching each opcode
        estimated_hours: 4-5
      - id: bytecode-vm-m3
        name: Control Flow
        description: Add jumps and conditionals.
        acceptance_criteria:
        - Unconditional jump sets instruction pointer to specified absolute or relative offset
        - Conditional jump pops stack top and jumps only if value evaluates to false
        - Loop back-edge correctly jumps backward creating repeated execution of loop body
        - Invalid jump targets outside bytecode bounds are detected and reported as errors
        pitfalls:
        - Jump offset calculation
        - Forgetting to pop condition
        - Infinite loops
        concepts:
        - Control flow
        - Jumps
        - Conditional execution
        skills:
        - Branch instruction implementation
        - Conditional logic evaluation
        - Jump target address calculation
        - Control flow graph understanding
        deliverables:
        - Unconditional jump instruction setting instruction pointer to target offset
        - Conditional jump instruction branching based on stack top truth value
        - Loop back-edge jumping instruction pointer backward to loop condition
        - Branch target validation ensuring jump offsets stay within bytecode bounds
        estimated_hours: 3-4
      - id: bytecode-vm-m4
        name: Variables and Functions
        description: Add local variables and function calls.
        acceptance_criteria:
        - Local variable store and load instructions access slots by index within current frame
        - Call frame stores return address so execution resumes at correct point after return
        - Function call pushes new frame with argument values accessible as local variables
        - Return instruction pops frame, restores caller state, and pushes return value onto stack
        pitfalls:
        - Frame pointer calculation
        - Argument passing order
        - Return value handling
        concepts:
        - Call frames
        - Local variables
        - Function calls
        skills:
        - Call stack frame management
        - Function calling conventions
        - Local variable storage and retrieval
        - Stack-based parameter passing
        - Return address handling
        deliverables:
        - Local variable slots indexed storage for function-scoped variable values
        - Call frame stack tracking return addresses and local variable base pointers
        - Function call instruction creating new frame and transferring execution control
        - Return instruction restoring caller frame and passing return value back
        estimated_hours: 5-7
    - id: ast-builder
      name: AST Builder
      description: Parse expressions to AST
      difficulty: intermediate
      estimated_hours: 12-20
      essence: Transforming flat token sequences into hierarchical tree structures that encode operator precedence, associativity, and syntactic relationships by recursively applying context-free grammar production rules and resolving ambiguities through parsing algorithms like recursive descent or precedence climbing.
      why_important: Parsers are the foundation of every compiler, interpreter, linter, and code analysis tool. Understanding grammar-to-code translation and tree-based program representation is essential for building developer tools, language processors, and working with compiler frontends.
      learning_outcomes:
      - Implement recursive descent parsing for expressions with correct operator precedence
      - Design AST node hierarchies that represent language constructs efficiently
      - Handle grammar ambiguities through precedence and associativity rules
      - Build error recovery mechanisms that provide useful diagnostics without halting parsing
      - Transform context-free grammar productions into parsing functions
      - Implement statement and declaration parsing with scope awareness
      - Debug parse trees by visualizing AST structure and validating tree invariants
      - Distinguish between concrete syntax trees and abstract syntax trees
      skills:
      - Grammar Design
      - Recursive Descent
      - Tree Data Structures
      - Precedence Climbing
      - Error Recovery
      - Context-Free Grammars
      - Syntax Analysis
      - Parser Combinators
      tags:
      - compilers
      - data-structures
      - grammar
      - intermediate
      - java
      - javascript
      - parsing
      - python
      - recursive-descent
      - syntax-tree
      architecture_doc: architecture-docs/ast-builder/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Java
        also_possible:
        - Go
        - Rust
        - C
      resources:
      - name: Crafting Interpreters - Parsing
        url: https://craftinginterpreters.com/parsing-expressions.html
        type: book
      - name: Pratt Parsing
        url: https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html
        type: article
      prerequisites:
      - type: skill
        name: Tokenizer/Lexer
      - type: skill
        name: Recursion
      - type: skill
        name: Tree data structures
      milestones:
      - id: ast-builder-m1
        name: AST Node Definitions
        description: Define AST node types for your language.
        acceptance_criteria:
        - Expression nodes represent literals, binary operators, unary operators, and identifiers as distinct types
        - Statement nodes represent if-else, while, return, and block constructs with correct child references
        - Visitor pattern or tagged union dispatch enables processing each node type without type-casting
        - Every AST node includes source location data (file, line, column) for error reporting and debugging
        pitfalls:
        - Missing node types
        - No location info
        - Mutable vs immutable nodes
        concepts:
        - AST nodes
        - Expression vs statement
        - Tree representation
        skills:
        - Data structure design
        - Tree traversal implementation
        - Type system basics
        - Pattern matching
        deliverables:
        - Base Node class with source location tracking including file name, line number, and column offset
        - Expression node types covering literal values, binary operations, unary operations, and identifier references
        - Statement node types covering if-else branches, while loops, return statements, and block groupings
        - Visitor pattern implementation enabling type-safe tree traversal without modifying node classes
        estimated_hours: 2-3
      - id: ast-builder-m2
        name: Recursive Descent Parser
        description: Implement recursive descent parsing for expressions.
        acceptance_criteria:
        - Each grammar production rule is implemented as a separate recursive parsing function
        - Operator precedence is respected so multiplication binds tighter than addition in the resulting AST
        - Function call expressions are parsed with the callee followed by a parenthesized argument list
        - Token consumption advances the stream correctly and reports errors on unexpected token types
        pitfalls:
        - Wrong precedence order
        - Left recursion
        - Infinite loops
        concepts:
        - Recursive descent
        - Operator precedence
        - Grammar rules
        skills:
        - Recursive function design
        - Operator precedence implementation
        - Grammar translation to code
        - Parser combinator patterns
        deliverables:
        - Token stream consumer with single-token lookahead for predictive parsing decisions
        - Precedence climbing algorithm that correctly handles operator precedence levels and grouping
        - Recursive parsing functions with one function per grammar production rule
        - Operator associativity handling that correctly nests left-associative and right-associative operators
        estimated_hours: 4-6
      - id: ast-builder-m3
        name: Statement Parsing
        description: Parse statements and declarations.
        acceptance_criteria:
        - Parser correctly handles let/var/const declarations with optional initializer expressions
        - If/else statements are parsed with condition, then-branch, and optional else-branch as child nodes
        - While loops are parsed with a condition expression and a body statement or block
        - Block statements group zero or more statements within braces and establish lexical scope boundaries
        pitfalls:
        - Dangling else ambiguity
        - Missing semicolons
        - Block scope boundaries
        concepts:
        - Statement parsing
        - Control flow
        - Declarations
        skills:
        - Control flow parsing
        - Symbol table management
        - Scope resolution
        - Statement vs expression handling
        deliverables:
        - Variable declaration parsing supporting let, var, and const with optional initializer expressions
        - Function definition parsing including parameter list, return type annotation, and body block
        - Control flow statement parsing for if/else, while, and for constructs with correct body nesting
        - Block statement parsing that groups multiple statements within braces and establishes a new scope
        estimated_hours: 4-5
      - id: ast-builder-m4
        name: Error Recovery
        description: Implement error handling and recovery.
        acceptance_criteria:
        - Parser reports multiple syntax errors in a single pass instead of stopping at the first error
        - After an error, the parser synchronizes to the next statement boundary and continues parsing
        - Error messages include the file name, line number, column, and a description of what was expected
        - Source locations in error messages accurately point to the token where the error was detected
        pitfalls:
        - Stopping at first error
        - Bad sync points
        - Cascading errors
        concepts:
        - Error recovery
        - Panic mode
        - Synchronization
        skills:
        - Error message design
        - Recovery strategy implementation
        - Fault tolerance
        - Diagnostic reporting
        deliverables:
        - Synchronization points that advance the token stream to a known recovery position after an error
        - Multiple error collection that continues parsing after the first error to report additional issues
        - Meaningful error messages that include the expected token, actual token, and source location
        - Panic mode recovery that discards tokens until a statement boundary is found to resume parsing
        estimated_hours: 2-3
    - id: ast-interpreter
      name: AST Tree-Walking Interpreter
      description: Evaluate AST directly
      difficulty: intermediate
      estimated_hours: 12-20
      essence: Recursive traversal of abstract syntax trees to evaluate expressions, manage variable bindings in nested scopes, and execute control flow by interpreting nodes directly without compilation to machine code.
      why_important: Building an AST interpreter teaches the foundational principles behind every interpreted language (Python, Ruby, JavaScript), providing essential knowledge for language design, compiler construction, and understanding how code execution actually works under the hood.
      learning_outcomes:
      - Implement recursive descent evaluation of arithmetic and logical expressions
      - Design environment chains for lexical scoping and variable resolution
      - Build a symbol table with nested scope management for variable binding
      - Implement control flow execution through conditional AST node evaluation
      - Handle function closures by capturing and preserving environment references
      - Debug scope-related issues like variable shadowing and undefined references
      - Traverse and interpret composite AST structures using the visitor pattern
      - Manage runtime state and execution context through call stacks
      skills:
      - AST Traversal
      - Environment Management
      - Recursive Evaluation
      - Lexical Scoping
      - Symbol Tables
      - Tree Walking
      - Closure Implementation
      - Runtime State Management
      tags:
      - compilers
      - data-structures
      - evaluation
      - intermediate
      - java
      - javascript
      - parsing
      - python
      - tree-walk
      - visitors
      architecture_doc: architecture-docs/ast-interpreter/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Java
        also_possible:
        - Go
        - Rust
      resources:
      - name: Crafting Interpreters - Evaluating
        url: https://craftinginterpreters.com/evaluating-expressions.html
        type: book
      - name: SICP - Metacircular Evaluator
        url: https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-26.html
        type: book
      prerequisites:
      - type: skill
        name: AST Builder
      - type: skill
        name: Recursion
      - type: skill
        name: Environment/scope concepts
      milestones:
      - id: ast-interpreter-m1
        name: Expression Evaluation
        description: Evaluate arithmetic and comparison expressions.
        acceptance_criteria:
        - Literal nodes evaluate to their corresponding runtime values (numbers, strings, booleans, nil)
        - Binary operators (+, -, *, /, <, >, ==, !=) produce correct results for valid operand types
        - Unary minus negates numeric values and logical-not inverts boolean truthiness
        - Parenthesized expressions are evaluated first, correctly overriding default operator precedence
        pitfalls:
        - Type errors at runtime
        - Division by zero
        - Short-circuit evaluation
        concepts:
        - Tree-walking
        - Evaluation
        - Operator semantics
        skills:
        - Recursive AST traversal
        - Operator precedence handling
        - Type coercion and checking
        - Runtime error handling
        deliverables:
        - Literal value evaluation that returns numbers, strings, and booleans as runtime values
        - Binary operator evaluation with runtime type checking for arithmetic, comparison, and equality
        - Unary operator evaluation supporting numeric negation and logical-not operations
        - Short-circuit evaluation for logical AND and OR that skips the right operand when result is determined
        estimated_hours: 3-4
      - id: ast-interpreter-m2
        name: Variables and Environment
        description: Implement variable binding and lookup.
        acceptance_criteria:
        - Environment stores name-to-value mappings and supports get, set, and define operations
        - Variable declaration with var or let creates a new binding in the current scope's environment
        - Variable assignment updates the binding in the nearest enclosing scope where the name is defined
        - Nested scopes look up variables by walking the parent environment chain until the name is found
        pitfalls:
        - Scope restoration
        - Shadowing
        - Assignment vs declaration
        concepts:
        - Environments
        - Scoping
        - Variable binding
        skills:
        - Hash map implementation for environments
        - Scope chain management
        - Variable resolution and lookup
        - Handling undefined variables
        deliverables:
        - Environment class that maps variable names to their current runtime values
        - Nested scopes with a parent environment chain enabling lexical scope variable lookup
        - Variable declaration that binds a name to an initial value in the current environment
        - Undefined variable error handling that raises a clear runtime error with the variable name
        estimated_hours: 3-4
      - id: ast-interpreter-m3
        name: Control Flow
        description: Implement if/else and loops.
        acceptance_criteria:
        - If/else statements correctly evaluate the condition and execute only the matching branch
        - While loops repeat the body as long as the condition evaluates to a truthy value
        - For loops execute the initializer once, check the condition each iteration, and run the increment
        - Break and continue statements correctly exit or restart the nearest enclosing loop iteration
        pitfalls:
        - Infinite loops
        - Break outside loop
        - Truthiness rules
        concepts:
        - Control flow
        - Loops
        - Conditionals
        skills:
        - Conditional branch evaluation
        - Loop iteration and termination
        - Break and continue statement handling
        - Boolean expression evaluation
        deliverables:
        - If/else statement execution that evaluates the condition and runs the appropriate branch
        - While loop execution with support for break and continue control flow statements
        - For loop desugaring that translates for-loops into equivalent while-loop AST structures
        - Return statement execution with value propagation back to the enclosing function call
        estimated_hours: 3-4
      - id: ast-interpreter-m4
        name: Functions
        description: Implement function definitions and calls.
        acceptance_criteria:
        - Function declarations create a callable value stored in the current environment by name
        - Function calls bind each argument to the corresponding parameter in a new local environment
        - Return statements unwind execution and deliver the return value to the call site
        - Closures correctly capture and access variables from the enclosing scope even after it exits
        pitfalls:
        - Closure capture
        - Return from nested function
        - Argument count
        concepts:
        - Functions
        - Closures
        - Call stack
        skills:
        - Function closure implementation
        - Call stack management
        - Parameter binding and argument passing
        - Return value propagation
        - Lexical scope capture
        deliverables:
        - Function declaration that stores the function name, parameters, and body in the environment
        - Function call execution that binds arguments to parameters and evaluates the body in a new scope
        - Closure capture that preserves the enclosing environment at the time of function definition
        - Recursion support allowing a function to call itself by name within its own body
        estimated_hours: 4-5
    advanced:
    - id: type-checker
      name: Type Checker
      description: Type inference
      difficulty: advanced
      estimated_hours: 20-35
      essence: Constraint generation and unification over type equations to automatically infer the most general types while detecting conflicts through substitution propagation and the occur-check algorithm.
      why_important: Type checkers are fundamental to modern language implementation, teaching you how compilers reason about program correctness and enabling you to build safer programming languages with advanced features like generics and type inference.
      learning_outcomes:
      - Implement type environments and substitution mechanisms for tracking variable types
      - Design and apply type inference using Algorithm W or constraint-based approaches
      - Build unification algorithms to solve type equations and detect conflicts
      - Implement Hindley-Milner polymorphism with let-generalization
      - Create semantic analysis passes for binding resolution and scope checking
      - Debug type errors with informative error messages and source locations
      - Handle recursive types and occur-check violations in unification
      - Design type rules for expressions, statements, and function applications
      skills:
      - Type Theory
      - Algorithm W
      - Unification Algorithms
      - Semantic Analysis
      - Constraint Solving
      - Polymorphic Type Systems
      - Abstract Syntax Trees
      - Compiler Design
      tags:
      - advanced
      - ai-ml
      - haskell
      - ocaml
      - rust
      - static-analysis
      - type-inference
      - type-system
      architecture_doc: architecture-docs/type-checker/index.md
      languages:
        recommended:
        - OCaml
        - Haskell
        - Rust
        also_possible:
        - Python
        - TypeScript
      resources:
      - name: Types and Programming Languages
        url: https://www.cis.upenn.edu/~bcpierce/tapl/
        type: book
      - name: Write You a Haskell
        url: http://dev.stephendiehl.com/fun/
        type: tutorial
      prerequisites:
      - type: skill
        name: AST Builder
      - type: skill
        name: Type systems basics
      - type: skill
        name: Recursive algorithms
      milestones:
      - id: type-checker-m1
        name: Type Representation
        description: Define type representations and type environment.
        acceptance_criteria:
        - Primitive types int, bool, and string are represented as distinct type nodes
        - Function types represent parameter types and return type as a single arrow type
        - Type variables for generics are represented as named placeholder type nodes
        - Type environment maps variable names to their declared or inferred types
        pitfalls:
        - Forgetting unit/void type
        - Mutable type variables
        - Scope handling
        concepts:
        - Type representations
        - Type environments
        - Scoping
        skills:
        - Algebraic data types for type representation
        - Symbol table implementation
        - Environment management and scoping rules
        deliverables:
        - Type AST (primitives, functions, generics)
        - Type environment (symbol to type)
        - Type equality and compatibility checks for structural or nominal comparison
        - Subtyping rules defining when one type can be used in place of another
        estimated_hours: 3-4
      - id: type-checker-m2
        name: Basic Type Checking
        description: Check types for expressions and statements.
        acceptance_criteria:
        - Literals have known types inferred from their syntactic form
        - Binary operators check that both operand types are compatible with the operator
        - Function calls verify that argument types match the declared parameter types
        - Assignments check that the right-hand side type is compatible with the variable type
        pitfalls:
        - Operator type rules
        - Function arity
        - Return type tracking
        concepts:
        - Type checking
        - Type rules
        - Error reporting
        skills:
        - Type rule implementation for expressions
        - Type checking visitor pattern
        - Semantic error reporting and recovery
        deliverables:
        - Type annotation parser extracting explicit type declarations from source AST
        - Expression type inference deducing types from literals, operators, and context
        - Type compatibility checker verifying assignment and argument type correctness
        - Error reporting module producing clear messages with source location for type mismatches
        estimated_hours: 5-7
      - id: type-checker-m3
        name: Type Inference
        description: Infer types where not explicitly annotated.
        acceptance_criteria:
        - Infer variable types from initializer expressions without explicit annotations
        - Infer function return types from the body expression or return statements
        - Unification algorithm finds most general unifier for a set of type constraints
        - Handle type constraints producing clear errors when constraints are unsatisfiable
        pitfalls:
        - Infinite types
        - Substitution application
        - Occurs check
        concepts:
        - Type inference
        - Unification
        - Constraints
        skills:
        - Constraint generation and solving
        - Unification algorithm implementation
        - Type substitution and occurs check
        deliverables:
        - Constraint generation pass collecting type equality constraints from expressions
        - Unification algorithm solving type constraints by finding consistent type substitutions
        - Type variable substitution applying solved constraints to replace type variables with concrete types
        - Let-polymorphism generalization abstracting inferred types over free type variables at let bindings
        estimated_hours: 6-10
      - id: type-checker-m4
        name: Polymorphism
        description: Add support for generic/polymorphic types.
        acceptance_criteria:
        - Let polymorphism allows a single definition to be used at multiple types
        - Generalize types at let bindings by quantifying over free type variables
        - Instantiate polymorphic types at use sites
        - Type schemes with forall quantification represent polymorphic types correctly
        pitfalls:
        - Value restriction
        - Generalization timing
        - Monomorphization
        concepts:
        - Polymorphism
        - Type schemes
        - Generalization
        skills:
        - Generic type parameter handling
        - Type scheme instantiation and generalization
        - Let-polymorphism implementation
        deliverables:
        - Generic type parameters declared on functions and data type definitions
        - Type variable instantiation replacing generic parameters with fresh type variables at use sites
        - Constraint solving resolving type variable bindings during polymorphic function application
        - Generic function call type checking inferring type arguments from actual parameter types
        estimated_hours: 6-10
    - id: simple-gc
      name: Simple GC
      description: Mark-sweep
      difficulty: advanced
      estimated_hours: 15-25
      essence: Automatic memory reclamation through graph traversal algorithms that distinguish live objects from garbage, requiring intricate pointer manipulation and careful coordination between allocation, marking, and deallocation phases to prevent memory corruption.
      why_important: Building a garbage collector reveals how high-level languages manage memory automatically and teaches systems programming fundamentals that apply to performance optimization, debugging memory leaks, and understanding runtime behavior in production systems.
      learning_outcomes:
      - Implement object metadata headers with type information and GC flags
      - Design root set enumeration from stack, registers, and global variables
      - Build graph traversal algorithms to trace object references
      - Implement mark-bit manipulation and reachability analysis
      - Debug memory corruption issues and use-after-free errors
      - Manage free lists and memory fragmentation
      - Measure collector pause times and memory overhead
      - Handle edge cases like circular references and weak pointers
      skills:
      - Manual Memory Management
      - Pointer Manipulation
      - Graph Traversal Algorithms
      - Systems Programming
      - Memory Debugging
      - Performance Profiling
      - Data Structure Design
      - Low-level C Programming
      tags:
      - advanced
      - c
      - garbage-collection
      - go
      - mark-sweep
      - memory-management
      - rust
      - tracing
      architecture_doc: architecture-docs/simple-gc/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible:
        - Python (educational)
      resources:
      - name: The Garbage Collection Handbook
        url: https://gchandbook.org/
        type: book
      - name: Baby's First GC
        url: https://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/
        type: article
      prerequisites:
      - type: skill
        name: Memory management basics
      - type: skill
        name: Graph traversal
      - type: skill
        name: C pointers
      milestones:
      - id: simple-gc-m1
        name: Object Model
        description: Define object representation with GC metadata.
        acceptance_criteria:
        - Object header includes type information for runtime dispatch
        - Store mark bit in each object header for garbage collection
        - Track total allocated object size for GC threshold triggering
        - Support at least three different object types (int, pair, string)
        pitfalls:
        - Header alignment
        - Type safety
        - Object size calculation
        concepts:
        - Object headers
        - Type tags
        - Allocation lists
        skills:
        - Memory layout design
        - Pointer arithmetic and alignment
        - Low-level data structure implementation
        - Type system design
        deliverables:
        - Object header structure containing type tag and mark bit
        - Reference tracking linking objects that point to other objects
        - Object allocation function creating new objects on the heap
        - Type information system distinguishing integers, pairs, and strings
        estimated_hours: 3-4
      - id: simple-gc-m2
        name: Root Discovery
        description: Identify and traverse GC roots.
        acceptance_criteria:
        - Identify and enumerate all GC root references on the stack
        - Include global variable references in the root set
        - Support register-based roots when applicable to target architecture
        - Distinguish precise roots (known pointers) from conservative scanning
        pitfalls:
        - Missing roots
        - Stack scanning direction
        - Interior pointers
        concepts:
        - Root set
        - Precise vs conservative
        - Stack scanning
        skills:
        - Stack introspection and traversal
        - Pointer identification techniques
        - Memory scanning strategies
        - Platform-specific calling conventions
        deliverables:
        - Stack scanning identifying root references from call stack frames
        - Global variable roots tracking references in static or global scope
        - Register contents inspection for architectures storing roots in registers
        - Root set construction building complete list of live reference origins
        estimated_hours: 3-4
      - id: simple-gc-m3
        name: Mark Phase
        description: Traverse and mark all reachable objects.
        acceptance_criteria:
        - Recursively mark all objects reachable from root set
        - Handle reference cycles without infinite loop using mark bits
        - Set and check mark bits to avoid revisiting already-marked objects
        - Optionally use explicit worklist instead of recursion to avoid stack overflow
        pitfalls:
        - Stack overflow on deep structures
        - Forgetting object types
        - Double marking
        concepts:
        - Graph traversal
        - Mark bits
        - Tri-color marking
        skills:
        - Recursive graph algorithms
        - Iterative traversal with explicit stacks
        - Bit manipulation for marking
        - Cycle detection in object graphs
        deliverables:
        - Object graph traversal starting from root set references
        - Mark bit setting flagging each reachable object as live
        - Explicit worklist processing avoiding deep recursion stack overflow
        - Reachability analysis determining which objects are still in use
        estimated_hours: 3-4
      - id: simple-gc-m4
        name: Sweep Phase
        description: Reclaim unmarked objects and reset marks.
        acceptance_criteria:
        - Walk the complete allocation list checking each object's mark bit
        - Free all unmarked objects and reclaim their memory
        - Reset mark bits on surviving objects for next collection cycle
        - Update allocation linked list to skip freed object entries
        pitfalls:
        - List corruption
        - Forgetting to clear marks
        - Memory leaks in objects
        concepts:
        - Memory reclamation
        - Linked list manipulation
        - GC thresholds
        skills:
        - Free list management
        - Memory coalescing techniques
        - Linked list manipulation without allocation
        - GC heuristics and tuning
        deliverables:
        - Heap traversal walking entire allocation list checking mark bits
        - Unmarked object collection freeing memory of unreachable objects
        - Free list update relinking allocation list after removing freed objects
        - Memory reclamation returning freed bytes to allocator for reuse
        estimated_hours: 3-4
    - id: bytecode-compiler
      name: Bytecode Compiler
      description: AST to bytecode
      difficulty: advanced
      estimated_hours: 25-40
      essence: AST traversal with instruction selection and emission, stack depth management for operand tracking, jump target resolution for control flow, and constant pool organization to generate compact executable bytecode for stack-based architectures.
      why_important: Bytecode compilation bridges high-level language semantics and machine execution, teaching fundamental compiler backend techniques used in Python, Java, and WebAssembly while developing skills in code generation and low-level optimization applicable to any systems programming role.
      learning_outcomes:
      - Implement single-pass AST traversal with instruction emission
      - Design instruction encoding schemes for stack-based operations
      - Build constant pool management for literals and symbols
      - Implement stack depth tracking and verification
      - Generate jump instructions and patch forward references for control flow
      - Design calling conventions with stack frames and local variables
      - Optimize bytecode with peephole patterns like constant folding
      - Debug bytecode output with disassemblers and execution traces
      skills:
      - Code Generation
      - Stack-Based Architecture
      - Instruction Encoding
      - AST Traversal
      - Bytecode Optimization
      - Virtual Machine Design
      - Control Flow Analysis
      - Symbol Resolution
      tags:
      - advanced
      - codegen
      - compilers
      - data-structures
      - ir
      - java
      - optimization
      - parsing
      - python
      - rust
      - vm
      architecture_doc: architecture-docs/bytecode-compiler/index.md
      languages:
        recommended:
        - Python
        - Java
        - Rust
        also_possible:
        - C
        - Go
        - TypeScript
      resources:
      - type: book
        name: Crafting Interpreters - Compiling Expressions
        url: https://craftinginterpreters.com/compiling-expressions.html
      - type: article
        name: A Python Interpreter Written in Python
        url: https://aosabook.org/en/500L/a-python-interpreter-written-in-python.html
      prerequisites:
      - type: skill
        name: AST builder
      - type: skill
        name: Bytecode VM
      - type: skill
        name: Stack-based execution
      milestones:
      - id: bytecode-compiler-m1
        name: Expression Compilation
        description: Compile arithmetic and boolean expressions to bytecode.
        acceptance_criteria:
        - Literal values are stored in constant pool and loaded via index-based instruction
        - Binary operations emit left operand, right operand, then operator instruction in order
        - Unary negation compiles to operand push followed by negate instruction
        - Compiled bytecode for expressions evaluates to same result as source expression
        pitfalls:
        - Wrong operand order for non-commutative ops
        - Constant pool indexing
        - Forgetting to handle all operators
        concepts:
        - Tree traversal
        - Stack-based code generation
        - Constant pools
        skills:
        - Recursive tree traversal
        - Stack machine programming
        - Bytecode instruction encoding
        - Expression evaluation
        deliverables:
        - Literal emission generating bytecode instructions for numeric and string constants
        - Binary operation compilation emitting opcodes for arithmetic and comparison operators
        - Unary operation compilation emitting opcodes for negation and logical not
        - Constant pool storing literal values referenced by bytecode load instructions
        estimated_hours: 4-6
      - id: bytecode-compiler-m2
        name: Variables and Assignment
        description: Compile variable declarations, assignments, and references.
        acceptance_criteria:
        - Variable declaration allocates named slot and optionally stores initial value
        - Assignment expression compiles to evaluate right side then store to variable slot
        - Variable read compiles to load instruction that pushes variable value onto stack
        - Inner scope variables shadow outer scope variables and are freed on scope exit
        pitfalls:
        - Scope lifetime management
        - Variable shadowing bugs
        - Uninitialized variables
        concepts:
        - Symbol tables
        - Variable resolution
        - Scope management
        skills:
        - Symbol table implementation
        - Lexical scoping
        - Variable lifetime management
        - Memory allocation
        deliverables:
        - Variable declaration compiling to stack slot allocation instruction
        - Variable assignment compiling to store instruction targeting named slot
        - Variable read compiling to load instruction from named stack slot
        - Scope management tracking variable visibility and stack depth per block
        estimated_hours: 5-8
      - id: bytecode-compiler-m3
        name: Control Flow
        description: Compile if statements, while loops, and logical operators.
        acceptance_criteria:
        - Conditional jump pops stack top and branches if value is falsy to target offset
        - If-else compiles to condition, conditional jump over then-block, then unconditional jump over else-block
        - While loop compiles to condition check, conditional exit jump, body, and backward loop jump
        - Forward jump targets are patched with correct offset after compilation of skipped block
        pitfalls:
        - Jump offset calculation
        - Forgetting to pop condition
        - Break/continue in nested loops
        concepts:
        - Jump patching
        - Backpatching
        - Short-circuit evaluation
        skills:
        - Control flow graph construction
        - Jump instruction generation
        - Conditional branching
        - Loop optimization
        deliverables:
        - Conditional jump instructions branching based on stack top boolean value
        - If-else compilation emitting conditional jump over then-branch to else-branch
        - While loop compilation emitting backward jump to loop condition check
        - Jump target patching resolving forward jump offsets after target is known
        estimated_hours: 6-10
      - id: bytecode-compiler-m4
        name: Functions
        description: Compile function definitions and calls.
        acceptance_criteria:
        - Function body compiles to separate bytecode chunk with its own constant pool
        - Call instruction creates new call frame with return address and local variable slots
        - Return instruction pops call frame and pushes return value onto caller stack
        - Arguments are accessible as local variables in called function frame
        pitfalls:
        - Frame pointer management
        - Argument count validation
        - Stack cleanup on return
        concepts:
        - Call frames
        - Parameter binding
        - Return addresses
        skills:
        - Calling convention design
        - Stack frame management
        - Function invocation protocols
        - Return value handling
        deliverables:
        - Function compilation generating separate bytecode chunk for function body
        - Call instruction pushing frame and transferring control to function bytecode
        - Return instruction popping frame and resuming caller execution context
        - Argument passing pushing arguments onto stack before call instruction
        estimated_hours: 10-16
    - id: wasm-emitter
      name: WebAssembly Emitter
      description: Emit .wasm from AST
      difficulty: advanced
      estimated_hours: 25-40
      essence: Stack-based bytecode generation targeting WebAssembly's binary module format, requiring manual encoding of type sections, function signatures, and instruction opcodes while maintaining structured control flow constraints.
      why_important: Building a WASM compiler teaches low-level code generation and binary format encoding‚Äîfundamental skills for systems programming, language implementation, and understanding how high-level code becomes executable machine instructions.
      learning_outcomes:
      - Implement binary module encoding with section headers, type signatures, and magic numbers
      - Design a code generator that maps abstract syntax trees to stack-based WASM instructions
      - Build structured control flow using WASM block, loop, and branch instructions
      - Debug binary format issues using tools like wasm-objdump and wasm-validate
      - Implement function compilation with local variables and parameter handling
      - Generate valid type sections mapping function signatures to type indices
      - Emit export sections to expose compiled functions to host environments
      - Validate generated modules against WebAssembly specification constraints
      skills:
      - Binary format encoding
      - Stack-based code generation
      - Compiler backends
      - Type system design
      - Bytecode emission
      - Control flow graphs
      - Module linking
      - Low-level debugging
      tags:
      - advanced
      - binary
      - binary-format
      - codegen
      - go
      - modules
      - rust
      - typescript
      - webassembly
      architecture_doc: architecture-docs/wasm-emitter/index.md
      languages:
        recommended:
        - Rust
        - Go
        - TypeScript
        also_possible:
        - Python
        - C
      resources:
      - type: specification
        name: WebAssembly Specification
        url: https://webassembly.github.io/spec/core/
      - type: tool
        name: WebAssembly Binary Toolkit
        url: https://github.com/WebAssembly/wabt
      prerequisites:
      - type: skill
        name: AST building
      - type: skill
        name: Binary formats
      - type: skill
        name: Stack machines
      milestones:
      - id: wasm-emitter-m1
        name: WASM Binary Format
        description: Understand and emit valid WASM module structure.
        acceptance_criteria:
        - Magic number 0x00 0x61 0x73 0x6D and version 1 header are emitted correctly
        - Section encoding writes section ID byte followed by LEB128-encoded content length
        - LEB128 integers encode both signed and unsigned values in variable-length format
        - Type section lists all function signatures used by the module
        pitfalls:
        - LEB128 edge cases
        - Section ordering
        - Size calculation
        concepts:
        - Binary formats
        - Variable-length encoding
        - Module structure
        skills:
        - Binary file format design
        - Low-level byte manipulation
        - WebAssembly module structure
        - Variable-length integer encoding
        deliverables:
        - Module structure emitter writing the top-level WASM binary module wrapper
        - Section encoding logic writing section IDs, sizes, and payloads in binary format
        - LEB128 encoding functions converting integers to variable-length binary representation
        - Type section emitter writing function signature definitions with parameter and return types
        estimated_hours: 5-8
      - id: wasm-emitter-m2
        name: Expression Compilation
        description: Compile arithmetic expressions to WASM instructions.
        acceptance_criteria:
        - i32 arithmetic operations produce correct results via WASM stack instructions
        - Stack-based code generation pushes operands and applies operators in correct order
        - Local variables are read and written using indexed local.get and local.set instructions
        - Constants emit correct i32.const or f64.const instructions with encoded immediate values
        pitfalls:
        - Operand order for non-commutative ops
        - Signed vs unsigned
        - Stack imbalance
        concepts:
        - Stack machines
        - Instruction encoding
        - Local variables
        skills:
        - Stack-based VM programming
        - Expression tree compilation
        - Type-aware code generation
        - Register-less computation
        deliverables:
        - Literal value emitter producing WASM const instructions for integer and float values
        - Binary operation compiler generating WASM i32.add, i32.sub, i32.mul instructions
        - Local variable compiler generating local.get and local.set instructions by index
        - Stack management logic ensuring operand stack depth is correct after each instruction
        estimated_hours: 5-8
      - id: wasm-emitter-m3
        name: Control Flow
        description: Compile if/else and loops to WASM structured control flow.
        acceptance_criteria:
        - Block and end structure correctly nests and terminates structured control flow
        - If, else, and end instructions implement two-branch conditional execution correctly
        - Loop with br_if generates a backward branch that repeats the loop body conditionally
        - Break to labels correctly targets the right enclosing block by depth index
        pitfalls:
        - Label depth calculation
        - Block type annotations
        - Unreachable code after br
        concepts:
        - Structured control flow
        - Label indices
        - Block nesting
        skills:
        - Structured control flow compilation
        - Label scope management
        - Branch target calculation
        - Loop and block construction
        deliverables:
        - Block and loop structure emitter generating WASM block and loop with matching end markers
        - If-else emitter generating WASM if, else, and end instructions for conditional branches
        - Branch instruction emitter generating br and br_if for jumping to enclosing labels
        - Return instruction emitter generating WASM return to exit the current function
        estimated_hours: 6-10
      - id: wasm-emitter-m4
        name: Functions and Exports
        description: Compile function definitions and export them.
        acceptance_criteria:
        - Function section maps each function index to its corresponding type signature
        - Code section format includes local count declarations followed by expression bytecode
        - Export section maps string names to function indices for host-callable entry points
        - Function calls emit the correct call instruction with the target function index
        pitfalls:
        - Parameter vs local indices
        - Body size encoding
        - Export name encoding
        concepts:
        - Module linking
        - Function ABI
        - Export mechanisms
        skills:
        - Function signature design
        - Module export systems
        - ABI design and implementation
        - Function body encoding
        deliverables:
        - Function type declaration emitter writing signatures to the type section
        - Function body emission writing local declarations and instruction sequences to the code section
        - Export section emitter making selected functions visible by name to the host environment
        - Import section emitter declaring functions provided by the host runtime environment
        estimated_hours: 9-14
    - id: build-linker
      name: Static Linker
      description: Multiple .o files to ELF executable with symbol resolution
      difficulty: advanced
      estimated_hours: 30-45
      essence: Symbol table construction, address relocation, and section merging to transform position-independent object code into a fixed-address executable binary with resolved cross-file references.
      why_important: Building a linker demystifies the final stage of compilation and reveals how high-level code becomes machine-executable programs, a foundational understanding for systems programming, toolchain development, and low-level debugging.
      learning_outcomes:
      - Parse ELF object file headers and extract section data
      - Implement symbol table merging with duplicate detection and resolution
      - Apply relocation entries to fix up instruction addresses and data references
      - Design section concatenation strategies for .text, .data, and .bss segments
      - Handle strong and weak symbol semantics across multiple object files
      - Generate ELF program headers mapping sections to loadable memory segments
      - Implement address layout algorithms assigning virtual memory addresses
      - Write well-formed ELF executables with correct entry points and segment permissions
      skills:
      - ELF Binary Format
      - Symbol Resolution
      - Address Relocation
      - Binary Parsing
      - Memory Layout Design
      - Object File Manipulation
      - System Programming
      - Toolchain Internals
      tags:
      - linker
      - elf
      - toolchain
      - symbol-resolution
      - binary-format
      architecture_doc: architecture-docs/build-linker/index.md
      languages:
        recommended:
        - C
        - Rust
        also_possible:
        - Go
      resources:
      - name: Linkers and Loaders (John Levine)
        url: https://www.iecc.com/linker/
        type: book
      - name: 'Ian Lance Taylor: Linkers'
        url: https://www.airs.com/blog/archives/38
        type: article
      prerequisites:
      - type: project
        name: elf-parser
      - type: project
        name: bytecode-compiler
      milestones:
      - id: build-linker-m1
        name: Section Merging
        description: Merge corresponding sections from multiple object files.
        acceptance_criteria:
        - Read multiple ELF .o files and identify matching section types (.text, .data, .bss, .rodata)
        - Concatenate same-name sections from different files maintaining alignment requirements
        - Track section-to-output mapping (which input section maps to which output offset)
        - Generate output section table with correct sizes and offsets
        pitfalls:
        - Sections may require alignment padding between merged inputs
        - .bss sections occupy no file space but need virtual address space allocation
        - Section flags (SHF_ALLOC, SHF_WRITE, SHF_EXECINSTR) must be consistent across merged sections
        concepts:
        - Section merging
        - Memory layout
        - Alignment requirements
        skills:
        - Binary file parsing
        - Section header manipulation
        - Memory alignment computation
        - ELF format specification
        deliverables:
        - Multi-file ELF reader loading section data from multiple .o files
        - Section merging with alignment-aware concatenation
        - Input-to-output offset mapping table for relocation processing
        - Output section table with correct sizes, offsets, and flags
        estimated_hours: 7-10
      - id: build-linker-m2
        name: Symbol Resolution
        description: Build a global symbol table resolving references across object files.
        acceptance_criteria:
        - Collect all global symbols from all input files into a unified symbol table
        - Detect and report undefined symbols that have no definition in any input file
        - Handle duplicate symbol definitions following strong/weak symbol rules
        - Resolve local symbols within their defining translation unit only
        pitfalls:
        - Multiple strong definitions of the same symbol must be an error
        - Weak symbols are overridden by strong symbols but valid when alone
        - COMMON symbols (uninitialized globals) have special merging rules based on size
        concepts:
        - Symbol resolution
        - Strong vs weak symbols
        - Translation units
        skills:
        - Symbol table construction
        - Hash table implementation
        - Name mangling handling
        - Duplicate symbol detection
        deliverables:
        - Global symbol table aggregating symbols from all input object files
        - Undefined symbol detection and error reporting
        - Strong/weak symbol resolution following ELF linking rules
        - Local symbol scoping restricted to defining translation unit
        estimated_hours: 7-10
      - id: build-linker-m3
        name: Relocations
        description: Apply relocations to fix up addresses in merged sections.
        acceptance_criteria:
        - Process R_X86_64_PC32 (relative) and R_X86_64_64 (absolute) relocation types
        - Calculate final symbol addresses from section base + offset in merged output
        - Patch relocation sites in section data with computed addresses
        - Handle addends from .rela sections correctly in address calculations
        pitfalls:
        - PC-relative relocations subtract the relocation site address from the symbol address
        - Addend is added to the computed address, not the symbol value alone
        - Truncation errors occur when 64-bit addresses don't fit in 32-bit relocation fields
        concepts:
        - Relocation types
        - Address patching
        - PC-relative addressing
        skills:
        - Address calculation
        - Bitwise operations for patching
        - Relocation entry processing
        - Offset arithmetic
        deliverables:
        - Relocation processor handling R_X86_64_PC32 and R_X86_64_64 types
        - Address calculation using merged section bases and symbol offsets
        - Section data patching with computed relocation values
        - Overflow detection for truncating relocations
        estimated_hours: 7-12
      - id: build-linker-m4
        name: Executable Generation
        description: Write a complete ELF executable with program headers.
        acceptance_criteria:
        - Generate valid ELF header with correct entry point address
        - Create program headers (PT_LOAD) for text and data segments with proper permissions
        - Set up proper page-aligned segment layout for virtual memory loading
        - Produced executable runs correctly on Linux without any external tools
        pitfalls:
        - PT_LOAD segments must be page-aligned (4096 bytes) for mmap loading
        - Entry point must point to _start symbol, not main (libc provides _start calling main)
        - Text segment needs PF_R|PF_X, data needs PF_R|PF_W, these must be separate PT_LOAD segments
        concepts:
        - ELF executable format
        - Program headers
        - Virtual memory layout
        skills:
        - Program header creation
        - Virtual address assignment
        - Segment permission setting
        - ELF file writing
        deliverables:
        - ELF executable writer producing valid header and program headers
        - Page-aligned segment layout for text and data
        - Entry point configuration pointing to _start symbol
        - Working executable that runs on Linux verified with simple test programs
        estimated_hours: 7-12
    expert:
    - id: build-interpreter
      name: Build Your Own Interpreter (Lox)
      description: Crafting Interpreters
      difficulty: expert
      estimated_hours: 40-80
      essence: Recursive descent parsing, abstract syntax tree traversal, lexical scanning with finite automata, and environment-based variable binding with lexical scoping rules.
      why_important: Building an interpreter from scratch demystifies how programming languages work internally and teaches foundational compiler theory that applies to debuggers, static analyzers, transpilers, and any tool that processes code.
      learning_outcomes:
      - Implement lexical scanning with regular expressions and finite state machines
      - Design abstract syntax tree representations using the Visitor pattern
      - Build a recursive descent parser that handles operator precedence and associativity
      - Implement environment chains for lexical scoping and closure capture
      - Design a tree-walking interpreter with runtime type checking
      - Build class-based object systems with method dispatch and inheritance
      - Implement function calls with stack-based activation records
      - Debug recursive algorithms and manage parser error recovery
      skills:
      - Lexical Analysis
      - Recursive Descent Parsing
      - AST Design
      - Tree-Walking Interpretation
      - Lexical Scoping
      - Object-Oriented Language Design
      - Visitor Pattern
      - Runtime Type Systems
      tags:
      - build-from-scratch
      - c
      - closures
      - compilers
      - evaluation
      - expert
      - java
      - rust
      - scoping
      - tree-walk
      architecture_doc: architecture-docs/build-interpreter/index.md
      languages:
        recommended:
        - Java
        - C
        - Rust
        - Go
        also_possible: []
      resources:
      - name: Crafting Interpreters (Free Online Book)
        url: https://craftinginterpreters.com
        type: book
      - name: Writing An Interpreter In Go
        url: https://interpreterbook.com
        type: book
      - name: CodeCrafters Interpreter Challenge
        url: https://app.codecrafters.io/courses/interpreter/overview
        type: tool
      prerequisites:
      - type: skill
        name: Basic programming language concepts
      - type: skill
        name: Recursion
      - type: skill
        name: Tree data structures
      - type: skill
        name: Object-oriented programming
      milestones:
      - id: build-interpreter-m1
        name: Scanner (Lexer)
        description: Build a scanner that converts Lox source code into tokens. Chapter 4 of Crafting Interpreters.
        acceptance_criteria:
        - Scanner recognizes all language tokens including keywords (var, fun, if, else, while, for, return, class, etc.)
        - String literals with double quotes and number literals with optional decimal points are correctly tokenized
        - Line numbers are tracked and reported accurately for each token to support meaningful error messages
        - Whitespace and comments (single-line // and optionally block /* */) are consumed and ignored between tokens
        - Lexical errors such as unterminated strings or unrecognized characters are reported with their source location
        pitfalls:
        - Confusing = and ==
        - Not handling unterminated strings
        - Newlines inside strings
        concepts:
        - Lexical analysis
        - Token representation
        - Error handling
        skills:
        - Finite state machines
        - Character-by-character processing
        - Token stream generation
        - Lexer error reporting
        deliverables:
        - Character stream to token stream converter that reads source code and emits a sequence of typed tokens
        - Token type definitions for keywords, identifiers, string literals, number literals, and operators
        - Line and column tracking that records the source position of each token for error reporting
        - String and number literal scanner that handles quoted strings with escape sequences and decimal numbers
        estimated_hours: 2-3
      - id: build-interpreter-m2
        name: Representing Code (AST)
        description: Define the abstract syntax tree classes for Lox. Chapter 5.
        acceptance_criteria:
        - Expression node classes cover binary, unary, grouping, and literal expressions with correct field definitions
        - Pretty-printer using the visitor pattern outputs a readable S-expression like (* (+ 1 2) 3) for nested expressions
        - Statement node classes cover print, expression-statement, and variable declaration with correct child references
        pitfalls:
        - Visitor pattern boilerplate
        - Mutable vs immutable AST nodes
        - Parent/child references creating cycles
        concepts:
        - Abstract Syntax Trees
        - Visitor pattern
        - Expression vs Statement
        skills:
        - Design patterns (Visitor)
        - Type-safe tree structures
        - Code generation from specifications
        - Metaprogramming
        deliverables:
        - Expression AST node classes for binary, unary, grouping, literal, variable, assign, call, and logical expressions
        - Statement AST node classes for print, expression, variable declaration, block, if, while, function, return, and class
        - Visitor pattern implementation that dispatches to type-specific visit methods for each AST node kind
        - Pretty printer that formats the AST as a parenthesized S-expression string for debugging and testing
        estimated_hours: 2-3
      - id: build-interpreter-m3
        name: Parsing Expressions
        description: Build a recursive descent parser for expressions. Chapter 6.
        acceptance_criteria:
        - 'Arithmetic expressions are parsed with correct precedence: * and / bind tighter than + and -'
        - Parenthesized sub-expressions override default precedence and are correctly nested in the AST
        - Comparison (<, >, <=, >=) and equality (==, !=) operators are parsed at their correct precedence levels
        - Syntax errors report the unexpected token, its line number, and a description of what was expected
        pitfalls:
        - Left recursion causes infinite loop
        - Forgetting closing paren
        - Error recovery
        concepts:
        - Recursive descent parsing
        - Operator precedence
        - Error recovery
        skills:
        - Recursive algorithms
        - Grammar-based parsing
        - Parse error recovery
        - Operator precedence handling
        deliverables:
        - Recursive descent expression parser with one function per precedence level of the grammar
        - Operator precedence handling that correctly nests binary operations according to language rules
        - Grouping expression parsing that handles parenthesized sub-expressions for explicit precedence control
        - Unary and binary expression node construction with correct operator token and operand references
        estimated_hours: 3-4
      - id: build-interpreter-m4
        name: Evaluating Expressions
        description: Build a tree-walking interpreter to evaluate expressions. Chapter 7.
        acceptance_criteria:
        - Arithmetic operations (+, -, *, /) evaluate correctly on numeric operands and return numeric results
        - Unary minus negates a number and unary ! inverts a boolean truthiness value
        - 'Truthiness rules are implemented: nil and false are falsy, all other values are truthy'
        - String concatenation with the + operator joins two string operands into a new combined string
        - Runtime errors are raised with a message when operators are applied to incompatible types (e.g., 'hello' - 1)
        pitfalls:
        - Division by zero
        - String + non-string
        - Java null vs Lox nil
        concepts:
        - Tree-walking interpretation
        - Dynamic typing
        - Runtime type checking
        skills:
        - Tree traversal algorithms
        - Runtime type checking
        - Dynamic dispatch
        - Arithmetic expression evaluation
        deliverables:
        - Tree-walking evaluator that recursively visits each AST node and computes a runtime value
        - Runtime value representation supporting numbers, strings, booleans, nil, and callable objects
        - Runtime type checking that validates operand types before performing each operation
        - Operator semantics implementation defining behavior for arithmetic, comparison, equality, and string operations
        estimated_hours: 2-3
      - id: build-interpreter-m5
        name: Statements and State
        description: Add statements, variables, and assignment. Chapter 8.
        acceptance_criteria:
        - Print statement evaluates its expression and writes the string representation to stdout
        - Variable declarations with var create a new binding initialized to the expression value or nil if omitted
        - Assignment expressions update the value of an existing variable binding in the nearest enclosing scope
        - Expression statements evaluate their expression for side effects and discard the result
        - Environment stores variable bindings and supports nested scopes with parent-chain lookup for variables
        pitfalls:
        - Using undeclared variable
        - Assignment to non-variable
        - Scoping issues
        concepts:
        - Statement vs Expression
        - Environment and bindings
        - Assignment as expression
        skills:
        - Symbol tables
        - Variable scoping
        - Environment management
        - State mutation
        deliverables:
        - Print statement that evaluates its expression argument and outputs the result to stdout with a newline
        - Variable declaration statement that binds a name to an optional initializer expression value
        - Environment data structure that stores variable name-to-value bindings for the current scope
        - Block statement that creates a new nested environment scope for the enclosed statements
        estimated_hours: 2-3
      - id: build-interpreter-m6
        name: Control Flow
        description: Add if, while, for, and logical operators. Chapter 9.
        acceptance_criteria:
        - If/else executes the then-branch when the condition is truthy and the else-branch (if present) otherwise
        - While loops repeat the body until the condition evaluates to a falsy value and then continue past the loop
        - For loops are desugared to a block containing the initializer, a while loop with the condition, and the increment
        - Logical and returns the left operand if falsy (short-circuit), otherwise evaluates and returns the right operand
        pitfalls:
        - Dangling else
        - Forgetting short-circuit
        - Infinite loops without timeout
        concepts:
        - Conditional execution
        - Loop desugaring
        - Short-circuit evaluation
        skills:
        - Control flow implementation
        - Boolean logic optimization
        - Loop transformation
        - AST desugaring
        deliverables:
        - If/else statement execution that evaluates the condition and runs the matching branch body
        - While loop execution that repeats the body statement as long as the condition evaluates to truthy
        - For loop desugaring that transforms for(init; cond; incr) into an equivalent while loop structure
        - Logical operators (and, or) with short-circuit evaluation that skip the right operand when determined
        estimated_hours: 2-3
      - id: build-interpreter-m7
        name: Functions
        description: Add function declarations and calls. Chapter 10.
        acceptance_criteria:
        - The fun keyword declares a named function and stores it as a callable value in the enclosing environment
        - Parameters are bound to argument values positionally and accessible as local variables within the function body
        - Return statement exits the function immediately and the returned value becomes the call expression's result
        - Functions are first-class values that can be stored in variables, passed as arguments, and returned from other functions
        - Recursive calls work correctly with each invocation getting its own environment and returning to the correct caller
        pitfalls:
        - Stack overflow from infinite recursion
        - Not restoring environment after call
        - Return outside function
        concepts:
        - First-class functions
        - Call frames
        - Return values
        skills:
        - Call stack management
        - Function activation records
        - Return value propagation
        - Recursion handling
        deliverables:
        - Function declaration using the fun keyword that binds a callable object to a name in the current scope
        - Function call execution that evaluates arguments, binds them to parameters, and runs the body in a new scope
        - Return statement that immediately exits the function body and delivers a value to the call site
        - Local variable scoping so each function call gets its own environment for parameter and local variable bindings
        estimated_hours: 3-4
      - id: build-interpreter-m8
        name: Closures
        description: Implement lexical scoping and closures. Chapter 11.
        acceptance_criteria:
        - Functions capture the enclosing environment at definition time and retain access to outer variables
        - Nested functions correctly access and modify variables from their enclosing function's scope
        - Closures persist and remain usable after the enclosing function has returned and its scope would normally be gone
        - Variable resolution at compile time determines the correct scope depth for each variable reference in closures
        pitfalls:
        - Capturing variable vs capturing value
        - Variable resolution with shadowing
        - This in closures
        concepts:
        - Lexical scoping
        - Closures
        - Environment chains
        skills:
        - Closure implementation
        - Lexical scope resolution
        - Environment capture
        - Variable lifetime management
        deliverables:
        - Closure implementation that captures a reference to the enclosing environment at function definition time
        - Free variable resolution that binds references to variables defined in outer scopes at compile or definition time
        - Closure as a first-class value that can be passed to functions, returned, and stored in data structures
        - Nested function support where inner functions close over variables from any enclosing function scope
        estimated_hours: 4-6
      - id: build-interpreter-m9
        name: Classes
        description: Add class declarations, instances, and methods. Chapter 12.
        acceptance_criteria:
        - The class keyword declares a named class and stores it as a callable class object in the environment
        - Calling ClassName() creates a new instance of the class and invokes the init() initializer if defined
        - Properties are accessed and assigned using dot notation (instance.field) on class instances
        - Methods defined inside the class body are callable on instances with 'this' bound to the receiver object
        - The init() initializer method is automatically called on instance creation and receives constructor arguments
        pitfalls:
        - this outside method
        - Returning from init()
        - Method vs function
        concepts:
        - Classes and instances
        - This binding
        - Constructors
        skills:
        - Object-oriented programming implementation
        - Instance method dispatch
        - Constructor patterns
        - Runtime object representation
        deliverables:
        - Class declaration that creates a class object with a name, methods, and optional initializer
        - Instance creation by calling the class name as a constructor function that returns a new object
        - Property get and set operations using dot notation on instance objects for field access and assignment
        - Method definition with implicit 'this' binding that provides access to the instance within method bodies
        estimated_hours: 4-5
      - id: build-interpreter-m10
        name: Inheritance
        description: Add class inheritance and super calls. Chapter 13.
        acceptance_criteria:
        - Syntax 'class Derived < Base' creates a subclass that inherits all methods from the superclass
        - Methods defined on the superclass are callable on instances of the derived class without redefinition
        - super.method() inside a subclass method calls the superclass version of that method, not the overridden one
        - Method resolution looks up the class hierarchy from the instance's class to its superclass chain in order
        pitfalls:
        - super outside class
        - Inheriting from non-class
        - Diamond inheritance
        concepts:
        - Single inheritance
        - Super calls
        - Method resolution
        skills:
        - Inheritance mechanism implementation
        - Method lookup chains
        - Super method resolution
        - Class hierarchy management
        deliverables:
        - Superclass specification using 'class Derived < Base' syntax in the class declaration
        - Method inheritance that allows instances of the derived class to call methods defined on the base class
        - Super keyword that invokes a method on the superclass, bypassing the subclass override if present
        - Initializer chaining where the derived class init calls super.init() to run the base class constructor
        estimated_hours: 3-4
    - id: build-gc
      name: Build Your Own Garbage Collector
      description: Memory management
      difficulty: expert
      estimated_hours: 40-60
      essence: Automatic memory reclamation through reachability analysis and object graph traversal, implementing mark-sweep algorithms, tri-color invariants for incremental collection, and generational heaps with write barriers to track inter-generational references while minimizing application pause times through concurrent marking strategies.
      why_important: Building a garbage collector teaches you low-level memory management, pointer manipulation, and algorithmic optimization that are critical for systems programming, runtime development, and understanding performance characteristics of managed languages.
      learning_outcomes:
      - Implement mark-sweep collection with root set identification and object graph traversal
      - Design tri-color marking invariants for incremental collection without stop-the-world pauses
      - Build generational heaps with nursery promotion and remembered set tracking
      - Debug memory corruption issues using pointer analysis and heap visualization
      - Optimize allocation strategies with bump-pointer allocation and free-list management
      - Implement write barriers for tracking inter-generational references
      - Design concurrent marking algorithms with safe points and memory barriers
      - Measure and profile GC pause times, throughput, and memory fragmentation
      skills:
      - Memory Management
      - Pointer Arithmetic
      - Graph Traversal Algorithms
      - Concurrent Programming
      - Performance Profiling
      - Low-level Debugging
      - Systems Programming
      - Memory Barriers
      tags:
      - build-from-scratch
      - c
      - expert
      - garbage-collection
      - mark-sweep
      - reference-counting
      - rust
      - zig
      architecture_doc: architecture-docs/build-gc/index.md
      languages:
        recommended:
        - C
        - Rust
        - Zig
        also_possible: []
      resources:
      - type: book
        name: The Garbage Collection Handbook
        url: https://gchandbook.org/
      - type: article
        name: Baby's First Garbage Collector
        url: https://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/
      - type: article
        name: Writing a Simple GC in C
        url: https://maplant.com/gc.html
      prerequisites:
      - type: skill
        name: Memory management
      - type: skill
        name: C/Rust
      - type: skill
        name: Data structures
      - type: skill
        name: Graph algorithms
      milestones:
      - id: build-gc-m1
        name: Mark-Sweep Collector
        description: Implement basic mark-sweep garbage collection.
        acceptance_criteria:
        - Object allocation returns memory from the managed heap and registers the object for garbage collection tracking
        - Root set identification correctly enumerates all pointers from the stack, global variables, and CPU registers
        - Mark phase performs a depth-first traversal and marks every object reachable from the root set as live
        - Sweep phase iterates the entire heap and reclaims all unmarked objects by adding them to the free list
        - Total memory reclaimed after a collection equals the sum of all unreachable object sizes in the heap
        pitfalls:
        - Missing roots
        - Marking already marked objects (infinite loop)
        - Not resetting marks
        concepts:
        - Reachability
        - Graph traversal
        - Memory reclamation
        skills:
        - Graph traversal algorithms
        - Manual memory management
        - Pointer manipulation and validation
        - Root set identification
        deliverables:
        - Root set identification that locates all GC roots from the stack, global variables, and registers
        - Mark phase that performs depth-first traversal of the object graph starting from all identified roots
        - Sweep phase that scans the heap and collects all unmarked objects into the free list for reuse
        - Free list management that maintains a linked list of reclaimed memory blocks for future allocations
        estimated_hours: 10-15
      - id: build-gc-m2
        name: Tri-color Marking
        description: Implement tri-color invariant for incremental GC.
        acceptance_criteria:
        - Objects transition correctly through white (unvisited), gray (discovered), and black (fully scanned) states
        - Worklist-based marking processes gray objects one at a time and transitions them to black after scanning their fields
        - Incremental marking can pause and resume between mutator execution phases without missing live objects
        - Write barriers intercept pointer updates and re-gray the affected objects to prevent premature collection
        pitfalls:
        - Write barrier missing
        - Violating tri-color invariant
        - Race conditions
        concepts:
        - Incremental GC
        - Write barriers
        - Tri-color abstraction
        skills:
        - Concurrent data structure design
        - Memory barrier implementation
        - Write barrier instrumentation
        - Incremental algorithm design
        deliverables:
        - White/gray/black tri-color abstraction where white is unvisited, gray is discovered, and black is fully scanned
        - Worklist-based marking that processes gray objects by scanning their references and turning them black
        - Write barrier that intercepts pointer stores during incremental marking to maintain the tri-color invariant
        - Termination condition that halts marking when the gray worklist is empty and all live objects are black
        estimated_hours: 12-18
      - id: build-gc-m3
        name: Generational Collection
        description: Implement generational GC with nursery and old generation.
        acceptance_criteria:
        - Young generation (nursery) allocates new objects and is collected frequently with low pause times
        - Old generation holds long-lived objects and is collected less frequently using a full mark-sweep or mark-compact
        - Promotion policy moves surviving objects to the old generation after a configurable tenuring threshold
        - Remembered set records cross-generational pointers so minor collections can find young-gen objects referenced from old-gen
        - Minor collections run significantly faster than major collections by only scanning the young generation plus remembered set
        pitfalls:
        - Missing remembered set entries
        - Promoting too early/late
        - Write barrier overhead
        concepts:
        - Generational hypothesis
        - Copying collection
        - Inter-generational pointers
        skills:
        - Object copying and relocation
        - Multi-region memory management
        - Cross-generational reference tracking
        - Age-based promotion strategies
        deliverables:
        - Young and old generation heap separation with independently collectible memory regions
        - Minor collection that scans only the young generation for fast, frequent garbage collection cycles
        - Object promotion policy that moves objects surviving a configurable number of minor collections to the old generation
        - Remembered set that tracks pointers from old-generation objects into the young generation for minor collection roots
        estimated_hours: 15-20
      - id: build-gc-m4
        name: Concurrent Collection
        description: Add concurrent marking for reduced pause times.
        acceptance_criteria:
        - Concurrent marking thread traces live objects in parallel with application execution without corruption
        - GC handshakes and safepoints synchronize the collector and mutator threads at well-defined safe points
        - SATB or incremental update write barrier ensures no live object is missed during concurrent marking
        - Concurrent sweep reclaims dead objects while the application continues running with minimal pause time
        pitfalls:
        - Data races
        - Missing SATB entries
        - Long pauses during STW phases
        concepts:
        - Concurrent algorithms
        - SATB/incremental update
        - Safepoints
        skills:
        - Thread synchronization primitives
        - Lock-free algorithm design
        - Stop-the-world coordination
        - Concurrent marking protocols
        - Memory fence operations
        deliverables:
        - Concurrent marking thread that traces the object graph while the application (mutator) continues to run
        - Snapshot-at-the-beginning (SATB) or incremental update barrier to maintain correctness during concurrent tracing
        - Safe point mechanism that coordinates between the GC thread and mutator threads for handshake synchronization
        - Low-pause concurrent sweep that reclaims memory without stopping the mutator threads for extended periods
        estimated_hours: 15-20
    - id: build-regex
      name: Build Your Own Regex Engine
      description: NFA/DFA, Thompson construction
      difficulty: expert
      estimated_hours: 40-60
      essence: Converting regular expression patterns into executable state machines (NFA/DFA) through formal language theory, handling epsilon transitions, non-determinism, and the powerset construction algorithm for deterministic optimization.
      why_important: Building a regex engine exposes you to compiler theory fundamentals (lexing, parsing, AST construction) and automata theory in a practical context, skills directly applicable to building parsers, lexers, and understanding how production regex libraries achieve linear-time matching guarantees.
      learning_outcomes:
      - Implement recursive descent or parser combinators to convert regex syntax into an Abstract Syntax Tree
      - Design Thompson's construction algorithm to transform AST nodes into epsilon-NFA fragments with correct precedence handling
      - Build NFA simulation engine using backtracking or parallel state tracking for pattern matching
      - Implement the powerset construction algorithm to convert NFA to DFA with state minimization
      - Debug non-determinism in state transitions and epsilon closure computation
      - Optimize DFA state tables using techniques like state minimization and transition caching
      - Handle operator precedence, special characters, and escape sequences in regex parsing
      - Measure and compare time complexity differences between NFA simulation and DFA execution
      skills:
      - Automata Theory
      - Compiler Construction
      - State Machine Design
      - Algorithm Optimization
      - Graph Algorithms
      - Recursive Parsing
      - Time Complexity Analysis
      - Pattern Matching
      tags:
      - automata
      - build-from-scratch
      - c
      - dfa
      - expert
      - game-dev
      - go
      - nfa
      - pattern-matching
      - rust
      architecture_doc: architecture-docs/build-regex/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        - Python
        also_possible: []
      resources:
      - type: article
        name: Regular Expression Matching
        url: https://swtch.com/~rsc/regexp/regexp1.html
      - type: book
        name: Introduction to Automata Theory
        url: https://www.amazon.com/Introduction-Automata-Theory-Languages-Computation/dp/0321455363
      prerequisites:
      - type: skill
        name: Automata theory
      - type: skill
        name: Graph algorithms
      - type: skill
        name: Recursion
      - type: skill
        name: C/Rust/Go
      milestones:
      - id: build-regex-m1
        name: Regex Parser
        description: Parse regex pattern into AST.
        acceptance_criteria:
        - Parser correctly handles literal character matching in patterns
        - Alternation operator splits pattern into two alternative branches
        - Concatenation implicitly chains adjacent pattern elements in sequence
        - Quantifiers *, +, and ? are parsed with correct binding to preceding element
        - Parenthesized groups override default operator precedence in pattern parsing
        - Character classes like [a-z] are parsed into sets of matching characters
        pitfalls:
        - Operator precedence wrong
        - Escape handling
        - Empty alternation branches
        concepts:
        - Parsing
        - AST construction
        - Regex syntax
        skills:
        - Recursive descent parsing
        - Abstract syntax tree design
        - String tokenization
        - Grammar rule implementation
        deliverables:
        - Tokenizer splitting regex pattern into operator and literal tokens
        - Recursive descent parser building abstract syntax tree from tokens
        - AST node types for literal, concatenation, alternation, and quantifiers
        - Escape sequence handling for special characters and character classes
        estimated_hours: 8-12
      - id: build-regex-m2
        name: Thompson's Construction (NFA)
        description: Convert regex AST to NFA using Thompson's construction.
        acceptance_criteria:
        - NFA states store transitions as maps from characters to destination state sets
        - Epsilon transitions connect states without requiring input character consumption
        - Thompson's construction produces correct NFA for each AST node type
        - Composed NFA correctly matches strings accepted by the original regex pattern
        pitfalls:
        - Wrong epsilon connections
        - Not marking accept state
        - Memory leaks from circular refs
        concepts:
        - NFAs
        - Thompson's construction
        - Epsilon transitions
        skills:
        - Graph construction algorithms
        - State machine design
        - Epsilon transition handling
        - Memory management for graph structures
        deliverables:
        - NFA state representation with labeled transitions and epsilon moves
        - Epsilon transition edges connecting states without consuming input
        - AST-to-NFA construction building automaton from parsed regex tree
        - NFA fragment composition for concatenation, alternation, and quantifiers
        estimated_hours: 10-15
      - id: build-regex-m3
        name: NFA Simulation
        description: Match strings using NFA simulation.
        acceptance_criteria:
        - Epsilon closure correctly computes full set of reachable states from starting set
        - Simulation tracks all possible NFA states simultaneously during input processing
        - Match returns true only when final state is reachable after consuming entire input
        - Simulation handles patterns with nested quantifiers without exponential blowup
        pitfalls:
        - Forgetting epsilon closure
        - Infinite loop on empty match
        - Wrong match boundaries
        concepts:
        - NFA simulation
        - State sets
        - Pattern matching
        skills:
        - Set operations and state tracking
        - Backtracking algorithms
        - String matching algorithms
        - Iterative state exploration
        deliverables:
        - Epsilon closure computation finding all states reachable via epsilon transitions
        - Simultaneous state tracking following all active NFA states in parallel
        - Match function accepting or rejecting input strings against NFA
        - Step-by-step simulation advancing state set on each input character
        estimated_hours: 8-12
      - id: build-regex-m4
        name: DFA Conversion & Optimization
        description: Convert NFA to DFA for faster matching.
        acceptance_criteria:
        - Subset construction produces DFA accepting exactly the same language as input NFA
        - DFA minimization reduces state count without changing accepted language
        - DFA execution processes each input character in constant time with single transition
        - Benchmarks demonstrate DFA is faster than NFA simulation for repeated matching
        pitfalls:
        - Exponential state blowup
        - Not handling dead states
        - Alphabet enumeration
        concepts:
        - Subset construction
        - DFA minimization
        - Lazy evaluation
        skills:
        - Powerset construction algorithm
        - State minimization techniques
        - Hash-based state deduplication
        - Performance optimization for automata
        deliverables:
        - Subset construction algorithm converting NFA to equivalent DFA
        - DFA state minimization merging equivalent states to reduce size
        - Optimized DFA execution matching input in linear time per character
        - Performance comparison benchmarking NFA simulation versus DFA execution
        estimated_hours: 12-18
    - id: build-jit
      name: JIT Compiler
      description: x86-64 JIT backend for bytecode VM
      difficulty: expert
      estimated_hours: 40-60
      essence: Runtime translation of bytecode to native x86-64 machine code with executable memory management, register allocation, and calling convention implementation for direct CPU execution without interpretation overhead.
      why_important: Building a JIT compiler provides deep understanding of the boundary between high-level languages and hardware, teaching you how production compilers, VMs (V8, JVM, PyPy), and game engines achieve performance through runtime code generation.
      learning_outcomes:
      - Implement x86-64 instruction encoding and executable memory allocation with proper page permissions
      - Design register allocation strategies for translating stack-based bytecode to register-based native code
      - Build function prologues and epilogues following System V AMD64 ABI calling conventions
      - Implement profiling counters and hot path detection for adaptive optimization
      - Debug machine code generation issues using disassemblers and runtime analysis
      - Manage instruction pointer transitions between interpreted and compiled code
      - Optimize bytecode patterns into efficient x86-64 instruction sequences
      - Handle inline caching and guard conditions for type-specialized code paths
      skills:
      - x86-64 Assembly
      - Executable Memory Management
      - Calling Conventions
      - Runtime Code Generation
      - Register Allocation
      - Binary Encoding
      - Performance Profiling
      - Low-level Debugging
      tags:
      - jit-compilation
      - x86-64
      - code-generation
      - runtime
      - compilers
      architecture_doc: architecture-docs/build-jit/index.md
      languages:
        recommended:
        - C
        - Rust
        - Zig
        also_possible: []
      resources:
      - name: Adventures in JIT Compilation
        url: https://eli.thegreenplace.net/2017/adventures-in-jit-compilation-part-1-an-interpreter/
        type: article
      - name: x86-64 Instruction Reference
        url: https://www.felixcloutier.com/x86/
        type: reference
      prerequisites:
      - type: project
        name: bytecode-vm
      - type: project
        name: bytecode-compiler
      milestones:
      - id: build-jit-m1
        name: Basic x86-64 Emitter
        description: Build a machine code emitter that generates x86-64 instructions into executable memory.
        acceptance_criteria:
        - Allocate executable memory using mmap with PROT_READ|PROT_WRITE|PROT_EXEC
        - Emit basic x86-64 instructions (mov, add, sub, ret) as raw bytes
        - Support register operands (rax, rbx, rcx, etc.) and immediate values
        - Generated code callable as a C function pointer returning correct results
        pitfalls:
        - x86-64 uses REX prefix for 64-bit operands (0x48 for most instructions)
        - ModR/M byte encoding determines register/memory operand layout
        - Some platforms require W^X policy (write then switch to execute, not both simultaneously)
        concepts:
        - x86-64 instruction encoding
        - Executable memory allocation
        - Machine code generation
        skills:
        - x86-64 Assembly Programming
        - Memory Protection Management
        - Binary Code Emission
        - Low-level Debugging
        deliverables:
        - Executable memory allocator using mmap with proper protection flags
        - x86-64 instruction encoder for mov, add, sub, cmp, ret
        - Register and immediate operand encoding with REX prefix handling
        - Function pointer casting and calling convention for generated code
        estimated_hours: 10-15
      - id: build-jit-m2
        name: Expression JIT
        description: JIT-compile arithmetic expressions from bytecode to native code.
        acceptance_criteria:
        - Translate bytecode arithmetic operations (ADD, SUB, MUL, DIV) to x86-64 instructions
        - Map VM stack operations to register allocation for common patterns
        - Handle comparison and conditional jump bytecodes with native cmp/jcc instructions
        - JIT-compiled expressions produce identical results to interpreter
        pitfalls:
        - Division requires dividend in rdx:rax (rdx=high, rax=low), must emit cqo before idiv
        - Conditional jumps need forward patching since target addresses aren't known during emission
        - Register spilling needed when expression depth exceeds available registers
        concepts:
        - Bytecode-to-native translation
        - Register allocation
        - Forward patching
        skills:
        - Runtime Code Generation
        - Register Management
        - Control Flow Patching
        - Expression Compilation
        deliverables:
        - Bytecode-to-x86-64 translator for arithmetic and comparison operations
        - Simple register allocator mapping stack slots to machine registers
        - Forward jump patching for conditional branches
        - Correctness test suite comparing JIT output to interpreter output
        estimated_hours: 10-15
      - id: build-jit-m3
        name: Function JIT and Calling Convention
        description: JIT-compile full functions with proper calling convention support.
        acceptance_criteria:
        - Generate function prologue/epilogue following System V AMD64 ABI
        - Support function calls between JIT-compiled and interpreted code
        - Handle local variables with stack frame allocation
        - Pass arguments and return values correctly through registers and stack
        pitfalls:
        - Stack must be 16-byte aligned before call instruction (ABI requirement)
        - Callee-saved registers (rbx, r12-r15, rbp) must be preserved across function calls
        - Mixing JIT and interpreted calls requires a trampoline to switch execution modes
        concepts:
        - Calling conventions
        - Stack frame management
        - ABI compliance
        skills:
        - x86-64 ABI Implementation
        - Stack Frame Construction
        - Function Call Optimization
        - Cross-mode Execution Bridging
        deliverables:
        - Function prologue/epilogue following System V AMD64 ABI
        - Stack frame allocation for local variables
        - Trampoline for JIT-to-interpreter and interpreter-to-JIT transitions
        - Correct argument passing via registers (rdi, rsi, rdx, rcx, r8, r9) and stack
        estimated_hours: 10-15
      - id: build-jit-m4
        name: Hot Path Detection and Optimization
        description: Add profiling-guided JIT compilation for frequently executed code paths.
        acceptance_criteria:
        - Count function/loop execution frequency to identify hot code
        - Trigger JIT compilation when execution count exceeds threshold
        - Apply basic optimizations (constant folding, dead code elimination) during JIT
        - Benchmark showing significant speedup for compute-intensive bytecode programs
        pitfalls:
        - Counter overhead must be minimal to avoid slowing down cold code
        - On-stack replacement (OSR) is complex, simpler to JIT whole functions on next call
        - Must invalidate JIT code when bytecode is modified (if supported)
        concepts:
        - Hot path detection
        - Tiered compilation
        - Runtime optimization
        skills:
        - Performance Profiling
        - Adaptive Optimization
        - Hot Loop Identification
        - Dynamic Recompilation
        deliverables:
        - Execution counter tracking per-function and per-loop invocation counts
        - Threshold-based JIT trigger replacing interpreter execution with native code
        - Basic optimization passes during JIT compilation
        - Benchmark suite demonstrating speedup on compute-intensive programs
        estimated_hours: 10-15
    - id: capstone-programming-language
      name: 'Capstone: Complete Programming Language'
      description: Build a complete statically-typed programming language from scratch ‚Äî lexer, parser, type checker, bytecode compiler, virtual machine, and garbage collector. This capstone integrates all compiler/language projects into one cohesive system.
      difficulty: expert
      estimated_hours: 80-120
      essence: Recursive descent parsing with lookahead, Hindley-Milner type inference unification, three-address code SSA transformation, stack-based bytecode interpretation, and tricolor mark-sweep garbage collection ‚Äî orchestrating lexical scanning, context-free grammar recognition, static type constraint solving, intermediate representation optimization, and automatic memory reclamation into a single executable pipeline.
      why_important: Building a complete language from scratch gives you the deepest possible understanding of how programming languages work ‚Äî from parsing to execution. This knowledge makes you better at debugging, optimizing, and designing APIs in any language.
      learning_outcomes:
      - Design and implement a complete language pipeline from source text to execution
      - Build a type system with inference, generics, and error recovery
      - Implement register allocation and basic optimization passes
      - Design a bytecode instruction set balancing simplicity and performance
      - Build a mark-and-sweep garbage collector integrated with the VM
      - Implement a standard library with I/O, collections, and string operations
      - Write comprehensive test suites covering parsing edge cases, type errors, and runtime behavior
      skills:
      - Lexical Analysis
      - Parsing
      - Type Systems
      - Code Generation
      - Virtual Machines
      - Garbage Collection
      - Optimization
      tags:
      - compilers
      - languages
      - capstone
      - systems-programming
      architecture_doc: architecture-docs/capstone-programming-language/index.md
      languages:
        recommended:
        - Rust
        - C
        - Go
        also_possible:
        - Java
        - Python
      resources:
      - name: Crafting Interpreters
        url: https://craftinginterpreters.com/
        type: book
      - name: Writing An Interpreter In Go
        url: https://interpreterbook.com/
        type: book
      - name: Engineering a Compiler (3rd Edition)
        url: https://www.elsevier.com/books/engineering-a-compiler/cooper/978-0-12-815412-0
        type: book
      prerequisites:
      - type: project
        id: tokenizer
        name: Tokenizer/Lexer
      - type: project
        id: ast-builder
        name: AST Builder
      - type: project
        id: type-checker
        name: Type Checker
      - type: project
        id: bytecode-compiler
        name: Bytecode Compiler
      - type: project
        id: bytecode-vm
        name: Bytecode VM
      - type: project
        id: simple-gc
        name: Simple GC
      milestones:
      - id: capstone-programming-language-m1
        name: Language Design & Lexer/Parser
        description: Design the language syntax and semantics. Implement a lexer and recursive descent parser that produces a typed AST with source location tracking and error recovery.
        acceptance_criteria:
        - 'Language spec document: types, control flow, functions, structs/classes, modules'
        - Lexer handles all token types including string interpolation and multiline strings
        - Parser produces typed AST with source spans for error reporting
        - 'Error recovery: parser reports multiple errors per file instead of stopping at first'
        pitfalls:
        - Ambiguous grammar requiring backtracking
        - Poor error messages from recursive descent
        - Operator precedence conflicts
        concepts:
        - language design
        - lexical analysis
        - recursive descent
        - error recovery
        - source locations
        skills:
        - Language design
        - Lexer
        - Parser
        - Error recovery
        deliverables:
        - Language spec, lexer, and error-recovering parser
        - Formal grammar specification in EBNF notation covering all language constructs
        - Lexer producing tokens with source location metadata for accurate error reporting
        - Panic mode error recovery allowing parser to continue after syntax errors and report multiple issues
        estimated_hours: '15'
      - id: capstone-programming-language-m2
        name: Type Checker & Semantic Analysis
        description: Implement type checking with type inference, generics, and comprehensive error reporting. Perform semantic analysis (scope resolution, use-before-define detection).
        acceptance_criteria:
        - Type inference for local variables (let x = 42 infers int)
        - Generic functions and types with constraint checking
        - Scope resolution with proper shadowing and use-before-define errors
        - Type error messages include expected vs actual types with source locations
        pitfalls:
        - Infinite types from recursive inference
        - Generic constraint solving complexity
        - Mutually recursive type definitions
        concepts:
        - type inference
        - unification
        - generics
        - scope analysis
        - symbol tables
        skills:
        - Type inference
        - Generic types
        - Semantic analysis
        deliverables:
        - Type checker with inference, generics, and semantic analysis
        - Hindley-Milner type inference engine unifying type variables and inferring function signatures
        - Generic type system supporting parametric polymorphism with constraint checking
        - Semantic analyzer detecting undefined variables, type mismatches, and unreachable code
        estimated_hours: '20'
      - id: capstone-programming-language-m3
        name: Bytecode Compiler & Optimization
        description: Compile the typed AST to bytecode with basic optimization passes (constant folding, dead code elimination, register allocation).
        acceptance_criteria:
        - Typed AST compiles to a well-defined bytecode instruction set
        - Constant folding evaluates compile-time expressions
        - Dead code elimination removes unreachable branches
        - Closures compile to captured variable environments
        pitfalls:
        - Incorrect optimization changing program semantics
        - Stack frame layout for closures
        - Debugging optimized code
        concepts:
        - bytecode design
        - constant folding
        - dead code elimination
        - closure conversion
        - SSA form
        skills:
        - Code generation
        - Optimization passes
        - Closure conversion
        deliverables:
        - Bytecode compiler with optimization passes
        - Constant folding pass evaluating compile-time expressions and propagating constants
        - Dead code elimination removing unreachable blocks and unused variables
        - Register allocator minimizing bytecode size using graph coloring or linear scan
        estimated_hours: '18'
      - id: capstone-programming-language-m4
        name: Virtual Machine & Garbage Collector
        description: Implement a stack-based VM executing the bytecode, with a mark-and-sweep garbage collector for automatic memory management.
        acceptance_criteria:
        - VM executes all bytecode instructions with correct semantics
        - Mark-and-sweep GC correctly identifies and frees unreachable objects
        - GC triggers automatically based on allocation pressure
        - Stack traces with source locations on runtime errors
        pitfalls:
        - GC pauses affecting responsiveness
        - Dangling references from incorrect root scanning
        - Stack overflow in deeply recursive programs
        concepts:
        - stack-based VM
        - instruction dispatch
        - mark-and-sweep
        - root scanning
        - write barriers
        skills:
        - VM implementation
        - Garbage collection
        - Memory management
        deliverables:
        - Bytecode VM with integrated garbage collector
        - Stack-based bytecode interpreter with computed goto or switch-based dispatch
        - Mark-and-sweep garbage collector tracing reachable objects from stack and global roots
        - Write barrier implementation tracking cross-generational references for incremental GC
        estimated_hours: '18'
      - id: capstone-programming-language-m5
        name: Standard Library & REPL
        description: Build a standard library (I/O, collections, strings, math) and an interactive REPL with tab completion and history.
        acceptance_criteria:
        - 'Standard library: file I/O, HashMap, ArrayList, String operations, math functions'
        - REPL with line editing, history, and tab completion
        - REPL supports multiline expressions and shows types of evaluated expressions
        - 'Self-hosting test: the language can run a non-trivial program (e.g., a simple calculator)'
        pitfalls:
        - Standard library performance vs simplicity tradeoff
        - REPL state management across evaluations
        concepts:
        - standard library design
        - FFI for I/O
        - REPL loop
        - self-hosting
        skills:
        - Standard library design
        - REPL
        - Self-hosting
        deliverables:
        - Standard library, REPL, and self-hosting demonstration
        - Standard library modules providing file I/O, collections, string manipulation, and math functions
        - Interactive REPL with readline integration supporting tab completion and command history
        - Self-hosting compiler written in the language itself, successfully compiling its own source code
        estimated_hours: '12'
- id: security
  name: Security
  icon: üîê
  subdomains:
  - name: Cryptography
  - name: Web Security
  projects:
    beginner:
    - id: hash-impl
      name: Hash Function
      description: SHA-256 from spec
      difficulty: beginner
      estimated_hours: 10-15
      essence: Merkle-Damg√•rd construction using iterative compression with bitwise logical functions (Ch, Maj, Œ£, œÉ), modular 32-bit addition, and message scheduling to produce collision-resistant fixed-length digests from arbitrary input data through repeated application of six logical functions and constants derived from prime cube roots.
      why_important: Building SHA-256 from scratch teaches low-level bit manipulation, cryptographic algorithm implementation, and how to translate formal mathematical specifications into working code‚Äîessential skills for security engineering and systems programming.
      learning_outcomes:
      - Implement message padding with length encoding to create 512-bit aligned blocks
      - Design message schedule generation using bitwise rotation and XOR operations
      - Build compression function with Ch, Maj, Œ£0, Œ£1, œÉ0, and œÉ1 logical functions
      - Apply modular 32-bit arithmetic for hash value accumulation
      - Translate NIST FIPS specification pseudocode into executable code
      - Debug bit-level operations using test vectors and intermediate value validation
      - Implement endianness conversion for cross-platform compatibility
      - Verify cryptographic correctness against standard test cases
      skills:
      - Bitwise Operations
      - Cryptographic Algorithms
      - Low-Level Programming
      - Specification Implementation
      - Modular Arithmetic
      - Binary Data Handling
      - Algorithm Verification
      - Security Primitives
      tags:
      - beginner-friendly
      - buckets
      - c
      - collision
      - hashing
      - implementation
      - javascript
      - python
      architecture_doc: architecture-docs/hash-impl/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - C
        also_possible:
        - Rust
        - Go
        - Java
      resources:
      - name: SHA-256 Step by Step
        url: https://blog.boot.dev/cryptography/how-sha-2-works-step-by-step-sha-256/
        type: tutorial
      - name: NIST SHA-256 Specification
        url: https://csrc.nist.gov/publications/detail/fips/180/4/final
        type: specification
      prerequisites:
      - type: skill
        name: Binary and hexadecimal representation
      - type: skill
        name: Bitwise operations
      - type: skill
        name: Basic understanding of cryptography
      milestones:
      - id: hash-impl-m1
        name: Message Preprocessing
        description: Implement message padding and parsing.
        acceptance_criteria:
        - Convert message to binary representation and append '1' padding bit
        - Append '1' bit followed by zeros to reach 448 mod 512 bit alignment
        - Pad with zeros to 448 mod 512 bits then append original length as 64-bit big-endian
        - Append original message length as 64-bit big-endian integer at block end
        - Parse padded message into array of 512-bit blocks for processing
        pitfalls:
        - Wrong bit/byte conversion
        - Endianness errors
        - Off-by-one in padding calculation
        concepts:
        - Bit padding
        - Message blocks
        - Binary representation
        skills:
        - Binary data manipulation
        - Bitwise operations
        - Padding algorithms
        - Byte-order conversion
        deliverables:
        - Padding to block size with '1' bit and zero-fill alignment
        - Length encoding appending original message length as 64-bit integer
        - Block parsing splitting padded message into 512-bit chunks
        - Endianness handling using big-endian byte ordering throughout
        estimated_hours: 2-3
      - id: hash-impl-m2
        name: Message Schedule
        description: Generate the message schedule from each 512-bit block.
        acceptance_criteria:
        - Parse 512-bit block into 16 initial 32-bit words in big-endian order
        - Extend schedule to 64 words using the SHA-256 schedule recurrence formula
        - Implement lower-sigma-0 and lower-sigma-1 functions using rotate and shift operations
        - Words stored as 32-bit unsigned integers with overflow masked to 32 bits
        pitfalls:
        - Not masking to 32 bits
        - Right rotate vs right shift
        - Wrong œÉ function parameters
        concepts:
        - Rotate operations
        - XOR operations
        - Word expansion
        skills:
        - Bitwise rotation and shifting
        - Logical operations (XOR, AND, OR)
        - 32-bit modular arithmetic
        - Array manipulation
        deliverables:
        - Word expansion from 16 initial words to 64 schedule entries
        - Schedule array construction using SHA-256 recurrence relation
        - Bit rotation functions implementing right-rotate for 32-bit words
        - XOR operations combining rotated and shifted values for sigma functions
        estimated_hours: 2-3
      - id: hash-impl-m3
        name: Compression Function
        description: Implement the main compression function.
        acceptance_criteria:
        - Initialize working variables (a through h) from current hash values per block
        - Execute 64 rounds of compression using schedule words and round constants
        - Implement upper-Sigma-0, upper-Sigma-1, Ch, and Maj functions using bitwise operations
        - Use correct K constants array with all 64 round constant values from specification
        - Update hash values by adding compressed working variables after all rounds complete
        pitfalls:
        - Mixing up Œ£ and œÉ functions
        - Wrong K constant values
        - Not masking intermediate results
        concepts:
        - Compression function
        - Round functions
        - Hash state
        skills:
        - Cryptographic round functions
        - State transformation
        - Working with lookup tables
        - Modular addition
        deliverables:
        - Working variables initialization from current hash state values
        - Round function iteration executing 64 compression rounds per block
        - Bitwise operations implementing Ch, Maj, Sigma-0, and Sigma-1 functions
        - Intermediate hash update adding compressed values to running hash state
        estimated_hours: 4-5
      - id: hash-impl-m4
        name: Final Hash Output
        description: Produce the final 256-bit hash output.
        acceptance_criteria:
        - Process all message blocks sequentially updating hash state after each block
        - Concatenate all eight final 32-bit hash values into 256-bit output
        - Output hash as 64-character lowercase hexadecimal string representation
        - Test output against NIST known test vectors for empty string and 'abc' inputs
        - Handle empty input correctly producing the standard SHA-256 empty hash value
        pitfalls:
        - Wrong hash for empty string
        - Endianness in output
        - Not resetting state between calls
        concepts:
        - Hash finalization
        - Test vectors
        - Hex encoding
        skills:
        - Hash output formatting
        - Hexadecimal encoding
        - State management and reset
        - Test-driven validation
        deliverables:
        - Final hash construction by concatenating all eight 32-bit hash values
        - Hex string formatting converting binary hash to 64-character hexadecimal output
        - Byte array output option for binary hash representation
        - Test vector verification against known SHA-256 reference values
        estimated_hours: 2-3
    - id: password-hashing
      name: Password Hashing
      description: bcrypt, salt
      difficulty: beginner
      estimated_hours: 4-6
      essence: Salted key derivation with iterative computational hardening to transform memorizable passwords into cryptographically strong hash digests while defending against rainbow tables, GPU-accelerated brute force, and timing side-channel attacks.
      why_important: Building this teaches you fundamental security engineering principles that apply across authentication systems, API security, and data protection‚Äîcritical knowledge for any production application handling user credentials.
      learning_outcomes:
      - Implement salt generation and storage for preventing rainbow table attacks
      - Build key stretching with PBKDF2 to increase computational cost of brute-force attempts
      - Design secure password verification flows with timing-attack resistance
      - Integrate bcrypt or Argon2id with proper work factor configuration
      - Understand cryptographic primitives like SHA-256 and their limitations for passwords
      - Debug common pitfalls like salt reuse, insufficient iterations, and plaintext storage
      - Compare memory-hard functions versus CPU-hard functions for password defense
      - Evaluate trade-offs between security strength and authentication latency
      skills:
      - Cryptographic Hashing
      - Salt Generation
      - Key Derivation Functions
      - Timing Attack Prevention
      - bcrypt/Argon2 Implementation
      - Password Security Design
      - Work Factor Tuning
      - Authentication Systems
      tags:
      - bcrypt
      - beginner-friendly
      - go
      - javascript
      - key-derivation
      - python
      - salt
      - security
      architecture_doc: architecture-docs/password-hashing/index.md
      languages:
        recommended:
        - Python
        - Go
        - JavaScript
        also_possible:
        - Java
        - C#
      resources:
      - name: How to Safely Store Passwords
        url: https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html
        type: documentation
      - name: Argon2 vs Bcrypt vs Scrypt vs PBKDF2 - Complete Guide
        url: https://guptadeepak.com/the-complete-guide-to-password-hashing-argon2-vs-bcrypt-vs-scrypt-vs-pbkdf2-2026/
        type: article
      - name: CrackStation - Salted Password Hashing
        url: https://crackstation.net/hashing-security.htm
        type: tutorial
      prerequisites:
      - type: skill
        name: Basic programming
      - type: skill
        name: Understanding of hashing
      milestones:
      - id: password-hashing-m1
        name: Basic Hashing with Salt
        description: Implement salted password hashing.
        acceptance_criteria:
        - Generate cryptographically random salt of at least 16 bytes
        - Hash password concatenated with salt using SHA-256
        - Store salt alongside hash for later verification
        - Verify password against stored salt and hash pair
        pitfalls:
        - Using predictable salt
        - Not storing salt
        - Timing attacks in comparison
        concepts:
        - Salting
        - Rainbow tables
        - Secure random generation
        skills:
        - Cryptographic random number generation
        - Secure password storage
        - Hash function implementation
        - Salt generation and storage
        deliverables:
        - Cryptographically secure random salt generation function
        - SHA-256 hash computation combining salt and password
        - Combined salt and hash storage in single output format
        - Password verification function comparing against stored hash
        estimated_hours: 1-2
      - id: password-hashing-m2
        name: Key Stretching
        description: Implement iterated hashing for slow verification.
        acceptance_criteria:
        - Apply multiple hash iterations to slow brute force attacks
        - Support configurable iteration count (minimum 100,000)
        - Implement PBKDF2 key derivation with HMAC-SHA256
        - Use constant-time comparison to prevent timing side channels
        pitfalls:
        - Too few iterations
        - Not using constant-time compare
        - iteration count not stored
        concepts:
        - Key stretching
        - PBKDF2
        - Timing attacks
        skills:
        - Iterative hashing implementation
        - Computational cost tuning
        - Constant-time comparison
        - Password verification timing
        deliverables:
        - PBKDF2-HMAC-SHA256 implementation with configurable parameters
        - Configurable iteration count stored with output
        - Derived key of configurable length in bytes
        - Constant-time comparison function preventing timing attacks
        estimated_hours: 1-2
      - id: password-hashing-m3
        name: Modern Password Hashing
        description: Implement or use bcrypt/Argon2.
        acceptance_criteria:
        - Understand and parse bcrypt output format including version and cost
        - Configure bcrypt work factor appropriate for current hardware
        - Optionally support Argon2id as memory-hard alternative
        - Implement migration strategy for upgrading legacy password hashes
        pitfalls:
        - Implementing crypto yourself
        - Too low work factor
        - Not planning for algorithm upgrades
        concepts:
        - Bcrypt
        - Argon2
        - Algorithm agility
        skills:
        - Using established crypto libraries
        - Algorithm versioning and migration
        - Security parameter selection
        - Production password management
        deliverables:
        - Bcrypt integration with configurable cost factor
        - Argon2id integration with memory and time parameters
        - Cost factor tuning utility measuring hash computation time
        - Migration support for upgrading old hash formats transparently
        estimated_hours: 1-2
    intermediate:
    - id: aes-impl
      name: AES Implementation
      description: Block cipher
      difficulty: intermediate
      estimated_hours: 15-25
      essence: Galois field GF(2^8) arithmetic, substitution-permutation network transformations, and cryptographic key scheduling that convert plaintext blocks into ciphertext through mathematically reversible operations resistant to cryptanalysis.
      why_important: Building AES from scratch demystifies the cryptographic primitives underlying modern secure communications, teaching you low-level bit manipulation, finite field mathematics, and the rigorous thinking required for security-critical code where a single implementation error can compromise an entire system.
      learning_outcomes:
      - Implement Galois field GF(2^8) addition and multiplication with irreducible polynomial reduction
      - Design and build the SubBytes, ShiftRows, MixColumns, and AddRoundKey transformations
      - Implement key expansion algorithms for AES-128, AES-192, and AES-256 variants
      - Build cipher modes of operation including ECB, CBC, CTR, and GCM
      - Debug timing-safe implementations that resist side-channel attacks
      - Validate correctness using NIST test vectors and standardized test suites
      - Understand the mathematical foundations of substitution-permutation networks
      - Analyze performance tradeoffs between lookup tables and on-the-fly computation
      skills:
      - Finite Field Arithmetic
      - Block Cipher Design
      - Bitwise Operations
      - Cryptographic Algorithms
      - Side-Channel Resistance
      - Standards Compliance
      - Low-Level Optimization
      - Security Implementation
      tags:
      - block-cipher
      - c
      - cryptography
      - galois-field
      - implementation
      - intermediate
      - modes
      - python
      - rust
      - security
      architecture_doc: architecture-docs/aes-impl/index.md
      languages:
        recommended:
        - C
        - Rust
        - Python
        also_possible:
        - Go
        - Java
      resources:
      - name: FIPS 197 (AES Spec)
        url: https://csrc.nist.gov/publications/detail/fips/197/final
        type: paper
      - name: A Stick Figure Guide to AES
        url: http://www.moserware.com/2009/09/stick-figure-guide-to-advanced.html
        type: article
      prerequisites:
      - type: skill
        name: Binary operations
      - type: skill
        name: Modular arithmetic
      - type: skill
        name: Basic cryptography concepts
      milestones:
      - id: aes-impl-m1
        name: Galois Field Arithmetic
        description: Implement GF(2^8) operations used in AES.
        acceptance_criteria:
        - Field addition correctly computes a XOR b for any two elements in GF(2^8)
        - Multiplication in GF(2^8) produces correct results verified against known test vectors
        - Multiplicative inverse lookup returns the correct inverse for every non-zero element in the field
        - Pre-computed S-box and inverse S-box tables match the values defined in the AES specification (FIPS 197)
        pitfalls:
        - Overflow handling
        - Wrong irreducible polynomial
        - Off-by-one in tables
        concepts:
        - Galois fields
        - Polynomial arithmetic
        - Lookup tables
        skills:
        - Finite field arithmetic
        - Bitwise operations
        - Lookup table optimization
        - Modular arithmetic
        deliverables:
        - GF(2^8) multiplication using the irreducible polynomial x^8+x^4+x^3+x+1 (0x11B)
        - Lookup tables for the S-box and inverse S-box precomputed from the multiplicative inverse in GF(2^8)
        - XOR-based field addition that operates as bitwise exclusive-or on byte operands
        - MixColumns multiplication implementation using xtime and reduction modulo the irreducible polynomial
        estimated_hours: 3-4
      - id: aes-impl-m2
        name: AES Core Operations
        description: Implement the four AES round operations.
        acceptance_criteria:
        - SubBytes replaces every byte in the 4x4 state matrix with its S-box substitution value
        - ShiftRows cyclically left-shifts row i by i positions in the state matrix
        - MixColumns multiplies each column of the state by the polynomial {03}x^3+{01}x^2+{01}x+{02} in GF(2^8)
        - AddRoundKey XORs each byte of the state with the corresponding byte of the expanded round key
        pitfalls:
        - State matrix orientation
        - ShiftRows direction
        - MixColumns coefficients
        concepts:
        - Substitution
        - Permutation
        - Diffusion
        skills:
        - State array manipulation
        - Matrix transformations
        - Bitwise substitution
        - Cryptographic primitives
        deliverables:
        - SubBytes transformation that substitutes each byte using the precomputed S-box lookup table
        - ShiftRows transformation that cyclically left-shifts each row of the state matrix
        - MixColumns transformation that multiplies each column by a fixed polynomial in GF(2^8)
        - AddRoundKey transformation that XORs the state matrix with the current round key
        estimated_hours: 4-6
      - id: aes-impl-m3
        name: Key Expansion
        description: Implement the key schedule for AES-128/192/256.
        acceptance_criteria:
        - Expanding a 128-bit key produces exactly 44 words (11 round keys of 4 words each)
        - RotWord and SubWord functions produce correct intermediate values verified against FIPS 197 examples
        - Round constants (Rcon) are correctly computed as successive powers of 2 in GF(2^8)
        - Key expansion supports AES-128, AES-192, and AES-256 key sizes with correct output lengths
        pitfalls:
        - Rcon indexing
        - Word vs byte ordering
        - Key length handling
        concepts:
        - Key schedule
        - Round constants
        - Key expansion
        skills:
        - Key scheduling algorithms
        - Recursive key generation
        - Bit rotation operations
        - Cryptographic key derivation
        deliverables:
        - Round constant (Rcon) generation producing the correct powers of x in GF(2^8) for each round
        - Key schedule algorithm that expands 128/192/256-bit keys into the required number of round keys
        - 'Correct number of round keys generated: 11 for AES-128, 13 for AES-192, and 15 for AES-256'
        - Word rotation (RotWord) and S-box application (SubWord) used within the key expansion process
        estimated_hours: 3-4
      - id: aes-impl-m4
        name: Encryption & Modes
        description: Complete encryption and implement cipher modes.
        acceptance_criteria:
        - Full AES encrypt function produces correct ciphertext matching NIST FIPS 197 test vectors
        - ECB mode encrypts each 16-byte block independently and produces deterministic output for same input
        - CBC mode XORs each plaintext block with the previous ciphertext block using the provided IV
        - PKCS7 padding is correctly applied so plaintext is extended to a multiple of 16 bytes before encryption
        pitfalls:
        - ECB mode vulnerabilities
        - IV reuse
        - Padding oracle attacks
        concepts:
        - Block cipher modes
        - CBC
        - Padding
        skills:
        - Block cipher implementation
        - IV generation and management
        - Padding scheme implementation
        - Cryptographic mode selection
        - Side-channel attack awareness
        deliverables:
        - Full AES encryption and decryption performing the correct number of rounds for the key size
        - ECB mode implementation that encrypts each block independently for reference and testing
        - CBC mode implementation with initialization vector XOR chaining between consecutive blocks
        - CTR mode implementation that encrypts a counter to produce a keystream for streaming encryption
        estimated_hours: 4-6
    - id: jwt-impl
      name: JWT Library
      description: Sign, verify
      difficulty: intermediate
      estimated_hours: 8-12
      essence: HMAC-SHA256 signature generation and verification using Base64URL-encoded JSON payloads, with time-based claim validation to ensure token authenticity and prevent tampering in stateless authentication systems.
      why_important: Authentication tokens are fundamental to modern API security and distributed systems. Building a JWT library from scratch teaches you cryptographic signing, secure token validation, and the security considerations that underpin most web authentication systems.
      learning_outcomes:
      - Implement Base64URL encoding and decoding for compact token representation
      - Design and build HMAC-SHA256 signature generation and verification
      - Implement time-based claim validation with exp, iat, and nbf checks
      - Build header and payload parsing with JSON serialization
      - Handle cryptographic key management for symmetric signing algorithms
      - Debug signature verification failures and timing attack vulnerabilities
      - Implement issuer (iss) and audience (aud) claim validation
      - Understand the security implications of algorithm confusion attacks
      skills:
      - HMAC Cryptography
      - Base64URL Encoding
      - Claims Validation
      - Token Authentication
      - Cryptographic Signatures
      - Time-based Security
      - JSON Serialization
      - Algorithm Verification
      tags:
      - claims
      - configuration
      - go
      - implementation
      - intermediate
      - javascript
      - python
      - security
      - signatures
      - tokens
      architecture_doc: architecture-docs/jwt-impl/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Go
        also_possible:
        - Java
        - Rust
      resources:
      - name: JWT Specification (RFC 7519)
        url: https://tools.ietf.org/html/rfc7519
        type: specification
      - name: JWT.io Debugger
        url: https://jwt.io/
        type: tool
      prerequisites:
      - type: skill
        name: JSON
      - type: skill
        name: Base64
      - type: skill
        name: HMAC basics
      milestones:
      - id: jwt-impl-m1
        name: JWT Structure
        description: Implement JWT encoding without signing.
        acceptance_criteria:
        - Header JSON contains the correct algorithm identifier and token type fields
        - Payload claims including iss, sub, aud, exp, and custom fields are encoded as valid JSON
        - Base64URL encoding produces URL-safe output without plus, slash, or equals padding characters
        - Final token follows the three-part header.payload.signature dot-separated format
        pitfalls:
        - Regular base64 vs base64url
        - Padding removal/addition
        - JSON key ordering
        concepts:
        - JWT structure
        - Base64url
        - Token format
        skills:
        - Base64 URL encoding/decoding
        - JSON serialization
        - Data structure manipulation
        - String encoding handling
        deliverables:
        - Header encoder that produces a JSON object with alg and typ fields for the JWT header
        - Payload encoder that serializes registered and custom claims into a JSON payload section
        - Base64URL encoder that converts binary data to URL-safe base64 without padding characters
        - Token assembler that concatenates the encoded header, payload, and signature with dot separators
        estimated_hours: 2-3
      - id: jwt-impl-m2
        name: HMAC Signing
        description: Implement HS256 signing and verification.
        acceptance_criteria:
        - Tokens signed with HMAC-SHA256 produce signatures matching RFC 7515 test vectors
        - Signature verification returns true only when the recomputed signature matches the token signature
        - Constant-time comparison is used for signature verification to prevent timing side-channel attacks
        - Tokens with invalid, truncated, or tampered signatures are correctly rejected with an error
        pitfalls:
        - Timing attacks in comparison
        - Algorithm confusion attacks
        - Key encoding
        concepts:
        - HMAC
        - Digital signatures
        - Token verification
        skills:
        - Cryptographic hashing (HMAC-SHA256)
        - Secure comparison operations
        - Secret key management
        - Digital signature verification
        deliverables:
        - HMAC-SHA256 implementation that computes a keyed hash over the signing input string
        - Signature generation module that signs the header.payload string using the secret key
        - Signature verification module that recomputes and compares the signature for a given token
        - Secret key handler that validates key length and securely stores the signing key in memory
        estimated_hours: 2-3
      - id: jwt-impl-m3
        name: Claims Validation
        description: Implement standard JWT claim validation.
        acceptance_criteria:
        - Tokens with an exp claim in the past are rejected as expired with appropriate error
        - Tokens with an nbf claim in the future are rejected as not-yet-valid with appropriate error
        - The iat claim is validated to ensure it is not unreasonably far in the past or future
        - The iss claim is verified against a configured allowlist and rejects unknown issuers
        - The aud claim is validated to ensure it matches the expected audience identifier
        pitfalls:
        - Clock skew handling
        - Audience as list
        - Missing required claims
        concepts:
        - JWT claims
        - Token validation
        - Time-based security
        skills:
        - Timestamp validation
        - Expiration time checking
        - Clock skew tolerance implementation
        - Claim presence verification
        deliverables:
        - Expiration check that rejects tokens whose exp claim is in the past
        - Not-before check that rejects tokens whose nbf claim is in the future
        - Issuer validation that verifies the iss claim matches a list of allowed issuers
        - Custom claims extractor that parses and returns application-specific payload fields
        estimated_hours: 2-3
    - id: session-management
      name: Session Management
      description: Secure session handling with tokens and cookies
      difficulty: advanced
      estimated_hours: '30'
      essence: Cryptographically secure session ID generation using CSPRNGs with 128+ bit entropy, distributed state synchronization across Redis clusters for horizontal scalability, and defense against session fixation and hijacking through token rotation, secure cookie attributes (HttpOnly, Secure, SameSite), and proper session lifecycle management across concurrent devices.
      why_important: Session management is critical infrastructure for authentication systems in production applications, and building this teaches you distributed systems concepts, security hardening, and how to handle stateful data at scale across multiple servers.
      learning_outcomes:
      - Implement cryptographically secure session ID generation using CSPRNGs with 128+ bit entropy
      - Design distributed session storage with Redis or similar backends for horizontal scalability
      - Build secure cookie handling with HttpOnly, Secure, SameSite, and __Host prefix attributes
      - Implement session fixation prevention through ID regeneration after authentication state changes
      - Handle concurrent session management across multiple devices with device fingerprinting
      - Design session timeout policies with idle timeout, absolute timeout, and token rotation
      - Debug session hijacking vulnerabilities and implement defense mechanisms
      - Implement CSRF protection integrated with session management
      skills:
      - Distributed Systems
      - Cryptographic Security
      - Session Storage Design
      - Cookie Security
      - CSRF Protection
      - Redis/Caching
      - Concurrent State Management
      - Authentication Patterns
      tags:
      - advanced
      - authentication
      - cookies
      - distributed
      - expiration
      - security
      - sessions
      - tokens
      architecture_doc: architecture-docs/session-management/index.md
      languages:
        recommended:
        - Go
        - Python
        - Java
        also_possible: []
      resources:
      - name: OWASP Session Management Cheat Sheet
        url: https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html
        type: documentation
      - name: Redis Session Management
        url: https://redis.io/solutions/session-management/
        type: documentation
      - name: Session Fixation Prevention
        url: https://owasp.org/www-community/controls/Session_Fixation_Protection
        type: article
      - name: JWT Introduction
        url: https://jwt.io/introduction
        type: documentation
      - name: Spring Session Data Redis
        url: https://www.javainuse.com/spring/springboot_session_redis
        type: tutorial
      prerequisites:
      - type: project
        id: build-redis
      milestones:
      - id: session-management-m1
        name: Secure Session Creation & Storage
        description: Implement cryptographically secure session IDs with distributed storage backend
        acceptance_criteria:
        - Generate cryptographically secure random session IDs of sufficient length
        - Store all session data on the server side keyed by session ID
        - Support multiple storage backends including Redis and database
        - Handle session expiration removing stale sessions after timeout
        pitfalls:
        - 'Session fixation: ALWAYS regenerate session ID after login'
        - Session ID in URL is insecure (referer leaks, logs) - use cookies only
        - Idle timeout and absolute timeout are different - need both
        - 'Race condition: concurrent requests can cause session data loss'
        concepts:
        - Cryptographically secure random number generation (CSPRNG)
        - Session ID entropy requirements (minimum 128 bits)
        - Redis/Memcached distributed storage patterns
        - Session data serialization and deserialization
        - Time-to-live (TTL) and expiration policies
        skills:
        - Session security
        - Distributed storage
        - Cookie handling
        deliverables:
        - Cryptographically secure session ID generation using random bytes
        - Server-side session data storage keyed by session identifier
        - Multiple storage backend support including Redis, database, and memory
        - Configurable session expiration with automatic cleanup of expired sessions
        estimated_hours: '10'
      - id: session-management-m2
        name: Cookie Security & Transport
        description: Implement secure cookie handling with proper flags and encryption
        acceptance_criteria:
        - Set HttpOnly, Secure, and SameSite flags on all session cookies
        - Encrypt or sign cookie values to prevent client-side tampering
        - Support header-based session tokens as cookie-less alternative
        - Implement CSRF protection using per-session anti-forgery tokens
        pitfalls:
        - SameSite=None requires Secure flag (HTTPS)
        - Cookie size limit is ~4KB - don't store too much data
        - CSRF tokens must be tied to session - not global
        - Double-submit cookie pattern needs both cookie AND header/form
        concepts:
        - HttpOnly flag prevents JavaScript access to cookies
        - Secure flag enforces HTTPS-only transmission
        - SameSite attribute controls cross-site request behavior
        - AES-GCM authenticated encryption for sensitive cookie data
        - HMAC-based cookie integrity verification
        skills:
        - Cookie security
        - CSRF prevention
        - Secure transport
        deliverables:
        - Secure cookie configuration with HttpOnly and Secure flags enabled
        - HttpOnly flag preventing JavaScript access to session cookie
        - SameSite attribute restricting cross-origin cookie transmission
        - Cookie value encryption protecting session ID from tampering
        estimated_hours: '10'
      - id: session-management-m3
        name: Multi-Device & Concurrent Sessions
        description: Handle multiple sessions per user, device tracking, and forced logout
        acceptance_criteria:
        - Track active sessions per user across multiple devices and browsers
        - Support listing and selective revocation of individual sessions
        - Enforce configurable concurrent session limits per user account
        - Track session activity timestamps for last-active monitoring
        pitfalls:
        - Device fingerprinting has false positives (VPN, browser updates)
        - Session limit without cleanup leads to locked-out users
        - Time-based anomaly detection needs to consider time zones
        - Don't show full session ID in UI - use hash or partial
        concepts:
        - Session enumeration and listing by user identifier
        - Device fingerprinting using User-Agent and client hints
        - Session revocation patterns (broadcast vs. distributed check)
        - Concurrent session limits and eviction strategies
        - Session metadata tracking (IP, last activity, device info)
        skills:
        - Device fingerprinting
        - Session listing
        - Forced logout
        deliverables:
        - Per-user device tracking recording session origin information
        - Session listing endpoint showing all active sessions for a user
        - Session revocation allowing users to terminate specific sessions
        - Concurrent session limit enforcing maximum active sessions per user
        estimated_hours: '10'
    - id: audit-logging
      name: Audit Logging System
      description: Immutable audit trail for compliance
      difficulty: intermediate
      estimated_hours: '30'
      essence: Append-only storage with cryptographic hash chaining (each entry includes hash of previous entry) for tamper detection, combined with Merkle tree indexing for efficient O(log N) verification proofs and time-range queries across distributed compliance event streams.
      why_important: Audit logs are required for compliance (SOC2, HIPAA, GDPR). Understanding audit systems helps design secure, compliant applications.
      learning_outcomes:
      - Design immutable append-only log storage
      - Implement hash chain for tamper detection
      - Build efficient audit log querying
      - Handle retention and archival policies
      skills:
      - Append-only Data Structures
      - Hash Chain Cryptography
      - Event Schema Design
      - Retention Policy Management
      - Compliance Reporting
      - Immutable Storage Patterns
      - Audit Query Optimization
      - Tamper Detection
      tags:
      - backend
      - compliance
      - immutable
      - intermediate
      - retention
      - security
      architecture_doc: architecture-docs/audit-logging/index.md
      languages:
        recommended:
        - Go
        - Python
        - Java
        also_possible: []
      resources:
      - name: Write-Ahead Log Pattern
        url: https://martinfowler.com/articles/patterns-of-distributed-systems/write-ahead-log.html
        type: article
      - name: Efficient Tamper-Evident Logging
        url: https://static.usenix.org/event/sec09/tech/full_papers/crosby.pdf
        type: paper
      - name: Append-only Log Guide
        url: https://questdb.com/glossary/append-only-log/
        type: documentation
      - name: Building Audit Log in Go
        url: https://medium.com/@alameerashraf/building-an-audit-log-system-for-a-go-application-ce131dc21394
        type: tutorial
      - name: SOC 2 Compliance Requirements
        url: https://www.venn.com/learn/soc2-compliance/
        type: documentation
      prerequisites:
      - type: project
        id: log-aggregator
      milestones:
      - id: audit-logging-m1
        name: Audit Event Model
        description: Design audit event schema with actor, action, resource, and context information.
        acceptance_criteria:
        - 'Schema defines required fields: actor, action, resource, timestamp, and outcome for every event'
        - Custom metadata fields can be attached to any event without modifying the core schema definition
        - Request context including client IP address and user agent string is captured in each audit event
        - Events are validated against the schema on creation and rejected if required fields are missing
        pitfalls:
        - PII in audit logs violates GDPR right to erasure
        - Missing actor context makes logs useless for investigation
        - Async logging loses context in distributed systems
        concepts:
        - Structured logging formats (JSON, CEF, LEEF)
        - Audit event schema design and versioning
        - Sensitive data masking and tokenization
        - Context propagation in distributed systems
        - Correlation IDs for tracing actor sessions
        skills:
        - Data modeling for compliance requirements
        - PII detection and redaction techniques
        - Structured logging implementation
        - Schema versioning and evolution
        deliverables:
        - Event schema defining who (actor), what (action), when (timestamp), and where (resource) fields
        - Event serialization module that converts audit events to JSON and binary formats for storage
        - Actor and resource identification with unique IDs, types, and human-readable display names
        - Event metadata container supporting custom key-value pairs for domain-specific audit context
        estimated_hours: '10'
      - id: audit-logging-m2
        name: Immutable Storage with Hash Chain
        description: Implement append-only storage with hash chain linking for tamper detection.
        acceptance_criteria:
        - Events are appended to the log with a monotonically increasing sequence number for total ordering
        - Each entry's hash includes the previous entry's hash, forming a verifiable chain of integrity
        - Tampering with any entry is detected by recomputing and comparing the hash chain from that point forward
        - Log rotation creates a new segment while preserving the hash chain continuity across segment boundaries
        pitfalls:
        - Hash chain breaks if events inserted out of order
        - Chain verification expensive on large datasets
        - Backup/restore must preserve hash chain integrity
        concepts:
        - Cryptographic hash functions for integrity verification
        - Merkle trees and hash chain data structures
        - Append-only log semantics and immutability
        - Monotonic timestamps and sequence numbers
        - Forward-secure cryptographic schemes
        skills:
        - Implementing cryptographic integrity checks
        - Designing append-only data structures
        - Atomic batch operations for consistency
        - Hash chain verification algorithms
        deliverables:
        - Append-only event storage that prevents modification or deletion of previously written records
        - Hash chain linking each event to the previous entry's hash for cryptographic tamper detection
        - Event ordering guarantees using monotonically increasing sequence numbers assigned at write time
        - Storage encryption that protects audit data at rest using AES-256 or equivalent symmetric encryption
        estimated_hours: '10'
      - id: audit-logging-m3
        name: Audit Query & Export
        description: Build efficient querying for audit logs with filtering, search, and compliance export formats.
        acceptance_criteria:
        - Queries filter events by actor, action, resource, and time range with sub-second response times
        - Pagination support returns results in configurable page sizes with stable cursor-based navigation
        - Export produces correctly formatted CSV and JSON files suitable for compliance audit submissions
        - Generated audit reports include summary statistics, event timelines, and per-actor activity breakdowns
        pitfalls:
        - Full text search without index causes table scan
        - Large exports exhaust memory (use streaming)
        - Time zone handling inconsistent in queries
        concepts:
        - Database indexing strategies for audit logs
        - Time-series data querying patterns
        - Cursor-based pagination for large datasets
        - Streaming data export with backpressure
        - Timezone normalization and UTC storage
        skills:
        - Building efficient database indexes
        - Implementing streaming exports with generators
        - Time range query optimization
        - Compliance report generation (CSV, JSON, XML)
        - Full-text search with proper indexing
        deliverables:
        - Time-range query engine that retrieves audit events between specified start and end timestamps
        - Actor and resource filtering that narrows results by actor ID, resource type, or action category
        - Full-text search capability that finds events matching keyword queries in action descriptions or metadata
        - Export module that produces compliance-ready reports in CSV, JSON, and PDF formats
        estimated_hours: '10'
    - id: sandbox
      name: Process Sandbox
      description: Sandboxing with seccomp, namespaces, capabilities
      difficulty: intermediate
      estimated_hours: 25-40
      essence: Kernel-level process isolation through coordinated application of namespace resource virtualization, Berkeley Packet Filter system call interception, capability-based privilege reduction, and cgroup resource quotas to create unprivileged execution environments that prevent system compromise.
      why_important: Building this teaches you the foundational security mechanisms underlying containers, sandboxes, and virtualization technologies used in production systems like Docker, Kubernetes, and browser sandboxes, skills critical for security engineering and systems programming roles.
      learning_outcomes:
      - Implement namespace isolation for PID, network, mount, and user resource virtualization
      - Design and apply seccomp-BPF filters to whitelist safe system calls and block dangerous operations
      - Configure cgroups v2 controllers to enforce hard limits on CPU time, memory usage, and I/O bandwidth
      - Drop Linux capabilities systematically to achieve least-privilege execution
      - Build a minimal root filesystem using pivot_root for complete filesystem isolation
      - Debug permission denied errors and capability requirements in restricted environments
      - Combine multiple isolation layers to implement defense-in-depth security architecture
      skills:
      - Linux Namespaces
      - Seccomp-BPF Filtering
      - Cgroups Resource Control
      - Capability Management
      - Filesystem Isolation
      - System Call Analysis
      - Defense in Depth
      - Privilege Separation
      tags:
      - c
      - go
      - intermediate
      - isolation
      - rust
      - seccomp
      - security
      - syscall-filtering
      architecture_doc: architecture-docs/sandbox/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible:
        - Python (with ctypes)
      resources:
      - name: Linux Namespaces
        url: https://man7.org/linux/man-pages/man7/namespaces.7.html
        type: documentation
      - name: Seccomp BPF
        url: https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html
        type: documentation
      prerequisites:
      - type: skill
        name: Linux system calls
      - type: skill
        name: Process management (fork, exec)
      - type: skill
        name: Basic understanding of Linux security
      - type: skill
        name: C programming
      milestones:
      - id: sandbox-m1
        name: Process Namespaces
        description: Use Linux namespaces to isolate process view of system resources.
        acceptance_criteria:
        - Create new PID namespace where sandboxed process sees itself as PID 1
        - Create new mount namespace providing isolated filesystem view
        - Create new network namespace blocking all network access by default
        - Create new UTS namespace isolating hostname from host system
        - Verify namespace isolation by inspecting /proc entries from inside sandbox
        pitfalls:
        - Forgetting SIGCHLD flag
        - Not mounting /proc in new namespace
        - Need root/CAP_SYS_ADMIN
        concepts:
        - Linux namespaces
        - Process isolation
        - clone() system call
        skills:
        - System programming in C/Rust/Go
        - Linux namespace APIs (clone, unshare, setns)
        - Process isolation and containerization fundamentals
        - Debugging isolated processes with strace/gdb
        deliverables:
        - PID namespace isolation giving sandboxed process its own PID 1
        - Network namespace isolation preventing external network access
        - Mount namespace setup providing isolated filesystem view
        - User namespace mapping limiting privilege inside sandbox
        estimated_hours: 5-8
      - id: sandbox-m2
        name: Filesystem Isolation
        description: Create isolated filesystem using chroot or pivot_root with minimal root filesystem.
        acceptance_criteria:
        - Create minimal root filesystem with only required binaries and libraries
        - Use chroot or pivot_root to change root directory for sandboxed process
        - Mount /proc, /dev, and /sys with appropriate restricted permissions
        - Prevent container escape via /proc/*/root or similar path traversals
        - Verify that parent host filesystem is completely inaccessible from sandbox
        pitfalls:
        - Forgetting to unmount old root
        - Missing /dev entries
        - Library dependencies
        concepts:
        - chroot vs pivot_root
        - Filesystem namespaces
        - Minimal rootfs
        skills:
        - Filesystem operations (mount, pivot_root, chroot)
        - Building minimal container rootfs
        - Managing filesystem namespaces
        - Library dependency resolution (ldd, static linking)
        deliverables:
        - Chroot or pivot_root changing filesystem root for sandbox
        - Read-only root filesystem preventing writes to system directories
        - Tmpfs mounts providing writable scratch space at specified paths
        - Bind mount restrictions blocking access to sensitive host paths
        estimated_hours: 5-8
      - id: sandbox-m3
        name: Seccomp System Call Filtering
        description: Use seccomp-BPF to restrict which system calls the sandboxed process can make.
        acceptance_criteria:
        - Create seccomp filter using BPF program syntax
        - Whitelist only safe system calls needed for sandboxed workload
        - Kill sandboxed process immediately on forbidden system call attempt
        - Allow read and write calls but block open of sensitive file paths
        - Test that filter correctly blocks dangerous operations like ptrace
        pitfalls:
        - Missing required syscalls (glibc uses many)
        - Architecture-specific syscall numbers
        - Not setting NO_NEW_PRIVS
        concepts:
        - Seccomp BPF
        - System call filtering
        - Whitelist vs blacklist
        skills:
        - Berkeley Packet Filter (BPF) programming
        - System call whitelisting and security policies
        - Seccomp filter design and testing
        - Low-level Linux security mechanisms
        deliverables:
        - Seccomp-BPF filter program restricting allowed system calls
        - System call whitelist specifying exactly which calls are permitted
        - Argument-level filtering restricting parameters of allowed calls
        - BPF filter compilation generating kernel-loadable program
        estimated_hours: 6-10
      - id: sandbox-m4
        name: Resource Limits with Cgroups
        description: Use cgroups to limit CPU, memory, and I/O resources for sandboxed processes.
        acceptance_criteria:
        - Create dedicated cgroup for sandboxed process and children
        - Set memory limit to configurable threshold (e.g., 64MB maximum)
        - Set CPU limit to configurable share (e.g., 10% of one core)
        - Set I/O bandwidth limit capping disk throughput rate
        - Verify that resource limits are enforced when sandbox exceeds them
        pitfalls:
        - Cgroups v1 vs v2 differences
        - Need root for cgroup operations
        - Controller not enabled
        concepts:
        - Cgroups
        - Resource limits
        - CPU quotas
        - Memory limits
        skills:
        - Cgroups configuration and management
        - Resource limit enforcement
        - CPU and memory quota tuning
        - Writing to cgroup filesystem hierarchy
        deliverables:
        - Memory limit cgroup restricting maximum resident memory usage
        - CPU limit cgroup throttling processor time allocation
        - PID limit cgroup capping maximum number of spawned processes
        - I/O bandwidth limit cgroup throttling disk read and write rates
        estimated_hours: 5-8
      - id: sandbox-m5
        name: Capability Dropping
        description: Drop Linux capabilities to run with minimal privileges.
        acceptance_criteria:
        - List all current process capabilities before and after dropping
        - Drop all Linux capabilities except the minimum required set
        - Set PR_SET_NO_NEW_PRIVS flag blocking future privilege escalation
        - Verify capabilities are correctly dropped using capget or /proc
        - Test that privileged operations like mounting and network config fail
        pitfalls:
        - Order of setgid/setuid matters
        - Some caps needed for basic operations
        - Ambient caps can re-enable dropped caps
        concepts:
        - Linux capabilities
        - Principle of least privilege
        - Privilege dropping
        skills:
        - Linux capability system usage
        - Privilege separation techniques
        - Secure process initialization
        - Using capset/prctl for capability management
        deliverables:
        - Capability bounding set defining maximum possible privileges
        - Capability drop logic removing all unnecessary Linux capabilities
        - No-new-privileges flag preventing privilege escalation after exec
        - Privilege verification utility confirming capabilities are dropped
        estimated_hours: 4-6
    advanced:
    - id: https-client
      name: HTTPS Client
      description: TLS handshake
      difficulty: advanced
      estimated_hours: 20-35
      essence: Low-level implementation of TLS record framing, stateful cryptographic handshake protocol, ECDHE-based key agreement, and AEAD-authenticated symmetric encryption over raw TCP sockets without relying on existing TLS libraries.
      why_important: Building a TLS client from the ground up teaches the cryptographic primitives and security protocols that secure virtually all internet communication, providing deep understanding of public-key infrastructure, cipher suites, and attack surface mitigation that's essential for backend, security, and systems engineering roles.
      learning_outcomes:
      - Implement TLS record layer protocol with proper framing and versioning
      - Construct ClientHello messages with cipher suite negotiation and extension handling
      - Perform elliptic curve Diffie-Hellman ephemeral (ECDHE) key exchange for perfect forward secrecy
      - Validate X.509 certificate chains including signature verification and trust anchor resolution
      - Derive session keys using HKDF-based key derivation from handshake secrets
      - Implement AEAD cipher modes (AES-GCM or ChaCha20-Poly1305) for authenticated encryption
      - Handle TLS alerts, state machine transitions, and protocol version negotiation
      - Debug cryptographic operations using packet captures and RFC compliance testing
      skills:
      - Cryptographic Protocols
      - Public Key Infrastructure
      - Elliptic Curve Cryptography
      - Certificate Validation
      - Symmetric Encryption (AEAD)
      - Key Derivation Functions
      - Binary Protocol Parsing
      - Network Security
      tags:
      - advanced
      - certificates
      - go
      - python
      - rust
      - security
      - tls
      - tool
      - verification
      architecture_doc: architecture-docs/https-client/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible:
        - C
        - Java
      resources:
      - name: TLS 1.3 RFC 8446
        url: https://datatracker.ietf.org/doc/html/rfc8446
        type: specification
      - name: Illustrated TLS 1.3
        url: https://tls13.xargs.org/
        type: tutorial
      prerequisites:
      - type: skill
        name: TCP sockets
      - type: skill
        name: Cryptography basics
      - type: skill
        name: X.509 certificates
      milestones:
      - id: https-client-m1
        name: TCP Socket & Record Layer
        description: Establish TCP connection and implement TLS record layer.
        acceptance_criteria:
        - TCP connect to remote host on port 443 for TLS communication
        - TLS record format correctly encodes type, version, length, and payload fields
        - Fragment handling reassembles records split across multiple TCP segments
        - Record layer parsing correctly reads and classifies incoming TLS records
        pitfalls:
        - Endianness
        - Partial reads
        - Record fragmentation
        concepts:
        - TLS records
        - Network protocols
        - Binary parsing
        skills:
        - TCP socket programming
        - Binary protocol implementation
        - State machine design
        - Network I/O handling
        deliverables:
        - TCP connection establishment to remote server on port 443
        - TLS record layer implementing type, version, length, and data framing
        - Record framing with proper fragment boundary handling on receive
        - Record types for handshake, alert, change-cipher-spec, and application data
        estimated_hours: 3-4
      - id: https-client-m2
        name: ClientHello
        description: Send ClientHello with supported cipher suites.
        acceptance_criteria:
        - Random bytes generation produces 32 cryptographically secure random bytes for ClientHello
        - Session ID field correctly included for TLS 1.2 compatibility in ClientHello message
        - Cipher suite list includes at least TLS_AES_128_GCM_SHA256 for TLS 1.3 support
        - SNI extension includes target hostname so server selects correct certificate
        pitfalls:
        - Extension ordering
        - Length field sizes
        - Version negotiation
        concepts:
        - TLS handshake
        - Cipher negotiation
        - Extensions
        skills:
        - Protocol message construction
        - Cryptographic suite selection
        - Extension handling
        - TLS handshake implementation
        deliverables:
        - ClientHello construction with TLS version, random, and cipher suites
        - Cipher suite list specifying supported encryption algorithm combinations
        - Extensions including SNI, supported versions, and key share
        - ServerHello parsing extracting server's chosen parameters and key share
        estimated_hours: 4-6
      - id: https-client-m3
        name: Key Exchange
        description: Perform ECDHE key exchange and derive keys.
        acceptance_criteria:
        - Generate ephemeral ECDHE key pair using X25519 or P-256 curve
        - Parse ServerHello and extract server's key share for shared secret computation
        - ECDH shared secret computation derives pre-master secret from key exchange
        - Key derivation via HKDF produces client and server traffic keys and IVs
        pitfalls:
        - Curve negotiation
        - Key derivation order
        - Hash transcript
        concepts:
        - ECDHE
        - Key derivation
        - Forward secrecy
        skills:
        - Elliptic curve cryptography
        - Key derivation functions
        - Cryptographic primitives integration
        - Secure key management
        deliverables:
        - ECDHE key exchange generating ephemeral key pair for forward secrecy
        - Key derivation using HKDF to produce traffic encryption keys from shared secret
        - Certificate validation verifying server identity through certificate chain
        - Finished message verification confirming handshake integrity on both sides
        estimated_hours: 6-8
      - id: https-client-m4
        name: Encrypted Communication
        description: Encrypt/decrypt application data with derived keys.
        acceptance_criteria:
        - AEAD encryption using AES-GCM encrypts application data with authentication tag
        - Sequence number handling computes unique nonce by XORing IV with incrementing counter
        - Finished message verification confirms handshake transcript hash matches expected value
        - Send and receive application data over the encrypted TLS channel successfully
        pitfalls:
        - Nonce reuse
        - Padding removal
        - Alert handling
        concepts:
        - AEAD encryption
        - Sequence numbers
        - TLS records
        skills:
        - Authenticated encryption
        - Secure channel management
        - Alert protocol handling
        - Production-grade TLS implementation
        deliverables:
        - Application data encryption using AEAD cipher with derived traffic keys
        - Request sending constructing and encrypting HTTP request for transmission
        - Response receiving decrypting and parsing HTTP response from encrypted records
        - Connection close with proper TLS close_notify alert exchange
        estimated_hours: 6-8
    - id: oauth2-provider
      name: OAuth2/OIDC Provider
      description: Identity provider with authorization code flow, PKCE, JWT
      difficulty: expert
      estimated_hours: '50'
      essence: Multi-party cryptographic authorization flows implementing RFC 6749 delegation with PKCE challenge-response protection, JWT-based identity assertion through asymmetric signatures, and distributed token lifecycle state management across authorization, token issuance, introspection, and revocation endpoints.
      why_important: Identity providers are foundational infrastructure for modern authentication systems, and building one teaches protocol-level security design, cryptographic operations, and the architectural patterns used by Auth0, Okta, and enterprise SSO systems that handle billions of authentication requests.
      learning_outcomes:
      - Implement PKCE authorization code flow with cryptographic challenge-response validation
      - Design JWT access token generation with asymmetric signing using RS256 or ES256
      - Build secure refresh token rotation with token family tracking to detect replay attacks
      - Implement RFC 7662 token introspection endpoint with proper scope-based authorization
      - Design client registration systems with secret generation and secure storage patterns
      - Build consent management flows with granular scope approval and persistence
      - Implement token revocation with distributed cache invalidation strategies
      - Debug OAuth2 security vulnerabilities including authorization code interception and CSRF attacks
      skills:
      - OAuth2 Protocol Design
      - JWT Cryptographic Signing
      - PKCE Security Extension
      - Token Lifecycle Management
      - Consent Flow Design
      - Authorization Code Flow
      - OpenID Connect
      - API Security Patterns
      tags:
      - authentication
      - authorization-code
      - expert
      - identity
      - jwt
      - oauth2
      - scopes
      - security
      - tokens
      architecture_doc: architecture-docs/oauth2-provider/index.md
      languages:
        recommended:
        - Go
        - Rust
        - C
        also_possible: []
      resources:
      - name: OAuth 2.0 RFC 6749
        url: https://datatracker.ietf.org/doc/html/rfc6749
        type: documentation
      - name: OpenID Connect Core Specification
        url: https://openid.net/specs/openid-connect-core-1_0.html
        type: documentation
      - name: PKCE RFC 7636
        url: https://datatracker.ietf.org/doc/html/rfc7636
        type: documentation
      - name: JWT RFC 7519
        url: https://datatracker.ietf.org/doc/html/rfc7519
        type: documentation
      - name: OAuth 2.0 Simplified
        url: https://www.oauth.com/oauth2-servers/
        type: tutorial
      prerequisites:
      - type: project
        id: http-server-basic
        name: HTTP Server (Basic)
      - type: skill
        name: Cryptography basics
      milestones:
      - id: oauth2-provider-m1
        name: Client Registration & Authorization Endpoint
        description: Implement client registration and the authorization endpoint with PKCE support
        acceptance_criteria:
        - OAuth clients are registered with a generated client_id, hashed client_secret, and configured redirect URIs
        - Authorization consent page displays the client name, requested scopes, and allow/deny buttons to the user
        - Authorization codes are generated as cryptographically random strings bound to the client and redirect URI
        - Redirect URI validation rejects authorization requests whose redirect URI does not match the registered value
        pitfalls:
        - Authorization codes must be single-use and short-lived (10 min max)
        - Always validate redirect_uri exactly - no partial matches
        - PKCE is mandatory for public clients (mobile, SPA)
        - State parameter prevents CSRF - must be unpredictable
        concepts:
        - Authorization code generation and secure random number generation
        - PKCE code_challenge verification using SHA256 hashing
        - OAuth2 state parameter validation and CSRF protection
        - Redirect URI validation and exact string matching
        - Authorization code expiration and single-use enforcement
        skills:
        - OAuth2 flows
        - PKCE
        - Cryptographic challenges
        deliverables:
        - Client registration API that creates OAuth client records with client_id, client_secret, and redirect URIs
        - Client credentials storage that hashes client secrets and stores grant types and allowed scopes per client
        - Authorization endpoint that initiates the authorization code flow by validating parameters and redirecting to consent
        - User consent handler that displays the requested scopes and captures the user's allow or deny decision
        estimated_hours: '12.5'
      - id: oauth2-provider-m2
        name: Token Endpoint & JWT Generation
        description: Implement the token endpoint with JWT access tokens and refresh token rotation
        acceptance_criteria:
        - Authorization code is exchanged for an access token and refresh token after client authentication
        - Access tokens are signed JWTs containing sub, iss, aud, exp, iat, and scope claims
        - Client credentials grant issues an access token directly to the client without user involvement
        - Token expiration is enforced so expired access tokens are rejected by resource servers on validation
        pitfalls:
        - Never include sensitive data in JWT payload - it's base64, not encrypted
        - 'Refresh token rotation: issue new refresh token on each use, invalidate old one'
        - Access tokens should be short-lived (15 min - 1 hour)
        - Always use constant-time comparison for token validation
        concepts:
        - JWT structure with header, payload, and signature components
        - Asymmetric cryptography using RS256 or ES256 algorithms
        - Refresh token rotation and token family tracking
        - Token binding and preventing token replay attacks
        - JWT claims validation including exp, iat, and aud
        skills:
        - JWT signing
        - Token rotation
        - Secure token storage
        deliverables:
        - Token endpoint that exchanges an authorization code for access and refresh tokens via HTTP POST
        - Authorization code exchanger that validates the code, client credentials, and redirect URI before issuing tokens
        - JWT token generator that creates signed access tokens with configurable claims and expiration times
        - Refresh token issuer that generates long-lived refresh tokens and stores them for later exchange
        estimated_hours: '12.5'
      - id: oauth2-provider-m3
        name: Token Introspection & Revocation
        description: Implement RFC 7662 token introspection and RFC 7009 token revocation
        acceptance_criteria:
        - Introspection endpoint returns active=true with token metadata for valid tokens per RFC 7662 format
        - Revocation endpoint accepts access or refresh tokens and immediately marks them as inactive per RFC 7009
        - Resource servers can validate tokens by checking JWT signature, expiration, and required claims locally
        - Revoked tokens are immediately rejected on subsequent introspection or validation attempts
        pitfalls:
        - Introspection endpoint must be protected (client authentication required)
        - Revoked JWTs need tracking until expiry (use jti claim)
        - 'Refresh token families: if old token reused, revoke entire family (theft detection)'
        - Consider using Redis for revocation list with TTL matching token expiry
        concepts:
        - Token introspection response format and active status
        - JWT identifier (jti claim) for revocation tracking
        - Client authentication methods for protected endpoints
        - Distributed cache systems for revocation lists
        - Token family revocation for breach detection
        skills:
        - Token lifecycle
        - Revocation strategies
        - Cache invalidation
        deliverables:
        - Token introspection endpoint that returns the active status and metadata of a presented token per RFC 7662
        - Token revocation endpoint that immediately invalidates an access or refresh token per RFC 7009
        - Token validation library that verifies JWT signature, expiration, and claims for resource server use
        - Token lifecycle manager that tracks token creation, usage, expiration, and revocation events
        estimated_hours: '12.5'
      - id: oauth2-provider-m4
        name: UserInfo Endpoint & Consent Management
        description: Implement OIDC UserInfo endpoint and user consent flows for scope approval
        acceptance_criteria:
        - UserInfo endpoint returns user claims such as name, email, and profile based on the granted scopes
        - Previously granted consent decisions are stored so the user is not prompted again for the same client and scopes
        - Users can revoke previously granted consent causing future authorization requests to re-prompt for approval
        - Scope-based claims filtering returns only the user fields that the granted scopes authorize access to
        pitfalls:
        - UserInfo must use access token, not ID token
        - Only return claims for scopes actually granted, not requested
        - Consent screen must clearly show what data will be shared
        - Allow users to revoke consent and see all authorized applications
        concepts:
        - OIDC standard claims and scope-to-claim mapping
        - Bearer token authentication for protected resources
        - Consent persistence and user authorization history
        - Claim filtering based on granted scopes
        - OIDC UserInfo endpoint response format and requirements
        skills:
        - OIDC compliance
        - Consent UX
        - Scope management
        deliverables:
        - UserInfo endpoint that returns the authenticated user's profile claims as JSON based on granted scopes
        - Scope-to-claims mapping that determines which user profile fields are returned for each requested scope
        - Consent screen UI that displays requested scopes with descriptions and records the user's decision
        - Consent persistence store that saves the user's grant decisions so repeat authorizations skip the consent prompt
        estimated_hours: '12.5'
    - id: rbac-system
      name: RBAC/ABAC Authorization
      description: Role and attribute based access control system
      difficulty: expert
      estimated_hours: '40'
      essence: Hierarchical permission graph traversal with efficient caching strategies, policy decision point (PDP) engines evaluating boolean rule sets over subject-object-environment attribute tuples, and optimized permission bitmap lookups supporting real-time authorization decisions at scale.
      why_important: Building this teaches you production authorization patterns critical for SaaS platforms, combining security policy enforcement with performance-sensitive permission checks that scale across millions of users and resources.
      learning_outcomes:
      - Implement hierarchical RBAC with role inheritance and efficient permission caching
      - Design attribute-based policy engines evaluating user, resource, and environmental conditions
      - Build policy decision points (PDP) with rule matching and conflict resolution
      - Implement multi-tenant resource isolation with row-level security patterns
      - Design audit logging systems capturing authorization decisions for compliance
      - Create policy testing frameworks with simulation and conflict detection
      - Optimize permission lookups using bitmap indexes and denormalized access control lists
      - Implement XACML-style policy evaluation with combining algorithms
      skills:
      - Access Control Models
      - Policy Evaluation Engines
      - Multi-tenancy Architecture
      - Audit Logging
      - Permission Caching
      - Security Policy Design
      tags:
      - abac
      - access-control
      - authentication
      - authorization
      - expert
      - permissions
      - policy
      - rbac
      - roles
      - security
      architecture_doc: architecture-docs/rbac-system/index.md
      languages:
        recommended:
        - Go
        - Python
        - Java
        also_possible: []
      resources:
      - name: NIST ABAC Guidelines SP 800-162
        url: https://csrc.nist.gov/pubs/sp/800/162/upd2/final
        type: documentation
      - name: Casbin Authorization Library
        url: https://www.casbin.org/
        type: tool
      - name: AWS ABAC Documentation
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html
        type: documentation
      - name: Open Policy Agent ABAC Tutorial
        url: https://medium.com/permify-tech-blog/attribute-based-access-control-abac-implementation-with-open-policy-agent-opa-b47052248f29
        type: tutorial
      - name: NIST RBAC Project
        url: https://csrc.nist.gov/projects/role-based-access-control
        type: documentation
      milestones:
      - id: rbac-system-m1
        name: Role & Permission Model
        description: Implement hierarchical roles with permission inheritance and efficient lookup
        acceptance_criteria:
        - Define roles with named permission sets attached
        - Support role hierarchy with permission inheritance via DAG
        - Assign one or more roles to individual users
        - Handle role templates for creating preconfigured role bundles
        pitfalls:
        - Role inheritance cycles cause infinite loops - validate DAG property
        - Cache invalidation needed when roles change - use versioning
        - Wildcard permissions (*) need careful handling to avoid over-granting
        - 'Role explosion: too many fine-grained roles become unmanageable'
        concepts:
        - Directed Acyclic Graph (DAG) validation for role hierarchies
        - Permission bitmap encoding for fast lookup operations
        - Copy-on-write data structures for safe concurrent access
        - Transitive closure computation for inherited permissions
        skills:
        - Role hierarchies
        - Permission modeling
        - Graph traversal
        deliverables:
        - Role definition with name and permission set
        - Permission definition with resource and action pair
        - Role-to-permission mapping supporting many-to-many relationships
        - User-to-role assignment with multiple roles per user
        estimated_hours: '10'
      - id: rbac-system-m2
        name: ABAC Policy Engine
        description: Extend to attribute-based policies with conditions on user/resource/environment attributes
        acceptance_criteria:
        - Define policies using user, resource, and environment attributes
        - Evaluate attribute conditions against runtime request context
        - Support policy combining logic with AND and OR operators
        - Handle policy priority and ordering with deny-overrides strategy
        pitfalls:
        - 'Deny-by-default: no matching policy = deny'
        - Explicit deny always wins over allow (principle of least privilege)
        - Context must include all attributes needed for evaluation
        - Policy ordering matters - define clear priority rules
        concepts:
        - Abstract Syntax Tree (AST) parsing for policy expressions
        - Context-aware evaluation with attribute resolution chain
        - Policy decision caching with attribute-based cache keys
        - Rego or CEL policy language integration patterns
        skills:
        - Policy languages
        - Condition evaluation
        - Context propagation
        deliverables:
        - Attribute-based access rules with condition expressions
        - Policy definition language specifying effect, resources, and conditions
        - Policy evaluation engine processing rules against request context
        - Context attribute propagation passing user, resource, and environment data
        estimated_hours: '10'
      - id: rbac-system-m3
        name: Resource-Based & Multi-tenancy
        description: Implement resource-level permissions and tenant isolation for SaaS applications
        acceptance_criteria:
        - Support fine-grained permissions on individual resources
        - Enforce tenant isolation preventing cross-tenant data access
        - Implement resource ownership granting full control to creators
        - Support controlled cross-tenant admin access with explicit grants
        pitfalls:
        - Tenant isolation must be enforced at database query level too
        - Admin roles should be tenant-scoped, not global
        - Share links need expiry and use limits to prevent abuse
        - Revoking share should be immediate - no caching of permissions
        concepts:
        - Row-level security (RLS) enforcement in database queries
        - Hierarchical resource ownership with parent-child relationships
        - Time-limited token generation with cryptographic signatures
        - Tenant context propagation through middleware chains
        skills:
        - Multi-tenancy
        - Resource ownership
        - Tenant isolation
        deliverables:
        - Resource ownership model tracking creator and tenant
        - Tenant isolation enforcing data separation between organizations
        - Cross-tenant permission grants via explicit sharing mechanism
        - Resource hierarchy supporting nested resource access inheritance
        estimated_hours: '10'
      - id: rbac-system-m4
        name: Audit Logging & Policy Testing
        description: Implement comprehensive audit logging and policy simulation for testing
        acceptance_criteria:
        - Log every access control decision with user, resource, and outcome
        - Support policy simulation testing changes before production deployment
        - Track all permission and role changes with timestamp and actor
        - Generate access audit reports for compliance review
        pitfalls:
        - Audit logs must be immutable - use append-only storage
        - Include enough context to understand decision without leaking secrets
        - Simulation environment must match production exactly
        - Test negative cases (should be denied) not just positive
        concepts:
        - Write-ahead logging with Merkle tree integrity verification
        - Policy decision provenance tracking with evaluation traces
        - Snapshot isolation for reproducible policy testing environments
        - Differential policy analysis for change impact assessment
        skills:
        - Security auditing
        - Policy testing
        - Compliance
        deliverables:
        - Access decision logging capturing every authorization check result
        - Policy simulation environment for testing rule changes safely
        - Automated test suite validating policy behavior against expected outcomes
        - Compliance reporting generating access audit summaries
        estimated_hours: '10'
    - id: secret-management
      name: Secret Management Vault
      description: Secure storage for API keys, credentials, certificates
      difficulty: expert
      estimated_hours: '45'
      essence: Cryptographic key hierarchy management using envelope encryption and Shamir's secret sharing for distributed unsealing, combined with policy-based access control enforcement and ephemeral credential generation to eliminate static secrets at scale.
      why_important: Building a secret management system teaches you production-grade cryptography, distributed systems security patterns, and credential lifecycle management‚Äîcritical skills for any infrastructure or security engineering role.
      learning_outcomes:
      - Implement envelope encryption with master key and data encryption keys for layered security
      - Design and enforce path-based access control lists with multiple authentication methods
      - Build dynamic credential generation for databases and cloud providers with automatic rotation
      - Implement Shamir's secret sharing for distributed key unsealing with threshold cryptography
      - Create audit logging systems that track all secret access without exposing sensitive data
      - Develop high-availability replication for stateful security systems
      - Debug and secure against timing attacks, key exposure, and unauthorized access vectors
      - Integrate cryptographic libraries safely while avoiding common implementation pitfalls
      skills:
      - Envelope Encryption
      - Access Control Lists
      - Dynamic Credential Generation
      - Shamir Secret Sharing
      - Cryptographic Key Management
      - Distributed Systems Security
      - Audit Logging
      - mTLS Authentication
      tags:
      - authentication
      - encryption
      - encryption-at-rest
      - expert
      - infrastructure
      - rotation
      - secrets
      - security
      - vault
      architecture_doc: architecture-docs/secret-management/index.md
      languages:
        recommended:
        - Go
        - Python
        - Rust
        also_possible: []
      resources:
      - name: Vault Documentation
        url: https://developer.hashicorp.com/vault/docs
        type: documentation
      - name: Envelope Encryption Guide
        url: https://docs.cloud.google.com/kms/docs/envelope-encryption
        type: documentation
      - name: Shamir's Secret Sharing Tutorial
        url: https://www.geeksforgeeks.org/computer-networks/shamirs-secret-sharing-algorithm-cryptography/
        type: tutorial
      - name: Database Dynamic Secrets
        url: https://developer.hashicorp.com/vault/tutorials/db-credentials/database-secrets
        type: tutorial
      - name: Rust Cryptography Libraries
        url: https://blog.logrocket.com/rust-cryptography-libraries-a-comprehensive-list/
        type: article
      prerequisites:
      - type: project
        id: http-server-basic
        name: HTTP Server (Basic)
      - type: skill
        name: Encryption basics (AES)
      milestones:
      - id: secret-management-m1
        name: Encrypted Secret Storage
        description: Implement envelope encryption with master key and data encryption keys
        acceptance_criteria:
        - Store all secrets encrypted at rest in the backend storage
        - Use AES-256-GCM authenticated encryption for secret values
        - Support secret versioning allowing retrieval of previous values
        - Handle encryption key rotation re-encrypting secrets with new key
        pitfalls:
        - Master key in memory can be extracted - consider memory encryption
        - Secret versioning needed for rotation without breaking consumers
        - Backup encrypted data AND keys separately (but both needed for recovery)
        - 'Key derivation: use high iteration count (480k+) for PBKDF2'
        concepts:
        - Envelope encryption (master key encrypts data keys, data keys encrypt secrets)
        - AES-GCM authenticated encryption with associated data (AEAD)
        - Key derivation functions (PBKDF2, Argon2) for password-based keys
        - Cryptographic key rotation and versioning strategies
        - Secure memory handling and zeroing sensitive data
        skills:
        - Envelope encryption
        - Key management
        - Secure storage
        deliverables:
        - Encryption at rest using AES-256-GCM for all stored secrets
        - Master key handling with secure generation and storage
        - Secret versioning maintaining history of value changes
        - Key rotation mechanism replacing encryption keys without downtime
        estimated_hours: '11'
      - id: secret-management-m2
        name: Access Policies & Authentication
        description: Implement path-based ACLs and multiple authentication methods (token, AppRole, mTLS)
        acceptance_criteria:
        - Define fine-grained access policies specifying allowed secret paths
        - Authenticate clients using tokens or TLS client certificates
        - Authorize secret access based on matching policy evaluation
        - Log all secret access attempts with identity and outcome
        pitfalls:
        - role_id can be embedded in code; secret_id must be delivered securely
        - Token lookup table grows unbounded - need periodic cleanup
        - 'Glob patterns in paths: ensure * doesn''t match too broadly'
        - Constant-time comparison for secret validation prevents timing attacks
        concepts:
        - Path-based access control lists with wildcard matching
        - JWT token structure and validation (signature, expiration, claims)
        - Mutual TLS authentication with client certificates
        - Role-based access control (RBAC) vs attribute-based access control (ABAC)
        - Constant-time comparison algorithms to prevent timing attacks
        skills:
        - Path-based ACLs
        - Authentication methods
        - Token management
        deliverables:
        - Policy definition specifying which identities can access which secrets
        - Token-based authentication issuing and validating access tokens
        - Policy evaluation engine checking requests against defined rules
        - Audit logging recording every secret access and policy evaluation
        estimated_hours: '11'
      - id: secret-management-m3
        name: Dynamic Secrets
        description: Generate short-lived credentials on-demand for databases, cloud providers, etc.
        acceptance_criteria:
        - Generate unique database credentials on demand for each request
        - Handle automatic credential rotation at configurable intervals
        - Revoke dynamically generated credentials when lease expires
        - Support multiple secret backends including databases and cloud providers
        pitfalls:
        - Lease reaper must handle engine failures gracefully
        - Max TTL should be enforced even for renewals
        - Revocation can fail - need retry logic and alerting
        - 'Connection pooling: don''t create new DB user for every request'
        concepts:
        - Time-to-live (TTL) and lease-based credential lifecycle
        - Database privilege models and user creation patterns
        - Credential revocation and cleanup workflows
        - Connection pooling and resource reuse strategies
        - Renewal vs regeneration for expiring credentials
        skills:
        - Dynamic credentials
        - Lease management
        - Secret rotation
        deliverables:
        - Dynamic secret generation creating credentials on demand
        - Database credential backend generating time-limited users
        - TTL management tracking and enforcing secret lease expiration
        - Secret revocation revoking credentials before lease expires
        estimated_hours: '11'
      - id: secret-management-m4
        name: Unsealing & High Availability
        description: Implement Shamir's secret sharing for master key unsealing and HA replication
        acceptance_criteria:
        - Implement Shamir's secret sharing to split master key into N shares
        - Require configurable threshold of key shares to unseal the vault
        - Handle high availability with leader election among cluster nodes
        - Support auto-unseal integration with cloud KMS providers
        pitfalls:
        - Never store all shares together - defeats the purpose
        - Clear shares from memory after reconstruction
        - Sealed state must reject all secret operations
        - 'HA: only one node should be active writer to prevent split-brain'
        concepts:
        - Shamir's secret sharing threshold scheme for key splitting
        - Raft consensus algorithm for distributed coordination
        - Write-ahead logging for durability in distributed systems
        - Split-brain prevention and leader election
        - Sealed vs unsealed state transitions in secure systems
        skills:
        - Shamir's secret sharing
        - Consensus
        - Replication
        deliverables:
        - Seal and unseal mechanism protecting master key at rest
        - Shamir's secret sharing splitting master key into key shares
        - High availability backend supporting multiple active instances
        - Standby node promotion for automatic failover on leader failure
        estimated_hours: '11'
    - id: fuzzer
      name: Fuzzing Framework
      description: Coverage-guided fuzzer like AFL with mutation strategies
      difficulty: advanced
      estimated_hours: '55'
      essence: Compile-time code instrumentation to track execution paths, genetic mutation algorithms to evolve test inputs, and feedback loops that maximize code coverage while detecting crashes and memory corruption through runtime monitoring and signal handling.
      why_important: Building a fuzzer teaches you low-level program analysis, instrumentation techniques, and security vulnerability discovery‚Äîskills critical for security engineering, compiler tooling, and building robust systems that handle untrusted input.
      learning_outcomes:
      - Understand fuzzing principles and techniques
      - Implement coverage-guided mutation
      - Build crash detection and triage
      - Design efficient fuzzing campaigns
      skills:
      - Coverage instrumentation
      - Mutation strategies
      - Corpus management
      - Crash detection
      - Input minimization
      - Parallel fuzzing
      tags:
      - advanced
      - coverage
      - crash-detection
      - mutation
      - security
      - testing
      - tool
      architecture_doc: architecture-docs/fuzzer/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible: []
      resources:
      - name: The Fuzzing Book
        url: https://www.fuzzingbook.org/
        type: book
      - name: AFL++ Documentation
        url: https://aflplus.plus/
        type: documentation
      - name: libFuzzer LLVM Docs
        url: https://llvm.org/docs/LibFuzzer.html
        type: documentation
      - name: Google Fuzzing Tutorials
        url: https://github.com/google/fuzzing/blob/master/tutorial/libFuzzerTutorial.md
        type: tutorial
      - name: AFL Fuzzing Tutorial
        url: https://fuzzing-project.org/tutorial3.html
        type: tutorial
      prerequisites:
      - type: skill
        name: Binary basics
      - type: skill
        name: Testing
      - type: skill
        name: Process management
      milestones:
      - id: fuzzer-m1
        name: Target Execution
        description: Execute target program with inputs
        acceptance_criteria:
        - Execute target program with provided test input data via configured delivery
        - Capture exit code and crash signals to classify execution outcome
        - Handle target timeout by killing the process after configurable deadline
        - Support different input delivery methods including stdin, file, and argv
        pitfalls:
        - Not setting timeouts leading to infinite hangs
        - Ignoring segmentation faults and other crash signals
        - Failing to isolate target process from fuzzer state
        - Not capturing stderr for crash diagnostics
        concepts:
        - Process forking and exec system calls
        - Standard input/output redirection for test data
        - Signal handling for crashes and hangs
        - Resource limits (memory, CPU time) enforcement
        skills:
        - Process execution
        - Timeout handling
        - Exit code analysis
        deliverables:
        - Fork and exec wrapper for launching target program with test input
        - Stdin and file input modes for delivering test data to target
        - Timeout enforcement killing target if execution exceeds time limit
        - Crash detection via signal analysis identifying segfaults and aborts
        - Resource limits preventing target from consuming excessive system resources
        - Exit status collection capturing return code and signal information
        estimated_hours: '11'
      - id: fuzzer-m2
        name: Coverage Tracking
        description: Track code coverage for guidance
        acceptance_criteria:
        - Instrument target for code coverage using llvm-cov or similar tooling
        - Track basic block and edge coverage using shared memory bitmap
        - Detect new coverage from test inputs by comparing against global bitmap
        - Build and maintain coverage bitmap recording all observed edge transitions
        pitfalls:
        - Hash collisions in coverage bitmap causing blind spots
        - Instrumenting library code unnecessarily
        - Not resetting coverage map between executions
        - Using coverage granularity that's too coarse
        concepts:
        - Compile-time instrumentation vs runtime hooking
        - Edge coverage tracking via hash maps
        - Branch hit counts and saturation
        - Shared memory for coverage feedback
        skills:
        - Instrumentation
        - Edge coverage
        - Bitmap
        deliverables:
        - Coverage bitmap for recording visited edge and branch information
        - Edge and branch coverage tracking via compile-time instrumentation
        - Compile-time instrumentation inserting coverage probes into target code
        - Coverage comparison detecting differences between execution runs
        - New coverage detection identifying previously unseen edge transitions
        - Coverage visualization showing which code paths have been exercised
        estimated_hours: '11'
      - id: fuzzer-m3
        name: Mutation Engine
        description: Generate new test inputs via mutation
        acceptance_criteria:
        - Implement bit flip mutations that toggle bits at each position sequentially
        - Support byte insertion and deletion mutations changing input structure
        - Implement arithmetic mutations adding and subtracting values from integer fields
        - Use coverage feedback to guide mutation strategy toward unexplored code paths
        pitfalls:
        - Mutating beyond input boundaries causing invalid tests
        - Over-mutating inputs losing semantic validity
        - Not preserving input structure for structured formats
        - Ignoring magic bytes and checksums in formats
        concepts:
        - Deterministic vs havoc mutation strategies
        - Walking bit flips for maximum coverage
        - Integer arithmetic mutations (add, subtract, interesting values)
        - Dictionary-based token insertion and replacement
        skills:
        - Bit flips
        - Arithmetic
        - Dictionary
        - Splice
        deliverables:
        - Bit flip mutations toggling individual bits at each position
        - Byte flip mutations inverting full bytes for broader changes
        - Arithmetic mutations adding and subtracting small values from fields
        - Block operations including byte insertion, deletion, and overwrite
        - Dictionary-based mutations inserting known interesting byte sequences
        - Havoc mode applying random combinations of multiple mutations per input
        estimated_hours: '11'
      - id: fuzzer-m4
        name: Corpus Management
        description: Manage and minimize test corpus
        acceptance_criteria:
        - Store inputs that increase code coverage into the persistent corpus directory
        - Minimize corpus by removing redundant inputs that cover same code paths
        - Support initial seed corpus loading user-provided starting inputs for fuzzing
        - Export crash-inducing inputs with reproduction information for bug reporting
        pitfalls:
        - Corpus growing unbounded without minimization
        - Discarding inputs that trigger rare edge cases
        - Not syncing corpus across parallel fuzzer instances
        - Minimizing inputs losing the crash trigger
        concepts:
        - Test case minimization through delta debugging
        - Coverage-based deduplication of redundant inputs
        - Energy assignment for scheduling interesting inputs
        - Crash bucketing and unique crash identification
        skills:
        - Corpus storage
        - Minimization
        - Deduplication
        deliverables:
        - Corpus storage saving interesting inputs as individual files on disk
        - Input minimization reducing test case size while preserving crash behavior
        - Corpus distillation removing redundant inputs with duplicate coverage
        - Crash deduplication grouping crashes by unique stack trace signatures
        - Coverage-based scoring prioritizing inputs that cover rare edge transitions
        - Queue scheduling selecting next input for mutation based on coverage score
        estimated_hours: '11'
      - id: fuzzer-m5
        name: Fuzzing Loop
        description: Main fuzzing orchestration
        acceptance_criteria:
        - Pick input from corpus based on coverage-weighted selection strategy
        - Mutate selected input and execute target to observe coverage and crashes
        - Track coverage bitmap and update corpus when new edges are discovered
        - Report crashes with full reproduction steps including input and crash details
        pitfalls:
        - Not balancing exploitation vs exploration
        - Starvation of low-energy queue entries
        - Race conditions in parallel fuzzing
        - Not persisting state for resumable fuzzing campaigns
        concepts:
        - Input queue scheduling and prioritization strategies
        - Real-time statistics tracking (exec/sec, coverage)
        - Distributed fuzzing synchronization protocols
        - Deterministic replay for reproducing crashes
        skills:
        - Scheduling
        - Statistics
        - Parallel fuzzing
        deliverables:
        - Main fuzzing loop orchestrating selection, mutation, and execution cycle
        - Statistics and reporting displaying executions per second and coverage metrics
        - Parallel and distributed fuzzing running multiple instances simultaneously
        - Sync between fuzzer instances sharing interesting inputs across workers
        - Adaptive mutation selection favoring strategies that discover new coverage
        - Timeout and resource management preventing runaway target processes
        estimated_hours: '11'
    - id: vulnerability-scanner
      name: Vulnerability Scanner
      description: Network/web vulnerability scanner with CVE detection
      difficulty: advanced
      estimated_hours: 40-60
      essence: Raw packet construction and TCP/IP stack manipulation for host enumeration and port probing, combined with protocol-specific banner analysis and CVE database correlation to map network attack surfaces and identify exploitable service configurations.
      why_important: Building this teaches you network protocol internals, security assessment methodology, and how to interact with real-world vulnerability databases‚Äîskills that directly translate to security engineering, penetration testing, and defensive security operations roles.
      learning_outcomes:
      - Implement ARP and ICMP-based host discovery techniques using raw packet manipulation
      - Design multi-threaded TCP port scanners with SYN, connect, and stealth scanning methods
      - Build service fingerprinting through banner grabbing and protocol-specific probing
      - Parse and query CVE databases via REST APIs to match service versions against known vulnerabilities
      - Implement network packet sniffing and protocol analysis for service detection
      - Handle asynchronous network I/O and connection timeouts for reliable scanning
      - Design structured report generation with severity scoring and remediation recommendations
      - Develop rate limiting and retry logic to handle unreliable network conditions
      skills:
      - Raw Socket Programming
      - Packet Crafting
      - Service Fingerprinting
      - CVE Database Integration
      - Network Protocol Analysis
      - Asynchronous I/O
      - Multi-threading
      - Security Assessment
      tags:
      - advanced
      - cve
      - detection
      - go
      - networking
      - python
      - reporting
      - security
      architecture_doc: architecture-docs/vulnerability-scanner/index.md
      languages:
        recommended:
        - Python
        - Go
        also_possible:
        - Rust
        - C
      resources:
      - name: Nmap Network Scanning
        url: https://nmap.org/book/toc.html
        type: book
      - name: NIST NVD API
        url: https://nvd.nist.gov/developers/vulnerabilities
        type: documentation
      prerequisites:
      - type: skill
        name: TCP/IP networking
      - type: skill
        name: Socket programming
      - type: skill
        name: Understanding of common vulnerabilities
      - type: skill
        name: Basic security concepts
      milestones:
      - id: vulnerability-scanner-m1
        name: Host Discovery
        description: Implement various techniques to discover live hosts on a network.
        acceptance_criteria:
        - ICMP echo ping scan detects responsive hosts in a target range
        - TCP SYN scan for common ports
        - ARP scan discovers hosts on the local network segment by MAC address
        - Handle rate limiting to avoid detection
        - Report discovered hosts with response times
        pitfalls:
        - Raw sockets need root
        - ICMP often blocked
        - Rate limiting needed
        concepts:
        - ICMP protocol
        - TCP handshake
        - ARP protocol
        - Network scanning
        skills:
        - Raw socket programming
        - ICMP packet crafting
        - ARP spoofing detection
        - Multi-threaded network scanning
        - Privilege escalation handling
        deliverables:
        - ICMP ping sweep sending echo requests to a range of IP addresses
        - TCP and UDP host probing detecting live hosts via port response analysis
        - ARP scanning for local network
        - Host database storing discovered hosts with IP, MAC, and response time data
        estimated_hours: 6-10
      - id: vulnerability-scanner-m2
        name: Port Scanning
        description: Implement various port scanning techniques to identify open services.
        acceptance_criteria:
        - TCP connect scan reports open, closed, and filtered port states
        - TCP SYN half-open scan detects open ports without completing the three-way handshake
        - UDP scan with common port payloads
        - Service version detection reads banners to identify software name and version
        - Scan 1000 common ports in under 30 seconds
        pitfalls:
        - Too aggressive scanning triggers IDS
        - Firewall may give false positives
        - UDP scanning is slow
        concepts:
        - TCP/UDP ports
        - Port states
        - Banner grabbing
        - Service detection
        skills:
        - TCP SYN/ACK scanning
        - UDP service probing
        - Stealth scanning techniques
        - IDS evasion strategies
        - Port state interpretation
        deliverables:
        - TCP connect scan establishing full connections to detect open ports
        - SYN scan sending half-open packets to detect ports without completing handshake
        - UDP scanning sending protocol-specific payloads to detect open UDP services
        - Service version detection parsing banners to identify running software versions
        estimated_hours: 8-12
      - id: vulnerability-scanner-m3
        name: Service Fingerprinting
        description: Identify specific service versions through probing and response analysis.
        acceptance_criteria:
        - HTTP server identification (Server header, behavior)
        - SSH version detection extracts protocol and software version from banner
        - SSL/TLS certificate analysis extracts issuer, expiry, and cipher suite information
        - Database service identification detects MySQL, PostgreSQL, Redis, and MongoDB instances
        - Match service responses against known signature database for accurate identification
        pitfalls:
        - Services may hide version info
        - Custom banners can mislead
        - SSL probing may fail on SNI
        concepts:
        - Service fingerprinting
        - Banner analysis
        - SSL/TLS analysis
        skills:
        - Protocol-specific probing
        - SSL/TLS certificate parsing
        - Version string extraction
        - Regular expression pattern matching
        - Service signature analysis
        deliverables:
        - Banner grabbing module connecting to open ports and capturing service identification strings
        - Service version detection matching banners against known software signature patterns
        - OS fingerprinting analyzing TCP/IP stack behavior to identify the host operating system
        - Technology stack identification detecting frameworks, languages, and platforms behind services
        estimated_hours: 8-12
      - id: vulnerability-scanner-m4
        name: Vulnerability Detection
        description: Check discovered services against known vulnerability databases.
        acceptance_criteria:
        - Query NVD/CVE database for version-based vulnerabilities
        - Check for common misconfigurations such as open directories and default pages
        - Test for specific vulnerabilities (e.g., default credentials)
        - Generate vulnerability report with severity scores
        - Cache CVE data for offline use
        pitfalls:
        - NVD API rate limits
        - CPE matching is imprecise
        - False positives from version detection
        concepts:
        - CVE/NVD database
        - CVSS scoring
        - CPE naming
        - Vulnerability assessment
        skills:
        - CVE database integration
        - API rate limiting handling
        - CVSS score interpretation
        - CPE string parsing
        - Vulnerability correlation
        deliverables:
        - Banner analysis module extracting version numbers for vulnerability correlation
        - CVE database integration querying NVD for known vulnerabilities by software version
        - Version-based vulnerability matching correlating detected versions with CVE entries
        - Configuration weakness checker testing for common misconfigurations and insecure defaults
        estimated_hours: 10-15
      - id: vulnerability-scanner-m5
        name: Report Generation
        description: Generate comprehensive scan reports in multiple formats.
        acceptance_criteria:
        - HTML report includes a summary dashboard with severity distribution charts
        - JSON export produces machine-readable output for pipeline automation integration
        - Group findings by severity level with critical issues listed first
        - Include remediation suggestions with specific steps for each vulnerability
        - Executive summary provides a non-technical overview suitable for management review
        pitfalls:
        - Large reports slow to generate
        - Missing context for findings
        - Overwhelming detail for executives
        concepts:
        - Report generation
        - Data visualization
        - Risk communication
        skills:
        - Multi-format report generation
        - Data aggregation and filtering
        - Risk prioritization
        - HTML/PDF rendering
        - Executive summary writing
        deliverables:
        - Vulnerability report structure organizing findings by host, service, and severity
        - Severity classification using CVSS scores to rank findings as critical, high, medium, or low
        - Remediation recommendations providing actionable fix guidance for each finding
        - Export formats supporting JSON for automation and HTML for human review
        estimated_hours: 6-10
    - id: binary-patcher
      name: Binary Instrumentation Tool
      description: ELF hook injection with trampolines, runtime patching via ptrace
      difficulty: advanced
      estimated_hours: 30-45
      essence: Runtime manipulation of executable code through instruction-level patching, trampoline-based control flow redirection, and direct memory modification of running processes via ptrace, requiring deep understanding of ELF format internals, instruction encoding, and process memory layout.
      why_important: Building this develops low-level systems programming expertise essential for security research, debugging tools, performance profilers, and dynamic analysis frameworks, providing foundational skills used in reverse engineering, vulnerability research, and production debugging tools like gdb and strace.
      learning_outcomes:
      - Parse and manipulate ELF file structures including section headers, program headers, and symbol tables
      - Implement instruction-level patching with correct opcode encoding and displacement calculation for x86-64/ARM architectures
      - Design trampoline mechanisms that preserve register state and redirect execution flow without corrupting program semantics
      - Inject executable code sections into binaries while maintaining proper alignment and relocation
      - Use ptrace system calls to attach to processes, read/write memory, and intercept execution at runtime
      - Handle position-independent code (PIC) and address space layout randomization (ASLR) during runtime patching
      - Debug memory corruption and segmentation faults in low-level binary manipulation code
      - Understand calling conventions, stack frames, and ABI requirements for cross-function hooking
      skills:
      - ELF Binary Format
      - x86-64 Assembly
      - Instruction Encoding
      - Process Memory Layout
      - ptrace System Calls
      - Runtime Code Injection
      - Trampoline Techniques
      - Binary Patching
      tags:
      - binary-patching
      - reverse-engineering
      - x86-64
      - elf
      - security
      architecture_doc: architecture-docs/binary-patcher/index.md
      languages:
        recommended:
        - C
        - Rust
        also_possible:
        - Go
      resources:
      - name: Binary Instrumentation Techniques
        url: https://blog.ret2.io/2017/11/16/dangerous-priv-escalation/
        type: article
      - name: ELF Modification (The Art of ELF)
        url: https://tmpout.sh/
        type: reference
      prerequisites:
      - type: project
        name: elf-parser
      - type: project
        name: disassembler
      milestones:
      - id: binary-patcher-m1
        name: Instruction-level Patching
        description: Read ELF binaries and patch individual instructions at specified offsets.
        acceptance_criteria:
        - Load ELF file and locate .text section with executable code
        - Replace instructions at given offsets with NOP sleds or new instructions
        - Maintain ELF structural integrity (headers, section sizes remain valid) after patching
        - Produce patched binary that executes correctly with modified behavior
        pitfalls:
        - Replacement instruction must be same size or padded with NOPs to fill original
        - Modifying .text size invalidates section offsets unless carefully adjusted
        - Patching branch targets requires recalculating relative offsets
        concepts:
        - Binary patching
        - Instruction replacement
        - ELF integrity
        skills:
        - ELF binary parsing and manipulation
        - Direct memory patching techniques
        - NOP padding and alignment
        - Offset calculation and validation
        deliverables:
        - ELF loader extracting .text section contents and addresses
        - Instruction-level patching at arbitrary offsets within code sections
        - NOP sled generation for padding replaced instructions
        - Binary writer producing valid patched ELF that loads and executes
        estimated_hours: 7-10
      - id: binary-patcher-m2
        name: Function Hooking with Trampolines
        description: Implement trampoline-based function interception.
        acceptance_criteria:
        - Detect function entry points from ELF symbol table
        - Replace function prologue with jump to trampoline code
        - Trampoline executes hook function then jumps to original function body
        - Hook can inspect and modify function arguments before original executes
        pitfalls:
        - x86-64 relative jump (E9) has ¬±2GB range, may need indirect jump for distant trampolines
        - Must save and restore all registers in trampoline to avoid corrupting original function state
        - Relocated original prologue instructions may contain RIP-relative addressing that needs fixup
        concepts:
        - Trampolines
        - Function hooking
        - Code relocation
        skills:
        - Assembly code generation for trampolines
        - Register preservation and restoration
        - RIP-relative addressing fixups
        - Far jump implementation for distant targets
        deliverables:
        - Function entry point detection from symbol table
        - Trampoline code generation with register save/restore
        - Prologue replacement with jump to trampoline
        - Original prologue relocation with RIP-relative fixups
        estimated_hours: 7-12
      - id: binary-patcher-m3
        name: Code Injection
        description: Inject new code sections into ELF binaries.
        acceptance_criteria:
        - Add new executable section (.inject) to ELF containing hook code
        - Update ELF headers and program headers to include injected section
        - Support injecting arbitrary C-compiled hook functions into target binary
        - Injected code can call functions from the original binary via symbol resolution
        pitfalls:
        - Adding sections may require shifting segment offsets and updating all headers
        - Injected code must be position-independent or relocated to its actual load address
        - New PT_LOAD segment needed if injected section doesn't fit in existing segment padding
        concepts:
        - ELF section injection
        - Code cave detection
        - Position-independent code
        skills:
        - ELF section header manipulation
        - Program header segment management
        - Position-independent code generation
        - Binary layout and alignment constraints
        deliverables:
        - Section injection adding new executable code to ELF
        - Header updates maintaining valid ELF structure after injection
        - Cross-reference resolution between injected and original code
        - Build pipeline compiling hook functions and injecting into target
        estimated_hours: 7-12
      - id: binary-patcher-m4
        name: Runtime Patching via ptrace
        description: Patch running processes using ptrace for runtime instrumentation.
        acceptance_criteria:
        - Attach to running process using PTRACE_ATTACH
        - Read and modify process memory using PTRACE_PEEKDATA/PTRACE_POKEDATA
        - Inject trampoline into running process's code at function entry points
        - Detach cleanly leaving process running with hooks active
        pitfalls:
        - Process is stopped during ptrace operations, minimize attach duration
        - Code pages may be read-only, must use mprotect (via injected syscall) to make writable
        - Instruction cache must be flushed after code modification on some architectures
        concepts:
        - Runtime instrumentation
        - Process memory manipulation
        - Live patching
        skills:
        - ptrace API for process control
        - Remote memory read/write operations
        - Memory protection manipulation via mprotect
        - Instruction cache invalidation
        deliverables:
        - Process attachment and memory read/write using ptrace
        - Runtime trampoline injection into running process code
        - Memory protection handling for modifying code pages
        - Clean detach protocol leaving hooks active in target process
        estimated_hours: 7-12
    expert:
    - id: build-tls
      name: Build Your Own TLS
      description: TLS 1.3 implementation
      difficulty: expert
      estimated_hours: 50-80
      essence: Binary protocol state machine implementing TLS 1.3 handshake sequencing, cryptographic primitive composition (ECDHE, HKDF, AEAD), and X.509 chain-of-trust verification over fragmented encrypted network streams.
      why_important: Building this demystifies how HTTPS secures the web by forcing you to implement modern cryptographic protocols from scratch, teaching skills directly applicable to security-critical systems and low-level network programming.
      learning_outcomes:
      - Implement TLS 1.3 state machine with proper handshake sequencing
      - Design binary protocol parsers with fragmentation and reassembly
      - Integrate cryptographic primitives (X25519, HKDF, AES-GCM) into protocol flow
      - Build X.509 certificate chain verification with trust anchor validation
      - Implement AEAD encryption for record layer protection
      - Debug timing-sensitive cryptographic operations and side-channel vulnerabilities
      - Handle key derivation schedules for handshake and application traffic keys
      - Implement zero-RTT resumption with replay attack mitigation
      skills:
      - Cryptographic Protocol Design
      - Binary Protocol Parsing
      - X.509 Certificate Verification
      - ECDHE Key Exchange
      - State Machine Implementation
      - AEAD Encryption
      - Network Security
      - Side-Channel Mitigation
      tags:
      - build-from-scratch
      - c
      - certificates
      - encryption
      - expert
      - go
      - handshake
      - pki
      - rust
      - security
      architecture_doc: architecture-docs/build-tls/index.md
      languages:
        recommended:
        - Rust
        - Go
        - C
        also_possible:
        - Python
        - Java
      resources:
      - type: specification
        name: RFC 8446 - TLS 1.3
        url: https://datatracker.ietf.org/doc/html/rfc8446
      - type: book
        name: Illustrated TLS 1.3
        url: https://tls13.xargs.org/
      prerequisites:
      - type: skill
        name: HTTPS client
      - type: skill
        name: AES implementation
      - type: skill
        name: Elliptic curve basics
      milestones:
      - id: build-tls-m1
        name: Record Layer
        description: Implement TLS record protocol for fragmenting and encrypting data.
        acceptance_criteria:
        - Record parser extracts 5-byte header with content type, protocol version, and length
        - Messages larger than 16KB are split into multiple TLS records correctly
        - Content type field routes records to correct handler for handshake, alert, or data
        - Fragmented handshake messages are reassembled into complete messages before processing
        pitfalls:
        - Version field confusion
        - Padding in TLS 1.3
        - Sequence number overflow
        concepts:
        - Record protocol
        - AEAD encryption
        - Protocol layering
        skills:
        - Binary protocol implementation
        - AEAD cipher integration
        - State management across layers
        deliverables:
        - Record header parsing extracting content type, version, and length fields
        - Fragmentation handling splitting large messages across multiple records
        - Content type handling routing handshake, alert, and application data records
        - Record reassembly combining fragmented messages into complete protocol messages
        estimated_hours: 8-12
      - id: build-tls-m2
        name: Key Exchange
        description: Implement ECDHE key exchange with X25519.
        acceptance_criteria:
        - Diffie-Hellman exchange produces identical shared secret on both client and server
        - ECDHE key exchange completes successfully using X25519 or P-256 curve parameters
        - Key derivation produces separate encryption keys for client-to-server and server-to-client
        - Pre-master secret is securely derived and never transmitted in plaintext
        pitfalls:
        - Endianness in key encoding
        - Transcript hash timing
        - Label format details
        concepts:
        - ECDH
        - Key derivation
        - Forward secrecy
        skills:
        - Elliptic curve cryptography
        - Secure random number generation
        - Key derivation functions
        - Constant-time operations
        deliverables:
        - Diffie-Hellman key exchange generating shared secret from public parameters
        - Elliptic curve key exchange using ECDHE with supported curve negotiation
        - Key derivation function expanding shared secret into encryption key material
        - Pre-master secret computation from selected key exchange algorithm output
        estimated_hours: 12-18
      - id: build-tls-m3
        name: Handshake Protocol
        description: Implement TLS 1.3 full handshake flow.
        acceptance_criteria:
        - ClientHello includes supported TLS versions, cipher suites, and SNI extension
        - ServerHello response selects mutually supported cipher suite and key exchange method
        - Handshake state machine rejects out-of-order messages with appropriate alert
        - Finished message verify data matches expected hash of all preceding handshake messages
        pitfalls:
        - Extension ordering
        - Transcript hash calculation
        - Cipher suite negotiation
        concepts:
        - Protocol negotiation
        - Message authentication
        - State machine
        skills:
        - Asynchronous state machine design
        - Cryptographic protocol implementation
        - Message parsing and serialization
        - Session management
        deliverables:
        - ClientHello message builder with supported cipher suites and extensions
        - ServerHello message parser extracting selected cipher suite and parameters
        - Handshake state machine tracking progression through handshake message sequence
        - Finished message verification confirming both parties derived matching keys
        estimated_hours: 15-25
      - id: build-tls-m4
        name: Certificate Verification
        description: Verify server certificate chain and signature.
        acceptance_criteria:
        - X.509 parser extracts subject name, issuer name, validity dates, and public key
        - Chain builder constructs path from server certificate to trusted root certificate
        - Signature verification confirms each certificate was signed by its issuer's key
        - Expired or not-yet-valid certificates are rejected with appropriate error message
        pitfalls:
        - Chain validation order
        - Wildcard matching
        - Signature algorithm selection
        concepts:
        - PKI
        - X.509
        - Certificate validation
        skills:
        - PKI chain validation
        - ASN.1 parsing
        - Digital signature verification
        - Certificate revocation checking
        deliverables:
        - X.509 certificate parsing extracting subject, issuer, and public key fields
        - Certificate chain building linking server certificate to trusted root CA
        - Signature verification validating each certificate signature in the chain
        - Certificate validity checking including expiration dates and revocation status
        estimated_hours: 15-25
    - id: anticheat-kernel-driver
      name: Vanguard Anti-Cheat Driver
      description: Build a kernel-mode driver to detect game cheats and protect memory integrity.
      difficulty: expert
      estimated_hours: 60-80
      essence: Kernel-mode security instrumentation using system-wide callbacks and page-table protections to monitor process memory integrity and prevent illegal instruction hooking or code injection.
      why_important: Anti-cheat systems are the ultimate cat-and-mouse game in security. Building one teaches you kernel-space programming, hardware-level protection mechanisms, and the deepest internals of operating systems.
      learning_outcomes:
      - Implement a Windows or Linux kernel driver with system-wide execution callbacks
      - Design memory integrity checks using cryptographic hashing of code sections
      - Build a hooking detection engine that monitors SSDT or IAT modification
      - Implement process protection using ObRegisterCallbacks to prevent handle duplication
      - Design a communication channel between user-mode and kernel-mode (IOCTL)
      - Debug kernel panics and BSODs using WinDbg or GDB with serial debugging
      - Implement hardware-assisted protection using VT-x or virtualization features
      - Validate system integrity by scanning for unauthorized driver signatures
      skills:
      - Kernel Programming
      - Security Research
      - Reverse Engineering
      - Windows/Linux Internals
      - C Programming
      tags:
      - security
      - kernel
      - anti-cheat
      - low-level
      languages:
        recommended:
        - C
        - C++
        - Assembly
        also_possible:
        - Rust
      milestones:
      - id: anticheat-kernel-driver-m1
        name: Kernel Entry & IOCTL
        description: Set up the driver entry point and establish communication with user-mode.
        acceptance_criteria:
        - Driver successfully loads and registers a device object
        - User-mode application can open a handle to the driver
        - IOCTL communication allows sending and receiving simple data packets
        - Clean unloading of the driver with no memory leaks or dangling objects
        pitfalls:
        - Memory leaks in non-paged pool
        - Improper handle closing in user-mode
        - Race conditions in IOCTL handlers
        concepts:
        - DriverEntry
        - IRP Handlers
        - IOCTL
        - Device Objects
        estimated_hours: 10
      - id: anticheat-kernel-driver-m2
        name: Process Monitoring & Protection
        description: Protect the game process from unauthorized access using kernel callbacks.
        acceptance_criteria:
        - Register process creation/deletion callbacks to monitor system activity
        - Implement handle-strip callbacks to prevent external tools from reading game memory
        - Successfully block access from tools like Cheat Engine or Process Hacker
        - Monitor for unauthorized thread injection into the target process
        pitfalls:
        - Over-aggressive blocking causing system instability
        - Recursive callbacks leading to stack overflow
        - Not handling system-critical processes
        concepts:
        - ObRegisterCallbacks
        - PsSetCreateProcessNotifyRoutine
        - Handle Stripping
        estimated_hours: 15
      - id: anticheat-kernel-driver-m3
        name: Memory Integrity Scanning
        description: Scan process memory for unauthorized modifications and instruction hooks.
        acceptance_criteria:
        - Verify code section integrity using pre-calculated hashes
        - Detect jumps or modifications in the IAT (Import Address Table)
        - Implement a scanning engine that walks the page tables to find executable r-x regions
        - Report integrity violations to the user-mode management service
        pitfalls:
        - Performance impact of full memory scans
        - False positives from JIT compilers
        - Handling dynamic relocation
        concepts:
        - Code Integrity
        - IAT Hooking
        - Page Table Walking
        - Memory Protection Flags
        estimated_hours: 20
- id: specialized
  name: Specialized
  icon: üîß
  subdomains:
  - name: Developer Tools
  - name: Advanced Networking
  - name: Embedded & Emulation
  projects:
    beginner:
    - id: markdown-renderer
      name: Markdown Renderer
      description: Markdown to HTML converter with CommonMark spec
      difficulty: beginner
      estimated_hours: 12-18
      essence: Text parsing with regular expressions and finite state machines to tokenize Markdown syntax, construct an abstract syntax tree representing document structure, and generate valid HTML through recursive tree traversal.
      why_important: Building this project develops fundamental compiler and parser skills applicable to template engines, code formatters, and DSL interpreters, while teaching how text processing tools that millions of developers use daily actually work under the hood.
      learning_outcomes:
      - Implement regex-based tokenization for multi-pass text parsing
      - Build a recursive descent parser to handle nested syntax structures
      - Design an abstract syntax tree to represent document hierarchy
      - Handle edge cases in ambiguous grammar rules and syntax conflicts
      - Implement tree traversal algorithms for HTML code generation
      - Debug pattern matching failures and parsing state management
      - Sanitize user input to prevent XSS attacks in generated HTML
      - Write comprehensive test suites covering CommonMark specification edge cases
      skills:
      - Regular Expressions
      - Recursive Parsing
      - AST Construction
      - State Machines
      - Tree Traversal
      - Text Tokenization
      - HTML Generation
      - Input Sanitization
      tags:
      - beginner-friendly
      - extensions
      - go
      - html-generation
      - javascript
      - python
      - syntax
      architecture_doc: architecture-docs/markdown-renderer/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Go
        also_possible:
        - Rust
        - Ruby
      resources:
      - name: CommonMark Spec
        url: https://spec.commonmark.org/
        type: specification
      - name: Building a Markdown Parser
        url: https://dev.to/kawaljain/building-my-own-markdown-parser-a-developers-journey-3b26
        type: tutorial
      - name: Sarvasv's MD Parser Notes
        url: https://sarvasvkulpati.com/mdparser
        type: tutorial
      prerequisites:
      - type: skill
        name: Regular expressions
      - type: skill
        name: String manipulation
      - type: skill
        name: HTML basics
      milestones:
      - id: markdown-renderer-m1
        name: Block Elements
        description: 'Parse block-level elements: headings, paragraphs, code blocks, horizontal rules.'
        acceptance_criteria:
        - 'Headings from # through ###### are converted to the corresponding h1 through h6 HTML elements'
        - Consecutive lines separated by blank lines are wrapped in paragraph p elements
        - Fenced code blocks with triple backticks and indented code blocks both render as pre and code elements
        - Horizontal rules from three or more dashes or asterisks are rendered as hr elements
        pitfalls:
        - Setext headings (underline style) need lookahead
        - Indented code vs list continuation is complex
        - Fenced code can have language identifier
        concepts:
        - Block parsing
        - State machines
        - Line-by-line processing
        skills:
        - Text parsing and tokenization
        - Pattern matching with regular expressions
        - State machine implementation
        - String manipulation techniques
        deliverables:
        - 'Heading parser that recognizes ATX-style hash prefixes from single # through ######'
        - Paragraph detector that groups consecutive non-blank lines into paragraph blocks
        - Fenced code block parser that extracts content between triple-backtick delimiters with optional language hints
        - Blockquote parser that handles greater-than prefixed lines including nested blockquotes
        estimated_hours: 4-5
      - id: markdown-renderer-m2
        name: Inline Elements
        description: 'Parse inline formatting: bold, italic, code, links, images.'
        acceptance_criteria:
        - Double asterisks and double underscores around text produce strong (bold) HTML elements
        - Single asterisks and single underscores around text produce em (italic) HTML elements
        - Backtick-delimited inline code renders as code HTML elements preserving inner whitespace
        - Bracket-parenthesis link syntax [text](url) renders as anchor elements with href attributes
        - Exclamation-bracket-parenthesis image syntax ![alt](url) renders as img elements with src and alt attributes
        pitfalls:
        - Underscore in middle_of_words shouldn't trigger emphasis
        - 'Mismatched delimiters: **bold* is invalid'
        - 'Escaping: \* should be literal asterisk'
        concepts:
        - Inline parsing
        - Regular expressions
        - Nested formatting
        skills:
        - Regular expression design and testing
        - Nested pattern matching
        - Character escaping and unescaping
        - String search and replace operations
        deliverables:
        - Bold and italic parser that detects asterisk and underscore emphasis markers at inline level
        - Inline code parser that extracts text between single backtick delimiters
        - Link parser that recognizes bracket-parenthesis syntax for hyperlink text and URL
        - Image parser that recognizes exclamation-bracket-parenthesis syntax for image alt text and URL
        estimated_hours: 4-5
      - id: markdown-renderer-m3
        name: Lists
        description: Parse ordered and unordered lists with nesting.
        acceptance_criteria:
        - Dash or asterisk prefixed lines render as unordered list ul with li items
        - Numeric-dot prefixed lines like 1. 2. 3. render as ordered list ol with li items
        - Indented list items nested under a parent item create properly nested ul or ol sub-lists
        - Tight lists without blank lines between items and loose lists with blank lines render differently per CommonMark spec
        pitfalls:
        - Mixed list types in same level is invalid
        - Continuation lines must be indented
        - Loose lists have paragraphs in items
        concepts:
        - Recursive parsing
        - Indentation tracking
        - Tree structures
        skills:
        - Recursive data structure handling
        - Tree traversal and manipulation
        - Indentation-based parsing
        - Nested list building
        deliverables:
        - Unordered list parser that recognizes dash and asterisk bullet markers at the start of lines
        - Ordered list parser that recognizes numeric-dot prefixes for sequentially numbered items
        - Nested list handler that detects indentation levels to create hierarchical list structures
        - List item content parser that handles multi-line items and inline formatting within list items
        estimated_hours: 4-5
      - id: markdown-renderer-m4
        name: HTML Generation
        description: Convert parsed structure to valid HTML output.
        acceptance_criteria:
        - Generated output is valid HTML5 that passes W3C validation without structural errors
        - Special characters including ampersand, less-than, greater-than, and quotes are escaped to HTML entities
        - HTML elements are properly nested with correct opening and closing tags and indentation
        - Optional wrapper template adds doctype, head, and body elements around the rendered content
        pitfalls:
        - 'Double-escaping: escape content before inline parsing'
        - 'Self-closing tags: <hr> not <hr></hr>'
        - Inline HTML in markdown should pass through
        concepts:
        - HTML generation
        - Character escaping
        - Tree traversal
        skills:
        - HTML tag generation and formatting
        - Safe HTML character escaping
        - Tree-to-string serialization
        - Output validation and testing
        deliverables:
        - HTML tag generator that wraps parsed content in the appropriate semantic HTML elements
        - Special character escaper that converts ampersands, angle brackets, and quotes to HTML entities
        - Pretty printer that outputs indented HTML with consistent formatting for human readability
        - Custom renderer plugin interface that allows users to override the output for specific element types
        estimated_hours: 3-4
    - id: hexdump
      name: Hexdump Utility
      description: Binary file viewer with hex/ASCII display
      difficulty: beginner
      estimated_hours: 8-12
      essence: Byte-level binary file reading with buffered I/O, hexadecimal and ASCII formatting engines with configurable column alignment and grouping, address offset calculation, and non-printable character mapping for visualizing raw file data.
      why_important: Building a hexdump utility teaches low-level file handling and bit manipulation patterns essential for systems programming, debugging, reverse engineering, and understanding how data is actually stored and structured in files.
      learning_outcomes:
      - Implement binary file reading with buffered I/O for efficient byte-level access
      - Design formatted output systems with aligned columns and configurable grouping
      - Build byte-to-hexadecimal conversion and ASCII mapping functions
      - Handle endianness considerations when displaying multi-byte values
      - Create command-line argument parsing for offset, length, and format options
      - Debug binary data representation issues across different file types
      - Implement character encoding handling for non-printable bytes in ASCII columns
      skills:
      - Binary File I/O
      - Hexadecimal Formatting
      - Bit Manipulation
      - CLI Argument Parsing
      - Buffered Reading
      - Character Encoding
      - Output Alignment
      tags:
      - beginner-friendly
      - binary
      - c
      - formatting
      - inspection
      - python
      - rust
      architecture_doc: architecture-docs/hexdump/index.md
      languages:
        recommended:
        - C
        - Python
        - Rust
        also_possible:
        - Go
        - JavaScript
      resources:
      - name: Let's Build a Hexdump Utility in C
        url: http://www.dmulholl.com/blog/lets-build-a-hexdump-utility.html
        type: tutorial
      - name: Hexdump man page
        url: https://man7.org/linux/man-pages/man1/hexdump.1.html
        type: reference
      prerequisites:
      - type: skill
        name: File I/O
      - type: skill
        name: Binary vs text
      - type: skill
        name: Formatted output
      milestones:
      - id: hexdump-m1
        name: Basic Hex Output
        description: Read binary file and output bytes in hexadecimal format.
        acceptance_criteria:
        - Read file in binary mode to preserve exact byte content without encoding
        - Output 16 bytes per line formatted as space-separated hex values
        - Show file offset in hexadecimal at the start of each output line
        - Handle files of any size by reading and processing in streaming chunks
        pitfalls:
        - Opening file in text mode mangles binary data
        - Last chunk may have fewer than 16 bytes
        - Large files should be streamed, not loaded entirely
        concepts:
        - Binary file I/O
        - Hexadecimal formatting
        - Chunked reading
        skills:
        - Binary file handling
        - Hexadecimal number formatting
        - Buffered file reading
        - Byte-level data manipulation
        deliverables:
        - File reading in fixed-size chunks for streaming binary processing
        - Byte to hex conversion formatting each byte as two-digit hexadecimal
        - Offset display showing file position at start of each output line
        - Byte grouping with space-separated hex pairs per output line
        estimated_hours: 2-3
      - id: hexdump-m2
        name: ASCII Column
        description: Add ASCII representation alongside hex output.
        acceptance_criteria:
        - Show printable ASCII characters (0x20-0x7E) in the right-side column
        - Replace non-printable bytes with '.' to prevent terminal corruption
        - Align ASCII column properly even on partial lines with fewer than 16 bytes
        - Handle partial final lines with padding to maintain column alignment
        pitfalls:
        - Don't print control characters (newlines, tabs corrupt output)
        - Alignment breaks on partial lines without padding
        - Unicode handling varies by terminal
        concepts:
        - ASCII encoding
        - String formatting
        - Column alignment
        skills:
        - ASCII character encoding
        - Control character filtering
        - String formatting and padding
        - Multi-column text alignment
        deliverables:
        - Printable character display showing ASCII representation of each byte
        - Non-printable character replacement using dot for control characters
        - Column alignment ensuring ASCII column lines up across all output lines
        - Line formatting combining offset, hex, and ASCII sections properly
        estimated_hours: 2-3
      - id: hexdump-m3
        name: Grouped Output
        description: Group hex bytes (2, 4, or 8 byte groupings) for better readability.
        acceptance_criteria:
        - Support -g option for group size accepting values of 1, 2, 4, or 8 bytes
        - Add extra space between groups while keeping bytes within groups adjacent
        - Default to 2-byte groups similar to xxd output format
        - Handle endianness display option for big-endian or little-endian multi-byte groups
        pitfalls:
        - Group boundaries at end of file need special handling
        - Endianness affects display for multi-byte groups
        - Different tools use different default groupings
        concepts:
        - Data grouping
        - Byte order
        - Flexible formatting
        skills:
        - Byte grouping and chunking
        - Endianness awareness
        - Flexible output formatting
        - Boundary condition handling
        deliverables:
        - 2-byte grouping option displaying hex pairs without internal spaces
        - 4-byte grouping option displaying hex quads for word-aligned viewing
        - Endianness display option for interpreting multi-byte group values
        - Custom grouping via command-line flag for flexible byte group sizes
        estimated_hours: 2-3
      - id: hexdump-m4
        name: CLI Options
        description: 'Add command-line options: offset, length, output format.'
        acceptance_criteria:
        - -s/--skip flag seeks to specified byte offset before starting hexdump output
        - -n/--length flag limits output to specified number of bytes from file
        - -C flag produces canonical hex+ASCII format matching standard hexdump -C output
        - Read from stdin when no file path is specified using dash as filename convention
        pitfalls:
        - Can't seek on stdin (must read and discard)
        - Length limit interacts with offset
        - Binary mode required for stdin
        concepts:
        - CLI design
        - Standard input
        - Argument parsing
        skills:
        - Command-line argument parsing
        - File seeking and positioning
        - Stream processing
        - Standard input/output handling
        deliverables:
        - Argument parsing supporting flags, options, and positional file path
        - Length limit option restricting output to specified number of bytes
        - Skip offset option starting hexdump at given byte offset in file
        - Output format selection choosing between canonical and grouped display
        estimated_hours: 2-3
    - id: diff-tool
      name: Diff Tool
      description: Text diff using LCS algorithm
      difficulty: beginner
      estimated_hours: 12-18
      essence: Sequence comparison through dynamic programming to compute edit distance and generate minimal change representations between text files using LCS and Myers' O(ND) diff algorithms.
      why_important: Building this teaches core algorithmic thinking through dynamic programming tables, memoization, and space-time tradeoffs while creating a practical tool that powers version control systems like Git.
      learning_outcomes:
      - Implement Longest Common Subsequence with dynamic programming tabulation and backtracking
      - Design Myers' diff algorithm with greedy forward search and O(ND) complexity optimization
      - Build edit distance calculation using dynamic programming state transitions
      - Generate unified diff format with context lines and hunk headers
      - Parse and tokenize text files into comparable sequence representations
      - Optimize space complexity using rolling arrays for DP tables
      - Implement command-line argument parsing with multiple output formats
      - Debug algorithm correctness using small test cases and edge conditions
      skills:
      - Dynamic Programming
      - Algorithm Analysis
      - String Processing
      - Edit Distance Algorithms
      - CLI Development
      - File I/O Operations
      - Output Formatting
      - Performance Optimization
      tags:
      - algorithms
      - beginner-friendly
      - go
      - hunks
      - javascript
      - lcs
      - patch
      - python
      - tool
      architecture_doc: architecture-docs/diff-tool/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Go
        also_possible:
        - C
        - Rust
        - Java
      resources:
      - name: Myers' Diff Algorithm Tutorial
        url: http://simplygenius.net/Article/DiffTutorial1
        type: tutorial
      - name: The Myers Difference Algorithm
        url: https://nathaniel.ai/myers-diff/
        type: article
      - name: Wikipedia - LCS
        url: https://en.wikipedia.org/wiki/Longest_common_subsequence
        type: reference
      prerequisites:
      - type: skill
        name: Dynamic programming basics
      - type: skill
        name: File I/O
      - type: skill
        name: String manipulation
      milestones:
      - id: diff-tool-m1
        name: Line Tokenization
        description: Read two files and split into line arrays for comparison.
        acceptance_criteria:
        - Read files handling different encodings such as UTF-8 and Latin-1
        - Split content by newlines while preserving empty lines in the sequence
        - Handle different line endings including LF, CRLF, and CR formats
        - Report line counts for each input file before comparison begins
        pitfalls:
        - Binary files will cause encoding errors
        - Large files can exhaust memory
        - Trailing newline handling varies between tools
        concepts:
        - File encoding
        - Line endings
        - Text normalization
        skills:
        - File I/O operations
        - String manipulation and parsing
        - Character encoding handling
        - Memory-efficient text processing
        deliverables:
        - File reading with line splitting preserving structure
        - Line normalization handling trailing whitespace differences
        - Empty line handling preserving blank lines in output
        - Encoding detection supporting UTF-8 and Latin-1 inputs
        estimated_hours: 2-3
      - id: diff-tool-m2
        name: LCS Algorithm
        description: Implement Longest Common Subsequence using dynamic programming.
        acceptance_criteria:
        - Build LCS length matrix from both input line sequences correctly
        - Backtrack through matrix to find the actual longest common subsequence
        - Handle input sequences of different lengths without index errors
        - Achieve O(mn) time and space complexity for the dynamic programming solution
        pitfalls:
        - Off-by-one errors in matrix indexing
        - Not handling empty sequences
        - Memory explosion for large files (optimize with Hirschberg)
        concepts:
        - Dynamic programming
        - 2D matrices
        - Backtracking
        skills:
        - Dynamic programming implementation
        - 2D array manipulation
        - Algorithm optimization
        - Recursive problem solving
        - Space-time complexity tradeoffs
        deliverables:
        - Longest common subsequence computation for line sequences
        - Dynamic programming table construction for optimal alignment
        - Backtracking implementation to recover actual LCS from table
        - Memory optimization to reduce space usage for large files
        estimated_hours: 4-5
      - id: diff-tool-m3
        name: Diff Generation
        description: Convert LCS result into diff hunks with context.
        acceptance_criteria:
        - Mark lines as unchanged, added, or deleted using diff algorithm output
        - Generate unified diff format with -/+ prefixes for changed lines
        - Group changes into hunks with configurable context lines between them
        - Include @@ line range markers showing hunk positions in both files
        pitfalls:
        - Off-by-one in line numbers (diff format is 1-indexed)
        - Forgetting to handle files with no common lines
        - Context overlapping between hunks
        concepts:
        - Diff algorithms
        - Unified diff format
        - Hunk generation
        skills:
        - Data structure transformation
        - Output formatting
        - Algorithm result interpretation
        - Line number tracking
        deliverables:
        - Edit script generation marking lines as added, deleted, or unchanged
        - Chunk (hunk) formation grouping nearby changes together
        - Context lines inclusion surrounding each hunk for readability
        - Unified diff format output compatible with standard tools
        estimated_hours: 4-5
      - id: diff-tool-m4
        name: CLI and Color Output
        description: Build command-line interface with colored output and options.
        acceptance_criteria:
        - Accept two file paths as command-line arguments for comparison
        - Color output shows red for deletions and green for additions in terminal
        - --no-color flag disables ANSI codes for plain text output
        - --context N flag sets the number of context lines around each hunk
        - Exit code 0 if files are same and 1 if files are different
        pitfalls:
        - ANSI codes break when piped to file
        - Windows needs special handling for colors
        - Exit codes are important for scripting
        concepts:
        - CLI argument parsing
        - ANSI colors
        - Exit codes
        - TTY detection
        skills:
        - Command-line argument parsing
        - Terminal output formatting
        - Cross-platform compatibility
        - Process exit code management
        - ANSI color code usage
        deliverables:
        - Argument parsing for file paths and formatting options
        - ANSI color output for visual diff highlighting in terminal
        - Context line count configuration via command-line flag
        - Side-by-side display option for parallel comparison view
        estimated_hours: 3-4
    - id: elf-parser
      name: ELF Binary Parser
      description: Parse ELF headers, sections, segments, symbol tables
      difficulty: beginner
      estimated_hours: 10-15
      essence: Low-level binary format parsing, interpreting multi-byte structures and pointer indirection, understanding the linking and loading process that transforms on-disk ELF files into executable process memory layouts.
      why_important: Building this demystifies the executable format underlying every Linux program and provides foundational knowledge for systems programming, reverse engineering, debugging, and understanding how compilers, linkers, and loaders work together.
      learning_outcomes:
      - Parse binary file headers with multi-byte integer fields and structure alignment
      - Implement section header table traversal to locate code, data, and metadata sections
      - Extract and decode symbol table entries to map function and variable names to addresses
      - Parse relocation entries to understand how linkers patch addresses at link-time
      - Decode program headers to understand segment loading and memory mapping
      - Handle different ELF variants for 32-bit and 64-bit architectures
      - Build a command-line tool that displays structured binary data in human-readable format
      - Debug binary parsing issues using hexdump and readelf for verification
      skills:
      - Binary Format Parsing
      - Structure Packing and Alignment
      - File I/O and Byte Manipulation
      - Symbol Table Processing
      - Dynamic Linking Concepts
      - Systems Programming
      - Endianness Handling
      tags:
      - binary-format
      - elf
      - systems-programming
      - parsing
      - low-level
      architecture_doc: architecture-docs/elf-parser/index.md
      languages:
        recommended:
        - C
        - Rust
        also_possible:
        - Go
        - Python
      resources:
      - name: ELF Specification
        url: https://refspecs.linuxfoundation.org/elf/elf.pdf
        type: reference
      - name: 'In-depth: ELF - The Extensible & Linkable Format'
        url: https://www.cavestory.org/guides/csguide/download/elf_format.pdf
        type: article
      prerequisites:
      - type: skill
        name: C or Rust basics
      - type: project
        name: hexdump
      milestones:
      - id: elf-parser-m1
        name: ELF Header and Sections
        description: Parse the ELF header and section header table.
        acceptance_criteria:
        - Read and validate ELF magic bytes (0x7f 'E' 'L' 'F') at file offset 0
        - Parse ELF header fields including class (32/64-bit), endianness, type, machine, and entry point
        - Read the section header table using e_shoff and e_shnum from the ELF header
        - Display section names by reading the section header string table (e_shstrndx)
        pitfalls:
        - Endianness must match the ELF file's declared endianness
        - Section header string table index can be SHN_UNDEF if no sections exist
        - 32-bit and 64-bit ELF have different struct sizes
        concepts:
        - Binary file parsing
        - ELF header structure
        - Section headers
        skills:
        - Binary file I/O in C/Rust
        - Struct memory layout and padding
        - Parsing fixed-size binary structures
        - Validating magic numbers and file formats
        deliverables:
        - ELF magic byte validation rejecting non-ELF files
        - ELF header display showing class, endianness, type, machine, entry point
        - Section header table parsing with name resolution from string table
        - Support for both 32-bit and 64-bit ELF formats
        estimated_hours: 3-4
      - id: elf-parser-m2
        name: Symbol Tables and Relocations
        description: Parse .symtab, .dynsym, and relocation sections.
        acceptance_criteria:
        - Parse .symtab and .dynsym sections to extract symbol names, types, bindings, and values
        - Resolve symbol names from the corresponding string table (.strtab or .dynstr)
        - Parse relocation sections (.rel and .rela) showing relocation types and target symbols
        - Display output formatted like readelf -s and readelf -r
        pitfalls:
        - Symbol name index points into different string tables depending on section
        - Relocation entries reference symbols by index into the symbol table
        - STT_SECTION symbols use section names, not the symbol string table
        concepts:
        - Symbol tables
        - String table lookup
        - Relocation entries
        skills:
        - Array indexing and offset calculations
        - String table dereferencing
        - Parsing variable-length data structures
        - Symbol visibility and binding types
        deliverables:
        - Symbol table parser extracting name, type, binding, visibility, and value
        - String table resolution for both .strtab and .dynstr sections
        - Relocation section parsing for .rel and .rela formats
        - Formatted output matching readelf display conventions
        estimated_hours: 3-4
      - id: elf-parser-m3
        name: Dynamic Linking Info
        description: Parse program headers and dynamic section for shared library dependencies.
        acceptance_criteria:
        - Parse program header table showing segment types, offsets, virtual addresses, and flags
        - Extract .dynamic section entries including DT_NEEDED (shared library dependencies)
        - Resolve library names from the dynamic string table
        - Display complete ELF summary similar to readelf -a output
        pitfalls:
        - Program headers may overlap with section headers for the same data
        - DT_NEEDED entries store string table offsets, not direct names
        - PT_INTERP segment contains the dynamic linker path as a null-terminated string
        concepts:
        - Program headers and segments
        - Dynamic linking
        - Shared library dependencies
        skills:
        - Understanding memory segments vs sections
        - Parsing null-terminated strings from binary data
        - Working with dynamic linker metadata
        - Extracting dependency information from binaries
        deliverables:
        - Program header table parser showing all segment types and attributes
        - Dynamic section parser extracting DT_NEEDED and other dynamic entries
        - Library dependency resolution from dynamic string table
        - Comprehensive ELF summary display combining all parsed information
        estimated_hours: 3-4
    intermediate:
    - id: config-parser
      name: Config File Parser
      description: Parse INI, TOML, and YAML formats
      difficulty: intermediate
      estimated_hours: 15-25
      essence: Character stream tokenization through finite state machines, recursive descent parsing translating grammar productions into mutually-recursive functions, and context-sensitive indentation handling that transforms three syntactically distinct formats (flat INI, table-based TOML, whitespace-hierarchical YAML) into unified in-memory data structures.
      why_important: Configuration file parsing is ubiquitous in software systems‚Äîfrom web servers to compilers‚Äîand building parsers from scratch teaches fundamental compiler frontend techniques (lexing, tokenization, recursive descent) that transfer directly to DSL design, data serialization, and language tooling.
      learning_outcomes:
      - Implement a lexer/tokenizer that converts character streams into semantic tokens
      - Design recursive descent parser functions mapping grammar rules to code
      - Build finite state machines for recognizing patterns in configuration syntax
      - Handle indentation-based parsing for hierarchical data structures
      - Transform parsed tokens into language-native data structures (maps, arrays, nested objects)
      - Debug parsing ambiguities and implement error recovery strategies
      - Validate parsed data against format specifications and handle edge cases
      - Compare tradeoffs between hand-written parsers and parser generator tools
      skills:
      - Lexical Analysis
      - Recursive Descent Parsing
      - Grammar Design
      - Tokenization
      - State Machine Implementation
      - Data Structure Mapping
      - Format Specification Compliance
      - Parser Error Handling
      tags:
      - compilers
      - configuration
      - formats
      - go
      - intermediate
      - merging
      - parsing
      - python
      - rust
      - validation
      architecture_doc: architecture-docs/config-parser/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible:
        - JavaScript
        - C
      resources:
      - name: TOML Specification
        url: https://toml.io/en/v1.0.0
        type: specification
      - name: INI File Format
        url: https://en.wikipedia.org/wiki/INI_file
        type: reference
      - name: Writing a Parser in Python
        url: https://www.freecodecamp.org/news/how-to-write-a-parser-in-python/
        type: tutorial
      prerequisites:
      - type: skill
        name: String manipulation
      - type: skill
        name: Data structures
      - type: skill
        name: Recursive thinking
      milestones:
      - id: config-parser-m1
        name: INI Parser
        description: Parse INI files with sections, key-value pairs, and comments.
        acceptance_criteria:
        - Parse [section] headers and create nested structure correctly
        - 'Parse key=value and key: value pairs with whitespace trimming'
        - 'Handle ; and # comment lines by ignoring them during parsing'
        - Support quoted strings with proper escape sequence processing
        - Return nested dictionary structure with sections as top-level keys
        pitfalls:
        - Not handling keys outside of sections (global keys)
        - Forgetting to handle inline comments after values
        - Breaking on = inside quoted strings
        concepts:
        - Line-based parsing
        - Regular expressions
        - Type coercion
        skills:
        - String parsing and manipulation
        - Regular expression pattern matching
        - Configuration file I/O
        - Data validation and type conversion
        deliverables:
        - Section header parsing with bracket notation support
        - Key-value pair parsing for both equals and colon delimiters
        - Comment handling for semicolon and hash prefixes
        - Multi-line value continuation with proper joining
        estimated_hours: 3-4
      - id: config-parser-m2
        name: TOML Tokenizer
        description: Build a lexer/tokenizer for TOML format.
        acceptance_criteria:
        - Tokenize brackets, dots, equals, strings, and numbers into typed tokens
        - Handle basic strings, literal strings, and multiline string variants correctly
        - Recognize integers, floats, booleans, and date-time values as distinct tokens
        - Track line and column numbers for meaningful error message reporting
        pitfalls:
        - TOML multiline strings have complex rules for leading newlines
        - Literal strings don't process escapes (backslash is literal)
        - 'Integer underscores are allowed: 1_000_000'
        concepts:
        - Lexical analysis
        - State machines
        - Unicode handling
        skills:
        - Tokenization and lexical analysis
        - Finite state machine implementation
        - Character stream processing
        - Unicode string handling
        deliverables:
        - Token type definitions for all TOML grammar elements
        - String literal handling with basic, literal, and multiline variants
        - Date and time value parsing into appropriate token types
        - Array and table tokens with proper bracket recognition
        estimated_hours: 4-5
      - id: config-parser-m3
        name: TOML Parser
        description: Build recursive descent parser for TOML tables and arrays.
        acceptance_criteria:
        - Parse [table] and [table.subtable] headers into nested dictionary structure
        - Parse [[array.of.tables]] into list of dictionary entries correctly
        - Handle inline tables using { key = value } syntax within expressions
        - Handle inline arrays using [ 1, 2, 3 ] syntax with mixed types
        - Dotted keys like physical.color = 'orange' create nested structure correctly
        pitfalls:
        - Can't redefine a key that already exists
        - Tables can be defined implicitly by dotted keys
        - Array of tables vs array value have different syntax
        concepts:
        - Recursive descent parsing
        - Symbol tables
        - Nested data structures
        skills:
        - Recursive parsing algorithms
        - AST construction and traversal
        - Namespace and scope management
        - Complex data structure manipulation
        deliverables:
        - Table parsing with nested dotted key path support
        - Array of tables parsing with double-bracket syntax
        - Inline tables and inline arrays with nested value support
        - Value type inference for automatic type conversion
        estimated_hours: 5-6
      - id: config-parser-m4
        name: YAML Subset Parser
        description: 'Parse a subset of YAML: indentation-based nesting, mappings, sequences.'
        acceptance_criteria:
        - Indentation-based block structure determines nesting depth correctly
        - 'Key: value mappings are parsed and stored as dictionary entries'
        - Sequences with - prefix are parsed into ordered list structures
        - Quoted and unquoted strings are handled with proper type inference
        - Flow style [list] and {map} inline syntax is parsed correctly
        pitfalls:
        - Tabs vs spaces (YAML forbids tabs for indentation)
        - Implicit type detection can be surprising (yes = true, 1.0 = float)
        - Multiline strings have multiple syntaxes (|, >, etc.)
        concepts:
        - Indentation-sensitive parsing
        - Implicit typing
        - Stack-based parsing
        skills:
        - Indentation-based syntax parsing
        - Context-aware parsing with stack management
        - Dynamic type inference
        - Multi-format data structure mapping
        deliverables:
        - Indentation-based structure detection and nesting resolution
        - Mapping parsing for key-value pairs with colon separator
        - Sequence parsing for list items with dash prefix
        - Scalar type inference for strings, numbers, and booleans
        estimated_hours: 5-6
    - id: packet-sniffer
      name: Packet Sniffer
      description: Network packet capture and protocol parsing
      difficulty: intermediate
      estimated_hours: 20-30
      essence: Raw packet capture from network interfaces, binary protocol header parsing across OSI layers (Ethernet, IPv4, TCP/UDP), and Berkeley Packet Filter expression evaluation for selective traffic inspection.
      why_important: Building this teaches you network protocol internals and binary data manipulation that are essential for network programming, security analysis, and debugging distributed systems.
      learning_outcomes:
      - Implement raw socket or libpcap-based packet capture with promiscuous mode configuration
      - Parse binary Ethernet, IPv4, TCP, and UDP headers using C structures and byte manipulation
      - Design and apply Berkeley Packet Filter (BPF) expressions for selective packet filtering
      - Handle network byte order conversion between network and host representations
      - Build protocol dissectors that recursively parse nested protocol headers
      - Debug network-level issues by inspecting packet contents and protocol flags
      - Implement output formatting for displaying captured packet metadata and payloads
      - Work with kernel-level networking APIs and understand datalink layer operations
      skills:
      - Raw Socket Programming
      - Binary Protocol Parsing
      - Network Packet Analysis
      - BPF Filter Syntax
      - libpcap API
      - Byte Order Conversion
      - Protocol State Tracking
      - Network Layer Programming
      tags:
      - analysis
      - c
      - go
      - intermediate
      - networking
      - packet-capture
      - python
      - raw-sockets
      architecture_doc: architecture-docs/packet-sniffer/index.md
      languages:
        recommended:
        - C
        - Python
        - Go
        also_possible:
        - Rust
      resources:
      - name: Libpcap Programming Tutorial
        url: https://www.tcpdump.org/pcap.html
        type: tutorial
      - name: Building a Packet Sniffer from Scratch
        url: https://aidanvidal.github.io/posts/Packet_Sniffer.html
        type: tutorial
      - name: Scapy Documentation
        url: https://scapy.readthedocs.io/
        type: reference
      prerequisites:
      - type: skill
        name: Networking basics
      - type: skill
        name: TCP/IP model
      - type: skill
        name: Binary parsing
      milestones:
      - id: packet-sniffer-m1
        name: Packet Capture Setup
        description: Set up packet capture using raw sockets or libpcap.
        acceptance_criteria:
        - List all available network interfaces by name
        - Open selected interface for packet capture
        - Capture packets in promiscuous mode receiving all traffic
        - Handle root or admin permission requirements gracefully
        pitfalls:
        - Requires root/admin privileges
        - Interface names vary by OS (eth0, en0, etc.)
        - Virtual interfaces may not support promiscuous mode
        concepts:
        - Raw sockets
        - Network interfaces
        - Privilege requirements
        skills:
        - Network programming
        - System-level I/O operations
        - Working with privileged operations
        - Platform-specific API usage
        deliverables:
        - Raw socket creation with appropriate protocol family
        - Promiscuous mode activation on selected interface
        - BPF filter compilation and attachment option
        - Ring buffer for captured packet storage
        estimated_hours: 4-5
      - id: packet-sniffer-m2
        name: Ethernet Frame Parsing
        description: Parse Ethernet frames to extract MAC addresses and protocol type.
        acceptance_criteria:
        - Extract destination MAC address from first 6 bytes
        - Extract source MAC address from bytes 6 through 12
        - Extract EtherType field from bytes 12 through 14
        - Format MAC addresses as colon-separated hex string
        pitfalls:
        - 802.1Q VLAN tags add 4 extra bytes
        - Jumbo frames can exceed 1500 bytes payload
        - Byte order is big-endian (network order)
        concepts:
        - Ethernet framing
        - MAC addresses
        - EtherType
        skills:
        - Binary data parsing
        - Working with network byte order
        - Struct packing and alignment
        - Bitwise operations for field extraction
        deliverables:
        - Ethernet header parsing extracting all 14 bytes
        - Source and destination MAC address extraction and formatting
        - EtherType field handling for protocol identification
        - Frame validation including minimum size and CRC checks
        estimated_hours: 3-4
      - id: packet-sniffer-m3
        name: IP Header Parsing
        description: Parse IPv4 headers to extract addresses and protocol.
        acceptance_criteria:
        - Extract IP version and header length from first byte
        - Extract total length, TTL, and protocol number fields
        - Extract source and destination IP addresses correctly
        - Handle variable header length when IP options are present
        pitfalls:
        - Header length is in 32-bit words, not bytes
        - Fragmented packets need reassembly
        - Options make header length variable
        concepts:
        - IP addressing
        - Protocol numbers
        - Header fields
        skills:
        - IPv4 protocol implementation
        - Variable-length header parsing
        - Network address manipulation
        - Handling protocol field enumerations
        deliverables:
        - IPv4 header parsing with all standard fields
        - IPv6 header parsing with next header chain support
        - Protocol field extraction identifying transport layer
        - Source and destination IP address extraction and formatting
        estimated_hours: 4-5
      - id: packet-sniffer-m4
        name: TCP/UDP Parsing
        description: Parse TCP and UDP headers for port information and flags.
        acceptance_criteria:
        - Extract source and destination port numbers correctly
        - Extract TCP flags including SYN, ACK, FIN, RST, and PSH
        - Extract TCP sequence and acknowledgment number fields
        - Identify common services by well-known port numbers
        pitfalls:
        - TCP data offset is in 32-bit words
        - Port numbers are unsigned 16-bit
        - Don't confuse TCP and UDP despite similar port fields
        concepts:
        - TCP flags
        - Port numbers
        - Transport layer
        skills:
        - Transport layer protocol parsing
        - Port-based service identification
        - TCP state machine understanding
        - Distinguishing protocol-specific fields
        deliverables:
        - TCP header parsing including flags and sequence numbers
        - UDP header parsing with length and checksum fields
        - Source and destination port extraction for both protocols
        - Payload extraction after transport header boundary
        estimated_hours: 4-5
      - id: packet-sniffer-m5
        name: Filtering and Output
        description: Add BPF filters and formatted output display.
        acceptance_criteria:
        - Support BPF filter expressions for capture-time filtering
        - Filter captured packets by protocol type (TCP, UDP, ICMP)
        - Filter captured packets by port number or IP address
        - Display formatted packet summary with protocol details
        pitfalls:
        - BPF syntax errors are cryptic
        - High traffic can overwhelm display
        - Timestamps should be high-resolution
        concepts:
        - Berkeley Packet Filter
        - Traffic filtering
        - Packet analysis
        skills:
        - Packet filtering with BPF syntax
        - High-volume data stream processing
        - Timestamp precision handling
        - Formatted packet display output
        deliverables:
        - Display filter supporting protocol and address criteria
        - Formatted packet summary with timestamp and key fields
        - Hex dump output option showing raw packet bytes
        - PCAP file export for offline analysis in Wireshark
        estimated_hours: 5-6
    - id: profiler
      name: CPU/Memory Profiler
      description: Sampling profiler with flame graphs, memory tracking
      difficulty: intermediate
      estimated_hours: '45'
      essence: Signal-driven periodic stack sampling with symbol table resolution to capture and aggregate call hierarchies, combined with heap allocation tracking and hierarchical flame graph visualization to identify performance bottlenecks in running programs.
      why_important: Building this teaches you fundamental systems programming concepts like signal handling, stack unwinding, and symbol table parsing, while developing practical skills in performance analysis that are critical for optimizing production systems across any technology stack.
      learning_outcomes:
      - Understand sampling-based profiling
      - Implement stack trace collection
      - Build flame graph visualization
      - Design memory allocation tracking
      skills:
      - Stack sampling
      - Timer signals
      - Symbol resolution
      - Flame graphs
      - Memory tracking
      - Profile analysis
      tags:
      - cpu
      - flame-graphs
      - instrumentation
      - intermediate
      - memory
      - sampling
      - tool
      architecture_doc: architecture-docs/profiler/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible: []
      resources:
      - name: Flame Graphs
        url: https://www.brendangregg.com/flamegraphs.html
        type: documentation
      - name: Python Profilers Official Docs
        url: https://docs.python.org/3/library/profile.html
        type: documentation
      - name: Linux perf Tutorial
        url: https://perfwiki.github.io/main/tutorial/
        type: tutorial
      - name: Go pprof Package
        url: https://pkg.go.dev/runtime/pprof
        type: documentation
      - name: Memory Profiling Introduction
        url: https://easyperf.net/blog/2024/02/12/Memory-Profiling-Part1
        type: article
      prerequisites:
      - type: skill
        name: C programming
      - type: skill
        name: Process internals
      - type: skill
        name: Debug symbols
      milestones:
      - id: profiler-m1
        name: Stack Sampling
        description: Periodically sample call stacks
        acceptance_criteria:
        - Sample call stacks at configurable regular time intervals
        - Capture both user-space and kernel stack frames when available
        - Support different sampling rates from 10Hz to 10KHz
        - Support targeting specific processes or individual threads
        pitfalls:
        - Signal handler performing unsafe operations like malloc
        - Not handling stripped binaries without frame pointers
        - Sampling too frequently causing performance overhead
        - Race conditions when accessing thread stack data
        - Missing samples in tight loops or syscalls
        concepts:
        - Signal-based interruption for deterministic sampling intervals
        - Unwinding stack frames through base pointers or frame pointers
        - Thread-safe stack capture in signal handlers
        - Sampling bias and frequency tradeoffs
        skills:
        - Signal handling
        - Stack walking
        - Timer setup
        deliverables:
        - Timer-based sampling using SIGPROF or ITIMER configuration
        - Signal handler capturing call stack snapshot on each sample
        - Stack frame walking traversing frame pointers to build call chain
        - Sample storage accumulating captured stacks for later analysis
        - Configurable sampling frequency controlling overhead versus accuracy
        - Low-overhead design minimizing impact on profiled application
        estimated_hours: '11'
      - id: profiler-m2
        name: Symbol Resolution
        description: Convert addresses to function names
        acceptance_criteria:
        - Load symbol tables from ELF binaries and shared libraries
        - Resolve raw memory addresses to human-readable function names
        - Parse DWARF debug information for source file and line numbers
        - Support JIT-compiled code symbol resolution when mapping is available
        pitfalls:
        - Forgetting to account for ASLR when resolving addresses
        - Not handling stripped binaries gracefully
        - Slow symbol resolution blocking the profiler pipeline
        - Missing symbols for dynamically loaded libraries
        - Inlined functions appearing as parent function names
        concepts:
        - DWARF debug information format and line number tables
        - Symbol table parsing from ELF or Mach-O binaries
        - Address space layout randomization affecting symbol lookup
        - Inline function unwinding and virtual method resolution
        skills:
        - Debug symbols
        - DWARF
        - addr2line
        deliverables:
        - Symbol table loading from ELF binary and shared library files
        - Address-to-function mapping resolving instruction pointers to names
        - Source file and line number information from debug metadata
        - C++ name demangling converting mangled symbols to readable names
        - Shared library symbol handling resolving addresses in loaded .so files
        - Symbol cache reducing repeated lookups for hot addresses
        estimated_hours: '11'
      - id: profiler-m3
        name: Flame Graph Generation
        description: Visualize profiles as flame graphs
        acceptance_criteria:
        - Aggregate captured samples by unique call stack signature
        - Generate interactive SVG flame graph from aggregated stack data
        - Support zoom and search interaction within flame graph visualization
        - Support inverted flame graph (icicle chart) showing callee perspective
        pitfalls:
        - Not normalizing stack depths causing unbalanced graphs
        - Poor color choices making graphs unreadable
        - Missing interactivity like zoom and search
        - Flame graph width not representing actual time percentages
        - Truncating long function names without tooltips
        concepts:
        - Stack folding and aggregation for visualization
        - Hierarchical data structures for call tree representation
        - SVG coordinate systems and responsive scaling
        - Color coding by call frequency or self-time
        skills:
        - Data aggregation
        - SVG generation
        - Interactive UI
        deliverables:
        - Stack aggregation merging identical call chains with sample counts
        - Folded stack format output compatible with standard flame graph tools
        - Interactive SVG flame graph with color-coded function categories
        - Color coding distinguishing user code, library, and kernel functions
        - Zoom and search functionality for navigating large flame graphs
        - Differential flame graph comparing two profiles side by side
        estimated_hours: '11'
      - id: profiler-m4
        name: Memory Profiling
        description: Track memory allocations
        acceptance_criteria:
        - Track all heap allocations recording size and call stack origin
        - Identify top allocation call sites ranked by total bytes allocated
        - Detect memory leaks by finding allocations never freed before exit
        - Generate allocation flame graph showing memory-heavy code paths
        pitfalls:
        - Recursive malloc calls when intercepting allocation functions
        - Not tracking realloc properly causing incorrect leak reports
        - Excessive memory overhead from tracking metadata
        - Missing allocations from static constructors or early init
        - Thread-local storage allocations appearing as leaks
        concepts:
        - Function interposition via LD_PRELOAD or dynamic linking
        - Tracking allocation metadata without excessive overhead
        - Distinguishing leaks from long-lived allocations
        - Backtrace capture at allocation sites
        skills:
        - Malloc interception
        - Allocation tracking
        - Leak detection
        deliverables:
        - Malloc and free interception hooking allocation functions
        - Allocation size tracking recording bytes allocated per call site
        - Stack trace capture at each allocation recording calling context
        - Memory usage timeline showing heap size over program execution
        - Leak detection identifying allocations without matching free calls
        - Memory allocation flame graph showing hot allocation call paths
        estimated_hours: '11'
    - id: simd-library
      name: SIMD Optimization Library
      description: SSE/AVX intrinsics for string search, memcpy, checksum
      difficulty: intermediate
      estimated_hours: 20-30
      essence: Data-parallel computation using CPU vector registers to process multiple values simultaneously, requiring explicit memory alignment, register management, and understanding of instruction-level parallelism to achieve 4-16x performance gains over scalar code.
      why_important: SIMD optimization is critical for performance-sensitive applications like multimedia processing, scientific computing, and game engines, and teaches low-level performance engineering skills that separate competent programmers from systems experts who can extract maximum hardware efficiency.
      learning_outcomes:
      - Implement memory operations using 128-bit and 256-bit vector registers with proper alignment
      - Write intrinsic functions that map directly to assembly instructions without inline assembly
      - Design data layouts that maximize cache efficiency and minimize shuffle operations
      - Benchmark SIMD code against scalar implementations to measure actual speedup
      - Analyze compiler auto-vectorization output and identify optimization opportunities
      - Debug alignment faults and understand memory boundary requirements for vector loads
      - Compare hand-written SIMD with compiler-generated code to evaluate trade-offs
      - Implement horizontal operations that reduce vectors to scalar results efficiently
      skills:
      - Vector Instructions
      - Memory Alignment
      - Performance Benchmarking
      - Assembly Analysis
      - CPU Architecture
      - Data-Parallel Programming
      - Compiler Optimization
      - Cache Optimization
      tags:
      - simd
      - performance
      - vectorization
      - cpu-architecture
      - optimization
      architecture_doc: architecture-docs/simd-library/index.md
      languages:
        recommended:
        - C
        - Rust
        also_possible:
        - C++
      resources:
      - name: Intel Intrinsics Guide
        url: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html
        type: reference
      - name: SIMD for C++ Developers
        url: http://const.me/articles/simd/simd.pdf
        type: article
      prerequisites:
      - type: skill
        name: C programming
      - type: skill
        name: CPU architecture basics
      milestones:
      - id: simd-library-m1
        name: SSE2 Basics (memset/memcpy)
        description: Implement basic memory operations using SSE2 128-bit registers.
        acceptance_criteria:
        - Implement SIMD memset using _mm_store_si128 processing 16 bytes per iteration
        - Implement SIMD memcpy using _mm_load_si128/_mm_store_si128 pairs
        - Handle alignment requirements with scalar prologue/epilogue for unaligned edges
        - Benchmark showing measurable speedup over scalar versions for large buffers
        pitfalls:
        - Aligned loads/stores require 16-byte aligned pointers, use unaligned variants for arbitrary data
        - Scalar fallback needed for buffers smaller than 16 bytes
        - Compiler may auto-vectorize scalar code, use volatile or compiler barriers for fair benchmarks
        concepts:
        - SSE2 intrinsics
        - Memory alignment
        - SIMD registers
        skills:
        - SSE2 intrinsics programming
        - Memory alignment optimization
        - SIMD register management
        - Performance benchmarking basics
        deliverables:
        - SIMD memset filling 16 bytes per iteration with alignment handling
        - SIMD memcpy copying 16 bytes per iteration with load/store pairs
        - Scalar prologue/epilogue for unaligned buffer edges
        - Benchmark comparing SIMD vs scalar throughput across buffer sizes
        estimated_hours: 5-7
      - id: simd-library-m2
        name: String Operations (strlen/memchr)
        description: Implement SIMD-optimized string search and scanning functions.
        acceptance_criteria:
        - Implement SIMD strlen using _mm_cmpeq_epi8 to find null terminators in 16-byte chunks
        - Implement SIMD memchr using comparison and movemask to find bytes in parallel
        - Use _mm_movemask_epi8 to convert comparison results to a bitmask for first-match detection
        - Handle edge cases at string boundaries and page boundaries safely
        pitfalls:
        - Reading past string end can cause page fault if crossing page boundary
        - movemask returns 16-bit mask, use __builtin_ctz to find first set bit
        - Must handle the case where match is in the alignment padding, not actual data
        concepts:
        - Parallel comparison
        - Bitmask extraction
        - Safe memory access patterns
        skills:
        - Bitwise operations for parallel comparison
        - Efficient string scanning with SIMD
        - Safe memory boundary handling
        deliverables:
        - SIMD strlen scanning 16 bytes per iteration for null terminator
        - SIMD memchr finding target byte across 16 bytes simultaneously
        - Bitmask-based first-match detection using movemask and bit scanning
        - Safe handling of buffer boundaries without over-reading
        estimated_hours: 5-7
      - id: simd-library-m3
        name: Math Operations (dot product/matrix)
        description: Implement SIMD-optimized mathematical operations.
        acceptance_criteria:
        - Implement SIMD dot product for float arrays using _mm_mul_ps and horizontal add
        - Implement 4x4 matrix multiplication using SSE for all row-column products
        - Support both SSE (128-bit) and AVX (256-bit) code paths with runtime detection
        - Benchmark showing significant speedup for matrix operations
        pitfalls:
        - Horizontal adds are slow on some CPUs, prefer shuffles and vertical adds
        - Matrix layout (row-major vs column-major) affects access pattern efficiency
        - AVX-SSE transition penalties if mixing 128 and 256-bit operations without vzeroupper
        concepts:
        - Floating-point SIMD
        - Horizontal operations
        - AVX extensions
        skills:
        - AVX vector math programming
        - Matrix multiplication optimization
        - Floating-point SIMD operations
        - Cache-friendly data layout design
        deliverables:
        - SIMD dot product processing 4 floats per iteration with accumulation
        - 4x4 matrix multiplication using SSE with proper memory layout
        - Runtime CPU feature detection for SSE/AVX code path selection
        - Performance benchmarks comparing scalar, SSE, and AVX implementations
        estimated_hours: 5-8
      - id: simd-library-m4
        name: Auto-vectorization Analysis
        description: Study compiler auto-vectorization and compare with hand-written SIMD.
        acceptance_criteria:
        - Write scalar versions of all functions that the compiler can auto-vectorize
        - Use compiler flags (-O3, -march=native, -ftree-vectorize) and inspect assembly output
        - Identify cases where hand-written SIMD outperforms auto-vectorization and explain why
        - Create a benchmark suite comparing scalar, auto-vectorized, and intrinsic versions
        pitfalls:
        - Compiler may not vectorize loops with complex control flow or aliasing
        - -ffast-math may be needed for floating-point vectorization but changes semantics
        - Benchmark variance from CPU frequency scaling can mask real differences
        concepts:
        - Auto-vectorization
        - Compiler optimization
        - Performance analysis
        skills:
        - Compiler optimization analysis
        - Assembly code reading
        - Benchmarking methodology
        - Compiler flag tuning
        deliverables:
        - Scalar implementations structured for auto-vectorization
        - Assembly analysis showing compiler vectorization decisions
        - Comprehensive benchmark suite with statistical analysis
        - Written analysis explaining when hand-written SIMD provides advantage
        estimated_hours: 5-8
    advanced:
    - id: disassembler
      name: x86 Disassembler
      description: Instruction decoder for x86/x64 binaries
      difficulty: advanced
      estimated_hours: 35-50
      essence: Variable-length instruction decoding requiring byte-level parsing of complex encoding schemes (prefixes, opcodes, ModRM/SIB bytes, immediates) and opcode table lookups to translate machine code into human-readable assembly mnemonics.
      why_important: Building a disassembler demystifies how CPUs execute code at the lowest level and provides foundational knowledge for reverse engineering, debuggers, binary analysis tools, and security research.
      learning_outcomes:
      - Implement variable-length instruction parsing with prefix, opcode, ModRM, and SIB byte decoding
      - Design opcode lookup tables mapping byte patterns to instruction mnemonics and operand types
      - Build binary file parsers to extract code sections from ELF and PE executable formats
      - Decode addressing modes using ModRM and SIB bytes to determine operand locations
      - Handle x86 instruction prefixes including legacy, REX, VEX, and EVEX encodings
      - Debug byte-level parsing logic by comparing output against professional disassemblers
      - Implement output formatters displaying addresses, raw bytes, mnemonics, and operands
      skills:
      - Binary Format Parsing
      - Instruction Encoding
      - Opcode Tables
      - Bit Manipulation
      - State Machine Design
      - Reverse Engineering
      - Low-level Debugging
      tags:
      - advanced
      - binary
      - binary-analysis
      - c
      - instruction-decoding
      - opcodes
      - python
      - rust
      - vm
      architecture_doc: architecture-docs/disassembler/index.md
      languages:
        recommended:
        - C
        - Rust
        - Python
        also_possible:
        - Go
        - C++
      resources:
      - name: Intel x86 Manual Vol. 2
        url: https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html
        type: reference
      - name: Medium - Building x86-64 Disassembler
        url: https://medium.com/@Koukyosyumei/learning-x86-64-machine-language-and-assembly-by-implementing-a-disassembler-dccc736ae85f
        type: tutorial
      - name: x86 Instruction Encoding
        url: http://wiki.osdev.org/X86-64_Instruction_Encoding
        type: reference
      prerequisites:
      - type: skill
        name: x86 assembly basics
      - type: skill
        name: Binary file handling
      - type: skill
        name: Bitwise operations
      milestones:
      - id: disassembler-m1
        name: Binary File Loading
        description: Load and parse ELF or PE executable headers to find code sections.
        acceptance_criteria:
        - Parse ELF header fields (or PE header for Windows binaries) correctly
        - Locate .text section containing executable code for disassembly
        - Extract code bytes and base address for instruction decoding
        - Handle both 32-bit and 64-bit binary formats without errors
        pitfalls:
        - Endianness varies (most x86 is little-endian)
        - Virtual address != file offset
        - Some binaries are stripped (no symbol table)
        concepts:
        - Executable formats
        - Binary parsing
        - Memory mapping
        skills:
        - Binary file I/O
        - Parsing executable headers (ELF/PE)
        - Working with file offsets and virtual addresses
        - Section header parsing
        deliverables:
        - ELF or PE header parsing for binary format detection
        - Section identification locating code and data segments
        - Entry point detection from executable header fields
        - Symbol table loading for function name resolution
        estimated_hours: 6-8
      - id: disassembler-m2
        name: Instruction Prefixes
        description: 'Decode x86 instruction prefixes: legacy, REX, VEX.'
        acceptance_criteria:
        - Detect legacy prefixes (66h, 67h, F0h, F2h, F3h, segment overrides) correctly
        - Decode REX prefix byte in 64-bit mode for register extension
        - Extract REX.W, REX.R, REX.X, and REX.B bit fields from prefix
        - Handle prefix ordering and multiple prefixes on single instruction
        pitfalls:
        - REX must be immediately before opcode
        - Some prefix combinations are invalid or have special meaning
        - VEX/EVEX prefixes in modern code need separate handling
        concepts:
        - Prefix encoding
        - Mode-dependent behavior
        - Bit field extraction
        skills:
        - Bit manipulation and masking
        - Decoding variable-length instruction prefixes
        - State machine implementation for prefix handling
        - Handling mode-dependent instruction encoding
        deliverables:
        - REX prefix handling for 64-bit register extensions
        - Operand size prefix detection for 16/32-bit overrides
        - Address size prefix detection for addressing mode overrides
        - Segment override prefix handling for non-default segments
        estimated_hours: 6-8
      - id: disassembler-m3
        name: Opcode Tables
        description: Build opcode lookup tables for instruction identification.
        acceptance_criteria:
        - One-byte opcode table maps all primary opcodes to instruction mnemonics
        - Two-byte opcode table handles 0F xx extended instruction encodings
        - Handle opcode extensions via ModRM.reg field for multiplexed opcodes
        - Map opcodes to mnemonics with correct operand size and type information
        pitfalls:
        - Opcode tables are large and error-prone to type manually
        - Some opcodes are invalid in certain modes
        - Three-byte opcodes exist (0F 38 xx, 0F 3A xx)
        concepts:
        - Lookup tables
        - Instruction encoding
        - Opcode extensions
        skills:
        - Data structure design for lookup tables
        - Working with multi-dimensional opcode maps
        - Handling opcode extensions (ModRM reg field)
        - Parsing escaped opcodes (0F prefix family)
        deliverables:
        - Primary opcode lookup table for single-byte opcodes
        - Extended opcode table for two-byte 0F-prefixed instructions
        - VEX and EVEX prefix handling for SIMD instructions
        - Instruction metadata including operand types and sizes
        estimated_hours: 8-10
      - id: disassembler-m4
        name: ModRM and SIB Decoding
        description: Decode ModRM and SIB bytes for operand addressing.
        acceptance_criteria:
        - Parse ModRM byte into mod, reg, and rm fields with correct bit masking
        - Handle SIB byte decoding when ModRM rm field equals 100 binary
        - Decode all 16 addressing modes including register and memory variants
        - Handle RIP-relative addressing mode used in 64-bit position-independent code
        pitfalls:
        - RIP-relative is 64-bit only, in 32-bit mod=00 rm=101 is disp32
        - SIB with index=100 means no index (unless REX.X set)
        - REX extends registers to r8-r15
        concepts:
        - Addressing modes
        - Register encoding
        - Memory operands
        skills:
        - Decoding complex addressing modes
        - Register encoding with REX extensions
        - Computing effective addresses from SIB bytes
        - Handling displacement values (8/32-bit)
        - RIP-relative addressing in 64-bit mode
        deliverables:
        - ModRM byte parsing extracting mod, reg, and rm fields
        - Register operand decoding from reg and rm field values
        - Memory operand decoding with displacement and base register
        - SIB byte handling for scaled-index-base addressing modes
        estimated_hours: 8-10
      - id: disassembler-m5
        name: Output Formatting
        description: Format disassembly output with addresses, bytes, and mnemonics.
        acceptance_criteria:
        - Show address, hex bytes, mnemonic, and operands for each instruction
        - Support both Intel and AT&T syntax output via command-line option
        - Label jump and call targets with known symbol names where available
        - Handle undefined or invalid opcodes gracefully without crashing disassembler
        pitfalls:
        - Hex bytes should be padded/aligned for readability
        - Relative addresses need base address to calculate target
        - Invalid opcodes should print as .byte or db
        concepts:
        - Assembly syntax
        - Address calculation
        - Output formatting
        skills:
        - Assembly syntax formatting (Intel/AT&T)
        - Hexadecimal output formatting and alignment
        - Address arithmetic for branch targets
        - Handling invalid/undefined opcodes gracefully
        deliverables:
        - Intel syntax output with destination-first operand ordering
        - AT&T syntax option with source-first operand ordering
        - Address and raw bytes display alongside decoded mnemonics
        - Symbol resolution replacing addresses with known function names
        estimated_hours: 6-8
    - id: terminal-multiplexer
      name: Terminal Multiplexer
      description: Simple tmux-like terminal manager
      difficulty: advanced
      estimated_hours: 30-40
      essence: Pseudo-terminal (PTY) lifecycle management, real-time parsing of VT100/ANSI escape sequences through state machines, and multiplexed process coordination with asynchronous I/O handling between master and slave terminal pairs.
      why_important: Building a terminal multiplexer exposes you to low-level Unix process control, PTY device programming, and stateful parsing‚Äîcritical skills for systems programming, understanding how SSH/screen/tmux work internally, and developing tools that programmatically control interactive programs.
      learning_outcomes:
      - Implement pseudo-terminal creation using posix_openpt, grantpt, and unlockpt system calls
      - Design a state machine parser for VT100/ANSI escape sequences with proper CSI handling
      - Build asynchronous I/O multiplexing using select/poll/epoll for multiple PTY pairs
      - Implement window splitting algorithms and layout management for rectangular panes
      - Handle terminal resize signals (SIGWINCH) and propagate dimensions via TIOCSWINSZ ioctl
      - Debug process groups, session leaders, and controlling terminal relationships
      - Manage bidirectional data flow between PTY master/slave file descriptors
      - Implement command mode parsing and key binding dispatch systems
      skills:
      - PTY/TTY Programming
      - ANSI Escape Sequence Parsing
      - Process Session Management
      - Asynchronous I/O Multiplexing
      - Signal Handling
      - Terminal Control ioctl
      - State Machine Design
      - Low-level Unix APIs
      tags:
      - advanced
      - c
      - go
      - panes
      - pty
      - rust
      - sessions
      - terminal
      - tmux
      architecture_doc: architecture-docs/terminal-multiplexer/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible:
        - Python
      resources:
      - name: PTY Programming
        url: https://www.man7.org/linux/man-pages/man7/pty.7.html
        type: reference
      - name: ANSI Escape Codes
        url: https://en.wikipedia.org/wiki/ANSI_escape_code
        type: reference
      - name: Building a Terminal Emulator
        url: https://www.uninformativ.de/blog/postings/2018-02-24/0/POSTING-en.html
        type: tutorial
      prerequisites:
      - type: skill
        name: Unix processes
      - type: skill
        name: Terminal basics
      - type: skill
        name: File descriptors
      milestones:
      - id: terminal-multiplexer-m1
        name: PTY Creation
        description: Create and manage pseudo-terminal pairs for running shell sessions.
        acceptance_criteria:
        - Open PTY master and slave pair with valid file descriptors
        - Fork child process with PTY as controlling terminal
        - Start shell in child process connected to the slave PTY device
        - Handle terminal size changes by forwarding TIOCSWINSZ to child process
        pitfalls:
        - Must call setsid() in child before opening slave
        - File descriptors need careful management across fork
        - SIGCHLD handling for child termination
        concepts:
        - Pseudo-terminals
        - Process groups
        - Controlling terminals
        skills:
        - System call programming
        - Process and session management
        - File descriptor manipulation
        - Signal handling
        deliverables:
        - Pseudoterminal allocation using posix_openpt or equivalent system call
        - Master and slave file descriptor setup with correct terminal attributes
        - PTY I/O handling for reading output and writing input between processes
        - Window size management forwarding TIOCSWINSZ ioctl on terminal resize
        estimated_hours: 6-8
      - id: terminal-multiplexer-m2
        name: Terminal Emulation
        description: Parse and render terminal escape sequences for display.
        acceptance_criteria:
        - Handle cursor movement (up, down, left, right)
        - Handle clear screen and clear line
        - Handle text attributes including bold, underline, and 256-color rendering
        - Maintain virtual screen buffer reflecting current terminal display state accurately
        pitfalls:
        - Many escape sequences have optional parameters
        - UTF-8 characters can span multiple bytes
        - Some sequences are terminal-specific
        concepts:
        - Terminal emulation
        - ANSI escape codes
        - Screen buffers
        skills:
        - State machine implementation
        - Text parsing and tokenization
        - Terminal I/O buffering
        - Character encoding handling
        deliverables:
        - ANSI escape sequence parser handling CSI, OSC, and control character codes
        - Cursor movement tracker maintaining row and column position state
        - Screen buffer storing character cells with attributes for each terminal line
        - Scrollback buffer retaining lines that scroll off the top of the visible area
        estimated_hours: 8-10
      - id: terminal-multiplexer-m3
        name: Window Management
        description: Support multiple panes in a split-screen layout.
        acceptance_criteria:
        - Vertical and horizontal splits create new panes with independent PTY sessions
        - Switch focus between panes using keyboard shortcuts for directional navigation
        - Resize panes by adjusting borders while maintaining minimum pane dimensions
        - Each pane runs an independent shell with its own scrollback buffer
        pitfalls:
        - Resize needs to update PTY size too
        - Border drawing reduces usable space
        - Focus tracking across splits is complex
        concepts:
        - Window management
        - Tree layouts
        - PTY multiplexing
        skills:
        - Layout algorithms and data structures
        - Terminal coordinate systems
        - Window resizing and redrawing
        - Input routing and focus management
        deliverables:
        - Pane splitting logic creating vertical and horizontal subdivisions of the window
        - Pane resizing handler adjusting pane dimensions while respecting minimum sizes
        - Window switching mechanism cycling between multiple named window sessions
        - Layout algorithm distributing available space among panes proportionally
        estimated_hours: 8-10
      - id: terminal-multiplexer-m4
        name: Key Bindings and UI
        description: Add command mode and key bindings for pane management.
        acceptance_criteria:
        - Prefix key (like Ctrl-b) enters command mode
        - Bindings for split, close, navigate, resize
        - Status bar displays current pane index, window name, and system time
        - Render all panes to the terminal with correct border characters between them
        pitfalls:
        - Raw mode disables Ctrl-C - need explicit handling
        - Must restore terminal settings on exit
        - Rendering must be efficient to avoid flicker
        concepts:
        - Raw terminal mode
        - Event loops
        - Key binding systems
        skills:
        - Terminal mode configuration
        - Event-driven programming
        - Command parsing
        - Efficient screen rendering
        deliverables:
        - Prefix key handler intercepting a configurable leader key for command mode
        - Custom key binding registry mapping key sequences to multiplexer commands
        - Status bar renderer displaying pane info, session name, and clock on screen
        - Copy mode enabling keyboard-driven text selection and clipboard copy from scrollback
        estimated_hours: 8-10
    - id: protocol-buffer
      name: Protocol Buffer
      description: Binary serialization format implementation
      difficulty: advanced
      estimated_hours: 25-35
      essence: Variable-length integer encoding (varints) combined with a tag-based wire format where each field is encoded as (field_number << 3 | wire_type) followed by payload data, enabling compact binary serialization with schema evolution through backward and forward compatibility.
      why_important: Building this teaches you the engineering tradeoffs in binary protocol design‚Äîhow to balance compactness, parsing speed, and schema evolution‚Äîskills directly applicable to distributed systems, database storage engines, and network protocol implementation.
      learning_outcomes:
      - Implement variable-length integer encoding with continuation bits for compact numeric representation
      - Design and implement a tag-length-value wire format using field numbers and wire type indicators
      - Build a schema parser that generates encoding/decoding logic from type definitions
      - Handle backward and forward compatibility through unknown field skipping and optional field semantics
      - Optimize byte-level operations for efficient serialization and deserialization
      - Debug binary protocols using hex dumps and understanding bit-level encoding details
      - Implement proper handling of all six wire types (VARINT, I64, I32, LEN, SGROUP, EGROUP)
      skills:
      - Binary Serialization
      - Varint Encoding
      - Wire Protocol Design
      - Schema-Driven Architecture
      - Bit Manipulation
      - Protocol Evolution
      - Parsing Techniques
      - Compact Data Structures
      tags:
      - advanced
      - binary
      - go
      - protocols
      - python
      - rust
      - schema
      - serialization
      - wire-format
      architecture_doc: architecture-docs/protocol-buffer/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible:
        - C
        - Java
      resources:
      - name: Protocol Buffers Encoding
        url: https://protobuf.dev/programming-guides/encoding/
        type: reference
      - name: Varint Encoding
        url: https://developers.google.com/protocol-buffers/docs/encoding#varints
        type: tutorial
      prerequisites:
      - type: skill
        name: Binary encoding
      - type: skill
        name: Schema concepts
      - type: skill
        name: Data structures
      milestones:
      - id: protocol-buffer-m1
        name: Varint Encoding
        description: Implement variable-length integer encoding used in Protocol Buffers.
        acceptance_criteria:
        - Encode unsigned integers as variable-length varint bytes
        - Encode signed integers using ZigZag mapping to unsigned
        - Decode varints from byte stream returning value and bytes consumed
        - Handle all values up to 64-bit integer range correctly
        pitfalls:
        - Signed integers need ZigZag, not two's complement
        - Varint can be up to 10 bytes for 64-bit values
        - Overflow possible if not handling properly
        concepts:
        - Variable-length encoding
        - Integer representation
        - ZigZag encoding
        skills:
        - Binary data manipulation
        - Bitwise operations
        - Variable-length integer encoding
        - Handling multi-byte sequences
        deliverables:
        - Unsigned integer varint encoding using 7-bit groups
        - Varint decoding reading bytes until continuation bit clears
        - Signed integer ZigZag encoding for efficient negative values
        - Full 64-bit integer support for both signed and unsigned
        estimated_hours: 4-5
      - id: protocol-buffer-m2
        name: Wire Types
        description: Implement the wire type system for field encoding.
        acceptance_criteria:
        - Support Type 0 varint for int32, int64, uint, bool, and enum
        - Support Type 1 fixed 64-bit for double and sfixed64 values
        - Support Type 2 length-delimited for string, bytes, and messages
        - Support Type 5 fixed 32-bit for float and sfixed32 values
        pitfalls:
        - Wire types 3 and 4 are deprecated (start/end group)
        - Fixed types are little-endian
        - Unknown fields should be preserved, not discarded
        concepts:
        - Wire types
        - Type-length-value encoding
        - Binary protocols
        skills:
        - Binary protocol design
        - Type system implementation
        - Little-endian byte ordering
        - Unknown field preservation
        deliverables:
        - Wire type encoding and decoding for all supported types
        - Length-delimited field encoding for strings and nested messages
        - Fixed 32-bit and 64-bit field encoding in little-endian format
        - Field number extraction from wire format key bytes
        estimated_hours: 5-6
      - id: protocol-buffer-m3
        name: Schema Parser
        description: Parse a simple schema definition to drive encoding/decoding.
        acceptance_criteria:
        - Parse message definitions with name and body braces
        - Parse field declarations with type, name, and field number
        - Support repeated field modifier for array-type fields
        - Support nested message definitions within parent messages
        pitfalls:
        - Field numbers must be unique within a message
        - Field numbers 19000-19999 are reserved
        - Nested messages need recursive parsing
        concepts:
        - Schema languages
        - Code generation
        - Parsing
        skills:
        - Parser implementation
        - Abstract syntax tree construction
        - Recursive descent parsing
        - Schema validation
        deliverables:
        - Lexer tokenizing .proto file into meaningful tokens
        - Message definition parser extracting name and field list
        - Field definition parser extracting type, name, and number
        - Import statement handling for multi-file schemas
        estimated_hours: 5-6
      - id: protocol-buffer-m4
        name: Message Serialization
        description: Serialize and deserialize messages according to schema.
        acceptance_criteria:
        - Serialize Python dict to binary bytes following schema definition
        - Deserialize binary bytes back to Python dict using schema
        - Handle missing optional fields by skipping during serialization
        - Handle repeated fields serializing each array element separately
        pitfalls:
        - Field order in wire format is arbitrary
        - Unknown fields should be skipped, not error
        - Empty repeated field = no entries (not encoded)
        concepts:
        - Serialization
        - Schema-driven encoding
        - Forward compatibility
        skills:
        - Schema-driven serialization
        - Binary format encoding/decoding
        - Backward compatibility handling
        - Repeated field serialization
        deliverables:
        - Field encoding writing typed values with field key prefix
        - Complete message encoding serializing all fields to bytes
        - Nested message handling encoding sub-messages as length-delimited
        - Repeated field encoding writing each element with same field number
        estimated_hours: 6-8
    - id: payment-gateway
      name: Payment Gateway
      description: Payment processing with PCI compliance
      difficulty: expert
      estimated_hours: '45'
      essence: Distributed transaction state management across unreliable networks with cryptographic verification, idempotent request handling to prevent duplicate charges, and asynchronous webhook reconciliation ensuring eventual consistency between payment provider events and internal ledger states.
      why_important: Building this teaches critical production payment system architecture patterns that power e-commerce platforms, including preventing duplicate charges that could cost businesses millions and implementing security standards required by financial regulations.
      learning_outcomes:
      - Implement idempotency key validation with distributed locking to prevent duplicate payment processing
      - Design payment intent state machines handling pending, requires_action, succeeded, and failed transitions
      - Build 3D Secure authentication flows with challenge redirects and verification callbacks
      - Implement HMAC-SHA256 webhook signature verification with timing-safe comparison
      - Design asynchronous webhook processing queues with retry logic and exponential backoff
      - Implement partial refund calculations with balance reconciliation and dispute tracking
      - Build PCI DSS-compliant architectures that minimize cardholder data exposure
      - Debug payment flow race conditions and handle network failures with proper rollback mechanisms
      skills:
      - Idempotent API Design
      - Payment State Machines
      - HMAC Signature Verification
      - Webhook Processing
      - PCI DSS Compliance
      - Transaction Reconciliation
      - 3D Secure (3DS2)
      - Distributed Locking
      tags:
      - api-integration
      - expert
      - fintech
      - idempotency
      - payments
      - security
      - service
      - stripe
      - transactions
      - webhooks
      architecture_doc: architecture-docs/payment-gateway/index.md
      languages:
        recommended:
        - Python
        - Go
        - Java
        also_possible: []
      resources:
      - name: Stripe Payment Intents API
        url: https://docs.stripe.com/payments/payment-intents
        type: documentation
      - name: Stripe Idempotent Requests Guide
        url: https://stripe.com/blog/idempotency
        type: article
      - name: PCI Security Standards Council
        url: https://www.pcisecuritystandards.org/standards/
        type: documentation
      - name: Stripe 3D Secure Documentation
        url: https://docs.stripe.com/payments/3d-secure
        type: documentation
      - name: Stripe Webhook Signature Verification
        url: https://docs.stripe.com/webhooks/signature
        type: tutorial
      prerequisites:
      - type: project
        id: http-server-basic
        name: HTTP Server (Basic)
      - type: skill
        name: Database transactions
      milestones:
      - id: payment-gateway-m1
        name: Payment Intent & Idempotency
        description: Implement payment intents with idempotency keys to prevent duplicate charges
        acceptance_criteria:
        - Create payment intents with client-provided idempotency key
        - Track payment state transitions through defined lifecycle stages
        - Handle duplicate requests safely returning same result for same key
        - Support attaching arbitrary metadata to payment intent records
        pitfalls:
        - Idempotency key reused with different params should error, not return old response
        - Client secret must be kept secret - only share with authenticated user
        - Amount in cents prevents floating point errors ($10.00 = 1000 cents)
        - Status transitions must be validated - can't go backwards
        concepts:
        - Idempotency key generation and storage patterns
        - Payment intent state machine design
        - Request deduplication using unique constraints
        - Atomic database transactions for payment operations
        skills:
        - Idempotency patterns
        - State machines
        - Atomic operations
        deliverables:
        - Payment intent creation API returning unique intent identifier
        - Idempotency key handling preventing duplicate charge processing
        - Intent state machine tracking lifecycle from created to completed
        - Intent expiration cancelling stale uncompleted payment intents
        estimated_hours: '11'
      - id: payment-gateway-m2
        name: Payment Processing & 3DS
        description: Implement payment confirmation with 3D Secure authentication flow
        acceptance_criteria:
        - Process card payments by submitting charges to payment provider
        - Implement 3D Secure authentication flow for supported card issuers
        - Handle authentication redirect returning user to merchant after 3DS
        - Support multiple payment methods including cards and bank transfers
        pitfalls:
        - 3DS redirects must include return_url for user to come back
        - Payment can succeed but webhook fail - always reconcile
        - 'Auth-capture: authorization expires (usually 7 days) - capture before expiry'
        - Currency mismatch between intent and capture causes errors
        concepts:
        - 3D Secure authentication challenge-response protocol
        - Synchronous vs asynchronous payment confirmation flows
        - Payment method tokenization and secure storage
        - Authorization hold mechanics and settlement timing
        skills:
        - 3DS flow
        - Payment processing
        - Error handling
        deliverables:
        - Payment method tokenization securing card details as opaque token
        - Charge creation submitting payment for processing by provider
        - 3D Secure authentication flow handling issuer verification step
        - Payment confirmation completing charge after successful authentication
        estimated_hours: '11'
      - id: payment-gateway-m3
        name: Refunds & Disputes
        description: Implement refund processing, partial refunds, and dispute handling
        acceptance_criteria:
        - Process both full and partial refunds against original payment
        - Track refund status through pending, completed, and failed stages
        - Handle chargeback notifications received from payment network
        - Support dispute evidence submission to contest chargebacks
        pitfalls:
        - Refund can take 5-10 business days to appear on statement
        - Dispute evidence deadline is strict - automate alerts
        - Dispute fee charged even if you win - prevention is key
        - 'Partial refunds: track total refunded vs original amount carefully'
        concepts:
        - Refund state transitions and reversibility windows
        - Chargeback lifecycle and evidence submission workflows
        - Ledger-based accounting for partial refund tracking
        - Dispute reason codes and automated response strategies
        skills:
        - Refund workflows
        - Dispute handling
        - Financial reconciliation
        deliverables:
        - Full and partial refund processing against completed charges
        - Partial refund support returning portion of original charge amount
        - Dispute handling tracking chargeback status and deadlines
        - Chargeback notification integration receiving alerts from payment network
        estimated_hours: '11'
      - id: payment-gateway-m4
        name: Webhook Reconciliation
        description: Implement reliable webhook processing with signature verification and reconciliation
        acceptance_criteria:
        - Receive and process payment webhooks from external provider
        - Update local payment status records based on webhook event data
        - Verify webhook payload signature to prevent spoofed notifications
        - Reconcile local payment records against expected provider state
        pitfalls:
        - Webhook signature verification is critical - never skip
        - Events can arrive out of order - check timestamps and states
        - Processor may retry webhooks - always be idempotent
        - Run periodic reconciliation - webhooks can fail silently
        concepts:
        - HMAC signature verification for webhook authenticity
        - Event ordering and causality preservation patterns
        - At-least-once delivery semantics and deduplication
        - Background job processing with retry exponential backoff
        skills:
        - Webhook security
        - Event processing
        - Reconciliation
        deliverables:
        - Webhook endpoint receiving payment event notifications from provider
        - Signature verification authenticating webhook payloads cryptographically
        - Event processing handler updating local records from webhook data
        - State reconciliation comparing local payment state against provider state
        estimated_hours: '11'
    - id: subscription-billing
      name: Subscription Billing
      description: Recurring payments, invoicing, dunning
      difficulty: expert
      estimated_hours: '50'
      essence: Recurring payment lifecycle management with prorated billing calculations, idempotent transaction processing, high-throughput usage metering, and webhook-driven state synchronization across distributed payment systems.
      why_important: Building this teaches you production payment processing patterns used by major SaaS platforms, including handling money reliably at scale, implementing complex business logic for pricing changes, and designing fault-tolerant distributed systems where financial accuracy is non-negotiable.
      learning_outcomes:
      - Implement idempotent webhook handlers with deduplication and retry logic for payment events
      - Design prorated billing calculations for mid-cycle plan upgrades and downgrades with credit handling
      - Build high-throughput usage metering with event batching, aggregation, and time-series storage
      - Implement subscription state machines handling trials, renewals, cancellations, and dunning workflows
      - Design tokenized payment storage and PCI-compliant card data handling patterns
      - Build invoice generation systems with line-item calculations, tax handling, and PDF rendering
      - Implement distributed transaction patterns ensuring atomicity across billing, inventory, and accounting systems
      - Design flexible pricing engines supporting tiered, per-seat, usage-based, and hybrid billing models
      skills:
      - Payment Gateway Integration
      - Idempotency Patterns
      - Webhook Processing
      - Prorated Billing Calculations
      - Usage Metering & Aggregation
      - State Machine Design
      - Distributed Transactions
      - Time-Series Data Storage
      tags:
      - billing
      - expert
      - fintech
      - invoices
      - metering
      - payments
      - recurring
      - recurring-payments
      - saas
      - subscriptions
      architecture_doc: architecture-docs/subscription-billing/index.md
      languages:
        recommended:
        - Python
        - Go
        - Java
        also_possible: []
      resources:
      - name: Stripe Billing Documentation
        url: https://docs.stripe.com/billing
        type: documentation
      - name: Designing Robust APIs with Idempotency
        url: https://stripe.com/blog/idempotency
        type: article
      - name: Building Usage-Based Billing Systems
        url: https://www.getlago.com/blog/usage-based-billing-system
        type: article
      - name: Payment Webhook Reliability Guide
        url: https://medium.com/@sohail_saifii/handling-payment-webhooks-reliably-idempotency-retries-validation-69b762720bf5
        type: tutorial
      - name: PCI Compliance Guide
        url: https://stripe.com/guides/pci-compliance
        type: documentation
      prerequisites:
      - type: project
        id: payment-gateway
      milestones:
      - id: subscription-billing-m1
        name: Plans & Pricing
        description: Implement flexible pricing plans with tiers, features, and currencies
        acceptance_criteria:
        - Define subscription plans with tiers, billing intervals, and pricing amounts
        - Support multiple pricing models (flat, tiered)
        - Handle plan versioning so existing subscribers keep their original plan terms
        - Support trial periods with automatic conversion to paid at expiry
        pitfalls:
        - 'Tiered vs Volume pricing: tiered charges each tier separately; volume uses single rate'
        - 'Currency precision: always use smallest unit (cents) to avoid float errors'
        - 'Plan changes: archive old plans rather than deleting (existing subscribers)'
        - 'Feature access: check plan features, not subscription status alone'
        concepts:
        - Recurring billing models (flat-rate, per-seat, tiered, volume-based)
        - Feature entitlements and access control lists
        - Multi-currency conversion and exchange rate handling
        - Price point psychology and monetization strategy
        - Plan versioning and grandfathering existing customers
        skills:
        - Pricing models
        - Feature flags
        - Multi-currency
        deliverables:
        - Plan definition schema supporting name, interval, currency, and feature list
        - Pricing tier configuration supporting flat-rate, per-unit, and tiered models
        - Feature entitlement matrix mapping plan tiers to accessible feature flags
        - Plan management API for creating, updating, and deprecating subscription plans
        estimated_hours: '12.5'
      - id: subscription-billing-m2
        name: Subscription Lifecycle
        description: Implement subscription creation, activation, renewal, and cancellation
        acceptance_criteria:
        - Create subscriptions for customers with selected plan and payment method
        - Handle upgrades and downgrades with appropriate effective date logic
        - Process cancellations recording reason and scheduling termination date
        - Handle subscription pausing and resuming with prorated billing adjustment
        pitfalls:
        - 'Billing anchor: ensures consistent billing date each month'
        - 'Past due vs unpaid: past_due = still retrying; unpaid = gave up'
        - 'Cancel at period end: most user-friendly, they paid for the period'
        - 'Month overflow: Jan 31 + 1 month = Feb 28, not Mar 3'
        concepts:
        - Subscription state machine transitions and valid state flows
        - Billing cycle calculation and anniversary date handling
        - Grace period policies and dunning management
        - Renewal retry logic with exponential backoff
        - Cancellation timing (immediate vs end-of-period vs scheduled)
        skills:
        - State machines
        - Billing cycles
        - Grace periods
        deliverables:
        - Subscription creation service provisioning new subscriptions for customers
        - Renewal processing engine generating invoices at each billing cycle boundary
        - Cancellation handler supporting immediate and end-of-period termination options
        - Reactivation service restoring cancelled subscriptions within a grace period
        estimated_hours: '12.5'
      - id: subscription-billing-m3
        name: Proration & Plan Changes
        description: Implement upgrade/downgrade with prorated charges and credits
        acceptance_criteria:
        - Calculate prorated charges on upgrade based on remaining days in cycle
        - Calculate credits on downgrade for unused portion of higher-tier plan
        - Handle mid-cycle plan changes with correct effective date and amount
        - Support immediate versus end-of-cycle plan change scheduling options
        pitfalls:
        - 'Proration direction: upgrade charges extra; downgrade credits'
        - Round proration carefully - small errors accumulate over many customers
        - Credits must be applied automatically on next invoice
        - Quantity changes are proration too - not just plan changes
        concepts:
        - Proration formulas for time-based and quantity-based changes
        - Credit balance management and application order
        - Invoice line items for upgrades and downgrades
        - Immediate vs scheduled plan change execution
        - Refund policies for downgrades and cancellations
        skills:
        - Proration calculation
        - Credits
        - Mid-cycle changes
        deliverables:
        - Proration calculation engine computing partial charges for mid-cycle changes
        - Upgrade and downgrade handler adjusting entitlements and billing amounts
        - Credit application service applying unused-time credits to the customer balance
        - Mid-cycle change processor handling plan switches at any point in the billing period
        estimated_hours: '12.5'
      - id: subscription-billing-m4
        name: Usage-Based Billing
        description: Implement metered billing with usage tracking, aggregation, and reporting
        acceptance_criteria:
        - Track usage events per subscription with idempotent event ingestion
        - Aggregate usage for billing period with correct start and end boundaries
        - Calculate charges based on usage tiers
        - Handle usage overage by applying the correct per-unit overage rate
        pitfalls:
        - 'Idempotency: same event reported twice shouldn''t be double-counted'
        - 'Clock skew: use server timestamps, not client timestamps'
        - 'Aggregation type matters: sum vs max give very different results'
        - 'Usage limits: alert before hitting limit, not after'
        concepts:
        - Event-based metering with deduplication strategies
        - Time-series aggregation (sum, count, max, last-value)
        - Usage reporting windows and billing period alignment
        - Rate limiting based on plan quotas
        - Idempotency keys for duplicate event prevention
        skills:
        - Usage metering
        - Aggregation
        - Rate limiting
        deliverables:
        - Usage tracking service recording metered events with timestamps and quantities
        - Metering API accepting usage reports and deduplicating repeated submissions
        - Usage aggregation engine totaling consumption per subscription per billing period
        - Overage charge calculator applying tiered rates when usage exceeds plan allowance
        estimated_hours: '12.5'
    - id: webhook-delivery
      name: Webhook Delivery System
      description: Reliable webhook dispatch with retries
      difficulty: advanced
      estimated_hours: '35'
      essence: Asynchronous message delivery orchestration across unreliable networks with guaranteed at-least-once semantics, implementing idempotent processing through delivery ID tracking, cryptographic payload verification via HMAC-SHA256, and fault tolerance through exponential backoff retry strategies with circuit breaker state machines to prevent cascading failures when downstream endpoints become unresponsive.
      why_important: 'Building this teaches critical production engineering skills for distributed systems: implementing fault-tolerant message delivery, protecting downstream services from cascading failures, and securing inter-service communication‚Äîcapabilities essential for backend infrastructure at scale.'
      learning_outcomes:
      - Implement HMAC-SHA256 signature verification for webhook authentication and replay attack prevention
      - Design retry logic with exponential backoff, jitter, and dead letter queues for guaranteed delivery
      - Build circuit breaker pattern with closed, open, and half-open states to protect failing endpoints
      - Implement rate limiting and request throttling to prevent overwhelming downstream services
      - Design idempotent event processing using delivery IDs to handle at-least-once delivery guarantees
      - Build persistent event log with replay capability for debugging and recovery scenarios
      - Implement async queue-based processing with worker pools for scalable webhook delivery
      - Design monitoring and observability for delivery success rates, latency metrics, and failure patterns
      skills:
      - Message Queue Architecture
      - Exponential Backoff Strategies
      - Circuit Breaker Pattern
      - HMAC Cryptographic Signing
      - Idempotent API Design
      - Fault Tolerance Engineering
      - Distributed Systems Reliability
      - Rate Limiting & Throttling
      tags:
      - advanced
      - async
      - cryptography
      - delivery
      - events
      - fault-tolerance
      - integration
      - reliability
      - retry
      - signing
      - webhooks
      architecture_doc: architecture-docs/webhook-delivery/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible: []
      resources:
      - name: RabbitMQ Tutorials
        url: https://www.rabbitmq.com/tutorials
        type: documentation
      - name: Circuit Breaker Pattern
        url: https://martinfowler.com/bliki/CircuitBreaker.html
        type: article
      - name: HMAC Webhook Verification
        url: https://hookdeck.com/webhooks/guides/how-to-implement-sha256-webhook-signature-verification
        type: tutorial
      - name: AWS Retry with Backoff
        url: https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/retry-backoff.html
        type: documentation
      - name: Celery with Redis
        url: https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/redis.html
        type: documentation
      prerequisites:
      - type: skill
        name: Message queue/pub-sub basics
      - type: project
        id: http-server-basic
        name: HTTP Server (Basic)
      milestones:
      - id: webhook-delivery-m1
        name: Webhook Registration & Security
        description: Implement webhook endpoint registration with signature verification
        acceptance_criteria:
        - Store webhook endpoints with associated secrets and event type subscriptions
        - Generate HMAC signatures for payloads and include them in delivery headers
        - Validate endpoint ownership by sending a challenge request and verifying the response
        - Handle secret rotation allowing new secrets without invalidating in-flight deliveries
        pitfalls:
        - HTTPS only - never deliver webhooks over HTTP
        - Timestamp tolerance prevents replay but allows clock skew
        - 'Signing secret rotation: support multiple active secrets temporarily'
        - 'URL validation: block private IPs to prevent SSRF'
        concepts:
        - HMAC-SHA256 signature generation and verification
        - URL parsing and validation against SSRF attacks
        - Secret rotation with overlapping validity periods
        - Timestamp-based replay attack prevention
        skills:
        - HMAC signatures
        - Endpoint validation
        - Secret management
        deliverables:
        - Endpoint registration API accepting callback URL and event type subscriptions
        - Secret key generation producing a cryptographically random signing key per webhook
        - Signature generation computing HMAC-SHA256 over the payload with the webhook secret
        - URL validation verifying the callback endpoint is reachable and returns a confirmation
        estimated_hours: '9'
      - id: webhook-delivery-m2
        name: Delivery Queue & Retry Logic
        description: Implement reliable delivery with exponential backoff and dead letter queue
        acceptance_criteria:
        - Queue webhook events for delivery with ordering guarantees per endpoint
        - Implement exponential backoff retries with configurable base delay and max attempts
        - Handle delivery timeouts by marking attempts as failed after a configured duration
        - Track delivery attempts recording status code, response time, and error for each try
        pitfalls:
        - Jitter prevents thundering herd when many webhooks fail simultaneously
        - 4xx errors (except 429) shouldn't retry - request is malformed
        - Dead letter queue needs alerting and manual review process
        - 'Circuit breaker: disable endpoint after consecutive failures'
        concepts:
        - Exponential backoff with jitter for retry scheduling
        - Message queue persistence and at-least-once delivery
        - Dead letter queue pattern for undeliverable messages
        - HTTP status code classification for retry decisions
        skills:
        - Retry strategies
        - Exponential backoff
        - Dead letter queues
        deliverables:
        - Delivery queue persisting pending webhook events for reliable ordered processing
        - HTTP delivery client sending POST requests with signed payload to registered endpoints
        - Retry logic with exponential backoff increasing delay between successive failed attempts
        - Dead letter queue capturing events that exhaust all retry attempts for manual inspection
        estimated_hours: '9'
      - id: webhook-delivery-m3
        name: Circuit Breaker & Rate Limiting
        description: Implement circuit breaker to protect failing endpoints and rate limiting
        acceptance_criteria:
        - Implement circuit breaker per endpoint with configurable failure threshold
        - Disable endpoints after repeated failures and alert the webhook owner
        - Rate limit delivery attempts to avoid overwhelming downstream endpoints
        - Support endpoint recovery transitioning from open to half-open to closed circuit state
        pitfalls:
        - 'Half-open: only allow limited test requests, not full traffic'
        - Circuit breaker timeout should increase on repeated failures
        - Rate limiting should respect Retry-After headers from receiver
        - Alert when circuit opens - endpoint owner needs to know
        concepts:
        - 'Circuit breaker state transitions: closed, open, half-open'
        - Token bucket or sliding window rate limiting
        - HTTP 429 status and Retry-After header handling
        - Endpoint health probe strategies during half-open state
        skills:
        - Circuit breaker pattern
        - Rate limiting
        - Health checks
        deliverables:
        - Per-endpoint circuit breaker halting delivery after consecutive failure threshold is reached
        - Rate limiting throttle capping the delivery rate per endpoint per time window
        - Backpressure handling slowing queue consumption when downstream endpoints are overloaded
        - Health tracking monitor recording success rates and latency per registered endpoint
        estimated_hours: '9'
      - id: webhook-delivery-m4
        name: Event Log & Replay
        description: Implement event logging for debugging and replay capability
        acceptance_criteria:
        - Store event delivery history including payload, attempts, and response codes
        - Support manual event replay by re-queuing a specific event for delivery
        - Provide delivery logs per webhook showing chronological attempt history
        - Handle replay with deduplication so endpoints can detect and ignore duplicate deliveries
        pitfalls:
        - Event log can grow huge - implement retention and archival
        - Replay should use new delivery IDs to track separately
        - Bulk replay can overwhelm endpoints - respect rate limits
        - Payload might be stale on replay - warn users
        concepts:
        - Event sourcing for complete delivery audit trail
        - Time-series database optimization for log queries
        - Idempotency keys for safe event replay
        - Log retention policies with cold storage archival
        skills:
        - Event sourcing
        - Log retention
        - Event replay
        deliverables:
        - Delivery log persisting every event payload, attempt, and response for auditing
        - Event replay API allowing re-delivery of specific events to their registered endpoint
        - Delivery status tracking dashboard showing pending, delivered, and failed event counts
        - Debugging tools displaying request and response headers and bodies for failed deliveries
        estimated_hours: '9'
    - id: cdn-implementation
      name: CDN Implementation
      description: Content delivery with edge caching
      difficulty: expert
      estimated_hours: '45'
      essence: Geographically distributed HTTP reverse proxy implementing cache coherence across edge nodes, request deduplication through collapsed forwarding, and multi-tier cache hierarchy with TTL-based expiration and invalidation propagation.
      why_important: Building this teaches you the core techniques behind how internet-scale services (Netflix, Cloudflare) deliver content globally with minimal latency, and develops critical skills in distributed systems, caching strategies, and performance optimization that are essential for backend and infrastructure engineering roles.
      learning_outcomes:
      - Implement edge caching with TTL expiration and Cache-Control header processing based on HTTP caching standards
      - Design and build cache invalidation mechanisms including purge (URL-specific), ban (pattern-based), and tag-based invalidation
      - Implement origin shielding with request collapsing to consolidate multiple edge requests into single origin fetches
      - Build distributed cache key generation and normalization across query parameters and headers
      - Develop cache coherence protocols to maintain consistency across multiple edge nodes
      - Implement conditional requests using ETags and Last-Modified headers for efficient revalidation
      - Design hierarchical caching layers with cache promotion and demotion strategies
      skills:
      - HTTP Caching Protocols
      - Distributed Systems
      - Cache Invalidation Strategies
      - Request Collapsing
      - Origin Shielding
      - TTL Management
      - Reverse Proxy Architecture
      tags:
      - caching
      - cdn
      - distributed
      - edge
      - expert
      - networking
      - origin
      - performance
      architecture_doc: architecture-docs/cdn-implementation/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible: []
      resources:
      - name: RFC 9111 HTTP Caching
        url: https://httpwg.org/specs/rfc9111.html
        type: documentation
      - name: Varnish Cache Documentation
        url: https://varnish-cache.readthedocs.io/
        type: documentation
      - name: NGINX Content Caching Guide
        url: https://docs.nginx.com/nginx/admin-guide/content-cache/content-caching/
        type: documentation
      - name: Cloudflare CDN Architecture
        url: https://developers.cloudflare.com/reference-architecture/architectures/cdn/
        type: documentation
      - name: CDN Origin Shield Guide
        url: https://www.cdnplanet.com/guides/origin-shield
        type: article
      prerequisites:
      - type: project
        id: http-server-basic
        name: HTTP Server (Basic)
      - type: project
        id: distributed-cache
        name: Distributed Cache
      milestones:
      - id: cdn-implementation-m1
        name: Edge Cache Implementation
        description: Implement edge caching with TTL and cache control headers
        acceptance_criteria:
        - Cached responses are served based on URL and relevant Vary header combinations
        - LRU or LFU eviction removes least valuable entries when cache capacity is exceeded
        - Cache respects Accept-Encoding variants serving correct compressed or plain responses
        - Conditional requests with ETag return 304 Not Modified for unchanged resources
        pitfalls:
        - 'Vary: * means never cache - handle this case'
        - 's-maxage vs max-age: CDN should use s-maxage'
        - 'ETag weak vs strong: weak allows semantic equivalence'
        - Cache key must include all Vary dimensions
        concepts:
        - HTTP cache validation with ETag and Last-Modified
        - Cache key construction from URL, headers, and Vary dimensions
        - 'Time-to-live (TTL) hierarchy: s-maxage, max-age, Expires'
        - 'Conditional requests: If-None-Match and If-Modified-Since'
        - Stale-while-revalidate for serving stale content during revalidation
        skills:
        - HTTP caching
        - Cache-Control
        - Vary headers
        deliverables:
        - Cache storage layer holding response bodies keyed by URL and vary headers
        - Cache key generation combining URL and Vary header values for unique identification
        - TTL management expiring cached entries based on Cache-Control header directives
        - Cache hit and miss handling serving cached responses or forwarding to origin
        estimated_hours: '15'
      - id: cdn-implementation-m2
        name: Cache Invalidation
        description: Implement purge, ban, and tag-based invalidation
        acceptance_criteria:
        - Purge API removes specific URL from cache and next request fetches fresh from origin
        - Surrogate key purge invalidates all resources tagged with specified key in single operation
        - Soft purge continues serving stale content while asynchronously revalidating from origin
        - Invalidation event propagates to all edge nodes within configurable time window
        pitfalls:
        - Surrogate keys enable efficient invalidation of related content
        - 'Propagation delay: clients may get stale content briefly'
        - Soft purge (stale-while-revalidate) better than hard purge
        - Bans can grow unbounded - implement TTL and cleanup
        concepts:
        - Surrogate-Key header for grouping related cache entries
        - 'Purge vs ban: exact match vs pattern-based invalidation'
        - Soft purge with stale-if-error for graceful degradation
        - Ban list management and memory-efficient storage
        - Cache tag propagation across distributed edge nodes
        skills:
        - Invalidation strategies
        - Purge propagation
        - Surrogate keys
        deliverables:
        - Purge by URL removing specific cached resource from all edge nodes
        - Purge by tag using surrogate keys to invalidate groups of related resources
        - Soft purge marking content as stale while serving it during background revalidation
        - Invalidation propagation distributing purge commands to all edge cache nodes
        estimated_hours: '15'
      - id: cdn-implementation-m3
        name: Origin Shield & Request Collapsing
        description: Implement origin shielding and request collapsing to reduce origin load
        acceptance_criteria:
        - Origin shield reduces origin requests by serving as intermediate cache for edge nodes
        - Concurrent requests for same resource result in single origin fetch with shared response
        - Cache miss storms do not overwhelm origin due to request collapsing and queuing
        - Stale-while-revalidate serves cached content while background request refreshes cache
        pitfalls:
        - Request collapsing timeout must be shorter than client timeout
        - Shield adds latency but dramatically reduces origin load
        - 'Negative caching: cache 404s briefly to prevent origin storms'
        - 'Health checks: bypass shield when origin is unhealthy'
        concepts:
        - Request coalescing to collapse concurrent identical requests
        - Origin shield as a secondary cache tier
        - Thundering herd prevention through cache warming
        - Negative response caching with short TTLs
        - Health-based routing to bypass failed shield nodes
        skills:
        - Origin protection
        - Request deduplication
        - Thundering herd
        deliverables:
        - Origin shield layer caching responses between edge nodes and origin server
        - Request collapsing for concurrent requests deduplicating identical origin fetches
        - Cache fill optimization reducing origin load during cache miss storms
        - Origin protection limiting concurrent requests to prevent origin overload
        estimated_hours: '15'
      - id: cdn-implementation-m4
        name: Edge Node Distribution & Routing
        description: Implement multi-edge-node architecture with geo-aware routing to direct clients to the nearest edge.
        acceptance_criteria:
        - Multiple edge nodes register with a control plane and report health status
        - Client requests are routed to the nearest healthy edge based on IP geolocation
        - Consistent hashing distributes cached content across edge nodes
        - Edge node failover redirects traffic to the next-nearest edge within 5 seconds
        pitfalls:
        - GeoIP database inaccuracy
        - Cache inconsistency across edges
        - Thundering herd on edge failover
        concepts:
        - geo-routing
        - anycast
        - consistent hashing
        - health checking
        skills:
        - Geo-routing
        - Consistent hashing
        - Failover
        deliverables:
        - Multi-edge CDN with geo-routing and failover
        - Geo-DNS resolver directing clients to nearest edge node based on IP geolocation
        - Health checking system monitoring edge node availability and removing unhealthy nodes from rotation
        - Consistent hashing mechanism distributing content across edge nodes with minimal rebalancing
        estimated_hours: '10'
      - id: cdn-implementation-m5
        name: CDN Analytics & Performance Optimization
        description: Build cache hit ratio analytics, bandwidth optimization with compression, and range request support.
        acceptance_criteria:
        - Real-time cache hit/miss ratio tracking per edge and per origin
        - Automatic Brotli/gzip compression for text-based content at the edge
        - HTTP Range request support for large files (video streaming, downloads)
        - Stale-while-revalidate support to serve stale content during origin refresh
        pitfalls:
        - Compression CPU overhead at edge
        - Range request cache fragmentation
        concepts:
        - cache analytics
        - content compression
        - range requests
        - stale-while-revalidate
        skills:
        - Cache analytics
        - Content compression
        - HTTP range requests
        deliverables:
        - CDN analytics dashboard and performance optimization features
        - Cache hit ratio tracker measuring hits/misses per content type and edge node
        - Content compression middleware supporting gzip/brotli encoding with configurable compression levels
        - HTTP range request handler supporting partial content delivery for large files and video streaming
        estimated_hours: '8'
    - id: order-matching-engine
      name: Order Matching Engine
      description: Low-latency trading engine with order book, price-time priority
      difficulty: advanced
      estimated_hours: '70'
      essence: Price-level order book management using hash maps and binary heaps for O(1) order insertion and cancellation, implementing FIFO price-time priority matching logic that executes trades in microseconds while coordinating concurrent order streams through lock-free data structures or fine-grained synchronization primitives.
      why_important: Building this develops critical skills in low-latency system design, lock-free concurrent data structures, and understanding how modern financial exchanges operate‚Äîknowledge directly applicable to high-frequency trading, real-time systems, and performance-critical backend engineering.
      learning_outcomes:
      - Design high-performance order matching systems
      - Implement price-time priority matching
      - Handle concurrent order operations safely
      - Build market data dissemination
      skills:
      - Order book data structures
      - Matching algorithms
      - Low-latency design
      - Concurrency
      - Market data feeds
      - Risk checks
      tags:
      - advanced
      - fifo
      - framework
      - game-dev
      - low-latency
      - matching
      - order-book
      architecture_doc: architecture-docs/order-matching-engine/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible: []
      resources:
      - name: Liquibook C++ Order Matching Engine
        url: https://github.com/enewhuis/liquibook
        type: tool
      - name: Matching Engines Guide by Jelle Pelgrims
        url: https://jellepelgrims.com/posts/matching_engines
        type: tutorial
      - name: Order Book Matching Algorithms
        url: https://hkopp.github.io/2016/10/order-book-matching-algorithms
        type: article
      - name: How to Build a Fast Limit Order Book
        url: https://gist.github.com/halfelf/db1ae032dc34278968f8bf31ee999a25
        type: tutorial
      - name: Price-Time Priority - MarketsWiki
        url: https://www.marketswiki.com/wiki/Price-time_priority
        type: documentation
      prerequisites:
      - type: skill
        name: Data structures
      - type: skill
        name: Concurrency
      - type: skill
        name: Networking
      milestones:
      - id: order-matching-engine-m1
        name: Order Book Data Structure
        description: Efficient order book with price levels
        acceptance_criteria:
        - Implement price-level ordered structure supporting bid and ask sides
        - Support both bid and ask sides with correct ordering
        - Enforce price-time priority for orders at same price level
        - Achieve efficient insertion and removal with O(log n) complexity
        pitfalls:
        - Not removing empty price levels causing memory leaks
        - Using linear search instead of tree structure for price lookup
        - Storing full order copies at multiple price levels
        - Breaking FIFO order when inserting into price level queues
        - Not maintaining separate bid and ask tree structures
        concepts:
        - Red-black trees for O(log n) price level lookups
        - FIFO queues at each price level for time priority
        - Limit order book depth aggregation and best bid/ask tracking
        - Memory-efficient order storage with minimal allocations
        - Price level consolidation and removal of empty levels
        skills:
        - Tree structures
        - Price levels
        - Order queues
        deliverables:
        - Order class with price, quantity, side, and timestamp fields
        - Price level structure with FIFO order queue
        - Separate bid and ask side order books
        - O(1) lookup for best bid and best ask prices
        - O(log n) insertion and removal at price levels
        - Hash map for O(1) order ID to order lookup
        estimated_hours: '14'
      - id: order-matching-engine-m2
        name: Order Operations
        description: Add, cancel, and modify orders
        acceptance_criteria:
        - Handle limit order placement at specified price level
        - Support market orders that match immediately at best price
        - Implement order cancellation that removes from book correctly
        - Handle order modification preserving or resetting time priority
        pitfalls:
        - Not validating order price against tick size constraints
        - Allowing modification of already partially filled orders incorrectly
        - Missing duplicate order ID checks causing state corruption
        - Not handling cancel requests for already filled orders gracefully
        - Forgetting to update aggregate book depth on order changes
        concepts:
        - Order ID generation and uniqueness guarantees across sessions
        - Cancel-replace semantics vs separate cancel and add operations
        - Order validation including price tick size and lot size rules
        - Idempotent order operations for duplicate request handling
        - Order state transitions and immutability of filled portions
        skills:
        - Order lifecycle
        - Validation
        - State management
        deliverables:
        - Add limit order to correct side and price level
        - Cancel existing order by order ID
        - Modify order quantity while preserving time priority
        - Input validation for price, quantity, and side fields
        - Order acknowledgment message sent to submitter
        - Execution report generated for fills and cancellations
        estimated_hours: '14'
      - id: order-matching-engine-m3
        name: Matching Engine
        description: Price-time priority matching algorithm
        acceptance_criteria:
        - Match incoming orders against resting orders by price-time priority
        - Generate trade execution records with both sides identified
        - Handle partial fills leaving residual quantity on the book
        - Update order book state correctly after each match cycle
        pitfalls:
        - Not checking for cross-spread conditions before placing limit orders
        - Incorrectly calculating remaining quantity after partial fills
        - Allowing orders to match against themselves without prevention
        - Processing orders out of time priority at same price level
        - Not generating trade events for each distinct fill
        concepts:
        - Price-time priority matching with timestamp precision requirements
        - Aggressive order walking the book until fully matched or depleted
        - Partial fill generation and remaining quantity tracking
        - Self-trade prevention for same participant orders
        - Market order vs limit order matching behavior differences
        skills:
        - Matching algorithms
        - Trade execution
        - Partial fills
        deliverables:
        - Price-time priority matching algorithm implementation
        - Full and partial fill handling with residual order tracking
        - Trade execution record with price, quantity, and timestamps
        - Aggressive versus passive order classification logic
        - Self-trade prevention check before generating executions
        - Match statistics tracking fill rate and execution counts
        estimated_hours: '14'
      - id: order-matching-engine-m4
        name: Concurrency & Performance
        description: Thread-safe operations and low latency
        acceptance_criteria:
        - Handle concurrent order submissions from multiple threads safely
        - Achieve sub-millisecond latency per order on average
        - Support throughput of at least 10K orders per second
        - Implement lock-free data structures where possible for hot paths
        pitfalls:
        - Using coarse-grained locks causing contention bottlenecks
        - Not aligning hot data structures to cache line boundaries
        - Missing memory barriers leading to reordering visibility issues
        - Excessive atomic operations degrading single-threaded performance
        - Not measuring tail latencies under concurrent load conditions
        concepts:
        - Lock-free data structures using atomic operations and CAS
        - Read-copy-update patterns for minimizing write contention
        - Memory ordering guarantees and proper use of barriers
        - Cache line alignment to prevent false sharing between threads
        - Batching operations to amortize synchronization costs
        skills:
        - Lock-free structures
        - Batching
        - Latency optimization
        deliverables:
        - Lock-free order book update mechanism using atomic operations
        - Order batching to reduce per-order overhead
        - Memory pre-allocation pool for order and trade objects
        - Cache-friendly data layout for hot path optimization
        - Latency measurement harness tracking p50, p99, and p999
        - Throughput benchmarking suite measuring orders per second
        estimated_hours: '14'
      - id: order-matching-engine-m5
        name: Market Data & API
        description: Market data feeds and trading API
        acceptance_criteria:
        - Publish order book snapshots with all price levels and quantities
        - Stream trade executions to subscribers in real time
        - Provide REST and WebSocket API for order management
        - Handle market data subscriptions with topic-based filtering
        pitfalls:
        - Not implementing proper WebSocket heartbeat and reconnection logic
        - Sending full order book snapshots instead of incremental deltas
        - Missing sequence number validation causing data inconsistency
        - Not rate limiting market data to prevent client overload
        - Blocking order processing thread with market data publication
        concepts:
        - Level 1 vs Level 2 market data snapshot and incremental updates
        - WebSocket binary protocols for low-latency market data streaming
        - FIX protocol message types for order entry and execution reports
        - Market data conflation strategies under high message rates
        - Sequence number gaps and recovery mechanisms for reliability
        skills:
        - Market data
        - WebSocket
        - FIX protocol
        deliverables:
        - Level 2 market data feed with full order book depth
        - Real-time trade feed broadcasting execution details
        - WebSocket streaming endpoint for live market data
        - REST API for order submission, cancellation, and status
        - Basic FIX protocol message parsing and generation
        - Rate limiting middleware to protect API endpoints
        estimated_hours: '14'
    - id: ledger-system
      name: Double-entry Ledger System
      description: Accounting system with journal entries, balance sheets, audit trail
      difficulty: advanced
      estimated_hours: '50'
      essence: Append-only transaction recording with atomic debit-credit pairs that maintain mathematical invariants (debits = credits) across account hierarchies, using cryptographic chaining for tamper-evidence and supporting efficient balance reconstruction through aggregation queries over immutable journal entries.
      why_important: Financial systems power critical business operations and require absolute correctness‚Äîbuilding this teaches you immutability patterns, data integrity constraints, and audit-compliant architecture that apply broadly to distributed systems, blockchain applications, and any domain where verifiable state transitions matter.
      learning_outcomes:
      - Understand double-entry accounting principles
      - Design immutable financial transaction systems
      - Implement balance calculations and reconciliation
      - Build audit-compliant financial systems
      skills:
      - Double-entry accounting
      - Transaction integrity
      - Balance calculation
      - Audit logging
      - Financial reporting
      - Idempotency
      tags:
      - advanced
      - audit-trail
      - databases
      - double-entry
      - immutable
      - reconciliation
      - storage
      architecture_doc: architecture-docs/ledger-system/index.md
      languages:
        recommended:
        - Go
        - Rust
        - Java
        also_possible: []
      resources:
      - name: Martin Fowler's Accounting Patterns
        url: https://martinfowler.com/eaaDev/AccountingNarrative.html
        type: article
      - name: Double-Entry Accounting for Engineers
        url: https://anvil.works/blog/double-entry-accounting-for-engineers
        type: tutorial
      - name: Accounting for Developers Part I
        url: https://www.moderntreasury.com/journal/accounting-for-developers-part-i
        type: article
      - name: 'SQL Server Ledger: Immutable Audit Trails'
        url: https://dzone.com/articles/sql-server-ledger-tamper-evident-audit-trails
        type: article
      - name: Designing Real-Time Ledger Systems
        url: https://finlego.com/tpost/c2pjjza3k1-designing-a-real-time-ledger-system-with
        type: tutorial
      prerequisites:
      - type: skill
        name: Database design
      - type: skill
        name: Transactions
      - type: skill
        name: API design
      milestones:
      - id: ledger-system-m1
        name: Account & Entry Model
        description: Design core ledger data model
        acceptance_criteria:
        - Account types include asset, liability, equity, income, and expense with correct normal balances
        - Journal entries store multiple debit and credit line items referencing specific accounts
        - Every journal entry enforces that total debits equal total credits before it can be posted
        - Multi-currency accounts store amounts in their native currency with exchange rate metadata
        pitfalls:
        - Allowing unbalanced entries where debits don't equal credits
        - Using mutable entry records that can be changed after posting
        - Not validating account type compatibility with debit/credit rules
        - Missing proper data types for monetary amounts leading to rounding errors
        - Allowing deletion of posted entries instead of reversal entries
        concepts:
        - 'Double-entry principle: every debit must equal total credits'
        - Account hierarchy with asset, liability, equity, revenue, expense types
        - Immutable journal entry design with posting date and reference
        - Chart of accounts structure with account codes and categories
        - Balancing validation before transaction persistence
        skills:
        - Schema design
        - Account types
        - Entry structure
        deliverables:
        - Account table with columns for ID, name, type, currency, and parent account reference
        - Journal entry table with columns for entry ID, date, description, and posting status
        - Entry line items table linking journal entries to accounts with debit or credit amounts
        - Currency handling module that stores amounts as fixed-point integers to avoid floating-point errors
        - Account hierarchy structure supporting parent-child relationships for chart of accounts nesting
        - Normal balance rules engine that enforces debit-normal for assets and credit-normal for liabilities
        estimated_hours: '10'
      - id: ledger-system-m2
        name: Transaction Recording
        description: Record transactions with double-entry
        acceptance_criteria:
        - Journal entries are created atomically so either all lines post or none do on failure
        - Account type validation ensures debit and credit lines reference appropriate account types
        - Each transaction receives a unique auto-generated ID for traceability and audit reference
        - Transaction metadata fields store arbitrary key-value pairs like invoice number or memo
        pitfalls:
        - Not wrapping journal entries in database transactions causing partial posts
        - Missing validation that sum of debits equals sum of credits
        - Allowing transactions to post to closed accounting periods
        - Not implementing idempotency causing duplicate entries on retry
        - Modifying posted entries instead of creating reversal entries
        concepts:
        - Atomic transaction boundaries ensuring all-or-nothing entry posting
        - Database transaction isolation levels for concurrent entry recording
        - Idempotency keys to prevent duplicate transaction recording
        - Entry validation rules including account existence and balance checks
        - Reversal entry pattern for correcting posted transactions
        skills:
        - Transaction atomicity
        - Validation
        - Idempotency
        deliverables:
        - Journal entry creation API that accepts a list of debit and credit lines atomically
        - Balance validation logic that rejects entries where total debits do not equal total credits
        - Idempotency key system that prevents duplicate entries from repeated API submissions
        - Transaction template library providing predefined entry patterns for common operations
        - Batch entry processor that creates multiple journal entries in a single atomic transaction
        - Entry reversal mechanism that creates a counter-entry to void a previously posted entry
        estimated_hours: '10'
      - id: ledger-system-m3
        name: Balance Calculation
        description: Calculate account balances efficiently
        acceptance_criteria:
        - Account balance is calculated efficiently using running totals rather than scanning all entries
        - Balance-at-date queries return the correct balance considering only entries posted on or before that date
        - Running balance updates are triggered automatically when new journal entries are posted
        - System handles large transaction volumes with sub-second balance query response times
        pitfalls:
        - Recalculating balances from scratch on every query causing performance issues
        - Not handling account type sign conventions correctly in balance calculation
        - Missing indexes on posting date and account columns
        - Computing balances without considering transaction posting order
        - Not validating trial balance sums to zero after transactions
        concepts:
        - Running balance maintenance with incremental updates on transaction post
        - Point-in-time balance queries using date range filtering
        - Materialized balance tables for query performance optimization
        - Account type sign conventions for debit and credit balance calculation
        - Trial balance validation ensuring zero sum across all accounts
        skills:
        - Running balances
        - Point-in-time
        - Aggregation
        deliverables:
        - Current balance calculator that sums all posted debits and credits for a given account
        - Point-in-time balance query that computes an account balance as of a specified date
        - Running balance table that caches account balances after each posted journal entry
        - Balance cache invalidation logic that updates cached balances when new entries are posted
        - Trial balance report generator that lists all accounts with their debit and credit totals
        - Account reconciliation tool that compares ledger balances against external bank statements
        estimated_hours: '10'
      - id: ledger-system-m4
        name: Audit Trail
        description: Immutable audit logging
        acceptance_criteria:
        - All changes are logged with timestamp, actor identity, and before/after values of modified fields
        - Posted journal entries cannot be modified or deleted; only reversals are allowed
        - Entry reversal creates a new offsetting entry that zeroes out the original without deleting it
        - Audit reports can be generated for any date range showing all entries and modifications
        pitfalls:
        - Allowing updates to posted entries breaking audit trail integrity
        - Not capturing sufficient audit metadata on each transaction
        - Missing hash chain validation allowing undetected tampering
        - Storing audit logs in mutable database tables
        - Not recording who authorized vs who entered transactions
        concepts:
        - Append-only ledger design preventing modification of historical entries
        - Cryptographic hashing of entry chains for tamper detection
        - Event sourcing pattern storing all state changes as events
        - Audit metadata including user, timestamp, and reason for each entry
        - Write-once storage guarantees for regulatory compliance
        skills:
        - Immutability
        - Audit logs
        - Change tracking
        deliverables:
        - Immutable entry storage that prevents UPDATE and DELETE operations on posted journal entries
        - Correction entry mechanism that creates reversal entries instead of modifying the original
        - Change history table that logs all account and entry modifications with timestamps and actors
        - User action audit log that records who performed each operation and when it occurred
        - Entry approval workflow that requires manager sign-off before journal entries are posted
        - Audit export module that generates downloadable reports of all entries for a given period
        estimated_hours: '10'
      - id: ledger-system-m5
        name: Financial Reports
        description: Generate standard financial reports
        acceptance_criteria:
        - Balance sheet displays assets equal to liabilities plus equity at the selected date
        - Income statement shows net income as total revenue minus total expenses for the period
        - Trial balance report lists all accounts and verifies total debits equal total credits
        - Reports are exported in PDF and CSV formats with proper formatting and headers
        pitfalls:
        - Not enforcing period closing preventing retroactive transaction modifications
        - Calculating income statement without proper period date filtering
        - Missing exchange rate handling for multi-currency transactions
        - Not matching account types to correct financial statement sections
        - Generating reports without validating trial balance is in balance
        concepts:
        - Trial balance report aggregating all account balances with debit/credit columns
        - Balance sheet generation using asset, liability, and equity accounts
        - Income statement calculation from revenue and expense accounts over period
        - Period closing process locking transactions in completed periods
        - Multi-currency translation using exchange rates at transaction and reporting dates
        skills:
        - Report generation
        - Period closing
        - Multi-currency
        deliverables:
        - Income statement generator that computes revenue minus expenses for a given period
        - Balance sheet generator that displays assets, liabilities, and equity at a point in time
        - Cash flow statement generator that categorizes cash movements into operating, investing, and financing
        - Period closing entries processor that transfers income and expense balances to retained earnings
        - Multi-currency reporting that converts foreign currency balances at period-end exchange rates
        - Report export module that outputs financial reports in PDF and CSV formats
        estimated_hours: '10'
    - id: lock-free-structures
      name: Lock-free Data Structures
      description: Lock-free queue, stack, hashmap with CAS operations
      difficulty: advanced
      estimated_hours: '50'
      essence: Thread-safe data structure design using atomic compare-and-swap operations without traditional mutex locks, requiring deep understanding of memory ordering semantics, the ABA problem, and safe memory reclamation techniques to prevent use-after-free in concurrent environments.
      why_important: Lock-free programming is essential for building high-performance systems that avoid contention bottlenecks, and mastering atomic operations, memory ordering, and the ABA problem provides deep understanding of CPU-level concurrency primitives used in production databases, operating systems, and real-time applications.
      learning_outcomes:
      - Understand lock-free programming principles
      - Implement CAS-based algorithms
      - Handle ABA problem and memory reclamation
      - Build high-performance concurrent structures
      skills:
      - Atomic operations
      - Compare-and-swap
      - Memory ordering
      - ABA problem
      - Hazard pointers
      - Lock-free algorithms
      tags:
      - aba-problem
      - advanced
      - atomic
      - cas
      - concurrency
      - wait-free
      architecture_doc: architecture-docs/lock-free-structures/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
        also_possible: []
      resources:
      - name: C++ Atomic Operations Reference
        url: https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange
        type: documentation
      - name: Michael-Scott Queue Paper
        url: https://www.cs.rochester.edu/u/scott/papers/1996_PODC_queues.pdf
        type: paper
      - name: Hazard Pointers Paper
        url: https://www.cs.otago.ac.nz/cosc440/readings/hazard-pointers.pdf
        type: paper
      - name: Lock-Free Programming in Java
        url: https://www.baeldung.com/lock-free-programming
        type: tutorial
      - name: Compare-and-Swap Tutorial
        url: https://jenkov.com/tutorials/java-concurrency/compare-and-swap.html
        type: tutorial
      prerequisites:
      - type: skill
        name: Concurrency basics
      - type: skill
        name: Memory model
      - type: skill
        name: C/C++ or Rust
      milestones:
      - id: lock-free-structures-m1
        name: Atomic Operations
        description: Understanding atomics and memory ordering
        acceptance_criteria:
        - Compare-and-swap wrapper correctly returns success or failure and the observed value on failure
        - Memory ordering semantics for relaxed, acquire, release, and seq_cst are correctly applied to each operation
        - ABA problem is explained and demonstrated with a test case showing the scenario and its consequences
        - Atomic counter incremented by multiple threads produces the exact expected total with no lost updates
        pitfalls:
        - Assuming sequential consistency when using relaxed memory ordering
        - Spinning indefinitely in CAS loops without backoff strategies
        - Mixing atomic and non-atomic operations on shared variables
        - Not understanding ABA problem in naive CAS implementations
        - Ignoring platform-specific memory model differences
        concepts:
        - Compare-and-swap (CAS) loop patterns and retry logic
        - 'Memory ordering constraints: acquire, release, sequentially consistent'
        - CPU cache coherence protocols and visibility guarantees
        - Atomic read-modify-write operations vs separate read/write
        - Happens-before relationships in concurrent execution
        skills:
        - Atomics
        - Memory ordering
        - CAS
        deliverables:
        - Atomic load and store wrappers that read and write shared variables with specified memory ordering
        - Compare-and-swap wrapper that atomically updates a variable only if it equals the expected value
        - Fetch-and-add operation that atomically increments a shared counter and returns the previous value
        - Memory ordering modes supporting relaxed, acquire, release, and sequentially consistent semantics
        - Compiler and CPU memory barrier primitives that prevent instruction reordering across fence points
        - Atomic reference counter that safely tracks shared ownership using atomic increment and decrement
        estimated_hours: '10'
      - id: lock-free-structures-m2
        name: Lock-free Stack
        description: Treiber stack implementation
        acceptance_criteria:
        - Treiber stack uses CAS on the top pointer to push and pop nodes without any mutex locks
        - Concurrent push and pop operations from multiple threads produce no lost or duplicated elements
        - Stack operations are linearizable meaning each appears to occur atomically at some point in time
        - Benchmark shows lock-free stack throughput compared against a mutex-protected stack under contention
        pitfalls:
        - Reusing stack nodes too early causing dangling pointer reads
        - Not handling empty stack case in pop operation
        - Incrementing version counter incorrectly or insufficiently
        - Memory leaks from nodes lost during failed CAS operations
        - Race conditions between reading next pointer and CAS
        concepts:
        - Top-of-stack pointer updates using single-word CAS
        - 'ABA problem: pointer reuse causing incorrect CAS success'
        - Tagged pointers or version counters to prevent ABA
        - Linearization points in push and pop operations
        - Stack node lifecycle and safe memory reclamation timing
        skills:
        - Treiber stack
        - CAS-based push/pop
        - ABA handling
        deliverables:
        - Node structure containing a data payload and an atomic pointer to the next node in the stack
        - Lock-free push operation that uses CAS to atomically prepend a new node to the stack top
        - Lock-free pop operation that uses CAS to atomically remove and return the top node from the stack
        - ABA problem demonstration showing how a naive CAS loop can incorrectly succeed on a reused pointer
        - Tagged pointer solution that appends a monotonic counter to pointers to detect ABA conditions
        - Stack traversal function that iterates through all nodes from top to bottom for debugging
        estimated_hours: '10'
      - id: lock-free-structures-m3
        name: Lock-free Queue
        description: Michael-Scott queue implementation
        acceptance_criteria:
        - Michael-Scott queue algorithm is implemented with separate CAS operations on head and tail pointers
        - Dummy sentinel node separates head and tail operations to avoid contention between enqueue and dequeue
        - FIFO ordering is preserved so elements dequeued appear in exactly the order they were enqueued
        - Empty queue dequeue correctly returns a not-found indicator without blocking or crashing
        pitfalls:
        - Not advancing tail pointer causing enqueue operations to fail
        - Missing helping step when tail is not at actual end
        - Premature node reclamation while other threads still accessing
        - Incorrect linearization when multiple threads help same operation
        - Forgetting to update both head and tail in edge cases
        concepts:
        - 'Two-pointer design: separate head for dequeue, tail for enqueue'
        - Sentinel dummy node to simplify empty queue handling
        - 'Helping mechanism: threads assist stuck operations to ensure progress'
        - Swing tail pointer forward when lagging behind actual tail
        - CAS on both data and next pointer in enqueue
        skills:
        - MS queue
        - Two-pointer queue
        - Helping mechanism
        deliverables:
        - Queue structure with atomic head and tail pointers for the enqueue and dequeue ends
        - Dummy node initializer that creates an empty queue with both head and tail pointing to a sentinel node
        - Lock-free enqueue operation that appends a new node at the tail using two-step CAS
        - Lock-free dequeue operation that removes and returns the node at the head using CAS
        - Tail-update helping mechanism where dequeuers advance a lagging tail pointer they observe
        - Linearizability argument documenting why the queue appears atomic to all concurrent observers
        estimated_hours: '10'
      - id: lock-free-structures-m4
        name: Hazard Pointers
        description: Safe memory reclamation
        acceptance_criteria:
        - Hazard pointer scheme correctly prevents premature deallocation of nodes still in use by other threads
        - In-use pointers are tracked per thread so the scan routine knows which retired nodes are still live
        - Retired nodes not appearing in any thread's hazard list are safely reclaimed and freed
        - Thread exit cleanup releases all hazard pointer slots and reclaims all remaining retired nodes
        pitfalls:
        - Forgetting to clear hazard pointer after finishing node access
        - Setting hazard pointer after loading node pointer (race condition)
        - Not rescanning retirement list after failed reclamation pass
        - Memory leaks from unbounded retirement list growth
        - Incorrect memory ordering when publishing hazard pointers
        concepts:
        - Per-thread hazard pointer lists protecting currently accessed nodes
        - Retirement list for deferred deletion of unprotected nodes
        - Scanning all hazard pointers before actually freeing memory
        - Threshold-based reclamation to batch free operations
        - Memory fence requirements when setting hazard pointers
        skills:
        - Hazard pointers
        - Memory reclamation
        - Deferred free
        deliverables:
        - Hazard pointer registry that maintains a global list of per-thread hazard pointer slots
        - Protect and release operations that announce and clear a thread's intent to access a shared node
        - Retired list that collects nodes removed from the data structure but not yet safe to free
        - Scan-and-reclaim routine that frees retired nodes not referenced by any thread's hazard pointers
        - Thread-local hazard pointer slots that minimize contention on the global registry during access
        - Integration adapter that adds hazard pointer protection to existing lock-free stack and queue operations
        estimated_hours: '10'
      - id: lock-free-structures-m5
        name: Lock-free Hash Map
        description: Concurrent hash map with lock-free operations
        acceptance_criteria:
        - Lock-free hash map supports concurrent insert, lookup, and delete without any mutex locks
        - Bucket resizing migrates entries to a larger array without blocking concurrent readers or writers
        - Concurrent reads and writes from multiple threads produce correct results with no lost updates or phantom reads
        - Throughput benchmark compares lock-free hash map operations per second against a mutex-based hash map
        pitfalls:
        - Not initializing parent buckets before accessing child buckets
        - Incorrect reverse bit calculation causing wrong insertion order
        - Missing sentinel node initialization during bucket split
        - Memory reclamation complexity with nodes shared across buckets
        - Contention hotspots on buckets with skewed hash distribution
        concepts:
        - 'Split-ordered list: logical hash order maintained physically'
        - Recursive bucket splitting for incremental resizing without locks
        - Sentinel nodes marking bucket boundaries in underlying list
        - Reverse bit ordering to preserve list structure during splits
        - Lock-free insertion into sorted list with helping for failed operations
        skills:
        - Concurrent hashing
        - Split-ordered lists
        - Resizing
        deliverables:
        - Hash bucket array with each bucket containing an atomic pointer to a linked list of entries
        - Lock-free insert operation that adds a key-value pair to the appropriate bucket using CAS
        - Lock-free lookup operation that traverses the bucket chain to find a matching key without locks
        - Lock-free delete operation that marks and physically removes an entry from its bucket chain
        - Atomic resize mechanism that gradually migrates entries from old buckets to a new larger array
        - Load factor monitor that triggers resize when the average bucket chain length exceeds a threshold
        estimated_hours: '10'
    - id: cache-optimized-structures
      name: Cache-Optimized Data Structures
      description: Cache-oblivious algorithms, B+ trees, memory layouts
      difficulty: advanced
      estimated_hours: 35-55
      essence: Minimizing cache misses through memory layout transformations and cache-oblivious algorithmic design that exploits spatial and temporal locality across the CPU memory hierarchy without hardware-specific parameters.
      why_important: Understanding cache behavior is critical for writing high-performance systems code‚Äîperformance differences between cache-friendly and cache-unfriendly implementations can easily be 10-100x. This project teaches low-level optimization skills essential for systems programming, game engines, databases, and high-performance computing.
      learning_outcomes:
      - Measure cache performance using hardware counters and profiling tools to identify cache misses and memory bottlenecks
      - Implement data structure transformations between Array-of-Structs and Struct-of-Arrays layouts to improve SIMD vectorization and cache line utilization
      - Design open-addressed hash tables with linear probing optimized for cache locality and reduced pointer chasing
      - Build cache-oblivious B-trees using van Emde Boas recursive layout that automatically adapts to unknown cache parameters
      - Apply loop tiling and blocking techniques to matrix operations for optimal data reuse across cache levels
      - Analyze memory access patterns and refactor algorithms to maximize spatial and temporal locality
      - Debug performance issues caused by false sharing, cache line alignment, and prefetching failures
      skills:
      - Cache profiling and analysis
      - Memory layout optimization
      - Data-oriented design
      - Cache-oblivious algorithms
      - SIMD and vectorization
      - Low-level performance tuning
      - Hardware-software co-design
      - Memory hierarchy exploitation
      tags:
      - advanced
      - alignment
      - c
      - c++
      - cache-lines
      - caching
      - prefetching
      - rust
      architecture_doc: architecture-docs/cache-optimized-structures/index.md
      languages:
        recommended:
        - C
        - C++
        - Rust
        also_possible:
        - Go
        - Zig
      resources:
      - name: What Every Programmer Should Know About Memory
        url: https://people.freebsd.org/~lstewart/articles/cpumemory.pdf
        type: paper
      - name: Cache-Oblivious Algorithms
        url: https://en.wikipedia.org/wiki/Cache-oblivious_algorithm
        type: article
      prerequisites:
      - type: skill
        name: Data structures (arrays, trees, hash tables)
      - type: skill
        name: Understanding of CPU cache hierarchy
      - type: skill
        name: Memory alignment concepts
      - type: skill
        name: Performance profiling basics
      milestones:
      - id: cache-optimized-structures-m1
        name: Cache Fundamentals & Benchmarking
        description: Understand cache behavior and build benchmarking tools to measure cache performance.
        acceptance_criteria:
        - Benchmark detects L1, L2, L3 cache sizes by measuring access latency at different array sizes
        - Sequential access benchmark runs at least 10x faster than random access for large arrays
        - Cache line stride benchmark shows constant time for strides up to 64 bytes then increasing
        - Performance counters report cache miss ratio for each benchmark configuration
        - Results document at least 10x difference between cache-friendly and cache-hostile code
        pitfalls:
        - Compiler optimizations hiding effects
        - Warmup effects
        - System noise in benchmarks
        concepts:
        - Cache hierarchy
        - Cache lines
        - Spatial/temporal locality
        - Prefetching
        skills:
        - Performance profiling and benchmarking
        - Memory access pattern analysis
        - Cache miss measurement techniques
        - Interpreting hardware performance counters
        deliverables:
        - Cache hierarchy measurement tool detecting L1, L2, and L3 cache sizes
        - Performance counter reader collecting cache miss and reference statistics
        - Benchmark harness measuring sequential versus random access timing differences
        - Cache miss measurement comparing access patterns with perf or cachegrind profiling
        estimated_hours: 6-8
      - id: cache-optimized-structures-m2
        name: Array of Structs vs Struct of Arrays
        description: Implement and compare AoS vs SoA memory layouts for better cache utilization.
        acceptance_criteria:
        - AoS particle system stores x, y, z, vx, vy, vz per particle in contiguous struct
        - SoA particle system stores separate arrays for x, y, z, vx, vy, vz values
        - SoA outperforms AoS when updating only position fields without accessing velocity
        - AoS performs competitively when all fields of each entity are accessed together
        - Benchmark demonstrates SIMD vectorization opportunity with SoA memory layout
        pitfalls:
        - Not aligning SoA arrays
        - Forgetting remainder in SIMD loops
        - Over-optimizing when AoS is actually fine
        concepts:
        - Data-oriented design
        - Memory layout
        - SIMD vectorization
        - Cache line utilization
        skills:
        - Memory layout transformation
        - SIMD intrinsics programming
        - Structure padding and alignment
        - Data-oriented design patterns
        deliverables:
        - AoS implementation storing all fields of each entity contiguously in memory
        - SoA implementation storing each field separately in its own contiguous array
        - Access pattern comparison benchmarking single-field versus all-field operations
        - Performance benchmarking with cache miss profiling for both memory layouts
        estimated_hours: 6-10
      - id: cache-optimized-structures-m3
        name: Cache-Friendly Hash Table
        description: Implement a hash table optimized for cache performance using open addressing and linear probing.
        acceptance_criteria:
        - Open addressing with linear probing keeps probe sequence in contiguous memory
        - Separate key and value arrays improve cache utilization during key-only probing phase
        - Robin Hood hashing reduces maximum probe distance for more consistent lookup performance
        - Benchmark shows fewer cache misses per lookup compared to chaining-based hash table
        - Hash table maintains correctness under high load factor with proper resize handling
        pitfalls:
        - Load factor too high (>70%)
        - Poor hash function
        - Not handling resize properly
        concepts:
        - Open addressing
        - Linear probing
        - Robin Hood hashing
        - Cache-conscious design
        skills:
        - Hash table implementation
        - Open addressing collision resolution
        - Dynamic resizing strategies
        - Load factor optimization
        deliverables:
        - Open addressing hash table with linear probing for cache-friendly lookups
        - Cache line aligned storage keeping keys in separate array from values
        - Software prefetching hints improving lookup performance for probable next access
        - Performance comparison benchmarking against chaining-based standard hash map
        estimated_hours: 8-12
      - id: cache-optimized-structures-m4
        name: Cache-Oblivious B-Tree
        description: Implement a van Emde Boas layout B-tree that performs well regardless of cache size.
        acceptance_criteria:
        - Van Emde Boas layout places frequently co-accessed tree nodes in same cache line
        - Search achieves O(log_B N) cache misses without knowing cache parameters explicitly
        - vEB layout outperforms standard layout for tree sizes exceeding L2 cache capacity
        - Performance scales well across different hardware with varying cache hierarchy sizes
        - Benchmark measures cache miss count confirming theoretical access pattern improvement
        pitfalls:
        - Complex index calculations
        - Not handling non-power-of-2 sizes
        - Overhead may exceed benefit for small trees
        concepts:
        - Cache-oblivious algorithms
        - van Emde Boas layout
        - Memory hierarchy independence
        skills:
        - Recursive memory layout design
        - Index arithmetic for implicit trees
        - Cache-oblivious algorithm implementation
        deliverables:
        - Van Emde Boas memory layout storing tree nodes in cache-optimal recursive order
        - Recursive layout construction converting sorted data into vEB-ordered array
        - Search implementation traversing vEB layout with cache-oblivious access pattern
        - Performance analysis comparing vEB layout against standard pointer-based B-tree
        estimated_hours: 10-15
      - id: cache-optimized-structures-m5
        name: Blocked Matrix Operations
        description: Implement cache-blocked matrix multiplication and other operations.
        acceptance_criteria:
        - Naive multiplication serves as baseline with known poor cache behavior for large matrices
        - Blocked multiplication achieves 2-10x speedup over naive for matrices exceeding cache size
        - Auto-tuner selects block size that minimizes execution time through empirical measurement
        - Blocked matrix transpose outperforms naive transpose for matrices larger than L1 cache
        - Blocking technique extends to LU decomposition with similar cache performance improvement
        pitfalls:
        - Block size too large (exceeds cache)
        - Not handling non-block-aligned sizes
        - Memory alignment issues
        concepts:
        - Loop tiling
        - Blocking
        - Cache blocking
        - Auto-tuning
        skills:
        - Loop tiling and blocking
        - Matrix operation optimization
        - Cache-aware algorithm tuning
        - Handling non-uniform matrix dimensions
        deliverables:
        - Naive matrix multiplication as baseline with O(n^3) cache-hostile access pattern
        - Blocked matrix multiplication processing submatrices that fit in cache
        - Block size auto-tuning selecting optimal tile size based on measured cache performance
        - Extended blocked operations for matrix transpose and LU decomposition
        estimated_hours: 6-10
    - id: build-ebpf-tracer
      name: eBPF Tracing Tool
      description: eBPF programs on kprobes/tracepoints with ring buffer
      difficulty: advanced
      estimated_hours: 30-45
      essence: Dynamic kernel instrumentation via eBPF bytecode that safely executes in kernel space, extracting runtime telemetry through kprobes, tracepoints, and BPF maps for efficient kernel-to-userspace data transfer.
      why_important: Building this teaches low-level Linux internals and kernel observability techniques used in production monitoring tools like Cilium and Falco, essential skills for systems programming and performance engineering roles.
      learning_outcomes:
      - Implement eBPF programs in C with proper verifier constraints and safety guarantees
      - Design efficient data collection using BPF maps (hash tables, ring buffers, arrays)
      - Attach kprobes and tracepoints to kernel functions for dynamic instrumentation
      - Build userspace loaders with libbpf or Rust bindings for eBPF lifecycle management
      - Debug kernel-space programs using bpf_trace_printk and BPF verifier logs
      - Measure syscall latency distributions with BPF histograms and perf events
      - Trace TCP connection state transitions using kernel tracepoints
      - Transfer high-volume telemetry data efficiently between kernel and userspace
      skills:
      - eBPF Programming
      - Kernel Tracing
      - BPF Maps
      - System Observability
      - Kprobes and Tracepoints
      - libbpf Framework
      - Performance Analysis
      - Kernel-Userspace Communication
      tags:
      - ebpf
      - tracing
      - observability
      - linux-kernel
      - performance
      architecture_doc: architecture-docs/build-ebpf-tracer/index.md
      languages:
        recommended:
        - C
        - Rust
        also_possible:
        - Go
      resources:
      - name: BPF Performance Tools (Brendan Gregg)
        url: https://www.brendangregg.com/bpf-performance-tools-book.html
        type: book
      - name: eBPF Documentation
        url: https://ebpf.io/what-is-ebpf/
        type: reference
      prerequisites:
      - type: project
        name: build-strace
      - type: project
        name: build-kernel-module
      milestones:
      - id: build-ebpf-tracer-m1
        name: Basic kprobe Tracer
        description: Attach an eBPF program to a kernel function and collect data in userspace.
        acceptance_criteria:
        - Write eBPF program in C compiled to BPF bytecode using clang -target bpf
        - Attach eBPF program to a kprobe (e.g., do_sys_openat2) using libbpf
        - Pass data from eBPF program to userspace via BPF ring buffer
        - Display traced events (function name, PID, timestamp) in userspace
        pitfalls:
        - eBPF verifier rejects unbounded loops and out-of-bounds memory access
        - Must use bpf_probe_read to safely read kernel memory in eBPF programs
        - Ring buffer requires proper polling in userspace to avoid event loss
        concepts:
        - eBPF programs
        - Kprobes
        - Ring buffer communication
        skills:
        - Kernel function hooking
        - Low-level C programming
        - Ring buffer management
        - BPF program compilation
        - System-level debugging
        deliverables:
        - eBPF C program compiled to BPF bytecode with clang
        - Kprobe attachment using libbpf skeleton or direct bpf() syscalls
        - Ring buffer setup for kernel-to-userspace event delivery
        - Userspace event consumer displaying PID, timestamp, and function info
        estimated_hours: 7-10
      - id: build-ebpf-tracer-m2
        name: Syscall Latency Histogram
        description: Measure and display syscall latency distributions using eBPF.
        acceptance_criteria:
        - Attach entry and exit probes to measure per-syscall duration using bpf_ktime_get_ns
        - Store latency data in BPF hash map keyed by syscall number
        - Generate log2 histogram buckets for latency distribution in kernel space
        - Display formatted histogram in userspace similar to bpftrace output
        pitfalls:
        - Entry/exit correlation requires storing start time keyed by thread ID
        - bpf_ktime_get_ns measures monotonic time, not wall clock time
        - Hash map size must be bounded, handle map-full condition gracefully
        concepts:
        - Entry/exit probes
        - BPF maps
        - Latency measurement
        skills:
        - Histogram data structures
        - Time measurement and profiling
        - Map-based state management
        - Statistical analysis of latency
        - Thread-aware tracing
        deliverables:
        - Entry/exit kprobe pair measuring syscall duration
        - BPF hash map for per-syscall latency accumulation
        - Log2 histogram computation in eBPF for efficient distribution tracking
        - Formatted histogram display in userspace with microsecond buckets
        estimated_hours: 7-10
      - id: build-ebpf-tracer-m3
        name: TCP Connection Tracer
        description: Trace TCP connection lifecycle events using tracepoints.
        acceptance_criteria:
        - Attach to sock:inet_sock_set_state tracepoint for TCP state change tracking
        - Capture source/destination IP, port, and TCP state transitions
        - Track connection duration from SYN to FIN/RST
        - Filter by PID, port, or address using BPF map-based configuration
        pitfalls:
        - IPv4 and IPv6 addresses have different sizes, must handle both
        - TCP state machine has many transitions, focus on established/close states initially
        - Tracepoint arguments are accessed through the tracepoint context struct, not register reading
        concepts:
        - Tracepoints
        - TCP state machine
        - Network tracing
        skills:
        - Network protocol tracing
        - IPv4/IPv6 dual-stack handling
        - Event-driven state tracking
        - Tracepoint context parsing
        - Connection lifecycle monitoring
        deliverables:
        - Tracepoint attachment for TCP state change events
        - Connection tracking with source/destination IP and port extraction
        - Connection lifetime measurement from establishment to teardown
        - Configurable filtering by PID, port, or address via BPF maps
        estimated_hours: 7-12
      - id: build-ebpf-tracer-m4
        name: Custom Dashboard and Maps
        description: Build a terminal dashboard combining multiple eBPF data sources.
        acceptance_criteria:
        - Run multiple eBPF programs simultaneously (syscall, network, scheduler tracing)
        - Use BPF per-CPU arrays and hash maps for high-performance data aggregation
        - Build terminal UI showing live-updating metrics, histograms, and top-N lists
        - Support runtime configuration changes (add/remove probes, change filters)
        pitfalls:
        - Per-CPU maps require summing across all CPUs in userspace for correct totals
        - Too many active probes can measurably impact system performance
        - Terminal UI refresh rate must balance responsiveness vs CPU overhead
        concepts:
        - Multi-program coordination
        - Per-CPU data structures
        - Live monitoring
        skills:
        - Multi-source data aggregation
        - Terminal UI programming
        - Per-CPU data collection
        - Real-time monitoring systems
        - Performance overhead optimization
        deliverables:
        - Multi-program eBPF manager running concurrent tracers
        - Per-CPU array aggregation for high-throughput data collection
        - Terminal dashboard with live histograms, counters, and top-N displays
        - Runtime probe management for adding/removing trace points
        estimated_hours: 7-12
    expert:
    - id: build-git
      name: Build Your Own Git
      description: Version control
      difficulty: expert
      estimated_hours: 30-50
      essence: Content-addressable storage using SHA-1/SHA-256 hashing to create an immutable object database where blobs, trees, and commits form a directed acyclic graph representing versioned filesystem snapshots with cryptographic integrity guarantees.
      why_important: Building this teaches you fundamental data structures used in distributed systems, how content-addressable storage enables efficient deduplication and integrity checking, and the graph algorithms underlying modern version control used daily by millions of developers.
      learning_outcomes:
      - Implement content-addressable storage using SHA-1 hashing for object identification
      - Design and build a directed acyclic graph (DAG) structure for commit history
      - Implement diff algorithms (Myers' algorithm) for efficient change detection
      - Build tree data structures to represent hierarchical filesystem snapshots
      - Implement three-way merge with conflict detection and resolution
      - Design reference-based systems for branches and HEAD pointer management
      - Implement file-based database using content hashing for deduplication
      - Debug hash collisions and understand cryptographic hash function properties
      skills:
      - Content-Addressable Storage
      - Cryptographic Hashing
      - Directed Acyclic Graphs
      - Diff Algorithms
      - File System Design
      - Object Serialization
      - Merge Strategies
      - Data Deduplication
      tags:
      - build-from-scratch
      - diff
      - expert
      - go
      - merkle-tree
      - objects
      - python
      - rust
      - version-control
      architecture_doc: architecture-docs/build-git/index.md
      languages:
        recommended:
        - Python
        - Rust
        - Go
        - C
        also_possible: []
      resources:
      - name: Write yourself a Git!
        url: https://wyag.thb.lt/
        type: tutorial
      - name: CodeCrafters Git Challenge
        url: https://app.codecrafters.io/courses/git/overview
        type: tool
      - name: Git Internals - Git Objects
        url: https://git-scm.com/book/en/v2/Git-Internals-Git-Objects
        type: documentation
      prerequisites:
      - type: skill
        name: File I/O and hashing
      - type: skill
        name: Tree data structures
      - type: skill
        name: Basic compression (zlib)
      - type: skill
        name: Graph algorithms
      milestones:
      - id: build-git-m1
        name: Repository Initialization
        description: Implement git init - create .git directory structure.
        acceptance_criteria:
        - Running init creates a .git directory in the current working directory with the correct structure
        - The .git/objects directory is created with subdirectories for object storage and packs
        - The .git/refs/heads directory is created for storing branch reference files
        - 'The .git/HEAD file is created with content ''ref: refs/heads/master'' pointing to the default branch'
        pitfalls:
        - Wrong permissions on .git directory (should be 0755)
        - Missing directories cause later commands to fail silently
        - 'HEAD file must have exact format ''ref: refs/heads/master'' with newline'
        - Creating .git inside existing repo causes confusion
        concepts:
        - Git repository structure
        - References (HEAD, branches)
        skills:
        - File system operations and directory creation
        - String formatting and path manipulation
        - Configuration file handling
        - Process initialization and setup
        deliverables:
        - .git directory structure creation with all required subdirectories for a valid git repository
        - HEAD file creation containing a symbolic reference pointing to refs/heads/master as the default branch
        - Objects and refs directories creation for storing git objects and branch/tag references respectively
        - Initial config file with repository format version and default settings for the new repository
        estimated_hours: 1-2
      - id: build-git-m2
        name: Object Storage (Blobs)
        description: Implement hash-object and cat-file for blob objects.
        acceptance_criteria:
        - hash-object computes the SHA-1 hash of 'blob {size}\0{content}' matching real git's output for the same input
        - Compressed object is stored at .git/objects/xx/yyyyyyyy where xx is the first two hex characters of the hash
        - cat-file retrieves and decompresses the stored object, outputting the original content without the header
        - 'Object format follows git''s specification: type tag, space, decimal size, null byte, then raw content bytes'
        pitfalls:
        - Forgetting null byte between header and content
        - Not compressing before storage
        - Binary vs text content
        concepts:
        - Content-addressable storage
        - SHA-1 hashing
        - Zlib compression
        skills:
        - Cryptographic hashing (SHA-1)
        - Data compression with zlib
        - Binary file I/O
        - Hexadecimal encoding and decoding
        - Content serialization
        deliverables:
        - SHA1 hash computation that produces a 40-character hex digest from the formatted object content
        - Zlib compression of the full object (header + content) before writing to the object store
        - Object file path derivation that splits the hash into a 2-character directory and 38-character filename
        - Blob object format implementation using the 'blob <size>\0<content>' header-content structure
        estimated_hours: 2-3
      - id: build-git-m3
        name: Tree Objects
        description: Implement tree objects that represent directory structure.
        acceptance_criteria:
        - Tree object stores a sorted list of (mode, name, SHA-1 hash) entries for files and subdirectories
        - ls-tree displays tree contents showing mode, object type, hash, and filename for each entry
        - write-tree creates a tree object from the current index reflecting the staged directory structure
        - Nested subdirectories produce nested tree objects with the parent tree referencing child tree hashes
        pitfalls:
        - Hash stored as binary (20 bytes), not hex (40 chars)
        - Sorting rules
        - Mode formatting
        concepts:
        - Tree data structure
        - Directory representation
        - Binary formats
        skills:
        - Binary data structure design
        - Tree traversal and manipulation
        - File mode and permission handling
        - Lexicographic sorting algorithms
        - Packed binary format parsing
        deliverables:
        - Tree object format implementation encoding entries as 'mode name\0<20-byte-hash>' sequences
        - Directory-to-tree conversion that recursively builds tree objects from the working directory structure
        - Tree hash calculation by hashing the formatted tree object content with the 'tree <size>' header
        - Recursive tree building that creates nested tree objects for subdirectories and references them by hash
        estimated_hours: 3-4
      - id: build-git-m4
        name: Commit Objects
        description: Implement commit objects with tree, parent, author, message.
        acceptance_criteria:
        - commit-tree creates a commit object referencing the given tree hash and optional parent commit hashes
        - Commit object contains tree hash, parent hashes, author, committer with timestamps, and the commit message
        - Timestamps are stored in Unix epoch format with a timezone offset (e.g., +0000 or -0500)
        - Chained commits form a linked history where each commit references its parent, creating a DAG of revisions
        pitfalls:
        - Timestamp format
        - Handling merge commits
        - Message can be multi-line
        concepts:
        - Commit graph
        - Directed acyclic graph (DAG)
        - Immutable history
        skills:
        - Graph data structure implementation
        - Timestamp and timezone handling
        - Multi-line text parsing
        - Parent-child relationship management
        - Metadata serialization
        deliverables:
        - Commit object format containing tree hash, parent commit(s), author, committer, and message fields
        - Parent commit linking that records one or more parent hashes for building the commit history graph
        - Timestamp and timezone handling that records author and committer times in Unix epoch with UTC offset
        - Commit hash calculation by hashing the complete commit object with the 'commit <size>' header
        estimated_hours: 2-3
      - id: build-git-m5
        name: References and Branches
        description: Implement branches as references to commits.
        acceptance_criteria:
        - Branch creation writes a new file at .git/refs/heads/<name> containing the target commit hash
        - Branch deletion removes the corresponding ref file from .git/refs/heads/ after safety checks
        - All branches are stored as plain text files in .git/refs/heads/ containing a single commit hash
        - HEAD contains a symbolic reference to the current branch and updates when branches are switched
        - Detached HEAD state writes a raw commit hash to the HEAD file when checking out a specific commit
        pitfalls:
        - Symbolic refs vs direct refs
        - Deleting branch you're on
        - Ref file locking
        concepts:
        - Git references
        - Symbolic references
        - Branch management
        skills:
        - Reference counting and lifecycle management
        - Symbolic link concepts
        - Atomic file operations
        - Concurrent file access patterns
        deliverables:
        - Reference files that store a 40-character commit hash pointing to the tip of a branch
        - Branch creation that writes a new ref file under .git/refs/heads/ with the target commit hash
        - 'HEAD as a symbolic reference file containing ''ref: refs/heads/<branch>'' for the current branch'
        - Detached HEAD handling that writes a raw commit hash to HEAD instead of a symbolic reference
        estimated_hours: 2-3
      - id: build-git-m6
        name: Index (Staging Area)
        description: Implement the staging area for preparing commits.
        acceptance_criteria:
        - Add command stages files to the index by creating blob objects and recording their hash in the index entry
        - Status command shows staged, unstaged, and untracked changes by comparing the index to HEAD and the working tree
        - Index is stored as a binary file at .git/index with a header, sorted entries, and SHA-1 checksum
        - Each index entry stores the file mode, SHA-1 hash, flags, and path name for the staged file
        pitfalls:
        - Index is binary, not text
        - Stat info for detecting changes
        - Path encoding and sorting
        concepts:
        - Staging area concept
        - Binary file formats
        - File metadata caching
        skills:
        - Binary file format design
        - Filesystem metadata extraction
        - Cache invalidation strategies
        - Byte-level data packing
        - Path normalization
        deliverables:
        - Index file binary format parser that reads the header, entries, and checksum from .git/index
        - Add operation that stages a file by computing its blob hash and inserting an entry into the index
        - Remove operation that unstages a file by deleting its entry from the index data structure
        - Index-to-tree conversion that builds tree objects from the sorted index entries for commit creation
        estimated_hours: 4-6
      - id: build-git-m7
        name: Diff Algorithm
        description: Implement diff to show changes between versions.
        acceptance_criteria:
        - Diff shows line-by-line differences with + for additions and - for deletions in the correct order
        - Unified diff format includes @@ hunk headers with line number ranges and surrounding context lines
        - Diff between the working tree and the index shows unstaged modifications to tracked files
        - Diff between two commits compares their tree objects and reports all file-level changes between them
        pitfalls:
        - Binary files
        - Line ending differences
        - Large files / long diffs
        concepts:
        - Diff algorithms (Myers, LCS)
        - Edit distance
        - Unified diff format
        skills:
        - Dynamic programming for sequence alignment
        - Algorithm optimization for large inputs
        - Text processing and line-based operations
        - Output formatting for readability
        deliverables:
        - Myers diff algorithm implementation that finds the shortest edit script between two sequences of lines
        - Line-based diff output that identifies added, removed, and unchanged lines between two file versions
        - Unified diff format output with @@ hunk headers showing line numbers and context lines around changes
        - Diff between trees that compares two tree objects and reports changed, added, and deleted file entries
        estimated_hours: 4-6
      - id: build-git-m8
        name: Merge (Three-way)
        description: Implement three-way merge with conflict detection.
        acceptance_criteria:
        - Merge base algorithm finds the lowest common ancestor commit of the two branches being merged
        - Non-conflicting changes from both branches are applied automatically to produce the merged result
        - Conflicting changes are marked with <<<<<<< ======= >>>>>>> markers in the affected files for manual resolution
        - Merge commit is created with two parent hashes, preserving the full branch topology in the commit graph
        pitfalls:
        - Finding merge base in complex history
        - Handling renames during merge
        - Nested conflicts
        concepts:
        - Three-way merge
        - Merge base calculation
        - Conflict resolution
        skills:
        - Graph traversal for ancestry finding
        - Conflict detection algorithms
        - Multi-way data merging
        - State machine design for merge resolution
        deliverables:
        - Common ancestor finding algorithm that locates the merge base commit between two branch tips
        - Three-way merge algorithm that combines changes from both branches relative to their common ancestor
        - Conflict detection and marking that identifies overlapping edits and inserts conflict markers in the file
        - Merge commit creation with two parent hashes recording both branch tips in the commit history
        estimated_hours: 6-10
    - id: build-text-editor
      name: Build Your Own Text Editor
      description: Vim-like editor
      difficulty: advanced
      estimated_hours: 20-30
      essence: Direct terminal control through ANSI escape sequences and raw mode configuration, implementing efficient text buffer manipulation with gap buffer or array-based data structures for insert/delete operations at cursor position.
      why_important: Building a text editor demystifies how tools like vim and emacs work at the system level, teaching low-level I/O, terminal control, and the data structure trade-offs that impact performance in interactive applications.
      learning_outcomes:
      - Configure terminal raw mode by manipulating termios flags to disable canonical input processing
      - Implement cursor positioning and screen rendering using ANSI escape sequences
      - Design gap buffer or array-based data structure for efficient text insertion and deletion at cursor
      - Handle edge cases in text manipulation including line wrapping, scrolling, and viewport management
      - Implement file I/O operations with proper error handling for loading and saving text
      - Build incremental search with real-time highlighting and cursor navigation
      - Parse and tokenize source code to implement syntax highlighting with state machines
      - Debug low-level terminal interactions and handle platform-specific termios differences
      skills:
      - Terminal I/O Programming
      - ANSI Escape Sequences
      - Raw Mode Configuration
      - Gap Buffer Implementation
      - File I/O Operations
      - State Machine Design
      - Incremental Search
      - Syntax Tokenization
      tags:
      - advanced
      - build-from-scratch
      - c
      - cursor
      - gap-buffer
      - go
      - rope
      - rust
      - syntax-highlighting
      architecture_doc: architecture-docs/build-text-editor/index.md
      languages:
        recommended:
        - C
        - Rust
        - Go
        also_possible: []
      resources:
      - name: Build Your Own Text Editor
        url: https://viewsourcecode.org/snaptoken/kilo/
        type: tutorial
      - name: antirez/kilo source
        url: https://github.com/antirez/kilo
        type: reference
      - name: Hecto (Rust version)
        url: https://philippflenker.com/hecto/
        type: tutorial
      prerequisites:
      - type: skill
        name: Terminal I/O
      - type: skill
        name: C or systems language
      - type: skill
        name: Basic data structures
      milestones:
      - id: build-text-editor-m1
        name: Raw Mode and Input
        description: Put terminal in raw mode and read keypresses.
        acceptance_criteria:
        - Raw mode disables echo so typed characters are not automatically displayed
        - Individual keypresses are read without waiting for Enter key confirmation
        - Arrow keys are recognized from multi-byte escape sequences and mapped to actions
        - Terminal is restored to original cooked mode when editor exits normally or on error
        pitfalls:
        - Not restoring terminal on crash
        - Ctrl+C killing before cleanup
        - Different terminal emulators
        concepts:
        - Terminal modes (canonical vs raw)
        - termios structure
        - Signal handling for cleanup
        skills:
        - System-level programming
        - Terminal I/O control
        - Signal handling and cleanup
        - POSIX termios API
        deliverables:
        - Terminal raw mode setup disabling echo and canonical line buffering
        - Keypress reader processing individual characters and escape sequences
        - Special key handling for arrow keys, home, end, and delete
        - Graceful cleanup restoring terminal settings on editor exit
        estimated_hours: 2-3
      - id: build-text-editor-m2
        name: Screen Refresh
        description: Clear screen and position cursor using escape sequences.
        acceptance_criteria:
        - Screen refresh redraws all visible lines using ANSI cursor positioning sequences
        - Cursor movement commands update both logical position and visible cursor location
        - Status bar at bottom shows current filename, total lines, and cursor row/column
        - Screen refresh avoids flicker by writing complete frame in single terminal write
        pitfalls:
        - Flickering (write small chunks vs one big write)
        - Off-by-one in cursor positioning (1-indexed)
        - Screen size detection
        concepts:
        - VT100 escape sequences
        - Screen buffers
        - Terminal graphics
        skills:
        - ANSI escape sequence manipulation
        - Screen buffer management
        - Terminal graphics programming
        - Cursor positioning control
        deliverables:
        - ANSI escape sequence writer controlling cursor position and screen clearing
        - Screen clearing and full redraw rendering buffer contents to terminal
        - Status bar displaying filename, line count, and cursor position information
        - Cursor positioning keeping visible cursor synchronized with logical position
        estimated_hours: 2-3
      - id: build-text-editor-m3
        name: File Viewing
        description: Load and display file contents with scrolling.
        acceptance_criteria:
        - File loader reads specified file and stores each line in editable buffer structure
        - Scrolling adjusts viewport so cursor line is always visible on screen
        - Horizontal scroll allows viewing and navigating lines longer than terminal width
        - Line numbers display in gutter with consistent width padding for alignment
        pitfalls:
        - Memory allocation for lines
        - Horizontal scrolling (long lines)
        - Tabs rendering
        concepts:
        - File I/O
        - Dynamic arrays
        - Viewport scrolling
        skills:
        - File I/O operations
        - Dynamic memory management
        - Viewport scrolling implementation
        - Line-based text buffer manipulation
        deliverables:
        - File loading reading text file contents into line-based buffer structure
        - Vertical scrolling adjusting viewport offset when cursor moves beyond visible area
        - Horizontal scrolling handling lines wider than terminal column width
        - Line number display showing row numbers in left margin gutter
        estimated_hours: 3-4
      - id: build-text-editor-m4
        name: Text Editing
        description: Insert and delete characters, handle Enter and Backspace.
        acceptance_criteria:
        - Inserting a character shifts remaining text right and advances cursor by one
        - Backspace removes character before cursor and delete removes character after cursor
        - Enter key splits current line into two lines at cursor position
        - Backspace at beginning of line joins it with previous line moving cursor accordingly
        pitfalls:
        - Cursor at end of line edge cases
        - Deleting at beginning of line (join with previous)
        - Memory reallocation
        concepts:
        - Text buffer operations
        - Gap buffer or simple array
        - Change tracking
        skills:
        - In-place text buffer editing
        - String manipulation and insertion
        - Memory reallocation strategies
        - Line joining and splitting operations
        deliverables:
        - Character insertion adding typed characters at current cursor position
        - Character deletion removing characters with backspace and delete keys
        - Line breaking splitting line at cursor position when Enter is pressed
        - Line joining merging current line with previous when backspacing at line start
        estimated_hours: 4-6
      - id: build-text-editor-m5
        name: Save and Undo
        description: Save file to disk and implement undo functionality.
        acceptance_criteria:
        - Save writes all buffer lines to file and clears the dirty/modified flag
        - Dirty flag is set on any edit and cleared on save, shown in status bar
        - Undo reverses the most recent edit operation restoring previous buffer state
        - Redo re-applies the last undone operation returning buffer to later state
        pitfalls:
        - Handling write errors
        - Undo across multiple edits
        - Memory for undo history
        concepts:
        - File writing
        - Undo architectures
        - Command pattern
        skills:
        - File system operations and error handling
        - Command pattern implementation
        - Undo/redo architecture design
        - Persistent state management
        deliverables:
        - File saving writing buffer contents back to disk with confirmation message
        - Dirty flag tracking whether buffer has unsaved modifications
        - Undo stack recording edit operations for reversal on undo command
        - Redo stack allowing re-application of previously undone operations
        estimated_hours: 3-4
      - id: build-text-editor-m6
        name: Search
        description: Implement incremental search functionality.
        acceptance_criteria:
        - Incremental search updates highlighted match after each character typed in query
        - Forward search finds next occurrence after current cursor position wrapping to start
        - Backward search finds previous occurrence before cursor wrapping to document end
        - Escape key cancels search and restores cursor to original position before search began
        pitfalls:
        - Search wrapping at end of file
        - Case sensitivity
        - Restoring state on cancel
        concepts:
        - Text searching
        - Incremental search UX
        - State management
        skills:
        - String searching algorithms
        - Incremental UI state management
        - Pattern matching implementation
        - Search result navigation
        deliverables:
        - Incremental search highlighting matches as user types search query
        - Forward and backward search navigation jumping between match occurrences
        - Search prompt in status bar area accepting query input and showing results
        - Search highlighting visually marking all matching occurrences in visible text
        estimated_hours: 2-3
      - id: build-text-editor-m7
        name: Syntax Highlighting
        description: Add syntax highlighting for common languages.
        acceptance_criteria:
        - File extension maps to correct syntax highlighting rules for that language
        - Keywords like if, else, for, while are displayed in distinct highlight color
        - String literals enclosed in quotes are highlighted including multi-line strings
        - Comments are highlighted differently from code including block comment spans
        pitfalls:
        - Multi-line strings and comments
        - Escape sequences in strings
        - Performance on large files
        concepts:
        - Syntax highlighting algorithms
        - State machines
        - ANSI color codes
        skills:
        - Lexical analysis and tokenization
        - State machine design
        - ANSI color code application
        - Syntax parsing fundamentals
        deliverables:
        - Language detection selecting syntax rules based on file extension
        - Keyword highlighting coloring reserved words in distinct color
        - String and comment highlighting with multi-line span support
        - Number literal highlighting distinguishing numeric values from identifiers
        estimated_hours: 4-6
    - id: build-bittorrent
      name: Build Your Own BitTorrent
      description: P2P file sharing
      difficulty: expert
      estimated_hours: 50-80
      essence: Decentralized file distribution through bencode metadata parsing, binary peer wire protocol implementation, and cryptographic piece verification with concurrent TCP connection management across an untrusted swarm.
      why_important: Building this teaches you distributed systems fundamentals, binary protocol implementation, and concurrent network programming‚Äîskills directly applicable to cloud infrastructure, microservices, and real-time data synchronization systems used in production environments.
      learning_outcomes:
      - Implement bencode parsing and serialization for torrent metadata extraction
      - Design concurrent peer connection management with TCP socket handling
      - Build binary wire protocol parsers for BitTorrent message framing
      - Implement piece selection strategies using rarest-first algorithm
      - Create SHA-1 hash verification for data integrity across distributed sources
      - Design request pipelining and choking algorithms for bandwidth optimization
      - Implement tracker communication using HTTP and UDP protocols
      - Build concurrent piece downloading with proper synchronization and error handling
      skills:
      - Binary Protocol Design
      - Concurrent TCP Networking
      - Cryptographic Hashing
      - P2P Architecture
      - State Machine Implementation
      - Distributed Systems
      - Bencode Parsing
      - Bandwidth Management
      tags:
      - build-from-scratch
      - expert
      - go
      - peer-to-peer
      - pieces
      - python
      - rust
      - swarm
      - torrent
      architecture_doc: architecture-docs/build-bittorrent/index.md
      languages:
        recommended:
        - Go
        - Rust
        - Python
        also_possible:
        - JavaScript
        - Java
      resources:
      - type: specification
        name: BitTorrent Protocol
        url: https://www.bittorrent.org/beps/bep_0003.html
      - type: tutorial
        name: Building a BitTorrent Client
        url: https://blog.jse.li/posts/torrent/
      - type: tool
        name: CodeCrafters BitTorrent
        url: https://app.codecrafters.io/courses/bittorrent/overview
      prerequisites:
      - type: skill
        name: Networking (TCP/UDP)
      - type: skill
        name: Concurrency
      - type: skill
        name: File I/O
      - type: skill
        name: Bencode format
      milestones:
      - id: build-bittorrent-m1
        name: Torrent File Parsing
        description: Parse .torrent files and extract metadata.
        acceptance_criteria:
        - Bencode decoder correctly parses strings, integers, lists, and nested dictionaries from raw bytes
        - Announce URL is extracted from the torrent metainfo and is a valid HTTP or UDP tracker URL
        - File info including name, length, and piece length is correctly extracted from the info dictionary
        - Info hash is a 20-byte SHA1 digest computed from the exact bencoded bytes of the info dictionary
        pitfalls:
        - Bencode parsing edge cases
        - Wrong info dict boundaries
        - Binary vs text strings
        concepts:
        - Bencode format
        - Hashing
        - Torrent metadata
        skills:
        - Binary data parsing
        - Data structure serialization
        - Cryptographic hashing (SHA-1)
        - File I/O operations
        deliverables:
        - 'Bencode decoder supporting all four types: byte strings, integers, lists, and dictionaries'
        - Metainfo extraction that reads the announce URL and info dictionary from the decoded torrent file
        - Info hash calculation by computing the SHA1 digest of the bencoded info dictionary bytes
        - Piece length and piece hashes extraction from the info dictionary for download verification
        estimated_hours: 8-12
      - id: build-bittorrent-m2
        name: Tracker Communication
        description: Communicate with tracker to get peer list.
        acceptance_criteria:
        - HTTP tracker announce request includes all required parameters (info_hash, peer_id, port, uploaded, downloaded, left)
        - Announce request is correctly URL-encoded and sent as an HTTP GET to the tracker
        - Compact peer list is parsed into a list of (IP address, port) pairs for peer connection
        - Client re-announces to the tracker at the interval specified in the tracker response
        pitfalls:
        - URL encoding info_hash
        - Compact vs dict peer format
        - Handling tracker errors
        concepts:
        - HTTP protocol
        - Tracker protocol
        - Peer discovery
        skills:
        - HTTP client implementation
        - URL encoding and query parameters
        - Network protocol design
        - Byte array manipulation
        deliverables:
        - HTTP GET request to the announce URL with info_hash, peer_id, port, and progress parameters
        - Compact peer list parser that decodes 6-byte (IP + port) entries from the tracker response
        - Periodic re-announce that sends updated uploaded/downloaded/left byte counts at the tracker's interval
        - Tracker response handler that parses the interval, complete, incomplete, and peers fields
        estimated_hours: 8-12
      - id: build-bittorrent-m3
        name: Peer Protocol
        description: Implement BitTorrent peer wire protocol.
        acceptance_criteria:
        - Handshake sends and receives the 68-byte message with protocol identifier, info hash, and peer ID
        - Message framing correctly reads the 4-byte length prefix and 1-byte message ID for each peer message
        - Bitfield exchange correctly sends and parses the peer's available-pieces bitmap after the handshake
        - Request and piece messages correctly transfer 16KB blocks with piece index, block offset, and data
        - Choking and unchoking state transitions are handled so downloads only proceed when the peer is unchoked
        pitfalls:
        - Endianness
        - Partial message reads
        - Blocking on choked peer
        concepts:
        - Wire protocol
        - Message framing
        - State machines
        skills:
        - TCP socket programming
        - Binary protocol implementation
        - State machine design
        - Concurrent connection handling
        - Message serialization and deserialization
        deliverables:
        - TCP connection establishment and BitTorrent handshake with protocol string and info hash verification
        - 'Message parser for all peer protocol messages: choke, unchoke, interested, have, bitfield, request, and piece'
        - Peer state machine tracking am_choking, am_interested, peer_choking, and peer_interested flags per connection
        - Request pipelining that sends multiple outstanding block requests to maximize download throughput
        estimated_hours: 15-20
      - id: build-bittorrent-m4
        name: Piece Management & Seeding
        description: Manage pieces, verify hashes, and seed to other peers.
        acceptance_criteria:
        - Piece verification rejects corrupt pieces whose SHA1 hash does not match the torrent metadata and re-requests them
        - Rarest-first selection downloads pieces in order of ascending availability across connected peers
        - Completed pieces are assembled into the output file at the correct byte offsets and the full file hash matches
        - Seeding mode serves requested blocks to connected peers and reports uploaded bytes to the tracker
        - Client maintains connections to multiple peers simultaneously and downloads different pieces from each
        pitfalls:
        - Hash verification failures
        - Race conditions in piece selection
        - Connection management
        concepts:
        - Content verification
        - Scheduling algorithms
        - Concurrent downloads
        skills:
        - Multi-threaded programming
        - Data integrity verification
        - Resource scheduling algorithms
        - Concurrent data structure access
        - Network bandwidth management
        deliverables:
        - Piece verification that computes SHA1 of each downloaded piece and compares it to the expected hash
        - Rarest-first piece selection strategy that prioritizes downloading pieces held by the fewest peers
        - Endgame mode that requests remaining blocks from all peers to avoid stalling on the last pieces
        - Upload capability that serves piece data to requesting peers for seeding after download completes
        estimated_hours: 15-20
    - id: build-dns
      name: Build Your Own DNS Server
      description: Recursive resolver
      difficulty: expert
      estimated_hours: 40-60
      essence: Binary protocol parsing of variable-length message formats with pointer compression, recursive query resolution traversing a hierarchical tree of distributed authoritative nameservers from root to leaf, and cache management with TTL-based expiration to minimize latency in domain-to-IP translation.
      why_important: Building a DNS server teaches foundational internet infrastructure and low-level network programming that underpins every web service, with skills directly applicable to building distributed systems, protocol implementations, and performance-critical networking applications.
      learning_outcomes:
      - Implement binary protocol parsing and serialization for DNS message formats
      - Design recursive resolution algorithms that traverse the DNS hierarchy from root servers
      - Build efficient caching mechanisms with TTL expiration and negative caching
      - Handle UDP and TCP transport protocols with proper packet size limits
      - Implement state management for concurrent query handling across multiple clients
      - Debug network protocols using packet capture tools like Wireshark
      - Optimize memory layouts for high-throughput packet processing
      - Understand zone file formats and authoritative name server data structures
      skills:
      - Binary Protocol Parsing
      - Recursive Algorithms
      - Network Socket Programming
      - Caching Strategies
      - UDP/TCP Protocols
      - Concurrent Request Handling
      - Memory-Efficient Data Structures
      - DNS Infrastructure
      tags:
      - build-from-scratch
      - c
      - expert
      - go
      - networking
      - records
      - recursion
      - resolver
      - rust
      - zone-files
      architecture_doc: architecture-docs/build-dns/index.md
      languages:
        recommended:
        - Go
        - Rust
        - C
        also_possible:
        - Python
        - JavaScript
      resources:
      - type: specification
        name: RFC 1035 - DNS
        url: https://tools.ietf.org/html/rfc1035
      - type: tool
        name: CodeCrafters DNS
        url: https://app.codecrafters.io/courses/dns-server/overview
      - type: article
        name: How DNS Works
        url: https://howdns.works/
      prerequisites:
      - type: skill
        name: UDP networking
      - type: skill
        name: DNS protocol basics
      - type: skill
        name: Caching strategies
      milestones:
      - id: build-dns-m1
        name: DNS Message Parsing
        description: Parse and construct DNS messages.
        acceptance_criteria:
        - Header parser correctly extracts the 16-bit ID, QR flag, opcode, rcode, and all section counts
        - Question section parser decodes domain names with label-length encoding and extracts QTYPE and QCLASS
        - Answer section parser reads resource records with name, type, class, TTL, and type-specific RDATA
        - Name compression using DNS pointer labels (0xC0 prefix) is correctly decoded during parsing
        - Message construction serializes a DNS response into the correct wire-format byte sequence
        pitfalls:
        - Compression pointer loops
        - Wrong byte order
        - Name not null-terminated
        concepts:
        - Binary protocols
        - Name compression
        - Message format
        skills:
        - Binary data manipulation
        - Network protocol implementation
        - Bit-level operations
        - Memory-safe parsing
        deliverables:
        - DNS header parser that reads the 12-byte header including ID, flags, and section counts
        - Question section parser that extracts the queried domain name, record type, and class
        - Resource record parser supporting A, AAAA, CNAME, MX, NS, SOA, and TXT record types
        - DNS message serializer that encodes header, questions, and resource records back into wire format bytes
        estimated_hours: 10-15
      - id: build-dns-m2
        name: Authoritative Server
        description: Respond to queries from local zone data.
        acceptance_criteria:
        - Zone file parser reads standard BIND-format zone files and loads records into an in-memory data structure
        - Query matching returns the correct record set for a given domain name and record type from the zone
        - SOA records are returned in the authority section for NXDOMAIN and negative responses
        - NS records are correctly included in the authority section for delegated subdomains
        - A and AAAA records are returned in the answer section when the queried name and type match a zone record
        pitfalls:
        - Wildcard matching
        - CNAME chasing
        - Case insensitivity
        concepts:
        - Zone files
        - Record types
        - Authoritative responses
        skills:
        - Zone file parsing
        - DNS record management
        - String pattern matching
        - Authoritative response construction
        deliverables:
        - Zone file parser that reads BIND-format zone files with SOA, NS, A, AAAA, CNAME, and MX records
        - Query matching engine that finds the best-matching records in the zone data for a given question
        - SOA and NS record handling that populates the authority section for authoritative responses
        - Authority and additional section population that includes glue records for NS referrals
        estimated_hours: 10-15
      - id: build-dns-m3
        name: Recursive Resolver
        description: Implement recursive resolution from root servers.
        acceptance_criteria:
        - Iterative queries start at root servers and follow referrals down the DNS hierarchy to the authoritative server
        - NS referrals are followed correctly by querying the referred nameserver for the next level of the domain
        - Root hints bootstrap the resolver with the IP addresses of the root nameservers for initial queries
        - Glue records (A records for NS names in the additional section) are used to avoid circular resolution dependencies
        pitfalls:
        - Infinite referral loops
        - Missing glue records
        - CNAME handling
        concepts:
        - Iterative resolution
        - Referrals
        - DNS hierarchy
        skills:
        - Multi-step query resolution
        - Root server interaction
        - Network request orchestration
        - Referral chain traversal
        deliverables:
        - Root server hints file containing the IP addresses of the 13 DNS root servers for bootstrapping resolution
        - Iterative query engine that follows NS referrals from root servers down to the authoritative server
        - CNAME following logic that transparently resolves CNAME chains to the final target record
        - Response construction that assembles the final answer from the results of the iterative resolution process
        estimated_hours: 12-18
      - id: build-dns-m4
        name: Caching & Performance
        description: Implement caching with TTL and negative caching.
        acceptance_criteria:
        - Cached records are returned immediately and evicted automatically when their TTL countdown reaches zero
        - Negative caching stores NXDOMAIN responses to avoid repeated queries for non-existent domains
        - Cache poisoning is prevented by validating that response records match the queried domain and ignoring out-of-bailiwick data
        - UDP server handles concurrent queries from multiple clients without blocking on any single resolution
        pitfalls:
        - TTL underflow
        - Cache poisoning
        - Memory exhaustion
        concepts:
        - Caching strategies
        - Security
        - Concurrency
        skills:
        - Cache invalidation strategies
        - Concurrent data structure design
        - Memory management at scale
        - Time-based expiration systems
        deliverables:
        - TTL-based cache that stores resolved records and evicts them when their time-to-live expires
        - Negative caching that stores NXDOMAIN and NODATA responses with the SOA minimum TTL for the zone
        - Cache lookup that checks for cached answers before initiating a recursive resolution query
        - Concurrent query handler that processes multiple DNS requests simultaneously using async I/O or threading
        estimated_hours: 10-15
    - id: build-debugger
      name: Build Your Own Debugger
      description: GDB-like
      difficulty: expert
      estimated_hours: 50-80
      essence: Process introspection through ptrace system calls for execution control, software breakpoint injection via instruction patching (INT3/0xCC), and parsing DWARF debug metadata to map machine addresses and register states back to source-level symbols and type information.
      why_important: Debuggers reveal how operating systems expose process internals and how compilers encode source-to-binary mappings, providing deep insight into systems programming, binary formats, and the runtime behavior of compiled code.
      learning_outcomes:
      - Implement process control using ptrace to pause, resume, and single-step program execution
      - Design software breakpoints by injecting int3 instructions and handling trap signals
      - Parse DWARF debug information to map machine addresses to source code locations
      - Read and modify process memory and CPU registers through operating system APIs
      - Handle signal-based debugging events and distinguish between breakpoint hits and program signals
      - Implement stack unwinding using frame pointers and DWARF call frame information
      - Build expression evaluators to inspect variable values using type metadata
      - Navigate compilation units and debug information entries to resolve symbols
      skills:
      - ptrace System Calls
      - Signal Handling
      - DWARF Parsing
      - Binary Instruction Patching
      - Memory Introspection
      - Process Control
      - Symbol Resolution
      - ELF Format
      tags:
      - breakpoints
      - build-from-scratch
      - c
      - c++
      - expert
      - inspection
      - ptrace
      - rust
      - stepping
      - tool
      architecture_doc: architecture-docs/build-debugger/index.md
      languages:
        recommended:
        - C
        - Rust
        - C++
        also_possible:
        - Go
      resources:
      - type: article
        name: Writing a Linux Debugger
        url: https://blog.tartanllama.xyz/writing-a-linux-debugger-setup/
      - type: book
        name: The Linux Programming Interface - Ch 26
        url: https://man7.org/tlpi/
      prerequisites:
      - type: skill
        name: Unix processes
      - type: skill
        name: Assembly basics
      - type: skill
        name: ELF format
      milestones:
      - id: build-debugger-m1
        name: Process Control
        description: Use ptrace to control debugee execution.
        acceptance_criteria:
        - Debugger forks a child process and attaches to it with ptrace before the child executes the target program
        - Single-step execution advances exactly one instruction and returns control to the debugger after each step
        - Continue execution resumes the tracee and blocks the debugger until the next signal or breakpoint is hit
        - Debugger correctly waits for and handles signals from the tracee using waitpid with status inspection
        pitfalls:
        - Signal handling in debugger
        - Race conditions
        - Zombie processes
        concepts:
        - ptrace
        - Process control
        - Unix signals
        skills:
        - System call debugging with ptrace
        - Process state management
        - Signal interception and forwarding
        - Parent-child process coordination
        deliverables:
        - ptrace() attachment that connects the debugger to a target process for inspection and control
        - Process start and stop control using SIGSTOP and SIGCONT signals to pause and resume execution
        - Single-step execution using PTRACE_SINGLESTEP to advance exactly one machine instruction at a time
        - Continue execution using PTRACE_CONT to resume the process until the next breakpoint or signal
        estimated_hours: 8-12
      - id: build-debugger-m2
        name: Breakpoints
        description: Implement software breakpoints using int3.
        acceptance_criteria:
        - Setting a breakpoint at a given address replaces the first byte of the instruction with 0xCC (INT3)
        - Original instruction byte is saved and correctly restored when the breakpoint is hit so execution can continue
        - Breakpoint hit is detected by the debugger via SIGTRAP, and the instruction pointer is adjusted back by one byte
        - Multiple breakpoints can be set simultaneously and each fires independently when its address is reached
        pitfalls:
        - Multi-byte instruction boundaries
        - Breakpoint in loop
        - Thread safety
        concepts:
        - Software breakpoints
        - Instruction patching
        - Program counter
        skills:
        - Binary instruction manipulation
        - Memory patching techniques
        - Instruction pointer control
        - Trap instruction handling
        deliverables:
        - Software breakpoint implementation that writes an INT3 (0xCC) byte at the target instruction address
        - Original instruction preservation that saves the overwritten byte for restoration when the breakpoint is hit
        - Breakpoint enable and disable toggling that inserts or restores the original byte without losing the breakpoint
        - Hit count tracking that increments a counter each time a breakpoint is triggered during execution
        estimated_hours: 10-15
      - id: build-debugger-m3
        name: Symbol Tables
        description: Parse DWARF debug info for source-level debugging.
        acceptance_criteria:
        - ELF section headers are parsed to locate .debug_info, .debug_line, .debug_abbrev, and .symtab sections
        - DWARF Debug Information Entries (DIEs) are decoded including tags, attributes, and tree structure
        - Address-to-line mapping converts a program counter value to the corresponding source file and line number
        - Name-to-address mapping resolves a function name to its entry point address for setting breakpoints by name
        pitfalls:
        - DWARF version differences
        - Inlined functions
        - Optimized code mapping
        concepts:
        - Debug information
        - Symbol tables
        - ELF/DWARF
        skills:
        - Binary format parsing
        - Debug metadata interpretation
        - Source-to-machine code mapping
        - Symbol resolution
        deliverables:
        - DWARF debug information parser that reads compilation unit entries from the .debug_info ELF section
        - Function name to address mapping built from DWARF subprogram DIEs for breakpoint-by-name support
        - Source file and line number mapping from the .debug_line section for source-level stepping and display
        - Variable location information extracted from DWARF location lists for inspecting local and global variables
        estimated_hours: 15-25
      - id: build-debugger-m4
        name: Variable Inspection
        description: Read and display variable values using debug info.
        acceptance_criteria:
        - Variable location is resolved from DWARF info to either a register, stack offset, or global memory address
        - Register and memory values are read correctly using ptrace and displayed as the appropriate data type
        - Different data types (int, float, char, pointer, array) are formatted and displayed in human-readable notation
        - Struct members are accessible by name, displaying each field with its type and current value
        pitfalls:
        - Optimized-out variables
        - Complex location expressions
        - Type alignment
        concepts:
        - Variable location
        - Type information
        - Register access
        skills:
        - Memory address calculation
        - Register value extraction
        - Type-aware data interpretation
        - Stack frame traversal
        deliverables:
        - Memory reader that fetches bytes at a variable's address using PTRACE_PEEKDATA for value inspection
        - Type-aware value formatter that interprets raw bytes as int, float, char, or pointer based on DWARF type info
        - Struct and array member access that navigates composite types using field offsets and element strides
        - Register value reader that retrieves CPU register contents using PTRACE_GETREGS for register-allocated variables
        estimated_hours: 17-28
    - id: build-lsp
      name: Build Your Own LSP Server
      description: Language server protocol
      difficulty: expert
      estimated_hours: 50-80
      essence: JSON-RPC message transport implementing bidirectional editor-server communication protocol, incremental document synchronization with version tracking, and AST-based semantic analysis for real-time code intelligence features like completion, hover, and navigation.
      why_important: Building an LSP server teaches fundamental compiler frontend techniques (lexing, parsing, semantic analysis) while learning how modern IDEs achieve language-agnostic tooling through standardized protocols, skills directly applicable to developer tools, IDE extensions, and language toolchain development.
      learning_outcomes:
      - Implement JSON-RPC 2.0 message framing and request-response handling over stdio/TCP
      - Design incremental parser that processes document edits without full re-parsing
      - Build symbol table and scope resolution for semantic token analysis
      - Implement completion engine with context-aware suggestion ranking
      - Create goto-definition using AST traversal and symbol reference tracking
      - Design diagnostic system with error recovery and multi-file dependency analysis
      - Implement code action provider with AST transformation and refactoring suggestions
      - Handle concurrent document state updates with version tracking and conflict resolution
      skills:
      - JSON-RPC Protocol
      - Abstract Syntax Trees
      - Incremental Parsing
      - Semantic Analysis
      - Symbol Resolution
      - Editor Integration
      - Concurrent State Management
      - Protocol Design
      tags:
      - build-from-scratch
      - compilers
      - completion
      - diagnostics
      - expert
      - go
      - language-server
      - protocols
      - refactoring
      - rust
      - typescript
      architecture_doc: architecture-docs/build-lsp/index.md
      languages:
        recommended:
        - TypeScript
        - Rust
        - Go
        also_possible:
        - Python
        - C#
      resources:
      - type: specification
        name: LSP Specification
        url: https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/
      - type: article
        name: LSP Tutorial
        url: https://code.visualstudio.com/api/language-extensions/language-server-extension-guide
      prerequisites:
      - type: skill
        name: JSON-RPC
      - type: skill
        name: AST/parsing
      - type: skill
        name: IDE concepts
      - type: skill
        name: Concurrency
      milestones:
      - id: build-lsp-m1
        name: JSON-RPC & Initialization
        description: Implement JSON-RPC transport and LSP initialization.
        acceptance_criteria:
        - JSON-RPC messages are correctly framed with Content-Length headers and parsed from the byte stream
        - Initialize/initialized handshake completes successfully and the server transitions to the ready state
        - Capability negotiation returns a ServerCapabilities object listing all features the server supports
        - Shutdown request returns success and the subsequent exit notification causes the server process to terminate with exit code 0
        pitfalls:
        - Content-Length calculation
        - Partial message handling
        - Encoding issues
        concepts:
        - JSON-RPC
        - Protocol negotiation
        - Streaming
        skills:
        - JSON-RPC protocol implementation
        - Bidirectional message streaming
        - LSP capability negotiation
        - Protocol version compatibility handling
        deliverables:
        - JSON-RPC message parser and serializer handling Content-Length framing over stdin/stdout transport
        - Initialize request handler that receives client capabilities and returns the server's supported capabilities
        - Server capabilities negotiation that advertises supported features (completion, hover, diagnostics, etc.)
        - Shutdown and exit handling that cleanly terminates the server process when the client disconnects
        estimated_hours: 8-12
      - id: build-lsp-m2
        name: Document Synchronization
        description: Track document changes from the editor.
        acceptance_criteria:
        - textDocument/didOpen notification stores the document content and triggers initial analysis
        - textDocument/didChange notification applies edits and updates the stored document to the latest version
        - textDocument/didClose notification removes the document from the server's in-memory store
        - Incremental sync applies range-based text changes correctly, updating only the modified portion of the document
        pitfalls:
        - Version mismatch
        - Offset calculation errors
        - Unicode handling
        concepts:
        - Document tracking
        - Incremental updates
        - Positions
        skills:
        - Text document state management
        - Incremental content synchronization
        - Position and range calculations
        - UTF-16 code unit handling
        deliverables:
        - didOpen handler that stores the full document text when a file is opened in the editor
        - didClose handler that removes the document from the server's in-memory document store
        - didChange handler that applies incremental or full text edits to the stored document content
        - Document state manager that tracks document URIs, version numbers, and current text content
        estimated_hours: 8-12
      - id: build-lsp-m3
        name: Language Features
        description: Implement completion, hover, and go-to-definition.
        acceptance_criteria:
        - textDocument/completion returns a list of relevant completion items based on the cursor position and context
        - textDocument/hover returns a MarkupContent response with type signature and documentation for the hovered symbol
        - textDocument/definition returns the source location (file URI, line, column) of the symbol's declaration
        - Symbol resolution correctly maps identifier references to their declaring scope and definition location
        pitfalls:
        - Stale symbol table
        - Scope visibility
        - Performance on large files
        concepts:
        - Symbol resolution
        - AST analysis
        - IDE features
        skills:
        - Abstract syntax tree traversal
        - Symbol table construction and queries
        - Context-aware code completion
        - Cross-reference resolution
        deliverables:
        - Completion provider that returns context-appropriate completion items with labels, kinds, and documentation
        - Hover provider that returns type information and documentation for the symbol under the cursor
        - Go-to-definition implementation that resolves a symbol reference to its declaration source location
        - Find-all-references implementation that locates every usage of a symbol across the open documents
        estimated_hours: 15-20
      - id: build-lsp-m4
        name: Diagnostics & Code Actions
        description: Report errors and suggest fixes.
        acceptance_criteria:
        - textDocument/publishDiagnostics pushes diagnostics to the client whenever the document content changes
        - textDocument/codeAction returns applicable code actions (quick fixes, refactors) for the given range or diagnostic
        - Diagnostic severity is correctly set to Error, Warning, Information, or Hint for each reported issue
        - Quick fix code actions include text edits that, when applied, resolve the associated diagnostic
        pitfalls:
        - Diagnostic spam
        - Outdated diagnostics
        - Large workspace performance
        concepts:
        - Static analysis
        - Quick fixes
        - Editor integration
        skills:
        - Static code analysis implementation
        - Diagnostic message formatting
        - Code action provider design
        - Workspace-wide analysis optimization
        deliverables:
        - Diagnostic publisher that sends error and warning diagnostics to the editor after each document change
        - Error and warning reporting with severity levels, source ranges, and descriptive messages per diagnostic
        - Code action provider that suggests fixes and refactorings applicable to the current selection or diagnostic
        - Quick fix implementations that generate text edits to automatically resolve common diagnostic issues
        estimated_hours: 15-20
    - id: build-emulator
      name: Build Your Own Emulator
      description: NES/GameBoy/CHIP-8
      difficulty: expert
      estimated_hours: 60-120
      essence: Cycle-accurate simulation of legacy CPU architectures through instruction-level interpretation, memory-mapped I/O handling, and precise timing synchronization between processor, graphics, and audio subsystems.
      why_important: Building an emulator teaches low-level systems programming and computer architecture fundamentals that are directly applicable to embedded systems, kernel development, compiler optimization, and performance-critical software engineering.
      learning_outcomes:
      - Implement fetch-decode-execute cycles for 8-bit CPU instruction sets
      - Design memory management systems with bank switching and memory-mapped registers
      - Build scanline-based graphics rendering pipelines that sync with CPU timing
      - Debug timing-sensitive code using cycle-accurate execution models
      - Handle interrupts and DMA transfers between CPU and peripheral components
      - Implement bitwise operations for graphics rendering and sprite manipulation
      - Optimize interpreter loops and instruction dispatch for performance
      - Reverse-engineer hardware behavior from incomplete specifications
      skills:
      - CPU Architecture
      - Instruction Set Simulation
      - Memory Mapping
      - Cycle-Accurate Timing
      - Binary Data Manipulation
      - Hardware Emulation
      - Low-Level Debugging
      - Systems Programming
      tags:
      - build-from-scratch
      - c
      - c++
      - cpu
      - expert
      - game-dev
      - instructions
      - io
      - memory-mapping
      - rust
      architecture_doc: architecture-docs/build-emulator/index.md
      languages:
        recommended:
        - C
        - Rust
        - C++
        also_possible:
        - Go
        - TypeScript
      resources:
      - type: guide
        name: Writing a CHIP-8 Emulator
        url: https://tobiasvl.github.io/blog/write-a-chip-8-emulator/
      - type: documentation
        name: Pan Docs (Game Boy)
        url: https://gbdev.io/pandocs/
      - type: reference
        name: NesDev Wiki
        url: https://www.nesdev.org/wiki/Nesdev_Wiki
      prerequisites:
      - type: skill
        name: Binary/hex
      - type: skill
        name: Assembly basics
      - type: skill
        name: Graphics basics
      milestones:
      - id: build-emulator-m1
        name: CPU Emulation
        description: Implement the instruction set and registers.
        acceptance_criteria:
        - All CPU registers are implemented with correct bit widths matching the target architecture specification
        - Fetch-decode-execute cycle reads the opcode at the program counter, decodes it, and dispatches the correct handler
        - All documented opcodes of the target ISA are implemented and produce correct register and memory side effects
        - CPU flags (zero, carry, half-carry, negative) are updated correctly after each arithmetic and logic instruction
        pitfalls:
        - Endianness
        - Flag edge cases
        - Undocumented opcodes
        concepts:
        - CPU architecture
        - Instruction sets
        - Machine code
        skills:
        - Binary arithmetic and bitwise operations
        - Register manipulation and state management
        - Opcode decoding and dispatch tables
        - Low-level debugging with hex dumps
        deliverables:
        - Instruction fetch-decode-execute loop that reads opcodes from memory and dispatches to handlers
        - Register file implementation with all general-purpose, flag, and special-purpose registers of the target CPU
        - ALU operations implementing arithmetic, logic, shift, and rotate instructions for the target ISA
        - Program counter management and CPU flags handling (zero, carry, negative, overflow) updated per instruction
        estimated_hours: 15-30
      - id: build-emulator-m2
        name: Memory System
        description: Implement memory mapping and bank switching.
        acceptance_criteria:
        - Memory map implements the correct address ranges for ROM, RAM, and I/O regions per the target platform spec
        - ROM loading places the program binary at the correct start address and enforces read-only access
        - RAM regions support read and write operations with correct byte-level addressing and initial state
        - Memory-mapped I/O reads and writes trigger the correct peripheral handler rather than accessing raw memory
        pitfalls:
        - Bank 0 special cases
        - Echo RAM
        - Write-only registers
        concepts:
        - Memory mapping
        - Bank switching
        - MMIO
        skills:
        - Address space partitioning and mapping
        - Dynamic memory region switching
        - Hardware register emulation
        - Pointer arithmetic and bounds checking
        deliverables:
        - Memory address space emulation providing a byte-addressable array covering the full address range of the target
        - Memory-mapped I/O regions that route reads and writes to peripheral device emulation handlers
        - ROM and RAM region definitions that enforce read-only access for ROM and read-write for RAM areas
        - Bank switching mechanism (if applicable) that swaps ROM or RAM banks based on mapper register writes
        estimated_hours: 12-20
      - id: build-emulator-m3
        name: Graphics
        description: Implement the display/PPU (Picture Processing Unit).
        acceptance_criteria:
        - Screen rendering produces a pixel-accurate display output matching the target platform's resolution and palette
        - Sprites are rendered with correct positioning, priority, flipping, and transparency per the target hardware behavior
        - Background tiles are rendered using the tile map and tile data with correct scrolling offsets
        - Rendering timing matches the target hardware's scanline and frame timing to avoid visual artifacts
        pitfalls:
        - Mid-frame register changes
        - Sprite limit per line
        - Priority rules
        concepts:
        - Tile graphics
        - Scanline rendering
        - Sprite systems
        skills:
        - Framebuffer management and pixel rendering
        - Scanline-based rendering loops
        - Sprite rendering and layering
        - PPU state machine implementation
        deliverables:
        - Display buffer (framebuffer) storing pixel data at the target platform's native resolution and color depth
        - Sprite and tile rendering engine that composites sprite and background layers per the target graphics hardware
        - Scanline timing emulation that renders each horizontal line at the correct cycle count per the display timing spec
        - Screen output module that blits the framebuffer to a host window surface at the target refresh rate
        estimated_hours: 18-35
      - id: build-emulator-m4
        name: Timing and Input
        description: Implement accurate timing and controller input.
        acceptance_criteria:
        - Cycle-accurate timing ensures CPU instructions consume the correct number of machine cycles per the ISA specification
        - Timer interrupts fire at the correct interval based on the timer register configuration and clock divider
        - Joypad input reads return the correct button state reflecting the currently pressed host keyboard or gamepad buttons
        - Audio output produces sound at the correct pitch and timing relative to the emulated CPU clock speed
        pitfalls:
        - Cycle counting accuracy
        - Timer edge cases
        - VBlank timing
        concepts:
        - Cycle accuracy
        - Hardware timers
        - Input handling
        skills:
        - Cycle-accurate timing synchronization
        - Input polling and event handling
        - Frame rate limiting and vsync
        - Interrupt timing and handling
        deliverables:
        - Cycle-accurate timing system that tracks CPU cycles per instruction and synchronizes subsystems accordingly
        - Input device mapping that translates host keyboard or gamepad events to the target platform's input register format
        - Interrupt handling that triggers the correct CPU interrupt vector on timer overflow, VBlank, and I/O events
        - Audio timing synchronization that generates sound samples at the correct rate relative to CPU cycle execution
        estimated_hours: 15-35
    - id: build-browser
      name: Build Your Own Browser
      description: Browser engine
      difficulty: expert
      estimated_hours: 80-150
      essence: Multi-stage rendering pipeline transforming markup text into pixels through tokenization and parsing (HTML/CSS to AST), tree construction (DOM/CSSOM), style resolution and cascade application, box model layout computation with recursive reflow, and rasterization to a 2D canvas through painting primitives.
      why_important: Building a browser engine demystifies the entire web stack by forcing you to implement the same parsing, layout, and rendering algorithms that power modern browsers, giving you deep insight into performance optimization and computer graphics fundamentals.
      learning_outcomes:
      - Implement recursive descent parsers for context-free grammars (HTML/CSS syntax)
      - Design tree data structures for DOM representation and traversal
      - Build CSS selector matching and specificity resolution algorithms
      - Implement the CSS box model with flow layout and positioning
      - Calculate layout trees with recursive reflow and constraint solving
      - Render visual output using 2D graphics primitives and painting algorithms
      - Debug complex state machines in tokenization and parsing phases
      - Optimize rendering performance through dirty rectangle tracking and layer invalidation
      skills:
      - Parser Implementation
      - DOM Tree Manipulation
      - CSS Box Model
      - Layout Algorithms
      - Rendering Pipelines
      - Computer Graphics
      - Memory Management
      - Algorithm Optimization
      tags:
      - build-from-scratch
      - c++
      - css-engine
      - expert
      - game-dev
      - go
      - html-parser
      - layout
      - rendering
      - rust
      - web
      architecture_doc: architecture-docs/build-browser/index.md
      languages:
        recommended:
        - Rust
        - C++
        - Go
        also_possible:
        - Python
        - TypeScript
      resources:
      - type: book
        name: Web Browser Engineering
        url: https://browser.engineering/
      - type: tutorial
        name: Let's build a browser engine
        url: https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html
      prerequisites:
      - type: skill
        name: HTML/CSS parsing
      - type: skill
        name: Tree data structures
      - type: skill
        name: Graphics basics
      milestones:
      - id: build-browser-m1
        name: HTML Parser
        description: Parse HTML into a DOM tree.
        acceptance_criteria:
        - Tokenizer correctly identifies start tags, end tags, attributes, self-closing tags, and text content
        - DOM tree is built with correct parent-child relationships reflecting the HTML nesting structure
        - Nested elements produce the correct tree depth with proper parent and sibling node references
        - Self-closing and void tags (br, hr, img, input) are handled without expecting a matching end tag
        pitfalls:
        - Malformed HTML handling
        - Entity decoding
        - Case sensitivity
        concepts:
        - DOM
        - Tokenization
        - Tree construction
        skills:
        - String manipulation and parsing
        - Recursive data structure construction
        - Error recovery in parsers
        - Memory-safe tree operations
        deliverables:
        - Tokenizer that splits raw HTML into start tags, end tags, self-closing tags, and text content tokens
        - DOM tree construction algorithm that builds a tree of element and text nodes from the token stream
        - Handler for self-closing and void elements (br, img, input) that do not require closing tags
        - Basic error recovery for malformed HTML such as missing closing tags or improperly nested elements
        estimated_hours: 12-20
      - id: build-browser-m2
        name: CSS Parser
        description: Parse CSS into a stylesheet.
        acceptance_criteria:
        - Selector parser handles tag selectors, class selectors, ID selectors, and combinators correctly
        - Property parser extracts property names and values including shorthand properties and units
        - 'Specificity is correctly calculated as (inline, #ids, .classes+[attrs], tags) for any selector'
        - 'Cascade applies rules in the correct order: user-agent defaults, then author styles ordered by specificity'
        pitfalls:
        - Selector combinators
        - Shorthand properties
        - '!important'
        concepts:
        - CSS parsing
        - Specificity
        - Cascade
        skills:
        - Tokenization and lexical analysis
        - Rule precedence calculation
        - Data structure design for stylesheets
        - Selector matching algorithms
        deliverables:
        - CSS tokenizer that extracts selectors, property names, values, and delimiters from a stylesheet string
        - Selector parser supporting tag, class, ID, descendant, child, and sibling combinators
        - Specificity calculator that computes (inline, id, class, tag) weights for each selector
        - Style rule storage structure that maps selectors to their declared property-value pairs
        estimated_hours: 15-25
      - id: build-browser-m3
        name: Layout
        description: Calculate box positions and sizes.
        acceptance_criteria:
        - Box model correctly computes total element dimensions as content + padding + border + margin
        - Block layout stacks child elements vertically with correct margin collapsing between adjacent blocks
        - Inline layout flows text and inline elements horizontally, wrapping to a new line at the container width
        - Width and height calculations respect explicit CSS values and default to auto-sizing based on content
        pitfalls:
        - Auto margins
        - Percentage units
        - Collapsing margins
        concepts:
        - Box model
        - Block formatting
        - Layout algorithms
        skills:
        - Geometric computation and positioning
        - Recursive layout algorithms
        - Constraint solving for dimensions
        - Coordinate system transformations
        deliverables:
        - Box model calculation that computes margin, border, padding, and content dimensions for each element
        - Block layout algorithm that stacks child boxes vertically and computes their y-positions
        - Inline layout algorithm that places inline boxes horizontally with line breaking at container boundaries
        - Layout tree construction from the styled DOM that pairs each DOM node with its computed box geometry
        estimated_hours: 20-35
      - id: build-browser-m4
        name: Rendering
        description: Paint the layout tree to a canvas.
        acceptance_criteria:
        - Background colors are painted as filled rectangles at the correct position and size for each box
        - Borders are rendered with the correct width, color, and style on all four sides of the box
        - Text is rendered at the correct baseline position using the computed font, size, and color properties
        - Z-ordering paints elements in the correct stacking order so later elements overlap earlier ones
        pitfalls:
        - Subpixel rendering
        - Font fallbacks
        - Clipping
        concepts:
        - Rendering pipeline
        - Display lists
        - Graphics
        skills:
        - 2D graphics programming
        - Rasterization and pixel manipulation
        - Color space conversions
        - Performance optimization for rendering loops
        deliverables:
        - Paint list generator that traverses the layout tree and emits draw commands for each visible box
        - Rectangle and text drawing commands that specify coordinates, colors, fonts, and dimensions
        - Basic display list optimization that clips invisible elements and sorts commands by paint order
        - Render target that outputs the display list to a canvas, window surface, or image buffer
        estimated_hours: 33-70
    - id: multiplayer-game-server
      name: Multiplayer Game Server
      description: Real-time game state synchronization
      difficulty: advanced
      estimated_hours: '60'
      essence: Real-time state synchronization across unreliable network connections with authoritative server validation, client-side prediction for input responsiveness, and retroactive lag compensation to ensure fair gameplay under varying latency conditions.
      why_important: Game servers push real-time systems to their limits with strict latency requirements, making this excellent practice for performance-critical code.
      learning_outcomes:
      - Implement authoritative server architecture
      - Build client-side prediction and reconciliation
      - Handle lag compensation for fair gameplay
      - Design tick-based game loops
      skills:
      - Network Protocol Design
      - Client-Side Prediction
      - Server Reconciliation
      - Lag Compensation
      - Fixed-Timestep Game Loops
      - Delta Compression
      - State Synchronization
      - UDP Socket Programming
      tags:
      - advanced
      - game-dev
      - interpolation
      - lobbies
      - networking
      - performance
      - real-time
      - service
      - state-sync
      - tick-rate
      architecture_doc: architecture-docs/multiplayer-game-server/index.md
      languages:
        recommended:
        - Go
        - Rust
        - C++
        also_possible: []
      resources:
      - name: Fast-Paced Multiplayer Series
        url: https://www.gabrielgambetta.com/client-server-game-architecture.html
        type: tutorial
      - name: Valve Lag Compensation Guide
        url: https://developer.valvesoftware.com/wiki/Latency_Compensating_Methods_in_Client/Server_In-game_Protocol_Design_and_Optimization
        type: article
      - name: Gaffer On Games Networked Physics
        url: https://www.gafferongames.com/categories/networked-physics/
        type: tutorial
      - name: Source Multiplayer Networking
        url: https://developer.valvesoftware.com/wiki/Source_Multiplayer_Networking
        type: documentation
      - name: GDC Vault - I Shot You First (Halo Reach)
        url: https://www.gdcvault.com/play/1014345/I-Shot-You-First-Networking
        type: video
      prerequisites:
      - type: project
        id: websocket-server
        name: WebSocket Server
      - type: project
        id: chat-app
        name: Real-time Chat
      - type: skill
        name: Game loop architecture
      milestones:
      - id: multiplayer-game-server-m1
        name: Game Loop & Tick System
        description: Implement a fixed timestep game loop that processes input, updates state, and broadcasts at consistent intervals.
        acceptance_criteria:
        - Game loop runs at a fixed rate such as 60 ticks per second with consistent timing between ticks
        - All pending player inputs are processed and applied to the game state within each tick
        - Game state including positions, velocities, and health is updated deterministically each tick
        - Tick timing remains consistent under varying CPU load with compensation for processing delays
        pitfalls:
        - Variable delta time causes non-deterministic simulation
        - Sleeping exact tick duration ignores processing time
        - Unbounded accumulator causes spiral of death when behind
        concepts:
        - Fixed timestep vs variable delta time
        - Accumulator pattern for frame-independent physics
        - Tick rate vs frame rate decoupling
        - Catch-up mechanics and simulation spiral prevention
        skills:
        - Game loop architecture
        - Real-time system timing
        - Deterministic simulation
        - Performance profiling
        deliverables:
        - Fixed timestep game loop that advances game state at a consistent configurable rate independent of frame time
        - Game state update processor that applies all pending player inputs and advances physics each tick
        - Tick rate configuration that allows adjusting the server simulation rate between 20 and 128 ticks per second
        - State snapshot system that captures the full game state at each tick for history and replay purposes
        estimated_hours: '15'
      - id: multiplayer-game-server-m2
        name: Client Prediction & Reconciliation
        description: Implement client-side prediction for responsive controls with server reconciliation to correct mispredictions.
        acceptance_criteria:
        - Client-side prediction applies player inputs locally to provide responsive movement without network latency
        - Server authoritative state is compared against client predicted state on each server update received
        - State discrepancies are reconciled by replaying unacknowledged inputs on top of the corrected server state
        - Prediction errors produce smooth visual corrections instead of jarring teleportation or snapping
        pitfalls:
        - Not storing enough input history causes rubber-banding
        - Exact float comparison fails due to precision
        - Overcorrecting causes jittery movement
        concepts:
        - Client-side prediction with input buffering
        - Server reconciliation and state correction
        - Input sequence numbering
        - Authoritative server vs optimistic client
        skills:
        - Client-server synchronization
        - Network protocol design
        - State interpolation
        - Input handling systems
        deliverables:
        - Client-side prediction engine that immediately applies local player inputs without waiting for server confirmation
        - Server authoritative state manager that maintains the canonical game state that overrides client predictions
        - State reconciliation logic that corrects the client's predicted state when it diverges from the server state
        - Smoothing correction applicator that interpolates between incorrect and correct states to avoid visual snapping
        estimated_hours: '15'
      - id: multiplayer-game-server-m3
        name: Lag Compensation
        description: Implement server-side lag compensation to make hit detection fair for high-latency players.
        acceptance_criteria:
        - Server rewinds game state to the client's estimated render time for accurate retroactive hit detection
        - Client network latency is measured and accounted for when processing combat actions on the server
        - Entity interpolation displays remote players at smoothly interpolated positions between server updates
        - Fairness is maintained across clients with different latencies by capping maximum rewind window
        pitfalls:
        - Unlimited rewind allows shooting around corners
        - Memory grows unbounded without history pruning
        - Interpolation fails at entity spawn/despawn boundaries
        concepts:
        - Lag compensation through state rewinding
        - Ray casting with historical hitboxes
        - Circular buffer for entity history
        - Maximum rewind time limits
        skills:
        - Hit detection algorithms
        - Time-based state management
        - Anti-cheat design
        - Memory-efficient data structures
        deliverables:
        - Server-side state rewind buffer that stores recent game state history for retroactive hit detection
        - Lag-compensated hit detection that validates shots against the game state at the time the client fired
        - Time synchronization protocol that estimates the clock offset between each client and the server
        - Entity interpolation renderer that smoothly displays remote entities between received state updates
        estimated_hours: '15'
      - id: multiplayer-game-server-m4
        name: State Synchronization
        description: Efficiently synchronize game state to clients using delta compression, interest management, and prioritization.
        acceptance_criteria:
        - Game state is serialized efficiently using a binary protocol with minimal overhead per field
        - Delta updates transmit only changed values reducing bandwidth by at least 50% compared to full snapshots
        - Full state snapshots are sent on client reconnection to synchronize the client to the current game state
        - Client-side interpolation smoothly renders entity positions between received server state updates
        pitfalls:
        - Sending full state every tick overwhelms bandwidth
        - Delta without baseline causes desync
        - Priority calculation per entity per client is O(n*m)
        concepts:
        - Delta compression and baseline snapshots
        - Area of interest management
        - Priority-based update scheduling
        - Bandwidth optimization techniques
        - Entity relevance scoring
        skills:
        - Network bandwidth optimization
        - Binary protocol design
        - Spatial partitioning
        - Update prioritization algorithms
        deliverables:
        - Delta compression encoder that transmits only changed fields since the last acknowledged client state
        - Priority-based update scheduler that sends nearby or important entities more frequently than distant ones
        - Reliable and unreliable channel selector that uses TCP for critical events and UDP for state updates
        - Bandwidth optimization module that limits update size and frequency to stay within configurable bitrate caps
        estimated_hours: '15'
    - id: build-vpn
      name: Build Your Own VPN
      description: VPN with TUN/TAP, encryption, key exchange
      difficulty: expert
      estimated_hours: 50-80
      essence: TUN/TAP virtual interface manipulation via ioctl system calls, UDP packet encapsulation with custom framing protocol, AES-GCM authenticated encryption with nonce management, and Diffie-Hellman ephemeral key exchange to establish encrypted point-to-point tunnels while handling routing table configuration and NAT traversal.
      why_important: Building a VPN from scratch teaches you the full networking stack from layer 2/3 interfaces through cryptographic protocols to application-layer tunneling, skills directly applicable to infrastructure security, network engineering, and understanding production VPN implementations like WireGuard and OpenVPN.
      learning_outcomes:
      - Implement TUN/TAP virtual network devices using ioctl system calls for packet capture
      - Design UDP-based transport protocol with packet framing and sequencing
      - Implement AES-GCM authenticated encryption with proper nonce management
      - Build Diffie-Hellman key exchange protocol with perfect forward secrecy
      - Configure Linux routing tables and iptables rules for traffic forwarding
      - Debug encrypted network traffic using packet analysis tools
      - Handle NAT traversal and connection state management across unreliable networks
      skills:
      - Virtual Network Interfaces
      - UDP Socket Programming
      - Symmetric Cryptography
      - Key Exchange Protocols
      - Network Routing
      - Packet Encapsulation
      - NAT Traversal
      tags:
      - build-from-scratch
      - c
      - encryption
      - expert
      - go
      - networking
      - routing
      - rust
      - tunneling
      architecture_doc: architecture-docs/build-vpn/index.md
      languages:
        recommended:
        - Go
        - Rust
        - C
        also_possible:
        - Python
      resources:
      - name: TUN/TAP Interface Tutorial
        url: https://www.kernel.org/doc/Documentation/networking/tuntap.txt
        type: documentation
      - name: WireGuard Whitepaper
        url: https://www.wireguard.com/papers/wireguard.pdf
        type: paper
      prerequisites:
      - type: skill
        name: Network programming (sockets, TCP/UDP)
      - type: skill
        name: Cryptography basics (AES, RSA)
      - type: skill
        name: Linux networking (iptables, routing)
      - type: skill
        name: TUN/TAP virtual interfaces
      milestones:
      - id: build-vpn-m1
        name: TUN/TAP Interface
        description: Create and configure a TUN device for IP packet capture.
        acceptance_criteria:
        - TUN device is created and visible via ip link show command
        - Raw IP packets are read from TUN file descriptor without protocol information header
        - Writing IP packets to TUN interface delivers them to local network stack
        - Interface IP address and MTU are configured and verified with ping to TUN address
        - TUN device persists while file descriptor is open and is cleaned up on close
        pitfalls:
        - Forgetting IFF_NO_PI causes 4-byte header
        - Must run as root
        - Interface disappears when fd closed
        concepts:
        - Virtual network interfaces
        - TUN vs TAP
        - IP packet structure
        skills:
        - Low-level network programming
        - System call interfaces (ioctl, open, read, write)
        - Working with file descriptors
        - Linux networking fundamentals
        deliverables:
        - TUN device creation opening /dev/net/tun with proper ioctl flags
        - Packet reading from TUN interface capturing outbound IP packets
        - Packet writing to TUN interface injecting inbound IP packets
        - Interface configuration setting IP address, netmask, and MTU via ioctl
        estimated_hours: 6-10
      - id: build-vpn-m2
        name: UDP Transport Layer
        description: Create UDP socket for tunneling encrypted packets between VPN endpoints.
        acceptance_criteria:
        - UDP server listens on configured port and receives packets from remote peers
        - Packets read from TUN are encapsulated in UDP and sent to remote VPN endpoint
        - Packets received from UDP are extracted and written to local TUN interface
        - Server mode handles multiple client connections tracking each by source address
        - Select or poll multiplexes TUN file descriptor and UDP socket for concurrent I/O
        pitfalls:
        - MTU issues (encryption overhead)
        - NAT traversal
        - Packet fragmentation
        concepts:
        - UDP tunneling
        - Multiplexing I/O
        - Client-server architecture
        skills:
        - UDP socket programming
        - Asynchronous I/O and event loops
        - Network protocol design
        - Handling concurrent connections
        deliverables:
        - UDP socket for peer-to-peer tunnel communication on configurable port
        - Packet encapsulation wrapping TUN packets inside UDP datagrams for transport
        - Peer address management tracking remote endpoint addresses for routing
        - NAT traversal basics enabling connections through network address translation
        estimated_hours: 6-8
      - id: build-vpn-m3
        name: Encryption Layer
        description: Add encryption to tunnel traffic using symmetric encryption (AES-GCM).
        acceptance_criteria:
        - Packets are encrypted with AES-256-GCM before being sent over UDP transport
        - Decryption verifies authentication tag and rejects tampered or corrupted packets
        - Each packet uses unique nonce and nonce reuse with same key never occurs
        - Anti-replay window rejects packets with previously seen nonce counter values
        - Encrypted tunnel passes data correctly verified by ping through VPN tunnel
        pitfalls:
        - Nonce reuse is catastrophic
        - Forgetting auth tag verification
        - Key management
        concepts:
        - Authenticated encryption
        - Nonces and IVs
        - Anti-replay protection
        skills:
        - Cryptographic library usage
        - Secure random number generation
        - Memory-safe handling of sensitive data
        - Understanding cipher modes and parameters
        deliverables:
        - Key derivation from shared secret producing AES-256 encryption key material
        - Packet encryption using AES-GCM authenticated encryption before UDP transmission
        - Packet decryption and authentication tag verification after UDP reception
        - Nonce and initialization vector management ensuring unique nonce per encrypted packet
        estimated_hours: 8-12
      - id: build-vpn-m4
        name: Key Exchange
        description: Implement secure key exchange using Diffie-Hellman or similar protocol.
        acceptance_criteria:
        - Ephemeral key pairs are generated fresh for each new VPN session connection
        - Public keys are exchanged over UDP and both sides derive identical shared secret
        - HKDF derives separate encryption keys for each traffic direction from shared secret
        - Session keys provide perfect forward secrecy so past traffic remains secure if keys leak
        - Key exchange completes within reasonable time and tunnel becomes usable afterward
        pitfalls:
        - Not verifying peer identity (MITM)
        - Reusing static keys
        - Weak random number generation
        concepts:
        - Diffie-Hellman key exchange
        - Perfect forward secrecy
        - Key derivation functions
        skills:
        - Public key cryptography implementation
        - Secure protocol handshake design
        - Key derivation and management
        - Authentication and identity verification
        deliverables:
        - Diffie-Hellman or ECDH key exchange deriving shared secret from ephemeral keys
        - Peer authentication verifying remote endpoint identity during handshake
        - Session key establishment deriving encryption keys using HKDF from shared secret
        - Key rotation periodically renegotiating session keys for forward secrecy
        estimated_hours: 8-10
      - id: build-vpn-m5
        name: Routing and NAT
        description: Configure routing tables and NAT for full VPN functionality.
        acceptance_criteria:
        - All client traffic is routed through VPN tunnel after route table configuration
        - Route to VPN server IP is preserved via original gateway preventing routing loop
        - Server NAT masquerade allows VPN clients to access internet through server external interface
        - Split tunneling routes only configured destination subnets through VPN leaving rest direct
        - Original routing table is restored cleanly when VPN connection disconnects
        pitfalls:
        - Locking yourself out (SSH)
        - DNS leaks
        - IPv6 leaks
        - Forgetting to restore routes
        concepts:
        - IP routing
        - NAT/masquerading
        - iptables
        - Split tunneling
        skills:
        - Linux routing table manipulation
        - iptables and firewall configuration
        - Network address translation (NAT)
        - Debugging network connectivity issues
        deliverables:
        - Route table manipulation configuring default gateway through VPN tunnel interface
        - Default gateway setup redirecting all traffic through VPN while keeping VPN server reachable
        - NAT masquerading on server translating VPN client addresses to server external address
        - Split tunneling configuration routing only specified subnets through VPN tunnel
        estimated_hours: 10-15
- id: software-engineering
  name: Software Engineering Practices
  icon: ‚úÖ
  subdomains:
  - name: Testing & Quality
  - name: CI/CD
  - name: Observability
  - name: Engineering Practices
  projects:
    beginner:
    - id: unit-testing-basics
      name: Unit Testing Fundamentals
      description: pytest/jest basics
      difficulty: beginner
      estimated_hours: 6-10
      essence: Isolated verification of individual functions and methods through programmatic assertions that validate expected behavior against actual output, combined with test doubles (mocks, stubs, fakes) that replace external dependencies to ensure deterministic, repeatable validation independent of databases, networks, or filesystem state.
      why_important: Unit testing is foundational to professional software development‚Äîit catches bugs early, enables confident refactoring, and is expected in virtually every modern development workflow from startups to enterprise teams.
      learning_outcomes:
      - Write isolated test cases using assertion frameworks like pytest, Jest, and JUnit
      - Design test fixtures and setup/teardown patterns to manage test state and dependencies
      - Implement mocking and stubbing to isolate units from external systems and APIs
      - Structure test suites with clear naming conventions and logical organization
      - Apply the Arrange-Act-Assert pattern for readable and maintainable test code
      - Debug failing tests by analyzing assertion messages and stack traces
      - Measure test coverage to identify untested code paths
      - Practice test-driven development by writing tests before implementation code
      skills:
      - Test Automation
      - Assertion Libraries
      - Test Fixtures
      - Mocking and Stubbing
      - Code Coverage Analysis
      - Test Organization
      - Debugging Test Failures
      - Test-Driven Development
      tags:
      - assertions
      - beginner-friendly
      - fixtures
      - java
      - javascript
      - mocking
      - python
      - test-runner
      architecture_doc: architecture-docs/unit-testing-basics/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Java
        also_possible:
        - Go
        - Rust
      resources:
      - name: pytest Documentation
        url: https://docs.pytest.org/
        type: documentation
      - name: Jest Documentation
        url: https://jestjs.io/docs/getting-started
        type: documentation
      prerequisites:
      - type: skill
        name: Basic programming
      - type: skill
        name: Functions and classes
      milestones:
      - id: unit-testing-basics-m1
        name: First Tests
        description: Write your first unit tests.
        acceptance_criteria:
        - Set up test framework with correct project configuration and imports
        - Write test for a pure function verifying expected return values
        - Use assertions assertEqual and assertTrue to verify function behavior
        - Run tests and see results showing pass count and failure details
        - Test edge cases including empty input, boundary values, and null arguments
        pitfalls:
        - Testing implementation not behavior
        - Not testing edge cases
        - Fragile assertions
        concepts:
        - Unit testing
        - Assertions
        - Test structure
        skills:
        - Writing basic test functions
        - Using assertion methods
        - Organizing tests in test files
        - Running test suites
        - Testing edge cases and boundaries
        deliverables:
        - Test function creation using the framework's test annotation or naming convention
        - Assertion usage demonstrating assertEqual, assertTrue, and assertRaises patterns
        - Test execution runner discovering and running all test functions in the suite
        - Pass and fail reporting output showing test names, results, and error details
        estimated_hours: 2-3
      - id: unit-testing-basics-m2
        name: Test Organization
        description: Organize tests with fixtures and setup.
        acceptance_criteria:
        - Group related tests in classes or modules for logical organization
        - Use setup and teardown to initialize and clean up shared test fixtures
        - Share fixtures between tests using class-level setup or fixture factories
        - Parameterized tests run the same assertion logic across multiple input values
        pitfalls:
        - Fixtures with side effects
        - Shared mutable state
        - Over-complicated fixtures
        concepts:
        - Fixtures
        - Parameterization
        - Test isolation
        skills:
        - Creating test fixtures
        - Using setup and teardown methods
        - Parameterizing test cases
        - Managing test data
        - Isolating test state
        deliverables:
        - Test file structure organized by module with clear naming conventions
        - Test suites and classes grouping related test cases logically
        - Setup and teardown methods running before and after each test or suite
        - Test naming convention using descriptive names that document expected behavior
        estimated_hours: 2-3
      - id: unit-testing-basics-m3
        name: Mocking and Isolation
        description: Test code with external dependencies.
        acceptance_criteria:
        - Mock external API calls to return controlled responses in tests
        - Mock file system operations to avoid real disk I/O during tests
        - Verify mock was called with expected arguments and expected call count
        - Patch at the correct import level so the mock replaces the right reference
        pitfalls:
        - Mocking too much
        - Wrong patch location
        - Not verifying interactions
        concepts:
        - Mocking
        - Dependency injection
        - Test isolation
        skills:
        - Creating mock objects
        - Patching external dependencies
        - Using dependency injection
        - Verifying mock interactions
        - Testing with stubs and fakes
        deliverables:
        - Mock objects replacing real dependencies with controllable test doubles
        - Dependency injection pattern allowing tests to substitute fake implementations
        - Stub functions returning predefined values for controlled test scenarios
        - Spy objects recording call arguments and counts for interaction verification
        estimated_hours: 2-3
    intermediate:
    - id: logging-structured
      name: Structured Logging
      description: JSON logs, aggregation
      difficulty: intermediate
      estimated_hours: 10-15
      essence: Log level-based event filtering, key-value serialization to JSON, and context/correlation identifier threading through execution paths for machine-parseable observability across distributed system boundaries.
      why_important: Production systems generate millions of log entries daily‚Äîstructured logging enables automated parsing, aggregation, and correlation across microservices, making debugging and monitoring scalable where grep and text parsing fail.
      learning_outcomes:
      - Implement log levels (DEBUG, INFO, WARN, ERROR) with appropriate severity filtering
      - Design JSON schema for structured log entries with timestamp, level, message, and contextual fields
      - Build context propagation mechanisms using thread-local storage or context variables
      - Implement correlation ID generation and propagation across service boundaries via HTTP headers
      - Integrate structured logging libraries (structlog, slog, SLF4J/Logback) with production applications
      - Configure log output destinations and rotation policies for production environments
      - Design logging strategies that avoid performance bottlenecks through lazy evaluation and sampling
      - Debug distributed system failures by tracing requests across multiple services using correlation IDs
      skills:
      - Structured logging
      - JSON serialization
      - Context propagation
      - Correlation ID tracing
      - Log aggregation patterns
      - MDC (Mapped Diagnostic Context)
      - Log level management
      - Production observability
      tags:
      - aggregation
      - go
      - intermediate
      - java
      - json
      - levels
      - python
      architecture_doc: architecture-docs/logging-structured/index.md
      languages:
        recommended:
        - Python
        - Go
        - Java
        also_possible:
        - JavaScript
        - Rust
      resources:
      - name: 12 Factor App - Logs
        url: https://12factor.net/logs
        type: article
      - name: Structured Logging Guide
        url: https://www.honeycomb.io/blog/structured-logging
        type: article
      prerequisites:
      - type: skill
        name: JSON
      - type: skill
        name: File I/O
      - type: skill
        name: Basic debugging concepts
      milestones:
      - id: logging-structured-m1
        name: Logger Core
        description: Build the core logging infrastructure.
        acceptance_criteria:
        - Log levels DEBUG, INFO, WARN, ERROR, and FATAL filter messages below the configured minimum level
        - Minimum log level is configurable per logger and can be changed at runtime without restart
        - Logging operations are thread-safe and do not corrupt output when called from multiple threads simultaneously
        - Log records are dispatched to multiple output destinations such as stdout, file, and remote collector
        pitfalls:
        - Race conditions
        - Blocking I/O in hot path
        - Missing context
        concepts:
        - Log levels
        - Thread safety
        - Handler pattern
        skills:
        - Concurrent programming
        - Implementing thread-safe data structures
        - Building extensible handler architectures
        - Configuring log levels and filtering
        deliverables:
        - Log level manager supporting DEBUG, INFO, WARN, ERROR, and FATAL with numeric ordering
        - Logger hierarchy system that creates child loggers inheriting parent configuration and context
        - Log record factory that constructs structured records with timestamp, level, message, and metadata
        - Handler dispatch mechanism that routes log records to one or more configured output destinations
        estimated_hours: 3-4
      - id: logging-structured-m2
        name: Structured Output
        description: Format logs as structured JSON.
        acceptance_criteria:
        - JSON output format produces valid single-line JSON with consistent field names for each log record
        - Each log record includes timestamp, level, message, logger name, and any attached context fields
        - Custom formatters can be registered and selected by name to control log output appearance
        - Pretty-print mode formats JSON with indentation and colors for readable developer console output
        pitfalls:
        - Non-serializable values
        - Missing flush
        - Large context objects
        concepts:
        - Structured logging
        - JSON format
        - Formatters
        skills:
        - JSON serialization and encoding
        - Custom formatter implementation
        - Stream-based output handling
        - Sanitizing sensitive data from logs
        deliverables:
        - JSON formatter that serializes log records as single-line JSON objects with consistent field ordering
        - Key-value pair enricher that attaches arbitrary context fields to each log record
        - Timestamp formatter supporting ISO 8601, Unix epoch, and custom strftime patterns
        - Custom formatter plugin system that allows users to register their own output format functions
        estimated_hours: 2-3
      - id: logging-structured-m3
        name: Context & Correlation
        description: Add request context and correlation IDs.
        acceptance_criteria:
        - A unique request ID is automatically injected into every log record within the same request scope
        - Context key-value pairs propagate through nested function calls without explicit parameter passing
        - Child loggers inherit all context fields from their parent and can add their own additional fields
        - Logging context is preserved correctly across async/await boundaries and coroutine switches
        pitfalls:
        - Context not propagating
        - Memory leaks
        - Async context issues
        concepts:
        - Correlation IDs
        - Context propagation
        - Request tracing
        skills:
        - Thread-local storage patterns
        - Middleware and decorator patterns
        - Distributed tracing fundamentals
        - Managing request lifecycle context
        deliverables:
        - Thread-local context storage that maintains key-value pairs scoped to the current execution context
        - Correlation ID propagator that injects and carries a unique request ID across function boundaries
        - Request context middleware that extracts request metadata and attaches it to the logging context
        - Async context bridge that preserves logging context when crossing async/await task boundaries
        estimated_hours: 3-4
    - id: ci-cd-pipeline
      name: CI/CD Pipeline Builder
      description: Automated build, test, deploy workflows
      difficulty: intermediate
      estimated_hours: '35'
      essence: Directed acyclic graph (DAG) execution of isolated build/test/deploy jobs with dependency resolution, artifact lifecycle tracking across stages, and traffic-shifting deployment patterns with automated health-based rollback.
      why_important: CI/CD is the backbone of modern software delivery. Understanding pipeline internals makes you better at debugging and optimizing delivery workflows.
      learning_outcomes:
      - Design multi-stage pipeline workflows
      - Implement build and test automation
      - Handle artifact versioning and storage
      - Automate deployment with rollback support
      skills:
      - YAML Pipeline DSL
      - Process Isolation
      - Artifact Versioning
      - Deployment Automation
      - Health Check Monitoring
      - Rollback Strategies
      - Concurrent Execution
      - Container Orchestration
      tags:
      - artifacts
      - automation
      - devops
      - infrastructure
      - intermediate
      - runners
      - stages
      - streaming
      - testing
      - triggers
      architecture_doc: architecture-docs/ci-cd-pipeline/index.md
      languages:
        recommended:
        - Python
        - Go
        - Shell
        also_possible: []
      resources:
      - name: GitLab CI/CD Pipeline Documentation
        url: https://docs.gitlab.com/ci/pipelines/
        type: documentation
      - name: Docker Container Isolation Best Practices
        url: https://snyk.io/blog/best-practices-for-container-isolation/
        type: article
      - name: Blue-Green and Canary Deployment Strategies
        url: https://www.harness.io/blog/blue-green-canary-deployment-strategies
        type: tutorial
      - name: Azure Pipelines YAML Schema Reference
        url: https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/
        type: documentation
      - name: Docker in CI/CD Pipelines Guide
        url: https://moss.sh/deployment/docker-in-ci-cd-pipelines-guide/
        type: tutorial
      prerequisites:
      - type: skill
        name: Shell scripting
      - type: skill
        name: Docker basics
      - type: skill
        name: Git hooks/webhooks
      milestones:
      - id: ci-cd-pipeline-m1
        name: Pipeline Definition Parser
        description: Parse YAML pipeline definitions with stages, jobs, steps, and dependencies between them.
        acceptance_criteria:
        - Parser reads YAML pipeline file and produces structured stage/job/step objects
        - Validation rejects malformed definitions with descriptive error messages
        - Dependency graph enables topological sort for correct parallel execution ordering
        - Conditional execution expressions (if/when) are parsed and evaluated per job context
        pitfalls:
        - Circular dependencies cause infinite loop
        - Missing dependency reference crashes at runtime
        - YAML anchors and aliases need special handling
        concepts:
        - YAML parsing and validation with schema enforcement
        - Directed acyclic graph (DAG) representation and traversal
        - Topological sorting for dependency resolution
        - Reference resolution and variable interpolation
        skills:
        - YAML Processing
        - Graph Algorithms
        - Dependency Management
        - Configuration Validation
        deliverables:
        - YAML pipeline definition parser extracting stages, jobs, and steps
        - Stage and job definition validation checking required fields and types
        - Dependency graph construction building DAG from job dependency declarations
        - Variable substitution resolving environment and pipeline variables in step definitions
        estimated_hours: '9'
      - id: ci-cd-pipeline-m2
        name: Job Executor
        description: Execute jobs in isolated environments with proper logging, timeout handling, and exit code propagation.
        acceptance_criteria:
        - Each job step executes in isolated Docker container with specified image
        - Step logs are streamed in real-time to pipeline output for monitoring progress
        - Step timeout terminates execution and marks step as failed after configured duration
        - Failed steps trigger configurable retry with backoff before marking job as failed
        pitfalls:
        - Zombie processes when parent killed without cleanup
        - Environment variable injection security risk
        - Large output causes memory exhaustion
        concepts:
        - Process isolation using containers or sandboxing
        - Resource limits and cgroup management
        - Standard stream handling and buffering
        - Signal propagation and graceful shutdown
        skills:
        - Process Management
        - Concurrent Execution
        - System Resource Control
        - Logging Infrastructure
        deliverables:
        - Docker container execution running each job step in isolated container environment
        - Script step runner executing shell commands with configurable working directory
        - Environment variable setup injecting pipeline and job-level variables into execution context
        - Output capture collecting stdout and stderr streams from each executed step
        estimated_hours: '9'
      - id: ci-cd-pipeline-m3
        name: Artifact Management
        description: Implement artifact upload, download, and caching between pipeline stages with versioning.
        acceptance_criteria:
        - Artifacts uploaded from one job are downloadable by subsequent dependent jobs
        - Artifact download verifies checksum to detect corruption during transfer or storage
        - Retention policy deletes artifacts older than configured age to reclaim storage space
        - Large artifacts are handled efficiently without exhausting memory during transfer
        pitfalls:
        - Symlinks in tarball can escape extraction directory
        - Cache key collision overwrites unrelated cache
        - Large artifacts exhaust memory without streaming
        concepts:
        - Content-addressable storage and hash-based deduplication
        - Streaming I/O for large file transfers
        - Path traversal vulnerability mitigation
        - Cache invalidation strategies
        skills:
        - File System Operations
        - Stream Processing
        - Security Hardening
        - Data Compression
        deliverables:
        - Artifact upload after job completion storing output files in versioned storage
        - Artifact download between jobs retrieving stored artifacts for dependent steps
        - Artifact storage backend persisting files with checksums for integrity verification
        - Artifact retention policy automatically cleaning expired artifacts after configured period
        estimated_hours: '9'
      - id: ci-cd-pipeline-m4
        name: Deployment Strategies
        description: Implement blue-green and canary deployment strategies with health checks and automatic rollback.
        acceptance_criteria:
        - Rolling deployment updates one instance at a time verifying health before proceeding
        - Blue-green deployment switches traffic atomically after new environment passes health checks
        - Canary deployment incrementally shifts traffic and monitors error rates at each stage
        - Manual approval gates pause deployment pipeline until authorized user approves continuation
        pitfalls:
        - Database migrations incompatible between versions
        - Session affinity breaks during traffic switch
        - Metrics lag causes delayed rollback decision
        concepts:
        - Load balancer configuration and traffic routing
        - Rolling update patterns and version management
        - Health check protocols and retry logic
        - State reconciliation during deployment transitions
        skills:
        - Deployment Orchestration
        - Traffic Management
        - Monitoring and Observability
        - Automated Testing
        deliverables:
        - Rolling deployment updating instances incrementally with health check validation
        - Blue-green deployment switching traffic between two identical environment versions
        - Canary deployment gradually shifting traffic percentage to new version with monitoring
        - Rollback automation reverting to previous version when deployment health checks fail
        estimated_hours: '9'
    advanced:
    - id: integration-testing
      name: Integration Testing Suite
      description: Test containers, mocking
      difficulty: advanced
      estimated_hours: 12-20
      essence: Coordinating multi-component interactions across network boundaries, data persistence layers, and external service contracts while maintaining test isolation and deterministic behavior despite asynchronous operations and stateful dependencies.
      why_important: Integration testing skills are critical for building reliable production systems, as most failures occur at component boundaries rather than within units. Mastering test isolation, database setup, and service mocking enables you to catch regression bugs early and deploy with confidence.
      learning_outcomes:
      - Configure isolated test databases with schema migrations and fixture management
      - Implement end-to-end API tests validating request/response contracts with real HTTP clients
      - Design test doubles and mocking strategies for external service dependencies
      - Build test harnesses that manage setup, teardown, and cleanup of stateful resources
      - Debug failures in asynchronous workflows involving multiple interacting components
      - Implement test data builders and factories for complex domain objects
      - Apply test isolation techniques to prevent test interference and flaky failures
      - Validate cross-cutting concerns like authentication, authorization, and error handling across layers
      skills:
      - Test Isolation
      - Database Testing
      - HTTP Client Testing
      - Service Mocking
      - Test Fixtures
      - Asynchronous Testing
      - Test Data Management
      - Contract Validation
      tags:
      - advanced
      - assertions
      - cleanup
      - fixtures
      - java
      - javascript
      - python
      - testing
      architecture_doc: architecture-docs/integration-testing/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Java
        also_possible:
        - Go
        - Ruby
      resources:
      - name: Testcontainers
        url: https://www.testcontainers.org/
        type: documentation
      - name: Martin Fowler - Integration Testing
        url: https://martinfowler.com/bliki/IntegrationTest.html
        type: article
      - name: Testcontainers Getting Started Guides
        url: https://testcontainers.com/guides/
        type: tutorial
      prerequisites:
      - type: skill
        name: Unit testing
      - type: skill
        name: Docker basics
      - type: skill
        name: Database knowledge
      milestones:
      - id: integration-testing-m1
        name: Test Database Setup
        description: Set up isolated database for testing.
        acceptance_criteria:
        - Spin up test database in Docker container with configured credentials and port
        - Migrations run before test suite executes to create required schema tables
        - Clean state between tests using transaction rollback or table truncation
        - Tear down database container after test suite completes to free resources
        pitfalls:
        - Port conflicts
        - Container startup time
        - Data leaking between tests
        concepts:
        - Test isolation
        - Containers in testing
        - Database fixtures
        skills:
        - Docker container management
        - Database schema migrations
        - Test data isolation strategies
        - Fixture and factory patterns
        - Container orchestration for tests
        deliverables:
        - Database container setup using Docker for isolated test environment
        - Schema migrations running before test suite to prepare database structure
        - Test data seeding inserting fixture records for deterministic test scenarios
        - Database cleanup resetting state between tests to prevent data leaks
        estimated_hours: 3-4
      - id: integration-testing-m2
        name: API Integration Tests
        description: Test API endpoints with real HTTP requests.
        acceptance_criteria:
        - Start test server with test configuration and database connection
        - Make real HTTP requests to running server and capture full responses
        - Verify response status codes and body content match expected values
        - Test complete authentication flow including registration, login, and protected access
        pitfalls:
        - Test order dependencies
        - Shared state
        - Slow tests
        concepts:
        - API testing
        - Test clients
        - Authentication in tests
        skills:
        - HTTP client configuration
        - JWT token generation for tests
        - Request/response validation
        - Test data cleanup strategies
        - Async test execution
        deliverables:
        - HTTP client for tests making real requests to running test server
        - Request and response assertions verifying status codes and response bodies
        - Authentication in tests with fixture users and valid session tokens
        - Error case testing verifying proper error responses for invalid requests
        estimated_hours: 4-5
      - id: integration-testing-m3
        name: External Service Mocking
        description: Mock external APIs while testing real internal integration.
        acceptance_criteria:
        - Mock HTTP responses for external APIs returning predefined response data
        - Verify external API was called with correct URL, method, headers, and body
        - Simulate error responses from external services to test error handling paths
        - Test retry logic by configuring mock to fail initially then succeed on retry
        pitfalls:
        - Missing mock causes real API call
        - Mock doesn't match real API
        - Order of mock setup
        concepts:
        - HTTP mocking
        - Service virtualization
        - Error simulation
        skills:
        - HTTP request interception
        - Mock server configuration
        - Network stub libraries
        - Contract testing patterns
        - Error response simulation
        deliverables:
        - Mock server setup intercepting outbound HTTP calls to external APIs
        - Response stubbing returning predefined responses for matched API requests
        - Request verification confirming external API was called with correct parameters
        - Network isolation ensuring no real external API calls escape during tests
        estimated_hours: 3-4
      - id: integration-testing-m4
        name: Container-Based Test Infrastructure
        description: Use Testcontainers to spin up real databases, message brokers, and services as Docker containers for tests.
        acceptance_criteria:
        - Tests use Testcontainers for PostgreSQL, Redis, and at least one message broker
        - Containers are automatically provisioned and torn down per test suite
        - Test data is isolated between test runs with automatic cleanup
        - CI pipeline runs containerized tests without manual Docker setup
        pitfalls:
        - Slow container startup
        - Port conflicts in CI
        - Docker socket permissions
        concepts:
        - Testcontainers
        - Docker-in-Docker
        - test isolation
        - CI/CD integration
        skills:
        - Testcontainers
        - Docker
        - CI integration
        deliverables:
        - Testcontainers-based integration test suite with CI support
        - Docker Compose configuration for local test environment with multiple service dependencies
        - Test fixture library with reusable container configurations and data seeding utilities
        - Performance benchmark suite measuring container startup time and test execution overhead
        estimated_hours: '4'
      - id: integration-testing-m5
        name: Contract Testing & End-to-End Flows
        description: Implement consumer-driven contract tests and end-to-end test flows spanning multiple services.
        acceptance_criteria:
        - Contract tests verify API compatibility between producer and consumer services
        - End-to-end tests cover full user flows (signup ‚Üí action ‚Üí verification)
        - Test report generation with coverage and failure analysis
        - Flaky test detection and automatic retry with exponential backoff
        pitfalls:
        - Brittle E2E tests
        - Contract versioning issues
        - Test environment drift
        concepts:
        - contract testing
        - consumer-driven contracts
        - E2E testing
        - flaky test detection
        skills:
        - Contract testing
        - E2E testing
        - Test reporting
        deliverables:
        - Contract testing framework and E2E test suite with reporting
        - Pact consumer contracts with provider verification for service boundaries
        - Test stability dashboard tracking flaky test occurrences and success rates over time
        - End-to-end test scenarios covering critical user flows with retry and timeout handling
        estimated_hours: '5'
    - id: cd-deployment
      name: CD with Blue-Green Deployment
      description: Zero-downtime deploys
      difficulty: advanced
      estimated_hours: 20-35
      essence: Atomic traffic cutover between duplicate production environments through load balancer state manipulation, synchronized with backward-compatible database schema evolution using expand-contract patterns to enable instant rollback without data loss.
      why_important: Blue-green deployment is a fundamental DevOps pattern used by major companies to ship updates without user-facing downtime, teaching you production-grade deployment orchestration, traffic management, and state synchronization strategies essential for infrastructure and platform engineering roles.
      learning_outcomes:
      - Implement dual environment infrastructure with identical configurations for atomic switching
      - Design load balancer traffic routing with weighted distribution and health check integration
      - Build automated deployment pipelines with pre-flight validation and smoke testing
      - Orchestrate backward-compatible database migrations using dual-write strategies
      - Develop rollback mechanisms with automated failure detection and traffic reversion
      - Create monitoring and alerting systems to detect deployment anomalies in real-time
      - Implement session draining and connection management for stateful applications
      - Design database synchronization strategies to maintain consistency across environments
      skills:
      - Infrastructure as Code
      - Load Balancer Configuration
      - Database Migration Patterns
      - Deployment Automation
      - Traffic Management
      - State Synchronization
      - Health Check Design
      - Rollback Strategies
      tags:
      - advanced
      - bash
      - canary
      - go
      - python
      - releases
      - rollback
      architecture_doc: architecture-docs/cd-deployment/index.md
      languages:
        recommended:
        - Bash
        - Python
        - Go
        also_possible:
        - JavaScript
        - Ruby
      resources:
      - type: article
        name: Blue-Green Deployments on AWS
        url: https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/welcome.html
      - type: documentation
        name: Kubernetes Rolling Updates
        url: https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/
      - type: article
        name: Martin Fowler on Blue-Green
        url: https://martinfowler.com/bliki/BlueGreenDeployment.html
      prerequisites:
      - type: skill
        name: CI pipeline basics
      - type: skill
        name: Docker/containers
      - type: skill
        name: Load balancer concepts
      - type: skill
        name: Shell scripting
      milestones:
      - id: cd-deployment-m1
        name: Dual Environment Setup
        description: Set up blue and green environments that can run simultaneously.
        acceptance_criteria:
        - Two identical environments run simultaneously on separate ports or addresses
        - Health check endpoint returns environment color, version, and uptime information
        - Deploying to one environment does not affect the other currently serving traffic
        - Environment variables configure database, port, and feature flags per environment
        pitfalls:
        - Forgetting database migrations
        - Environment config drift
        - Not testing both environments equally
        concepts:
        - Environment isolation
        - Infrastructure as code
        - Health checks
        skills:
        - Infrastructure as Code
        - Container Orchestration
        - Service Health Monitoring
        - Environment Configuration Management
        deliverables:
        - Blue and green environment provisioning with identical configurations
        - Environment health check endpoints reporting readiness and version info
        - Environment isolation ensuring independent deployment of each environment
        - Configuration management injecting environment-specific settings via variables
        estimated_hours: 4-6
      - id: cd-deployment-m2
        name: Load Balancer Switching
        description: Implement traffic switching between blue and green environments.
        acceptance_criteria:
        - Nginx or HAProxy routes all traffic to the currently active environment
        - Traffic switch uses graceful reload so no in-flight connections are dropped
        - Health check confirms target environment is healthy before switching traffic to it
        - Rollback switches traffic back to previous environment within seconds
        pitfalls:
        - Using restart instead of reload
        - Not testing config before applying
        - Forgetting to drain connections
        concepts:
        - Reverse proxy
        - Graceful reload
        - Connection draining
        skills:
        - Load Balancer Configuration
        - Zero-Downtime Deployments
        - Traffic Management
        - Reverse Proxy Setup
        deliverables:
        - Load balancer configuration routing traffic to active environment upstream
        - Traffic switching mechanism redirecting requests to target environment atomically
        - Health check validation verifying target environment before switching traffic
        - Instant rollback capability switching traffic back to previous environment
        estimated_hours: 4-6
      - id: cd-deployment-m3
        name: Deployment Automation
        description: Automate the deployment process with pre-deployment checks.
        acceptance_criteria:
        - Deployment script automates full sequence from build through traffic switch
        - New version is deployed to inactive environment while active serves live traffic
        - Smoke tests validate critical API endpoints return expected responses after deploy
        - Switch trigger only executes after all pre-deployment checks pass successfully
        pitfalls:
        - No timeout handling
        - Missing error handling
        - Not verifying after switch
        concepts:
        - Deployment automation
        - Smoke testing
        - Idempotent deployments
        skills:
        - CI/CD Pipeline Design
        - Deployment Scripting
        - Integration Testing
        - Automated Rollback Mechanisms
        deliverables:
        - Deployment script orchestrating build, deploy, test, and switch steps
        - Artifact deployment to inactive environment without affecting live traffic
        - Smoke test automation verifying basic functionality after deployment completes
        - Switch trigger initiating traffic cutover after smoke tests pass successfully
        estimated_hours: 5-8
      - id: cd-deployment-m4
        name: Rollback & Database Migrations
        description: Handle rollbacks and database migrations safely.
        acceptance_criteria:
        - Rollback completes within seconds by switching traffic to still-running previous environment
        - Expand-contract migrations add nullable columns before enforcing constraints later
        - Both blue and green application versions work correctly with current database schema
        - Database version is tracked and migrations are applied idempotently without duplication
        pitfalls:
        - Breaking migrations that prevent rollback
        - Running contract too early
        - Not testing rollback regularly
        concepts:
        - Expand-contract pattern
        - Backward compatibility
        - Database versioning
        skills:
        - Database Schema Versioning
        - Backward Compatible Migrations
        - Rollback Strategy Implementation
        - Data Migration Testing
        deliverables:
        - Rollback automation reverting traffic to previous environment on failure detection
        - Database migration strategy using expand-contract pattern for compatibility
        - Backward-compatible migrations ensuring both old and new code work with schema
        - Migration rollback scripts undoing schema changes if deployment fails
        estimated_hours: 5-8
    - id: metrics-dashboard
      name: Metrics & Alerting Dashboard
      description: Prometheus, Grafana
      difficulty: advanced
      estimated_hours: 25-40
      essence: Time-series data ingestion with write-ahead logging and block-based storage, label-based indexing for multi-dimensional queries, threshold-based rule evaluation with notification routing, and real-time metric aggregation using counters, gauges, and histograms over configurable time windows.
      why_important: Building this teaches you production observability patterns that power modern cloud infrastructure, combining distributed systems concepts with real-time data processing and visualization skills essential for SRE and backend engineering roles.
      learning_outcomes:
      - Implement metrics instrumentation using Prometheus client libraries with custom collectors
      - Design time-series data storage with efficient indexing and retention policies
      - Build PromQL queries with range vectors, aggregation operators, and rate calculations
      - Develop alerting rules with threshold conditions and notification routing
      - Create real-time visualization dashboards with Grafana panels and data sources
      - Implement HTTP metrics scraping endpoints with proper exposition format
      - Debug high-cardinality metrics issues and optimize storage performance
      - Design monitoring architecture with service discovery and target configuration
      skills:
      - Time-Series Databases
      - Metrics Instrumentation
      - PromQL Query Language
      - Alerting & Notifications
      - Data Visualization
      - HTTP Scraping
      - Service Discovery
      - Performance Monitoring
      tags:
      - advanced
      - devops
      - go
      - grafana
      - python
      - time-series
      - visualization
      architecture_doc: architecture-docs/metrics-dashboard/index.md
      languages:
        recommended:
        - Go
        - Python
        also_possible:
        - JavaScript
        - Rust
      resources:
      - type: documentation
        name: Prometheus Documentation
        url: https://prometheus.io/docs/introduction/overview/
      - type: documentation
        name: Grafana Documentation
        url: https://grafana.com/docs/grafana/latest/
      - type: article
        name: Google SRE Book - Monitoring
        url: https://sre.google/sre-book/monitoring-distributed-systems/
      prerequisites:
      - type: skill
        name: HTTP APIs
      - type: skill
        name: Time-series data concepts
      - type: skill
        name: Basic statistics
      - type: skill
        name: Docker
      milestones:
      - id: metrics-dashboard-m1
        name: Metrics Collection
        description: Implement metrics collection with counters, gauges, and histograms.
        acceptance_criteria:
        - Counter metrics are ingested and stored as monotonically increasing cumulative values
        - Gauge metrics are ingested and stored as point-in-time values that can increase or decrease
        - Histogram metrics bucket observations into configurable ranges and track count and sum
        - Labels and dimensions are parsed, validated, and indexed for multi-dimensional metric queries
        - Prometheus exposition format is supported for scraping metrics from compatible endpoints
        pitfalls:
        - Not using labels effectively
        - Too many unique label values (cardinality explosion)
        - Forgetting thread safety
        concepts:
        - Time-series data
        - Metric types
        - Label cardinality
        skills:
        - Concurrent data structures in Go
        - Time-series data modeling
        - Thread-safe metric aggregation
        - Label-based metric organization
        deliverables:
        - Metric ingestion API endpoint that accepts counter, gauge, and histogram data points via HTTP
        - Push and pull model support allowing metrics to be sent by clients or scraped from endpoints
        - Metric type handler that processes counter increments, gauge sets, and histogram observations differently
        - Label handler that parses and indexes label key-value pairs attached to each metric data point
        estimated_hours: 6-10
      - id: metrics-dashboard-m2
        name: Storage & Querying
        description: Implement time-series storage and basic query capabilities.
        acceptance_criteria:
        - Time-series storage persists millions of data points with efficient write throughput and compression
        - Data retention and compaction policies automatically delete or downsample data older than configured limits
        - Range queries return all data points within the specified start and end timestamps at the requested resolution
        - Aggregation functions correctly compute sum, average, rate, and percentile over grouped time series
        pitfalls:
        - Not implementing compaction
        - Inefficient range queries
        - Memory issues with large datasets
        concepts:
        - Time-series databases
        - Data compaction
        - Query optimization
        skills:
        - Time-series database design
        - Data retention and compaction strategies
        - Efficient range query implementation
        - Memory-efficient data structures
        deliverables:
        - Time-series storage engine that persists metric samples indexed by name, labels, and timestamp
        - Query language parser that supports metric selection, label filtering, and time range specification
        - Aggregation function library implementing sum, average, max, min, rate, and percentile calculations
        - Downsampling processor that reduces data resolution for older time ranges to save storage space
        estimated_hours: 6-10
      - id: metrics-dashboard-m3
        name: Visualization Dashboard
        description: Build a web dashboard for visualizing metrics.
        acceptance_criteria:
        - Line charts display time-series data with proper axis labels, legends, and tooltip details
        - Dashboard layout and panel configuration is saved and loaded from a persistent JSON schema
        - Auto-refresh updates all panels at the configured interval without requiring a manual page reload
        - Time range selector allows users to choose relative or absolute time windows for all dashboard panels
        pitfalls:
        - Not handling missing data points
        - Chart performance with many points
        - Time zone issues
        concepts:
        - Data visualization
        - Dashboard design
        - Real-time updates
        skills:
        - Real-time data visualization
        - WebSocket or polling for live updates
        - Frontend charting libraries
        - Handling sparse time-series data
        deliverables:
        - Dashboard configuration system that defines panels, layout, and data source queries in a JSON schema
        - Chart renderer that draws line charts, bar charts, and heatmaps from time-series query results
        - Real-time update mechanism that refreshes chart data at a configurable auto-refresh interval
        - Dashboard sharing feature that generates shareable read-only links or embeddable iframe URLs
        estimated_hours: 6-10
      - id: metrics-dashboard-m4
        name: Alerting System
        description: Implement alerting rules with notifications.
        acceptance_criteria:
        - Alert rules define a metric query, comparison operator, threshold value, and evaluation interval
        - Threshold-based alerts fire when metric values exceed or drop below the configured threshold for a duration
        - Alert states transition correctly between pending, firing, and resolved with appropriate notifications at each stage
        - Notification channels deliver alert messages via the configured integration with customizable message templates
        pitfalls:
        - Alert fatigue from too many alerts
        - Flapping alerts
        - Missing alert resolution notifications
        concepts:
        - Alert rules
        - State machines
        - Notification routing
        skills:
        - Rule engine implementation
        - State machine design for alerts
        - Notification delivery systems
        - Alert deduplication and grouping
        deliverables:
        - Alert rule definition system that specifies metric conditions, thresholds, and evaluation intervals
        - Alert evaluator that periodically checks metric values against defined alert rule conditions
        - Notification channel integrations for email, Slack, PagerDuty, and webhook alert delivery
        - Alert silencing mechanism that suppresses notifications for specified alert rules during maintenance windows
        estimated_hours: 7-10
    - id: load-testing-framework
      name: Load Testing Framework
      description: Performance testing with distributed load
      difficulty: expert
      estimated_hours: '45'
      essence: Coordinated execution of concurrent virtual users across distributed worker nodes with low-overhead latency instrumentation, statistical aggregation of timing percentiles using HDR histograms, and real-time streaming of performance metrics under sustained high-throughput conditions.
      why_important: Building this teaches you production-grade distributed systems patterns, high-performance concurrent programming, and the statistical nuances of measuring system performance under load‚Äîskills critical for infrastructure and backend engineering roles.
      learning_outcomes:
      - Implement virtual user simulation with goroutines and realistic timing distributions
      - Design a coordinator-worker architecture for distributed test execution across multiple nodes
      - Build low-overhead request instrumentation that captures timing with nanosecond precision
      - Implement HDR histogram aggregation for accurate percentile calculation across distributed workers
      - Design real-time metrics pipelines using channels and streaming aggregation
      - Build WebSocket-based dashboards for live performance visualization
      - Debug coordinated failure scenarios in distributed systems
      - Optimize goroutine pooling and memory allocation for sustained high-throughput workloads
      skills:
      - Go Concurrency Patterns
      - Distributed System Coordination
      - Statistical Performance Analysis
      - HDR Histogram Implementation
      - Real-time Data Aggregation
      - WebSocket Communication
      - Worker Pool Architecture
      - High-Performance Instrumentation
      tags:
      - benchmarks
      - distributed
      - expert
      - framework
      - load-testing
      - metrics
      - performance
      - scenarios
      - testing
      architecture_doc: architecture-docs/load-testing-framework/index.md
      languages:
        recommended:
        - Go
        - Python
        - Rust
        also_possible: []
      resources:
      - name: Grafana k6 Documentation
        url: https://grafana.com/docs/k6/latest/
        type: documentation
      - name: Locust Official Documentation
        url: https://docs.locust.io/
        type: documentation
      - name: HdrHistogram on GitHub
        url: https://github.com/HdrHistogram/HdrHistogram
        type: tool
      - name: AWS Distributed Load Testing Architecture
        url: https://aws.amazon.com/solutions/implementations/distributed-load-testing-on-aws/
        type: documentation
      - name: Go Concurrency Patterns Tutorial
        url: https://oneuptime.com/blog/post/2026-01-07-go-goroutines-channels-concurrency/view
        type: tutorial
      prerequisites:
      - type: project
        id: http-server-basic
        name: HTTP Server (Basic)
      - type: skill
        name: Concurrency and threading
      milestones:
      - id: load-testing-framework-m1
        name: Virtual User Simulation
        description: Implement virtual users with realistic think times and behavior
        acceptance_criteria:
        - Concurrent virtual users are spawned up to a configurable maximum with a ramp-up period
        - Each virtual user executes the defined scenario as a repeating sequence of HTTP requests
        - Configurable think time delays between requests simulate realistic user pacing and behavior
        - Session and cookie management persists authentication tokens across requests within a virtual user
        pitfalls:
        - Think time prevents unrealistic load - real users pause between actions
        - Connection pooling affects results - configure properly
        - 'Coordinated omission: measure time from request creation, not send'
        - 'Virtual user state: some tests need session/cookie persistence'
        concepts:
        - HTTP client connection pooling and keep-alive
        - Poisson distribution for realistic arrival patterns
        - Cookie and session state management
        - Request/response lifecycle and timing points
        skills:
        - User simulation
        - HTTP client
        - Think times
        deliverables:
        - User scenario definition DSL that describes a sequence of HTTP requests with parameters
        - Request executor that sends HTTP requests with configured method, headers, body, and assertions
        - Think time simulator that pauses between requests for a configurable random or fixed duration
        - Session handler that maintains cookies, tokens, and state across sequential requests in a scenario
        estimated_hours: '15'
      - id: load-testing-framework-m2
        name: Distributed Workers
        description: Implement distributed load generation with coordinator and workers
        acceptance_criteria:
        - Virtual user load is distributed evenly across all connected worker nodes in the cluster
        - Coordinator signals synchronized test start and stop across all worker nodes simultaneously
        - Metrics from all workers are aggregated into a unified result set with consistent timestamps
        - Worker failures are detected and their load is redistributed to remaining healthy workers
        pitfalls:
        - Network latency between workers adds to reported response time
        - 'Time synchronization: use relative times or sync clocks'
        - 'Worker failure mid-test: decide whether to continue or abort'
        - Metric aggregation can create memory pressure
        concepts:
        - Leader election and consensus protocols
        - gRPC or message queue for worker coordination
        - Clock skew and time synchronization in distributed systems
        - Scatter-gather pattern for metric aggregation
        skills:
        - Distributed coordination
        - Worker management
        - Aggregation
        deliverables:
        - Worker node agent that connects to the coordinator and awaits test distribution instructions
        - Work distribution logic that partitions virtual users evenly across connected worker nodes
        - Result collector that aggregates raw response data from all workers into a central data store
        - Coordinator node that orchestrates test lifecycle including start, stop, and worker health monitoring
        estimated_hours: '15'
      - id: load-testing-framework-m3
        name: Real-time Metrics & Reporting
        description: Implement live metrics dashboard with percentiles and analysis
        acceptance_criteria:
        - Latency percentiles including p50, p90, p95, and p99 are calculated and updated in real time
        - Throughput in requests per second and error rates are tracked and displayed continuously
        - Live dashboard updates at least once per second showing current test metrics and progress
        - Final report is exported in HTML and JSON formats with summary statistics and time-series charts
        pitfalls:
        - Average hides outliers - always report percentiles
        - HDR histogram more accurate than naive quantile calculation
        - 'Streaming percentiles: use t-digest for memory efficiency'
        - Report generator should handle empty metrics gracefully
        concepts:
        - HDR Histogram for accurate percentile calculation
        - T-digest algorithm for streaming quantiles
        - Time-series windowing and downsampling
        - WebSocket or SSE for live metric streaming
        skills:
        - Percentile calculation
        - Time series
        - Streaming aggregation
        deliverables:
        - Response time tracker that records latency for every request with nanosecond precision
        - Throughput calculator that computes requests per second over sliding time windows
        - Error rate tracker that counts and categorizes HTTP errors, timeouts, and connection failures
        - Live dashboard that displays real-time charts of latency, throughput, and error rate during tests
        estimated_hours: '15'
    - id: log-aggregator
      name: Log Aggregation System
      description: Centralized logging with search and alerts
      difficulty: intermediate
      estimated_hours: '40'
      essence: Stream-based log ingestion with label-only indexing using inverted indexes and bloom filters for membership testing, combined with chunk-based compression and time-series storage for high-throughput write performance and efficient label-filtered queries.
      why_important: Log systems are critical for debugging. Understanding log indexing helps optimize queries and reduce storage costs.
      learning_outcomes:
      - Design efficient log ingestion pipeline
      - Build inverted index for label-based queries
      - Implement log compression and chunking
      - Handle high-volume log streams
      skills:
      - Inverted Index Design
      - Bloom Filter Optimization
      - Stream Processing
      - Label-Based Querying
      - Log Compression
      - Query Language Parsing
      - Time-Series Storage
      - Write-Ahead Logging
      tags:
      - collection
      - databases
      - indexing
      - intermediate
      - observability
      - querying
      - search
      architecture_doc: architecture-docs/log-aggregator/index.md
      languages:
        recommended:
        - Go
        - Rust
        - Python
        also_possible: []
      resources:
      - name: Grafana Loki Documentation
        url: https://grafana.com/docs/loki/latest/
        type: documentation
      - name: LogQL Query Guide
        url: https://grafana.com/docs/loki/latest/query/
        type: documentation
      - name: Bloom Filters by Example
        url: http://llimllib.github.io/bloomfilter-tutorial/
        type: tutorial
      - name: Writing a Search Engine with Bloom Filters
        url: https://www.stavros.io/posts/bloom-filter-search-engine/
        type: article
      - name: Loki Bloom Filter Operations
        url: https://grafana.com/docs/loki/latest/operations/bloom-filters/
        type: documentation
      prerequisites:
      - type: skill
        name: Networking (TCP/UDP)
      - type: skill
        name: File I/O and buffering
      - type: skill
        name: Data serialization
      milestones:
      - id: log-aggregator-m1
        name: Log Ingestion
        description: Build log ingestion endpoint that accepts structured logs with labels and timestamps.
        acceptance_criteria:
        - Logs are received via HTTP, TCP, and UDP endpoints with correct protocol handling for each
        - Multiple log formats including JSON, syslog, and custom regex patterns are parsed into structured fields
        - Ingestion handles burst rates of at least 10,000 messages per second without dropping entries
        - Incoming logs are buffered to disk or memory during downstream outage and replayed on recovery
        pitfalls:
        - Out-of-order timestamps complicate querying
        - Memory pressure from unbounded batching
        - Label value with special characters breaks parsing
        concepts:
        - HTTP request parsing and validation
        - Structured logging formats (JSON, logfmt)
        - Time series data modeling with labels
        - Batch processing and buffering strategies
        skills:
        - REST API design
        - Data validation and sanitization
        - Timestamp handling and normalization
        - Memory management and buffering
        deliverables:
        - HTTP log receiver endpoint that accepts JSON-formatted log entries via POST requests
        - Syslog receiver that parses RFC 5424 and RFC 3164 formatted syslog messages over TCP and UDP
        - File tail agent that watches log files for new lines and forwards them to the ingestion pipeline
        - Log parser that extracts structured fields from unstructured log messages using configurable patterns
        estimated_hours: '13.5'
      - id: log-aggregator-m2
        name: Log Index
        description: Build inverted index for fast label-based log queries with bloom filters for negative lookups.
        acceptance_criteria:
        - Inverted index maps each unique term to the set of log entries containing that term
        - Structured fields including level, service, and hostname are indexed for fast filtered lookups
        - Time-based partitions allow queries to scan only the relevant time window instead of all data
        - Index compaction merges small segments into larger ones to reduce disk usage and query overhead
        pitfalls:
        - Bloom filter false positives require verification
        - High cardinality labels bloat index
        - Index rebuild on corruption is expensive
        concepts:
        - Inverted index data structures
        - Bloom filter probabilistic data structures
        - Label cardinality and index sizing
        - Index persistence and recovery mechanisms
        skills:
        - Data structure implementation
        - Probabilistic algorithms
        - Index optimization techniques
        - Serialization and deserialization
        deliverables:
        - Inverted index mapping each unique term to the list of log entries containing that term
        - Time-based partitioning that segments the index into hourly or daily shards for efficient queries
        - Field extraction pipeline that parses structured fields like log level, service, and request ID
        - Index storage engine that persists the inverted index to disk with memory-mapped file access
        estimated_hours: '13.5'
      - id: log-aggregator-m3
        name: Log Query Engine
        description: Build LogQL-style query engine with label filtering, text search, and log processing functions.
        acceptance_criteria:
        - Full-text search matches log entries containing any or all of the specified keywords
        - Field filters like level=ERROR and service=api narrow results to matching structured values
        - Regular expression patterns in queries match against log message text and structured fields
        - Query pagination returns results in pages with a cursor for efficient traversal of large result sets
        pitfalls:
        - Unbounded query scans entire log history
        - JSON parsing failures silently drop entries
        - Pipeline order matters for filter efficiency
        concepts:
        - Query language parsing and AST construction
        - Query optimization and execution planning
        - Stream processing and filtering pipelines
        - Regular expression pattern matching
        skills:
        - Parser implementation
        - Query optimization
        - Stream processing
        - Performance profiling and tuning
        deliverables:
        - Query language parser that supports full-text search, field filters, and boolean operators
        - Full-text search engine that finds log entries matching a keyword or phrase across all fields
        - Field filter evaluator that restricts results to entries matching specific structured field values
        - Time range query executor that limits search results to entries within a specified time window
        estimated_hours: '13.5'
      - id: log-aggregator-m4
        name: Log Storage & Compression
        description: Implement efficient log storage with chunk-based compression and retention policies.
        acceptance_criteria:
        - Logs are stored in compressed chunks (snappy/gzip) organized by time windows
        - Retention policies automatically delete logs older than configured TTL
        - Storage layer supports both local disk and S3-compatible object storage
        - Write-ahead log ensures no data loss during ingestion
        pitfalls:
        - Decompression performance on queries
        - Storage leak from failed cleanup
        - WAL growing unbounded
        concepts:
        - chunk-based storage
        - compression
        - retention policies
        - write-ahead logging
        skills:
        - Compression algorithms
        - Storage engines
        - Retention policies
        deliverables:
        - Compressed log storage engine with retention and WAL
        - Benchmark comparing compression ratios across zstd, lz4, and gzip for log data
        - Time-based and size-based retention policy engine with configurable rules per log stream
        - WAL recovery tool that replays writes after crash and validates chunk integrity
        estimated_hours: '8'
      - id: log-aggregator-m5
        name: Multi-Tenant & Alerting
        description: Add multi-tenant isolation with per-tenant rate limits and log-based alerting rules.
        acceptance_criteria:
        - Tenant isolation ensures logs from one tenant cannot be queried by another
        - Per-tenant ingestion rate limits prevent noisy neighbor issues
        - Alert rules trigger on log patterns (error rate spikes, specific messages)
        - Alert deduplication and notification via webhook
        pitfalls:
        - Tenant ID injection attacks
        - Alert storms from cascading failures
        concepts:
        - multi-tenancy
        - rate limiting
        - pattern-based alerting
        - deduplication
        skills:
        - Multi-tenancy
        - Alerting
        - Rate limiting
        deliverables:
        - Multi-tenant log aggregator with alerting
        - Tenant isolation layer with per-tenant rate limits and quota enforcement
        - Rule-based alerting engine matching log patterns and triggering webhooks or notifications
        - Alert deduplication system preventing duplicate notifications within configurable time windows
        estimated_hours: '8'
    - id: alerting-system
      name: Alerting System
      description: Metric-based alerts with escalation
      difficulty: intermediate
      estimated_hours: '35'
      essence: Time-series metric evaluation with state machine transitions (pending ‚Üí firing ‚Üí resolved), label-based alert deduplication and grouping, and policy-driven routing trees that fan-out notifications to multiple receivers based on matcher expressions.
      why_important: Alerting is critical for on-call. Understanding alert fatigue, grouping, and routing helps design better alert systems.
      learning_outcomes:
      - Design alert rule evaluation engine
      - Implement alert state machine (pending, firing, resolved)
      - Build notification routing and grouping
      - Handle alert silencing and inhibition
      skills:
      - Time-series querying
      - Alert state machines
      - Notification routing
      - Label-based grouping
      - Silence management
      - Webhook integration
      - Template rendering
      - Inhibition rules
      tags:
      - devops
      - distributed-systems
      - escalation
      - intermediate
      - notifications
      - observability
      - thresholds
      - webhooks
      architecture_doc: architecture-docs/alerting-system/index.md
      languages:
        recommended:
        - Go
        - Python
        also_possible: []
      resources:
      - name: Prometheus Alerting Overview
        url: https://prometheus.io/docs/alerting/latest/overview/
        type: documentation
      - name: Alertmanager Configuration
        url: https://prometheus.io/docs/alerting/latest/configuration/
        type: documentation
      - name: Google SRE Monitoring
        url: https://sre.google/sre-book/monitoring-distributed-systems/
        type: book
      - name: Alerting Tutorial
        url: https://prometheus.io/docs/tutorials/alerting_based_on_metrics/
        type: tutorial
      - name: Prometheus Alertmanager Best Practices
        url: https://www.sysdig.com/blog/prometheus-alertmanager
        type: article
      prerequisites:
      - type: project
        id: metrics-collector
      milestones:
      - id: alerting-system-m1
        name: Alert Rule Evaluation
        description: Build rule evaluation engine that periodically queries metrics and triggers alerts based on thresholds.
        acceptance_criteria:
        - Evaluates PromQL-like expressions against the current metric data and returns numeric results
        - Supports comparison operators (>, <, >=, <=, ==, !=) for threshold-based alerting
        - Handles for-duration by keeping alerts in pending state until the condition holds for the specified period
        - Returns correct firing or resolved status for each evaluated alert rule after each evaluation cycle
        pitfalls:
        - Flapping alerts from noisy metrics need hysteresis
        - for_duration resets if metric briefly returns normal
        - Template rendering errors crash evaluation loop
        concepts:
        - Cron-based scheduling for periodic metric queries
        - Threshold comparison with configurable operators
        - Time-series query result evaluation
        - Alert state transitions (pending, firing, resolved)
        - Template rendering for dynamic alert messages
        skills:
        - Time-series database querying
        - Periodic job scheduling
        - Expression evaluation
        - State machine implementation
        - Template engine integration
        deliverables:
        - Rule expression parser that converts PromQL-like alert expressions into evaluable rule objects
        - Periodic evaluation loop that checks all active rules at a configurable interval (e.g., every 15 seconds)
        - Threshold comparison engine supporting greater-than, less-than, and equality operators on metric values
        - Duration-based firing logic that transitions alerts from pending to firing after the configured for-duration
        estimated_hours: '9'
      - id: alerting-system-m2
        name: Alert Grouping
        description: Group related alerts together to reduce notification noise with configurable grouping keys.
        acceptance_criteria:
        - Alerts are grouped by user-configurable label sets such as alertname, cluster, or service
        - Aggregation reduces notification noise by batching multiple alerts into a single notification
        - group_wait delays the first notification to collect more alerts, group_interval controls re-send frequency
        - Group key generation produces a stable, deterministic key from the sorted set of grouping label values
        pitfalls:
        - Group key change orphans alert in old group
        - Long group_wait delays critical alerts
        - Memory leak from empty groups not cleaned up
        concepts:
        - Grouping keys for alert aggregation
        - Time-based batching windows
        - Hash-based group partitioning
        - Group lifecycle management
        - Notification deduplication
        skills:
        - Map-based grouping algorithms
        - Timer management for delayed processing
        - Memory management for long-lived collections
        - Hash function selection
        deliverables:
        - Group-by-labels logic that aggregates related alerts into a single notification group
        - Aggregated notification builder that combines grouped alerts into a single consolidated message
        - Group wait and group interval configuration controlling initial wait time and re-notification frequency
        - Group resolution handler that sends resolved notifications when all alerts in a group have cleared
        estimated_hours: '9'
      - id: alerting-system-m3
        name: Silencing & Inhibition
        description: Implement alert silencing for maintenance windows and inhibition to suppress alerts when related alerts fire.
        acceptance_criteria:
        - Alerts matching the silence matcher labels are suppressed and do not trigger notifications
        - Time-based silence windows correctly activate at the start time and expire at the end time
        - Inhibition rules suppress target alerts when the source alert with matching labels is actively firing
        - Expired silences are automatically removed and previously silenced alerts resume normal notification flow
        pitfalls:
        - Inhibition loop when alerts inhibit each other
        - Silence with wrong matchers misses alerts
        - Race between silence creation and firing alert
        concepts:
        - Label matching for silence rules
        - Time-based silence activation windows
        - Alert inhibition dependency graphs
        - Matcher syntax and evaluation
        - Concurrent access to silence state
        skills:
        - Label selector implementation
        - Time range validation
        - Cycle detection in directed graphs
        - Race condition prevention
        - Atomic state updates
        deliverables:
        - Silence matcher that suppresses alerts whose labels match a user-defined set of matchers
        - Time-based silencing with configurable start time, end time, and optional recurrence schedule
        - Inhibition rules that suppress lower-severity alerts when a higher-severity related alert is firing
        - Priority-based suppression that prevents notification of inhibited alerts while preserving their state
        estimated_hours: '9'
      - id: alerting-system-m4
        name: Notification Routing
        description: Route alerts to different receivers (Slack, PagerDuty, email) based on matching rules.
        acceptance_criteria:
        - Alerts are routed to different notification channels based on severity and label-matching rules
        - Multiple notification channels including email, Slack, and webhook are supported simultaneously
        - Notification grouping batches alerts by alert name and labels to reduce duplicate notifications
        - Rate limiting prevents notification storms by enforcing a maximum send frequency per channel
        pitfalls:
        - Missing default route causes unrouted alerts
        - continue=true on all routes sends duplicate notifications
        - Rate limiting by receiver prevents important alerts
        concepts:
        - Tree-based routing configuration
        - Label matchers for route selection
        - Receiver integration patterns
        - Notification retry logic
        - Rate limiting per destination
        skills:
        - Routing tree traversal
        - Plugin architecture for integrations
        - Exponential backoff implementation
        - Rate limiter design
        - HTTP client configuration
        deliverables:
        - Notification channel configuration supporting email, Slack, PagerDuty, and generic webhook targets
        - Routing rules that direct alerts to channels based on severity, team, and label-match criteria
        - Escalation policies that promote unacknowledged alerts to higher-priority channels after a timeout
        - On-call schedule integration that routes critical alerts to the currently on-call responder
        estimated_hours: '9'
    - id: apm-system
      name: APM System
      description: Application performance monitoring
      difficulty: advanced
      estimated_hours: '45'
      essence: Distributed span ingestion with context propagation and parent-child relationships, time-series trace storage with efficient querying, probabilistic and adaptive sampling algorithms that preserve trace fidelity, and service dependency graph construction from span relationships to visualize microservice communication patterns and error propagation.
      why_important: APM helps identify performance bottlenecks across microservices. Understanding trace data helps design better instrumentation.
      learning_outcomes:
      - Design span collection and storage
      - Build service dependency maps from traces
      - Implement latency percentile analysis
      - Handle high-volume trace sampling
      skills:
      - Distributed Tracing
      - Span Collection & Storage
      - Service Dependency Graphs
      - Adaptive Sampling Algorithms
      - Time-Series Data Processing
      - High-Volume Data Ingestion
      - Latency Percentile Analysis
      tags:
      - advanced
      - devops
      - distributed
      - distributed-systems
      - errors
      - observability
      - performance
      - transactions
      architecture_doc: architecture-docs/apm-system/index.md
      languages:
        recommended:
        - Go
        - Python
        - Java
        also_possible: []
      resources:
      - name: OpenTelemetry Tracing Documentation
        url: https://opentelemetry.io/docs/concepts/signals/traces/
        type: documentation
      - name: Jaeger Distributed Tracing Platform
        url: https://www.jaegertracing.io/
        type: documentation
      - name: Adaptive Sampling in Jaeger
        url: https://medium.com/jaegertracing/adaptive-sampling-in-jaeger-50f336f4334
        type: article
      - name: Complete Guide to Trace Sampling
        url: https://logz.io/learn/sampling-in-distributed-tracing-guide/
        type: tutorial
      - name: Datadog APM Tracing Guide
        url: https://docs.datadoghq.com/tracing/
        type: documentation
      prerequisites:
      - type: project
        id: distributed-tracing
      milestones:
      - id: apm-system-m1
        name: Trace Collection
        description: Build trace collector that ingests spans from multiple services with proper parent-child linking.
        acceptance_criteria:
        - Ingestion endpoint receives spans via HTTP or gRPC and returns acknowledgment within 100ms
        - OpenTelemetry trace format is parsed correctly including span context, attributes, and events
        - Spans are stored with trace ID indexing so all spans in a trace can be retrieved in a single query
        - System handles high-volume span ingestion of at least 1000 spans per second without data loss
        pitfalls:
        - Late-arriving spans miss assembly window
        - Memory grows unbounded with incomplete traces
        - Clock skew makes span ordering incorrect
        concepts:
        - Span ID and trace ID propagation across service boundaries
        - Parent-child span relationships and context inheritance
        - Buffering strategies for out-of-order span arrival
        - Clock synchronization and timestamp normalization
        skills:
        - Distributed systems data collection
        - Time-series data handling
        - Protocol buffer serialization
        - In-memory data structure design
        deliverables:
        - Trace data ingestion API accepting spans via HTTP and gRPC endpoints in OpenTelemetry format
        - Trace sampling module that decides which traces to keep based on configurable sampling policies
        - Trace storage backend that persists span data with indexing for efficient retrieval by trace ID
        - Trace indexing layer that creates secondary indexes on service name, operation, and time range
        estimated_hours: '15'
      - id: apm-system-m2
        name: Service Map
        description: Build service dependency map from trace data showing call relationships and error rates.
        acceptance_criteria:
        - Service dependency graph is correctly built from parent-child span relationships across services
        - Inter-service call metrics (request count, latency, error rate) are computed for each edge in the graph
        - Topology changes such as new services or removed dependencies are detected within one refresh interval
        - Service map visualization clearly displays nodes for services and edges for their communication paths
        pitfalls:
        - Same-service spans inflate node metrics
        - Async calls appear as separate traces
        - High cardinality operations bloat map
        concepts:
        - Graph data structures for service topology
        - Aggregating metrics across service boundaries
        - Edge weight calculation from span durations
        - Detecting cyclic dependencies in service calls
        skills:
        - Graph algorithms and traversal
        - Data aggregation pipelines
        - Metric computation and rollup
        - Visualization data formatting
        deliverables:
        - Service dependency extraction logic that derives caller-callee relationships from span parent links
        - Call graph construction that builds a directed graph of inter-service communication patterns
        - Map visualization that renders the service dependency graph as an interactive node-and-edge diagram
        - Real-time update mechanism that refreshes the service map as new trace data arrives
        estimated_hours: '15'
      - id: apm-system-m3
        name: Trace Sampling
        description: Implement adaptive sampling to reduce storage costs while preserving interesting traces.
        acceptance_criteria:
        - Head-based sampling drops traces at the configured probability before any spans are stored
        - Tail-based sampling retains error traces and high-latency traces regardless of the base sampling rate
        - Sampling rates are independently configurable per service so high-traffic services can be sampled lower
        - Sampled traces include all their spans so no trace is stored with missing child span data
        pitfalls:
        - Head-based sampling loses interesting traces
        - Rate limiting causes bursty drops
        - Adaptive sampling oscillates under variable load
        concepts:
        - Head-based versus tail-based sampling strategies
        - Probabilistic data structures for trace decision caching
        - Adaptive rate calculation from system load metrics
        - Priority sampling based on error rates and latency
        skills:
        - Statistical sampling techniques
        - Memory-efficient probabilistic algorithms
        - Dynamic threshold adjustment
        - Performance optimization under load
        deliverables:
        - Head-based sampling that makes a probabilistic keep/drop decision at trace creation time
        - Tail-based sampling that evaluates completed traces and keeps those matching interest criteria
        - Sampling rate configuration that allows per-service and per-operation sampling percentage tuning
        - Consistent sampling using trace-ID-based hashing so all spans in a trace get the same decision
        estimated_hours: '15'
      - id: apm-system-m4
        name: Performance Analytics & Anomaly Detection
        description: Build performance analytics with percentile calculations (p50/p95/p99) and automatic anomaly detection.
        acceptance_criteria:
        - Calculate latency percentiles (p50, p95, p99) per service and endpoint
        - Detect anomalies using statistical methods (z-score, moving average deviation)
        - Generate performance regression alerts when latency exceeds baselines
        - Historical comparison (this week vs last week) for trend analysis
        pitfalls:
        - Percentile aggregation across services is not mathematically correct
        - False positive alerts during deployments
        concepts:
        - percentile calculation
        - anomaly detection
        - time-series analysis
        - baseline comparison
        skills:
        - Statistical analysis
        - Anomaly detection
        - Time-series
        deliverables:
        - Performance analytics engine with anomaly detection
        - Real-time percentile calculator maintaining p50/p95/p99 latencies using t-digest algorithm
        - Anomaly detection system comparing current metrics against historical baselines with confidence intervals
        - Time-series database storing aggregated performance metrics with configurable retention
        estimated_hours: '8'
      - id: apm-system-m5
        name: APM SDK & Auto-Instrumentation
        description: Build an APM SDK that auto-instruments HTTP clients, database drivers, and framework middleware.
        acceptance_criteria:
        - SDK automatically instruments outgoing HTTP requests with trace context propagation
        - Database query spans are captured with query text and execution time
        - Framework middleware (Express/Flask/Gin) is instrumented for incoming request spans
        - Context propagation works across async boundaries (async/await, goroutines)
        pitfalls:
        - Monkey-patching breaking original behavior
        - Context loss in async code
        - Performance overhead of tracing
        concepts:
        - monkey-patching
        - middleware
        - context propagation
        - W3C Trace Context
        skills:
        - Instrumentation
        - Middleware
        - Context propagation
        deliverables:
        - APM SDK with auto-instrumentation for HTTP, DB, and frameworks
        - HTTP client middleware injecting W3C Trace Context headers and capturing request/response metadata
        - Database driver wrapper automatically tracing SQL queries with parameter sanitization
        - Framework integration modules for Express/Flask/Spring providing automatic endpoint instrumentation
        estimated_hours: '10'
    - id: metrics-collector
      name: Metrics Collector
      description: Time-series metrics collection and aggregation
      difficulty: advanced
      estimated_hours: '50'
      essence: Pull-based metric scraping with time-series storage using delta-of-delta compression and XOR encoding, combined with a declarative query language for real-time aggregations over high-cardinality labeled data streams.
      why_important: Understanding metrics systems helps you design better instrumentation, optimize queries, and debug performance issues in production.
      learning_outcomes:
      - Design pull-based metrics collection
      - Implement time-series storage efficiently
      - Build query language for aggregations
      - Handle high-cardinality metrics
      skills:
      - Time-Series Compression
      - Service Discovery
      - Pull-Based Scraping
      - Query Language Design
      - LSM Tree Storage
      - Delta-of-Delta Encoding
      - Label-Based Indexing
      - Downsampling Strategies
      tags:
      - advanced
      - databases
      - distributed-systems
      - exposition
      - labels
      - observability
      - prometheus
      - scraping
      architecture_doc: architecture-docs/metrics-collector/index.md
      languages:
        recommended:
        - Go
        - Python
        also_possible: []
      resources:
      - name: Prometheus Documentation
        url: https://prometheus.io/docs/introduction/overview/
        type: documentation
      - name: Write a TSDB from Scratch
        url: https://nakabonne.dev/posts/write-tsdb-from-scratch/
        type: tutorial
      - name: PromQL Querying Basics
        url: https://prometheus.io/docs/prometheus/latest/querying/basics/
        type: documentation
      - name: Gorilla Time Series Compression
        url: https://blog.acolyer.org/2016/05/03/gorilla-a-fast-scalable-in-memory-time-series-database/
        type: paper
      - name: Monitoring 101 Best Practices
        url: https://www.datadoghq.com/blog/monitoring-101-collecting-data/
        type: article
      prerequisites:
      - type: project
        id: time-series-db
      milestones:
      - id: metrics-collector-m1
        name: Metrics Data Model
        description: Implement the metrics data model with counters, gauges, histograms, and summaries with labels.
        acceptance_criteria:
        - Counter, gauge, and histogram metric types are defined with their correct semantic behaviors
        - Labels attach key-value pairs to metrics enabling multi-dimensional filtering and aggregation
        - Metric metadata including help text and unit is stored and exposed alongside metric values
        - Metric names are validated against a naming convention rejecting invalid characters or reserved prefixes
        pitfalls:
        - High cardinality labels cause memory explosion
        - Counter resets on restart need special handling
        - Histogram bucket boundaries can't change after creation
        concepts:
        - Time series data structures with labels and timestamps
        - Metric type semantics (counter monotonicity, gauge mutability)
        - Label cardinality and its impact on memory usage
        - Metric metadata and type information encoding
        skills:
        - Data structure design for time series
        - Memory-efficient label storage
        - Type system implementation
        - Metric validation and constraints
        deliverables:
        - Metric type definitions for counter, gauge, and histogram with their increment and observe methods
        - Label support system that attaches key-value dimension pairs to each metric for filtering
        - Timestamp handler that records the observation time for each metric data point in UTC
        - Metric metadata store that tracks description, unit, and type information for each registered metric
        estimated_hours: '12.5'
      - id: metrics-collector-m2
        name: Scrape Engine
        description: Build a scrape engine that pulls metrics from configured targets with service discovery support.
        acceptance_criteria:
        - Scrape targets are discovered from static config files and dynamic service discovery backends
        - Metrics are pulled from HTTP endpoints by parsing the Prometheus exposition format text
        - Scrape timeouts cancel the HTTP request and mark the target as down for that scrape cycle
        - Service discovery integrations update the target list automatically when services are added or removed
        pitfalls:
        - Too aggressive scraping overwhelms targets
        - Network timeouts block scrape loop
        - Label collision between target and metric labels
        concepts:
        - HTTP endpoint scraping and parsing text-based formats
        - Service discovery patterns (static, DNS, Kubernetes)
        - Concurrent scraping with worker pools
        - Target health checking and retry logic
        skills:
        - HTTP client implementation with timeouts
        - Concurrent programming with goroutines
        - Text parsing and protocol handling
        - Service discovery integration
        deliverables:
        - Target discovery module that finds scrape endpoints via static configuration or service discovery
        - HTTP scraper that fetches and parses Prometheus exposition format text from target endpoints
        - Scrape interval scheduler that triggers metric collection at configurable per-target intervals
        - Scrape timeout handler that aborts and reports targets that fail to respond within the deadline
        estimated_hours: '12.5'
      - id: metrics-collector-m3
        name: Time Series Storage
        description: Implement efficient time-series storage with compression, retention, and downsampling.
        acceptance_criteria:
        - Time series are stored efficiently with less than 2 bytes per sample using delta compression
        - Gorilla-style compression reduces storage size by encoding timestamp deltas and XOR-ed float values
        - Retention policies automatically delete data older than the configured retention period
        - High cardinality label combinations are handled without excessive memory usage or index bloat
        pitfalls:
        - Chunk boundaries at exact times cause off-by-one
        - Concurrent writes corrupt chunk data
        - Compression ratio degrades with irregular timestamps
        concepts:
        - Time series compression algorithms (delta encoding, gorilla compression)
        - Write-ahead logging for durability
        - Block-based storage with chunk boundaries
        - Downsampling and retention policies
        skills:
        - File-based storage management
        - Compression algorithm implementation
        - Concurrent write handling with locks
        - Memory-mapped file I/O
        deliverables:
        - Time series data structure that stores timestamped samples indexed by metric name and label set
        - Compression engine using Gorilla-style delta-of-delta encoding for timestamps and XOR for values
        - Retention policy manager that deletes or downsamples data older than a configurable duration
        - Index structure that maps metric name and label combinations to their time series data blocks
        estimated_hours: '12.5'
      - id: metrics-collector-m4
        name: Query Engine
        description: Build a PromQL-like query engine with instant queries, range queries, and aggregation functions.
        acceptance_criteria:
        - PromQL-like queries with metric name, label filters, and aggregation functions are parsed and executed
        - Aggregation functions sum, avg, max, min, and count produce correct results grouped by specified labels
        - Range queries return all data points within the specified start and end time window at the requested step interval
        - Label matchers support exact match, not-equal, regex match, and negative regex selectors
        pitfalls:
        - Rate calculation wrong across counter resets
        - Label matching with regex can be slow
        - Query on high-cardinality labels causes OOM
        concepts:
        - Expression parsing and abstract syntax trees
        - Vector and matrix data types for query results
        - Aggregation operators over label dimensions
        - Rate and derivative calculations with counter reset detection
        skills:
        - Query language parser implementation
        - Time series math and interpolation
        - Label matcher optimization
        - Memory-efficient query execution
        deliverables:
        - PromQL-style query parser that supports instant queries, range queries, and label matchers
        - Range query executor that returns time series data points within a specified time window
        - Aggregation engine implementing sum, avg, max, min, count, and quantile functions over label groups
        - Label matcher that filters time series by exact match, regex, and not-equal label selectors
        estimated_hours: '12.5'
    expert:
    - id: build-test-framework
      name: Build Your Own Test Framework
      description: pytest/jest clone
      difficulty: expert
      estimated_hours: 40-60
      essence: Test discovery through AST parsing and reflection, assertion introspection with detailed failure reporting, fixture dependency injection, and pluggable test execution lifecycle management with parallel runner orchestration.
      why_important: Building a test framework reveals how metaprogramming, code introspection, and AST manipulation work in real tools millions of developers rely on daily, while teaching advanced software architecture patterns like plugin systems, dependency injection, and reporter abstractions that apply across many domains.
      learning_outcomes:
      - Implement AST-based test discovery using language reflection APIs
      - Design assertion libraries with rich failure reporting and introspection
      - Build fixture dependency graphs with automatic resolution and cleanup ordering
      - Create pluggable architecture with hooks for reporters, plugins, and extensions
      - Implement parallel test execution with process isolation and result aggregation
      - Parse and execute CLI arguments for test filtering, verbosity, and configuration
      - Generate structured test reports in multiple formats (JSON, XML, HTML)
      - Handle test lifecycle events including setup, teardown, and error recovery
      skills:
      - Abstract Syntax Trees
      - Code Introspection
      - Plugin Architecture
      - Dependency Injection
      - Process Parallelization
      - CLI Design
      - Reporter Patterns
      - Metaprogramming
      tags:
      - assertions
      - build-from-scratch
      - expert
      - fixtures
      - framework
      - go
      - javascript
      - mocking
      - python
      - runners
      - testing
      architecture_doc: architecture-docs/build-test-framework/index.md
      languages:
        recommended:
        - Python
        - JavaScript
        - Go
        also_possible:
        - Rust
        - Java
      resources:
      - type: repository
        name: pytest source
        url: https://github.com/pytest-dev/pytest
      - type: article
        name: Building a Test Framework
        url: https://www.destroyallsoftware.com/screencasts/catalog/building-a-test-framework
      prerequisites:
      - type: skill
        name: Reflection/metaprogramming
      - type: skill
        name: Assertion libraries
      - type: skill
        name: CLI development
      milestones:
      - id: build-test-framework-m1
        name: Test Discovery & Execution
        description: Discover and run test functions.
        acceptance_criteria:
        - Discovery finds all functions matching test naming convention in specified modules
        - Runner executes each test and records whether it passed, failed, or errored
        - Each test runs in isolation so one test failure does not affect subsequent tests
        - Parallel execution runs independent tests concurrently reducing total suite time
        pitfalls:
        - Module import side effects
        - Test isolation
        - Path handling
        concepts:
        - Reflection
        - Module loading
        - Test isolation
        skills:
        - File system traversal and glob patterns
        - Dynamic module importing and execution
        - Process isolation and sandboxing
        - Test collection and filtering
        deliverables:
        - Test function discovery scanning modules for test-prefixed functions
        - Test runner executing discovered tests and collecting pass/fail results
        - Test isolation ensuring each test runs independently without shared state
        - Parallel test execution running independent tests concurrently for speed
        estimated_hours: 10-15
      - id: build-test-framework-m2
        name: Assertions & Matchers
        description: Implement rich assertion library.
        acceptance_criteria:
        - assertEqual fails with descriptive message showing expected versus actual values
        - Collection assertions verify contains, length, and subset relationships correctly
        - assertRaises fails if expected exception type is not raised by callable
        - Custom matchers provide domain-specific failure messages for better diagnostics
        pitfalls:
        - Unhelpful error messages
        - Float comparison
        - Exception context
        concepts:
        - Assertions
        - Diff algorithms
        - Context managers
        skills:
        - Deep object comparison and diffing
        - Custom assertion DSL design
        - Exception handling and introspection
        - String formatting and colorization
        deliverables:
        - assertEqual, assertTrue, assertFalse verifying expected values and conditions
        - Collection assertions checking list contents, length, and membership
        - Exception assertions verifying that expected exceptions are raised
        - Custom matcher API allowing user-defined assertion predicates with messages
        estimated_hours: 8-12
      - id: build-test-framework-m3
        name: Fixtures & Setup/Teardown
        description: Implement test fixtures for setup and cleanup.
        acceptance_criteria:
        - setUp runs before each test and tearDown runs after each test regardless of outcome
        - Shared fixtures create expensive resources once and share across multiple tests
        - Module-scoped fixtures are created once per module and torn down after all tests complete
        - Fixture injection provides named resources as test function parameters automatically
        pitfalls:
        - Fixture cleanup on error
        - Circular dependencies
        - Scope leaks
        concepts:
        - Dependency injection
        - Resource management
        - Generators
        skills:
        - Dependency injection pattern implementation
        - Context manager and cleanup protocol
        - Scope management and lifecycle control
        - Generator-based resource handling
        deliverables:
        - setUp and tearDown hooks running before and after each test function
        - Shared fixture factory creating reusable test resources across multiple tests
        - Fixture scoping controlling lifetime at function, class, or module level
        - Fixture dependency injection providing test resources via parameter names
        estimated_hours: 12-18
      - id: build-test-framework-m4
        name: Reporting & CLI
        description: Generate test reports and provide CLI interface.
        acceptance_criteria:
        - Report displays each test name with pass/fail status and execution duration
        - Summary shows total count of passed, failed, and skipped tests with overall time
        - CLI accepts glob patterns to select test files and name filters to select tests
        - JUnit XML output is compatible with CI tools like Jenkins and GitHub Actions
        pitfalls:
        - Exit codes
        - Terminal detection
        - XML escaping
        concepts:
        - CLI design
        - Reporting formats
        - CI integration
        skills:
        - Argument parsing and CLI design
        - XML and JSON serialization
        - Terminal capability detection and ANSI codes
        - Exit code conventions and signal handling
        deliverables:
        - Pass/fail reporting displaying test results with execution time per test
        - Summary statistics showing total passed, failed, skipped, and execution duration
        - CLI interface accepting file patterns, verbosity, and filter arguments
        - JUnit XML output format for integration with CI/CD pipeline tools
        estimated_hours: 10-15
    - id: build-ci-system
      name: Build Your Own CI System
      description: Pipeline executor
      difficulty: expert
      estimated_hours: 50-80
      essence: Event-driven orchestration of isolated container workloads through webhook-triggered job queuing, YAML DSL parsing with schema validation, and distributed execution across worker nodes with real-time log aggregation and state management.
      why_important: Building a CI system exposes you to production-grade distributed systems architecture, container orchestration, and event-driven design patterns that are fundamental to modern DevOps infrastructure and cloud-native application deployment.
      learning_outcomes:
      - Implement YAML schema validation and AST parsing for pipeline configuration
      - Design container-based job isolation using Docker API or containerd
      - Build webhook receivers with signature verification and replay protection
      - Implement distributed job queues with message brokers for task distribution
      - Design real-time log streaming from isolated containers to web clients
      - Build idempotent job executors with retry logic and timeout handling
      - Implement artifact storage and caching strategies for build optimization
      - Design role-based access control for pipeline management and execution
      skills:
      - Container Orchestration
      - Message Queue Systems
      - Webhook Processing
      - YAML Schema Validation
      - Distributed Job Scheduling
      - Real-time Log Streaming
      - Event-Driven Architecture
      - Process Isolation
      tags:
      - agents
      - artifacts
      - build-from-scratch
      - expert
      - go
      - pipelines
      - python
      - rust
      - triggers
      architecture_doc: architecture-docs/build-ci-system/index.md
      languages:
        recommended:
        - Go
        - Python
        - Rust
        also_possible:
        - JavaScript
        - Java
      resources:
      - type: article
        name: How GitHub Actions Works
        url: https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions
      - type: article
        name: Building a CI Server
        url: https://blog.boot.dev/education/build-ci-cd-server/
      - type: documentation
        name: Jenkins Architecture
        url: https://www.jenkins.io/doc/developer/architecture/
      prerequisites:
      - type: skill
        name: Docker/containers
      - type: skill
        name: Git hooks/webhooks
      - type: skill
        name: Process management
      - type: skill
        name: Queue systems
      milestones:
      - id: build-ci-system-m1
        name: Pipeline Configuration Parser
        description: Parse and validate pipeline configuration files (YAML).
        acceptance_criteria:
        - YAML pipeline files are parsed into a structured pipeline object with stages, jobs, and step definitions
        - Step and stage structure correctly captures sequential steps within a job and parallel stages in the pipeline
        - Environment variables from the pipeline config, system, and secrets are substituted into step commands
        - Conditional execution uses if-expressions to skip or run steps based on branch name, event type, or custom conditions
        - Matrix builds expand multiple axis values into the cartesian product of job configurations for parallel testing
        pitfalls:
        - Circular job dependencies
        - Invalid matrix combinations
        - Missing required fields
        concepts:
        - YAML parsing
        - Configuration validation
        - DAG construction
        skills:
        - YAML schema design and validation
        - Directed acyclic graph algorithms
        - Configuration parsing libraries
        - Dependency resolution patterns
        deliverables:
        - YAML pipeline definition parser that reads job definitions, steps, and configuration from a CI config file
        - Stage and job dependency graph builder that determines execution order and parallelism from declared dependencies
        - Environment variable substitution engine that resolves $VAR and ${VAR} references in pipeline configuration
        - Matrix build configuration that expands axis definitions into multiple job instances with different parameter combinations
        estimated_hours: 8-12
      - id: build-ci-system-m2
        name: Job Execution Engine
        description: Execute pipeline jobs in isolated containers.
        acceptance_criteria:
        - Container-based isolation ensures each job runs in a fresh Docker container with no state from previous jobs
        - Step execution runs each shell command in sequence and stops the job on the first non-zero exit code
        - Environment variables including secrets are injected into the container without leaking to logs
        - Stdout and stderr output from each step is captured in real-time and stored for later retrieval
        - Artifacts specified by glob patterns are collected from the job workspace and uploaded to artifact storage
        pitfalls:
        - Container cleanup on failure
        - Timeout handling
        - Large output handling
        concepts:
        - Container isolation
        - Process execution
        - Resource management
        skills:
        - Docker container orchestration
        - Process lifecycle management
        - Resource quota enforcement
        - Stream handling for logs
        - Signal handling and cleanup
        deliverables:
        - Docker container job execution that runs each job in an isolated container with the specified image
        - Script step runner that executes shell commands sequentially and captures stdout/stderr with exit codes
        - Artifact collection that copies specified output files from the job container to persistent storage
        - Parallel job execution engine that runs independent jobs concurrently up to the configured worker limit
        estimated_hours: 12-18
      - id: build-ci-system-m3
        name: Webhook & Queue System
        description: Handle webhooks and queue jobs for execution.
        acceptance_criteria:
        - Git webhook handler validates the webhook signature and parses push, PR, and tag event payloads
        - Job queue reliably stores pending jobs and delivers each job to exactly one worker for execution
        - Multiple workers execute jobs concurrently with the maximum concurrency enforced by the pool configuration
        - Rate limiting prevents a burst of webhook events from overwhelming the system by throttling queue intake
        - Priority scheduling allows critical pipelines (e.g., production deploys) to be processed before lower-priority jobs
        pitfalls:
        - Webhook replay attacks
        - Queue starvation
        - Resource exhaustion
        concepts:
        - Webhook security
        - Work queues
        - Concurrency control
        skills:
        - HMAC signature verification
        - Message queue implementation
        - Rate limiting and backpressure
        - Worker pool management
        - Idempotency key handling
        deliverables:
        - GitHub and GitLab webhook handler that receives push, pull-request, and tag events via HTTP POST
        - Job queue backed by Redis or a database that stores pending pipeline runs with their trigger context
        - Worker process pool that dequeues and executes pipeline jobs with configurable concurrency limits
        - Pipeline triggering logic that matches webhook events to pipeline configurations and creates pipeline runs
        estimated_hours: 10-15
      - id: build-ci-system-m4
        name: Web Dashboard
        description: Build a dashboard for viewing builds and logs.
        acceptance_criteria:
        - Build list view shows recent pipeline runs with their status, trigger, branch, and duration at a glance
        - Real-time log streaming displays job output in the browser within seconds of it being produced
        - Build status badges return SVG images showing pass/fail status embeddable in repository README files
        - Pipeline DAG visualization renders stages and jobs as nodes with arrows showing dependency relationships
        pitfalls:
        - WebSocket connection management
        - Large log performance
        - Badge caching
        concepts:
        - Real-time streaming
        - Data visualization
        - SVG generation
        skills:
        - WebSocket protocol implementation
        - Server-sent events streaming
        - Pagination for large datasets
        - SVG badge generation
        - Frontend state management
        deliverables:
        - Build history page displaying all pipeline runs with status, branch, commit, and duration information
        - Real-time log streaming that pushes step output to the browser as the job executes via WebSocket or SSE
        - Pipeline visualization as a directed acyclic graph showing stages, jobs, and their dependency relationships
        - Build artifact download page listing available artifacts with download links for each pipeline run
        estimated_hours: 10-15
    - id: build-observability-platform
      name: Build Your Own Observability Platform
      description: Logs/metrics/traces
      difficulty: expert
      estimated_hours: 80-120
      essence: Correlating heterogeneous telemetry signals through unified schema design, trace context propagation across process boundaries, and polyglot storage engines balancing columnar time-series compression with structured event indexing and span graph traversal.
      why_important: Building a unified observability platform teaches you to design high-throughput data pipelines, implement correlation across disparate signal types, and architect storage systems that balance query performance with cost efficiency‚Äîskills directly applicable to monitoring infrastructure at scale.
      learning_outcomes:
      - Design unified data models with trace context propagation and exemplar linking across signal types
      - Implement high-throughput ingestion pipelines handling millions of data points per second
      - Build specialized storage backends for time-series metrics, structured logs, and span-based traces
      - Develop cross-signal query engines that correlate traces to metrics via exemplars and logs via trace IDs
      - Implement cardinality-aware indexing strategies for high-dimensional telemetry data
      - Design intelligent alerting systems that detect anomalies across correlated signals
      - Optimize data compression and retention policies for cost-effective long-term storage
      - Build real-time aggregation engines for metrics rollups and trace sampling decisions
      skills:
      - Distributed tracing architecture
      - Time-series database design
      - High-throughput data ingestion
      - Context propagation mechanisms
      - Cardinality management
      - Query optimization
      - Anomaly detection algorithms
      - OpenTelemetry protocol
      tags:
      - build-from-scratch
      - dashboards
      - devops
      - expert
      - framework
      - go
      - logs
      - rust
      - traces
      architecture_doc: architecture-docs/build-observability-platform/index.md
      languages:
        recommended:
        - Go
        - Rust
        also_possible:
        - Java
        - Python
      resources:
      - type: book
        name: Observability Engineering
        url: https://www.oreilly.com/library/view/observability-engineering/9781492076438/
      - type: article
        name: Three Pillars of Observability
        url: https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html
      - type: documentation
        name: OpenTelemetry Collector
        url: https://opentelemetry.io/docs/collector/
      prerequisites:
      - type: skill
        name: Distributed tracing
      - type: skill
        name: Metrics systems
      - type: skill
        name: Log aggregation
      - type: skill
        name: Time-series databases
      - type: skill
        name: Full-text search
      milestones:
      - id: build-observability-platform-m1
        name: Unified Data Model
        description: Design a unified data model that correlates logs, metrics, and traces.
        acceptance_criteria:
        - All telemetry signals share common resource attributes like service name and version
        - Logs can be queried by trace_id to find all logs for a request
        - Metrics include exemplar trace IDs linking to representative traces
        - Resource model identifies service name, version, instance, and environment
        - Data model is compatible with OpenTelemetry semantic conventions
        pitfalls:
        - Schema drift across signals
        - Missing correlation IDs
        - Cardinality explosion
        concepts:
        - Data modeling
        - Signal correlation
        - OpenTelemetry semantics
        skills:
        - Data schema design
        - Cross-signal correlation
        - OpenTelemetry protocol implementation
        - Temporal data modeling
        deliverables:
        - Common attribute schema shared across logs, metrics, and traces
        - Trace-log correlation linking log records to trace and span IDs
        - Metric-trace correlation using exemplars to connect metrics to traces
        - Resource attribution model mapping telemetry to service instances
        estimated_hours: 10-15
      - id: build-observability-platform-m2
        name: Data Ingestion Pipeline
        description: Build a high-throughput ingestion pipeline for all signal types.
        acceptance_criteria:
        - OTLP receiver accepts logs, metrics, and traces over gRPC protocol
        - Batch ingestion buffers up to configurable size before flushing to storage
        - Pipeline normalizes timestamps, resource attributes, and field names consistently
        - Backpressure returns appropriate error codes when ingestion capacity is exceeded
        - Pipeline processes at least 10,000 events per second on commodity hardware
        pitfalls:
        - Data loss under load
        - Memory exhaustion
        - Head-of-line blocking
        concepts:
        - Data pipelines
        - Backpressure
        - Protocol buffers
        skills:
        - High-throughput data processing
        - Protocol buffer serialization
        - Backpressure handling
        - Memory-efficient buffering
        - Concurrent pipeline processing
        deliverables:
        - OTLP protocol receiver accepting gRPC and HTTP telemetry data
        - Batch ingestion pipeline buffering and flushing incoming data
        - Data normalization layer converting varied formats to unified model
        - Backpressure mechanism throttling producers when pipeline is saturated
        estimated_hours: 15-20
      - id: build-observability-platform-m3
        name: Multi-Signal Storage
        description: Implement storage backends optimized for each signal type.
        acceptance_criteria:
        - Log storage supports full-text search returning results within 500ms for 1M logs
        - Metric storage handles high-cardinality time series with configurable retention
        - Trace storage reconstructs full span trees from individual span records
        - Cross-signal queries retrieve correlated logs and traces for a given time range
        - Storage engines support configurable retention policies with automatic cleanup
        pitfalls:
        - Query performance at scale
        - Storage costs
        - Data consistency across stores
        concepts:
        - Inverted indexes
        - Time-series compression
        - Tiered storage
        skills:
        - Time-series database optimization
        - Inverted index implementation
        - Data compression algorithms
        - Multi-backend storage architecture
        deliverables:
        - Log storage engine with full-text search indexing capability
        - Time-series database for metric storage with downsampling support
        - Trace storage with span tree reconstruction and querying
        - Cross-signal index enabling correlation queries across data types
        estimated_hours: 20-30
      - id: build-observability-platform-m4
        name: Unified Query Interface
        description: Build a query interface that can correlate across all signal types.
        acceptance_criteria:
        - Query language supports filtering by time range, service, and attributes
        - Cross-signal queries join traces with associated logs in a single request
        - Query API returns paginated results with cursor-based navigation
        - Aggregate queries compute percentiles, rates, and histograms over metrics
        - Query execution completes within acceptable latency for interactive exploration
        pitfalls:
        - Query complexity explosion
        - Slow correlation queries
        - Missing correlation data
        concepts:
        - Query planning
        - Data correlation
        - Unified observability
        skills:
        - Query optimization
        - Cross-database joins
        - Distributed query execution
        - Query language design
        deliverables:
        - Query language supporting filters across logs, metrics, and traces
        - Cross-signal query engine joining data from multiple storage backends
        - Query API returning paginated results with streaming support
        - Query execution plan optimizer selecting efficient access paths
        estimated_hours: 15-25
      - id: build-observability-platform-m5
        name: Alerting & Anomaly Detection
        description: Implement intelligent alerting that correlates anomalies across signals.
        acceptance_criteria:
        - Alert rules trigger when configured threshold conditions are met
        - Anomaly detection identifies deviations from learned baseline patterns
        - Notifications route to correct channels based on alert severity and team
        - Alert deduplication suppresses duplicate notifications for the same incident
        pitfalls:
        - Alert fatigue
        - False positives
        - Missing root causes
        concepts:
        - Multi-signal correlation
        - Statistical anomaly detection
        - Root cause analysis
        skills:
        - Anomaly detection algorithms
        - Multi-dimensional correlation analysis
        - Statistical modeling
        - Alert rule engine design
        deliverables:
        - Multi-signal alert rules evaluating conditions across logs, metrics, and traces
        - Anomaly detection engine identifying unusual patterns in metric streams
        - Alert notification system with configurable channels and routing
        - Alert silencing and deduplication preventing notification fatigue
        estimated_hours: 20-30
- id: world-scale
  name: World-Scale Infrastructure (Final Boss)
  icon: üèÜ
  subdomains:
  - name: High-Scale Systems
  - name: Game Infrastructure
  - name: AI Platform Engineering
  - name: Distributed Reliability
  projects:
    beginner: []
    intermediate: []
    advanced: []
    expert:
    - id: ad-exchange-engine
      name: Lighthouse Ad-Exchange
      description: Real-time bidding engine handling 1M+ QPS with <10ms latency
      difficulty: expert
      estimated_hours: 80-120
      architecture_doc: architecture-docs/ad-exchange-engine/index.md
      languages:
        recommended:
        - Rust
        - C++
        - Go
      milestones:
      - id: ad-exchange-engine-m1
        name: The C10M Gateway
        description: Build a high-performance network listener capable of handling 10 million concurrent connections.
        acceptance_criteria:
        - Implement a zero-copy network stack using io_uring or DPDK
        - Achieve 1M+ packets per second throughput on a single core
        - Use lock-free ring buffers for inter-thread communication
        - Minimal memory allocations in the hot path (zero-alloc path)
      - id: ad-exchange-engine-m2
        name: Ultra-Low Latency Bidding
        description: Implement the RTB (Real-Time Bidding) logic that decides which ad to show in <5ms.
        acceptance_criteria:
        - In-memory auction logic using cache-aligned data structures
        - Integration with a low-latency KV store (e.g., local Aerospike or custom shared memory)
        - Support for complex targeting rules (geofencing, user segments) using bitsets
        - Tail latency (p99) remains under 10ms under heavy load
      - id: ad-exchange-engine-m3
        name: Fraud Detection at Scale
        description: Real-time stream processing to detect and filter out bot traffic.
        acceptance_criteria:
        - Implement a sliding window anomaly detection algorithm
        - Handle 100GB/s of telemetry data using SIMD-accelerated filtering
        - Integrate with a distributed cache for shared blacklists
        - False positive rate below 0.01%
      - id: ad-exchange-engine-m4
        name: Global State & Settlement
        description: Sync bidding state across multiple regions and handle financial settlement.
        acceptance_criteria:
        - Implement eventually consistent budget tracking across regions
        - Handle late-arriving events and deduplication
        - Generate immutable financial logs for auditing
        - Support regional failover without overspending budgets
    - id: mmo-engine-core
      name: Chronos MMO Engine
      description: High-performance multiplayer engine with spatial partitioning and rollback netcode
      difficulty: expert
      estimated_hours: 100-150
      architecture_doc: architecture-docs/mmo-engine-core/index.md
      languages:
        recommended:
        - C++
        - Rust
        - Zig
      milestones:
      - id: mmo-engine-core-m1
        name: Spatial Intelligence
        description: Implement massive-scale spatial partitioning for entity management.
        acceptance_criteria:
        - Build a dynamic Octree or Hashed Spatial Grid supporting 100k+ entities
        - Efficient k-nearest neighbor (k-NN) queries for entity interaction
        - Support for seamless world partitioning (no loading screens)
        - SIMD-accelerated frustum culling and collision checks
      - id: mmo-engine-core-m2
        name: Interest Management (AoI)
        description: Optimize bandwidth by only sending relevant data to each player.
        acceptance_criteria:
        - Implement Area of Interest (AoI) filtering with custom shapes
        - Support for different update frequencies based on distance or importance
        - Delta compression for entity property updates
        - Handle "edge cases" like fast-moving objects crossing partition boundaries
      - id: mmo-engine-core-m3
        name: Competitive Netcode
        description: Build a low-latency UDP protocol with reliability and ordering.
        acceptance_criteria:
        - Custom UDP layer with bit-packing and packet fragmentation
        - Congestion control optimized for real-time games (low jitter)
        - State snapshot interpolation for smooth movement
        - Encryption and anti-tamper verification for every packet
      - id: mmo-engine-core-m4
        name: The Rollback Boss
        description: Implement client-side prediction and server reconciliation (rollback).
        acceptance_criteria:
        - Deterministic simulation on both client and server
        - Ability to "rewind" and "resimulate" multiple frames in <1ms
        - Smooth visual correction of mispredictions
        - Latency compensation for combat/interaction (lag compensation)
    - id: mlops-pipeline-auto
      name: Aether MLOps Pipeline
      description: Autonomous ML infrastructure with real-time feature stores and auto-retraining
      difficulty: expert
      estimated_hours: 70-100
      architecture_doc: architecture-docs/mlops-pipeline-auto/index.md
      languages:
        recommended:
        - Python
        - Go
        - Rust
      milestones:
      - id: mlops-pipeline-auto-m1
        name: Real-time Feature Store
        description: Build a low-latency store for ML features with point-in-time correctness.
        acceptance_criteria:
        - Support for both online (Redis/memory) and offline (Parquet/S3) feature retrieval
        - Automated feature computation from streaming data (Kafka/Flink)
        - Prevent "data leakage" with strict point-in-time joins
        - Sub-20ms retrieval for 1000+ features simultaneously
      - id: mlops-pipeline-auto-m2
        name: Distributed Training Orchestrator
        description: Manage large-scale training jobs across clusters of GPUs.
        acceptance_criteria:
        - Automated GPU partitioning and scheduling
        - Fault-tolerant checkpointing and resume logic
        - Integration with distributed training frameworks (PyTorch DDP/Ray)
        - Real-time logging of hardware metrics (TFLOPS, memory bandwidth)
      - id: mlops-pipeline-auto-m3
        name: Dynamic Model Serving
        description: Deploy models with auto-scaling and intelligent traffic routing.
        acceptance_criteria:
        - Support for Canary and Blue-Green deployments with automatic rollback
        - Multi-model inference (A/B testing) at the edge
        - Request batching and GPU inference optimization (TensorRT/ONNX)
        - Latency-aware auto-scaling based on custom metrics
      - id: mlops-pipeline-auto-m4
        name: The Feedback Loop
        description: Implement automated monitoring and self-retraining pipelines.
        acceptance_criteria:
        - Real-time data drift and concept drift detection
        - Automated trigger for re-training when performance drops
        - Automated evaluation and comparison between new and old models
        - Full lineage tracking from data source to deployed model
    - id: distributed-consensus-raft
      name: Titan Consensus Engine
      description: Industrial-grade Raft implementation for mission-critical distributed state
      difficulty: expert
      estimated_hours: 60-90
      architecture_doc: architecture-docs/distributed-consensus-raft/index.md
      languages:
        recommended:
        - Rust
        - Go
        - Zig
      milestones:
      - id: distributed-consensus-raft-m1
        name: Election & Safety
        description: Core Raft leader election and heartbeat mechanism.
        acceptance_criteria:
        - Randomized election timeouts to prevent split votes
        - Guaranteed "one leader per term" safety
        - Persistent term and vote state to survive crashes
        - Functional RPC layer for RequestVote and AppendEntries
      - id: distributed-consensus-raft-m2
        name: Log Replication
        description: Synchronize state across the cluster with consistency guarantees.
        acceptance_criteria:
        - Log matching property implementation
        - Handling of follower crashes and network partitions
        - Committing entries only after majority replication
        - Conflict resolution for logs from previous terms
      - id: distributed-consensus-raft-m3
        name: Log Compaction
        description: Manage log growth with snapshots and state machine installs.
        acceptance_criteria:
        - Incremental snapshotting of the state machine
        - InstallSnapshot RPC for catching up slow followers
        - Safe log truncation after snapshotting
        - Minimal impact on system availability during snapshotting
      - id: distributed-consensus-raft-m4
        name: Membership Changes
        description: Safely add or remove nodes from the cluster without downtime.
        acceptance_criteria:
        - Implement Joint Consensus (two-phase membership change)
        - Prevention of "disruptive servers" during transition
        - Automated catch-up of new nodes before they join the quorum
        - No loss of availability during cluster expansion/shrinking
