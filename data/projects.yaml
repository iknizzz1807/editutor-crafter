domains:
- id: app-dev
  name: Application Development
  icon: ðŸŒ
  subdomains:
  - name: Web Frontend
  - name: Web Backend
  - name: Full-stack
  - name: Mobile
  - name: Desktop & CLI
  projects:
    beginner:
    - id: todo-app
      name: Todo App
      description: Basic CRUD application
      detailed: true
    - id: weather-app
      name: Weather App
      description: API consumption, async data
      detailed: true
    - id: portfolio-site
      name: Portfolio Website
      description: Static site, responsive design
      detailed: true
    - id: calculator
      name: Calculator
      description: UI state management
      detailed: true
    - id: rest-api-design
      name: Production REST API
      detailed: true
    intermediate:
    - id: blog-platform
      name: Blog Platform
      description: Full CRUD, auth, markdown
      detailed: true
    - id: chat-app
      name: Real-time Chat
      description: WebSocket, real-time updates
      detailed: true
    - id: ecommerce-basic
      name: E-commerce (Basic)
      description: Cart, checkout flow
      detailed: true
    - id: grpc-service
      name: gRPC Microservice
      detailed: true
    - id: circuit-breaker
      name: Circuit Breaker Pattern
      detailed: true
    advanced:
    - id: social-network
      name: Social Network
      description: Feed, followers, notifications
      detailed: true
    - id: video-streaming
      name: Video Streaming
      description: HLS, adaptive bitrate
      detailed: true
    - id: api-gateway
      name: API Gateway
      detailed: true
    - id: distributed-tracing
      name: Distributed Tracing System
      detailed: true
    expert:
    - id: build-react
      name: Build Your Own React
      description: Virtual DOM, reconciliation, hooks, fiber
      detailed: true
    - id: build-bundler
      name: Build Your Own Bundler
      description: Module resolution, tree shaking, code splitting
      detailed: true
    - id: build-spreadsheet
      name: Build Your Own Spreadsheet
      description: Excel-like app with formulas
      detailed: true
    - id: build-web-framework
      name: Build Your Own Web Framework
      description: Express/Django clone
      detailed: true
- id: systems
  name: Systems & Low-Level
  icon: âš™ï¸
  subdomains:
  - name: Systems Programming
  - name: Networking
  - name: Operating Systems
  projects:
    beginner:
    - id: cat-clone
      name: Cat Clone
      description: File reading, stdout
      detailed: true
    - id: wc-clone
      name: Wc Clone
      description: Line/word/byte counting
      detailed: true
    - id: grep-clone
      name: Grep Clone
      description: Pattern matching basics
      detailed: true
    - id: file-copy
      name: File Copy (cp clone)
      description: File I/O, permissions
      detailed: true
    intermediate:
    - id: shell-basic
      name: Shell (Basic)
      description: Pipes, redirects
      detailed: true
    - id: http-server-basic
      name: HTTP Server (Basic)
      description: Static file serving
      detailed: true
    - id: memory-pool
      name: Memory Pool Allocator
      description: Fixed-size block allocation
      detailed: true
    - id: process-spawner
      name: Process Spawner
      description: fork/exec, process lifecycle
      detailed: true
      bridge: true
    - id: signal-handler
      name: Signal Handler
      description: SIGINT, SIGTERM, signal masks
      detailed: true
      bridge: true
    advanced:
    - id: http2-server
      name: HTTP/2 Server
      description: Multiplexing, HPACK
      detailed: true
    - id: container-basic
      name: Container (Basic)
      description: Namespaces isolation
      detailed: true
    - id: mini-shell
      name: Mini Shell
      description: Job control, background processes
      detailed: true
      bridge: true
    - id: virtual-memory-sim
      name: Virtual Memory Simulator
      description: Page tables, TLB
      detailed: true
      bridge: true
    expert:
    - id: build-docker
      name: Build Your Own Docker
      description: Container runtime with namespaces, cgroups
      detailed: true
      languages:
      - Go
      - Rust
      - C
    - id: build-shell
      name: Build Your Own Shell
      description: Full Unix shell with job control
      detailed: true
      languages:
      - C
      - Rust
      - Go
    - id: build-allocator
      name: Build Your Own Memory Allocator
      description: malloc/free implementation
      detailed: true
      languages:
      - C
      - Rust
      - Zig
    - id: build-os
      name: Build Your Own OS
      description: Operating system kernel
      detailed: true
      languages:
      - C
      - Rust
      - Zig
    - id: build-tcp-stack
      name: Build Your Own TCP/IP Stack
      description: Network stack implementation
      detailed: true
      languages:
      - C
      - Rust
      - Go
- id: data-storage
  name: Data & Storage
  icon: ðŸ—„ï¸
  subdomains:
  - name: Databases
  - name: Data Engineering
  projects:
    beginner:
    - id: json-db
      name: JSON File Database
      description: File-based storage
      detailed: true
    - id: kv-memory
      name: In-memory Key-Value Store
      description: Hash map based
      detailed: true
    intermediate:
    - id: btree-impl
      name: B-tree Implementation
      description: Insert, delete, rebalance
      detailed: true
    - id: sql-parser
      name: SQL Parser
      description: SELECT, WHERE, JOIN
      detailed: true
    advanced:
    - id: query-optimizer
      name: Query Optimizer
      description: Cost estimation
      detailed: true
    - id: wal-impl
      name: WAL Implementation
      description: Write-ahead logging
      detailed: true
    expert:
    - id: build-redis
      name: Build Your Own Redis
      description: In-memory data store with RESP protocol
      detailed: true
    - id: build-sqlite
      name: Build Your Own SQLite
      description: Embedded SQL database
      detailed: true
    - id: build-kafka
      name: Build Your Own Kafka
      description: Distributed message queue
      detailed: true
- id: distributed
  name: Distributed & Cloud
  icon: â˜ï¸
  subdomains:
  - name: Distributed Systems
  - name: Cloud & DevOps
  - name: Consensus
  - name: Blockchain
  projects:
    beginner:
    - id: service-discovery
      name: Service Discovery
      description: Registry, health checks
      detailed: true
    - id: rpc-basic
      name: RPC Framework (Basic)
      description: Remote procedure calls
      detailed: true
      bridge: true
    intermediate:
    - id: load-balancer-basic
      name: Load Balancer (Basic)
      description: Round-robin
      detailed: true
    - id: rate-limiter
      name: Rate Limiter
      description: Token bucket
      detailed: true
    - id: leader-election
      name: Leader Election
      description: Bully algorithm, ring election
      detailed: true
      bridge: true
    - id: replicated-log
      name: Replicated Log
      description: Append-only log, basic replication
      detailed: true
      bridge: true
    - id: vector-clocks
      name: Vector Clocks
      description: Logical time, causality
      detailed: true
      bridge: true
    advanced:
    - id: distributed-cache
      name: Distributed Cache
      description: Consistent hashing
      detailed: true
    - id: gossip-protocol
      name: Gossip Protocol
      description: Epidemic broadcast
      detailed: true
      bridge: true
    - id: 2pc-impl
      name: Two-Phase Commit
      description: Distributed transactions
      detailed: true
      bridge: true
    expert:
    - id: build-raft
      name: Build Your Own Raft
      description: Consensus algorithm
      detailed: true
      languages:
      - Go
      - Rust
      - Java
    - id: build-blockchain
      name: Build Your Own Blockchain
      description: Proof of work, P2P network
      detailed: true
      languages:
      - Python
      - Go
      - Rust
    - id: build-distributed-kv
      name: Build Your Own Distributed KV Store
      description: Partitioning, replication
      detailed: true
- id: ai-ml
  name: AI & Machine Learning
  icon: ðŸ§ 
  subdomains:
  - name: Classical ML
  - name: Deep Learning
  - name: NLP
  projects:
    beginner:
    - id: linear-regression
      name: Linear Regression
      description: Gradient descent
      detailed: true
    - id: knn
      name: KNN Classifier
      description: Distance metrics
      detailed: true
    - id: chatbot-intent
      name: Intent-Based Chatbot
      detailed: true
    intermediate:
    - id: neural-network-basic
      name: Neural Network (micrograd)
      description: Forward/backward pass
      detailed: true
    - id: word2vec
      name: Word Embeddings
      description: Skip-gram, CBOW
      detailed: true
    - id: rag-system
      name: RAG System (Retrieval Augmented Generation)
      detailed: true
    - id: semantic-search
      name: Semantic Search Engine
      detailed: true
    - id: recommendation-engine
      name: Recommendation Engine
      detailed: true
    advanced:
    - id: transformer-scratch
      name: Transformer from Scratch
      description: Attention mechanism
      detailed: true
    - id: gan
      name: GAN
      description: Generator, discriminator
      detailed: true
    - id: ai-agent-framework
      name: AI Agent Framework
      detailed: true
    - id: llm-eval-framework
      name: LLM Evaluation Framework
      detailed: true
    expert:
    - id: build-nn-framework
      name: Build Your Own Neural Network Framework
      description: PyTorch/TensorFlow clone
      detailed: true
    - id: build-transformer
      name: Build Your Own Transformer
      description: Full GPT implementation
      detailed: true
- id: game-dev
  name: Game Development
  icon: ðŸŽ®
  subdomains:
  - name: Game Programming
  - name: Graphics
  - name: Engine Development
  projects:
    beginner:
    - id: pong
      name: Pong
      description: Basic game loop
      detailed: true
    - id: snake
      name: Snake
      description: Grid movement
      detailed: true
    - id: tetris
      name: Tetris
      description: Rotation, line clearing
      detailed: true
    intermediate:
    - id: platformer
      name: Platformer
      description: Gravity, jumping
      detailed: true
    - id: topdown-shooter
      name: Top-down Shooter
      description: Enemies, projectiles
      detailed: true
    advanced:
    - id: software-3d
      name: Software 3D Renderer
      description: No GPU, pure math
      detailed: true
    - id: ecs-arch
      name: ECS Architecture
      description: Entity-component-system
      detailed: true
    expert:
    - id: build-game-engine
      name: Build Your Own Game Engine
      description: Full 2D/3D engine
      detailed: true
    - id: build-raytracer
      name: Build Your Own Ray Tracer
      description: Path tracing renderer
      detailed: true
- id: compilers
  name: Languages & Compilers
  icon: ðŸ“
  subdomains:
  - name: Parsing & Lexing
  - name: Interpreters
  - name: Compilers
  - name: Runtime Systems
  projects:
    beginner:
    - id: calculator-parser
      name: Calculator Parser
      description: Arithmetic expressions
      detailed: true
    - id: json-parser
      name: JSON Parser
      description: Recursive descent
      detailed: true
    - id: tokenizer
      name: Tokenizer/Lexer
      description: Token types, state machine
      detailed: true
      bridge: true
    intermediate:
    - id: lisp-interp
      name: Lisp Interpreter
      description: S-expressions, eval
      detailed: true
    - id: bytecode-vm
      name: Bytecode VM
      description: Stack-based
      detailed: true
    - id: ast-builder
      name: AST Builder
      description: Parse expressions to AST
      detailed: true
      bridge: true
    - id: ast-interpreter
      name: AST Tree-Walking Interpreter
      description: Evaluate AST directly
      detailed: true
      bridge: true
    advanced:
    - id: type-checker
      name: Type Checker
      description: Type inference
      detailed: true
    - id: simple-gc
      name: Simple GC
      description: Mark-sweep
      detailed: true
    - id: bytecode-compiler
      name: Bytecode Compiler
      description: AST to bytecode
      detailed: true
      bridge: true
    - id: wasm-emitter
      name: WebAssembly Emitter
      description: Emit .wasm from AST
      detailed: true
      bridge: true
    expert:
    - id: build-interpreter
      name: Build Your Own Interpreter (Lox)
      description: Crafting Interpreters
      detailed: true
      languages:
      - Java
      - C
      - Rust
      - Go
    - id: build-gc
      name: Build Your Own Garbage Collector
      description: Memory management
      detailed: true
      languages:
      - C
      - Rust
      - Zig
    - id: build-regex
      name: Build Your Own Regex Engine
      description: NFA/DFA, Thompson construction
      detailed: true
      languages:
      - C
      - Rust
      - Go
      - Python
- id: security
  name: Security
  icon: ðŸ”
  subdomains:
  - name: Cryptography
  - name: Web Security
  projects:
    beginner:
    - id: hash-impl
      name: Hash Function
      description: SHA-256 from spec
      detailed: true
    - id: password-hashing
      name: Password Hashing
      description: bcrypt, salt
      detailed: true
    intermediate:
    - id: aes-impl
      name: AES Implementation
      description: Block cipher
      detailed: true
    - id: jwt-impl
      name: JWT Library
      description: Sign, verify
      detailed: true
    advanced:
    - id: https-client
      name: HTTPS Client
      description: TLS handshake
      detailed: true
    expert:
    - id: build-tls
      name: Build Your Own TLS
      description: TLS 1.3 implementation
      detailed: true
- id: cs-fundamentals
  name: CS Fundamentals
  icon: ðŸŽ“
  subdomains:
  - name: Data Structures
  - name: Algorithms
  projects:
    beginner:
    - id: linked-list
      name: Linked List
      description: Single, double, circular
      detailed: true
    - id: stack-queue
      name: Stack & Queue
      description: Array and linked
      detailed: true
    intermediate:
    - id: bst
      name: Binary Search Tree
      description: Insert, delete, traversal
      detailed: true
    - id: hash-table
      name: Hash Table
      description: Chaining, open addressing
      detailed: true
    advanced:
    - id: red-black-tree
      name: Red-Black Tree
      description: Balanced tree
      detailed: true
    - id: graph-algos
      name: Graph Algorithms
      description: BFS, DFS, Dijkstra
      detailed: true
    expert:
    - id: build-btree
      name: Build Your Own B-tree
      description: Disk-friendly tree
      detailed: true
- id: specialized
  name: Specialized
  icon: ðŸ”§
  subdomains:
  - name: Developer Tools
  - name: Advanced Networking
  - name: Embedded & Emulation
  projects:
    beginner:
    - id: markdown-renderer
      name: Markdown Renderer
      description: Markdown to HTML converter with CommonMark spec
      detailed: true
    - id: hexdump
      name: Hexdump Utility
      description: Binary file viewer with hex/ASCII display
      detailed: true
    - id: diff-tool
      name: Diff Tool
      description: Text diff using LCS algorithm
      detailed: true
    intermediate:
    - id: config-parser
      name: Config File Parser
      description: Parse INI, TOML, and YAML formats
      detailed: true
    - id: packet-sniffer
      name: Packet Sniffer
      description: Network packet capture and protocol parsing
      detailed: true
    - id: chip8-emulator
      name: CHIP-8 Emulator
      description: Simple VM emulator for CHIP-8 games
      detailed: true
    advanced:
    - id: disassembler
      name: x86 Disassembler
      description: Instruction decoder for x86/x64 binaries
      detailed: true
    - id: terminal-multiplexer
      name: Terminal Multiplexer
      description: Simple tmux-like terminal manager
      detailed: true
    - id: protocol-buffer
      name: Protocol Buffer
      description: Binary serialization format implementation
      detailed: true
    expert:
    - id: build-git
      name: Build Your Own Git
      description: Version control
      detailed: true
    - id: build-text-editor
      name: Build Your Own Text Editor
      description: Vim-like editor
      detailed: true
    - id: build-bittorrent
      name: Build Your Own BitTorrent
      description: P2P file sharing
      detailed: true
    - id: build-dns
      name: Build Your Own DNS Server
      description: Recursive resolver
      detailed: true
    - id: build-debugger
      name: Build Your Own Debugger
      description: GDB-like
      detailed: true
    - id: build-lsp
      name: Build Your Own LSP Server
      description: Language server protocol
      detailed: true
    - id: build-emulator
      name: Build Your Own Emulator
      description: NES/GameBoy/CHIP-8
      detailed: true
    - id: build-browser
      name: Build Your Own Browser
      description: Browser engine
      detailed: true
- id: software-engineering
  name: Software Engineering Practices
  icon: âœ…
  subdomains:
  - name: Testing & Quality
  - name: CI/CD
  - name: Observability
  - name: Engineering Practices
  projects:
    beginner:
    - id: unit-testing-basics
      name: Unit Testing Fundamentals
      description: pytest/jest basics
      detailed: true
    - id: git-workflow
      name: Git Workflow Mastery
      description: Branching, PRs, rebasing
      detailed: true
    - id: documentation-project
      name: Documentation Project
      description: README, API docs
      detailed: true
    intermediate:
    - id: tdd-kata
      name: TDD Kata Series
      description: Red-green-refactor
      detailed: true
    - id: ci-pipeline
      name: CI Pipeline Setup
      description: GitHub Actions/Jenkins
      detailed: true
    - id: logging-structured
      name: Structured Logging
      description: JSON logs, aggregation
      detailed: true
    - id: code-review-practice
      name: Code Review Practice
      description: Review real PRs
      detailed: true
    advanced:
    - id: integration-testing
      name: Integration Testing Suite
      description: Test containers, mocking
      detailed: true
    - id: cd-deployment
      name: CD with Blue-Green Deployment
      description: Zero-downtime deploys
      detailed: true
    - id: metrics-dashboard
      name: Metrics & Alerting Dashboard
      description: Prometheus, Grafana
      detailed: true
    - id: distributed-tracing
      name: Distributed Tracing
      description: OpenTelemetry, Jaeger
      detailed: true
    expert:
    - id: build-test-framework
      name: Build Your Own Test Framework
      description: pytest/jest clone
      detailed: true
    - id: build-ci-system
      name: Build Your Own CI System
      description: Pipeline executor
      detailed: true
    - id: build-observability-platform
      name: Build Your Own Observability Platform
      description: Logs/metrics/traces
      detailed: true
expert_projects:
  2pc-impl:
    id: 2pc-impl
    name: Two-Phase Commit
    description: Implement 2PC for distributed transactions. Learn atomic commitment and coordinator recovery.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Distributed systems
    - Transaction concepts
    - Failure handling
    languages:
      recommended:
      - Go
      - Java
      - Python
      also_possible:
      - Rust
      - Erlang
    resources:
    - name: 2PC Paper
      url: https://www.cs.princeton.edu/courses/archive/fall16/cos418/papers/bernstein-ch7.pdf
      type: paper
    - name: Distributed Transactions
      url: https://www.the-paper-trail.org/post/2014-10-16-consensus-protocols-two-phase-commit/
      type: article
    milestones:
    - id: 1
      name: Transaction Log
      description: Implement durable transaction log for recovery.
      acceptance_criteria:
      - Write-ahead logging
      - Log PREPARE, COMMIT, ABORT records
      - Survive crashes
      - Recovery from log
      hints:
        level1: Log must be durable (fsync). Write before sending messages.
        level2: 'Log format: [tx_id, state, participants]. State = PREPARE/COMMIT/ABORT.'
        level3: "import json\nimport os\n\nclass TransactionLog:\n    def __init__(self, path: str):\n        self.path =\
          \ path\n        self.file = open(path, 'a+')\n    \n    def log(self, tx_id: str, state: str, data: dict = None):\n\
          \        record = {'tx_id': tx_id, 'state': state, 'data': data or {}}\n        self.file.write(json.dumps(record)\
          \ + '\\n')\n        self.file.flush()\n        os.fsync(self.file.fileno())\n    \n    def recover(self) -> dict:\n\
          \        '''Return {tx_id: last_state}'''\n        self.file.seek(0)\n        transactions = {}\n        for line\
          \ in self.file:\n            if line.strip():\n                record = json.loads(line)\n                transactions[record['tx_id']]\
          \ = record\n        return transactions\n\nclass Coordinator:\n    def __init__(self, log_path: str):\n        self.log\
          \ = TransactionLog(log_path)\n        self.pending = {}  # tx_id -> {participants, votes}\n    \n    def recover(self):\n\
          \        '''Recover after crash'''\n        transactions = self.log.recover()\n        for tx_id, record in transactions.items():\n\
          \            if record['state'] == 'PREPARE':\n                # Was in prepare phase, need to abort or retry\n\
          \                self.abort_transaction(tx_id, record['data']['participants'])\n            elif record['state']\
          \ == 'COMMIT':\n                # Committed but may not have notified all\n                self.send_decision(tx_id,\
          \ 'COMMIT', record['data']['participants'])"
      pitfalls:
      - Incomplete writes
      - Recovery order
      - Log truncation
      concepts:
      - Write-ahead logging
      - Durability
      - Crash recovery
      estimated_hours: 3-4
    - id: 2
      name: Prepare Phase
      description: Implement the voting/prepare phase.
      acceptance_criteria:
      - Coordinator sends PREPARE to all participants
      - Participants vote YES/NO
      - Participant logs vote before responding
      - Handle vote timeout
      hints:
        level1: 'PREPARE asks: can you commit? Participant checks and votes.'
        level2: Participant must log YES vote before responding (promise to commit if asked).
        level3: "class Coordinator:\n    def begin_2pc(self, tx_id: str, participants: list):\n        # Log prepare intent\n\
          \        self.log.log(tx_id, 'PREPARE', {'participants': participants})\n        \n        self.pending[tx_id] =\
          \ {\n            'participants': participants,\n            'votes': {},\n            'state': 'PREPARING'\n   \
          \     }\n        \n        # Send PREPARE to all participants\n        for p in participants:\n            self.send_to(p,\
          \ 'PREPARE', {'tx_id': tx_id})\n        \n        # Wait for votes (with timeout)\n        threading.Timer(5.0,\
          \ lambda: self.check_votes(tx_id)).start()\n    \n    def on_vote(self, participant: str, tx_id: str, vote: str):\n\
          \        if tx_id not in self.pending:\n            return\n        \n        self.pending[tx_id]['votes'][participant]\
          \ = vote\n        \n        # Check if all votes received\n        if len(self.pending[tx_id]['votes']) == len(self.pending[tx_id]['participants']):\n\
          \            self.decide(tx_id)\n\nclass Participant:\n    def on_prepare(self, tx_id: str):\n        if self.can_commit(tx_id):\n\
          \            # Log YES vote (promise)\n            self.log.log(tx_id, 'VOTE_YES')\n            self.send_to_coordinator('VOTE',\
          \ {'tx_id': tx_id, 'vote': 'YES'})\n        else:\n            self.log.log(tx_id, 'VOTE_NO')\n            self.send_to_coordinator('VOTE',\
          \ {'tx_id': tx_id, 'vote': 'NO'})"
      pitfalls:
      - Logging before response
      - Vote timeout
      - Participant crash after voting
      concepts:
      - Voting protocol
      - Prepare phase
      - Participant state
      estimated_hours: 4-5
    - id: 3
      name: Commit Phase
      description: Implement the decision/commit phase.
      acceptance_criteria:
      - All YES -> COMMIT, any NO -> ABORT
      - Log decision before sending
      - Send decision to all participants
      - Handle acknowledgment
      hints:
        level1: Decision is final once logged. Must eventually reach all participants.
        level2: Retry sending decision to failed participants.
        level3: "class Coordinator:\n    def decide(self, tx_id: str):\n        votes = self.pending[tx_id]['votes']\n   \
          \     participants = self.pending[tx_id]['participants']\n        \n        # All YES -> COMMIT, otherwise ABORT\n\
          \        all_yes = all(v == 'YES' for v in votes.values())\n        decision = 'COMMIT' if all_yes else 'ABORT'\n\
          \        \n        # Log decision BEFORE sending\n        self.log.log(tx_id, decision, {'participants': participants})\n\
          \        \n        # Send decision to all participants\n        self.send_decision(tx_id, decision, participants)\n\
          \    \n    def send_decision(self, tx_id: str, decision: str, participants: list):\n        pending_acks = set(participants)\n\
          \        \n        while pending_acks:\n            for p in list(pending_acks):\n                if self.send_to(p,\
          \ decision, {'tx_id': tx_id}):\n                    # Wait for ACK\n                    pass\n            time.sleep(1)\
          \  # Retry failed participants\n    \n    def on_ack(self, participant: str, tx_id: str):\n        # Remove from\
          \ pending, eventually log COMPLETE\n        pass\n\nclass Participant:\n    def on_commit(self, tx_id: str):\n \
          \       self.log.log(tx_id, 'COMMIT')\n        self.apply_transaction(tx_id)\n        self.send_ack(tx_id)\n   \
          \ \n    def on_abort(self, tx_id: str):\n        self.log.log(tx_id, 'ABORT')\n        self.rollback_transaction(tx_id)\n\
          \        self.send_ack(tx_id)"
      pitfalls:
      - Decision before logging
      - Lost messages
      - Participant uncertainty
      concepts:
      - Commit protocol
      - Atomic commitment
      - Decision logging
      estimated_hours: 4-5
    - id: 4
      name: Failure Recovery
      description: Handle coordinator and participant failures.
      acceptance_criteria:
      - Coordinator crash recovery
      - Participant timeout handling
      - Participant crash recovery
      - Blocking scenario handling
      hints:
        level1: 'On recovery: check log. If COMMIT logged, re-send. If only PREPARE, can abort.'
        level2: 'Participant in doubt (voted YES, no decision): must wait or ask coordinator.'
        level3: "class Participant:\n    def recover(self):\n        '''Recovery after crash'''\n        transactions = self.log.recover()\n\
          \        for tx_id, record in transactions.items():\n            state = record['state']\n            \n       \
          \     if state == 'VOTE_YES':\n                # I voted YES but don't know decision\n                # Must ask\
          \ coordinator\n                self.query_coordinator(tx_id)\n            elif state == 'COMMIT':\n            \
          \    # Was committed, ensure applied\n                if not self.is_applied(tx_id):\n                    self.apply_transaction(tx_id)\n\
          \            elif state == 'ABORT':\n                # Was aborted, ensure rolled back\n                if not self.is_rolled_back(tx_id):\n\
          \                    self.rollback_transaction(tx_id)\n    \n    def query_coordinator(self, tx_id: str):\n    \
          \    '''Ask coordinator for decision (may block if coordinator down)'''\n        response = self.send_to_coordinator('QUERY',\
          \ {'tx_id': tx_id})\n        if response:\n            if response['decision'] == 'COMMIT':\n                self.on_commit(tx_id)\n\
          \            else:\n                self.on_abort(tx_id)\n        else:\n            # Coordinator unreachable -\
          \ must wait\n            # This is the blocking problem of 2PC\n            self.in_doubt.add(tx_id)\n\nclass Coordinator:\n\
          \    def on_query(self, tx_id: str):\n        '''Participant asking for decision'''\n        transactions = self.log.recover()\n\
          \        if tx_id in transactions:\n            return {'decision': transactions[tx_id]['state']}\n        else:\n\
          \            # Never heard of it, must have aborted\n            return {'decision': 'ABORT'}"
      pitfalls:
      - In-doubt blocking
      - Coordinator single point of failure
      - Network partitions
      concepts:
      - Crash recovery
      - Blocking protocols
      - Failure handling
      estimated_hours: 4-6
  aes-impl:
    id: aes-impl
    name: AES Encryption Implementation
    description: Implement AES encryption from scratch. Learn symmetric cryptography, block ciphers, and modes of operation.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Binary operations
    - Modular arithmetic
    - Basic cryptography concepts
    languages:
      recommended:
      - C
      - Rust
      - Python
      also_possible:
      - Go
      - Java
    resources:
    - name: FIPS 197 (AES Spec)
      url: https://csrc.nist.gov/publications/detail/fips/197/final
      type: paper
    - name: A Stick Figure Guide to AES
      url: http://www.moserware.com/2009/09/stick-figure-guide-to-advanced.html
      type: article
    milestones:
    - id: 1
      name: Galois Field Arithmetic
      description: Implement GF(2^8) operations used in AES.
      acceptance_criteria:
      - Addition (XOR)
      - Multiplication in GF(2^8)
      - Multiplicative inverse
      - Pre-compute lookup tables
      hints:
        level1: GF(2^8) addition = XOR. Multiplication involves polynomial math.
        level2: Use irreducible polynomial x^8 + x^4 + x^3 + x + 1 (0x11B).
        level3: "def gf_add(a, b):\n    '''Addition in GF(2^8) is XOR'''\n    return a ^ b\n\ndef gf_mul(a, b):\n    '''Multiplication\
          \ in GF(2^8) with irreducible polynomial 0x11B'''\n    p = 0\n    for _ in range(8):\n        if b & 1:\n      \
          \      p ^= a\n        hi_bit = a & 0x80\n        a = (a << 1) & 0xFF\n        if hi_bit:\n            a ^= 0x1B\
          \  # Reduce by x^8 + x^4 + x^3 + x + 1\n        b >>= 1\n    return p\n\n# Pre-compute lookup tables for efficiency\n\
          MUL2 = [gf_mul(i, 2) for i in range(256)]\nMUL3 = [gf_mul(i, 3) for i in range(256)]\n\ndef gf_inverse(a):\n   \
          \ '''Find multiplicative inverse using extended Euclidean algorithm'''\n    if a == 0:\n        return 0\n    #\
          \ Or use: a^254 = a^(-1) in GF(2^8)\n    result = a\n    for _ in range(6):\n        result = gf_mul(result, result)\n\
          \        result = gf_mul(result, a)\n    return gf_mul(result, result)"
      pitfalls:
      - Overflow handling
      - Wrong irreducible polynomial
      - Off-by-one in tables
      concepts:
      - Galois fields
      - Polynomial arithmetic
      - Lookup tables
      estimated_hours: 3-4
    - id: 2
      name: AES Core Operations
      description: Implement the four AES round operations.
      acceptance_criteria:
      - SubBytes (S-box substitution)
      - ShiftRows
      - MixColumns
      - AddRoundKey
      hints:
        level1: AES state is 4x4 byte matrix. Each round applies 4 transformations.
        level2: 'SubBytes: apply S-box to each byte. ShiftRows: rotate rows.'
        level3: "# S-box (pre-computed)\nSBOX = [\n    0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5,\n    # ... (256 values\
          \ total)\n]\n\ndef sub_bytes(state):\n    '''Apply S-box to each byte'''\n    for i in range(4):\n        for j\
          \ in range(4):\n            state[i][j] = SBOX[state[i][j]]\n\ndef shift_rows(state):\n    '''Rotate rows: row 0\
          \ by 0, row 1 by 1, row 2 by 2, row 3 by 3'''\n    state[1] = state[1][1:] + state[1][:1]\n    state[2] = state[2][2:]\
          \ + state[2][:2]\n    state[3] = state[3][3:] + state[3][:3]\n\ndef mix_columns(state):\n    '''Mix each column\
          \ using GF(2^8) matrix multiplication'''\n    for j in range(4):\n        a = [state[i][j] for i in range(4)]\n\
          \        state[0][j] = MUL2[a[0]] ^ MUL3[a[1]] ^ a[2] ^ a[3]\n        state[1][j] = a[0] ^ MUL2[a[1]] ^ MUL3[a[2]]\
          \ ^ a[3]\n        state[2][j] = a[0] ^ a[1] ^ MUL2[a[2]] ^ MUL3[a[3]]\n        state[3][j] = MUL3[a[0]] ^ a[1] ^\
          \ a[2] ^ MUL2[a[3]]\n\ndef add_round_key(state, round_key):\n    '''XOR state with round key'''\n    for i in range(4):\n\
          \        for j in range(4):\n            state[i][j] ^= round_key[i][j]"
      pitfalls:
      - State matrix orientation
      - ShiftRows direction
      - MixColumns coefficients
      concepts:
      - Substitution
      - Permutation
      - Diffusion
      estimated_hours: 4-6
    - id: 3
      name: Key Expansion
      description: Implement the key schedule for AES-128/192/256.
      acceptance_criteria:
      - Expand 128-bit key to round keys
      - RotWord and SubWord functions
      - Round constants (Rcon)
      - Support different key sizes
      hints:
        level1: Key expansion creates 11 round keys (128-bit) from original key.
        level2: Each round key derived from previous. Special transform every 4th word.
        level3: "RCON = [0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80, 0x1B, 0x36]\n\ndef rot_word(word):\n    '''Rotate\
          \ 4-byte word left by 1'''\n    return word[1:] + word[:1]\n\ndef sub_word(word):\n    '''Apply S-box to each byte\
          \ of word'''\n    return [SBOX[b] for b in word]\n\ndef key_expansion(key):\n    '''Expand 16-byte key to 44 words\
          \ (11 round keys)'''\n    # Key is 16 bytes = 4 words\n    w = [list(key[i:i+4]) for i in range(0, 16, 4)]\n   \
          \ \n    for i in range(4, 44):  # Generate 40 more words\n        temp = w[i-1].copy()\n        if i % 4 == 0:\n\
          \            temp = sub_word(rot_word(temp))\n            temp[0] ^= RCON[i // 4 - 1]\n        w.append([w[i-4][j]\
          \ ^ temp[j] for j in range(4)])\n    \n    # Convert to round keys (4 words each)\n    round_keys = []\n    for\
          \ r in range(11):\n        round_key = [[w[r*4 + j][i] for j in range(4)] for i in range(4)]\n        round_keys.append(round_key)\n\
          \    \n    return round_keys"
      pitfalls:
      - Rcon indexing
      - Word vs byte ordering
      - Key length handling
      concepts:
      - Key schedule
      - Round constants
      - Key expansion
      estimated_hours: 3-4
    - id: 4
      name: Encryption & Modes
      description: Complete encryption and implement cipher modes.
      acceptance_criteria:
      - Full AES encrypt function
      - ECB mode (for learning)
      - CBC mode with IV
      - Padding (PKCS7)
      hints:
        level1: 10 rounds for AES-128. Last round skips MixColumns.
        level2: 'CBC: each block XORed with previous ciphertext before encryption.'
        level3: "def aes_encrypt_block(plaintext, key):\n    '''Encrypt 16-byte block'''\n    # Convert to state matrix (column-major)\n\
          \    state = [[plaintext[i + 4*j] for j in range(4)] for i in range(4)]\n    round_keys = key_expansion(key)\n \
          \   \n    # Initial round\n    add_round_key(state, round_keys[0])\n    \n    # Main rounds\n    for r in range(1,\
          \ 10):\n        sub_bytes(state)\n        shift_rows(state)\n        mix_columns(state)\n        add_round_key(state,\
          \ round_keys[r])\n    \n    # Final round (no MixColumns)\n    sub_bytes(state)\n    shift_rows(state)\n    add_round_key(state,\
          \ round_keys[10])\n    \n    # Convert back to bytes\n    return bytes([state[i][j] for j in range(4) for i in range(4)])\n\
          \ndef pkcs7_pad(data, block_size=16):\n    pad_len = block_size - (len(data) % block_size)\n    return data + bytes([pad_len]\
          \ * pad_len)\n\ndef aes_cbc_encrypt(plaintext, key, iv):\n    '''CBC mode encryption'''\n    plaintext = pkcs7_pad(plaintext)\n\
          \    ciphertext = b''\n    prev_block = iv\n    \n    for i in range(0, len(plaintext), 16):\n        block = bytes(a\
          \ ^ b for a, b in zip(plaintext[i:i+16], prev_block))\n        encrypted = aes_encrypt_block(block, key)\n     \
          \   ciphertext += encrypted\n        prev_block = encrypted\n    \n    return ciphertext"
      pitfalls:
      - ECB mode vulnerabilities
      - IV reuse
      - Padding oracle attacks
      concepts:
      - Block cipher modes
      - CBC
      - Padding
      estimated_hours: 4-6
  ast-builder:
    id: ast-builder
    name: AST Builder (Parser)
    description: Build a parser that converts tokens into an Abstract Syntax Tree. Learn grammar rules and tree structures.
    difficulty: intermediate
    estimated_hours: 12-20
    prerequisites:
    - Tokenizer/Lexer
    - Recursion
    - Tree data structures
    languages:
      recommended:
      - Python
      - JavaScript
      - Java
      also_possible:
      - Go
      - Rust
      - C
    resources:
    - name: Crafting Interpreters - Parsing
      url: https://craftinginterpreters.com/parsing-expressions.html
      type: book
    - name: Pratt Parsing
      url: https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html
      type: article
    milestones:
    - id: 1
      name: AST Node Definitions
      description: Define AST node types for your language.
      acceptance_criteria:
      - Expression nodes (literals, binary, unary)
      - Statement nodes (if, while, return)
      - Use visitor pattern or tagged unions
      - Include source location in nodes
      hints:
        level1: AST = tree of nodes, each representing a language construct.
        level2: Use inheritance or tagged unions. Visitor pattern for traversal.
        level3: "from dataclasses import dataclass\nfrom typing import List, Optional, Any\n\n@dataclass\nclass Expr:\n  \
          \  '''Base expression class'''\n    pass\n\n@dataclass\nclass Literal(Expr):\n    value: Any\n\n@dataclass\nclass\
          \ Identifier(Expr):\n    name: str\n\n@dataclass\nclass Binary(Expr):\n    left: Expr\n    operator: str\n    right:\
          \ Expr\n\n@dataclass\nclass Unary(Expr):\n    operator: str\n    operand: Expr\n\n@dataclass\nclass Call(Expr):\n\
          \    callee: Expr\n    arguments: List[Expr]\n\n@dataclass\nclass Stmt:\n    '''Base statement class'''\n    pass\n\
          \n@dataclass\nclass ExprStmt(Stmt):\n    expression: Expr\n\n@dataclass\nclass VarDecl(Stmt):\n    name: str\n \
          \   initializer: Optional[Expr]\n\n@dataclass\nclass If(Stmt):\n    condition: Expr\n    then_branch: Stmt\n   \
          \ else_branch: Optional[Stmt]\n\n@dataclass\nclass While(Stmt):\n    condition: Expr\n    body: Stmt\n\n@dataclass\n\
          class Block(Stmt):\n    statements: List[Stmt]\n\n@dataclass\nclass Function(Stmt):\n    name: str\n    params:\
          \ List[str]\n    body: List[Stmt]"
      pitfalls:
      - Missing node types
      - No location info
      - Mutable vs immutable nodes
      concepts:
      - AST nodes
      - Expression vs statement
      - Tree representation
      estimated_hours: 2-3
    - id: 2
      name: Recursive Descent Parser
      description: Implement recursive descent parsing for expressions.
      acceptance_criteria:
      - One function per grammar rule
      - Handle operator precedence
      - Parse function calls
      - Consume tokens correctly
      hints:
        level1: Each grammar rule = one parsing function. Lower precedence = called first.
        level2: expression -> equality -> comparison -> term -> factor -> unary -> primary
        level3: "class Parser:\n    def __init__(self, tokens):\n        self.tokens = tokens\n        self.current = 0\n\
          \    \n    def peek(self):\n        return self.tokens[self.current]\n    \n    def is_at_end(self):\n        return\
          \ self.peek().type == TokenType.EOF\n    \n    def advance(self):\n        if not self.is_at_end():\n          \
          \  self.current += 1\n        return self.tokens[self.current - 1]\n    \n    def check(self, type):\n        if\
          \ self.is_at_end():\n            return False\n        return self.peek().type == type\n    \n    def match(self,\
          \ *types):\n        for type in types:\n            if self.check(type):\n                self.advance()\n     \
          \           return True\n        return False\n    \n    def consume(self, type, message):\n        if self.check(type):\n\
          \            return self.advance()\n        raise ParseError(message, self.peek())\n    \n    # Grammar: expression\
          \ -> equality\n    def expression(self):\n        return self.equality()\n    \n    # equality -> comparison (('=='\
          \ | '!=') comparison)*\n    def equality(self):\n        expr = self.comparison()\n        while self.match(TokenType.EQUAL_EQUAL,\
          \ TokenType.BANG_EQUAL):\n            op = self.tokens[self.current - 1].value\n            right = self.comparison()\n\
          \            expr = Binary(expr, op, right)\n        return expr\n    \n    # comparison -> term (('<' | '>' | '<='\
          \ | '>=') term)*\n    def comparison(self):\n        expr = self.term()\n        while self.match(TokenType.LESS,\
          \ TokenType.GREATER, TokenType.LESS_EQUAL, TokenType.GREATER_EQUAL):\n            op = self.tokens[self.current\
          \ - 1].value\n            right = self.term()\n            expr = Binary(expr, op, right)\n        return expr\n\
          \    \n    # term -> factor (('+' | '-') factor)*\n    def term(self):\n        expr = self.factor()\n        while\
          \ self.match(TokenType.PLUS, TokenType.MINUS):\n            op = self.tokens[self.current - 1].value\n         \
          \   right = self.factor()\n            expr = Binary(expr, op, right)\n        return expr"
      pitfalls:
      - Wrong precedence order
      - Left recursion
      - Infinite loops
      concepts:
      - Recursive descent
      - Operator precedence
      - Grammar rules
      estimated_hours: 4-6
    - id: 3
      name: Statement Parsing
      description: Parse statements and declarations.
      acceptance_criteria:
      - Variable declarations
      - If/else statements
      - While loops
      - Block statements with scoping
      hints:
        level1: statement = exprStmt | ifStmt | whileStmt | block | declaration
        level2: 'Block: ''{'' statement* ''}''. Declaration: ''var'' NAME (''='' expr)? '';'''
        level3: "def statement(self):\n    if self.match(TokenType.IF):\n        return self.if_statement()\n    if self.match(TokenType.WHILE):\n\
          \        return self.while_statement()\n    if self.match(TokenType.LBRACE):\n        return Block(self.block())\n\
          \    if self.match(TokenType.RETURN):\n        return self.return_statement()\n    return self.expression_statement()\n\
          \ndef if_statement(self):\n    self.consume(TokenType.LPAREN, \"Expect '(' after 'if'.\")\n    condition = self.expression()\n\
          \    self.consume(TokenType.RPAREN, \"Expect ')' after condition.\")\n    \n    then_branch = self.statement()\n\
          \    else_branch = None\n    if self.match(TokenType.ELSE):\n        else_branch = self.statement()\n    \n    return\
          \ If(condition, then_branch, else_branch)\n\ndef while_statement(self):\n    self.consume(TokenType.LPAREN, \"Expect\
          \ '(' after 'while'.\")\n    condition = self.expression()\n    self.consume(TokenType.RPAREN, \"Expect ')' after\
          \ condition.\")\n    body = self.statement()\n    return While(condition, body)\n\ndef block(self):\n    statements\
          \ = []\n    while not self.check(TokenType.RBRACE) and not self.is_at_end():\n        statements.append(self.declaration())\n\
          \    self.consume(TokenType.RBRACE, \"Expect '}' after block.\")\n    return statements\n\ndef declaration(self):\n\
          \    if self.match(TokenType.VAR):\n        return self.var_declaration()\n    if self.match(TokenType.FUN):\n \
          \       return self.function('function')\n    return self.statement()\n\ndef var_declaration(self):\n    name =\
          \ self.consume(TokenType.IDENTIFIER, 'Expect variable name.').value\n    initializer = None\n    if self.match(TokenType.EQUAL):\n\
          \        initializer = self.expression()\n    self.consume(TokenType.SEMICOLON, \"Expect ';' after variable declaration.\"\
          )\n    return VarDecl(name, initializer)"
      pitfalls:
      - Dangling else ambiguity
      - Missing semicolons
      - Block scope boundaries
      concepts:
      - Statement parsing
      - Control flow
      - Declarations
      estimated_hours: 4-5
    - id: 4
      name: Error Recovery
      description: Implement error handling and recovery.
      acceptance_criteria:
      - Report multiple errors (don't stop at first)
      - Synchronize after errors
      - Meaningful error messages
      - Track error locations
      hints:
        level1: 'Panic mode: on error, skip tokens until synchronization point.'
        level2: 'Sync points: statement boundaries (semicolons, keywords like if/while/class).'
        level3: "class ParseError(Exception):\n    def __init__(self, message, token):\n        self.message = message\n \
          \       self.token = token\n        super().__init__(f'[line {token.line}] Error at {repr(token.value)}: {message}')\n\
          \nclass Parser:\n    def __init__(self, tokens):\n        self.tokens = tokens\n        self.current = 0\n     \
          \   self.errors = []\n        self.had_error = False\n    \n    def error(self, message, token=None):\n        token\
          \ = token or self.peek()\n        error = ParseError(message, token)\n        self.errors.append(error)\n      \
          \  self.had_error = True\n        return error\n    \n    def synchronize(self):\n        '''Skip tokens until we\
          \ reach a statement boundary'''\n        self.advance()\n        \n        while not self.is_at_end():\n       \
          \     # After semicolon, we're at statement boundary\n            if self.tokens[self.current - 1].type == TokenType.SEMICOLON:\n\
          \                return\n            \n            # These keywords start statements\n            if self.peek().type\
          \ in (\n                TokenType.CLASS, TokenType.FUN, TokenType.VAR,\n                TokenType.FOR, TokenType.IF,\
          \ TokenType.WHILE,\n                TokenType.RETURN\n            ):\n                return\n            \n   \
          \         self.advance()\n    \n    def declaration(self):\n        try:\n            if self.match(TokenType.VAR):\n\
          \                return self.var_declaration()\n            return self.statement()\n        except ParseError:\n\
          \            self.synchronize()\n            return None"
      pitfalls:
      - Stopping at first error
      - Bad sync points
      - Cascading errors
      concepts:
      - Error recovery
      - Panic mode
      - Synchronization
      estimated_hours: 2-3
  ast-interpreter:
    id: ast-interpreter
    name: AST Tree-Walking Interpreter
    description: Build an interpreter that directly evaluates the AST. Learn environments, scoping, and evaluation.
    difficulty: intermediate
    estimated_hours: 12-20
    prerequisites:
    - AST Builder
    - Recursion
    - Environment/scope concepts
    languages:
      recommended:
      - Python
      - JavaScript
      - Java
      also_possible:
      - Go
      - Rust
    resources:
    - name: Crafting Interpreters - Evaluating
      url: https://craftinginterpreters.com/evaluating-expressions.html
      type: book
    - name: SICP - Metacircular Evaluator
      url: https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-26.html
      type: book
    milestones:
    - id: 1
      name: Expression Evaluation
      description: Evaluate arithmetic and comparison expressions.
      acceptance_criteria:
      - Literals evaluate to themselves
      - Binary operators (+, -, *, /, <, >, ==)
      - Unary operators (-, !)
      - Parentheses for grouping
      hints:
        level1: 'Recursive evaluate: check node type, recursively evaluate children, apply operation.'
        level2: Use visitor pattern or isinstance checks for different node types.
        level3: "class Interpreter:\n    def evaluate(self, node):\n        method_name = f'eval_{type(node).__name__}'\n\
          \        method = getattr(self, method_name, self.generic_eval)\n        return method(node)\n    \n    def generic_eval(self,\
          \ node):\n        raise RuntimeError(f'No eval method for {type(node).__name__}')\n    \n    def eval_Literal(self,\
          \ node):\n        return node.value\n    \n    def eval_Binary(self, node):\n        left = self.evaluate(node.left)\n\
          \        right = self.evaluate(node.right)\n        \n        ops = {\n            '+': lambda a, b: a + b,\n  \
          \          '-': lambda a, b: a - b,\n            '*': lambda a, b: a * b,\n            '/': lambda a, b: a / b,\n\
          \            '<': lambda a, b: a < b,\n            '>': lambda a, b: a > b,\n            '==': lambda a, b: a ==\
          \ b,\n            '!=': lambda a, b: a != b,\n            'and': lambda a, b: a and b,\n            'or': lambda\
          \ a, b: a or b,\n        }\n        \n        if node.operator not in ops:\n            raise RuntimeError(f'Unknown\
          \ operator: {node.operator}')\n        \n        return ops[node.operator](left, right)\n    \n    def eval_Unary(self,\
          \ node):\n        operand = self.evaluate(node.operand)\n        if node.operator == '-':\n            return -operand\n\
          \        elif node.operator == '!':\n            return not operand"
      pitfalls:
      - Type errors at runtime
      - Division by zero
      - Short-circuit evaluation
      concepts:
      - Tree-walking
      - Evaluation
      - Operator semantics
      estimated_hours: 3-4
    - id: 2
      name: Variables and Environment
      description: Implement variable binding and lookup.
      acceptance_criteria:
      - Environment maps names to values
      - Variable declaration (var/let)
      - Variable assignment
      - Nested scopes with parent lookup
      hints:
        level1: Environment = dict + parent pointer. Lookup checks local then parent.
        level2: New scope = new environment with current as parent.
        level3: "class Environment:\n    def __init__(self, parent=None):\n        self.values = {}\n        self.parent =\
          \ parent\n    \n    def define(self, name, value):\n        self.values[name] = value\n    \n    def get(self, name):\n\
          \        if name in self.values:\n            return self.values[name]\n        if self.parent:\n            return\
          \ self.parent.get(name)\n        raise RuntimeError(f'Undefined variable: {name}')\n    \n    def assign(self, name,\
          \ value):\n        if name in self.values:\n            self.values[name] = value\n            return\n        if\
          \ self.parent:\n            self.parent.assign(name, value)\n            return\n        raise RuntimeError(f'Undefined\
          \ variable: {name}')\n\nclass Interpreter:\n    def __init__(self):\n        self.environment = Environment()\n\
          \    \n    def eval_Identifier(self, node):\n        return self.environment.get(node.name)\n    \n    def eval_VarDecl(self,\
          \ node):\n        value = None\n        if node.initializer:\n            value = self.evaluate(node.initializer)\n\
          \        self.environment.define(node.name, value)\n    \n    def eval_Assignment(self, node):\n        value =\
          \ self.evaluate(node.value)\n        self.environment.assign(node.name, value)\n        return value\n    \n   \
          \ def eval_Block(self, node):\n        # New scope\n        previous = self.environment\n        self.environment\
          \ = Environment(parent=previous)\n        \n        try:\n            result = None\n            for stmt in node.statements:\n\
          \                result = self.evaluate(stmt)\n            return result\n        finally:\n            self.environment\
          \ = previous"
      pitfalls:
      - Scope restoration
      - Shadowing
      - Assignment vs declaration
      concepts:
      - Environments
      - Scoping
      - Variable binding
      estimated_hours: 3-4
    - id: 3
      name: Control Flow
      description: Implement if/else and loops.
      acceptance_criteria:
      - if/else statements
      - while loops
      - for loops (optional)
      - break/continue (optional)
      hints:
        level1: Evaluate condition, then evaluate appropriate branch.
        level2: 'Loops: evaluate condition, if true evaluate body, repeat.'
        level3: "def eval_If(self, node):\n    condition = self.evaluate(node.condition)\n    \n    if self.is_truthy(condition):\n\
          \        return self.evaluate(node.then_branch)\n    elif node.else_branch:\n        return self.evaluate(node.else_branch)\n\
          \    return None\n\ndef eval_While(self, node):\n    result = None\n    while self.is_truthy(self.evaluate(node.condition)):\n\
          \        try:\n            result = self.evaluate(node.body)\n        except BreakException:\n            break\n\
          \        except ContinueException:\n            continue\n    return result\n\ndef eval_For(self, node):\n    #\
          \ for (init; condition; increment) body\n    # Desugar to: { init; while (condition) { body; increment; } }\n  \
          \  previous = self.environment\n    self.environment = Environment(parent=previous)\n    \n    try:\n        if\
          \ node.initializer:\n            self.evaluate(node.initializer)\n        \n        while True:\n            if\
          \ node.condition:\n                if not self.is_truthy(self.evaluate(node.condition)):\n                    break\n\
          \            \n            try:\n                self.evaluate(node.body)\n            except BreakException:\n\
          \                break\n            except ContinueException:\n                pass\n            \n            if\
          \ node.increment:\n                self.evaluate(node.increment)\n    finally:\n        self.environment = previous\n\
          \ndef is_truthy(self, value):\n    if value is None:\n        return False\n    if isinstance(value, bool):\n  \
          \      return value\n    return True"
      pitfalls:
      - Infinite loops
      - Break outside loop
      - Truthiness rules
      concepts:
      - Control flow
      - Loops
      - Conditionals
      estimated_hours: 3-4
    - id: 4
      name: Functions
      description: Implement function definitions and calls.
      acceptance_criteria:
      - Function declarations
      - Function calls with arguments
      - Return statements
      - Closures (capture environment)
      hints:
        level1: Function = (params, body, closure_env). Call = new env with args bound.
        level2: Closure captures defining environment, not calling environment.
        level3: "class LoxFunction:\n    def __init__(self, declaration, closure):\n        self.declaration = declaration\n\
          \        self.closure = closure  # Environment when defined\n    \n    def call(self, interpreter, arguments):\n\
          \        # Create new environment for this call\n        environment = Environment(parent=self.closure)\n      \
          \  \n        # Bind parameters to arguments\n        for i, param in enumerate(self.declaration.params):\n     \
          \       environment.define(param, arguments[i])\n        \n        try:\n            interpreter.execute_block(self.declaration.body,\
          \ environment)\n        except ReturnException as ret:\n            return ret.value\n        \n        return None\n\
          \    \n    def arity(self):\n        return len(self.declaration.params)\n\nclass Interpreter:\n    def eval_FunctionDecl(self,\
          \ node):\n        function = LoxFunction(node, self.environment)\n        self.environment.define(node.name, function)\n\
          \    \n    def eval_Call(self, node):\n        callee = self.evaluate(node.callee)\n        arguments = [self.evaluate(arg)\
          \ for arg in node.arguments]\n        \n        if not hasattr(callee, 'call'):\n            raise RuntimeError('Can\
          \ only call functions')\n        \n        if len(arguments) != callee.arity():\n            raise RuntimeError(f'Expected\
          \ {callee.arity()} arguments but got {len(arguments)}')\n        \n        return callee.call(self, arguments)\n\
          \    \n    def eval_Return(self, node):\n        value = None\n        if node.value:\n            value = self.evaluate(node.value)\n\
          \        raise ReturnException(value)"
      pitfalls:
      - Closure capture
      - Return from nested function
      - Argument count
      concepts:
      - Functions
      - Closures
      - Call stack
      estimated_hours: 4-5
  blog-platform:
    id: blog-platform
    name: Blog Platform
    description: Build a full-featured blog platform with authentication, markdown support, and CRUD operations. Learn full-stack
      web development fundamentals.
    difficulty: intermediate
    estimated_hours: 25-35
    prerequisites:
    - HTML/CSS/JavaScript
    - Basic backend knowledge (Node.js or Python)
    - Database basics (SQL or MongoDB)
    languages:
      recommended:
      - JavaScript
      - Python
      - TypeScript
      also_possible:
      - Go
      - Ruby
      - PHP
    resources:
    - name: Build a Blog with Next.js
      url: https://nextjs.org/learn/basics/create-nextjs-app
      type: tutorial
    - name: Flask Mega-Tutorial
      url: https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world
      type: tutorial
    milestones:
    - id: 1
      name: Project Setup & Database Schema
      description: Set up the project structure and design the database.
      acceptance_criteria:
      - Project initialized with chosen framework
      - Database connection established
      - User table (id, email, password_hash, name)
      - Post table (id, title, content, author_id, created_at, updated_at)
      - Migration system in place
      hints:
        level1: Start with SQLite for development, switch to PostgreSQL for production.
        level2: Use an ORM (Prisma, SQLAlchemy, TypeORM) for database operations.
        level3: "// Prisma schema example\nmodel User {\n  id       Int    @id @default(autoincrement())\n  email    String\
          \ @unique\n  name     String\n  password String\n  posts    Post[]\n}\nmodel Post {\n  id        Int      @id @default(autoincrement())\n\
          \  title     String\n  content   String\n  author    User     @relation(fields: [authorId], references: [id])\n\
          \  authorId  Int\n  createdAt DateTime @default(now())\n}"
      pitfalls:
      - Not indexing frequently queried columns
      - Storing passwords in plain text
      - Not using foreign key constraints
      concepts:
      - Database design
      - ORM usage
      - Migrations
      estimated_hours: 3-4
    - id: 2
      name: User Authentication
      description: Implement user registration and login with secure password handling.
      acceptance_criteria:
      - Registration with email/password
      - Password hashed with bcrypt
      - Login returns JWT or session
      - Protected routes require authentication
      - Logout functionality
      hints:
        level1: Use bcrypt with cost factor 10-12 for password hashing.
        level2: 'JWT: sign with secret, include user id and expiry.'
        level3: "app.post('/login', async (req, res) => {\n  const user = await User.findByEmail(req.body.email);\n  if (!user\
          \ || !await bcrypt.compare(req.body.password, user.password)) {\n    return res.status(401).json({ error: 'Invalid\
          \ credentials' });\n  }\n  const token = jwt.sign({ userId: user.id }, SECRET, { expiresIn: '7d' });\n  res.json({\
          \ token });\n});"
      pitfalls:
      - Storing JWT in localStorage (XSS vulnerable)
      - Not validating email format
      - Exposing whether email exists on failed login
      concepts:
      - Password hashing
      - JWT authentication
      - HTTP-only cookies
      - CSRF protection
      estimated_hours: 5-6
    - id: 3
      name: Blog CRUD Operations
      description: Implement create, read, update, delete for blog posts.
      acceptance_criteria:
      - Create post with title and content
      - View all posts (paginated)
      - View single post
      - Edit own posts only
      - Delete own posts only
      - Markdown support for content
      hints:
        level1: Use marked or markdown-it library for markdown rendering.
        level2: 'Pagination: OFFSET/LIMIT or cursor-based for large datasets.'
        level3: "app.get('/posts', async (req, res) => {\n  const page = parseInt(req.query.page) || 1;\n  const limit = 10;\n\
          \  const offset = (page - 1) * limit;\n  const [posts, total] = await Promise.all([\n    Post.findMany({ skip: offset,\
          \ take: limit, orderBy: { createdAt: 'desc' } }),\n    Post.count()\n  ]);\n  res.json({ posts, pagination: { page,\
          \ limit, total, pages: Math.ceil(total / limit) } });\n});"
      pitfalls:
      - XSS from rendering user markdown (sanitize HTML)
      - N+1 queries when loading posts with authors
      - Not checking ownership on edit/delete
      concepts:
      - CRUD operations
      - Authorization
      - Markdown rendering
      - Pagination
      estimated_hours: 6-8
    - id: 4
      name: Frontend UI
      description: Build the frontend interface for the blog.
      acceptance_criteria:
      - Homepage with post list
      - Post detail page
      - Login/register forms
      - Post editor with live preview
      - Responsive design
      hints:
        level1: Use a component library or build minimal components.
        level2: Split editor and preview into side-by-side panels.
        level3: "function PostEditor({ value, onChange }) {\n  return (\n    <div className=\"editor-container\">\n      <textarea\
          \ value={value} onChange={(e) => onChange(e.target.value)} className=\"editor\" />\n      <div className=\"preview\"\
          \ dangerouslySetInnerHTML={{ __html: marked(value) }} />\n    </div>\n  );\n}"
      pitfalls:
      - Not handling loading states
      - No error boundaries
      - SEO issues with client-side rendering
      concepts:
      - Component architecture
      - Form handling
      - State management
      estimated_hours: 6-8
  bst:
    id: bst
    name: Binary Search Tree
    description: Implement a binary search tree with standard operations. Learn tree traversals and recursive algorithms.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - Recursion
    - Trees basics
    languages:
      recommended:
      - Python
      - Java
      - C++
      also_possible:
      - Go
      - Rust
    resources:
    - name: BST - GeeksforGeeks
      url: https://www.geeksforgeeks.org/binary-search-tree-data-structure/
      type: tutorial
    - name: Visualgo BST
      url: https://visualgo.net/en/bst
      type: interactive
    milestones:
    - id: 1
      name: Insert and Search
      description: Implement basic insert and search operations.
      acceptance_criteria:
      - Insert maintaining BST property
      - Search returns node or null
      - Handle duplicates (left or ignore)
      - Recursive implementation
      hints:
        level1: 'BST property: left < node < right.'
        level2: 'Recursive: if less go left, if greater go right.'
        level3: "class Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = self.right =\
          \ None\n\nclass BST:\n    def __init__(self):\n        self.root = None\n    \n    def insert(self, value):\n  \
          \      self.root = self._insert(self.root, value)\n    \n    def _insert(self, node, value):\n        if node is\
          \ None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(node.left,\
          \ value)\n        elif value > node.value:\n            node.right = self._insert(node.right, value)\n        return\
          \ node\n    \n    def search(self, value):\n        return self._search(self.root, value)\n    \n    def _search(self,\
          \ node, value):\n        if node is None or node.value == value:\n            return node\n        if value < node.value:\n\
          \            return self._search(node.left, value)\n        return self._search(node.right, value)"
      pitfalls:
      - Not returning updated node
      - Handling empty tree
      - Duplicate handling inconsistency
      concepts:
      - BST property
      - Recursive tree operations
      - Node creation
      estimated_hours: 2-3
    - id: 2
      name: Traversals
      description: Implement tree traversal methods.
      acceptance_criteria:
      - Inorder traversal (sorted output)
      - Preorder traversal
      - Postorder traversal
      - Level-order (BFS) traversal
      - Iterative versions (optional)
      hints:
        level1: 'Inorder: left, node, right. Preorder: node, left, right.'
        level2: Level-order uses a queue.
        level3: "def inorder(self):\n    result = []\n    self._inorder(self.root, result)\n    return result\n\ndef _inorder(self,\
          \ node, result):\n    if node:\n        self._inorder(node.left, result)\n        result.append(node.value)\n  \
          \      self._inorder(node.right, result)\n\ndef level_order(self):\n    if not self.root:\n        return []\n \
          \   result = []\n    queue = [self.root]\n    while queue:\n        node = queue.pop(0)\n        result.append(node.value)\n\
          \        if node.left:\n            queue.append(node.left)\n        if node.right:\n            queue.append(node.right)\n\
          \    return result\n\n# Iterative inorder using stack\ndef inorder_iterative(self):\n    result = []\n    stack\
          \ = []\n    node = self.root\n    while stack or node:\n        while node:\n            stack.append(node)\n  \
          \          node = node.left\n        node = stack.pop()\n        result.append(node.value)\n        node = node.right\n\
          \    return result"
      pitfalls:
      - Stack overflow on deep trees
      - Queue vs stack confusion
      - Forgetting to visit all nodes
      concepts:
      - Tree traversals
      - Recursion vs iteration
      - BFS vs DFS
      estimated_hours: 2-3
    - id: 3
      name: Delete Operation
      description: Implement node deletion with all cases.
      acceptance_criteria:
      - Delete leaf node
      - Delete node with one child
      - Delete node with two children (successor)
      - Maintain BST property
      hints:
        level1: 'Three cases: leaf, one child, two children.'
        level2: 'Two children: replace with inorder successor (min of right subtree).'
        level3: "def delete(self, value):\n    self.root = self._delete(self.root, value)\n\ndef _delete(self, node, value):\n\
          \    if node is None:\n        return None\n    if value < node.value:\n        node.left = self._delete(node.left,\
          \ value)\n    elif value > node.value:\n        node.right = self._delete(node.right, value)\n    else:\n      \
          \  # Node to delete found\n        if node.left is None:\n            return node.right\n        if node.right is\
          \ None:\n            return node.left\n        # Two children: find inorder successor\n        successor = self._min_node(node.right)\n\
          \        node.value = successor.value\n        node.right = self._delete(node.right, successor.value)\n    return\
          \ node\n\ndef _min_node(self, node):\n    while node.left:\n        node = node.left\n    return node"
      pitfalls:
      - Not handling all cases
      - Memory leaks
      - Breaking parent links
      concepts:
      - Node deletion
      - Inorder successor
      - Tree restructuring
      estimated_hours: 2-3
    - id: 4
      name: Tree Properties
      description: Implement utility methods for tree properties.
      acceptance_criteria:
      - Height of tree
      - Size (node count)
      - Min and max values
      - Is valid BST check
      - Balance factor (optional)
      hints:
        level1: Height = 1 + max(left_height, right_height).
        level2: 'Valid BST: each node must be within a range.'
        level3: "def height(self):\n    return self._height(self.root)\n\ndef _height(self, node):\n    if node is None:\n\
          \        return -1  # or 0, depending on convention\n    return 1 + max(self._height(node.left), self._height(node.right))\n\
          \ndef is_valid_bst(self):\n    return self._is_valid(self.root, float('-inf'), float('inf'))\n\ndef _is_valid(self,\
          \ node, min_val, max_val):\n    if node is None:\n        return True\n    if node.value <= min_val or node.value\
          \ >= max_val:\n        return False\n    return (self._is_valid(node.left, min_val, node.value) and\n          \
          \  self._is_valid(node.right, node.value, max_val))\n\ndef size(self):\n    return self._size(self.root)\n\ndef\
          \ _size(self, node):\n    if node is None:\n        return 0\n    return 1 + self._size(node.left) + self._size(node.right)"
      pitfalls:
      - Height of empty tree
      - BST validation range
      - Counting correctly
      concepts:
      - Tree properties
      - Range validation
      - Recursive counting
      estimated_hours: 2-3
  btree-impl:
    id: btree-impl
    name: B-tree Implementation
    description: Implement a B-tree data structure. Learn disk-friendly tree structures used in databases and file systems.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Trees basics
    - Algorithm complexity
    languages:
      recommended:
      - C
      - Python
      - Rust
      also_possible:
      - Go
      - Java
    resources:
    - name: B-tree Wikipedia
      url: https://en.wikipedia.org/wiki/B-tree
      type: article
    - name: B-tree Visualization
      url: https://www.cs.usfca.edu/~galles/visualization/BTree.html
      type: interactive
    milestones:
    - id: 1
      name: B-tree Node Structure
      description: Define B-tree node with keys and children.
      acceptance_criteria:
      - Node holds up to 2t-1 keys
      - Node has up to 2t children
      - Keys are sorted within node
      - Leaf indicator
      hints:
        level1: t = minimum degree. Node has t-1 to 2t-1 keys (except root).
        level2: Children array has one more element than keys array.
        level3: "class BTreeNode:\n    def __init__(self, t, is_leaf=True):\n        self.t = t  # Minimum degree\n      \
          \  self.keys = []  # List of keys\n        self.children = []  # List of child nodes\n        self.is_leaf = is_leaf\n\
          \    \n    @property\n    def is_full(self):\n        return len(self.keys) == 2 * self.t - 1\n    \n    @property\n\
          \    def is_underflow(self):\n        return len(self.keys) < self.t - 1\n\nclass BTree:\n    def __init__(self,\
          \ t):\n        self.t = t  # Minimum degree\n        self.root = BTreeNode(t)"
      pitfalls:
      - Off-by-one in capacity
      - Not tracking leaf status
      - Children count mismatch
      concepts:
      - B-tree properties
      - Node capacity
      - Tree structure
      estimated_hours: 2-3
    - id: 2
      name: Search
      description: Implement search operation.
      acceptance_criteria:
      - Binary search within node
      - Recurse to appropriate child
      - Return key and node if found
      - Handle not found
      hints:
        level1: Find position where key would be, then check or descend.
        level2: Use bisect for binary search in Python.
        level3: "import bisect\n\ndef search(self, key, node=None):\n    if node is None:\n        node = self.root\n    \n\
          \    # Find position where key would be\n    i = bisect.bisect_left(node.keys, key)\n    \n    # Check if found\n\
          \    if i < len(node.keys) and node.keys[i] == key:\n        return (node, i)\n    \n    # If leaf, key not in tree\n\
          \    if node.is_leaf:\n        return None\n    \n    # Recurse to appropriate child\n    return self.search(key,\
          \ node.children[i])"
      pitfalls:
      - Index bounds
      - Leaf check
      - Key comparison
      concepts:
      - Binary search
      - Tree traversal
      - Key lookup
      estimated_hours: 2-3
    - id: 3
      name: Insert with Split
      description: Implement insertion with node splitting.
      acceptance_criteria:
      - Insert into non-full leaf
      - Split full nodes proactively
      - Split root to grow tree
      - Maintain sorted order
      hints:
        level1: Split before descending if child is full (proactive split).
        level2: 'Split: median goes to parent, left/right halves become children.'
        level3: "def insert(self, key):\n    root = self.root\n    if root.is_full:\n        # Create new root\n        new_root\
          \ = BTreeNode(self.t, is_leaf=False)\n        new_root.children.append(self.root)\n        self._split_child(new_root,\
          \ 0)\n        self.root = new_root\n    self._insert_non_full(self.root, key)\n\ndef _split_child(self, parent,\
          \ i):\n    t = self.t\n    child = parent.children[i]\n    new_node = BTreeNode(t, child.is_leaf)\n    \n    # Middle\
          \ key goes to parent\n    mid = t - 1\n    parent.keys.insert(i, child.keys[mid])\n    \n    # Right half goes to\
          \ new node\n    new_node.keys = child.keys[mid+1:]\n    child.keys = child.keys[:mid]\n    \n    if not child.is_leaf:\n\
          \        new_node.children = child.children[t:]\n        child.children = child.children[:t]\n    \n    parent.children.insert(i\
          \ + 1, new_node)\n\ndef _insert_non_full(self, node, key):\n    i = bisect.bisect_left(node.keys, key)\n    if node.is_leaf:\n\
          \        node.keys.insert(i, key)\n    else:\n        if node.children[i].is_full:\n            self._split_child(node,\
          \ i)\n            if key > node.keys[i]:\n                i += 1\n        self._insert_non_full(node.children[i],\
          \ key)"
      pitfalls:
      - Split at wrong position
      - Not updating root
      - Child index after split
      concepts:
      - Node splitting
      - Proactive split
      - Tree growth
      estimated_hours: 4-6
    - id: 4
      name: Delete with Rebalancing
      description: Implement deletion with borrowing and merging.
      acceptance_criteria:
      - Delete from leaf
      - Delete from internal node (predecessor/successor)
      - Borrow from sibling
      - Merge nodes when underflow
      hints:
        level1: 'Internal delete: replace with predecessor/successor, delete from leaf.'
        level2: Before descending, ensure child has >= t keys.
        level3: "def delete(self, key):\n    self._delete(self.root, key)\n    # Shrink tree if root is empty\n    if len(self.root.keys)\
          \ == 0 and not self.root.is_leaf:\n        self.root = self.root.children[0]\n\ndef _delete(self, node, key):\n\
          \    i = bisect.bisect_left(node.keys, key)\n    \n    if i < len(node.keys) and node.keys[i] == key:\n        if\
          \ node.is_leaf:\n            node.keys.pop(i)\n        else:\n            self._delete_internal(node, i)\n    elif\
          \ not node.is_leaf:\n        self._ensure_child_not_minimal(node, i)\n        # Recompute i after potential restructuring\n\
          \        i = bisect.bisect_left(node.keys, key)\n        if i > len(node.keys):\n            i = len(node.keys)\n\
          \        self._delete(node.children[i], key)\n\ndef _ensure_child_not_minimal(self, parent, i):\n    child = parent.children[i]\n\
          \    if len(child.keys) >= self.t:\n        return\n    # Try borrowing from siblings or merge\n    # ... (borrow\
          \ from left, borrow from right, or merge)"
      pitfalls:
      - Merge vs borrow decision
      - Predecessor vs successor choice
      - Root shrinking
      concepts:
      - Node merging
      - Sibling borrowing
      - Tree shrinking
      estimated_hours: 5-8
  build-allocator:
    id: build-allocator
    name: Build Your Own Memory Allocator
    description: Implement malloc/free with various allocation strategies.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - C programming
    - Pointers and memory
    - Data structures
    - System calls (sbrk, mmap)
    languages:
      recommended:
      - C
      - Rust
      - Zig
      also_possible: []
    resources:
    - type: article
      name: Implementing malloc
      url: https://moss.cs.iit.edu/cs351/slides/slides-malloc.pdf
    - type: book
      name: The C Programming Language
      url: https://en.wikipedia.org/wiki/The_C_Programming_Language
    - type: article
      name: Writing a Memory Allocator
      url: http://dmitrysoshnikov.com/compilers/writing-a-memory-allocator/
    milestones:
    - id: 1
      name: Basic Allocator with sbrk
      description: Implement simple bump allocator using sbrk.
      acceptance_criteria:
      - Request memory from OS via sbrk
      - Track allocated blocks
      - Basic malloc/free interface
      - Handle alignment
      hints:
        level1: Use sbrk(0) to get current break, sbrk(size) to extend heap.
        level2: Store metadata (size, free flag) before each block.
        level3: "## Basic Allocator\n\n```c\n#include <unistd.h>\n#include <stdint.h>\n\n// Block header\ntypedef struct block_header\
          \ {\n    size_t size;          // Size of block (not including header)\n    int is_free;          // 1 if free,\
          \ 0 if allocated\n    struct block_header *next;  // Next block in list\n} block_header_t;\n\n#define HEADER_SIZE\
          \ sizeof(block_header_t)\n#define ALIGN(size) (((size) + 7) & ~7)  // 8-byte alignment\n\nstatic block_header_t\
          \ *head = NULL;\nstatic block_header_t *tail = NULL;\n\n// Request memory from OS\nstatic block_header_t *request_space(size_t\
          \ size) {\n    block_header_t *block = sbrk(0);\n    void *request = sbrk(HEADER_SIZE + size);\n    \n    if (request\
          \ == (void*)-1) {\n        return NULL;  // sbrk failed\n    }\n    \n    block->size = size;\n    block->is_free\
          \ = 0;\n    block->next = NULL;\n    \n    if (tail) {\n        tail->next = block;\n    }\n    tail = block;\n\
          \    \n    if (!head) {\n        head = block;\n    }\n    \n    return block;\n}\n\nvoid *my_malloc(size_t size)\
          \ {\n    if (size == 0) return NULL;\n    \n    size = ALIGN(size);\n    \n    // First fit: find first free block\
          \ that fits\n    block_header_t *current = head;\n    while (current) {\n        if (current->is_free && current->size\
          \ >= size) {\n            current->is_free = 0;\n            return (void*)(current + 1);  // Return pointer after\
          \ header\n        }\n        current = current->next;\n    }\n    \n    // No free block found, request more space\n\
          \    block_header_t *block = request_space(size);\n    if (!block) return NULL;\n    \n    return (void*)(block\
          \ + 1);\n}\n\nvoid my_free(void *ptr) {\n    if (!ptr) return;\n    \n    block_header_t *block = (block_header_t*)ptr\
          \ - 1;\n    block->is_free = 1;\n}\n```"
      pitfalls:
      - Forgetting alignment
      - Memory leaks in metadata
      - Not handling sbrk failure
      concepts:
      - Heap management
      - Memory alignment
      - System calls
      estimated_hours: 8-12
    - id: 2
      name: Free List Management
      description: Implement efficient free block tracking.
      acceptance_criteria:
      - Explicit free list
      - Block splitting
      - Block coalescing
      - First-fit, best-fit, worst-fit strategies
      hints:
        level1: Maintain a linked list of only free blocks for faster allocation.
        level2: When freeing, merge adjacent free blocks to reduce fragmentation.
        level3: "## Free List with Coalescing\n\n```c\n// Split block if it's too large\nstatic void split_block(block_header_t\
          \ *block, size_t size) {\n    if (block->size >= size + HEADER_SIZE + 16) {  // Only split if remainder is useful\n\
          \        block_header_t *new_block = (block_header_t*)((char*)(block + 1) + size);\n        new_block->size = block->size\
          \ - size - HEADER_SIZE;\n        new_block->is_free = 1;\n        new_block->next = block->next;\n        \n   \
          \     block->size = size;\n        block->next = new_block;\n    }\n}\n\n// Coalesce adjacent free blocks\nstatic\
          \ void coalesce() {\n    block_header_t *current = head;\n    while (current && current->next) {\n        if (current->is_free\
          \ && current->next->is_free) {\n            // Merge with next block\n            current->size += HEADER_SIZE +\
          \ current->next->size;\n            current->next = current->next->next;\n            // Don't advance - check if\
          \ we can merge more\n        } else {\n            current = current->next;\n        }\n    }\n}\n\n// Best-fit\
          \ allocation\nvoid *my_malloc_bestfit(size_t size) {\n    size = ALIGN(size);\n    \n    block_header_t *best =\
          \ NULL;\n    block_header_t *current = head;\n    \n    while (current) {\n        if (current->is_free && current->size\
          \ >= size) {\n            if (!best || current->size < best->size) {\n                best = current;\n        \
          \        if (best->size == size) break;  // Perfect fit\n            }\n        }\n        current = current->next;\n\
          \    }\n    \n    if (best) {\n        split_block(best, size);\n        best->is_free = 0;\n        return (void*)(best\
          \ + 1);\n    }\n    \n    // Request new space...\n    return NULL;\n}\n\nvoid my_free_coalesce(void *ptr) {\n \
          \   if (!ptr) return;\n    \n    block_header_t *block = (block_header_t*)ptr - 1;\n    block->is_free = 1;\n  \
          \  \n    coalesce();\n}\n```"
      pitfalls:
      - Fragmentation from not coalescing
      - Splitting blocks too small
      - Corrupting free list pointers
      concepts:
      - Free lists
      - Fragmentation
      - Allocation strategies
      estimated_hours: 10-15
    - id: 3
      name: Segregated Free Lists
      description: Implement size-class based allocation for better performance.
      acceptance_criteria:
      - Multiple free lists by size class
      - Fast allocation for common sizes
      - Reduced fragmentation
      - Efficient small allocations
      hints:
        level1: Maintain separate free lists for different size ranges (16, 32, 64, 128... bytes).
        level2: Round up allocation size to next size class. Search only appropriate list.
        level3: "## Segregated Free Lists\n\n```c\n#define NUM_SIZE_CLASSES 10\n// Size classes: 16, 32, 64, 128, 256, 512,\
          \ 1024, 2048, 4096, larger\n\nstatic block_header_t *free_lists[NUM_SIZE_CLASSES] = {NULL};\n\nstatic int get_size_class(size_t\
          \ size) {\n    if (size <= 16) return 0;\n    if (size <= 32) return 1;\n    if (size <= 64) return 2;\n    if (size\
          \ <= 128) return 3;\n    if (size <= 256) return 4;\n    if (size <= 512) return 5;\n    if (size <= 1024) return\
          \ 6;\n    if (size <= 2048) return 7;\n    if (size <= 4096) return 8;\n    return 9;  // Larger allocations\n}\n\
          \nstatic size_t get_class_size(int class) {\n    static size_t sizes[] = {16, 32, 64, 128, 256, 512, 1024, 2048,\
          \ 4096, 0};\n    return sizes[class];\n}\n\n// Remove block from its free list\nstatic void remove_from_freelist(block_header_t\
          \ *block) {\n    int class = get_size_class(block->size);\n    \n    if (free_lists[class] == block) {\n       \
          \ free_lists[class] = block->next;\n    } else {\n        block_header_t *prev = free_lists[class];\n        while\
          \ (prev && prev->next != block) {\n            prev = prev->next;\n        }\n        if (prev) {\n            prev->next\
          \ = block->next;\n        }\n    }\n}\n\n// Add block to appropriate free list\nstatic void add_to_freelist(block_header_t\
          \ *block) {\n    int class = get_size_class(block->size);\n    block->next = free_lists[class];\n    free_lists[class]\
          \ = block;\n}\n\nvoid *segregated_malloc(size_t size) {\n    size = ALIGN(size);\n    int class = get_size_class(size);\n\
          \    \n    // Search in this class and larger\n    for (int i = class; i < NUM_SIZE_CLASSES; i++) {\n        block_header_t\
          \ *block = free_lists[i];\n        while (block) {\n            if (block->size >= size) {\n                remove_from_freelist(block);\n\
          \                block->is_free = 0;\n                // Could split if much larger\n                return (void*)(block\
          \ + 1);\n            }\n            block = block->next;\n        }\n    }\n    \n    // Request new space\n   \
          \ size_t alloc_size = (class < 9) ? get_class_size(class) : size;\n    return request_and_return(alloc_size);\n\
          }\n```"
      pitfalls:
      - Internal fragmentation from size classes
      - Complex bookkeeping
      - Cache unfriendly access patterns
      concepts:
      - Size classes
      - Segregated storage
      - Performance optimization
      estimated_hours: 10-15
    - id: 4
      name: Thread Safety & mmap
      description: Add thread safety and large allocation support.
      acceptance_criteria:
      - Lock-free or mutex-based thread safety
      - Use mmap for large allocations
      - Per-thread caches
      - Memory debugging support
      hints:
        level1: Use mutex to protect allocator state. For large allocs, use mmap directly.
        level2: mmap allocations can be munmapped directly, avoiding fragmentation.
        level3: "## Thread-Safe Allocator\n\n```c\n#include <pthread.h>\n#include <sys/mman.h>\n\n#define MMAP_THRESHOLD 128\
          \ * 1024  // Use mmap for > 128KB\n\nstatic pthread_mutex_t alloc_mutex = PTHREAD_MUTEX_INITIALIZER;\n\nvoid *thread_safe_malloc(size_t\
          \ size) {\n    if (size == 0) return NULL;\n    \n    // Large allocations: use mmap directly\n    if (size >= MMAP_THRESHOLD)\
          \ {\n        size_t total = size + HEADER_SIZE;\n        void *ptr = mmap(NULL, total, PROT_READ | PROT_WRITE,\n\
          \                        MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n        if (ptr == MAP_FAILED) return NULL;\n   \
          \     \n        block_header_t *block = ptr;\n        block->size = size;\n        block->is_free = 0;\n       \
          \ block->next = NULL;  // Mark as mmap'd (could use flag)\n        \n        return (void*)(block + 1);\n    }\n\
          \    \n    // Small allocations: use segregated lists with mutex\n    pthread_mutex_lock(&alloc_mutex);\n    void\
          \ *result = segregated_malloc(size);\n    pthread_mutex_unlock(&alloc_mutex);\n    \n    return result;\n}\n\nvoid\
          \ thread_safe_free(void *ptr) {\n    if (!ptr) return;\n    \n    block_header_t *block = (block_header_t*)ptr -\
          \ 1;\n    \n    // Check if this was mmap'd (large allocation)\n    if (block->size >= MMAP_THRESHOLD) {\n     \
          \   munmap(block, block->size + HEADER_SIZE);\n        return;\n    }\n    \n    pthread_mutex_lock(&alloc_mutex);\n\
          \    block->is_free = 1;\n    add_to_freelist(block);\n    pthread_mutex_unlock(&alloc_mutex);\n}\n\n// Debug: check\
          \ for memory leaks\nvoid debug_print_stats() {\n    pthread_mutex_lock(&alloc_mutex);\n    \n    size_t total_allocated\
          \ = 0;\n    size_t total_free = 0;\n    int num_blocks = 0;\n    \n    block_header_t *current = head;\n    while\
          \ (current) {\n        num_blocks++;\n        if (current->is_free) {\n            total_free += current->size;\n\
          \        } else {\n            total_allocated += current->size;\n        }\n        current = current->next;\n\
          \    }\n    \n    printf(\"Blocks: %d, Allocated: %zu, Free: %zu\\n\",\n           num_blocks, total_allocated,\
          \ total_free);\n    \n    pthread_mutex_unlock(&alloc_mutex);\n}\n```"
      pitfalls:
      - Lock contention
      - Deadlocks
      - Memory leaks from lost blocks
      concepts:
      - Thread safety
      - Virtual memory
      - Memory debugging
      estimated_hours: 12-18
  build-bittorrent:
    id: build-bittorrent
    name: Build Your Own BitTorrent
    description: Build a BitTorrent client with P2P file sharing.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Networking (TCP/UDP)
    - Concurrency
    - File I/O
    - Bencode format
    languages:
      recommended:
      - Go
      - Rust
      - Python
      also_possible:
      - JavaScript
      - Java
    resources:
    - type: specification
      name: BitTorrent Protocol
      url: https://www.bittorrent.org/beps/bep_0003.html
    - type: tutorial
      name: Building a BitTorrent Client
      url: https://blog.jse.li/posts/torrent/
    - type: interactive
      name: CodeCrafters BitTorrent
      url: https://app.codecrafters.io/courses/bittorrent/overview
    milestones:
    - id: 1
      name: Torrent File Parsing
      description: Parse .torrent files and extract metadata.
      acceptance_criteria:
      - Bencode decoder
      - Extract announce URL
      - Extract file info
      - Calculate info hash
      hints:
        level1: 'Bencode: i42e=int, 4:spam=string, l...e=list, d...e=dict'
        level2: Info hash = SHA1 of bencoded info dict. Used to identify torrent.
        level3: "## Torrent Parsing\n\n```python\nimport hashlib\n\ndef decode_bencode(data: bytes, pos=0):\n    if data[pos:pos+1]\
          \ == b'i':  # Integer\n        end = data.index(b'e', pos)\n        return int(data[pos+1:end]), end + 1\n    \n\
          \    elif data[pos:pos+1] == b'l':  # List\n        result = []\n        pos += 1\n        while data[pos:pos+1]\
          \ != b'e':\n            item, pos = decode_bencode(data, pos)\n            result.append(item)\n        return result,\
          \ pos + 1\n    \n    elif data[pos:pos+1] == b'd':  # Dict\n        result = {}\n        pos += 1\n        while\
          \ data[pos:pos+1] != b'e':\n            key, pos = decode_bencode(data, pos)\n            value, pos = decode_bencode(data,\
          \ pos)\n            result[key] = value\n        return result, pos + 1\n    \n    else:  # String (length:content)\n\
          \        colon = data.index(b':', pos)\n        length = int(data[pos:colon])\n        start = colon + 1\n     \
          \   return data[start:start+length], start + length\n\nclass Torrent:\n    def __init__(self, filepath):\n     \
          \   with open(filepath, 'rb') as f:\n            data = f.read()\n        \n        self.meta, _ = decode_bencode(data)\n\
          \        self.announce = self.meta[b'announce'].decode()\n        \n        info = self.meta[b'info']\n        self.name\
          \ = info[b'name'].decode()\n        self.piece_length = info[b'piece length']\n        self.pieces = info[b'pieces']\
          \  # Concatenated SHA1 hashes\n        \n        if b'files' in info:  # Multi-file torrent\n            self.files\
          \ = [\n                {'path': '/'.join(p.decode() for p in f[b'path']),\n                 'length': f[b'length']}\n\
          \                for f in info[b'files']\n            ]\n            self.total_length = sum(f['length'] for f in\
          \ self.files)\n        else:  # Single file\n            self.files = [{'path': self.name, 'length': info[b'length']}]\n\
          \            self.total_length = info[b'length']\n        \n        # Calculate info hash\n        info_start =\
          \ data.index(b'4:info') + 6\n        info_end = len(data) - 1  # Before final 'e'\n        self.info_hash = hashlib.sha1(data[info_start:info_end]).digest()\n\
          \    \n    @property\n    def num_pieces(self):\n        return len(self.pieces) // 20\n    \n    def piece_hash(self,\
          \ index):\n        return self.pieces[index*20:(index+1)*20]\n```"
      pitfalls:
      - Bencode parsing edge cases
      - Wrong info dict boundaries
      - Binary vs text strings
      concepts:
      - Bencode format
      - Hashing
      - Torrent metadata
      estimated_hours: 8-12
    - id: 2
      name: Tracker Communication
      description: Communicate with tracker to get peer list.
      acceptance_criteria:
      - HTTP tracker protocol
      - Announce request
      - Parse peer list
      - Periodic re-announce
      hints:
        level1: GET request with info_hash, peer_id, port, uploaded, downloaded, left.
        level2: Response is bencoded dict with 'peers' (compact or dict format).
        level3: "## Tracker Client\n\n```python\nimport requests\nimport struct\nimport random\nimport string\n\nclass TrackerClient:\n\
          \    def __init__(self, torrent):\n        self.torrent = torrent\n        self.peer_id = self.generate_peer_id()\n\
          \        self.port = 6881\n        self.uploaded = 0\n        self.downloaded = 0\n    \n    def generate_peer_id(self):\n\
          \        # Format: -XX0000-xxxxxxxxxxxx\n        return f\"-PY0001-{''.join(random.choices(string.digits, k=12))}\"\
          .encode()\n    \n    def announce(self, event=None):\n        params = {\n            'info_hash': self.torrent.info_hash,\n\
          \            'peer_id': self.peer_id,\n            'port': self.port,\n            'uploaded': self.uploaded,\n\
          \            'downloaded': self.downloaded,\n            'left': self.torrent.total_length - self.downloaded,\n\
          \            'compact': 1,\n        }\n        if event:\n            params['event'] = event\n        \n      \
          \  response = requests.get(self.torrent.announce, params=params)\n        return self.parse_response(response.content)\n\
          \    \n    def parse_response(self, data):\n        response, _ = decode_bencode(data)\n        \n        if b'failure\
          \ reason' in response:\n            raise Exception(response[b'failure reason'].decode())\n        \n        interval\
          \ = response.get(b'interval', 1800)\n        peers = self.parse_peers(response[b'peers'])\n        \n        return\
          \ {'interval': interval, 'peers': peers}\n    \n    def parse_peers(self, peers_data):\n        if isinstance(peers_data,\
          \ list):  # Dictionary format\n            return [\n                (p[b'ip'].decode(), p[b'port'])\n         \
          \       for p in peers_data\n            ]\n        else:  # Compact format: 6 bytes per peer (4 IP + 2 port)\n\
          \            peers = []\n            for i in range(0, len(peers_data), 6):\n                ip = '.'.join(str(b)\
          \ for b in peers_data[i:i+4])\n                port = struct.unpack('>H', peers_data[i+4:i+6])[0]\n            \
          \    peers.append((ip, port))\n            return peers\n```"
      pitfalls:
      - URL encoding info_hash
      - Compact vs dict peer format
      - Handling tracker errors
      concepts:
      - HTTP protocol
      - Tracker protocol
      - Peer discovery
      estimated_hours: 8-12
    - id: 3
      name: Peer Protocol
      description: Implement BitTorrent peer wire protocol.
      acceptance_criteria:
      - Handshake
      - Message framing
      - Bitfield exchange
      - Request/piece messages
      - Choking/unchoking
      hints:
        level1: 'Handshake: pstrlen + pstr + reserved + info_hash + peer_id'
        level2: 'Messages: 4-byte length + 1-byte type + payload'
        level3: "## Peer Protocol\n\n```python\nimport struct\nimport socket\n\nclass PeerConnection:\n    CHOKE = 0\n   \
          \ UNCHOKE = 1\n    INTERESTED = 2\n    NOT_INTERESTED = 3\n    HAVE = 4\n    BITFIELD = 5\n    REQUEST = 6\n   \
          \ PIECE = 7\n    CANCEL = 8\n    \n    def __init__(self, ip, port, info_hash, peer_id):\n        self.socket =\
          \ socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.connect((ip, port))\n        self.info_hash\
          \ = info_hash\n        self.peer_id = peer_id\n        self.bitfield = None\n        self.choked = True\n      \
          \  self.interested = False\n    \n    def handshake(self):\n        pstr = b'BitTorrent protocol'\n        handshake\
          \ = bytes([len(pstr)]) + pstr + bytes(8) + self.info_hash + self.peer_id\n        self.socket.send(handshake)\n\
          \        \n        response = self.socket.recv(68)\n        if response[28:48] != self.info_hash:\n            raise\
          \ Exception(\"Info hash mismatch\")\n        \n        return response[48:68]  # Peer's peer_id\n    \n    def recv_message(self):\n\
          \        length_bytes = self.socket.recv(4)\n        if not length_bytes:\n            return None\n        \n \
          \       length = struct.unpack('>I', length_bytes)[0]\n        if length == 0:  # Keep-alive\n            return\
          \ ('keep-alive', None)\n        \n        msg_id = self.socket.recv(1)[0]\n        payload = self.socket.recv(length\
          \ - 1) if length > 1 else b''\n        \n        return (msg_id, payload)\n    \n    def send_interested(self):\n\
          \        self.socket.send(struct.pack('>IB', 1, self.INTERESTED))\n        self.interested = True\n    \n    def\
          \ send_request(self, index, begin, length):\n        payload = struct.pack('>III', index, begin, length)\n     \
          \   self.socket.send(struct.pack('>IB', 13, self.REQUEST) + payload)\n    \n    def parse_bitfield(self, payload):\n\
          \        self.bitfield = []\n        for byte in payload:\n            for i in range(8):\n                self.bitfield.append(bool(byte\
          \ & (128 >> i)))\n    \n    def parse_piece(self, payload):\n        index = struct.unpack('>I', payload[:4])[0]\n\
          \        begin = struct.unpack('>I', payload[4:8])[0]\n        data = payload[8:]\n        return index, begin,\
          \ data\n    \n    def download_piece(self, piece_index, piece_length):\n        BLOCK_SIZE = 16384  # 16 KB\n  \
          \      data = bytearray(piece_length)\n        \n        # Send all requests\n        for offset in range(0, piece_length,\
          \ BLOCK_SIZE):\n            length = min(BLOCK_SIZE, piece_length - offset)\n            self.send_request(piece_index,\
          \ offset, length)\n        \n        # Receive all blocks\n        received = 0\n        while received < piece_length:\n\
          \            msg_id, payload = self.recv_message()\n            if msg_id == self.PIECE:\n                idx, begin,\
          \ block = self.parse_piece(payload)\n                if idx == piece_index:\n                    data[begin:begin+len(block)]\
          \ = block\n                    received += len(block)\n        \n        return bytes(data)\n```"
      pitfalls:
      - Endianness
      - Partial message reads
      - Blocking on choked peer
      concepts:
      - Wire protocol
      - Message framing
      - State machines
      estimated_hours: 15-20
    - id: 4
      name: Piece Management & Seeding
      description: Manage pieces, verify hashes, and seed to other peers.
      acceptance_criteria:
      - Piece verification
      - Rarest-first selection
      - File assembly
      - Serving requests
      - Multiple peer connections
      hints:
        level1: 'Verify each piece''s SHA1 hash. Rarest-first: download pieces others have least.'
        level2: Run multiple peer connections concurrently. Balance downloading vs uploading.
        level3: "## Piece Manager\n\n```python\nimport asyncio\nimport hashlib\nfrom collections import defaultdict\n\nclass\
          \ PieceManager:\n    def __init__(self, torrent):\n        self.torrent = torrent\n        self.have = [False] *\
          \ torrent.num_pieces\n        self.pending = set()  # Pieces being downloaded\n        self.piece_availability =\
          \ defaultdict(int)  # piece -> count of peers\n    \n    def update_availability(self, peer_bitfield):\n       \
          \ for i, has in enumerate(peer_bitfield):\n            if has:\n                self.piece_availability[i] += 1\n\
          \    \n    def select_piece(self, peer_bitfield):\n        # Rarest first\n        candidates = [\n            (self.piece_availability[i],\
          \ i)\n            for i in range(self.torrent.num_pieces)\n            if peer_bitfield[i] and not self.have[i]\
          \ and i not in self.pending\n        ]\n        if not candidates:\n            return None\n        \n        candidates.sort()\
          \  # Sort by availability\n        piece_index = candidates[0][1]\n        self.pending.add(piece_index)\n     \
          \   return piece_index\n    \n    def verify_piece(self, index, data):\n        expected_hash = self.torrent.piece_hash(index)\n\
          \        actual_hash = hashlib.sha1(data).digest()\n        return expected_hash == actual_hash\n    \n    def piece_received(self,\
          \ index, data):\n        if self.verify_piece(index, data):\n            self.have[index] = True\n            self.pending.discard(index)\n\
          \            self.write_piece(index, data)\n            return True\n        else:\n            self.pending.discard(index)\n\
          \            return False\n    \n    def write_piece(self, index, data):\n        offset = index * self.torrent.piece_length\n\
          \        # Handle multi-file torrents...\n        with open(self.torrent.name, 'r+b') as f:\n            f.seek(offset)\n\
          \            f.write(data)\n\nclass BitTorrentClient:\n    def __init__(self, torrent_path):\n        self.torrent\
          \ = Torrent(torrent_path)\n        self.tracker = TrackerClient(self.torrent)\n        self.piece_manager = PieceManager(self.torrent)\n\
          \        self.peers = []\n    \n    async def download(self):\n        # Get peers from tracker\n        response\
          \ = self.tracker.announce('started')\n        \n        # Connect to peers\n        tasks = []\n        for ip,\
          \ port in response['peers'][:50]:  # Limit connections\n            task = asyncio.create_task(self.connect_peer(ip,\
          \ port))\n            tasks.append(task)\n        \n        await asyncio.gather(*tasks, return_exceptions=True)\n\
          \    \n    async def connect_peer(self, ip, port):\n        try:\n            peer = PeerConnection(ip, port, self.torrent.info_hash,\
          \ \n                                  self.tracker.peer_id)\n            peer.handshake()\n            \n      \
          \      # Get bitfield\n            msg_id, payload = peer.recv_message()\n            if msg_id == PeerConnection.BITFIELD:\n\
          \                peer.parse_bitfield(payload)\n                self.piece_manager.update_availability(peer.bitfield)\n\
          \            \n            peer.send_interested()\n            \n            # Wait for unchoke\n            while\
          \ peer.choked:\n                msg_id, _ = peer.recv_message()\n                if msg_id == PeerConnection.UNCHOKE:\n\
          \                    peer.choked = False\n            \n            # Download pieces\n            while not all(self.piece_manager.have):\n\
          \                piece_index = self.piece_manager.select_piece(peer.bitfield)\n                if piece_index is\
          \ None:\n                    break\n                \n                piece_length = self.torrent.piece_length\n\
          \                if piece_index == self.torrent.num_pieces - 1:\n                    piece_length = self.torrent.total_length\
          \ % self.torrent.piece_length\n                \n                data = peer.download_piece(piece_index, piece_length)\n\
          \                self.piece_manager.piece_received(piece_index, data)\n                \n        except Exception\
          \ as e:\n            print(f\"Peer {ip}:{port} error: {e}\")\n```"
      pitfalls:
      - Hash verification failures
      - Race conditions in piece selection
      - Connection management
      concepts:
      - Content verification
      - Scheduling algorithms
      - Concurrent downloads
      estimated_hours: 15-20
  build-blockchain:
    id: build-blockchain
    name: Build Your Own Blockchain
    description: Build a blockchain with proof of work and P2P networking.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Cryptographic hashing
    - P2P networking
    - Consensus concepts
    - Merkle trees
    languages:
      recommended:
      - Python
      - Go
      - Rust
      also_possible:
      - JavaScript
      - Java
    resources:
    - type: article
      name: Bitcoin Whitepaper
      url: https://bitcoin.org/bitcoin.pdf
    - type: book
      name: Mastering Bitcoin
      url: https://github.com/bitcoinbook/bitcoinbook
    - type: tutorial
      name: Build a Blockchain in Python
      url: https://hackernoon.com/learn-blockchains-by-building-one-117428612f46
    milestones:
    - id: 1
      name: Block Structure & Hashing
      description: Implement block data structure with cryptographic linking.
      acceptance_criteria:
      - Block header (prev hash, timestamp, nonce, merkle root)
      - Transaction structure
      - Block hashing (SHA-256)
      - Chain validation
      hints:
        level1: Each block contains hash of previous block, creating a chain.
        level2: Merkle root summarizes all transactions efficiently.
        level3: "## Block Structure\n\n```python\nimport hashlib\nimport time\nimport json\n\nclass Transaction:\n    def\
          \ __init__(self, sender, recipient, amount):\n        self.sender = sender\n        self.recipient = recipient\n\
          \        self.amount = amount\n        self.timestamp = time.time()\n    \n    def to_dict(self):\n        return\
          \ {\n            'sender': self.sender,\n            'recipient': self.recipient,\n            'amount': self.amount,\n\
          \            'timestamp': self.timestamp\n        }\n    \n    def hash(self):\n        return hashlib.sha256(\n\
          \            json.dumps(self.to_dict(), sort_keys=True).encode()\n        ).hexdigest()\n\nclass Block:\n    def\
          \ __init__(self, index, transactions, previous_hash, nonce=0):\n        self.index = index\n        self.timestamp\
          \ = time.time()\n        self.transactions = transactions\n        self.previous_hash = previous_hash\n        self.nonce\
          \ = nonce\n        self.merkle_root = self.calculate_merkle_root()\n        self.hash = self.calculate_hash()\n\
          \    \n    def calculate_merkle_root(self):\n        if not self.transactions:\n            return hashlib.sha256(b'').hexdigest()\n\
          \        \n        hashes = [tx.hash() for tx in self.transactions]\n        \n        while len(hashes) > 1:\n\
          \            if len(hashes) % 2 == 1:\n                hashes.append(hashes[-1])  # Duplicate last if odd\n    \
          \        \n            hashes = [\n                hashlib.sha256((hashes[i] + hashes[i+1]).encode()).hexdigest()\n\
          \                for i in range(0, len(hashes), 2)\n            ]\n        \n        return hashes[0]\n    \n  \
          \  def calculate_hash(self):\n        block_string = json.dumps({\n            'index': self.index,\n          \
          \  'timestamp': self.timestamp,\n            'merkle_root': self.merkle_root,\n            'previous_hash': self.previous_hash,\n\
          \            'nonce': self.nonce\n        }, sort_keys=True)\n        return hashlib.sha256(block_string.encode()).hexdigest()\n\
          \nclass Blockchain:\n    def __init__(self):\n        self.chain = [self.create_genesis_block()]\n        self.pending_transactions\
          \ = []\n    \n    def create_genesis_block(self):\n        return Block(0, [], \"0\")\n    \n    def is_chain_valid(self):\n\
          \        for i in range(1, len(self.chain)):\n            current = self.chain[i]\n            previous = self.chain[i-1]\n\
          \            \n            if current.hash != current.calculate_hash():\n                return False\n        \
          \    if current.previous_hash != previous.hash:\n                return False\n        return True\n```"
      pitfalls:
      - Mutable transaction data
      - Hash calculation order
      - Genesis block handling
      concepts:
      - Cryptographic hashing
      - Merkle trees
      - Data integrity
      estimated_hours: 8-12
    - id: 2
      name: Proof of Work
      description: Implement mining with difficulty adjustment.
      acceptance_criteria:
      - Hash target/difficulty
      - Nonce search
      - Difficulty adjustment
      - Block reward
      hints:
        level1: Find nonce such that hash starts with N zeros (difficulty).
        level2: Adjust difficulty based on time between blocks to maintain target rate.
        level3: "## Proof of Work\n\n```python\nclass Blockchain:\n    def __init__(self):\n        self.chain = [self.create_genesis_block()]\n\
          \        self.difficulty = 4  # Number of leading zeros required\n        self.target_block_time = 10  # seconds\n\
          \        self.adjustment_interval = 10  # blocks\n        self.block_reward = 50\n    \n    def get_difficulty_target(self):\n\
          \        # Target: hash must be less than this\n        return '0' * self.difficulty + 'f' * (64 - self.difficulty)\n\
          \    \n    def mine_block(self, miner_address):\n        # Add coinbase transaction (block reward)\n        coinbase\
          \ = Transaction('COINBASE', miner_address, self.block_reward)\n        transactions = [coinbase] + self.pending_transactions\n\
          \        \n        new_block = Block(\n            len(self.chain),\n            transactions,\n            self.chain[-1].hash\n\
          \        )\n        \n        # Mine: find valid nonce\n        target = self.get_difficulty_target()\n        while\
          \ new_block.hash > target:\n            new_block.nonce += 1\n            new_block.hash = new_block.calculate_hash()\n\
          \        \n        self.chain.append(new_block)\n        self.pending_transactions = []\n        \n        # Adjust\
          \ difficulty\n        if len(self.chain) % self.adjustment_interval == 0:\n            self.adjust_difficulty()\n\
          \        \n        return new_block\n    \n    def adjust_difficulty(self):\n        # Calculate actual time for\
          \ last N blocks\n        start_block = self.chain[-self.adjustment_interval]\n        end_block = self.chain[-1]\n\
          \        actual_time = end_block.timestamp - start_block.timestamp\n        expected_time = self.target_block_time\
          \ * self.adjustment_interval\n        \n        # Adjust difficulty\n        ratio = actual_time / expected_time\n\
          \        if ratio < 0.5:\n            self.difficulty += 1  # Too fast, increase difficulty\n        elif ratio\
          \ > 2.0:\n            self.difficulty = max(1, self.difficulty - 1)  # Too slow\n        \n        print(f\"Difficulty\
          \ adjusted to {self.difficulty}\")\n    \n    def get_balance(self, address):\n        balance = 0\n        for\
          \ block in self.chain:\n            for tx in block.transactions:\n                if tx.recipient == address:\n\
          \                    balance += tx.amount\n                if tx.sender == address:\n                    balance\
          \ -= tx.amount\n        return balance\n```"
      pitfalls:
      - Difficulty too high/low
      - No difficulty adjustment
      - Mining centralization
      concepts:
      - Proof of work
      - Mining
      - Difficulty adjustment
      estimated_hours: 10-15
    - id: 3
      name: P2P Networking
      description: Implement peer-to-peer network for block propagation.
      acceptance_criteria:
      - Peer discovery
      - Block broadcasting
      - Transaction broadcasting
      - Chain synchronization
      hints:
        level1: Nodes connect to peers, share new blocks and transactions.
        level2: On new block, validate and broadcast to peers. Sync chain on startup.
        level3: "## P2P Network\n\n```python\nimport socket\nimport threading\nimport json\n\nclass P2PNode:\n    def __init__(self,\
          \ host, port, blockchain):\n        self.host = host\n        self.port = port\n        self.blockchain = blockchain\n\
          \        self.peers = set()  # (host, port) tuples\n        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\
          \    \n    def start(self):\n        self.server.bind((self.host, self.port))\n        self.server.listen(10)\n\
          \        \n        threading.Thread(target=self.accept_connections).start()\n    \n    def accept_connections(self):\n\
          \        while True:\n            client, addr = self.server.accept()\n            threading.Thread(target=self.handle_client,\
          \ args=(client,)).start()\n    \n    def handle_client(self, client):\n        try:\n            data = client.recv(65536).decode()\n\
          \            message = json.loads(data)\n            \n            if message['type'] == 'GET_CHAIN':\n        \
          \        self.send_chain(client)\n            elif message['type'] == 'NEW_BLOCK':\n                self.receive_block(message['block'])\n\
          \            elif message['type'] == 'NEW_TX':\n                self.receive_transaction(message['transaction'])\n\
          \            elif message['type'] == 'GET_PEERS':\n                self.send_peers(client)\n        finally:\n \
          \           client.close()\n    \n    def connect_to_peer(self, host, port):\n        self.peers.add((host, port))\n\
          \        # Sync chain from peer\n        self.request_chain(host, port)\n    \n    def request_chain(self, host,\
          \ port):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.connect((host, port))\n\
          \        sock.send(json.dumps({'type': 'GET_CHAIN'}).encode())\n        \n        data = sock.recv(1000000).decode()\n\
          \        chain_data = json.loads(data)\n        sock.close()\n        \n        # Replace chain if longer and valid\n\
          \        if len(chain_data) > len(self.blockchain.chain):\n            new_chain = self.deserialize_chain(chain_data)\n\
          \            if self.blockchain.is_chain_valid_full(new_chain):\n                self.blockchain.chain = new_chain\n\
          \                print(\"Chain replaced with longer valid chain\")\n    \n    def broadcast_block(self, block):\n\
          \        message = json.dumps({\n            'type': 'NEW_BLOCK',\n            'block': block.to_dict()\n      \
          \  })\n        \n        for host, port in self.peers:\n            try:\n                sock = socket.socket(socket.AF_INET,\
          \ socket.SOCK_STREAM)\n                sock.connect((host, port))\n                sock.send(message.encode())\n\
          \                sock.close()\n            except:\n                pass  # Peer offline\n    \n    def receive_block(self,\
          \ block_data):\n        block = Block.from_dict(block_data)\n        \n        # Validate block\n        if block.previous_hash\
          \ != self.blockchain.chain[-1].hash:\n            # We might be behind, request full chain\n            return\n\
          \        \n        if not self.blockchain.is_valid_block(block):\n            return\n        \n        self.blockchain.chain.append(block)\n\
          \        print(f\"Added block {block.index}\")\n```"
      pitfalls:
      - Network partitions
      - Eclipse attacks
      - Sybil attacks
      concepts:
      - P2P networking
      - Gossip protocols
      - Chain synchronization
      estimated_hours: 12-18
    - id: 4
      name: Consensus & Fork Resolution
      description: Handle chain forks and achieve consensus.
      acceptance_criteria:
      - Longest chain rule
      - Fork detection
      - Orphan block handling
      - Double-spend prevention
      hints:
        level1: Accept the longest valid chain. Reorganize on longer chain.
        level2: Keep orphan blocks in case parent arrives later. Wait for confirmations.
        level3: "## Fork Resolution\n\n```python\nclass Blockchain:\n    def __init__(self):\n        self.chain = [self.create_genesis_block()]\n\
          \        self.orphan_blocks = {}  # parent_hash -> [blocks]\n        self.confirmations_required = 6\n    \n   \
          \ def add_block(self, block):\n        # Case 1: Extends main chain\n        if block.previous_hash == self.chain[-1].hash:\n\
          \            if self.is_valid_block(block):\n                self.chain.append(block)\n                self.process_orphans(block.hash)\n\
          \                return True\n        \n        # Case 2: Orphan (parent not found)\n        parent_in_chain = self.find_block_by_hash(block.previous_hash)\n\
          \        if not parent_in_chain:\n            self.orphan_blocks.setdefault(block.previous_hash, []).append(block)\n\
          \            return False\n        \n        # Case 3: Fork - extends a previous block\n        fork_chain = self.build_fork_chain(block)\n\
          \        if len(fork_chain) > len(self.chain):\n            # Reorganize to longer chain\n            self.reorganize(fork_chain)\n\
          \            return True\n        \n        return False\n    \n    def process_orphans(self, parent_hash):\n  \
          \      if parent_hash in self.orphan_blocks:\n            orphans = self.orphan_blocks.pop(parent_hash)\n      \
          \      for orphan in orphans:\n                self.add_block(orphan)\n    \n    def build_fork_chain(self, new_block):\n\
          \        # Build chain from genesis to new block\n        chain = [new_block]\n        current_hash = new_block.previous_hash\n\
          \        \n        while current_hash != self.chain[0].hash:\n            block = self.find_block_by_hash(current_hash)\n\
          \            if not block:\n                return []  # Can't build complete chain\n            chain.insert(0,\
          \ block)\n            current_hash = block.previous_hash\n        \n        return [self.chain[0]] + chain\n   \
          \ \n    def reorganize(self, new_chain):\n        # Find common ancestor\n        old_blocks = []\n        for i,\
          \ (old, new) in enumerate(zip(self.chain, new_chain)):\n            if old.hash != new.hash:\n                old_blocks\
          \ = self.chain[i:]\n                break\n        \n        # Return transactions from old blocks to mempool\n\
          \        for block in old_blocks:\n            for tx in block.transactions:\n                if tx.sender != 'COINBASE':\n\
          \                    self.pending_transactions.append(tx)\n        \n        self.chain = new_chain\n        print(f\"\
          Chain reorganized, dropped {len(old_blocks)} blocks\")\n    \n    def is_confirmed(self, tx_hash):\n        for\
          \ i, block in enumerate(reversed(self.chain)):\n            for tx in block.transactions:\n                if tx.hash()\
          \ == tx_hash:\n                    confirmations = i + 1\n                    return confirmations >= self.confirmations_required\n\
          \        return False\n```"
      pitfalls:
      - 51% attacks
      - Selfish mining
      - Long reorgs
      concepts:
      - Nakamoto consensus
      - Fork resolution
      - Finality
      estimated_hours: 12-18
  build-browser:
    id: build-browser
    name: Build Your Own Browser
    description: Build a simple web browser engine. Learn HTML/CSS parsing, layout, and rendering.
    difficulty: expert
    estimated_hours: 80-150
    prerequisites:
    - HTML/CSS parsing
    - Tree data structures
    - Graphics basics
    languages:
      recommended:
      - Rust
      - C++
      - Go
      also_possible:
      - Python
      - TypeScript
    resources:
    - type: book
      name: Web Browser Engineering
      url: https://browser.engineering/
    - type: tutorial
      name: Let's build a browser engine
      url: https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html
    milestones:
    - id: 1
      name: HTML Parser
      description: Parse HTML into a DOM tree.
      acceptance_criteria:
      - Tokenize HTML
      - Build DOM tree
      - Handle nested elements
      - Self-closing tags
      hints:
        level1: 'State machine tokenizer: data state, tag open, tag name, attribute states.'
        level2: Build tree with stack. Push on open tag, pop on close. Handle implicit closes.
        level3: "from dataclasses import dataclass, field\nfrom typing import List, Dict, Optional\n\n@dataclass\nclass Node:\n\
          \    pass\n\n@dataclass\nclass Element(Node):\n    tag_name: str\n    attributes: Dict[str, str] = field(default_factory=dict)\n\
          \    children: List[Node] = field(default_factory=list)\n\n@dataclass\nclass Text(Node):\n    data: str\n\nclass\
          \ HTMLParser:\n    SELF_CLOSING = {'area', 'base', 'br', 'col', 'embed', 'hr', 'img', 'input', 'link', 'meta', 'param',\
          \ 'source', 'track', 'wbr'}\n    \n    def __init__(self, html: str):\n        self.html = html\n        self.pos\
          \ = 0\n    \n    def parse(self) -> Element:\n        # Create implicit root\n        root = Element('html')\n \
          \       self._parse_nodes(root)\n        return root\n    \n    def _parse_nodes(self, parent: Element):\n     \
          \   while self.pos < len(self.html):\n            if self._peek() == '<':\n                if self._peek(1) == '/':\n\
          \                    return  # Close tag - handled by caller\n                elif self._peek(1) == '!':\n     \
          \               self._skip_comment()\n                else:\n                    elem = self._parse_element()\n\
          \                    if elem:\n                        parent.children.append(elem)\n            else:\n       \
          \         text = self._parse_text()\n                if text.strip():\n                    parent.children.append(Text(text))\n\
          \    \n    def _parse_element(self) -> Optional[Element]:\n        self._consume('<')\n        tag_name = self._parse_tag_name()\n\
          \        attrs = self._parse_attributes()\n        \n        self._skip_whitespace()\n        \n        # Self-closing\
          \ syntax or void element\n        if self._peek() == '/' or tag_name.lower() in self.SELF_CLOSING:\n           \
          \ if self._peek() == '/':\n                self._consume('/')\n            self._consume('>')\n            return\
          \ Element(tag_name.lower(), attrs)\n        \n        self._consume('>')\n        \n        elem = Element(tag_name.lower(),\
          \ attrs)\n        self._parse_nodes(elem)\n        \n        # Consume close tag\n        if self._peek() == '<'\
          \ and self._peek(1) == '/':\n            self._consume('</')\n            self._parse_tag_name()  # Should match\n\
          \            self._consume('>')\n        \n        return elem\n    \n    def _parse_attributes(self) -> Dict[str,\
          \ str]:\n        attrs = {}\n        while True:\n            self._skip_whitespace()\n            if self._peek()\
          \ in ('>', '/', ''):\n                break\n            \n            name = self._parse_attr_name()\n        \
          \    self._skip_whitespace()\n            \n            if self._peek() == '=':\n                self._consume('=')\n\
          \                value = self._parse_attr_value()\n            else:\n                value = ''\n            \n\
          \            attrs[name.lower()] = value\n        return attrs"
      pitfalls:
      - Malformed HTML handling
      - Entity decoding
      - Case sensitivity
      concepts:
      - DOM
      - Tokenization
      - Tree construction
      estimated_hours: 12-20
    - id: 2
      name: CSS Parser
      description: Parse CSS into a stylesheet.
      acceptance_criteria:
      - Selector parsing
      - Property parsing
      - Specificity calculation
      - Cascade
      hints:
        level1: 'Selectors: element, class, id, combinators. Properties are key: value pairs.'
        level2: 'Specificity: (inline, id, class, element). Higher wins. Later wins on tie.'
        level3: "@dataclass\nclass Selector:\n    tag: Optional[str] = None\n    id: Optional[str] = None\n    classes: List[str]\
          \ = field(default_factory=list)\n    \n    def specificity(self) -> tuple:\n        # (inline, ids, classes, elements)\n\
          \        return (\n            0,\n            1 if self.id else 0,\n            len(self.classes),\n          \
          \  1 if self.tag else 0\n        )\n    \n    def matches(self, elem: Element) -> bool:\n        if self.tag and\
          \ self.tag != elem.tag_name:\n            return False\n        if self.id and self.id != elem.attributes.get('id'):\n\
          \            return False\n        elem_classes = set(elem.attributes.get('class', '').split())\n        if not\
          \ all(c in elem_classes for c in self.classes):\n            return False\n        return True\n\n@dataclass\nclass\
          \ Rule:\n    selectors: List[Selector]\n    declarations: Dict[str, str]\n\nclass CSSParser:\n    def __init__(self,\
          \ css: str):\n        self.css = css\n        self.pos = 0\n    \n    def parse(self) -> List[Rule]:\n        rules\
          \ = []\n        while self.pos < len(self.css):\n            self._skip_whitespace_and_comments()\n            if\
          \ self.pos >= len(self.css):\n                break\n            rules.append(self._parse_rule())\n        return\
          \ rules\n    \n    def _parse_selector(self) -> Selector:\n        sel = Selector()\n        while self._peek()\
          \ not in (',', '{', ''):\n            if self._peek() == '#':\n                self._consume('#')\n            \
          \    sel.id = self._parse_identifier()\n            elif self._peek() == '.':\n                self._consume('.')\n\
          \                sel.classes.append(self._parse_identifier())\n            elif self._peek().isalpha():\n      \
          \          sel.tag = self._parse_identifier()\n            else:\n                break\n        return sel\n  \
          \  \n    def _parse_declarations(self) -> Dict[str, str]:\n        decls = {}\n        self._consume('{')\n    \
          \    while self._peek() != '}':\n            self._skip_whitespace()\n            name = self._parse_identifier()\n\
          \            self._skip_whitespace()\n            self._consume(':')\n            self._skip_whitespace()\n    \
          \        value = self._parse_value()\n            decls[name] = value\n            self._skip_whitespace()\n   \
          \         if self._peek() == ';':\n                self._consume(';')\n        self._consume('}')\n        return\
          \ decls\n\n# Style computation\ndef compute_style(elem: Element, stylesheet: List[Rule]) -> Dict[str, str]:\n  \
          \  # Collect matching rules with specificity\n    matched = []\n    for rule in stylesheet:\n        for selector\
          \ in rule.selectors:\n            if selector.matches(elem):\n                matched.append((selector.specificity(),\
          \ rule.declarations))\n    \n    # Sort by specificity\n    matched.sort(key=lambda x: x[0])\n    \n    # Apply\
          \ in order (later/higher specificity wins)\n    style = {}\n    for _, decls in matched:\n        style.update(decls)\n\
          \    \n    return style"
      pitfalls:
      - Selector combinators
      - Shorthand properties
      - '!important'
      concepts:
      - CSS parsing
      - Specificity
      - Cascade
      estimated_hours: 15-25
    - id: 3
      name: Layout
      description: Calculate box positions and sizes.
      acceptance_criteria:
      - Box model (margin, border, padding)
      - Block layout
      - Inline layout
      - Width/height calculation
      hints:
        level1: Layout tree mirrors DOM but has computed styles. Block = vertical, Inline = horizontal.
        level2: Width flows down (parent constrains child). Height flows up (content determines parent).
        level3: "@dataclass\nclass Dimensions:\n    content: Rect = field(default_factory=Rect)\n    padding: EdgeSizes =\
          \ field(default_factory=EdgeSizes)\n    border: EdgeSizes = field(default_factory=EdgeSizes)\n    margin: EdgeSizes\
          \ = field(default_factory=EdgeSizes)\n    \n    def padding_box(self) -> Rect:\n        return self.content.expanded_by(self.padding)\n\
          \    \n    def border_box(self) -> Rect:\n        return self.padding_box().expanded_by(self.border)\n    \n   \
          \ def margin_box(self) -> Rect:\n        return self.border_box().expanded_by(self.margin)\n\n@dataclass\nclass\
          \ LayoutBox:\n    box_type: str  # 'block', 'inline', 'anonymous'\n    dimensions: Dimensions = field(default_factory=Dimensions)\n\
          \    children: List['LayoutBox'] = field(default_factory=list)\n    style: Dict[str, str] = field(default_factory=dict)\n\
          \    node: Optional[Element] = None\n\ndef layout_block(box: LayoutBox, containing_block: Dimensions):\n    # Calculate\
          \ width based on containing block\n    calculate_block_width(box, containing_block)\n    \n    # Position box below\
          \ previous siblings\n    calculate_block_position(box, containing_block)\n    \n    # Layout children\n    layout_block_children(box)\n\
          \    \n    # Height is determined by children\n    calculate_block_height(box)\n\ndef calculate_block_width(box:\
          \ LayoutBox, container: Dimensions):\n    style = box.style\n    \n    # Default to auto\n    width = style.get('width',\
          \ 'auto')\n    \n    # Get margin, padding, border values\n    margin_left = to_px(style.get('margin-left', '0'))\n\
          \    margin_right = to_px(style.get('margin-right', '0'))\n    padding_left = to_px(style.get('padding-left', '0'))\n\
          \    padding_right = to_px(style.get('padding-right', '0'))\n    border_left = to_px(style.get('border-left-width',\
          \ '0'))\n    border_right = to_px(style.get('border-right-width', '0'))\n    \n    total = margin_left + border_left\
          \ + padding_left + padding_right + border_right + margin_right\n    \n    if width != 'auto':\n        total +=\
          \ to_px(width)\n    \n    # Adjust for containing block\n    underflow = container.content.width - total\n    \n\
          \    if width == 'auto':\n        # Expand to fill\n        box.dimensions.content.width = underflow\n    else:\n\
          \        box.dimensions.content.width = to_px(width)\n        # Adjust auto margins\n        if style.get('margin-left')\
          \ == 'auto' and style.get('margin-right') == 'auto':\n            margin_left = underflow / 2\n            margin_right\
          \ = underflow / 2\n    \n    box.dimensions.padding.left = padding_left\n    box.dimensions.padding.right = padding_right\n\
          \    box.dimensions.margin.left = margin_left\n    box.dimensions.margin.right = margin_right"
      pitfalls:
      - Auto margins
      - Percentage units
      - Collapsing margins
      concepts:
      - Box model
      - Block formatting
      - Layout algorithms
      estimated_hours: 20-35
    - id: 4
      name: Rendering
      description: Paint the layout tree to a canvas.
      acceptance_criteria:
      - Background colors
      - Borders
      - Text rendering
      - Z-ordering
      hints:
        level1: Walk layout tree. Draw backgrounds, then borders, then content (painter's algorithm).
        level2: Text needs font metrics. Use a graphics library (SDL, Cairo, etc.).
        level3: "@dataclass\nclass DisplayCommand:\n    pass\n\n@dataclass\nclass SolidColor(DisplayCommand):\n    color:\
          \ Color\n    rect: Rect\n\n@dataclass\nclass DrawText(DisplayCommand):\n    text: str\n    x: float\n    y: float\n\
          \    color: Color\n    font_size: float\n\ndef build_display_list(layout_root: LayoutBox) -> List[DisplayCommand]:\n\
          \    commands = []\n    render_layout_box(commands, layout_root)\n    return commands\n\ndef render_layout_box(commands:\
          \ List, box: LayoutBox):\n    render_background(commands, box)\n    render_borders(commands, box)\n    \n    if\
          \ box.box_type == 'block':\n        for child in box.children:\n            render_layout_box(commands, child)\n\
          \    elif box.box_type == 'inline':\n        render_text(commands, box)\n\ndef render_background(commands: List,\
          \ box: LayoutBox):\n    color = box.style.get('background-color')\n    if color and color != 'transparent':\n  \
          \      commands.append(SolidColor(\n            color=parse_color(color),\n            rect=box.dimensions.border_box()\n\
          \        ))\n\ndef render_borders(commands: List, box: LayoutBox):\n    color = box.style.get('border-color')\n\
          \    if not color:\n        return\n    \n    d = box.dimensions\n    border_box = d.border_box()\n    \n    # Top\
          \ border\n    if d.border.top > 0:\n        commands.append(SolidColor(\n            color=parse_color(color),\n\
          \            rect=Rect(\n                x=border_box.x,\n                y=border_box.y,\n                width=border_box.width,\n\
          \                height=d.border.top\n            )\n        ))\n    \n    # ... similar for other sides\n\ndef\
          \ render_text(commands: List, box: LayoutBox):\n    if box.node and isinstance(box.node, Text):\n        color =\
          \ box.style.get('color', 'black')\n        font_size = to_px(box.style.get('font-size', '16px'))\n        \n   \
          \     commands.append(DrawText(\n            text=box.node.data,\n            x=box.dimensions.content.x,\n    \
          \        y=box.dimensions.content.y + font_size,  # Baseline\n            color=parse_color(color),\n          \
          \  font_size=font_size\n        ))\n\n# Execute display list\ndef paint(commands: List[DisplayCommand], canvas):\n\
          \    for cmd in commands:\n        if isinstance(cmd, SolidColor):\n            canvas.fill_rect(cmd.rect, cmd.color)\n\
          \        elif isinstance(cmd, DrawText):\n            canvas.draw_text(cmd.text, cmd.x, cmd.y, cmd.color, cmd.font_size)"
      pitfalls:
      - Subpixel rendering
      - Font fallbacks
      - Clipping
      concepts:
      - Rendering pipeline
      - Display lists
      - Graphics
      estimated_hours: 33-70
  build-btree:
    id: build-btree
    name: Build Your Own B-tree
    description: Implement a disk-friendly B-tree for database indexing. Learn balanced trees and disk I/O optimization.
    difficulty: expert
    estimated_hours: 30-50
    prerequisites:
    - Binary search trees
    - File I/O
    - Database concepts
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Java
      - Python
    resources:
    - type: paper
      name: The Ubiquitous B-Tree
      url: https://dl.acm.org/doi/10.1145/356770.356776
    - type: book
      name: Database Internals - Ch 2-4
      url: https://www.databass.dev/
    milestones:
    - id: 1
      name: Node Structure
      description: Design and implement B-tree node layout for disk storage.
      acceptance_criteria:
      - Fixed-size pages
      - Key and child pointer layout
      - Leaf vs internal nodes
      - Serialization/deserialization
      hints:
        level1: Each node is one disk page (e.g., 4KB). Store keys, values (leaf), child pointers (internal).
        level2: Use fixed-size slots. Track number of keys. Leaf nodes have no children.
        level3: "import struct\n\nPAGE_SIZE = 4096\nMAX_KEY_SIZE = 256\nMAX_VALUE_SIZE = 256\n\nclass BTreeNode:\n    # Header:\
          \ is_leaf(1) + num_keys(2) + parent_page(4) = 7 bytes\n    HEADER_SIZE = 7\n    \n    def __init__(self, page_id,\
          \ is_leaf=True):\n        self.page_id = page_id\n        self.is_leaf = is_leaf\n        self.keys = []\n     \
          \   self.values = []     # For leaf nodes\n        self.children = []   # For internal nodes (page IDs)\n      \
          \  self.parent = None\n    \n    @classmethod\n    def max_keys(cls, is_leaf):\n        if is_leaf:\n          \
          \  # Each entry: key_len(2) + key + val_len(2) + val\n            entry_size = 2 + MAX_KEY_SIZE + 2 + MAX_VALUE_SIZE\n\
          \        else:\n            # Each entry: key_len(2) + key + child_ptr(4)\n            entry_size = 2 + MAX_KEY_SIZE\
          \ + 4\n        return (PAGE_SIZE - cls.HEADER_SIZE) // entry_size\n    \n    def serialize(self):\n        data\
          \ = struct.pack('>BHI', \n            1 if self.is_leaf else 0,\n            len(self.keys),\n            self.parent\
          \ or 0\n        )\n        \n        for i, key in enumerate(self.keys):\n            key_bytes = key.encode('utf-8')[:MAX_KEY_SIZE]\n\
          \            data += struct.pack('>H', len(key_bytes)) + key_bytes\n            \n            if self.is_leaf:\n\
          \                val_bytes = self.values[i].encode('utf-8')[:MAX_VALUE_SIZE]\n                data += struct.pack('>H',\
          \ len(val_bytes)) + val_bytes\n            else:\n                data += struct.pack('>I', self.children[i])\n\
          \        \n        # Last child pointer for internal nodes\n        if not self.is_leaf and self.children:\n   \
          \         data += struct.pack('>I', self.children[-1])\n        \n        # Pad to page size\n        return data.ljust(PAGE_SIZE,\
          \ b'\\x00')\n    \n    @classmethod\n    def deserialize(cls, page_id, data):\n        is_leaf, num_keys, parent\
          \ = struct.unpack('>BHI', data[:7])\n        node = cls(page_id, bool(is_leaf))\n        node.parent = parent if\
          \ parent else None\n        # ... parse keys and values/children\n        return node"
      pitfalls:
      - Page boundary alignment
      - Variable-length key handling
      - Endianness
      concepts:
      - Disk pages
      - Serialization
      - Node layout
      estimated_hours: 5-8
    - id: 2
      name: Search
      description: Implement efficient key lookup traversing the tree.
      acceptance_criteria:
      - Binary search within nodes
      - Tree traversal
      - Leaf node lookup
      - Range queries
      hints:
        level1: Binary search in node to find key or child to follow. Recurse until leaf.
        level2: Cache nodes for performance. Track path for later operations.
        level3: "import bisect\n\nclass BTree:\n    def __init__(self, pager, order):\n        self.pager = pager  # Handles\
          \ disk I/O\n        self.order = order  # Max children per node\n        self.root_page = None\n    \n    def search(self,\
          \ key):\n        if self.root_page is None:\n            return None\n        \n        node = self.pager.get_page(self.root_page)\n\
          \        \n        while True:\n            # Binary search for key position\n            idx = bisect.bisect_left(node.keys,\
          \ key)\n            \n            if node.is_leaf:\n                # Check if key exists\n                if idx\
          \ < len(node.keys) and node.keys[idx] == key:\n                    return node.values[idx]\n                return\
          \ None\n            else:\n                # Follow child pointer\n                child_page = node.children[idx]\n\
          \                node = self.pager.get_page(child_page)\n    \n    def range_search(self, start_key, end_key):\n\
          \        \"\"\"Return all key-value pairs in range [start, end]\"\"\"\n        results = []\n        \n        #\
          \ Find starting leaf\n        node, idx = self._find_leaf_position(start_key)\n        if node is None:\n      \
          \      return results\n        \n        # Scan leaves using sibling pointers\n        while node:\n           \
          \ while idx < len(node.keys):\n                if node.keys[idx] > end_key:\n                    return results\n\
          \                results.append((node.keys[idx], node.values[idx]))\n                idx += 1\n            \n  \
          \          # Move to next leaf (requires sibling pointer)\n            node = self._next_leaf(node)\n          \
          \  idx = 0\n        \n        return results"
      pitfalls:
      - Off-by-one in binary search
      - Empty tree handling
      - Key not found cases
      concepts:
      - Binary search
      - Tree traversal
      - I/O efficiency
      estimated_hours: 4-6
    - id: 3
      name: Insertion with Splitting
      description: Insert keys and handle node splits when full.
      acceptance_criteria:
      - Insert into correct leaf
      - Split full nodes
      - Propagate splits up
      - Handle root splits
      hints:
        level1: Insert in leaf. If full, split at middle. Push middle key to parent.
        level2: Parent might also split. New root if root splits.
        level3: "def insert(self, key, value):\n    if self.root_page is None:\n        # Create root\n        root = BTreeNode(self.pager.allocate_page(),\
          \ is_leaf=True)\n        root.keys.append(key)\n        root.values.append(value)\n        self.pager.write_page(root)\n\
          \        self.root_page = root.page_id\n        return\n    \n    # Find leaf\n    path = []  # Stack of (node,\
          \ index) for path from root\n    node = self.pager.get_page(self.root_page)\n    \n    while not node.is_leaf:\n\
          \        idx = bisect.bisect_left(node.keys, key)\n        path.append((node, idx))\n        node = self.pager.get_page(node.children[idx])\n\
          \    \n    # Insert in leaf\n    idx = bisect.bisect_left(node.keys, key)\n    node.keys.insert(idx, key)\n    node.values.insert(idx,\
          \ value)\n    \n    # Split if necessary\n    if len(node.keys) > self.order - 1:\n        self._split_leaf(node,\
          \ path)\n    else:\n        self.pager.write_page(node)\n\ndef _split_leaf(self, node, path):\n    mid = len(node.keys)\
          \ // 2\n    \n    # Create new right node\n    right = BTreeNode(self.pager.allocate_page(), is_leaf=True)\n   \
          \ right.keys = node.keys[mid:]\n    right.values = node.values[mid:]\n    \n    # Update left node\n    node.keys\
          \ = node.keys[:mid]\n    node.values = node.values[:mid]\n    \n    # Push up to parent\n    push_key = right.keys[0]\n\
          \    \n    if not path:\n        # Split root - create new root\n        new_root = BTreeNode(self.pager.allocate_page(),\
          \ is_leaf=False)\n        new_root.keys = [push_key]\n        new_root.children = [node.page_id, right.page_id]\n\
          \        self.root_page = new_root.page_id\n        self.pager.write_page(new_root)\n    else:\n        parent,\
          \ parent_idx = path.pop()\n        parent.keys.insert(parent_idx, push_key)\n        parent.children.insert(parent_idx\
          \ + 1, right.page_id)\n        \n        if len(parent.keys) > self.order - 1:\n            self._split_internal(parent,\
          \ path)\n        else:\n            self.pager.write_page(parent)\n    \n    self.pager.write_page(node)\n    self.pager.write_page(right)"
      pitfalls:
      - Split index calculation
      - Parent key vs child key
      - Root split handling
      concepts:
      - Node splitting
      - Tree balancing
      - Propagation
      estimated_hours: 8-12
    - id: 4
      name: Deletion with Merging
      description: Delete keys and handle underflow by borrowing or merging.
      acceptance_criteria:
      - Delete from leaf
      - Borrow from siblings
      - Merge underfull nodes
      - Handle root underflow
      hints:
        level1: Delete from leaf. If too few keys, try borrowing from sibling, else merge.
        level2: 'Borrowing: rotate through parent. Merging: combine nodes, delete parent key.'
        level3: "def delete(self, key):\n    path = []\n    node = self.pager.get_page(self.root_page)\n    \n    # Find key\n\
          \    while not node.is_leaf:\n        idx = bisect.bisect_left(node.keys, key)\n        path.append((node, idx))\n\
          \        node = self.pager.get_page(node.children[idx])\n    \n    # Delete from leaf\n    try:\n        idx = node.keys.index(key)\n\
          \    except ValueError:\n        return False  # Key not found\n    \n    node.keys.pop(idx)\n    node.values.pop(idx)\n\
          \    \n    # Check underflow\n    min_keys = (self.order - 1) // 2\n    if len(node.keys) >= min_keys or not path:\n\
          \        self.pager.write_page(node)\n        return True\n    \n    self._handle_underflow(node, path)\n    return\
          \ True\n\ndef _handle_underflow(self, node, path):\n    parent, parent_idx = path[-1]\n    min_keys = (self.order\
          \ - 1) // 2\n    \n    # Try borrowing from left sibling\n    if parent_idx > 0:\n        left = self.pager.get_page(parent.children[parent_idx\
          \ - 1])\n        if len(left.keys) > min_keys:\n            self._borrow_from_left(node, left, parent, parent_idx)\n\
          \            return\n    \n    # Try borrowing from right sibling\n    if parent_idx < len(parent.children) - 1:\n\
          \        right = self.pager.get_page(parent.children[parent_idx + 1])\n        if len(right.keys) > min_keys:\n\
          \            self._borrow_from_right(node, right, parent, parent_idx)\n            return\n    \n    # Must merge\n\
          \    if parent_idx > 0:\n        left = self.pager.get_page(parent.children[parent_idx - 1])\n        self._merge_nodes(left,\
          \ node, parent, parent_idx - 1, path)\n    else:\n        right = self.pager.get_page(parent.children[parent_idx\
          \ + 1])\n        self._merge_nodes(node, right, parent, parent_idx, path)\n\ndef _borrow_from_left(self, node, left,\
          \ parent, idx):\n    if node.is_leaf:\n        # Move last key/value from left to node\n        node.keys.insert(0,\
          \ left.keys.pop())\n        node.values.insert(0, left.values.pop())\n        parent.keys[idx - 1] = node.keys[0]\n\
          \    else:\n        # Rotate: left.last -> parent -> node.first\n        node.keys.insert(0, parent.keys[idx - 1])\n\
          \        parent.keys[idx - 1] = left.keys.pop()\n        node.children.insert(0, left.children.pop())\n    \n  \
          \  self.pager.write_page(node)\n    self.pager.write_page(left)\n    self.pager.write_page(parent)"
      pitfalls:
      - Sibling selection
      - Key rotation through parent
      - Empty root handling
      concepts:
      - Node merging
      - Key redistribution
      - Tree rebalancing
      estimated_hours: 13-24
  build-bundler:
    id: build-bundler
    name: Build Your Own Bundler
    description: Build a JavaScript bundler like webpack/rollup with module resolution, tree shaking, and code splitting.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - JavaScript AST
    - Module systems (CommonJS, ESM)
    - File system operations
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible:
      - Go
      - Rust
    resources:
    - type: article
      name: Minipack - Simple bundler
      url: https://github.com/ronami/minipack
    - type: docs
      name: Rollup Plugin Development
      url: https://rollupjs.org/plugin-development/
    milestones:
    - id: 1
      name: Module Parsing
      description: Parse JavaScript files and extract dependencies.
      acceptance_criteria:
      - Parse ES modules (import/export)
      - Extract dependency list
      - Handle relative/absolute paths
      - Build module graph
      hints:
        level1: Use a parser like @babel/parser to get AST. Walk AST to find ImportDeclaration nodes.
        level2: Track both static imports and dynamic imports. Resolve paths relative to current file.
        level3: "const parser = require('@babel/parser');\nconst traverse = require('@babel/traverse').default;\nconst path\
          \ = require('path');\nconst fs = require('fs');\n\nfunction parseModule(filePath) {\n    const content = fs.readFileSync(filePath,\
          \ 'utf-8');\n    \n    const ast = parser.parse(content, {\n        sourceType: 'module',\n        plugins: ['jsx']\n\
          \    });\n    \n    const dependencies = [];\n    \n    traverse(ast, {\n        ImportDeclaration({ node }) {\n\
          \            dependencies.push({\n                source: node.source.value,\n                specifiers: node.specifiers.map(s\
          \ => ({\n                    type: s.type,\n                    imported: s.imported?.name,\n                  \
          \  local: s.local.name\n                }))\n            });\n        },\n        \n        ExportNamedDeclaration({\
          \ node }) {\n            if (node.source) {\n                dependencies.push({\n                    source: node.source.value,\n\
          \                    isReExport: true\n                });\n            }\n        },\n        \n        CallExpression({\
          \ node }) {\n            if (node.callee.type === 'Import') {\n                // Dynamic import\n             \
          \   if (node.arguments[0].type === 'StringLiteral') {\n                    dependencies.push({\n               \
          \         source: node.arguments[0].value,\n                        isDynamic: true\n                    });\n \
          \               }\n            }\n        }\n    });\n    \n    return {\n        filePath,\n        ast,\n    \
          \    content,\n        dependencies\n    };\n}\n\nfunction buildModuleGraph(entryPath) {\n    const modules = new\
          \ Map();\n    const queue = [path.resolve(entryPath)];\n    \n    while (queue.length > 0) {\n        const filePath\
          \ = queue.shift();\n        \n        if (modules.has(filePath)) continue;\n        \n        const module = parseModule(filePath);\n\
          \        modules.set(filePath, module);\n        \n        for (const dep of module.dependencies) {\n          \
          \  const resolvedPath = resolvePath(dep.source, filePath);\n            module.dependencies.find(d => d.source ===\
          \ dep.source).resolved = resolvedPath;\n            queue.push(resolvedPath);\n        }\n    }\n    \n    return\
          \ modules;\n}"
      pitfalls:
      - Circular dependencies
      - Node modules resolution
      - File extensions
      concepts:
      - AST parsing
      - Dependency analysis
      - Module graph
      estimated_hours: 10-15
    - id: 2
      name: Module Resolution
      description: Implement Node.js-style module resolution.
      acceptance_criteria:
      - Relative path resolution
      - node_modules lookup
      - Package.json main/exports
      - Index file fallback
      hints:
        level1: 'Relative: resolve against current file. Bare specifiers: walk up node_modules.'
        level2: Check package.json for main field. Try .js, .json, /index.js extensions.
        level3: "function resolvePath(specifier, fromPath) {\n    const fromDir = path.dirname(fromPath);\n    \n    // Relative\
          \ or absolute path\n    if (specifier.startsWith('.') || specifier.startsWith('/')) {\n        return resolveFile(path.resolve(fromDir,\
          \ specifier));\n    }\n    \n    // Bare specifier - node_modules\n    return resolveNodeModule(specifier, fromDir);\n\
          }\n\nfunction resolveFile(filePath) {\n    // Try exact path\n    if (fs.existsSync(filePath) && fs.statSync(filePath).isFile())\
          \ {\n        return filePath;\n    }\n    \n    // Try extensions\n    const extensions = ['.js', '.jsx', '.ts',\
          \ '.tsx', '.json'];\n    for (const ext of extensions) {\n        if (fs.existsSync(filePath + ext)) {\n       \
          \     return filePath + ext;\n        }\n    }\n    \n    // Try as directory\n    if (fs.existsSync(filePath) &&\
          \ fs.statSync(filePath).isDirectory()) {\n        // Check package.json\n        const pkgPath = path.join(filePath,\
          \ 'package.json');\n        if (fs.existsSync(pkgPath)) {\n            const pkg = JSON.parse(fs.readFileSync(pkgPath,\
          \ 'utf-8'));\n            if (pkg.main) {\n                return resolveFile(path.join(filePath, pkg.main));\n\
          \            }\n        }\n        \n        // Try index\n        return resolveFile(path.join(filePath, 'index'));\n\
          \    }\n    \n    throw new Error(`Cannot resolve: ${filePath}`);\n}\n\nfunction resolveNodeModule(specifier, fromDir)\
          \ {\n    // Handle scoped packages\n    const parts = specifier.split('/');\n    const packageName = specifier.startsWith('@')\
          \ \n        ? parts.slice(0, 2).join('/') \n        : parts[0];\n    const subpath = specifier.startsWith('@')\n\
          \        ? parts.slice(2).join('/')\n        : parts.slice(1).join('/');\n    \n    // Walk up directory tree\n\
          \    let dir = fromDir;\n    while (dir !== path.dirname(dir)) {\n        const nodeModules = path.join(dir, 'node_modules',\
          \ packageName);\n        if (fs.existsSync(nodeModules)) {\n            if (subpath) {\n                return resolveFile(path.join(nodeModules,\
          \ subpath));\n            }\n            return resolveFile(nodeModules);\n        }\n        dir = path.dirname(dir);\n\
          \    }\n    \n    throw new Error(`Cannot find module: ${specifier}`);\n}"
      pitfalls:
      - Symlinks
      - Package exports field
      - Conditional exports
      concepts:
      - Module resolution
      - Package management
      - Path resolution
      estimated_hours: 8-12
    - id: 3
      name: Bundle Generation
      description: Generate a single JavaScript bundle from module graph.
      acceptance_criteria:
      - Wrap modules in functions
      - Module registry
      - Handle imports/exports
      - Source maps
      hints:
        level1: Each module becomes a function. Registry maps IDs to modules. Require loads and caches.
        level2: Transform import/export to require/exports. Assign numeric IDs to modules.
        level3: "const generate = require('@babel/generator').default;\nconst t = require('@babel/types');\n\nfunction generateBundle(moduleGraph,\
          \ entryPath) {\n    const modules = [];\n    const moduleIds = new Map();\n    let nextId = 0;\n    \n    // Assign\
          \ IDs\n    for (const [filePath] of moduleGraph) {\n        moduleIds.set(filePath, nextId++);\n    }\n    \n  \
          \  // Transform each module\n    for (const [filePath, module] of moduleGraph) {\n        const id = moduleIds.get(filePath);\n\
          \        const transformedCode = transformModule(module, moduleIds);\n        \n        modules.push(`\n       \
          \     ${id}: function(module, exports, require) {\n                ${transformedCode}\n            }`);\n    }\n\
          \    \n    const entryId = moduleIds.get(path.resolve(entryPath));\n    \n    return `\n        (function(modules)\
          \ {\n            const installedModules = {};\n            \n            function require(moduleId) {\n        \
          \        if (installedModules[moduleId]) {\n                    return installedModules[moduleId].exports;\n   \
          \             }\n                \n                const module = installedModules[moduleId] = {\n             \
          \       exports: {}\n                };\n                \n                modules[moduleId](module, module.exports,\
          \ require);\n                \n                return module.exports;\n            }\n            \n           \
          \ return require(${entryId});\n        })({${modules.join(',')}});\n    `;\n}\n\nfunction transformModule(module,\
          \ moduleIds) {\n    traverse(module.ast, {\n        ImportDeclaration(path) {\n            const depId = moduleIds.get(path.node._resolved);\n\
          \            // Transform to require\n            // import { foo } from './bar' -> const { foo } = require(1)\n\
          \            const requireCall = t.callExpression(\n                t.identifier('require'),\n                [t.numericLiteral(depId)]\n\
          \            );\n            // ... handle different import types\n            path.replaceWith(/* ... */);\n  \
          \      },\n        \n        ExportNamedDeclaration(path) {\n            // exports.foo = foo;\n        },\n   \
          \     \n        ExportDefaultDeclaration(path) {\n            // exports.default = ...;\n        }\n    });\n  \
          \  \n    return generate(module.ast).code;\n}"
      pitfalls:
      - Circular dependencies
      - Live bindings
      - Default export handling
      concepts:
      - Code generation
      - Module wrapping
      - Runtime
      estimated_hours: 15-25
    - id: 4
      name: Tree Shaking
      description: Eliminate unused code from the bundle.
      acceptance_criteria:
      - Track used exports
      - Remove unused code
      - Handle side effects
      - Preserve necessary code
      hints:
        level1: Build usage graph from imports. Mark used exports. Remove unused declarations.
        level2: Side effects (top-level code) must be preserved unless marked pure.
        level3: "function treeShake(moduleGraph, entryPath) {\n    // Track what's used\n    const usedExports = new Map();\
          \  // filePath -> Set of export names\n    const usedModules = new Set();\n    \n    // Start from entry\n    function\
          \ markUsed(filePath, importedNames) {\n        usedModules.add(filePath);\n        \n        const module = moduleGraph.get(filePath);\n\
          \        if (!usedExports.has(filePath)) {\n            usedExports.set(filePath, new Set());\n        }\n     \
          \   \n        const used = usedExports.get(filePath);\n        \n        if (importedNames === '*') {\n        \
          \    // Namespace import - mark all\n            module.exports.forEach(exp => used.add(exp));\n        } else {\n\
          \            importedNames.forEach(name => used.add(name));\n        }\n        \n        // Follow dependencies\n\
          \        for (const dep of module.dependencies) {\n            const names = dep.specifiers?.map(s => s.imported\
          \ || 'default') || ['*'];\n            markUsed(dep.resolved, names);\n        }\n    }\n    \n    markUsed(path.resolve(entryPath),\
          \ ['*']);\n    \n    // Remove unused\n    for (const [filePath, module] of moduleGraph) {\n        if (!usedModules.has(filePath))\
          \ {\n            moduleGraph.delete(filePath);\n            continue;\n        }\n        \n        const used =\
          \ usedExports.get(filePath);\n        \n        traverse(module.ast, {\n            ExportNamedDeclaration(path)\
          \ {\n                if (path.node.declaration) {\n                    const name = path.node.declaration.id?.name;\n\
          \                    if (name && !used.has(name)) {\n                        // Check for side effects\n       \
          \                 if (!hasSideEffects(path.node.declaration)) {\n                            path.remove();\n  \
          \                      }\n                    }\n                }\n            },\n            \n            //\
          \ Remove unused top-level declarations\n            FunctionDeclaration(path) {\n                if (path.parent.type\
          \ === 'Program') {\n                    const name = path.node.id.name;\n                    if (!isUsed(name, module,\
          \ used)) {\n                        path.remove();\n                    }\n                }\n            }\n  \
          \      });\n    }\n    \n    return moduleGraph;\n}\n\nfunction hasSideEffects(node) {\n    // Conservative: assume\
          \ functions are pure\n    // But check for top-level calls, assignments to globals, etc.\n    let found = false;\n\
          \    traverse(node, {\n        CallExpression() { found = true; },\n        AssignmentExpression({ node }) {\n \
          \           if (node.left.type === 'MemberExpression') found = true;\n        }\n    });\n    return found;\n}"
      pitfalls:
      - Side effect detection
      - Re-exports
      - Dynamic access patterns
      concepts:
      - Dead code elimination
      - Static analysis
      - Purity
      estimated_hours: 17-28
  build-ci-system:
    id: build-ci-system
    name: Build Your Own CI System
    description: Build a continuous integration system that runs pipelines based on code changes.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Docker/containers
    - Git hooks/webhooks
    - Process management
    - Queue systems
    languages:
      recommended:
      - Go
      - Python
      - Rust
      also_possible:
      - JavaScript
      - Java
    resources:
    - type: article
      name: How GitHub Actions Works
      url: https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions
    - type: article
      name: Building a CI Server
      url: https://blog.boot.dev/education/build-ci-cd-server/
    - type: documentation
      name: Jenkins Architecture
      url: https://www.jenkins.io/doc/developer/architecture/
    milestones:
    - id: 1
      name: Pipeline Configuration Parser
      description: Parse and validate pipeline configuration files (YAML).
      acceptance_criteria:
      - YAML pipeline parsing
      - Step/stage structure
      - Environment variables
      - Conditional execution
      - Matrix builds
      hints:
        level1: Define a schema for pipelines with stages, jobs, and steps.
        level2: Support variables, conditionals (if), and matrix for parallel variants.
        level3: "## Pipeline Configuration\n\n```yaml\n# Example: .ci/pipeline.yaml\nname: Build and Test\n\non:\n  push:\n\
          \    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\nenv:\n  GO_VERSION: \"1.21\"\n\njobs:\n\
          \  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        go-version: [\"1.20\", \"1.21\"]\n \
          \   steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        \n      - name: Setup Go\n      \
          \  run: |\n          wget https://go.dev/dl/go${{ matrix.go-version }}.linux-amd64.tar.gz\n          tar -xzf go${{\
          \ matrix.go-version }}.linux-amd64.tar.gz\n          \n      - name: Run Tests\n        run: go test ./...\n   \
          \     \n  build:\n    needs: test\n    if: github.ref == 'refs/heads/main'\n    steps:\n      - name: Build\n  \
          \      run: go build -o app ./cmd/app\n        \n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n\
          \        with:\n          name: app-binary\n          path: app\n```\n\n```go\npackage pipeline\n\nimport (\n  \
          \  \"gopkg.in/yaml.v3\"\n)\n\ntype Pipeline struct {\n    Name string            `yaml:\"name\"`\n    On   Trigger\
          \           `yaml:\"on\"`\n    Env  map[string]string `yaml:\"env\"`\n    Jobs map[string]*Job   `yaml:\"jobs\"\
          `\n}\n\ntype Trigger struct {\n    Push        *PushTrigger        `yaml:\"push,omitempty\"`\n    PullRequest *PullRequestTrigger\
          \ `yaml:\"pull_request,omitempty\"`\n    Schedule    []CronTrigger       `yaml:\"schedule,omitempty\"`\n}\n\ntype\
          \ Job struct {\n    RunsOn   string            `yaml:\"runs-on\"`\n    Needs    []string          `yaml:\"needs,omitempty\"\
          `\n    If       string            `yaml:\"if,omitempty\"`\n    Strategy *Strategy         `yaml:\"strategy,omitempty\"\
          `\n    Env      map[string]string `yaml:\"env,omitempty\"`\n    Steps    []Step            `yaml:\"steps\"`\n}\n\
          \ntype Strategy struct {\n    Matrix    map[string][]interface{} `yaml:\"matrix\"`\n    FailFast  bool         \
          \            `yaml:\"fail-fast\"`\n    MaxParallel int                    `yaml:\"max-parallel\"`\n}\n\ntype Step\
          \ struct {\n    Name    string            `yaml:\"name\"`\n    Uses    string            `yaml:\"uses,omitempty\"\
          `\n    Run     string            `yaml:\"run,omitempty\"`\n    With    map[string]string `yaml:\"with,omitempty\"\
          `\n    Env     map[string]string `yaml:\"env,omitempty\"`\n    If      string            `yaml:\"if,omitempty\"\
          `\n}\n\nfunc ParsePipeline(data []byte) (*Pipeline, error) {\n    var p Pipeline\n    if err := yaml.Unmarshal(data,\
          \ &p); err != nil {\n        return nil, err\n    }\n    return &p, p.Validate()\n}\n\nfunc (p *Pipeline) Validate()\
          \ error {\n    // Check for circular dependencies\n    // Validate step configurations\n    // Verify matrix combinations\n\
          \    return nil\n}\n```"
      pitfalls:
      - Circular job dependencies
      - Invalid matrix combinations
      - Missing required fields
      concepts:
      - YAML parsing
      - Configuration validation
      - DAG construction
      estimated_hours: 8-12
    - id: 2
      name: Job Execution Engine
      description: Execute pipeline jobs in isolated containers.
      acceptance_criteria:
      - Container-based isolation
      - Step execution
      - Environment injection
      - Output capture
      - Artifact handling
      hints:
        level1: Use Docker to run each job in isolation. Mount workspace as volume.
        level2: Capture stdout/stderr, handle exit codes, support timeouts.
        level3: "## Job Execution\n\n```go\ntype JobExecutor struct {\n    dockerClient *docker.Client\n    workspace    string\n\
          }\n\ntype JobResult struct {\n    Status    string\n    StartTime time.Time\n    EndTime   time.Time\n    Steps\
          \     []StepResult\n    Artifacts []string\n}\n\ntype StepResult struct {\n    Name     string\n    Status   string\n\
          \    Output   string\n    Duration time.Duration\n    ExitCode int\n}\n\nfunc (e *JobExecutor) Execute(ctx context.Context,\
          \ job *Job, matrix map[string]interface{}) (*JobResult, error) {\n    result := &JobResult{\n        Status:   \
          \ \"running\",\n        StartTime: time.Now(),\n    }\n    \n    // Create container\n    containerConfig := &container.Config{\n\
          \        Image:      job.RunsOn,\n        WorkingDir: \"/workspace\",\n        Env:        buildEnv(job.Env, matrix),\n\
          \    }\n    \n    hostConfig := &container.HostConfig{\n        Binds: []string{\n            fmt.Sprintf(\"%s:/workspace\"\
          , e.workspace),\n        },\n    }\n    \n    resp, err := e.dockerClient.ContainerCreate(ctx, containerConfig,\
          \ hostConfig, nil, nil, \"\")\n    if err != nil {\n        return nil, err\n    }\n    defer e.dockerClient.ContainerRemove(ctx,\
          \ resp.ID, types.ContainerRemoveOptions{Force: true})\n    \n    if err := e.dockerClient.ContainerStart(ctx, resp.ID,\
          \ types.ContainerStartOptions{}); err != nil {\n        return nil, err\n    }\n    \n    // Execute steps\n   \
          \ for _, step := range job.Steps {\n        stepResult := e.executeStep(ctx, resp.ID, step, matrix)\n        result.Steps\
          \ = append(result.Steps, stepResult)\n        \n        if stepResult.Status == \"failed\" {\n            result.Status\
          \ = \"failed\"\n            break\n        }\n    }\n    \n    if result.Status == \"running\" {\n        result.Status\
          \ = \"success\"\n    }\n    result.EndTime = time.Now()\n    \n    return result, nil\n}\n\nfunc (e *JobExecutor)\
          \ executeStep(ctx context.Context, containerID string, step Step, matrix map[string]interface{}) StepResult {\n\
          \    result := StepResult{\n        Name: step.Name,\n    }\n    start := time.Now()\n    \n    // Interpolate variables\n\
          \    script := interpolateVariables(step.Run, matrix)\n    \n    // Create exec\n    execConfig := types.ExecConfig{\n\
          \        Cmd:          []string{\"/bin/sh\", \"-c\", script},\n        AttachStdout: true,\n        AttachStderr:\
          \ true,\n    }\n    \n    execResp, err := e.dockerClient.ContainerExecCreate(ctx, containerID, execConfig)\n  \
          \  if err != nil {\n        result.Status = \"failed\"\n        result.Output = err.Error()\n        return result\n\
          \    }\n    \n    // Attach and capture output\n    attachResp, _ := e.dockerClient.ContainerExecAttach(ctx, execResp.ID,\
          \ types.ExecStartCheck{})\n    defer attachResp.Close()\n    \n    var output bytes.Buffer\n    io.Copy(&output,\
          \ attachResp.Reader)\n    \n    // Get exit code\n    inspectResp, _ := e.dockerClient.ContainerExecInspect(ctx,\
          \ execResp.ID)\n    \n    result.Duration = time.Since(start)\n    result.Output = output.String()\n    result.ExitCode\
          \ = inspectResp.ExitCode\n    \n    if result.ExitCode == 0 {\n        result.Status = \"success\"\n    } else {\n\
          \        result.Status = \"failed\"\n    }\n    \n    return result\n}\n```"
      pitfalls:
      - Container cleanup on failure
      - Timeout handling
      - Large output handling
      concepts:
      - Container isolation
      - Process execution
      - Resource management
      estimated_hours: 12-18
    - id: 3
      name: Webhook & Queue System
      description: Handle webhooks and queue jobs for execution.
      acceptance_criteria:
      - Git webhook handling
      - Job queue
      - Concurrent execution
      - Rate limiting
      - Priority scheduling
      hints:
        level1: Parse webhook payloads to determine which pipelines to trigger.
        level2: Use a work queue with configurable parallelism. Track job state.
        level3: "## Webhook & Queue\n\n```go\n// Webhook handler\nfunc (s *CIServer) handleWebhook(w http.ResponseWriter,\
          \ r *http.Request) {\n    // Verify signature\n    signature := r.Header.Get(\"X-Hub-Signature-256\")\n    if !verifySignature(r.Body,\
          \ signature, s.webhookSecret) {\n        http.Error(w, \"Invalid signature\", http.StatusUnauthorized)\n       \
          \ return\n    }\n    \n    var payload WebhookPayload\n    if err := json.NewDecoder(r.Body).Decode(&payload); err\
          \ != nil {\n        http.Error(w, err.Error(), http.StatusBadRequest)\n        return\n    }\n    \n    // Determine\
          \ event type\n    eventType := r.Header.Get(\"X-GitHub-Event\")\n    \n    // Find matching pipelines\n    pipelines\
          \ := s.findMatchingPipelines(payload.Repository, eventType, payload)\n    \n    // Queue jobs\n    for _, pipeline\
          \ := range pipelines {\n        build := &Build{\n            ID:         uuid.New().String(),\n            Pipeline:\
          \   pipeline,\n            Commit:     payload.After,\n            Branch:     payload.Ref,\n            Author:\
          \     payload.Pusher.Name,\n            Status:     \"queued\",\n            QueuedAt:   time.Now(),\n        }\n\
          \        s.queue.Enqueue(build)\n    }\n    \n    w.WriteHeader(http.StatusAccepted)\n}\n\n// Job Queue\ntype JobQueue\
          \ struct {\n    mu          sync.Mutex\n    pending     []*Build\n    running     map[string]*Build\n    maxParallel\
          \ int\n    workers     chan struct{}\n}\n\nfunc NewJobQueue(maxParallel int) *JobQueue {\n    return &JobQueue{\n\
          \        pending:     make([]*Build, 0),\n        running:     make(map[string]*Build),\n        maxParallel: maxParallel,\n\
          \        workers:     make(chan struct{}, maxParallel),\n    }\n}\n\nfunc (q *JobQueue) Enqueue(build *Build) {\n\
          \    q.mu.Lock()\n    // Priority: main branch > PRs > other branches\n    insertIdx := len(q.pending)\n    for\
          \ i, b := range q.pending {\n        if build.Priority() > b.Priority() {\n            insertIdx = i\n         \
          \   break\n        }\n    }\n    q.pending = append(q.pending[:insertIdx], append([]*Build{build}, q.pending[insertIdx:]...)...)\n\
          \    q.mu.Unlock()\n    \n    q.tryDispatch()\n}\n\nfunc (q *JobQueue) tryDispatch() {\n    q.mu.Lock()\n    defer\
          \ q.mu.Unlock()\n    \n    for len(q.pending) > 0 && len(q.running) < q.maxParallel {\n        build := q.pending[0]\n\
          \        q.pending = q.pending[1:]\n        q.running[build.ID] = build\n        \n        go func(b *Build) {\n\
          \            q.workers <- struct{}{}  // Acquire worker slot\n            defer func() { <-q.workers }()\n     \
          \       \n            b.Run()\n            \n            q.mu.Lock()\n            delete(q.running, b.ID)\n    \
          \        q.mu.Unlock()\n            \n            q.tryDispatch()  // Try to dispatch next job\n        }(build)\n\
          \    }\n}\n```"
      pitfalls:
      - Webhook replay attacks
      - Queue starvation
      - Resource exhaustion
      concepts:
      - Webhook security
      - Work queues
      - Concurrency control
      estimated_hours: 10-15
    - id: 4
      name: Web Dashboard
      description: Build a dashboard for viewing builds and logs.
      acceptance_criteria:
      - Build list view
      - Real-time log streaming
      - Build status badges
      - Pipeline visualization
      hints:
        level1: Use WebSockets for real-time log streaming.
        level2: Show pipeline as a graph with job nodes and dependency edges.
        level3: "## Dashboard Implementation\n\n```go\n// Log streaming via WebSocket\nfunc (s *CIServer) handleLogStream(w\
          \ http.ResponseWriter, r *http.Request) {\n    buildID := r.URL.Query().Get(\"build\")\n    \n    upgrader := websocket.Upgrader{}\n\
          \    conn, err := upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        return\n    }\n    defer conn.Close()\n\
          \    \n    // Subscribe to build logs\n    logChan := s.logBroker.Subscribe(buildID)\n    defer s.logBroker.Unsubscribe(buildID,\
          \ logChan)\n    \n    // Send existing logs\n    existingLogs := s.logStore.GetLogs(buildID)\n    for _, log :=\
          \ range existingLogs {\n        conn.WriteJSON(log)\n    }\n    \n    // Stream new logs\n    for {\n        select\
          \ {\n        case log, ok := <-logChan:\n            if !ok {\n                return\n            }\n         \
          \   if err := conn.WriteJSON(log); err != nil {\n                return\n            }\n        case <-r.Context().Done():\n\
          \            return\n        }\n    }\n}\n```\n\n```javascript\n// Frontend Log Viewer\nfunction LogViewer({ buildId\
          \ }) {\n  const [logs, setLogs] = useState([]);\n  const logEndRef = useRef(null);\n  \n  useEffect(() => {\n  \
          \  const ws = new WebSocket(`ws://localhost:8080/logs?build=${buildId}`);\n    \n    ws.onmessage = (event) => {\n\
          \      const log = JSON.parse(event.data);\n      setLogs(prev => [...prev, log]);\n    };\n    \n    return ()\
          \ => ws.close();\n  }, [buildId]);\n  \n  useEffect(() => {\n    logEndRef.current?.scrollIntoView({ behavior: 'smooth'\
          \ });\n  }, [logs]);\n  \n  return (\n    <div className=\"log-viewer\">\n      {logs.map((log, i) => (\n      \
          \  <div key={i} className={`log-line ${log.level}`}>\n          <span className=\"timestamp\">{log.timestamp}</span>\n\
          \          <span className=\"step\">[{log.step}]</span>\n          <span className=\"message\">{log.message}</span>\n\
          \        </div>\n      ))}\n      <div ref={logEndRef} />\n    </div>\n  );\n}\n\n// Pipeline Graph\nfunction PipelineGraph({\
          \ pipeline, buildStatus }) {\n  return (\n    <div className=\"pipeline-graph\">\n      {Object.entries(pipeline.jobs).map(([name,\
          \ job]) => (\n        <JobNode\n          key={name}\n          name={name}\n          job={job}\n          status={buildStatus.jobs[name]?.status\
          \ || 'pending'}\n          needs={job.needs || []}\n        />\n      ))}\n    </div>\n  );\n}\n```\n\n```html\n\
          <!-- Status Badge endpoint -->\n/api/badge/:repo/:branch\n\nReturns SVG:\n<svg xmlns=\"http://www.w3.org/2000/svg\"\
          \ width=\"100\" height=\"20\">\n  <rect width=\"100\" height=\"20\" fill=\"#555\"/>\n  <rect x=\"50\" width=\"50\"\
          \ height=\"20\" fill=\"#4c1\"/>  <!-- green for passing -->\n  <text x=\"25\" y=\"14\" fill=\"#fff\" text-anchor=\"\
          middle\">build</text>\n  <text x=\"75\" y=\"14\" fill=\"#fff\" text-anchor=\"middle\">passing</text>\n</svg>\n```"
      pitfalls:
      - WebSocket connection management
      - Large log performance
      - Badge caching
      concepts:
      - Real-time streaming
      - Data visualization
      - SVG generation
      estimated_hours: 10-15
  build-debugger:
    id: build-debugger
    name: Build Your Own Debugger
    description: Build a debugger like GDB. Learn ptrace, breakpoints, and symbol tables.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Unix processes
    - Assembly basics
    - ELF format
    languages:
      recommended:
      - C
      - Rust
      - C++
      also_possible:
      - Go
    resources:
    - type: blog
      name: Writing a Linux Debugger
      url: https://blog.tartanllama.xyz/writing-a-linux-debugger-setup/
    - type: book
      name: The Linux Programming Interface - Ch 26
      url: https://man7.org/tlpi/
    milestones:
    - id: 1
      name: Process Control
      description: Use ptrace to control debugee execution.
      acceptance_criteria:
      - Fork and trace child
      - Single-step execution
      - Continue execution
      - Wait for signals
      hints:
        level1: ptrace(PTRACE_TRACEME) in child, parent uses PTRACE_CONT, PTRACE_SINGLESTEP.
        level2: Child stops on exec. Use waitpid to detect stops. Check WIFSTOPPED.
        level3: "#include <sys/ptrace.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <stdio.h>\n\nstruct debugger\
          \ {\n    pid_t pid;\n    int running;\n};\n\nvoid run_debugee(const char* prog) {\n    ptrace(PTRACE_TRACEME, 0,\
          \ NULL, NULL);\n    execl(prog, prog, NULL);\n}\n\nstruct debugger* create_debugger(const char* prog) {\n    struct\
          \ debugger* dbg = malloc(sizeof(struct debugger));\n    \n    pid_t pid = fork();\n    if (pid == 0) {\n       \
          \ // Child\n        run_debugee(prog);\n    }\n    \n    // Parent\n    dbg->pid = pid;\n    dbg->running = 0;\n\
          \    \n    // Wait for child to stop at exec\n    int status;\n    waitpid(pid, &status, 0);\n    \n    // Set options\n\
          \    ptrace(PTRACE_SETOPTIONS, pid, NULL, PTRACE_O_EXITKILL);\n    \n    return dbg;\n}\n\nvoid continue_execution(struct\
          \ debugger* dbg) {\n    ptrace(PTRACE_CONT, dbg->pid, NULL, NULL);\n    dbg->running = 1;\n    \n    int status;\n\
          \    waitpid(dbg->pid, &status, 0);\n    dbg->running = 0;\n    \n    if (WIFSTOPPED(status)) {\n        printf(\"\
          Stopped with signal %d\\n\", WSTOPSIG(status));\n    } else if (WIFEXITED(status)) {\n        printf(\"Exited with\
          \ code %d\\n\", WEXITSTATUS(status));\n    }\n}\n\nvoid single_step(struct debugger* dbg) {\n    ptrace(PTRACE_SINGLESTEP,\
          \ dbg->pid, NULL, NULL);\n    int status;\n    waitpid(dbg->pid, &status, 0);\n}"
      pitfalls:
      - Signal handling in debugger
      - Race conditions
      - Zombie processes
      concepts:
      - ptrace
      - Process control
      - Unix signals
      estimated_hours: 8-12
    - id: 2
      name: Breakpoints
      description: Implement software breakpoints using int3.
      acceptance_criteria:
      - Set breakpoint at address
      - Replace instruction with int3 (0xCC)
      - Restore on hit
      - Handle breakpoint hit
      hints:
        level1: Read original byte with PTRACE_PEEKTEXT, write 0xCC. On hit, restore and rewind PC.
        level2: Breakpoint hit causes SIGTRAP. PC points past int3, so decrement by 1.
        level3: "#include <stdint.h>\n#include <sys/ptrace.h>\n\nstruct breakpoint {\n    uint64_t addr;\n    uint8_t saved_byte;\n\
          \    int enabled;\n};\n\nstruct breakpoint* set_breakpoint(struct debugger* dbg, uint64_t addr) {\n    struct breakpoint*\
          \ bp = malloc(sizeof(struct breakpoint));\n    bp->addr = addr;\n    bp->enabled = 0;\n    \n    // Read original\
          \ instruction\n    long data = ptrace(PTRACE_PEEKTEXT, dbg->pid, addr, NULL);\n    bp->saved_byte = (uint8_t)(data\
          \ & 0xFF);\n    \n    // Replace with int3 (0xCC)\n    long modified = (data & ~0xFF) | 0xCC;\n    ptrace(PTRACE_POKETEXT,\
          \ dbg->pid, addr, modified);\n    \n    bp->enabled = 1;\n    return bp;\n}\n\nvoid disable_breakpoint(struct debugger*\
          \ dbg, struct breakpoint* bp) {\n    if (!bp->enabled) return;\n    \n    long data = ptrace(PTRACE_PEEKTEXT, dbg->pid,\
          \ bp->addr, NULL);\n    long restored = (data & ~0xFF) | bp->saved_byte;\n    ptrace(PTRACE_POKETEXT, dbg->pid,\
          \ bp->addr, restored);\n    \n    bp->enabled = 0;\n}\n\nvoid handle_breakpoint_hit(struct debugger* dbg, struct\
          \ breakpoint* bp) {\n    // Get registers\n    struct user_regs_struct regs;\n    ptrace(PTRACE_GETREGS, dbg->pid,\
          \ NULL, &regs);\n    \n    // PC is now past int3, rewind\n    regs.rip = bp->addr;\n    ptrace(PTRACE_SETREGS,\
          \ dbg->pid, NULL, &regs);\n    \n    // Disable breakpoint, single step, re-enable\n    disable_breakpoint(dbg,\
          \ bp);\n    single_step(dbg);\n    enable_breakpoint(dbg, bp);\n}"
      pitfalls:
      - Multi-byte instruction boundaries
      - Breakpoint in loop
      - Thread safety
      concepts:
      - Software breakpoints
      - Instruction patching
      - Program counter
      estimated_hours: 10-15
    - id: 3
      name: Symbol Tables
      description: Parse DWARF debug info for source-level debugging.
      acceptance_criteria:
      - Parse ELF sections
      - Read DWARF DIEs
      - Map addresses to lines
      - Map names to addresses
      hints:
        level1: ELF has .debug_info, .debug_line sections. Use libdwarf or parse manually.
        level2: DWARF uses DIEs (Debug Info Entries) in tree structure. Line table maps PC to source.
        level3: "// Using libdwarf\n#include <libdwarf/libdwarf.h>\n#include <libdwarf/dwarf.h>\n\nstruct line_info {\n  \
          \  uint64_t addr;\n    unsigned int line;\n    const char* file;\n};\n\nstruct symbol_table {\n    Dwarf_Debug dbg;\n\
          \    struct line_info* lines;\n    size_t num_lines;\n    // function name -> address map\n};\n\nstruct symbol_table*\
          \ load_symbols(const char* binary) {\n    struct symbol_table* st = calloc(1, sizeof(struct symbol_table));\n  \
          \  \n    int fd = open(binary, O_RDONLY);\n    Dwarf_Error err;\n    \n    if (dwarf_init(fd, DW_DLC_READ, NULL,\
          \ NULL, &st->dbg, &err) != DW_DLV_OK) {\n        return NULL;\n    }\n    \n    // Iterate compilation units\n \
          \   Dwarf_Unsigned cu_header_length;\n    while (dwarf_next_cu_header(st->dbg, &cu_header_length, NULL, NULL, NULL,\
          \ NULL, &err) == DW_DLV_OK) {\n        Dwarf_Die cu_die;\n        dwarf_siblingof(st->dbg, NULL, &cu_die, &err);\n\
          \        \n        // Get line table\n        Dwarf_Line* lines;\n        Dwarf_Signed num_lines;\n        dwarf_srclines(cu_die,\
          \ &lines, &num_lines, &err);\n        \n        for (int i = 0; i < num_lines; i++) {\n            Dwarf_Addr addr;\n\
          \            Dwarf_Unsigned lineno;\n            char* file;\n            \n            dwarf_lineaddr(lines[i],\
          \ &addr, &err);\n            dwarf_lineno(lines[i], &lineno, &err);\n            dwarf_linesrc(lines[i], &file,\
          \ &err);\n            \n            // Store mapping\n            add_line_info(st, addr, lineno, file);\n     \
          \   }\n    }\n    \n    return st;\n}\n\nuint64_t symbol_to_addr(struct symbol_table* st, const char* name) {\n\
          \    // Search DIEs for function with matching name\n    // Return low_pc attribute\n}\n\nstruct line_info* addr_to_line(struct\
          \ symbol_table* st, uint64_t addr) {\n    // Binary search in sorted line table\n    // Return entry with largest\
          \ addr <= target\n}"
      pitfalls:
      - DWARF version differences
      - Inlined functions
      - Optimized code mapping
      concepts:
      - Debug information
      - Symbol tables
      - ELF/DWARF
      estimated_hours: 15-25
    - id: 4
      name: Variable Inspection
      description: Read and display variable values using debug info.
      acceptance_criteria:
      - Get variable location from DWARF
      - Read register/memory values
      - Handle different types
      - Display struct members
      hints:
        level1: DWARF location expressions describe where variable lives (register, stack, etc.).
        level2: Use DW_AT_type to get variable type. Recursively handle pointers, arrays, structs.
        level3: "// Variable location from DWARF\nenum loc_type { LOC_REG, LOC_ADDR, LOC_EXPR };\n\nstruct var_location {\n\
          \    enum loc_type type;\n    union {\n        int reg;           // Register number\n        uint64_t addr;   \
          \  // Memory address\n        Dwarf_Loc* expr;   // Location expression\n    };\n};\n\nstruct var_location get_var_location(struct\
          \ symbol_table* st, const char* var_name, uint64_t pc) {\n    // Find variable DIE\n    Dwarf_Die var_die = find_variable_die(st,\
          \ var_name, pc);\n    \n    // Get location attribute\n    Dwarf_Attribute loc_attr;\n    dwarf_attr(var_die, DW_AT_location,\
          \ &loc_attr, NULL);\n    \n    Dwarf_Loc* locs;\n    Dwarf_Signed num_locs;\n    dwarf_loclist(loc_attr, &locs,\
          \ &num_locs, NULL);\n    \n    // Interpret first location atom\n    struct var_location result;\n    \n    switch\
          \ (locs[0].lr_atom) {\n        case DW_OP_reg0 ... DW_OP_reg31:\n            result.type = LOC_REG;\n          \
          \  result.reg = locs[0].lr_atom - DW_OP_reg0;\n            break;\n        case DW_OP_fbreg:  // Offset from frame\
          \ base\n            result.type = LOC_ADDR;\n            result.addr = get_frame_base(dbg) + locs[0].lr_number;\n\
          \            break;\n        // ... handle other cases\n    }\n    \n    return result;\n}\n\nchar* read_variable(struct\
          \ debugger* dbg, struct symbol_table* st, const char* name) {\n    uint64_t pc = get_pc(dbg);\n    struct var_location\
          \ loc = get_var_location(st, name, pc);\n    Dwarf_Die type_die = get_var_type(st, name, pc);\n    \n    uint64_t\
          \ value;\n    if (loc.type == LOC_REG) {\n        value = get_register_value(dbg, loc.reg);\n    } else {\n    \
          \    value = ptrace(PTRACE_PEEKDATA, dbg->pid, loc.addr, NULL);\n    }\n    \n    // Format based on type\n    return\
          \ format_value(value, type_die);\n}"
      pitfalls:
      - Optimized-out variables
      - Complex location expressions
      - Type alignment
      concepts:
      - Variable location
      - Type information
      - Register access
      estimated_hours: 17-28
  build-distributed-kv:
    id: build-distributed-kv
    name: Build Your Own Distributed KV Store
    description: Build a distributed key-value store with partitioning and replication. Learn consistent hashing, shard management,
      and distributed transactions.
    difficulty: expert
    estimated_hours: 70-120
    prerequisites:
    - Build Raft or 2PC
    - Consistent hashing
    - RPC framework
    languages:
      recommended:
      - Go
      - Rust
      - Java
      also_possible:
      - C++
      - Scala
    resources:
    - type: paper
      name: 'Dynamo: Amazon''s Key-Value Store'
      url: https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf
    - type: course
      name: MIT 6.824 Distributed Systems
      url: https://pdos.csail.mit.edu/6.824/
    milestones:
    - id: 1
      name: Consistent Hashing
      description: Implement consistent hashing for key distribution across nodes.
      acceptance_criteria:
      - Hash ring with virtual nodes
      - Key lookup
      - Node addition/removal
      - Minimal key redistribution
      hints:
        level1: Hash both keys and nodes onto ring. Key belongs to first node clockwise.
        level2: Virtual nodes improve balance. Each physical node has multiple positions.
        level3: "import hashlib\nfrom bisect import bisect_right\nfrom typing import List, Dict, Any\n\nclass ConsistentHash:\n\
          \    def __init__(self, virtual_nodes: int = 150):\n        self.virtual_nodes = virtual_nodes\n        self.ring\
          \ = []  # Sorted list of (hash, node_id)\n        self.nodes = {}  # node_id -> node_info\n    \n    def _hash(self,\
          \ key: str) -> int:\n        return int(hashlib.sha256(key.encode()).hexdigest(), 16)\n    \n    def add_node(self,\
          \ node_id: str, node_info: Any):\n        self.nodes[node_id] = node_info\n        \n        for i in range(self.virtual_nodes):\n\
          \            virtual_key = f'{node_id}:{i}'\n            hash_val = self._hash(virtual_key)\n            # Insert\
          \ maintaining sorted order\n            idx = bisect_right([h for h, _ in self.ring], hash_val)\n            self.ring.insert(idx,\
          \ (hash_val, node_id))\n    \n    def remove_node(self, node_id: str):\n        if node_id not in self.nodes:\n\
          \            return\n        \n        del self.nodes[node_id]\n        self.ring = [(h, n) for h, n in self.ring\
          \ if n != node_id]\n    \n    def get_node(self, key: str) -> str:\n        if not self.ring:\n            raise\
          \ NoNodesError()\n        \n        hash_val = self._hash(key)\n        idx = bisect_right([h for h, _ in self.ring],\
          \ hash_val)\n        \n        # Wrap around to first node if past end\n        if idx >= len(self.ring):\n    \
          \        idx = 0\n        \n        return self.ring[idx][1]\n    \n    def get_nodes(self, key: str, n: int) ->\
          \ List[str]:\n        \"\"\"Get n distinct nodes for replication\"\"\"\n        if len(self.nodes) < n:\n      \
          \      raise InsufficientNodes()\n        \n        hash_val = self._hash(key)\n        idx = bisect_right([h for\
          \ h, _ in self.ring], hash_val)\n        \n        result = []\n        seen = set()\n        \n        for i in\
          \ range(len(self.ring)):\n            _, node_id = self.ring[(idx + i) % len(self.ring)]\n            if node_id\
          \ not in seen:\n                result.append(node_id)\n                seen.add(node_id)\n                if len(result)\
          \ == n:\n                    break\n        \n        return result"
      pitfalls:
      - Hot spots with few nodes
      - Virtual node count tuning
      - Rebalancing overhead
      concepts:
      - Consistent hashing
      - Load balancing
      - Data partitioning
      estimated_hours: 10-15
    - id: 2
      name: Replication
      description: Implement replication with configurable consistency levels.
      acceptance_criteria:
      - N replicas per key
      - Read/write quorums (R + W > N)
      - Sloppy quorum and hinted handoff
      - Conflict resolution
      hints:
        level1: For N=3, R=2, W=2 gives strong consistency. R=1, W=1 for availability.
        level2: 'Hinted handoff: if replica down, store hint at another node for later delivery.'
        level3: "from dataclasses import dataclass\nfrom typing import Optional\nimport time\n\n@dataclass\nclass VersionedValue:\n\
          \    value: bytes\n    version: int  # Vector clock or timestamp\n    timestamp: float\n\nclass ReplicatedStore:\n\
          \    def __init__(self, ring: ConsistentHash, n: int = 3, r: int = 2, w: int = 2):\n        self.ring = ring\n \
          \       self.n = n  # Replication factor\n        self.r = r  # Read quorum\n        self.w = w  # Write quorum\n\
          \        self.hints = {}  # target_node -> [(key, value)]\n    \n    async def put(self, key: str, value: bytes)\
          \ -> bool:\n        nodes = self.ring.get_nodes(key, self.n)\n        version = int(time.time() * 1000000)\n   \
          \     \n        versioned = VersionedValue(value, version, time.time())\n        \n        # Write to replicas\n\
          \        successes = 0\n        for node in nodes:\n            try:\n                await self._write_to_node(node,\
          \ key, versioned)\n                successes += 1\n            except NodeUnavailable:\n                # Hinted\
          \ handoff\n                hint_node = self._find_hint_node(nodes)\n                if hint_node:\n            \
          \        self._store_hint(hint_node, node, key, versioned)\n        \n        return successes >= self.w\n    \n\
          \    async def get(self, key: str) -> Optional[bytes]:\n        nodes = self.ring.get_nodes(key, self.n)\n     \
          \   \n        # Read from replicas\n        responses = []\n        for node in nodes:\n            try:\n     \
          \           val = await self._read_from_node(node, key)\n                if val:\n                    responses.append(val)\n\
          \                if len(responses) >= self.r:\n                    break\n            except NodeUnavailable:\n\
          \                continue\n        \n        if len(responses) < self.r:\n            raise ReadQuorumNotMet()\n\
          \        \n        # Return most recent version\n        latest = max(responses, key=lambda v: v.version)\n    \
          \    \n        # Read repair: update stale replicas\n        asyncio.create_task(self._read_repair(key, latest,\
          \ nodes))\n        \n        return latest.value\n    \n    async def _read_repair(self, key, latest, nodes):\n\
          \        for node in nodes:\n            try:\n                current = await self._read_from_node(node, key)\n\
          \                if not current or current.version < latest.version:\n                    await self._write_to_node(node,\
          \ key, latest)\n            except NodeUnavailable:\n                pass"
      pitfalls:
      - Quorum calculation
      - Read repair races
      - Hint delivery ordering
      concepts:
      - Quorum consensus
      - Eventual consistency
      - Hinted handoff
      estimated_hours: 15-25
    - id: 3
      name: Cluster Management
      description: Implement cluster membership and failure detection.
      acceptance_criteria:
      - Gossip protocol for membership
      - Failure detection
      - Automatic rebalancing
      - Cluster state convergence
      hints:
        level1: 'Gossip: periodically exchange state with random peers. State converges.'
        level2: Phi accrual failure detector adapts to network conditions.
        level3: "import random\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Set\n\n@dataclass\n\
          class NodeState:\n    status: str  # 'alive', 'suspect', 'dead'\n    heartbeat: int\n    last_seen: float = field(default_factory=time.time)\n\
          \nclass GossipProtocol:\n    def __init__(self, node_id: str, seeds: List[str]):\n        self.node_id = node_id\n\
          \        self.seeds = seeds\n        self.members: Dict[str, NodeState] = {}\n        self.heartbeat = 0\n     \
          \   self.suspect_timeout = 5.0\n        self.dead_timeout = 30.0\n    \n    async def run(self, interval: float\
          \ = 1.0):\n        while True:\n            await asyncio.sleep(interval)\n            self.heartbeat += 1\n   \
          \         \n            # Update own state\n            self.members[self.node_id] = NodeState('alive', self.heartbeat)\n\
          \            \n            # Gossip to random peer\n            peers = [n for n in self.members if n != self.node_id]\n\
          \            if not peers:\n                peers = self.seeds\n            \n            target = random.choice(peers)\n\
          \            await self._gossip_to(target)\n            \n            # Check for failures\n            self._detect_failures()\n\
          \    \n    async def _gossip_to(self, target: str):\n        try:\n            # Send our view, receive theirs\n\
          \            their_state = await rpc_call(target, 'gossip', self.members)\n            self._merge_state(their_state)\n\
          \        except Exception:\n            pass  # Target unreachable\n    \n    def _merge_state(self, remote: Dict[str,\
          \ NodeState]):\n        for node_id, state in remote.items():\n            if node_id not in self.members:\n   \
          \             self.members[node_id] = state\n            elif state.heartbeat > self.members[node_id].heartbeat:\n\
          \                self.members[node_id] = state\n                self.members[node_id].last_seen = time.time()\n\
          \    \n    def _detect_failures(self):\n        now = time.time()\n        for node_id, state in self.members.items():\n\
          \            if node_id == self.node_id:\n                continue\n            \n            age = now - state.last_seen\n\
          \            \n            if state.status == 'alive' and age > self.suspect_timeout:\n                state.status\
          \ = 'suspect'\n                self._on_node_suspect(node_id)\n            elif state.status == 'suspect' and age\
          \ > self.dead_timeout:\n                state.status = 'dead'\n                self._on_node_dead(node_id)\n   \
          \ \n    def _on_node_dead(self, node_id: str):\n        # Trigger rebalancing\n        self.ring.remove_node(node_id)\n\
          \        # Move data from failed node's range\n        asyncio.create_task(self._rebalance_for_failure(node_id))"
      pitfalls:
      - Gossip message size
      - Split brain
      - Cascade failures
      concepts:
      - Gossip protocols
      - Failure detection
      - Cluster membership
      estimated_hours: 18-30
    - id: 4
      name: Transactions
      description: Implement distributed transactions across shards.
      acceptance_criteria:
      - Single-key linearizability
      - Multi-key transactions (2PC)
      - Optimistic concurrency control
      - Deadlock detection
      hints:
        level1: 'Single-key: use Raft per shard. Multi-key: coordinate with 2PC.'
        level2: 'Optimistic: validate reads at commit time. Abort on conflict.'
        level3: "from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Set\nimport uuid\n\
          \nclass TxnState(Enum):\n    ACTIVE = 'active'\n    PREPARED = 'prepared'\n    COMMITTED = 'committed'\n    ABORTED\
          \ = 'aborted'\n\n@dataclass\nclass Transaction:\n    id: str\n    read_set: Dict[str, int]  # key -> version read\n\
          \    write_set: Dict[str, bytes]  # key -> new value\n    state: TxnState = TxnState.ACTIVE\n\nclass TransactionCoordinator:\n\
          \    def __init__(self, store: ReplicatedStore):\n        self.store = store\n        self.transactions: Dict[str,\
          \ Transaction] = {}\n    \n    def begin(self) -> str:\n        txn_id = str(uuid.uuid4())\n        self.transactions[txn_id]\
          \ = Transaction(id=txn_id, read_set={}, write_set={})\n        return txn_id\n    \n    async def read(self, txn_id:\
          \ str, key: str) -> bytes:\n        txn = self.transactions[txn_id]\n        \n        # Check write set first (read\
          \ your writes)\n        if key in txn.write_set:\n            return txn.write_set[key]\n        \n        # Read\
          \ from store\n        value, version = await self.store.get_versioned(key)\n        txn.read_set[key] = version\n\
          \        return value\n    \n    def write(self, txn_id: str, key: str, value: bytes):\n        txn = self.transactions[txn_id]\n\
          \        txn.write_set[key] = value\n    \n    async def commit(self, txn_id: str) -> bool:\n        txn = self.transactions[txn_id]\n\
          \        \n        # Group keys by shard\n        shards = self._group_by_shard(set(txn.read_set) | set(txn.write_set))\n\
          \        \n        # Phase 1: Prepare\n        prepared = []\n        for shard, keys in shards.items():\n     \
          \       try:\n                # Lock keys, validate read set\n                success = await self._prepare_shard(shard,\
          \ txn, keys)\n                if not success:\n                    await self._abort_prepared(prepared, txn)\n \
          \                   return False\n                prepared.append(shard)\n            except Exception:\n      \
          \          await self._abort_prepared(prepared, txn)\n                return False\n        \n        # Phase 2:\
          \ Commit\n        for shard in prepared:\n            await self._commit_shard(shard, txn)\n        \n        txn.state\
          \ = TxnState.COMMITTED\n        return True\n    \n    async def _prepare_shard(self, shard, txn, keys) -> bool:\n\
          \        # Validate read versions haven't changed\n        for key in keys:\n            if key in txn.read_set:\n\
          \                current_version = await self.store.get_version(key)\n                if current_version != txn.read_set[key]:\n\
          \                    return False  # Conflict\n        \n        # Acquire locks\n        locked = await self.store.try_lock_keys(shard,\
          \ keys, txn.id)\n        return locked"
      pitfalls:
      - Coordinator failure during 2PC
      - Lock timeout tuning
      - Phantom reads
      concepts:
      - Distributed transactions
      - 2PC
      - Serializability
      estimated_hours: 27-50
  build-dns:
    id: build-dns
    name: Build Your Own DNS Server
    description: Build a DNS server with recursive resolution.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - UDP networking
    - DNS protocol basics
    - Caching strategies
    languages:
      recommended:
      - Go
      - Rust
      - C
      also_possible:
      - Python
      - JavaScript
    resources:
    - type: rfc
      name: RFC 1035 - DNS
      url: https://tools.ietf.org/html/rfc1035
    - type: interactive
      name: CodeCrafters DNS
      url: https://app.codecrafters.io/courses/dns-server/overview
    - type: article
      name: How DNS Works
      url: https://howdns.works/
    milestones:
    - id: 1
      name: DNS Message Parsing
      description: Parse and construct DNS messages.
      acceptance_criteria:
      - Header parsing
      - Question section
      - Answer section
      - Name compression
      - Message construction
      hints:
        level1: 'DNS message: header (12 bytes) + question + answer + authority + additional'
        level2: Name compression uses pointers (0xC0 prefix) to reduce message size.
        level3: "## DNS Message Parsing\n\n```go\npackage dns\n\nimport (\n    \"bytes\"\n    \"encoding/binary\"\n)\n\ntype\
          \ Header struct {\n    ID      uint16\n    Flags   uint16\n    QDCount uint16  // Question count\n    ANCount uint16\
          \  // Answer count\n    NSCount uint16  // Authority count\n    ARCount uint16  // Additional count\n}\n\ntype Question\
          \ struct {\n    Name  string\n    Type  uint16\n    Class uint16\n}\n\ntype ResourceRecord struct {\n    Name  \
          \   string\n    Type     uint16\n    Class    uint16\n    TTL      uint32\n    RDLength uint16\n    RData    []byte\n\
          }\n\ntype Message struct {\n    Header    Header\n    Questions []Question\n    Answers   []ResourceRecord\n   \
          \ Authority []ResourceRecord\n    Additional []ResourceRecord\n}\n\nfunc ParseMessage(data []byte) (*Message, error)\
          \ {\n    reader := bytes.NewReader(data)\n    msg := &Message{}\n    \n    // Parse header\n    binary.Read(reader,\
          \ binary.BigEndian, &msg.Header)\n    \n    // Parse questions\n    for i := 0; i < int(msg.Header.QDCount); i++\
          \ {\n        q := Question{}\n        q.Name = parseName(data, reader)\n        binary.Read(reader, binary.BigEndian,\
          \ &q.Type)\n        binary.Read(reader, binary.BigEndian, &q.Class)\n        msg.Questions = append(msg.Questions,\
          \ q)\n    }\n    \n    // Parse answers\n    for i := 0; i < int(msg.Header.ANCount); i++ {\n        rr := parseRR(data,\
          \ reader)\n        msg.Answers = append(msg.Answers, rr)\n    }\n    \n    return msg, nil\n}\n\nfunc parseName(data\
          \ []byte, reader *bytes.Reader) string {\n    var name []string\n    for {\n        length, _ := reader.ReadByte()\n\
          \        \n        if length == 0 {\n            break\n        }\n        \n        // Check for compression pointer\n\
          \        if length&0xC0 == 0xC0 {\n            nextByte, _ := reader.ReadByte()\n            offset := int(length&0x3F)<<8\
          \ | int(nextByte)\n            // Follow pointer\n            ptrReader := bytes.NewReader(data[offset:])\n    \
          \        name = append(name, parseName(data, ptrReader))\n            break\n        }\n        \n        label\
          \ := make([]byte, length)\n        reader.Read(label)\n        name = append(name, string(label))\n    }\n    return\
          \ strings.Join(name, \".\")\n}\n\nfunc (m *Message) Serialize() []byte {\n    buf := new(bytes.Buffer)\n    \n \
          \   // Write header\n    binary.Write(buf, binary.BigEndian, m.Header)\n    \n    // Write questions\n    for _,\
          \ q := range m.Questions {\n        writeName(buf, q.Name)\n        binary.Write(buf, binary.BigEndian, q.Type)\n\
          \        binary.Write(buf, binary.BigEndian, q.Class)\n    }\n    \n    // Write answers\n    for _, rr := range\
          \ m.Answers {\n        writeRR(buf, rr)\n    }\n    \n    return buf.Bytes()\n}\n\nfunc writeName(buf *bytes.Buffer,\
          \ name string) {\n    parts := strings.Split(name, \".\")\n    for _, part := range parts {\n        buf.WriteByte(byte(len(part)))\n\
          \        buf.WriteString(part)\n    }\n    buf.WriteByte(0)  // Null terminator\n}\n```"
      pitfalls:
      - Compression pointer loops
      - Wrong byte order
      - Name not null-terminated
      concepts:
      - Binary protocols
      - Name compression
      - Message format
      estimated_hours: 10-15
    - id: 2
      name: Authoritative Server
      description: Respond to queries from local zone data.
      acceptance_criteria:
      - Zone file parsing
      - Query matching
      - SOA records
      - NS records
      - A/AAAA records
      hints:
        level1: Load zone data from file. Match queries against records.
        level2: Return NXDOMAIN for non-existent names. Set authority flag.
        level3: "## Authoritative Server\n\n```go\ntype Zone struct {\n    Origin  string\n    TTL     uint32\n    Records\
          \ map[string][]ResourceRecord\n}\n\nfunc LoadZone(filename string) (*Zone, error) {\n    zone := &Zone{\n      \
          \  Records: make(map[string][]ResourceRecord),\n    }\n    \n    // Parse zone file (simplified)\n    file, _ :=\
          \ os.Open(filename)\n    scanner := bufio.NewScanner(file)\n    \n    for scanner.Scan() {\n        line := scanner.Text()\n\
          \        if strings.HasPrefix(line, \";\") || line == \"\" {\n            continue\n        }\n        \n      \
          \  parts := strings.Fields(line)\n        name := parts[0]\n        if name == \"@\" {\n            name = zone.Origin\n\
          \        }\n        \n        rr := ResourceRecord{\n            Name:  name,\n            Class: 1,  // IN\n  \
          \      }\n        \n        // Parse type and data\n        switch parts[len(parts)-2] {\n        case \"A\":\n\
          \            rr.Type = 1\n            rr.RData = net.ParseIP(parts[len(parts)-1]).To4()\n        case \"AAAA\":\n\
          \            rr.Type = 28\n            rr.RData = net.ParseIP(parts[len(parts)-1]).To16()\n        case \"NS\":\n\
          \            rr.Type = 2\n            // Encode name\n        case \"MX\":\n            rr.Type = 15\n         \
          \   // Encode preference + exchange\n        }\n        \n        zone.Records[name] = append(zone.Records[name],\
          \ rr)\n    }\n    \n    return zone, nil\n}\n\ntype AuthServer struct {\n    zones map[string]*Zone\n}\n\nfunc (s\
          \ *AuthServer) HandleQuery(query *Message) *Message {\n    response := &Message{\n        Header: Header{\n    \
          \        ID:      query.Header.ID,\n            Flags:   0x8400,  // Response + Authoritative\n            QDCount:\
          \ query.Header.QDCount,\n        },\n        Questions: query.Questions,\n    }\n    \n    for _, q := range query.Questions\
          \ {\n        // Find matching zone\n        zone := s.findZone(q.Name)\n        if zone == nil {\n            response.Header.Flags\
          \ |= 0x0003  // NXDOMAIN\n            continue\n        }\n        \n        // Find matching records\n        records\
          \ := zone.Records[q.Name]\n        for _, rr := range records {\n            if rr.Type == q.Type || q.Type == 255\
          \ {  // 255 = ANY\n                response.Answers = append(response.Answers, rr)\n            }\n        }\n \
          \   }\n    \n    response.Header.ANCount = uint16(len(response.Answers))\n    return response\n}\n```"
      pitfalls:
      - Wildcard matching
      - CNAME chasing
      - Case insensitivity
      concepts:
      - Zone files
      - Record types
      - Authoritative responses
      estimated_hours: 10-15
    - id: 3
      name: Recursive Resolver
      description: Implement recursive resolution from root servers.
      acceptance_criteria:
      - Iterative queries to authoritative servers
      - Following referrals
      - Root hints
      - Glue records
      hints:
        level1: Start from root servers, follow NS referrals until authoritative answer.
        level2: Glue records provide IP addresses for nameservers in referrals.
        level3: "## Recursive Resolver\n\n```go\nvar RootServers = []string{\n    \"198.41.0.4\",   // a.root-servers.net\n\
          \    \"199.9.14.201\", // b.root-servers.net\n    // ... more root servers\n}\n\ntype Resolver struct {\n    cache\
          \ *Cache\n}\n\nfunc (r *Resolver) Resolve(name string, qtype uint16) ([]ResourceRecord, error) {\n    // Check cache\
          \ first\n    if cached := r.cache.Get(name, qtype); cached != nil {\n        return cached, nil\n    }\n    \n \
          \   // Start from root\n    nameservers := RootServers\n    \n    for {\n        // Query one of the nameservers\n\
          \        response, err := r.queryNS(nameservers[0], name, qtype)\n        if err != nil {\n            // Try next\
          \ nameserver\n            nameservers = nameservers[1:]\n            continue\n        }\n        \n        // Got\
          \ authoritative answer\n        if response.Header.Flags&0x0400 != 0 {  // AA flag\n            r.cache.Set(name,\
          \ qtype, response.Answers)\n            return response.Answers, nil\n        }\n        \n        // Got referral\
          \ - follow it\n        if len(response.Authority) > 0 {\n            nameservers = r.extractNS(response)\n     \
          \       continue\n        }\n        \n        // No answer, no referral\n        return nil, fmt.Errorf(\"resolution\
          \ failed\")\n    }\n}\n\nfunc (r *Resolver) queryNS(server, name string, qtype uint16) (*Message, error) {\n   \
          \ query := &Message{\n        Header: Header{\n            ID:      uint16(rand.Int()),\n            Flags:   0x0100,\
          \  // RD (Recursion Desired) = 0 for iterative\n            QDCount: 1,\n        },\n        Questions: []Question{{\n\
          \            Name:  name,\n            Type:  qtype,\n            Class: 1,\n        }},\n    }\n    \n    conn,\
          \ _ := net.Dial(\"udp\", server+\":53\")\n    defer conn.Close()\n    \n    conn.Write(query.Serialize())\n    \n\
          \    buf := make([]byte, 512)\n    n, _ := conn.Read(buf)\n    \n    return ParseMessage(buf[:n])\n}\n\nfunc (r\
          \ *Resolver) extractNS(response *Message) []string {\n    var servers []string\n    \n    // Get NS names from authority\
          \ section\n    nsNames := make(map[string]bool)\n    for _, rr := range response.Authority {\n        if rr.Type\
          \ == 2 {  // NS\n            nsNames[string(rr.RData)] = true\n        }\n    }\n    \n    // Look for glue records\
          \ (A records for NS)\n    for _, rr := range response.Additional {\n        if rr.Type == 1 {  // A record\n   \
          \         if nsNames[rr.Name] {\n                ip := net.IP(rr.RData)\n                servers = append(servers,\
          \ ip.String())\n            }\n        }\n    }\n    \n    // If no glue, need to resolve NS names separately\n\
          \    if len(servers) == 0 {\n        for name := range nsNames {\n            addrs, _ := r.Resolve(name, 1)  //\
          \ Resolve A record\n            for _, rr := range addrs {\n                servers = append(servers, net.IP(rr.RData).String())\n\
          \            }\n        }\n    }\n    \n    return servers\n}\n```"
      pitfalls:
      - Infinite referral loops
      - Missing glue records
      - CNAME handling
      concepts:
      - Iterative resolution
      - Referrals
      - DNS hierarchy
      estimated_hours: 12-18
    - id: 4
      name: Caching & Performance
      description: Implement caching with TTL and negative caching.
      acceptance_criteria:
      - TTL-based expiration
      - Negative caching
      - Cache poisoning prevention
      - UDP server with concurrency
      hints:
        level1: Cache answers with TTL countdown. Negative cache NXDOMAIN responses.
        level2: Validate response matches query. Use random query IDs.
        level3: "## DNS Cache\n\n```go\ntype CacheEntry struct {\n    Records    []ResourceRecord\n    Expiration time.Time\n\
          \    Negative   bool  // NXDOMAIN cache\n}\n\ntype Cache struct {\n    mu      sync.RWMutex\n    entries map[string]*CacheEntry\
          \  // key: name+type\n}\n\nfunc (c *Cache) makeKey(name string, qtype uint16) string {\n    return fmt.Sprintf(\"\
          %s:%d\", strings.ToLower(name), qtype)\n}\n\nfunc (c *Cache) Get(name string, qtype uint16) []ResourceRecord {\n\
          \    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    entry, ok := c.entries[c.makeKey(name, qtype)]\n    if !ok\
          \ {\n        return nil\n    }\n    \n    if time.Now().After(entry.Expiration) {\n        return nil  // Expired\n\
          \    }\n    \n    if entry.Negative {\n        return []ResourceRecord{}  // Negative cache hit\n    }\n    \n \
          \   // Adjust TTLs in returned records\n    remaining := uint32(entry.Expiration.Sub(time.Now()).Seconds())\n  \
          \  records := make([]ResourceRecord, len(entry.Records))\n    for i, rr := range entry.Records {\n        records[i]\
          \ = rr\n        records[i].TTL = remaining\n    }\n    \n    return records\n}\n\nfunc (c *Cache) Set(name string,\
          \ qtype uint16, records []ResourceRecord) {\n    if len(records) == 0 {\n        return\n    }\n    \n    // Use\
          \ minimum TTL\n    minTTL := records[0].TTL\n    for _, rr := range records[1:] {\n        if rr.TTL < minTTL {\n\
          \            minTTL = rr.TTL\n        }\n    }\n    \n    c.mu.Lock()\n    c.entries[c.makeKey(name, qtype)] = &CacheEntry{\n\
          \        Records:    records,\n        Expiration: time.Now().Add(time.Duration(minTTL) * time.Second),\n    }\n\
          \    c.mu.Unlock()\n}\n\nfunc (c *Cache) SetNegative(name string, qtype uint16, ttl uint32) {\n    c.mu.Lock()\n\
          \    c.entries[c.makeKey(name, qtype)] = &CacheEntry{\n        Negative:   true,\n        Expiration: time.Now().Add(time.Duration(ttl)\
          \ * time.Second),\n    }\n    c.mu.Unlock()\n}\n\n// Server with validation\nfunc (s *Server) handleQuery(addr *net.UDPAddr,\
          \ query *Message) {\n    response := s.resolver.Resolve(query)\n    \n    // Validate response\n    if response.Header.ID\
          \ != query.Header.ID {\n        return  // Ignore mismatched response\n    }\n    \n    s.conn.WriteToUDP(response.Serialize(),\
          \ addr)\n}\n```"
      pitfalls:
      - TTL underflow
      - Cache poisoning
      - Memory exhaustion
      concepts:
      - Caching strategies
      - Security
      - Concurrency
      estimated_hours: 10-15
  build-docker:
    id: build-docker
    name: Build Your Own Docker
    description: Build a container runtime that can run isolated processes using Linux primitives. You'll learn about namespaces,
      cgroups, and filesystem isolation.
    difficulty: expert
    estimated_hours: 30-50
    prerequisites:
    - Linux system administration
    - Process management (fork, exec)
    - Filesystem concepts
    - Basic networking
    languages:
      recommended:
      - Go
      - C
      - Rust
      also_possible: []
    resources:
    - name: Containers from Scratch
      url: https://ericchiang.github.io/post/containers-from-scratch/
      type: blog
    - name: Containers from Scratch (Video) - Liz Rice
      url: https://www.youtube.com/watch?v=8fi7uSYlOdc
      type: video
    - name: Linux Namespaces man page
      url: https://man7.org/linux/man-pages/man7/namespaces.7.html
      type: documentation
    - name: OCI Runtime Specification
      url: https://github.com/opencontainers/runtime-spec
      type: documentation
    milestones:
    - id: 1
      name: Process Isolation (Namespaces)
      description: Isolate a process using Linux namespaces (PID, UTS, mount).
      acceptance_criteria:
      - Process runs with its own PID namespace (PID 1)
      - Process has its own hostname (UTS)
      - Process has its own mount namespace
      - Uses clone() or unshare() syscalls
      hints:
        level1: Use CLONE_NEWPID, CLONE_NEWUTS, CLONE_NEWNS flags with clone().
        level2: 'unshare command in bash: unshare --pid --uts --mount --fork /bin/bash'
        level3: 'Linux namespace types:

          - PID: Process IDs

          - UTS: Hostname

          - Mount: Filesystem mounts

          - Net: Network stack

          - User: User/group IDs

          - IPC: Inter-process comm

          - Cgroup: Cgroup root'
      pitfalls:
      - Not using CLONE_NEWPID correctly
      - /proc still showing host PIDs
      - Permission issues (usually need root)
      concepts:
      - Linux namespaces
      - Process isolation
      - clone() syscall
      estimated_hours: 4-6
    - id: 2
      name: Resource Limits (cgroups)
      description: Limit container resources using cgroups (CPU, memory).
      acceptance_criteria:
      - Can limit memory usage (OOM-killed if exceeds)
      - Can limit CPU shares/quota
      - Cgroup filesystem properly set up
      - Process added to cgroup before exec
      hints:
        level1: Create directory in /sys/fs/cgroup/memory/mycontainer. Write PID to tasks file.
        level2: Set memory.limit_in_bytes for memory limit. Use cpu.cfs_quota_us for CPU.
        level3: 'cgroup v2 paths:

          - /sys/fs/cgroup/mycontainer/memory.max

          - /sys/fs/cgroup/mycontainer/cpu.max

          - /sys/fs/cgroup/mycontainer/cgroup.procs'
      pitfalls:
      - cgroup v1 vs v2 differences
      - Not cleaning up cgroups on exit
      - Memory limits not accounting kernel memory
      concepts:
      - Control groups
      - Resource limiting
      - OOM killer
      estimated_hours: 3-4
    - id: 3
      name: Filesystem Isolation (chroot/pivot_root)
      description: Give container its own root filesystem using chroot or pivot_root.
      acceptance_criteria:
      - Container sees only its own filesystem
      - Host filesystem is not accessible
      - Proc filesystem mounted in container
      - Works with any root filesystem (Alpine, Ubuntu)
      hints:
        level1: chroot /path/to/rootfs /bin/sh - simplest form
        level2: pivot_root is more secure than chroot. Requires mount namespace.
        level3: 'Setup steps:

          1. Create mount namespace

          2. Mount new root somewhere

          3. pivot_root newroot putold

          4. Unmount putold

          5. Mount /proc, /sys, etc.'
      pitfalls:
      - chroot is escapable without mount namespace
      - Forgetting to mount /proc
      - Not having required binaries in rootfs
      concepts:
      - chroot jails
      - pivot_root
      - Root filesystem
      estimated_hours: 3-4
    - id: 4
      name: Layered Filesystem (OverlayFS)
      description: Implement layered filesystem for efficient image storage.
      acceptance_criteria:
      - Multiple read-only layers merged into single view
      - Write layer captures all modifications
      - Copy-on-write semantics
      - Layers can be shared between containers
      hints:
        level1: 'OverlayFS: mount -t overlay overlay -o lowerdir=base,upperdir=changes,workdir=work target'
        level2: 'Multiple lower dirs: lowerdir=layer3:layer2:layer1 (later = higher priority)'
        level3: "Docker layer structure:\n/var/lib/docker/overlay2/\n  <layer-id>/\n    diff/    # Layer contents\n    lower\
          \    # Link to parent\n    work/    # OverlayFS work dir"
      pitfalls:
      - OverlayFS doesn't support all filesystems
      - Renaming directories has copy-up overhead
      - Open file handles survive layer changes
      concepts:
      - Union filesystems
      - Copy-on-write
      - Docker image layers
      estimated_hours: 4-6
    - id: 5
      name: Container Networking
      description: Set up network namespace with virtual ethernet pair.
      acceptance_criteria:
      - Container has its own network stack
      - veth pair connects container to host bridge
      - Container can reach the internet (via NAT)
      - Containers can communicate with each other
      hints:
        level1: ip netns add container; ip link add veth0 type veth peer name veth1
        level2: Move one end of veth into container namespace. Connect other to bridge.
        level3: 'Network setup:

          1. Create network namespace

          2. Create veth pair

          3. Move veth1 to container

          4. Attach veth0 to docker0 bridge

          5. Configure IP addresses

          6. Set up iptables NAT'
      pitfalls:
      - DNS resolution (need /etc/resolv.conf)
      - iptables rules for masquerading
      - Bridge vs host network
      concepts:
      - Network namespaces
      - Virtual ethernet
      - Linux bridge networking
      estimated_hours: 5-8
    - id: 6
      name: Image Format and CLI
      description: Implement OCI image format and Docker-compatible CLI.
      acceptance_criteria:
      - Pull images from Docker Hub
      - Parse OCI image manifest
      - Extract and layer filesystem
      - docker run equivalent command
      hints:
        level1: OCI image is tarball with manifest.json pointing to layer tarballs.
        level2: 'Docker Hub API: GET /v2/<name>/manifests/<tag>'
        level3: 'CLI commands to implement:

          - run: Create and start container

          - exec: Run command in existing container

          - ps: List running containers

          - images: List local images'
      pitfalls:
      - Image manifest v1 vs v2
      - Content-addressable storage
      - Image and layer deduplication
      concepts:
      - OCI specification
      - Container registry protocol
      - CLI design
      estimated_hours: 6-10
  build-emulator:
    id: build-emulator
    name: Build Your Own Emulator
    description: Build a NES, GameBoy, or CHIP-8 emulator. Learn CPU emulation, memory mapping, and timing.
    difficulty: expert
    estimated_hours: 60-120
    prerequisites:
    - Binary/hex
    - Assembly basics
    - Graphics basics
    languages:
      recommended:
      - C
      - Rust
      - C++
      also_possible:
      - Go
      - TypeScript
    resources:
    - type: guide
      name: Writing a CHIP-8 Emulator
      url: https://tobiasvl.github.io/blog/write-a-chip-8-emulator/
    - type: docs
      name: Pan Docs (Game Boy)
      url: https://gbdev.io/pandocs/
    - type: wiki
      name: NesDev Wiki
      url: https://www.nesdev.org/wiki/Nesdev_Wiki
    milestones:
    - id: 1
      name: CPU Emulation
      description: Implement the instruction set and registers.
      acceptance_criteria:
      - All CPU registers
      - Fetch-decode-execute cycle
      - All opcodes implemented
      - Correct flags behavior
      hints:
        level1: CHIP-8 has 35 opcodes. Game Boy has ~500. Start simple.
        level2: Use a big switch statement or function pointer table for opcodes.
        level3: "// CHIP-8 CPU example\nstruct CPU {\n    uint8_t V[16];      // General registers V0-VF\n    uint16_t I;\
          \         // Index register\n    uint16_t PC;        // Program counter\n    uint8_t SP;         // Stack pointer\n\
          \    uint16_t stack[16]; // Call stack\n    uint8_t delay_timer;\n    uint8_t sound_timer;\n};\n\nvoid cpu_init(struct\
          \ CPU* cpu) {\n    memset(cpu, 0, sizeof(struct CPU));\n    cpu->PC = 0x200;  // Programs start at 0x200\n}\n\n\
          void cpu_cycle(struct CPU* cpu, uint8_t* memory) {\n    // Fetch: 2-byte opcode\n    uint16_t opcode = (memory[cpu->PC]\
          \ << 8) | memory[cpu->PC + 1];\n    cpu->PC += 2;\n    \n    // Decode and execute\n    uint8_t x = (opcode >> 8)\
          \ & 0x0F;  // Second nibble\n    uint8_t y = (opcode >> 4) & 0x0F;  // Third nibble\n    uint8_t n = opcode & 0x0F;\
          \         // Fourth nibble\n    uint8_t nn = opcode & 0xFF;        // Second byte\n    uint16_t nnn = opcode & 0x0FFF;\
          \    // Last 12 bits\n    \n    switch (opcode & 0xF000) {\n        case 0x0000:\n            switch (opcode) {\n\
          \                case 0x00E0: // CLS - Clear screen\n                    clear_display();\n                    break;\n\
          \                case 0x00EE: // RET - Return\n                    cpu->SP--;\n                    cpu->PC = cpu->stack[cpu->SP];\n\
          \                    break;\n            }\n            break;\n        \n        case 0x1000: // JP addr - Jump\n\
          \            cpu->PC = nnn;\n            break;\n        \n        case 0x2000: // CALL addr\n            cpu->stack[cpu->SP]\
          \ = cpu->PC;\n            cpu->SP++;\n            cpu->PC = nnn;\n            break;\n        \n        case 0x3000:\
          \ // SE Vx, byte - Skip if equal\n            if (cpu->V[x] == nn) cpu->PC += 2;\n            break;\n        \n\
          \        case 0x6000: // LD Vx, byte\n            cpu->V[x] = nn;\n            break;\n        \n        case 0x7000:\
          \ // ADD Vx, byte\n            cpu->V[x] += nn;\n            break;\n        \n        case 0x8000: // Arithmetic\n\
          \            switch (n) {\n                case 0x0: cpu->V[x] = cpu->V[y]; break;  // LD\n                case\
          \ 0x1: cpu->V[x] |= cpu->V[y]; break; // OR\n                case 0x2: cpu->V[x] &= cpu->V[y]; break; // AND\n \
          \               case 0x4: // ADD with carry\n                    cpu->V[0xF] = (cpu->V[x] + cpu->V[y]) > 255 ? 1\
          \ : 0;\n                    cpu->V[x] += cpu->V[y];\n                    break;\n                // ... more arithmetic\
          \ ops\n            }\n            break;\n        \n        case 0xD000: // DRW - Draw sprite\n            draw_sprite(cpu,\
          \ memory, x, y, n);\n            break;\n        // ... remaining opcodes\n    }\n}"
      pitfalls:
      - Endianness
      - Flag edge cases
      - Undocumented opcodes
      concepts:
      - CPU architecture
      - Instruction sets
      - Machine code
      estimated_hours: 15-30
    - id: 2
      name: Memory System
      description: Implement memory mapping and bank switching.
      acceptance_criteria:
      - Memory map implementation
      - ROM loading
      - RAM regions
      - Memory-mapped I/O
      hints:
        level1: 'CHIP-8 is simple: 4KB flat memory. Game Boy has ROM banking.'
        level2: 'Memory-mapped I/O: writes to certain addresses control hardware.'
        level3: "// Game Boy memory map\nstruct MMU {\n    uint8_t* rom;           // Cartridge ROM\n    uint8_t* rom_banks;\
          \     // Switchable ROM banks\n    uint8_t wram[8192];     // Work RAM\n    uint8_t vram[8192];     // Video RAM\n\
          \    uint8_t oam[160];       // Sprite attribute memory\n    uint8_t io[128];        // I/O registers\n    uint8_t\
          \ hram[127];      // High RAM\n    uint8_t ie;             // Interrupt enable\n    \n    int rom_bank;        \
          \   // Current ROM bank number\n    int mbc_type;           // Memory Bank Controller type\n};\n\nuint8_t mmu_read(struct\
          \ MMU* mmu, uint16_t addr) {\n    switch (addr & 0xF000) {\n        case 0x0000:\n        case 0x1000:\n       \
          \ case 0x2000:\n        case 0x3000:\n            // ROM bank 0 (fixed)\n            return mmu->rom[addr];\n  \
          \      \n        case 0x4000:\n        case 0x5000:\n        case 0x6000:\n        case 0x7000:\n            //\
          \ Switchable ROM bank\n            return mmu->rom_banks[(mmu->rom_bank - 1) * 0x4000 + (addr - 0x4000)];\n    \
          \    \n        case 0x8000:\n        case 0x9000:\n            // Video RAM\n            return mmu->vram[addr -\
          \ 0x8000];\n        \n        case 0xC000:\n        case 0xD000:\n            // Work RAM\n            return mmu->wram[addr\
          \ - 0xC000];\n        \n        case 0xF000:\n            if (addr >= 0xFF00 && addr <= 0xFF7F) {\n            \
          \    // I/O registers\n                return read_io(mmu, addr);\n            }\n            if (addr >= 0xFF80\
          \ && addr <= 0xFFFE) {\n                return mmu->hram[addr - 0xFF80];\n            }\n            if (addr ==\
          \ 0xFFFF) {\n                return mmu->ie;\n            }\n            break;\n    }\n    return 0xFF;  // Unmapped\n\
          }\n\nvoid mmu_write(struct MMU* mmu, uint16_t addr, uint8_t value) {\n    // Handle MBC bank switching\n    if (addr\
          \ >= 0x2000 && addr <= 0x3FFF) {\n        // ROM bank select\n        mmu->rom_bank = value & 0x1F;\n        if\
          \ (mmu->rom_bank == 0) mmu->rom_bank = 1;\n        return;\n    }\n    // ... handle other regions\n}"
      pitfalls:
      - Bank 0 special cases
      - Echo RAM
      - Write-only registers
      concepts:
      - Memory mapping
      - Bank switching
      - MMIO
      estimated_hours: 12-20
    - id: 3
      name: Graphics
      description: Implement the display/PPU (Picture Processing Unit).
      acceptance_criteria:
      - Screen rendering
      - Sprites
      - Background tiles
      - Correct timing
      hints:
        level1: 'CHIP-8: 64x32 monochrome, XOR drawing. Game Boy: tile-based with layers.'
        level2: Render line by line (scanline rendering). Check sprite priority.
        level3: "// Game Boy PPU (simplified)\nstruct PPU {\n    uint8_t lcdc;           // LCD Control\n    uint8_t stat;\
          \           // LCD Status\n    uint8_t scy, scx;       // Scroll Y/X\n    uint8_t ly;             // Current scanline\n\
          \    uint8_t lyc;            // LY Compare\n    uint8_t bgp;            // Background palette\n    uint8_t obp0,\
          \ obp1;     // Sprite palettes\n    \n    uint8_t* vram;\n    uint8_t* oam;\n    uint32_t framebuffer[160 * 144];\n\
          \    \n    int mode;               // 0=HBlank, 1=VBlank, 2=OAM, 3=Transfer\n    int cycles;\n};\n\nvoid ppu_step(struct\
          \ PPU* ppu, int cpu_cycles) {\n    ppu->cycles += cpu_cycles;\n    \n    switch (ppu->mode) {\n        case 2: \
          \ // OAM Scan (80 cycles)\n            if (ppu->cycles >= 80) {\n                ppu->cycles -= 80;\n          \
          \      ppu->mode = 3;\n            }\n            break;\n        \n        case 3:  // Pixel Transfer (172 cycles\
          \ avg)\n            if (ppu->cycles >= 172) {\n                ppu->cycles -= 172;\n                render_scanline(ppu);\n\
          \                ppu->mode = 0;\n            }\n            break;\n        \n        case 0:  // HBlank (204 cycles)\n\
          \            if (ppu->cycles >= 204) {\n                ppu->cycles -= 204;\n                ppu->ly++;\n      \
          \          \n                if (ppu->ly == 144) {\n                    ppu->mode = 1;  // VBlank\n            \
          \        request_interrupt(INT_VBLANK);\n                } else {\n                    ppu->mode = 2;\n        \
          \        }\n            }\n            break;\n        \n        case 1:  // VBlank (10 lines)\n            if (ppu->cycles\
          \ >= 456) {\n                ppu->cycles -= 456;\n                ppu->ly++;\n                \n               \
          \ if (ppu->ly > 153) {\n                    ppu->ly = 0;\n                    ppu->mode = 2;\n                }\n\
          \            }\n            break;\n    }\n}\n\nvoid render_scanline(struct PPU* ppu) {\n    if (!(ppu->lcdc & 0x80))\
          \ return;  // LCD off\n    \n    int y = ppu->ly;\n    \n    // Render background\n    if (ppu->lcdc & 0x01) {\n\
          \        for (int x = 0; x < 160; x++) {\n            int scrolled_x = (x + ppu->scx) & 255;\n            int scrolled_y\
          \ = (y + ppu->scy) & 255;\n            \n            // Get tile from tilemap\n            int tile_x = scrolled_x\
          \ / 8;\n            int tile_y = scrolled_y / 8;\n            int tile_idx = get_bg_tile(ppu, tile_x, tile_y);\n\
          \            \n            // Get pixel from tile\n            int pixel_x = scrolled_x % 8;\n            int pixel_y\
          \ = scrolled_y % 8;\n            int color = get_tile_pixel(ppu, tile_idx, pixel_x, pixel_y);\n            \n  \
          \          ppu->framebuffer[y * 160 + x] = apply_palette(color, ppu->bgp);\n        }\n    }\n    \n    // Render\
          \ sprites on top\n    if (ppu->lcdc & 0x02) {\n        render_sprites(ppu, y);\n    }\n}"
      pitfalls:
      - Mid-frame register changes
      - Sprite limit per line
      - Priority rules
      concepts:
      - Tile graphics
      - Scanline rendering
      - Sprite systems
      estimated_hours: 18-35
    - id: 4
      name: Timing and Input
      description: Implement accurate timing and controller input.
      acceptance_criteria:
      - Cycle-accurate timing
      - Timer interrupts
      - Joypad input
      - Audio (optional)
      hints:
        level1: Each instruction takes specific cycles. Run PPU/timers in sync.
        level2: 'Game Boy: 4.19 MHz CPU, timer can fire at different rates.'
        level3: "// Main emulation loop with timing\nstruct Emulator {\n    struct CPU cpu;\n    struct MMU mmu;\n    struct\
          \ PPU ppu;\n    struct Timer timer;\n    uint8_t joypad;\n    \n    uint64_t total_cycles;\n};\n\nvoid emulator_frame(struct\
          \ Emulator* emu) {\n    // One frame = 70224 cycles (at 59.7 Hz)\n    const int CYCLES_PER_FRAME = 70224;\n    int\
          \ cycles_this_frame = 0;\n    \n    while (cycles_this_frame < CYCLES_PER_FRAME) {\n        // Handle interrupts\n\
          \        int interrupt_cycles = handle_interrupts(&emu->cpu, &emu->mmu);\n        \n        // Execute one instruction\n\
          \        int cpu_cycles = cpu_step(&emu->cpu, &emu->mmu);\n        \n        int total = interrupt_cycles + cpu_cycles;\n\
          \        \n        // Step other components\n        timer_step(&emu->timer, total);\n        ppu_step(&emu->ppu,\
          \ total);\n        \n        cycles_this_frame += total;\n        emu->total_cycles += total;\n    }\n}\n\n// Timer\
          \ implementation\nstruct Timer {\n    uint8_t div;    // Divider (increments at 16384 Hz)\n    uint8_t tima;   //\
          \ Timer counter\n    uint8_t tma;    // Timer modulo (reload value)\n    uint8_t tac;    // Timer control\n    \n\
          \    int div_counter;\n    int timer_counter;\n};\n\nvoid timer_step(struct Timer* t, int cycles) {\n    // DIV\
          \ increments every 256 cycles\n    t->div_counter += cycles;\n    while (t->div_counter >= 256) {\n        t->div_counter\
          \ -= 256;\n        t->div++;\n    }\n    \n    // TIMA if enabled\n    if (t->tac & 0x04) {\n        t->timer_counter\
          \ += cycles;\n        \n        int threshold;\n        switch (t->tac & 0x03) {\n            case 0: threshold\
          \ = 1024; break;  // 4096 Hz\n            case 1: threshold = 16; break;    // 262144 Hz\n            case 2: threshold\
          \ = 64; break;    // 65536 Hz\n            case 3: threshold = 256; break;   // 16384 Hz\n        }\n        \n\
          \        while (t->timer_counter >= threshold) {\n            t->timer_counter -= threshold;\n            t->tima++;\n\
          \            \n            if (t->tima == 0) {  // Overflow\n                t->tima = t->tma;  // Reload\n    \
          \            request_interrupt(INT_TIMER);\n            }\n        }\n    }\n}\n\n// Joypad\nuint8_t read_joypad(struct\
          \ Emulator* emu) {\n    uint8_t select = emu->mmu.io[0x00];  // P1 register\n    uint8_t result = select | 0x0F;\
          \  // Upper bits from select, lower bits high\n    \n    if (!(select & 0x10)) {  // Direction keys selected\n \
          \       if (emu->joypad & KEY_RIGHT) result &= ~0x01;\n        if (emu->joypad & KEY_LEFT)  result &= ~0x02;\n \
          \       if (emu->joypad & KEY_UP)    result &= ~0x04;\n        if (emu->joypad & KEY_DOWN)  result &= ~0x08;\n \
          \   }\n    if (!(select & 0x20)) {  // Button keys selected\n        if (emu->joypad & KEY_A)      result &= ~0x01;\n\
          \        if (emu->joypad & KEY_B)      result &= ~0x02;\n        if (emu->joypad & KEY_SELECT) result &= ~0x04;\n\
          \        if (emu->joypad & KEY_START)  result &= ~0x08;\n    }\n    \n    return result;\n}"
      pitfalls:
      - Cycle counting accuracy
      - Timer edge cases
      - VBlank timing
      concepts:
      - Cycle accuracy
      - Hardware timers
      - Input handling
      estimated_hours: 15-35
  build-game-engine:
    id: build-game-engine
    name: Build Your Own Game Engine
    description: Build a 2D/3D game engine with rendering, physics, and entity management.
    difficulty: expert
    estimated_hours: 100-200
    prerequisites:
    - Graphics programming basics
    - Linear algebra
    - C/C++ or Rust
    - Game architecture patterns
    languages:
      recommended:
      - C++
      - Rust
      - C
      also_possible:
      - Zig
    resources:
    - type: book
      name: Game Engine Architecture
      url: https://www.gameenginebook.com/
    - type: video
      name: Handmade Hero
      url: https://handmadehero.org/
    - type: tutorial
      name: Learn OpenGL
      url: https://learnopengl.com/
    milestones:
    - id: 1
      name: Window & Rendering Foundation
      description: Create window and basic rendering pipeline.
      acceptance_criteria:
      - Window creation (SDL/GLFW)
      - OpenGL/Vulkan context
      - Basic 2D sprite rendering
      - Texture loading
      - Shader system
      hints:
        level1: Use SDL2 or GLFW for cross-platform windowing. Initialize OpenGL context.
        level2: Create vertex buffer for quad, write basic vertex/fragment shaders.
        level3: "## Rendering Foundation\n\n```cpp\n// Window and OpenGL setup\n#include <SDL2/SDL.h>\n#include <glad/glad.h>\n\
          \nclass Window {\npublic:\n    SDL_Window* window;\n    SDL_GLContext glContext;\n    int width, height;\n    \n\
          \    bool init(const char* title, int w, int h) {\n        SDL_Init(SDL_INIT_VIDEO);\n        \n        SDL_GL_SetAttribute(SDL_GL_CONTEXT_MAJOR_VERSION,\
          \ 4);\n        SDL_GL_SetAttribute(SDL_GL_CONTEXT_MINOR_VERSION, 1);\n        SDL_GL_SetAttribute(SDL_GL_CONTEXT_PROFILE_MASK,\
          \ SDL_GL_CONTEXT_PROFILE_CORE);\n        \n        window = SDL_CreateWindow(title, SDL_WINDOWPOS_CENTERED, SDL_WINDOWPOS_CENTERED,\n\
          \                                  w, h, SDL_WINDOW_OPENGL);\n        glContext = SDL_GL_CreateContext(window);\n\
          \        gladLoadGLLoader((GLADloadproc)SDL_GL_GetProcAddress);\n        \n        width = w; height = h;\n    \
          \    return true;\n    }\n    \n    void swap() { SDL_GL_SwapWindow(window); }\n};\n\n// Shader\nclass Shader {\n\
          public:\n    GLuint program;\n    \n    void compile(const char* vertSrc, const char* fragSrc) {\n        GLuint\
          \ vert = glCreateShader(GL_VERTEX_SHADER);\n        glShaderSource(vert, 1, &vertSrc, NULL);\n        glCompileShader(vert);\n\
          \        \n        GLuint frag = glCreateShader(GL_FRAGMENT_SHADER);\n        glShaderSource(frag, 1, &fragSrc,\
          \ NULL);\n        glCompileShader(frag);\n        \n        program = glCreateProgram();\n        glAttachShader(program,\
          \ vert);\n        glAttachShader(program, frag);\n        glLinkProgram(program);\n        \n        glDeleteShader(vert);\n\
          \        glDeleteShader(frag);\n    }\n    \n    void use() { glUseProgram(program); }\n    void setMat4(const char*\
          \ name, const float* mat) {\n        glUniformMatrix4fv(glGetUniformLocation(program, name), 1, GL_FALSE, mat);\n\
          \    }\n};\n\n// Sprite Batch for efficient 2D rendering\nclass SpriteBatch {\n    GLuint vao, vbo, ebo;\n    std::vector<Vertex>\
          \ vertices;\n    Shader* shader;\n    \npublic:\n    void begin() { vertices.clear(); }\n    \n    void draw(Texture*\
          \ tex, float x, float y, float w, float h) {\n        // Add 4 vertices for quad\n        vertices.push_back({{x,\
          \ y}, {0, 0}});\n        vertices.push_back({{x+w, y}, {1, 0}});\n        vertices.push_back({{x+w, y+h}, {1, 1}});\n\
          \        vertices.push_back({{x, y+h}, {0, 1}});\n    }\n    \n    void end() {\n        glBindBuffer(GL_ARRAY_BUFFER,\
          \ vbo);\n        glBufferData(GL_ARRAY_BUFFER, vertices.size() * sizeof(Vertex),\n                    vertices.data(),\
          \ GL_DYNAMIC_DRAW);\n        \n        shader->use();\n        glBindVertexArray(vao);\n        glDrawElements(GL_TRIANGLES,\
          \ (vertices.size() / 4) * 6, GL_UNSIGNED_INT, 0);\n    }\n};\n```"
      pitfalls:
      - OpenGL state leaks
      - Shader compilation errors not checked
      - Wrong vertex attribute setup
      concepts:
      - Graphics APIs
      - Shader programming
      - Batch rendering
      estimated_hours: 20-30
    - id: 2
      name: Entity Component System
      description: Implement ECS architecture for game objects.
      acceptance_criteria:
      - Entity management
      - Component storage
      - System execution
      - Component queries
      - Entity creation/destruction
      hints:
        level1: Entity is just an ID. Components are data. Systems process entities with specific components.
        level2: Use archetypes or sparse sets for efficient component storage and iteration.
        level3: "## ECS Implementation\n\n```cpp\nusing Entity = uint32_t;\nusing ComponentType = uint32_t;\n\n// Component\
          \ storage using sparse set\ntemplate<typename T>\nclass ComponentArray {\n    std::vector<T> dense;\n    std::vector<Entity>\
          \ denseToEntity;\n    std::unordered_map<Entity, size_t> entityToIndex;\n    \npublic:\n    void insert(Entity entity,\
          \ T component) {\n        size_t index = dense.size();\n        dense.push_back(component);\n        denseToEntity.push_back(entity);\n\
          \        entityToIndex[entity] = index;\n    }\n    \n    void remove(Entity entity) {\n        size_t index = entityToIndex[entity];\n\
          \        size_t lastIndex = dense.size() - 1;\n        \n        // Swap with last\n        dense[index] = dense[lastIndex];\n\
          \        denseToEntity[index] = denseToEntity[lastIndex];\n        entityToIndex[denseToEntity[index]] = index;\n\
          \        \n        dense.pop_back();\n        denseToEntity.pop_back();\n        entityToIndex.erase(entity);\n\
          \    }\n    \n    T& get(Entity entity) { return dense[entityToIndex[entity]]; }\n    bool has(Entity entity) {\
          \ return entityToIndex.count(entity) > 0; }\n    \n    auto begin() { return dense.begin(); }\n    auto end() {\
          \ return dense.end(); }\n};\n\nclass World {\n    Entity nextEntity = 0;\n    std::unordered_map<ComponentType,\
          \ void*> componentArrays;\n    std::set<Entity> entities;\n    \npublic:\n    Entity createEntity() {\n        Entity\
          \ e = nextEntity++;\n        entities.insert(e);\n        return e;\n    }\n    \n    void destroyEntity(Entity\
          \ e) {\n        entities.erase(e);\n        // Remove from all component arrays...\n    }\n    \n    template<typename\
          \ T>\n    void addComponent(Entity e, T component) {\n        getComponentArray<T>()->insert(e, component);\n  \
          \  }\n    \n    template<typename T>\n    T& getComponent(Entity e) {\n        return getComponentArray<T>()->get(e);\n\
          \    }\n    \n    template<typename... Components>\n    void forEach(std::function<void(Entity, Components&...)>\
          \ func) {\n        for (Entity e : entities) {\n            if ((getComponentArray<Components>()->has(e) && ...))\
          \ {\n                func(e, getComponent<Components>(e)...);\n            }\n        }\n    }\n};\n\n// Components\n\
          struct Transform { float x, y, rotation, scaleX, scaleY; };\nstruct Velocity { float vx, vy; };\nstruct Sprite {\
          \ Texture* texture; float width, height; };\n\n// Systems\nvoid MovementSystem(World& world, float dt) {\n    world.forEach<Transform,\
          \ Velocity>([dt](Entity e, Transform& t, Velocity& v) {\n        t.x += v.vx * dt;\n        t.y += v.vy * dt;\n\
          \    });\n}\n\nvoid RenderSystem(World& world, SpriteBatch& batch) {\n    world.forEach<Transform, Sprite>([&batch](Entity\
          \ e, Transform& t, Sprite& s) {\n        batch.draw(s.texture, t.x, t.y, s.width * t.scaleX, s.height * t.scaleY);\n\
          \    });\n}\n```"
      pitfalls:
      - Component iteration invalidation
      - Memory fragmentation
      - System ordering dependencies
      concepts:
      - ECS architecture
      - Data-oriented design
      - Cache efficiency
      estimated_hours: 20-30
    - id: 3
      name: Physics & Collision
      description: Implement 2D physics and collision detection.
      acceptance_criteria:
      - Rigid body dynamics
      - Collision detection (AABB, circles)
      - Collision response
      - Spatial partitioning
      - Physics timestep
      hints:
        level1: Semi-implicit Euler for integration. AABB for broad phase, then narrow phase.
        level2: Fixed timestep with accumulator for deterministic physics.
        level3: "## 2D Physics\n\n```cpp\nstruct RigidBody {\n    float mass, invMass;\n    float vx, vy;\n    float ax, ay;\n\
          \    float restitution;  // Bounciness\n};\n\nstruct Collider {\n    enum Type { AABB, Circle } type;\n    union\
          \ {\n        struct { float halfW, halfH; } aabb;\n        struct { float radius; } circle;\n    };\n};\n\nclass\
          \ PhysicsWorld {\n    float gravity = -9.81f;\n    float fixedDeltaTime = 1.0f / 60.0f;\n    float accumulator =\
          \ 0.0f;\n    \npublic:\n    void update(World& world, float dt) {\n        accumulator += dt;\n        \n      \
          \  while (accumulator >= fixedDeltaTime) {\n            fixedUpdate(world, fixedDeltaTime);\n            accumulator\
          \ -= fixedDeltaTime;\n        }\n    }\n    \n    void fixedUpdate(World& world, float dt) {\n        // Apply forces\n\
          \        world.forEach<Transform, RigidBody>([this, dt](Entity e, Transform& t, RigidBody& rb) {\n            if\
          \ (rb.invMass > 0) {\n                rb.ay += gravity;\n            }\n            \n            // Semi-implicit\
          \ Euler\n            rb.vx += rb.ax * dt;\n            rb.vy += rb.ay * dt;\n            t.x += rb.vx * dt;\n  \
          \          t.y += rb.vy * dt;\n            \n            rb.ax = rb.ay = 0;\n        });\n        \n        // Collision\
          \ detection & response\n        detectAndResolveCollisions(world);\n    }\n    \n    void detectAndResolveCollisions(World&\
          \ world) {\n        std::vector<std::tuple<Entity, Entity>> pairs;\n        \n        // Broad phase: find potential\
          \ collisions\n        world.forEach<Transform, Collider>([&](Entity e1, Transform& t1, Collider& c1) {\n       \
          \     world.forEach<Transform, Collider>([&](Entity e2, Transform& t2, Collider& c2) {\n                if (e1 >=\
          \ e2) return;\n                if (broadPhaseCheck(t1, c1, t2, c2)) {\n                    pairs.emplace_back(e1,\
          \ e2);\n                }\n            });\n        });\n        \n        // Narrow phase: resolve collisions\n\
          \        for (auto [e1, e2] : pairs) {\n            auto& t1 = world.getComponent<Transform>(e1);\n            auto&\
          \ t2 = world.getComponent<Transform>(e2);\n            auto& c1 = world.getComponent<Collider>(e1);\n          \
          \  auto& c2 = world.getComponent<Collider>(e2);\n            \n            CollisionInfo info;\n            if (narrowPhaseCheck(t1,\
          \ c1, t2, c2, info)) {\n                resolveCollision(world, e1, e2, info);\n            }\n        }\n    }\n\
          \    \n    void resolveCollision(World& world, Entity e1, Entity e2, CollisionInfo& info) {\n        auto& rb1 =\
          \ world.getComponent<RigidBody>(e1);\n        auto& rb2 = world.getComponent<RigidBody>(e2);\n        auto& t1 =\
          \ world.getComponent<Transform>(e1);\n        auto& t2 = world.getComponent<Transform>(e2);\n        \n        //\
          \ Separate objects\n        float totalInvMass = rb1.invMass + rb2.invMass;\n        t1.x -= info.normal.x * info.depth\
          \ * (rb1.invMass / totalInvMass);\n        t1.y -= info.normal.y * info.depth * (rb1.invMass / totalInvMass);\n\
          \        t2.x += info.normal.x * info.depth * (rb2.invMass / totalInvMass);\n        t2.y += info.normal.y * info.depth\
          \ * (rb2.invMass / totalInvMass);\n        \n        // Impulse-based response\n        float relVelN = (rb2.vx\
          \ - rb1.vx) * info.normal.x + (rb2.vy - rb1.vy) * info.normal.y;\n        if (relVelN > 0) return;  // Already separating\n\
          \        \n        float e = std::min(rb1.restitution, rb2.restitution);\n        float j = -(1 + e) * relVelN /\
          \ totalInvMass;\n        \n        rb1.vx -= j * rb1.invMass * info.normal.x;\n        rb1.vy -= j * rb1.invMass\
          \ * info.normal.y;\n        rb2.vx += j * rb2.invMass * info.normal.x;\n        rb2.vy += j * rb2.invMass * info.normal.y;\n\
          \    }\n};\n```"
      pitfalls:
      - Variable timestep physics
      - Tunneling (fast objects passing through)
      - Collision jitter
      concepts:
      - Physics simulation
      - Collision detection
      - Impulse resolution
      estimated_hours: 25-40
    - id: 4
      name: Resource & Scene Management
      description: Implement asset loading and scene system.
      acceptance_criteria:
      - Asset loading (textures, audio, data)
      - Resource caching
      - Scene serialization
      - Scene transitions
      - Game loop
      hints:
        level1: Load resources once, cache by path. Scenes contain entities and their components.
        level2: Use JSON/binary for scene serialization. Implement proper cleanup on scene switch.
        level3: "## Resource & Scene Management\n\n```cpp\nclass ResourceManager {\n    std::unordered_map<std::string, std::shared_ptr<Texture>>\
          \ textures;\n    std::unordered_map<std::string, std::shared_ptr<Sound>> sounds;\n    \npublic:\n    std::shared_ptr<Texture>\
          \ loadTexture(const std::string& path) {\n        if (textures.count(path)) return textures[path];\n        \n \
          \       auto tex = std::make_shared<Texture>();\n        tex->loadFromFile(path);\n        textures[path] = tex;\n\
          \        return tex;\n    }\n    \n    void unloadUnused() {\n        for (auto it = textures.begin(); it != textures.end();)\
          \ {\n            if (it->second.use_count() == 1) {\n                it = textures.erase(it);\n            } else\
          \ {\n                ++it;\n            }\n        }\n    }\n};\n\nclass Scene {\npublic:\n    std::string name;\n\
          \    World world;\n    \n    virtual void onEnter() = 0;\n    virtual void onExit() = 0;\n    virtual void update(float\
          \ dt) = 0;\n    virtual void render() = 0;\n    \n    void saveToFile(const std::string& path) {\n        json j;\n\
          \        j[\"name\"] = name;\n        j[\"entities\"] = json::array();\n        \n        world.forEach<Transform>([&](Entity\
          \ e, Transform& t) {\n            json entity;\n            entity[\"id\"] = e;\n            entity[\"transform\"\
          ] = {t.x, t.y, t.rotation, t.scaleX, t.scaleY};\n            // Serialize other components...\n            j[\"\
          entities\"].push_back(entity);\n        });\n        \n        std::ofstream file(path);\n        file << j.dump(2);\n\
          \    }\n    \n    void loadFromFile(const std::string& path) {\n        std::ifstream file(path);\n        json\
          \ j = json::parse(file);\n        \n        name = j[\"name\"];\n        for (auto& entity : j[\"entities\"]) {\n\
          \            Entity e = world.createEntity();\n            auto& t = entity[\"transform\"];\n            world.addComponent(e,\
          \ Transform{t[0], t[1], t[2], t[3], t[4]});\n            // Deserialize other components...\n        }\n    }\n\
          };\n\nclass Engine {\n    Window window;\n    ResourceManager resources;\n    std::unique_ptr<Scene> currentScene;\n\
          \    std::unique_ptr<Scene> nextScene;\n    bool running = true;\n    \npublic:\n    void run() {\n        float\
          \ lastTime = SDL_GetTicks() / 1000.0f;\n        \n        while (running) {\n            float currentTime = SDL_GetTicks()\
          \ / 1000.0f;\n            float dt = currentTime - lastTime;\n            lastTime = currentTime;\n            \n\
          \            processInput();\n            \n            if (nextScene) {\n                if (currentScene) currentScene->onExit();\n\
          \                currentScene = std::move(nextScene);\n                currentScene->onEnter();\n            }\n\
          \            \n            if (currentScene) {\n                currentScene->update(dt);\n                currentScene->render();\n\
          \            }\n            \n            window.swap();\n        }\n    }\n    \n    void changeScene(std::unique_ptr<Scene>\
          \ scene) {\n        nextScene = std::move(scene);\n    }\n};\n```"
      pitfalls:
      - Resource leaks
      - Scene state corruption during transition
      - Serialization version incompatibility
      concepts:
      - Resource management
      - Scene graphs
      - Game architecture
      estimated_hours: 20-30
  build-gc:
    id: build-gc
    name: Build Your Own Garbage Collector
    description: Implement a garbage collector with mark-sweep and generational collection.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - Memory management
    - C/Rust
    - Data structures
    - Graph algorithms
    languages:
      recommended:
      - C
      - Rust
      - Zig
      also_possible: []
    resources:
    - type: book
      name: The Garbage Collection Handbook
      url: https://gchandbook.org/
    - type: article
      name: Baby's First Garbage Collector
      url: https://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/
    - type: article
      name: Writing a Simple GC in C
      url: https://maplant.com/gc.html
    milestones:
    - id: 1
      name: Mark-Sweep Collector
      description: Implement basic mark-sweep garbage collection.
      acceptance_criteria:
      - Object allocation
      - Root set identification
      - Mark phase (DFS)
      - Sweep phase
      - Memory reclamation
      hints:
        level1: Mark all reachable objects from roots, then sweep unmarked ones.
        level2: Roots are global variables and stack. Mark with DFS to find all reachable.
        level3: "## Mark-Sweep GC\n\n```c\n#include <stdlib.h>\n#include <stdbool.h>\n\ntypedef struct Object {\n    bool\
          \ marked;\n    struct Object* next;  // Intrusive list of all objects\n    // Object type info for tracing\n   \
          \ enum { OBJ_INT, OBJ_PAIR } type;\n    union {\n        int value;\n        struct { struct Object* head; struct\
          \ Object* tail; } pair;\n    };\n} Object;\n\ntypedef struct {\n    Object* firstObject;  // All allocated objects\n\
          \    Object** stack;       // VM stack (roots)\n    int stackSize;\n    int numObjects;\n    int maxObjects;\n}\
          \ VM;\n\nVM* newVM() {\n    VM* vm = malloc(sizeof(VM));\n    vm->firstObject = NULL;\n    vm->stack = malloc(sizeof(Object*)\
          \ * 256);\n    vm->stackSize = 0;\n    vm->numObjects = 0;\n    vm->maxObjects = 8;  // Initial threshold\n    return\
          \ vm;\n}\n\nObject* allocate(VM* vm, int type) {\n    if (vm->numObjects >= vm->maxObjects) {\n        gc(vm);\n\
          \    }\n    \n    Object* obj = malloc(sizeof(Object));\n    obj->marked = false;\n    obj->type = type;\n    obj->next\
          \ = vm->firstObject;\n    vm->firstObject = obj;\n    vm->numObjects++;\n    return obj;\n}\n\nvoid mark(Object*\
          \ obj) {\n    if (obj == NULL || obj->marked) return;\n    \n    obj->marked = true;\n    \n    if (obj->type ==\
          \ OBJ_PAIR) {\n        mark(obj->pair.head);\n        mark(obj->pair.tail);\n    }\n}\n\nvoid markAll(VM* vm) {\n\
          \    for (int i = 0; i < vm->stackSize; i++) {\n        mark(vm->stack[i]);\n    }\n}\n\nvoid sweep(VM* vm) {\n\
          \    Object** obj = &vm->firstObject;\n    while (*obj) {\n        if (!(*obj)->marked) {\n            Object* unreached\
          \ = *obj;\n            *obj = unreached->next;\n            free(unreached);\n            vm->numObjects--;\n  \
          \      } else {\n            (*obj)->marked = false;  // Reset for next GC\n            obj = &(*obj)->next;\n \
          \       }\n    }\n}\n\nvoid gc(VM* vm) {\n    int numObjects = vm->numObjects;\n    \n    markAll(vm);\n    sweep(vm);\n\
          \    \n    vm->maxObjects = vm->numObjects * 2;  // Grow threshold\n    \n    printf(\"Collected %d objects, %d\
          \ remaining.\\n\",\n           numObjects - vm->numObjects, vm->numObjects);\n}\n```"
      pitfalls:
      - Missing roots
      - Marking already marked objects (infinite loop)
      - Not resetting marks
      concepts:
      - Reachability
      - Graph traversal
      - Memory reclamation
      estimated_hours: 10-15
    - id: 2
      name: Tri-color Marking
      description: Implement tri-color invariant for incremental GC.
      acceptance_criteria:
      - White/gray/black coloring
      - Worklist-based marking
      - Incremental marking
      - Write barriers
      hints:
        level1: White=unmarked, gray=marked but children not scanned, black=fully scanned.
        level2: Process gray objects incrementally. Write barrier adds to gray set.
        level3: "## Tri-color Marking\n\n```c\ntypedef enum { WHITE, GRAY, BLACK } Color;\n\ntypedef struct Object {\n   \
          \ Color color;\n    // ... other fields ...\n} Object;\n\ntypedef struct {\n    Object** grayList;\n    int grayCount;\n\
          \    int grayCapacity;\n} GC;\n\nvoid makeGray(GC* gc, Object* obj) {\n    if (obj == NULL || obj->color != WHITE)\
          \ return;\n    \n    obj->color = GRAY;\n    if (gc->grayCount >= gc->grayCapacity) {\n        gc->grayCapacity\
          \ *= 2;\n        gc->grayList = realloc(gc->grayList, gc->grayCapacity * sizeof(Object*));\n    }\n    gc->grayList[gc->grayCount++]\
          \ = obj;\n}\n\nvoid markRoots(VM* vm, GC* gc) {\n    for (int i = 0; i < vm->stackSize; i++) {\n        makeGray(gc,\
          \ vm->stack[i]);\n    }\n}\n\n// Process some gray objects (incremental)\nbool markSome(GC* gc, int workAmount)\
          \ {\n    int work = 0;\n    while (gc->grayCount > 0 && work < workAmount) {\n        Object* obj = gc->grayList[--gc->grayCount];\n\
          \        \n        // Scan children\n        if (obj->type == OBJ_PAIR) {\n            makeGray(gc, obj->pair.head);\n\
          \            makeGray(gc, obj->pair.tail);\n        }\n        \n        obj->color = BLACK;\n        work++;\n\
          \    }\n    return gc->grayCount == 0;  // Done?\n}\n\n// Write barrier: when mutator changes a pointer in a black\
          \ object\nvoid writeBarrier(GC* gc, Object* parent, Object* child) {\n    if (parent->color == BLACK && child->color\
          \ == WHITE) {\n        // Maintain tri-color invariant\n        makeGray(gc, child);  // or makeGray(gc, parent)\
          \ for snapshot-at-the-beginning\n    }\n}\n\n// Incremental GC cycle\nvoid incrementalGC(VM* vm, GC* gc) {\n   \
          \ static enum { IDLE, MARKING, SWEEPING } phase = IDLE;\n    static Object** sweepPtr = NULL;\n    \n    switch\
          \ (phase) {\n        case IDLE:\n            // All objects start white\n            markRoots(vm, gc);\n      \
          \      phase = MARKING;\n            break;\n            \n        case MARKING:\n            if (markSome(gc, 10))\
          \ {  // Process 10 objects per step\n                sweepPtr = &vm->firstObject;\n                phase = SWEEPING;\n\
          \            }\n            break;\n            \n        case SWEEPING:\n            // Sweep a few objects\n \
          \           for (int i = 0; i < 10 && *sweepPtr; i++) {\n                if ((*sweepPtr)->color == WHITE) {\n  \
          \                  Object* unreached = *sweepPtr;\n                    *sweepPtr = unreached->next;\n          \
          \          free(unreached);\n                } else {\n                    (*sweepPtr)->color = WHITE;  // Reset\n\
          \                    sweepPtr = &(*sweepPtr)->next;\n                }\n            }\n            if (*sweepPtr\
          \ == NULL) {\n                phase = IDLE;\n            }\n            break;\n    }\n}\n```"
      pitfalls:
      - Write barrier missing
      - Violating tri-color invariant
      - Race conditions
      concepts:
      - Incremental GC
      - Write barriers
      - Tri-color abstraction
      estimated_hours: 12-18
    - id: 3
      name: Generational Collection
      description: Implement generational GC with nursery and old generation.
      acceptance_criteria:
      - Young generation (nursery)
      - Old generation
      - Promotion policy
      - Remembered set
      - Minor vs major collection
      hints:
        level1: Most objects die young. Collect nursery frequently, old gen rarely.
        level2: Remembered set tracks old->young pointers so we don't scan all old objects.
        level3: "## Generational GC\n\n```c\ntypedef struct {\n    // Young generation (bump allocation)\n    char* nurseryStart;\n\
          \    char* nurseryEnd;\n    char* nurseryAlloc;\n    \n    // Old generation\n    Object* oldGenList;\n    \n  \
          \  // Remembered set: old objects pointing to young\n    Object** rememberedSet;\n    int rememberedCount;\n   \
          \ int rememberedCapacity;\n    \n    int youngCollections;\n    int promotionAge;  // Survive this many collections\
          \ to promote\n} GenerationalGC;\n\nObject* allocYoung(GenerationalGC* gc, size_t size) {\n    size = ALIGN(size);\n\
          \    \n    if (gc->nurseryAlloc + size > gc->nurseryEnd) {\n        minorCollection(gc);\n        if (gc->nurseryAlloc\
          \ + size > gc->nurseryEnd) {\n            // Still no space, trigger major collection\n            majorCollection(gc);\n\
          \        }\n    }\n    \n    Object* obj = (Object*)gc->nurseryAlloc;\n    gc->nurseryAlloc += size;\n    obj->age\
          \ = 0;\n    obj->isOld = false;\n    return obj;\n}\n\nvoid addToRememberedSet(GenerationalGC* gc, Object* oldObj)\
          \ {\n    if (gc->rememberedCount >= gc->rememberedCapacity) {\n        gc->rememberedCapacity *= 2;\n        gc->rememberedSet\
          \ = realloc(gc->rememberedSet,\n                                    gc->rememberedCapacity * sizeof(Object*));\n\
          \    }\n    gc->rememberedSet[gc->rememberedCount++] = oldObj;\n}\n\n// Write barrier for generational GC\nvoid\
          \ generationalWriteBarrier(GenerationalGC* gc, Object* parent, Object* child) {\n    if (parent->isOld && !child->isOld)\
          \ {\n        addToRememberedSet(gc, parent);\n    }\n}\n\nvoid minorCollection(GenerationalGC* gc) {\n    // Roots:\
          \ stack + remembered set\n    // Copy live young objects to either:\n    // 1. Survivor space (if young)\n    //\
          \ 2. Old generation (if survived enough)\n    \n    char* toSpace = allocateToSpace();\n    char* toPtr = toSpace;\n\
          \    \n    // Process roots\n    for (int i = 0; i < vm->stackSize; i++) {\n        if (!vm->stack[i]->isOld) {\n\
          \            vm->stack[i] = copyObject(vm->stack[i], &toPtr, gc);\n        }\n    }\n    \n    // Process remembered\
          \ set\n    for (int i = 0; i < gc->rememberedCount; i++) {\n        Object* old = gc->rememberedSet[i];\n      \
          \  // Scan old object's children\n        scanChildren(old, &toPtr, gc);\n    }\n    \n    // Process copied objects\
          \ (Cheney's algorithm)\n    char* scanPtr = toSpace;\n    while (scanPtr < toPtr) {\n        Object* obj = (Object*)scanPtr;\n\
          \        scanChildren(obj, &toPtr, gc);\n        scanPtr += objectSize(obj);\n    }\n    \n    // Swap spaces\n\
          \    gc->nurseryAlloc = gc->nurseryStart;\n    gc->rememberedCount = 0;\n    gc->youngCollections++;\n}\n\nObject*\
          \ copyObject(Object* obj, char** toPtr, GenerationalGC* gc) {\n    if (obj->forwarding) return obj->forwarding;\
          \  // Already copied\n    \n    obj->age++;\n    \n    if (obj->age >= gc->promotionAge) {\n        // Promote to\
          \ old generation\n        Object* copy = allocOld(gc, objectSize(obj));\n        memcpy(copy, obj, objectSize(obj));\n\
          \        copy->isOld = true;\n        obj->forwarding = copy;\n        return copy;\n    } else {\n        // Copy\
          \ to to-space\n        Object* copy = (Object*)*toPtr;\n        memcpy(copy, obj, objectSize(obj));\n        *toPtr\
          \ += objectSize(obj);\n        obj->forwarding = copy;\n        return copy;\n    }\n}\n```"
      pitfalls:
      - Missing remembered set entries
      - Promoting too early/late
      - Write barrier overhead
      concepts:
      - Generational hypothesis
      - Copying collection
      - Inter-generational pointers
      estimated_hours: 15-20
    - id: 4
      name: Concurrent Collection
      description: Add concurrent marking for reduced pause times.
      acceptance_criteria:
      - Concurrent marking thread
      - Handshakes/safepoints
      - SATB or incremental update
      - Concurrent sweep
      hints:
        level1: GC thread marks concurrently while mutator runs. Need synchronization.
        level2: Safepoints where mutator checks for GC requests. SATB logs overwritten pointers.
        level3: "## Concurrent GC\n\n```c\n#include <pthread.h>\n#include <stdatomic.h>\n\ntypedef struct {\n    atomic_bool\
          \ gcRequested;\n    atomic_bool gcInProgress;\n    \n    // SATB (Snapshot-At-The-Beginning) buffer\n    Object**\
          \ satbBuffer;\n    atomic_int satbCount;\n    \n    pthread_t gcThread;\n    pthread_mutex_t mutex;\n    pthread_cond_t\
          \ cond;\n} ConcurrentGC;\n\n// Mutator safepoint - called periodically\nvoid safepoint(ConcurrentGC* gc) {\n   \
          \ if (atomic_load(&gc->gcRequested)) {\n        // Wait for GC to complete initial marking\n        pthread_mutex_lock(&gc->mutex);\n\
          \        while (atomic_load(&gc->gcInProgress)) {\n            pthread_cond_wait(&gc->cond, &gc->mutex);\n     \
          \   }\n        pthread_mutex_unlock(&gc->mutex);\n    }\n}\n\n// SATB write barrier - log overwritten pointer\n\
          void satbWriteBarrier(ConcurrentGC* gc, Object** slot) {\n    if (atomic_load(&gc->gcInProgress)) {\n        Object*\
          \ old = *slot;\n        if (old != NULL) {\n            int idx = atomic_fetch_add(&gc->satbCount, 1);\n       \
          \     gc->satbBuffer[idx] = old;  // Log old value\n        }\n    }\n}\n\n// GC thread\nvoid* gcThreadFunc(void*\
          \ arg) {\n    ConcurrentGC* gc = (ConcurrentGC*)arg;\n    VM* vm = gc->vm;\n    \n    while (1) {\n        // Wait\
          \ for GC request\n        pthread_mutex_lock(&gc->mutex);\n        while (!atomic_load(&gc->gcRequested)) {\n  \
          \          pthread_cond_wait(&gc->cond, &gc->mutex);\n        }\n        pthread_mutex_unlock(&gc->mutex);\n   \
          \     \n        atomic_store(&gc->gcInProgress, true);\n        \n        // Phase 1: Initial mark (stop-the-world,\
          \ mark roots)\n        stopTheWorld(vm);\n        markRoots(vm, gc);\n        resumeTheWorld(vm);\n        \n  \
          \      // Phase 2: Concurrent mark\n        while (gc->grayCount > 0) {\n            Object* obj = gc->grayList[--gc->grayCount];\n\
          \            scanAndMarkChildren(gc, obj);\n        }\n        \n        // Phase 3: Remark (stop-the-world, process\
          \ SATB buffer)\n        stopTheWorld(vm);\n        processSATBBuffer(gc);\n        while (gc->grayCount > 0) {\n\
          \            Object* obj = gc->grayList[--gc->grayCount];\n            scanAndMarkChildren(gc, obj);\n        }\n\
          \        resumeTheWorld(vm);\n        \n        // Phase 4: Concurrent sweep\n        concurrentSweep(gc);\n   \
          \     \n        atomic_store(&gc->gcInProgress, false);\n        atomic_store(&gc->gcRequested, false);\n      \
          \  \n        pthread_mutex_lock(&gc->mutex);\n        pthread_cond_broadcast(&gc->cond);\n        pthread_mutex_unlock(&gc->mutex);\n\
          \    }\n}\n\nvoid processSATBBuffer(ConcurrentGC* gc) {\n    int count = atomic_exchange(&gc->satbCount, 0);\n \
          \   for (int i = 0; i < count; i++) {\n        makeGray(gc, gc->satbBuffer[i]);\n    }\n}\n```"
      pitfalls:
      - Data races
      - Missing SATB entries
      - Long pauses during STW phases
      concepts:
      - Concurrent algorithms
      - SATB/incremental update
      - Safepoints
      estimated_hours: 15-20
  build-git:
    id: build-git
    name: Build Your Own Git
    description: Build a version control system that implements core Git operations. Understand content-addressable storage
      and the Git object model.
    difficulty: expert
    estimated_hours: 30-50
    prerequisites:
    - File I/O and hashing
    - Tree data structures
    - Basic compression (zlib)
    - Graph algorithms
    languages:
      recommended:
      - Python
      - Rust
      - Go
      - C
      also_possible: []
    resources:
    - name: Write yourself a Git!
      url: https://wyag.thb.lt/
      type: tutorial
    - name: CodeCrafters Git Challenge
      url: https://app.codecrafters.io/courses/git/overview
      type: interactive
    - name: Git Internals - Git Objects
      url: https://git-scm.com/book/en/v2/Git-Internals-Git-Objects
      type: documentation
    milestones:
    - id: 1
      name: Repository Initialization
      description: Implement git init - create .git directory structure.
      acceptance_criteria:
      - Creates .git directory
      - Creates .git/objects directory
      - Creates .git/refs/heads directory
      - Creates .git/HEAD file pointing to refs/heads/master
      hints:
        level1: 'mkdir -p for nested directories. HEAD contains ''ref: refs/heads/master'''
        level2: .git/objects stores all content. .git/refs stores branch pointers.
        level3: ".git structure:\n.git/\n  HEAD           # ref: refs/heads/master\n  objects/       # blob, tree, commit\
          \ objects\n  refs/\n    heads/       # branch refs\n    tags/        # tag refs"
      concepts:
      - Git repository structure
      - References (HEAD, branches)
      estimated_hours: 1-2
      pitfalls:
      - Wrong permissions on .git directory (should be 0755)
      - Missing directories cause later commands to fail silently
      - 'HEAD file must have exact format ''ref: refs/heads/master'' with newline'
      - Creating .git inside existing repo causes confusion
    - id: 2
      name: Object Storage (Blobs)
      description: Implement hash-object and cat-file for blob objects.
      acceptance_criteria:
      - hash-object computes SHA-1 of content
      - Stores compressed object in .git/objects/xx/yyyy
      - cat-file retrieves and decompresses
      - 'Object format: ''blob {size}\0{content}'''
      hints:
        level1: SHA-1 hash of 'blob {size}\0{content}'. Store zlib-compressed.
        level2: 'Object path: first 2 chars of hash = directory, rest = filename.'
        level3: "def hash_object(data, type='blob'):\n  header = f'{type} {len(data)}\\0'.encode()\n  store = header + data\n\
          \  sha = hashlib.sha1(store).hexdigest()\n  path = f'.git/objects/{sha[:2]}/{sha[2:]}'\n  compressed = zlib.compress(store)\n\
          \  write_file(path, compressed)\n  return sha"
      pitfalls:
      - Forgetting null byte between header and content
      - Not compressing before storage
      - Binary vs text content
      concepts:
      - Content-addressable storage
      - SHA-1 hashing
      - Zlib compression
      estimated_hours: 2-3
    - id: 3
      name: Tree Objects
      description: Implement tree objects that represent directory structure.
      acceptance_criteria:
      - Tree object stores list of (mode, name, hash) entries
      - ls-tree displays tree contents
      - write-tree creates tree from working directory
      - Nested trees for subdirectories
      hints:
        level1: 'Tree entries: mode (100644 for file, 40000 for dir), name, 20-byte hash.'
        level2: 'Entries sorted by name. Each entry is binary: mode + space + name + null + hash.'
        level3: "# Tree object format and implementation\ndef write_tree(directory):\n    entries = []\n    for name in sorted(os.listdir(directory)):\n\
          \        path = os.path.join(directory, name)\n        if name == '.git':\n            continue\n        if os.path.isfile(path):\n\
          \            mode = b'100644'\n            sha = hash_object(open(path, 'rb').read(), 'blob')\n        else:\n \
          \           mode = b'40000'\n            sha = write_tree(path)  # Recurse for subdirectories\n        entries.append((mode,\
          \ name.encode(), bytes.fromhex(sha)))\n\n    # Build tree content: mode + space + name + null + 20-byte SHA\n  \
          \  data = b''\n    for mode, name, sha_bytes in entries:\n        data += mode + b' ' + name + b'\\x00' + sha_bytes\n\
          \n    return hash_object(data, 'tree')\n\ndef ls_tree(tree_sha):\n    content = read_object(tree_sha)\n    entries\
          \ = []\n    while content:\n        # Find space after mode\n        space_idx = content.index(b' ')\n        mode\
          \ = content[:space_idx].decode()\n        content = content[space_idx + 1:]\n        # Find null after name\n  \
          \      null_idx = content.index(b'\\x00')\n        name = content[:null_idx].decode()\n        content = content[null_idx\
          \ + 1:]\n        # Next 20 bytes are SHA\n        sha = content[:20].hex()\n        content = content[20:]\n   \
          \     entries.append((mode, sha, name))\n    return entries"
      pitfalls:
      - Hash stored as binary (20 bytes), not hex (40 chars)
      - Sorting rules
      - Mode formatting
      concepts:
      - Tree data structure
      - Directory representation
      - Binary formats
      estimated_hours: 3-4
    - id: 4
      name: Commit Objects
      description: Implement commit objects with tree, parent, author, message.
      acceptance_criteria:
      - commit-tree creates commit object
      - Commit has tree hash, parent(s), author, committer, message
      - Timestamp in Unix format with timezone
      - Chained commits form history
      hints:
        level1: Commit points to tree. First commit has no parent.
        level2: 'Format: tree, parent (optional), author, committer, blank line, message.'
        level3: 'Commit format:

          tree {tree-sha}

          parent {parent-sha}  # optional

          author {name} <{email}> {timestamp} {tz}

          committer {name} <{email}> {timestamp} {tz}


          {commit message}'
      pitfalls:
      - Timestamp format
      - Handling merge commits
      - Message can be multi-line
      concepts:
      - Commit graph
      - Directed acyclic graph (DAG)
      - Immutable history
      estimated_hours: 2-3
    - id: 5
      name: References and Branches
      description: Implement branches as references to commits.
      acceptance_criteria:
      - branch creates new branch (ref file)
      - branch -d deletes branch
      - Branches stored in .git/refs/heads/
      - HEAD points to current branch
      - Detached HEAD when pointing directly to commit
      hints:
        level1: 'Branch = file containing commit hash. HEAD = file containing ''ref: refs/heads/xxx''.'
        level2: update-ref command safely updates ref files.
        level3: '# Create branch

          echo {commit-sha} > .git/refs/heads/{branch-name}

          # Switch branch (update HEAD)

          echo "ref: refs/heads/{branch}" > .git/HEAD'
      pitfalls:
      - Symbolic refs vs direct refs
      - Deleting branch you're on
      - Ref file locking
      concepts:
      - Git references
      - Symbolic references
      - Branch management
      estimated_hours: 2-3
    - id: 6
      name: Index (Staging Area)
      description: Implement the staging area for preparing commits.
      acceptance_criteria:
      - add stages files to index
      - status shows staged/unstaged changes
      - Index is binary file .git/index
      - Stores (mode, sha, flags, path) per entry
      hints:
        level1: Index caches file info for quick status checks.
        level2: Binary format with header, entries, and optional extensions.
        level3: 'Index format (simplified):

          Header: DIRC, version (2-4), entry count

          Entries: ctime, mtime, dev, ino, mode, uid, gid, size, sha, flags, path'
      pitfalls:
      - Index is binary, not text
      - Stat info for detecting changes
      - Path encoding and sorting
      concepts:
      - Staging area concept
      - Binary file formats
      - File metadata caching
      estimated_hours: 4-6
    - id: 7
      name: Diff Algorithm
      description: Implement diff to show changes between versions.
      acceptance_criteria:
      - Shows line-by-line differences
      - Unified diff format
      - diff between working tree and index
      - diff between commits
      hints:
        level1: Use Myers diff algorithm or Longest Common Subsequence.
        level2: 'Unified diff: @@ -start,count +start,count @@ then - and + lines.'
        level3: 'Myers algorithm finds shortest edit script.

          Key insight: d-path on k-diagonal.

          O((N+M)D) where D is edit distance.'
      pitfalls:
      - Binary files
      - Line ending differences
      - Large files / long diffs
      concepts:
      - Diff algorithms (Myers, LCS)
      - Edit distance
      - Unified diff format
      estimated_hours: 4-6
    - id: 8
      name: Merge (Three-way)
      description: Implement three-way merge with conflict detection.
      acceptance_criteria:
      - Finds common ancestor (merge base)
      - Applies non-conflicting changes automatically
      - Marks conflicts with <<<< ==== >>>> markers
      - Creates merge commit with two parents
      hints:
        level1: Merge base = lowest common ancestor in commit graph.
        level2: 'Three-way: if only one side changed, take that. If both changed same, conflict.'
        level3: 'For each file:

          base = file in merge-base

          ours = file in current branch

          theirs = file in other branch

          if ours == theirs: keep ours

          elif ours == base: take theirs

          elif theirs == base: take ours

          else: conflict'
      pitfalls:
      - Finding merge base in complex history
      - Handling renames during merge
      - Nested conflicts
      concepts:
      - Three-way merge
      - Merge base calculation
      - Conflict resolution
      estimated_hours: 6-10
  build-interpreter:
    id: build-interpreter
    name: Build Your Own Interpreter (Lox)
    description: Build a complete interpreter for the Lox programming language. Based on the excellent 'Crafting Interpreters'
      book by Bob Nystrom.
    difficulty: expert
    estimated_hours: 40-80
    prerequisites:
    - Basic programming language concepts
    - Recursion
    - Tree data structures
    - Object-oriented programming
    languages:
      recommended:
      - Java
      - C
      - Rust
      also_possible:
      - Python
      - Go
      - TypeScript
    resources:
    - name: Crafting Interpreters (Free Online Book)
      url: https://craftinginterpreters.com
      type: book
    - name: Writing An Interpreter In Go
      url: https://interpreterbook.com
      type: book
    - name: CodeCrafters Interpreter Challenge
      url: https://app.codecrafters.io/courses/interpreter/overview
      type: interactive
    milestones:
    - id: 1
      name: Scanner (Lexer)
      description: Build a scanner that converts Lox source code into tokens. Chapter 4 of Crafting Interpreters.
      acceptance_criteria:
      - Recognizes all Lox tokens
      - Handles string and number literals
      - Tracks line numbers
      - Ignores whitespace and comments
      - Reports lexical errors
      hints:
        level1: Consume characters one by one. Match against known patterns.
        level2: 'Token types: ( ) { } , . - + ; / * ! != = == > >= < <=

          Keywords: and class else false for fun if nil or print return super this true var while'
        level3: "class Scanner:\n    def __init__(self, source):\n        self.source = source\n        self.tokens = []\n\
          \        self.start = 0\n        self.current = 0\n        self.line = 1\n\n    def scan_tokens(self):\n       \
          \ while not self.is_at_end():\n            self.start = self.current\n            self.scan_token()\n        self.tokens.append(Token(TokenType.EOF,\
          \ \"\", None, self.line))\n        return self.tokens\n\n    def scan_token(self):\n        c = self.advance()\n\
          \        match c:\n            case '(': self.add_token(TokenType.LEFT_PAREN)\n            case ')': self.add_token(TokenType.RIGHT_PAREN)\n\
          \            case '-': self.add_token(TokenType.MINUS)\n            case '+': self.add_token(TokenType.PLUS)\n \
          \           case '!':\n                self.add_token(TokenType.BANG_EQUAL if self.match('=') else TokenType.BANG)\n\
          \            case '=':\n                self.add_token(TokenType.EQUAL_EQUAL if self.match('=') else TokenType.EQUAL)\n\
          \            case '<':\n                self.add_token(TokenType.LESS_EQUAL if self.match('=') else TokenType.LESS)\n\
          \            case '\"': self.string()\n            case c if c.isdigit(): self.number()\n            case c if c.isalpha()\
          \ or c == '_': self.identifier()\n            case ' ' | '\\r' | '\\t': pass\n            case '\\n': self.line\
          \ += 1\n            case _: self.error(f\"Unexpected character: {c}\")\n\n    def match(self, expected):\n     \
          \   if self.is_at_end() or self.source[self.current] != expected:\n            return False\n        self.current\
          \ += 1\n        return True"
      pitfalls:
      - Confusing = and ==
      - Not handling unterminated strings
      - Newlines inside strings
      concepts:
      - Lexical analysis
      - Token representation
      - Error handling
      estimated_hours: 2-3
    - id: 2
      name: Representing Code (AST)
      description: Define the abstract syntax tree classes for Lox. Chapter 5.
      acceptance_criteria:
      - Expression classes for binary, unary, grouping, literal
      - Pretty-printer using visitor pattern
      - Statement classes for print, expression, var
      hints:
        level1: Use the Visitor pattern for operations on AST nodes.
        level2: Generate classes from grammar rules. Expr -> Binary, Unary, Literal, Grouping...
        level3: 'Expr types:

          - Binary: left op right (1 + 2)

          - Unary: op right (-1, !true)

          - Literal: value (42, "hello")

          - Grouping: expression ((1 + 2))'
      pitfalls:
      - Visitor pattern boilerplate
      - Mutable vs immutable AST nodes
      - Parent/child references creating cycles
      concepts:
      - Abstract Syntax Trees
      - Visitor pattern
      - Expression vs Statement
      estimated_hours: 2-3
    - id: 3
      name: Parsing Expressions
      description: Build a recursive descent parser for expressions. Chapter 6.
      acceptance_criteria:
      - Parses arithmetic with correct precedence
      - Handles parentheses for grouping
      - Parses comparison and equality
      - Reports syntax errors with context
      hints:
        level1: One function per precedence level. Lower precedence = higher in call stack.
        level2: 'Precedence (low to high):

          equality (== !=)

          comparison (< > <= >=)

          term (+ -)

          factor (* /)

          unary (! -)

          primary (literals, grouping)'
        level3: "# Recursive descent parser with precedence handling\nclass Parser:\n    def __init__(self, tokens):\n   \
          \     self.tokens = tokens\n        self.current = 0\n\n    def expression(self):\n        return self.equality()\n\
          \n    def equality(self):  # == !=\n        expr = self.comparison()\n        while self.match(TokenType.BANG_EQUAL,\
          \ TokenType.EQUAL_EQUAL):\n            operator = self.previous()\n            right = self.comparison()\n     \
          \       expr = Binary(expr, operator, right)\n        return expr\n\n    def comparison(self):  # > >= < <=\n  \
          \      expr = self.term()\n        while self.match(TokenType.GREATER, TokenType.GREATER_EQUAL,\n              \
          \           TokenType.LESS, TokenType.LESS_EQUAL):\n            operator = self.previous()\n            right =\
          \ self.term()\n            expr = Binary(expr, operator, right)\n        return expr\n\n    def term(self):  # +\
          \ -\n        expr = self.factor()\n        while self.match(TokenType.MINUS, TokenType.PLUS):\n            operator\
          \ = self.previous()\n            right = self.factor()\n            expr = Binary(expr, operator, right)\n     \
          \   return expr\n\n    def factor(self):  # * /\n        expr = self.unary()\n        while self.match(TokenType.SLASH,\
          \ TokenType.STAR):\n            operator = self.previous()\n            right = self.unary()\n            expr =\
          \ Binary(expr, operator, right)\n        return expr\n\n    def unary(self):  # ! -\n        if self.match(TokenType.BANG,\
          \ TokenType.MINUS):\n            operator = self.previous()\n            right = self.unary()\n            return\
          \ Unary(operator, right)\n        return self.primary()\n\n    def primary(self):\n        if self.match(TokenType.FALSE):\
          \ return Literal(False)\n        if self.match(TokenType.TRUE): return Literal(True)\n        if self.match(TokenType.NIL):\
          \ return Literal(None)\n        if self.match(TokenType.NUMBER, TokenType.STRING):\n            return Literal(self.previous().literal)\n\
          \        if self.match(TokenType.LEFT_PAREN):\n            expr = self.expression()\n            self.consume(TokenType.RIGHT_PAREN,\
          \ \"Expect ')' after expression.\")\n            return Grouping(expr)\n        raise self.error(self.peek(), \"\
          Expect expression.\")"
      pitfalls:
      - Left recursion causes infinite loop
      - Forgetting closing paren
      - Error recovery
      concepts:
      - Recursive descent parsing
      - Operator precedence
      - Error recovery
      estimated_hours: 3-4
    - id: 4
      name: Evaluating Expressions
      description: Build a tree-walking interpreter to evaluate expressions. Chapter 7.
      acceptance_criteria:
      - Evaluates arithmetic operations
      - Handles unary minus and negation
      - Implements truthiness
      - String concatenation with +
      - Runtime error for type mismatches
      hints:
        level1: Implement Visitor that returns evaluated value. Recursively evaluate children.
        level2: Lox is dynamically typed. Operations check types at runtime.
        level3: 'Truthiness: false and nil are falsy, everything else truthy.

          Equality: nil == nil, numbers by value, strings by content.'
      pitfalls:
      - Division by zero
      - String + non-string
      - Java null vs Lox nil
      concepts:
      - Tree-walking interpretation
      - Dynamic typing
      - Runtime type checking
      estimated_hours: 2-3
    - id: 5
      name: Statements and State
      description: Add statements, variables, and assignment. Chapter 8.
      acceptance_criteria:
      - Print statement outputs to stdout
      - Variable declarations with var
      - Assignment expressions
      - Expression statements
      - Environment stores variable bindings
      hints:
        level1: Environment is a map from variable name to value.
        level2: var a = 1; creates binding. a = 2; updates existing binding.
        level3: "# Environment for variable storage\nclass Environment:\n    def __init__(self, enclosing=None):\n       \
          \ self.values = {}\n        self.enclosing = enclosing  # For nested scopes\n\n    def define(self, name, value):\n\
          \        self.values[name] = value\n\n    def get(self, name):\n        if name.lexeme in self.values:\n       \
          \     return self.values[name.lexeme]\n        if self.enclosing:\n            return self.enclosing.get(name)\n\
          \        raise RuntimeError(f\"Undefined variable '{name.lexeme}'.\")\n\n    def assign(self, name, value):\n  \
          \      if name.lexeme in self.values:\n            self.values[name.lexeme] = value\n            return\n      \
          \  if self.enclosing:\n            self.enclosing.assign(name, value)\n            return\n        raise RuntimeError(f\"\
          Undefined variable '{name.lexeme}'.\")\n\n# Parsing statements\ndef statement(self):\n    if self.match(TokenType.PRINT):\n\
          \        return self.print_statement()\n    if self.match(TokenType.VAR):\n        return self.var_declaration()\n\
          \    return self.expression_statement()\n\ndef var_declaration(self):\n    name = self.consume(TokenType.IDENTIFIER,\
          \ \"Expect variable name.\")\n    initializer = None\n    if self.match(TokenType.EQUAL):\n        initializer =\
          \ self.expression()\n    self.consume(TokenType.SEMICOLON, \"Expect ';' after variable declaration.\")\n    return\
          \ Var(name, initializer)"
      pitfalls:
      - Using undeclared variable
      - Assignment to non-variable
      - Scoping issues
      concepts:
      - Statement vs Expression
      - Environment and bindings
      - Assignment as expression
      estimated_hours: 2-3
    - id: 6
      name: Control Flow
      description: Add if, while, for, and logical operators. Chapter 9.
      acceptance_criteria:
      - if/else with proper scoping
      - while loops
      - for loops (desugared to while)
      - Logical and/or with short-circuit
      hints:
        level1: for (init; cond; incr) body -> { init; while (cond) { body; incr; } }
        level2: 'Short-circuit: a and b returns a if falsy, else b. a or b returns a if truthy, else b.'
        level3: "# Control flow implementation\nclass Interpreter:\n    def visit_if_stmt(self, stmt):\n        if self.is_truthy(self.evaluate(stmt.condition)):\n\
          \            self.execute(stmt.then_branch)\n        elif stmt.else_branch:\n            self.execute(stmt.else_branch)\n\
          \n    def visit_while_stmt(self, stmt):\n        while self.is_truthy(self.evaluate(stmt.condition)):\n        \
          \    self.execute(stmt.body)\n\n    def visit_logical_expr(self, expr):\n        left = self.evaluate(expr.left)\n\
          \        # Short-circuit evaluation\n        if expr.operator.type == TokenType.OR:\n            if self.is_truthy(left):\n\
          \                return left\n        else:  # AND\n            if not self.is_truthy(left):\n                return\
          \ left\n        return self.evaluate(expr.right)\n\n# For loop desugaring in parser:\ndef for_statement(self):\n\
          \    self.consume(TokenType.LEFT_PAREN, \"Expect '(' after 'for'.\")\n\n    initializer = None if self.match(TokenType.SEMICOLON)\
          \ else \\\n                  self.var_declaration() if self.match(TokenType.VAR) else \\\n                  self.expression_statement()\n\
          \n    condition = Literal(True) if self.check(TokenType.SEMICOLON) else self.expression()\n    self.consume(TokenType.SEMICOLON,\
          \ \"Expect ';' after loop condition.\")\n\n    increment = None if self.check(TokenType.RIGHT_PAREN) else self.expression()\n\
          \    self.consume(TokenType.RIGHT_PAREN, \"Expect ')' after for clauses.\")\n\n    body = self.statement()\n\n \
          \   # Desugar to while loop\n    if increment:\n        body = Block([body, Expression(increment)])\n    body =\
          \ While(condition, body)\n    if initializer:\n        body = Block([initializer, body])\n    return body"
      pitfalls:
      - Dangling else
      - Forgetting short-circuit
      - Infinite loops without timeout
      concepts:
      - Conditional execution
      - Loop desugaring
      - Short-circuit evaluation
      estimated_hours: 2-3
    - id: 7
      name: Functions
      description: Add function declarations and calls. Chapter 10.
      acceptance_criteria:
      - fun keyword declares functions
      - Parameters and return values
      - Return statement exits early
      - Functions are first-class values
      - Call stack for recursion
      hints:
        level1: 'Function value stores: name, parameters, body AST, and defining environment.'
        level2: 'Call: create new environment for local vars, execute body, return result.'
        level3: "# Function declaration and calling\nclass LoxFunction:\n    def __init__(self, declaration, closure):\n \
          \       self.declaration = declaration\n        self.closure = closure  # Environment where function was defined\n\
          \n    def call(self, interpreter, arguments):\n        # Create new environment for function scope\n        environment\
          \ = Environment(self.closure)\n\n        # Bind parameters to arguments\n        for i, param in enumerate(self.declaration.params):\n\
          \            environment.define(param.lexeme, arguments[i])\n\n        try:\n            interpreter.execute_block(self.declaration.body,\
          \ environment)\n        except Return as return_value:\n            return return_value.value\n        return None\n\
          \n    def arity(self):\n        return len(self.declaration.params)\n\nclass Return(Exception):\n    def __init__(self,\
          \ value):\n        super().__init__()\n        self.value = value\n\n# In interpreter:\ndef visit_return_stmt(self,\
          \ stmt):\n    value = None\n    if stmt.value:\n        value = self.evaluate(stmt.value)\n    raise Return(value)\n\
          \ndef visit_call_expr(self, expr):\n    callee = self.evaluate(expr.callee)\n    arguments = [self.evaluate(arg)\
          \ for arg in expr.arguments]\n\n    if not hasattr(callee, 'call'):\n        raise RuntimeError(\"Can only call\
          \ functions and classes.\")\n    if len(arguments) != callee.arity():\n        raise RuntimeError(f\"Expected {callee.arity()}\
          \ arguments but got {len(arguments)}.\")\n\n    return callee.call(self, arguments)"
      pitfalls:
      - Stack overflow from infinite recursion
      - Not restoring environment after call
      - Return outside function
      concepts:
      - First-class functions
      - Call frames
      - Return values
      estimated_hours: 3-4
    - id: 8
      name: Closures
      description: Implement lexical scoping and closures. Chapter 11.
      acceptance_criteria:
      - Functions capture enclosing environment
      - Nested functions work correctly
      - Closures persist after outer function returns
      - Resolving variables at compile time
      hints:
        level1: Closure = function + captured environment. Environment chains to enclosing scopes.
        level2: 'Resolver pass: determine which declaration each variable refers to.'
        level3: "Example:\nfun makeCounter() {\n  var i = 0;\n  fun count() {\n    i = i + 1;\n    return i;\n  }\n  return\
          \ count;\n}\nvar counter = makeCounter();\ncounter(); // 1\ncounter(); // 2"
      pitfalls:
      - Capturing variable vs capturing value
      - Variable resolution with shadowing
      - This in closures
      concepts:
      - Lexical scoping
      - Closures
      - Environment chains
      estimated_hours: 4-6
    - id: 9
      name: Classes
      description: Add class declarations, instances, and methods. Chapter 12.
      acceptance_criteria:
      - class keyword declares classes
      - Instances created with ClassName()
      - Properties accessed with dot notation
      - Methods with implicit 'this'
      - 'Initializer method: init()'
      hints:
        level1: Class is a map of method names to functions. Instance has class + fields map.
        level2: this bound when method is accessed. Method call binds this to instance.
        level3: "# Class and instance implementation\nclass LoxClass:\n    def __init__(self, name, superclass, methods):\n\
          \        self.name = name\n        self.superclass = superclass\n        self.methods = methods\n\n    def call(self,\
          \ interpreter, arguments):\n        instance = LoxInstance(self)\n        # Call initializer if present\n      \
          \  initializer = self.find_method(\"init\")\n        if initializer:\n            initializer.bind(instance).call(interpreter,\
          \ arguments)\n        return instance\n\n    def arity(self):\n        initializer = self.find_method(\"init\")\n\
          \        return 0 if not initializer else initializer.arity()\n\n    def find_method(self, name):\n        if name\
          \ in self.methods:\n            return self.methods[name]\n        if self.superclass:\n            return self.superclass.find_method(name)\n\
          \        return None\n\nclass LoxInstance:\n    def __init__(self, klass):\n        self.klass = klass\n       \
          \ self.fields = {}\n\n    def get(self, name):\n        if name.lexeme in self.fields:\n            return self.fields[name.lexeme]\n\
          \        method = self.klass.find_method(name.lexeme)\n        if method:\n            return method.bind(self)\
          \  # Bind 'this'\n        raise RuntimeError(f\"Undefined property '{name.lexeme}'.\")\n\n    def set(self, name,\
          \ value):\n        self.fields[name.lexeme] = value\n\n# Bind 'this' to method\ndef bind(self, instance):\n    environment\
          \ = Environment(self.closure)\n    environment.define(\"this\", instance)\n    return LoxFunction(self.declaration,\
          \ environment)"
      pitfalls:
      - this outside method
      - Returning from init()
      - Method vs function
      concepts:
      - Classes and instances
      - This binding
      - Constructors
      estimated_hours: 4-5
    - id: 10
      name: Inheritance
      description: Add class inheritance and super calls. Chapter 13.
      acceptance_criteria:
      - class Derived < Base syntax
      - Methods inherited from superclass
      - super.method() calls parent
      - Proper method resolution order
      hints:
        level1: Subclass stores reference to superclass. Method lookup chains upward.
        level2: super is looked up in enclosing class, not dynamically.
        level3: "class Doughnut {\n  cook() { print \"Fry\"; }\n}\nclass BostonCream < Doughnut {\n  cook() {\n    super.cook();\n\
          \    print \"Pipe cream\";\n  }\n}"
      pitfalls:
      - super outside class
      - Inheriting from non-class
      - Diamond inheritance
      concepts:
      - Single inheritance
      - Super calls
      - Method resolution
      estimated_hours: 3-4
  build-kafka:
    id: build-kafka
    name: Build Your Own Kafka
    description: Build a distributed message queue with partitions and consumer groups. Learn pub/sub, ordering, and scalability.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - Replicated log
    - Distributed systems
    - Binary protocols
    languages:
      recommended:
      - Go
      - Java
      - Rust
      also_possible:
      - Scala
      - C++
    resources:
    - type: book
      name: 'Kafka: The Definitive Guide'
      url: https://www.confluent.io/resources/kafka-the-definitive-guide/
    - type: paper
      name: 'Kafka: a Distributed Messaging System'
      url: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf
    milestones:
    - id: 1
      name: Topic and Partitions
      description: Implement topic management with multiple partitions.
      acceptance_criteria:
      - Topic creation with partition count
      - Partition as append-only log
      - Message key hashing for partition assignment
      - Offset tracking
      hints:
        level1: Topic = name + list of partitions. Each partition is an independent log.
        level2: Hash message key to determine partition. Null key = round-robin.
        level3: "import hashlib\nfrom dataclasses import dataclass\nfrom typing import Optional, List\n\n@dataclass\nclass\
          \ Message:\n    key: Optional[bytes]\n    value: bytes\n    timestamp: int\n    offset: int = -1  # Set by partition\n\
          \nclass Partition:\n    def __init__(self, topic: str, partition_id: int, log_dir: str):\n        self.topic = topic\n\
          \        self.id = partition_id\n        self.log_path = f'{log_dir}/{topic}-{partition_id}'\n        self.messages\
          \ = []  # In production, this is file-backed\n        self.next_offset = 0\n        self._load_log()\n    \n   \
          \ def append(self, msg: Message) -> int:\n        msg.offset = self.next_offset\n        self.next_offset += 1\n\
          \        self.messages.append(msg)\n        self._persist(msg)\n        return msg.offset\n    \n    def read(self,\
          \ start_offset: int, max_messages: int) -> List[Message]:\n        if start_offset >= self.next_offset:\n      \
          \      return []\n        end = min(start_offset + max_messages, self.next_offset)\n        return self.messages[start_offset:end]\n\
          \nclass Topic:\n    def __init__(self, name: str, num_partitions: int, log_dir: str):\n        self.name = name\n\
          \        self.partitions = [\n            Partition(name, i, log_dir)\n            for i in range(num_partitions)\n\
          \        ]\n    \n    def get_partition(self, key: Optional[bytes]) -> Partition:\n        if key is None:\n   \
          \         # Round-robin for null keys\n            return self.partitions[self._rr_counter() % len(self.partitions)]\n\
          \        \n        # Hash-based assignment\n        hash_val = int(hashlib.md5(key).hexdigest(), 16)\n        return\
          \ self.partitions[hash_val % len(self.partitions)]\n    \n    def produce(self, key: Optional[bytes], value: bytes)\
          \ -> tuple:\n        partition = self.get_partition(key)\n        msg = Message(key=key, value=value, timestamp=int(time.time()\
          \ * 1000))\n        offset = partition.append(msg)\n        return partition.id, offset"
      pitfalls:
      - Partition assignment consistency
      - Offset gaps
      - Key null handling
      concepts:
      - Partitioning
      - Ordered logs
      - Horizontal scaling
      estimated_hours: 10-15
    - id: 2
      name: Producer
      description: Implement producer with batching and acknowledgments.
      acceptance_criteria:
      - Batch messages for efficiency
      - Configurable acks (0, 1, all)
      - Retry logic
      - Idempotent producer (optional)
      hints:
        level1: Batch messages by partition. Send when batch full or timeout.
        level2: 'acks=0: fire and forget. acks=1: leader ack. acks=all: all replicas.'
        level3: "from collections import defaultdict\nimport asyncio\n\nclass Producer:\n    def __init__(self, bootstrap_servers,\
          \ acks='1', batch_size=16384, linger_ms=5):\n        self.brokers = bootstrap_servers\n        self.acks = acks\n\
          \        self.batch_size = batch_size\n        self.linger_ms = linger_ms\n        \n        self.batches = defaultdict(list)\
          \  # (topic, partition) -> [messages]\n        self.batch_sizes = defaultdict(int)\n        self.pending = {}  #\
          \ Future results\n        \n        self._sender_task = asyncio.create_task(self._sender_loop())\n    \n    async\
          \ def send(self, topic: str, key: bytes, value: bytes) -> asyncio.Future:\n        partition = self._partition_for(topic,\
          \ key)\n        \n        future = asyncio.Future()\n        msg = ProducerRecord(topic, partition, key, value,\
          \ future)\n        \n        batch_key = (topic, partition)\n        self.batches[batch_key].append(msg)\n     \
          \   self.batch_sizes[batch_key] += len(value)\n        \n        # Check if batch is full\n        if self.batch_sizes[batch_key]\
          \ >= self.batch_size:\n            await self._send_batch(batch_key)\n        \n        return future\n    \n  \
          \  async def _sender_loop(self):\n        while True:\n            await asyncio.sleep(self.linger_ms / 1000)\n\
          \            \n            for batch_key in list(self.batches.keys()):\n                if self.batches[batch_key]:\n\
          \                    await self._send_batch(batch_key)\n    \n    async def _send_batch(self, batch_key):\n    \
          \    topic, partition = batch_key\n        messages = self.batches.pop(batch_key, [])\n        self.batch_sizes.pop(batch_key,\
          \ 0)\n        \n        if not messages:\n            return\n        \n        try:\n            broker = await\
          \ self._get_leader(topic, partition)\n            response = await broker.produce(\n                topic=topic,\n\
          \                partition=partition,\n                messages=[(m.key, m.value) for m in messages],\n        \
          \        acks=self.acks\n            )\n            \n            # Resolve futures\n            for i, msg in enumerate(messages):\n\
          \                msg.future.set_result(RecordMetadata(\n                    topic=topic,\n                    partition=partition,\n\
          \                    offset=response.base_offset + i\n                ))\n        except Exception as e:\n     \
          \       for msg in messages:\n                msg.future.set_exception(e)"
      pitfalls:
      - Batch timeout handling
      - Leader failover during send
      - Duplicate messages
      concepts:
      - Batching
      - Acknowledgments
      - At-least-once delivery
      estimated_hours: 12-18
    - id: 3
      name: Consumer Groups
      description: Implement consumer groups with partition assignment and rebalancing.
      acceptance_criteria:
      - Group membership protocol
      - Partition assignment strategies
      - Offset commits
      - Rebalancing on join/leave
      hints:
        level1: Each partition assigned to exactly one consumer in group. Rebalance on membership change.
        level2: Coordinator manages group. Consumers send heartbeats. Leader assigns partitions.
        level3: "class ConsumerGroup:\n    def __init__(self, group_id: str, coordinator):\n        self.group_id = group_id\n\
          \        self.coordinator = coordinator\n        self.members = {}  # member_id -> ConsumerMember\n        self.generation\
          \ = 0\n        self.leader = None\n        self.assignment = {}  # member_id -> [partitions]\n    \n    def join(self,\
          \ member_id: str, subscriptions: List[str]) -> JoinGroupResponse:\n        self.members[member_id] = ConsumerMember(member_id,\
          \ subscriptions)\n        \n        # Trigger rebalance\n        self.generation += 1\n        \n        # Elect\
          \ leader (first member)\n        if self.leader is None or self.leader not in self.members:\n            self.leader\
          \ = member_id\n        \n        return JoinGroupResponse(\n            generation=self.generation,\n          \
          \  leader=self.leader,\n            member_id=member_id,\n            members=list(self.members.keys()) if member_id\
          \ == self.leader else []\n        )\n    \n    def sync(self, member_id: str, generation: int, assignment: dict)\
          \ -> SyncGroupResponse:\n        if generation != self.generation:\n            raise RebalanceInProgress()\n  \
          \      \n        if member_id == self.leader:\n            self.assignment = assignment\n        \n        return\
          \ SyncGroupResponse(\n            assignment=self.assignment.get(member_id, [])\n        )\n\nclass Consumer:\n\
          \    def __init__(self, group_id: str, bootstrap_servers):\n        self.group_id = group_id\n        self.member_id\
          \ = None\n        self.generation = None\n        self.assignment = []\n        self.offsets = {}  # (topic, partition)\
          \ -> offset\n    \n    async def subscribe(self, topics: List[str]):\n        self.subscriptions = topics\n    \
          \    await self._join_group()\n    \n    async def poll(self, timeout_ms: int) -> List[ConsumerRecord]:\n      \
          \  records = []\n        for topic, partition in self.assignment:\n            offset = self.offsets.get((topic,\
          \ partition), 0)\n            batch = await self._fetch(topic, partition, offset)\n            records.extend(batch)\n\
          \            if batch:\n                self.offsets[(topic, partition)] = batch[-1].offset + 1\n        return\
          \ records\n    \n    async def commit(self):\n        await self.coordinator.commit_offsets(\n            self.group_id,\n\
          \            self.member_id,\n            self.generation,\n            self.offsets\n        )\n\n# Partition assignment\
          \ (Range strategy)\ndef range_assignment(members: List[str], partitions: List[tuple]) -> dict:\n    assignment =\
          \ {m: [] for m in members}\n    \n    # Group partitions by topic\n    by_topic = defaultdict(list)\n    for topic,\
          \ partition in partitions:\n        by_topic[topic].append(partition)\n    \n    for topic, parts in by_topic.items():\n\
          \        parts.sort()\n        per_consumer = len(parts) // len(members)\n        extra = len(parts) % len(members)\n\
          \        \n        idx = 0\n        for i, member in enumerate(sorted(members)):\n            count = per_consumer\
          \ + (1 if i < extra else 0)\n            for p in parts[idx:idx+count]:\n                assignment[member].append((topic,\
          \ p))\n            idx += count\n    \n    return assignment"
      pitfalls:
      - Rebalance storms
      - Stuck rebalances
      - Duplicate processing during rebalance
      concepts:
      - Consumer groups
      - Partition assignment
      - Rebalancing
      estimated_hours: 18-30
    - id: 4
      name: Replication
      description: Implement partition replication for fault tolerance.
      acceptance_criteria:
      - Leader/follower replication
      - ISR (In-Sync Replicas)
      - Leader election on failure
      - High watermark
      hints:
        level1: Each partition has leader and followers. Leader handles reads/writes. Followers fetch from leader.
        level2: ISR = replicas within lag threshold. Only commit when all ISR have message.
        level3: "class ReplicatedPartition:\n    def __init__(self, topic, partition_id, replicas, leader_id):\n        self.topic\
          \ = topic\n        self.id = partition_id\n        self.replicas = replicas  # [broker_id, ...]\n        self.leader\
          \ = leader_id\n        self.isr = set(replicas)  # In-sync replicas\n        \n        self.log = []  # Local log\n\
          \        self.high_watermark = -1  # Last committed offset\n        self.leo = -1  # Log end offset\n        \n\
          \        # Leader state\n        self.replica_leo = {}  # replica_id -> their LEO\n    \n    def append_as_leader(self,\
          \ messages: List) -> int:\n        base_offset = self.leo + 1\n        \n        for i, msg in enumerate(messages):\n\
          \            msg.offset = base_offset + i\n            self.log.append(msg)\n        \n        self.leo = self.log[-1].offset\n\
          \        self._update_high_watermark()\n        return base_offset\n    \n    def fetch_as_follower(self, fetch_offset:\
          \ int, max_bytes: int) -> FetchResponse:\n        messages = []\n        size = 0\n        \n        for msg in\
          \ self.log[fetch_offset:]:\n            if size + len(msg.value) > max_bytes:\n                break\n         \
          \   messages.append(msg)\n            size += len(msg.value)\n        \n        return FetchResponse(\n        \
          \    messages=messages,\n            high_watermark=self.high_watermark\n        )\n    \n    def update_follower_state(self,\
          \ follower_id: int, follower_leo: int):\n        self.replica_leo[follower_id] = follower_leo\n        \n      \
          \  # Check if follower is in sync\n        if follower_leo >= self.leo - self.max_lag:\n            self.isr.add(follower_id)\n\
          \        else:\n            self.isr.discard(follower_id)\n        \n        self._update_high_watermark()\n   \
          \ \n    def _update_high_watermark(self):\n        # HW = min LEO of all ISR\n        if not self.isr:\n       \
          \     return\n        \n        isr_leos = [self.replica_leo.get(r, self.leo) for r in self.isr]\n        new_hw\
          \ = min(isr_leos)\n        \n        if new_hw > self.high_watermark:\n            self.high_watermark = new_hw\n\
          \nclass ReplicaFetcher:\n    \"\"\"Follower thread that fetches from leader\"\"\"\n    \n    def __init__(self,\
          \ partition, leader_broker):\n        self.partition = partition\n        self.leader = leader_broker\n    \n  \
          \  async def run(self):\n        while True:\n            try:\n                response = await self.leader.fetch(\n\
          \                    self.partition.topic,\n                    self.partition.id,\n                    fetch_offset=self.partition.leo\
          \ + 1\n                )\n                \n                for msg in response.messages:\n                    self.partition.log.append(msg)\n\
          \                    self.partition.leo = msg.offset\n                \n                self.partition.high_watermark\
          \ = min(\n                    self.partition.high_watermark,\n                    response.high_watermark\n    \
          \            )\n            except LeaderNotAvailable:\n                await asyncio.sleep(1)\n               \
          \ await self._find_new_leader()"
      pitfalls:
      - ISR shrinking to empty
      - Unclean leader election
      - Data loss on failover
      concepts:
      - Replication
      - ISR
      - High watermark
      - Exactly-once semantics
      estimated_hours: 20-37
  build-lsp:
    id: build-lsp
    name: Build Your Own LSP Server
    description: Build a Language Server Protocol server for IDE features.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - JSON-RPC
    - AST/parsing
    - IDE concepts
    - Concurrency
    languages:
      recommended:
      - TypeScript
      - Rust
      - Go
      also_possible:
      - Python
      - C#
    resources:
    - type: specification
      name: LSP Specification
      url: https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/
    - type: article
      name: LSP Tutorial
      url: https://code.visualstudio.com/api/language-extensions/language-server-extension-guide
    milestones:
    - id: 1
      name: JSON-RPC & Initialization
      description: Implement JSON-RPC transport and LSP initialization.
      acceptance_criteria:
      - JSON-RPC message handling
      - Initialize/initialized handshake
      - Capability negotiation
      - Shutdown/exit
      hints:
        level1: LSP uses JSON-RPC 2.0 over stdio. Messages have Content-Length header.
        level2: Server advertises capabilities in initialize response.
        level3: "## LSP Transport\n\n```typescript\nimport * as readline from 'readline';\n\ninterface Message {\n  jsonrpc:\
          \ '2.0';\n  id?: number | string;\n  method?: string;\n  params?: any;\n  result?: any;\n  error?: { code: number;\
          \ message: string };\n}\n\nclass LSPServer {\n  private pendingData = '';\n  private contentLength = -1;\n  \n \
          \ constructor() {\n    process.stdin.on('data', (data) => this.handleData(data.toString()));\n  }\n  \n  private\
          \ handleData(data: string) {\n    this.pendingData += data;\n    \n    while (true) {\n      if (this.contentLength\
          \ === -1) {\n        const headerEnd = this.pendingData.indexOf('\\r\\n\\r\\n');\n        if (headerEnd === -1)\
          \ return;\n        \n        const header = this.pendingData.slice(0, headerEnd);\n        const match = header.match(/Content-Length:\
          \ (\\d+)/);\n        if (!match) throw new Error('Invalid header');\n        \n        this.contentLength = parseInt(match[1]);\n\
          \        this.pendingData = this.pendingData.slice(headerEnd + 4);\n      }\n      \n      if (this.pendingData.length\
          \ < this.contentLength) return;\n      \n      const content = this.pendingData.slice(0, this.contentLength);\n\
          \      this.pendingData = this.pendingData.slice(this.contentLength);\n      this.contentLength = -1;\n      \n\
          \      this.handleMessage(JSON.parse(content));\n    }\n  }\n  \n  private handleMessage(msg: Message) {\n    if\
          \ (msg.method) {\n      this.handleRequest(msg);\n    } else if (msg.id !== undefined) {\n      // Response to our\
          \ request (not common for servers)\n    }\n  }\n  \n  private handleRequest(msg: Message) {\n    switch (msg.method)\
          \ {\n      case 'initialize':\n        this.sendResponse(msg.id, {\n          capabilities: {\n            textDocumentSync:\
          \ 1,  // Full sync\n            completionProvider: { triggerCharacters: ['.'] },\n            hoverProvider: true,\n\
          \            definitionProvider: true,\n          }\n        });\n        break;\n        \n      case 'initialized':\n\
          \        // Client is ready\n        break;\n        \n      case 'shutdown':\n        this.sendResponse(msg.id,\
          \ null);\n        break;\n        \n      case 'exit':\n        process.exit(0);\n    }\n  }\n  \n  private sendResponse(id:\
          \ number | string, result: any) {\n    this.send({ jsonrpc: '2.0', id, result });\n  }\n  \n  private send(msg:\
          \ Message) {\n    const content = JSON.stringify(msg);\n    const header = `Content-Length: ${Buffer.byteLength(content)}\\\
          r\\n\\r\\n`;\n    process.stdout.write(header + content);\n  }\n}\n```"
      pitfalls:
      - Content-Length calculation
      - Partial message handling
      - Encoding issues
      concepts:
      - JSON-RPC
      - Protocol negotiation
      - Streaming
      estimated_hours: 8-12
    - id: 2
      name: Document Synchronization
      description: Track document changes from the editor.
      acceptance_criteria:
      - textDocument/didOpen
      - textDocument/didChange
      - textDocument/didClose
      - Incremental sync
      hints:
        level1: Track open documents in a map. Update on didChange.
        level2: 'Incremental sync: apply text edits to stored content.'
        level3: "## Document Sync\n\n```typescript\ninterface TextDocument {\n  uri: string;\n  languageId: string;\n  version:\
          \ number;\n  content: string;\n}\n\nclass DocumentManager {\n  private documents = new Map<string, TextDocument>();\n\
          \  \n  open(uri: string, languageId: string, version: number, content: string) {\n    this.documents.set(uri, {\
          \ uri, languageId, version, content });\n  }\n  \n  close(uri: string) {\n    this.documents.delete(uri);\n  }\n\
          \  \n  get(uri: string): TextDocument | undefined {\n    return this.documents.get(uri);\n  }\n  \n  update(uri:\
          \ string, version: number, changes: TextDocumentContentChangeEvent[]) {\n    const doc = this.documents.get(uri);\n\
          \    if (!doc) return;\n    \n    for (const change of changes) {\n      if ('range' in change) {\n        // Incremental\
          \ change\n        const start = this.offsetAt(doc.content, change.range.start);\n        const end = this.offsetAt(doc.content,\
          \ change.range.end);\n        doc.content = doc.content.slice(0, start) + change.text + doc.content.slice(end);\n\
          \      } else {\n        // Full content\n        doc.content = change.text;\n      }\n    }\n    doc.version =\
          \ version;\n  }\n  \n  private offsetAt(content: string, position: Position): number {\n    const lines = content.split('\\\
          n');\n    let offset = 0;\n    for (let i = 0; i < position.line; i++) {\n      offset += lines[i].length + 1; \
          \ // +1 for newline\n    }\n    return offset + position.character;\n  }\n  \n  positionAt(content: string, offset:\
          \ number): Position {\n    const lines = content.slice(0, offset).split('\\n');\n    return {\n      line: lines.length\
          \ - 1,\n      character: lines[lines.length - 1].length\n    };\n  }\n}\n\n// In LSPServer\ncase 'textDocument/didOpen':\n\
          \  this.documents.open(\n    msg.params.textDocument.uri,\n    msg.params.textDocument.languageId,\n    msg.params.textDocument.version,\n\
          \    msg.params.textDocument.text\n  );\n  this.validate(msg.params.textDocument.uri);\n  break;\n\ncase 'textDocument/didChange':\n\
          \  this.documents.update(\n    msg.params.textDocument.uri,\n    msg.params.textDocument.version,\n    msg.params.contentChanges\n\
          \  );\n  this.validate(msg.params.textDocument.uri);\n  break;\n```"
      pitfalls:
      - Version mismatch
      - Offset calculation errors
      - Unicode handling
      concepts:
      - Document tracking
      - Incremental updates
      - Positions
      estimated_hours: 8-12
    - id: 3
      name: Language Features
      description: Implement completion, hover, and go-to-definition.
      acceptance_criteria:
      - textDocument/completion
      - textDocument/hover
      - textDocument/definition
      - Symbol resolution
      hints:
        level1: Parse document to AST. Find symbol at cursor position.
        level2: Build symbol table for quick lookups. Handle imports/includes.
        level3: "## Language Features\n\n```typescript\ninterface Symbol {\n  name: string;\n  kind: SymbolKind;\n  location:\
          \ Location;\n  type?: string;\n  documentation?: string;\n}\n\nclass LanguageService {\n  private symbolTable =\
          \ new Map<string, Symbol[]>();  // uri -> symbols\n  \n  analyze(uri: string, content: string) {\n    const ast\
          \ = this.parse(content);\n    const symbols: Symbol[] = [];\n    \n    // Walk AST to collect symbols\n    this.walk(ast,\
          \ (node) => {\n      if (node.type === 'FunctionDeclaration') {\n        symbols.push({\n          name: node.name,\n\
          \          kind: SymbolKind.Function,\n          location: { uri, range: node.range },\n          type: this.formatFunctionType(node),\n\
          \          documentation: node.docComment\n        });\n      } else if (node.type === 'VariableDeclaration') {\n\
          \        symbols.push({\n          name: node.name,\n          kind: SymbolKind.Variable,\n          location: {\
          \ uri, range: node.range },\n          type: node.declaredType\n        });\n      }\n    });\n    \n    this.symbolTable.set(uri,\
          \ symbols);\n  }\n  \n  getCompletions(uri: string, position: Position): CompletionItem[] {\n    const doc = this.documents.get(uri);\n\
          \    const symbols = this.symbolTable.get(uri) || [];\n    const context = this.getContextAtPosition(doc.content,\
          \ position);\n    \n    // Filter symbols based on context\n    return symbols\n      .filter(s => this.isVisible(s,\
          \ position))\n      .map(s => ({\n        label: s.name,\n        kind: s.kind,\n        detail: s.type,\n     \
          \   documentation: s.documentation\n      }));\n  }\n  \n  getHover(uri: string, position: Position): Hover | null\
          \ {\n    const symbol = this.findSymbolAtPosition(uri, position);\n    if (!symbol) return null;\n    \n    return\
          \ {\n      contents: {\n        kind: 'markdown',\n        value: `**${symbol.name}**: ${symbol.type || 'unknown'}\\\
          n\\n${symbol.documentation || ''}`\n      }\n    };\n  }\n  \n  getDefinition(uri: string, position: Position):\
          \ Location | null {\n    const word = this.getWordAtPosition(uri, position);\n    \n    // Search all documents\
          \ for definition\n    for (const [docUri, symbols] of this.symbolTable) {\n      const symbol = symbols.find(s =>\
          \ s.name === word);\n      if (symbol) {\n        return symbol.location;\n      }\n    }\n    \n    return null;\n\
          \  }\n  \n  private findSymbolAtPosition(uri: string, position: Position): Symbol | null {\n    const word = this.getWordAtPosition(uri,\
          \ position);\n    const symbols = this.symbolTable.get(uri) || [];\n    return symbols.find(s => s.name === word)\
          \ || null;\n  }\n}\n```"
      pitfalls:
      - Stale symbol table
      - Scope visibility
      - Performance on large files
      concepts:
      - Symbol resolution
      - AST analysis
      - IDE features
      estimated_hours: 15-20
    - id: 4
      name: Diagnostics & Code Actions
      description: Report errors and suggest fixes.
      acceptance_criteria:
      - textDocument/publishDiagnostics
      - textDocument/codeAction
      - Diagnostic severity
      - Quick fixes
      hints:
        level1: Push diagnostics to client on document change.
        level2: Code actions provide fixes for diagnostics at a location.
        level3: "## Diagnostics & Code Actions\n\n```typescript\ninterface Diagnostic {\n  range: Range;\n  severity: DiagnosticSeverity;\n\
          \  code?: string | number;\n  source?: string;\n  message: string;\n  relatedInformation?: DiagnosticRelatedInformation[];\n\
          }\n\nclass DiagnosticProvider {\n  validate(uri: string, content: string): Diagnostic[] {\n    const diagnostics:\
          \ Diagnostic[] = [];\n    const ast = this.parse(content);\n    \n    // Check for errors\n    this.walk(ast, (node)\
          \ => {\n      // Undefined variable\n      if (node.type === 'Identifier' && !this.isDefined(node.name)) {\n   \
          \     diagnostics.push({\n          range: node.range,\n          severity: DiagnosticSeverity.Error,\n        \
          \  code: 'undefined-variable',\n          source: 'my-lsp',\n          message: `'${node.name}' is not defined`\n\
          \        });\n      }\n      \n      // Type mismatch\n      if (node.type === 'Assignment') {\n        const leftType\
          \ = this.getType(node.left);\n        const rightType = this.getType(node.right);\n        if (leftType && rightType\
          \ && !this.isAssignable(leftType, rightType)) {\n          diagnostics.push({\n            range: node.range,\n\
          \            severity: DiagnosticSeverity.Error,\n            code: 'type-mismatch',\n            source: 'my-lsp',\n\
          \            message: `Type '${rightType}' is not assignable to type '${leftType}'`\n          });\n        }\n\
          \      }\n      \n      // Unused variable (warning)\n      if (node.type === 'VariableDeclaration' && !this.isUsed(node.name))\
          \ {\n        diagnostics.push({\n          range: node.range,\n          severity: DiagnosticSeverity.Warning,\n\
          \          code: 'unused-variable',\n          source: 'my-lsp',\n          message: `'${node.name}' is declared\
          \ but never used`\n        });\n      }\n    });\n    \n    return diagnostics;\n  }\n}\n\nclass CodeActionProvider\
          \ {\n  getCodeActions(uri: string, range: Range, diagnostics: Diagnostic[]): CodeAction[] {\n    const actions:\
          \ CodeAction[] = [];\n    \n    for (const diag of diagnostics) {\n      switch (diag.code) {\n        case 'undefined-variable':\n\
          \          // Suggest declaring the variable\n          const varName = this.extractVarName(diag.message);\n   \
          \       actions.push({\n            title: `Declare variable '${varName}'`,\n            kind: CodeActionKind.QuickFix,\n\
          \            diagnostics: [diag],\n            edit: {\n              changes: {\n                [uri]: [{\n  \
          \                range: { start: { line: range.start.line, character: 0 }, end: { line: range.start.line, character:\
          \ 0 } },\n                  newText: `let ${varName};\\n`\n                }]\n              }\n            }\n\
          \          });\n          break;\n          \n        case 'unused-variable':\n          // Suggest removing or\
          \ prefixing with _\n          actions.push({\n            title: 'Remove unused variable',\n            kind: CodeActionKind.QuickFix,\n\
          \            diagnostics: [diag],\n            edit: {\n              changes: {\n                [uri]: [{ range:\
          \ diag.range, newText: '' }]\n              }\n            }\n          });\n          break;\n      }\n    }\n\
          \    \n    return actions;\n  }\n}\n\n// Publish diagnostics\nprivate validate(uri: string) {\n  const doc = this.documents.get(uri);\n\
          \  if (!doc) return;\n  \n  const diagnostics = this.diagnosticProvider.validate(uri, doc.content);\n  \n  this.sendNotification('textDocument/publishDiagnostics',\
          \ {\n    uri,\n    diagnostics\n  });\n}\n```"
      pitfalls:
      - Diagnostic spam
      - Outdated diagnostics
      - Large workspace performance
      concepts:
      - Static analysis
      - Quick fixes
      - Editor integration
      estimated_hours: 15-20
  build-nn-framework:
    id: build-nn-framework
    name: Build Your Own Neural Network Framework
    description: Build a PyTorch/TensorFlow-like deep learning framework with automatic differentiation.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - Linear algebra
    - Calculus (chain rule)
    - Python/NumPy
    - Basic neural networks
    languages:
      recommended:
      - Python
      - Rust
      - C++
      also_possible:
      - Julia
    resources:
    - type: video
      name: Micrograd by Karpathy
      url: https://www.youtube.com/watch?v=VMj-3S1tku0
    - type: article
      name: Autodiff from Scratch
      url: https://sidsite.com/posts/autodiff/
    - type: code
      name: Tinygrad
      url: https://github.com/tinygrad/tinygrad
    milestones:
    - id: 1
      name: Tensor & Operations
      description: Implement tensor data structure with basic operations.
      acceptance_criteria:
      - N-dimensional array storage
      - Element-wise operations
      - Matrix multiplication
      - Broadcasting
      - GPU support (optional)
      hints:
        level1: Wrap NumPy arrays, track shape and data. Implement __add__, __mul__, etc.
        level2: Support broadcasting rules. Matrix multiply with @ operator.
        level3: "## Tensor Implementation\n\n```python\nimport numpy as np\n\nclass Tensor:\n    def __init__(self, data,\
          \ requires_grad=False):\n        self.data = np.array(data, dtype=np.float32)\n        self.requires_grad = requires_grad\n\
          \        self.grad = None\n        self._backward = lambda: None\n        self._prev = set()\n    \n    @property\n\
          \    def shape(self):\n        return self.data.shape\n    \n    def __repr__(self):\n        return f\"Tensor({self.data},\
          \ requires_grad={self.requires_grad})\"\n    \n    def __add__(self, other):\n        other = other if isinstance(other,\
          \ Tensor) else Tensor(other)\n        out = Tensor(self.data + other.data, requires_grad=self.requires_grad or other.requires_grad)\n\
          \        \n        def _backward():\n            if self.requires_grad:\n                self.grad = self.grad +\
          \ out.grad if self.grad is not None else out.grad.copy()\n            if other.requires_grad:\n                other.grad\
          \ = other.grad + out.grad if other.grad is not None else out.grad.copy()\n        \n        out._backward = _backward\n\
          \        out._prev = {self, other}\n        return out\n    \n    def __mul__(self, other):\n        other = other\
          \ if isinstance(other, Tensor) else Tensor(other)\n        out = Tensor(self.data * other.data, requires_grad=self.requires_grad\
          \ or other.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n            \
          \    grad = other.data * out.grad\n                self.grad = self.grad + grad if self.grad is not None else grad\n\
          \            if other.requires_grad:\n                grad = self.data * out.grad\n                other.grad =\
          \ other.grad + grad if other.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev\
          \ = {self, other}\n        return out\n    \n    def __matmul__(self, other):\n        out = Tensor(self.data @\
          \ other.data, requires_grad=self.requires_grad or other.requires_grad)\n        \n        def _backward():\n   \
          \         if self.requires_grad:\n                grad = out.grad @ other.data.T\n                self.grad = self.grad\
          \ + grad if self.grad is not None else grad\n            if other.requires_grad:\n                grad = self.data.T\
          \ @ out.grad\n                other.grad = other.grad + grad if other.grad is not None else grad\n        \n   \
          \     out._backward = _backward\n        out._prev = {self, other}\n        return out\n    \n    def sum(self):\n\
          \        out = Tensor(self.data.sum(), requires_grad=self.requires_grad)\n        \n        def _backward():\n \
          \           if self.requires_grad:\n                grad = np.ones_like(self.data) * out.grad\n                self.grad\
          \ = self.grad + grad if self.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev\
          \ = {self}\n        return out\n```"
      pitfalls:
      - Broadcasting gradient shapes
      - In-place operations breaking grad
      - Memory management
      concepts:
      - Tensor operations
      - Broadcasting
      - Memory layout
      estimated_hours: 12-18
    - id: 2
      name: Automatic Differentiation
      description: Implement reverse-mode autodiff (backpropagation).
      acceptance_criteria:
      - Computational graph
      - Reverse-mode autodiff
      - Gradient accumulation
      - Topological sort for backward pass
      hints:
        level1: Build computation graph as operations happen. Backward traverses in reverse.
        level2: Topological sort ensures gradients flow correctly. Accumulate, don't replace.
        level3: "## Backpropagation\n\n```python\nclass Tensor:\n    # ... previous code ...\n    \n    def backward(self):\n\
          \        # Build topological order\n        topo = []\n        visited = set()\n        \n        def build_topo(v):\n\
          \            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n     \
          \               build_topo(child)\n                topo.append(v)\n        \n        build_topo(self)\n        \n\
          \        # Initialize gradient of output\n        self.grad = np.ones_like(self.data)\n        \n        # Backpropagate\n\
          \        for node in reversed(topo):\n            node._backward()\n    \n    def relu(self):\n        out = Tensor(np.maximum(0,\
          \ self.data), requires_grad=self.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n\
          \                grad = (self.data > 0) * out.grad\n                self.grad = self.grad + grad if self.grad is\
          \ not None else grad\n        \n        out._backward = _backward\n        out._prev = {self}\n        return out\n\
          \    \n    def exp(self):\n        out = Tensor(np.exp(self.data), requires_grad=self.requires_grad)\n        \n\
          \        def _backward():\n            if self.requires_grad:\n                grad = out.data * out.grad\n    \
          \            self.grad = self.grad + grad if self.grad is not None else grad\n        \n        out._backward =\
          \ _backward\n        out._prev = {self}\n        return out\n    \n    def log(self):\n        out = Tensor(np.log(self.data),\
          \ requires_grad=self.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n  \
          \              grad = out.grad / self.data\n                self.grad = self.grad + grad if self.grad is not None\
          \ else grad\n        \n        out._backward = _backward\n        out._prev = {self}\n        return out\n\ndef\
          \ softmax(x):\n    exp_x = (x - Tensor(x.data.max(axis=-1, keepdims=True))).exp()\n    return exp_x * Tensor(1.0\
          \ / exp_x.data.sum(axis=-1, keepdims=True))\n\ndef cross_entropy(pred, target):\n    # pred: (batch, classes), target:\
          \ (batch,) indices\n    batch_size = pred.shape[0]\n    log_probs = pred.log()\n    # Select log prob of correct\
          \ class\n    loss = Tensor(0.0, requires_grad=True)\n    for i in range(batch_size):\n        loss = loss - log_probs.data[i,\
          \ target[i]]\n    return loss * Tensor(1.0 / batch_size)\n```"
      pitfalls:
      - Gradient not accumulated
      - Wrong topological order
      - Vanishing/exploding gradients
      concepts:
      - Computational graphs
      - Chain rule
      - Backpropagation
      estimated_hours: 15-20
    - id: 3
      name: Layers & Modules
      description: Implement neural network layers and module system.
      acceptance_criteria:
      - Linear layer
      - Activation functions
      - Module base class
      - Parameter management
      - Forward pass
      hints:
        level1: 'Module stores parameters. Linear layer: y = Wx + b.'
        level2: Implement parameters() method to collect all trainable tensors.
        level3: "## Module System\n\n```python\nclass Module:\n    def __init__(self):\n        self._parameters = {}\n  \
          \      self._modules = {}\n    \n    def __setattr__(self, name, value):\n        if isinstance(value, Tensor) and\
          \ value.requires_grad:\n            self._parameters[name] = value\n        elif isinstance(value, Module):\n  \
          \          self._modules[name] = value\n        super().__setattr__(name, value)\n    \n    def parameters(self):\n\
          \        params = list(self._parameters.values())\n        for module in self._modules.values():\n            params.extend(module.parameters())\n\
          \        return params\n    \n    def zero_grad(self):\n        for p in self.parameters():\n            p.grad\
          \ = None\n    \n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\nclass\
          \ Linear(Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        # Xavier\
          \ initialization\n        k = np.sqrt(1 / in_features)\n        self.weight = Tensor(np.random.uniform(-k, k, (in_features,\
          \ out_features)), requires_grad=True)\n        self.bias = Tensor(np.zeros(out_features), requires_grad=True)\n\
          \    \n    def forward(self, x):\n        return x @ self.weight + self.bias\n\nclass ReLU(Module):\n    def forward(self,\
          \ x):\n        return x.relu()\n\nclass Sequential(Module):\n    def __init__(self, *layers):\n        super().__init__()\n\
          \        for i, layer in enumerate(layers):\n            setattr(self, f'layer_{i}', layer)\n        self.layers\
          \ = layers\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n     \
          \   return x\n\n# Example: MLP\nclass MLP(Module):\n    def __init__(self, in_dim, hidden_dim, out_dim):\n     \
          \   super().__init__()\n        self.fc1 = Linear(in_dim, hidden_dim)\n        self.fc2 = Linear(hidden_dim, out_dim)\n\
          \    \n    def forward(self, x):\n        x = self.fc1(x).relu()\n        x = self.fc2(x)\n        return x\n```"
      pitfalls:
      - Parameter not tracked
      - In-place modifications
      - Wrong initialization
      concepts:
      - Module pattern
      - Parameter management
      - Initialization
      estimated_hours: 12-18
    - id: 4
      name: Optimizers & Training
      description: Implement optimizers and training loop.
      acceptance_criteria:
      - SGD optimizer
      - Adam optimizer
      - Learning rate scheduling
      - Mini-batch training
      - Model save/load
      hints:
        level1: 'SGD: param -= lr * grad. Adam adds momentum and adaptive learning.'
        level2: Track running averages for Adam. Clip gradients to prevent explosion.
        level3: "## Optimizers\n\n```python\nclass Optimizer:\n    def __init__(self, parameters, lr=0.01):\n        self.parameters\
          \ = parameters\n        self.lr = lr\n    \n    def zero_grad(self):\n        for p in self.parameters:\n      \
          \      p.grad = None\n    \n    def step(self):\n        raise NotImplementedError\n\nclass SGD(Optimizer):\n  \
          \  def __init__(self, parameters, lr=0.01, momentum=0.0):\n        super().__init__(parameters, lr)\n        self.momentum\
          \ = momentum\n        self.velocities = [np.zeros_like(p.data) for p in parameters]\n    \n    def step(self):\n\
          \        for i, p in enumerate(self.parameters):\n            if p.grad is None:\n                continue\n   \
          \         self.velocities[i] = self.momentum * self.velocities[i] - self.lr * p.grad\n            p.data += self.velocities[i]\n\
          \nclass Adam(Optimizer):\n    def __init__(self, parameters, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n    \
          \    super().__init__(parameters, lr)\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps\
          \ = eps\n        self.t = 0\n        self.m = [np.zeros_like(p.data) for p in parameters]\n        self.v = [np.zeros_like(p.data)\
          \ for p in parameters]\n    \n    def step(self):\n        self.t += 1\n        for i, p in enumerate(self.parameters):\n\
          \            if p.grad is None:\n                continue\n            \n            # Update biased first moment\
          \ estimate\n            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * p.grad\n            # Update biased\
          \ second raw moment estimate\n            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (p.grad ** 2)\n\
          \            \n            # Compute bias-corrected estimates\n            m_hat = self.m[i] / (1 - self.beta1 **\
          \ self.t)\n            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n            \n            # Update parameters\n\
          \            p.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n\n# Training loop\ndef train(model, X, y,\
          \ epochs=100, batch_size=32, lr=0.01):\n    optimizer = Adam(model.parameters(), lr=lr)\n    n_samples = len(X)\n\
          \    \n    for epoch in range(epochs):\n        # Shuffle data\n        indices = np.random.permutation(n_samples)\n\
          \        total_loss = 0\n        \n        for i in range(0, n_samples, batch_size):\n            batch_idx = indices[i:i+batch_size]\n\
          \            x_batch = Tensor(X[batch_idx], requires_grad=False)\n            y_batch = y[batch_idx]\n         \
          \   \n            # Forward\n            pred = model(x_batch)\n            loss = cross_entropy(softmax(pred),\
          \ y_batch)\n            \n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n\
          \            optimizer.step()\n            \n            total_loss += loss.data\n        \n        if epoch % 10\
          \ == 0:\n            print(f\"Epoch {epoch}, Loss: {total_loss / (n_samples / batch_size):.4f}\")\n```"
      pitfalls:
      - Bias correction in Adam
      - Learning rate too high/low
      - Not shuffling data
      concepts:
      - Optimization algorithms
      - Momentum
      - Adaptive learning rates
      estimated_hours: 15-20
  build-observability-platform:
    id: build-observability-platform
    name: Build Your Own Observability Platform
    description: Build a unified observability platform combining logs, metrics, and traces.
    difficulty: expert
    estimated_hours: 80-120
    prerequisites:
    - Distributed tracing
    - Metrics systems
    - Log aggregation
    - Time-series databases
    - Full-text search
    languages:
      recommended:
      - Go
      - Rust
      also_possible:
      - Java
      - Python
    resources:
    - type: book
      name: Observability Engineering
      url: https://www.oreilly.com/library/view/observability-engineering/9781492076438/
    - type: article
      name: Three Pillars of Observability
      url: https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html
    - type: documentation
      name: OpenTelemetry Collector
      url: https://opentelemetry.io/docs/collector/
    milestones:
    - id: 1
      name: Unified Data Model
      description: Design a unified data model that correlates logs, metrics, and traces.
      acceptance_criteria:
      - Common attributes
      - Trace-log correlation
      - Metric-trace correlation
      - Resource attribution
      - OpenTelemetry compatibility
      hints:
        level1: 'All signals share: timestamp, service, resource attributes, trace context.'
        level2: Use trace_id/span_id to correlate logs with traces. Use exemplars for metrics.
        level3: "## Unified Data Model\n\n```go\n// Common resource attributes\ntype Resource struct {\n    ServiceName  \
          \    string            `json:\"service.name\"`\n    ServiceVersion   string            `json:\"service.version\"\
          `\n    ServiceInstance  string            `json:\"service.instance.id\"`\n    HostName         string          \
          \  `json:\"host.name\"`\n    Environment      string            `json:\"deployment.environment\"`\n    Attributes\
          \       map[string]string `json:\"attributes\"`\n}\n\n// Trace context for correlation\ntype TraceContext struct\
          \ {\n    TraceID string `json:\"trace_id,omitempty\"`\n    SpanID  string `json:\"span_id,omitempty\"`\n}\n\n//\
          \ Log entry\ntype LogRecord struct {\n    Timestamp     time.Time         `json:\"timestamp\"`\n    Resource   \
          \   Resource          `json:\"resource\"`\n    TraceContext  TraceContext      `json:\"trace_context,omitempty\"\
          `\n    SeverityText  string            `json:\"severity_text\"`\n    SeverityNum   int               `json:\"severity_number\"\
          `\n    Body          string            `json:\"body\"`\n    Attributes    map[string]string `json:\"attributes\"\
          `\n}\n\n// Metric data point\ntype MetricDataPoint struct {\n    Timestamp   time.Time         `json:\"timestamp\"\
          `\n    Resource    Resource          `json:\"resource\"`\n    Name        string            `json:\"name\"`\n  \
          \  Type        string            `json:\"type\"` // counter, gauge, histogram\n    Value       float64         \
          \  `json:\"value\"`\n    Labels      map[string]string `json:\"labels\"`\n    Exemplars   []Exemplar        `json:\"\
          exemplars,omitempty\"`\n}\n\n// Exemplar links metric to trace\ntype Exemplar struct {\n    TraceID   string   \
          \ `json:\"trace_id\"`\n    SpanID    string    `json:\"span_id\"`\n    Value     float64   `json:\"value\"`\n  \
          \  Timestamp time.Time `json:\"timestamp\"`\n}\n\n// Span (trace segment)\ntype Span struct {\n    TraceID     \
          \ string            `json:\"trace_id\"`\n    SpanID       string            `json:\"span_id\"`\n    ParentSpanID\
          \ string            `json:\"parent_span_id,omitempty\"`\n    Resource     Resource          `json:\"resource\"`\n\
          \    Name         string            `json:\"name\"`\n    Kind         string            `json:\"kind\"` // client,\
          \ server, internal\n    StartTime    time.Time         `json:\"start_time\"`\n    EndTime      time.Time       \
          \  `json:\"end_time\"`\n    Status       string            `json:\"status\"`\n    Attributes   map[string]string\
          \ `json:\"attributes\"`\n    Events       []SpanEvent       `json:\"events\"`\n    Links        []SpanLink     \
          \   `json:\"links\"`\n}\n\n// Correlation helpers\nfunc (m *MetricDataPoint) GetExemplarTraces() []string {\n  \
          \  traceIDs := make([]string, len(m.Exemplars))\n    for i, e := range m.Exemplars {\n        traceIDs[i] = e.TraceID\n\
          \    }\n    return traceIDs\n}\n\nfunc (db *ObservabilityDB) GetLogsForTrace(traceID string) ([]LogRecord, error)\
          \ {\n    return db.logs.Query(LogQuery{TraceID: traceID})\n}\n\nfunc (db *ObservabilityDB) GetMetricsWithExemplar(traceID\
          \ string) ([]MetricDataPoint, error) {\n    return db.metrics.QueryByExemplar(traceID)\n}\n```"
      pitfalls:
      - Schema drift across signals
      - Missing correlation IDs
      - Cardinality explosion
      concepts:
      - Data modeling
      - Signal correlation
      - OpenTelemetry semantics
      estimated_hours: 10-15
    - id: 2
      name: Data Ingestion Pipeline
      description: Build a high-throughput ingestion pipeline for all signal types.
      acceptance_criteria:
      - OTLP protocol support
      - Batch ingestion
      - Backpressure handling
      - Data validation
      - Routing/filtering
      hints:
        level1: Accept OTLP (gRPC and HTTP). Buffer incoming data before writing to storage.
        level2: 'Implement pipelines: receivers -> processors -> exporters.'
        level3: "## Ingestion Pipeline\n\n```go\n// Pipeline architecture\ntype Pipeline struct {\n    receivers  []Receiver\n\
          \    processors []Processor\n    exporters  []Exporter\n    bufferSize int\n    dataChan   chan Signal\n}\n\ntype\
          \ Signal interface {\n    Type() string  // \"logs\", \"metrics\", \"traces\"\n    Resource() Resource\n}\n\n//\
          \ OTLP Receiver\ntype OTLPReceiver struct {\n    grpcServer *grpc.Server\n    httpServer *http.Server\n    output\
          \     chan<- Signal\n}\n\nfunc (r *OTLPReceiver) ReceiveTraces(ctx context.Context, req *otlp.TracesRequest) (*otlp.TracesResponse,\
          \ error) {\n    for _, resourceSpan := range req.ResourceSpans {\n        resource := convertResource(resourceSpan.Resource)\n\
          \        for _, scopeSpan := range resourceSpan.ScopeSpans {\n            for _, span := range scopeSpan.Spans {\n\
          \                r.output <- &Span{\n                    Resource:  resource,\n                    TraceID:   hex.EncodeToString(span.TraceId),\n\
          \                    SpanID:    hex.EncodeToString(span.SpanId),\n                    Name:      span.Name,\n  \
          \                  StartTime: time.Unix(0, int64(span.StartTimeUnixNano)),\n                    EndTime:   time.Unix(0,\
          \ int64(span.EndTimeUnixNano)),\n                    // ... convert other fields\n                }\n          \
          \  }\n        }\n    }\n    return &otlp.TracesResponse{}, nil\n}\n\n// Processors\ntype FilterProcessor struct\
          \ {\n    rules []FilterRule\n}\n\nfunc (p *FilterProcessor) Process(signal Signal) (Signal, bool) {\n    for _,\
          \ rule := range p.rules {\n        if rule.Matches(signal) {\n            if rule.Action == \"drop\" {\n       \
          \         return nil, false\n            }\n        }\n    }\n    return signal, true\n}\n\ntype AttributeProcessor\
          \ struct {\n    actions []AttributeAction\n}\n\nfunc (p *AttributeProcessor) Process(signal Signal) (Signal, bool)\
          \ {\n    for _, action := range p.actions {\n        switch action.Type {\n        case \"insert\":\n          \
          \  signal.SetAttribute(action.Key, action.Value)\n        case \"delete\":\n            signal.DeleteAttribute(action.Key)\n\
          \        case \"hash\":\n            // Hash PII\n            val := signal.GetAttribute(action.Key)\n         \
          \   signal.SetAttribute(action.Key, hash(val))\n        }\n    }\n    return signal, true\n}\n\n// Backpressure\
          \ handling\nfunc (p *Pipeline) Run(ctx context.Context) {\n    // Bounded buffer with backpressure\n    buffer :=\
          \ make(chan Signal, p.bufferSize)\n    \n    // Receiver goroutines write to buffer\n    for _, receiver := range\
          \ p.receivers {\n        go receiver.Start(ctx, buffer)\n    }\n    \n    // Worker pool processes signals\n   \
          \ var wg sync.WaitGroup\n    for i := 0; i < runtime.NumCPU(); i++ {\n        wg.Add(1)\n        go func() {\n \
          \           defer wg.Done()\n            for signal := range buffer {\n                // Run through processors\n\
          \                var ok bool\n                for _, proc := range p.processors {\n                    signal, ok\
          \ = proc.Process(signal)\n                    if !ok {\n                        break\n                    }\n \
          \               }\n                if !ok {\n                    continue\n                }\n                \n\
          \                // Send to exporters\n                for _, exp := range p.exporters {\n                    exp.Export(signal)\n\
          \                }\n            }\n        }()\n    }\n    \n    <-ctx.Done()\n    close(buffer)\n    wg.Wait()\n\
          }\n```"
      pitfalls:
      - Data loss under load
      - Memory exhaustion
      - Head-of-line blocking
      concepts:
      - Data pipelines
      - Backpressure
      - Protocol buffers
      estimated_hours: 15-20
    - id: 3
      name: Multi-Signal Storage
      description: Implement storage backends optimized for each signal type.
      acceptance_criteria:
      - Log storage (full-text search)
      - Metric storage (time-series)
      - Trace storage (graph queries)
      - Retention policies
      - Compaction
      hints:
        level1: 'Use different storage strategies: inverted index for logs, TSDB for metrics, span storage for traces.'
        level2: 'Implement tiered storage: hot (recent, fast) -> warm -> cold (archived, slow).'
        level3: "## Multi-Signal Storage\n\n```go\n// Storage interface per signal type\ntype LogStorage interface {\n   \
          \ Insert(log *LogRecord) error\n    Search(query LogQuery) ([]LogRecord, error)\n    Aggregate(query AggregationQuery)\
          \ (map[string]int64, error)\n}\n\ntype MetricStorage interface {\n    Insert(metric *MetricDataPoint) error\n  \
          \  Query(query MetricQuery) ([]MetricDataPoint, error)\n    Downsample(resolution time.Duration) error\n}\n\ntype\
          \ TraceStorage interface {\n    InsertSpan(span *Span) error\n    GetTrace(traceID string) (*Trace, error)\n   \
          \ SearchTraces(query TraceQuery) ([]*TraceSummary, error)\n    GetServiceGraph(start, end time.Time) (*ServiceGraph,\
          \ error)\n}\n\n// Log storage with inverted index\ntype LogStore struct {\n    index    *InvertedIndex\n    segments\
          \ []*LogSegment\n    mu       sync.RWMutex\n}\n\ntype InvertedIndex struct {\n    terms map[string]*PostingList\
          \  // term -> doc IDs\n    fields map[string]*FieldIndex  // field name -> field values -> doc IDs\n}\n\nfunc (s\
          \ *LogStore) Search(query LogQuery) ([]LogRecord, error) {\n    // Parse query into terms\n    terms := tokenize(query.Query)\n\
          \    \n    // Intersect posting lists\n    var docIDs *roaring.Bitmap\n    for _, term := range terms {\n      \
          \  if posting := s.index.terms[term]; posting != nil {\n            if docIDs == nil {\n                docIDs =\
          \ posting.bitmap.Clone()\n            } else {\n                docIDs.And(posting.bitmap)\n            }\n    \
          \    }\n    }\n    \n    // Apply field filters\n    if query.Service != \"\" {\n        fieldBitmap := s.index.fields[\"\
          service\"].Get(query.Service)\n        docIDs.And(fieldBitmap)\n    }\n    \n    // Fetch documents\n    results\
          \ := make([]LogRecord, 0)\n    it := docIDs.Iterator()\n    for it.HasNext() {\n        docID := it.Next()\n   \
          \     log := s.getDocument(docID)\n        results = append(results, log)\n    }\n    \n    return results, nil\n\
          }\n\n// Metric storage with time-series optimization\ntype MetricStore struct {\n    series map[string]*TimeSeries\
          \  // metric+labels hash -> series\n    wal    *WAL\n}\n\ntype TimeSeries struct {\n    chunks []*Chunk\n    current\
          \ *Chunk\n}\n\ntype Chunk struct {\n    startTime  int64\n    endTime    int64\n    timestamps []int64\n    values\
          \     []float64\n    // Use delta encoding and compression\n}\n\n// Tiered storage\ntype TieredStorage struct {\n\
          \    hot   Storage  // Recent data, fast SSD\n    warm  Storage  // Older data, cheaper storage\n    cold  Storage\
          \  // Archive, object storage\n    \n    hotRetention  time.Duration  // e.g., 24h\n    warmRetention time.Duration\
          \  // e.g., 7d\n}\n\nfunc (t *TieredStorage) Compact() {\n    now := time.Now()\n    \n    // Move hot -> warm\n\
          \    hotCutoff := now.Add(-t.hotRetention)\n    t.hot.MoveOlderThan(hotCutoff, t.warm)\n    \n    // Move warm ->\
          \ cold\n    warmCutoff := now.Add(-t.warmRetention)\n    t.warm.MoveOlderThan(warmCutoff, t.cold)\n}\n```"
      pitfalls:
      - Query performance at scale
      - Storage costs
      - Data consistency across stores
      concepts:
      - Inverted indexes
      - Time-series compression
      - Tiered storage
      estimated_hours: 20-30
    - id: 4
      name: Unified Query Interface
      description: Build a query interface that can correlate across all signal types.
      acceptance_criteria:
      - Cross-signal queries
      - Automatic correlation
      - Query language
      - Saved queries
      - Query performance
      hints:
        level1: Allow jumping from metric anomaly -> exemplar traces -> related logs.
        level2: Implement a query language that can express correlations explicitly.
        level3: "## Unified Query Interface\n\n```go\n// Query language supporting cross-signal correlation\ntype Query struct\
          \ {\n    // Base query\n    Signal    string    // \"logs\", \"metrics\", \"traces\"\n    Filter    string    //\
          \ Signal-specific filter\n    TimeRange TimeRange\n    \n    // Correlation\n    Correlate *CorrelationQuery `json:\"\
          correlate,omitempty\"`\n}\n\ntype CorrelationQuery struct {\n    // Follow trace context\n    FollowTrace bool\n\
          \    // Get exemplars from metrics\n    GetExemplars bool\n    // Include related signals\n    Include []string\
          \  // [\"logs\", \"metrics\"]\n}\n\n// Example queries:\n// 1. \"Show me all logs for traces with errors\"\n// {\n\
          //   \"signal\": \"traces\",\n//   \"filter\": \"status = 'ERROR'\",\n//   \"correlate\": { \"include\": [\"logs\"\
          ] }\n// }\n\n// 2. \"Show me traces for this metric spike\"\n// {\n//   \"signal\": \"metrics\",\n//   \"filter\"\
          : \"http_latency_p99 > 1s\",\n//   \"correlate\": { \"getExemplars\": true }\n// }\n\nfunc (db *ObservabilityDB)\
          \ ExecuteQuery(q Query) (*QueryResult, error) {\n    result := &QueryResult{\n        Signal: q.Signal,\n    }\n\
          \    \n    // Execute base query\n    switch q.Signal {\n    case \"logs\":\n        logs, _ := db.logs.Search(parseLogFilter(q.Filter),\
          \ q.TimeRange)\n        result.Logs = logs\n        \n    case \"metrics\":\n        metrics, _ := db.metrics.Query(parseMetricFilter(q.Filter),\
          \ q.TimeRange)\n        result.Metrics = metrics\n        \n    case \"traces\":\n        traces, _ := db.traces.Search(parseTraceFilter(q.Filter),\
          \ q.TimeRange)\n        result.Traces = traces\n    }\n    \n    // Handle correlations\n    if q.Correlate != nil\
          \ {\n        result.Correlated = db.correlate(result, q.Correlate)\n    }\n    \n    return result, nil\n}\n\nfunc\
          \ (db *ObservabilityDB) correlate(result *QueryResult, corr *CorrelationQuery) *CorrelatedData {\n    data := &CorrelatedData{}\n\
          \    \n    // Collect trace IDs from results\n    traceIDs := make(map[string]bool)\n    \n    if corr.GetExemplars\
          \ && len(result.Metrics) > 0 {\n        for _, m := range result.Metrics {\n            for _, e := range m.Exemplars\
          \ {\n                traceIDs[e.TraceID] = true\n            }\n        }\n    }\n    \n    if len(result.Traces)\
          \ > 0 {\n        for _, t := range result.Traces {\n            traceIDs[t.TraceID] = true\n        }\n    }\n \
          \   \n    // Fetch correlated data\n    for _, include := range corr.Include {\n        switch include {\n     \
          \   case \"logs\":\n            for traceID := range traceIDs {\n                logs, _ := db.logs.Search(LogQuery{TraceID:\
          \ traceID})\n                data.Logs = append(data.Logs, logs...)\n            }\n        case \"traces\":\n \
          \           for traceID := range traceIDs {\n                trace, _ := db.traces.GetTrace(traceID)\n         \
          \       data.Traces = append(data.Traces, trace)\n            }\n        }\n    }\n    \n    return data\n}\n```\n\
          \n```javascript\n// UI for correlated exploration\nfunction ObservabilityExplorer() {\n  const [query, setQuery]\
          \ = useState('');\n  const [results, setResults] = useState(null);\n  const [selectedTrace, setSelectedTrace] =\
          \ useState(null);\n  \n  const explore = async (q) => {\n    const res = await fetch('/api/query', {\n      method:\
          \ 'POST',\n      body: JSON.stringify(q)\n    }).then(r => r.json());\n    setResults(res);\n  };\n  \n  return\
          \ (\n    <div className=\"explorer\">\n      <QueryEditor value={query} onChange={setQuery} onRun={explore} />\n\
          \      \n      {results && (\n        <>\n          <SignalTabs>\n            {results.metrics && <MetricsPanel\
          \ data={results.metrics} onExemplarClick={traceId => {\n              explore({ signal: 'traces', filter: `traceId\
          \ = '${traceId}'`, correlate: { include: ['logs'] }});\n            }} />}\n            {results.traces && <TracesPanel\
          \ data={results.traces} onSelect={setSelectedTrace} />}\n            {results.logs && <LogsPanel data={results.logs}\
          \ />}\n          </SignalTabs>\n          \n          {selectedTrace && (\n            <CorrelatedView\n       \
          \       trace={selectedTrace}\n              logs={results.correlated?.logs}\n              metrics={results.correlated?.metrics}\n\
          \            />\n          )}\n        </>\n      )}\n    </div>\n  );\n}\n```"
      pitfalls:
      - Query complexity explosion
      - Slow correlation queries
      - Missing correlation data
      concepts:
      - Query planning
      - Data correlation
      - Unified observability
      estimated_hours: 15-25
    - id: 5
      name: Alerting & Anomaly Detection
      description: Implement intelligent alerting that correlates anomalies across signals.
      acceptance_criteria:
      - Multi-signal alerts
      - Anomaly detection
      - Alert correlation/deduplication
      - Root cause suggestions
      hints:
        level1: 'Trigger alerts based on combinations: error logs + latency spike + trace errors.'
        level2: Use statistical methods (z-score, MAD) for anomaly detection on metrics.
        level3: "## Multi-Signal Alerting\n\n```go\n// Multi-signal alert rule\ntype AlertRule struct {\n    Name        string\n\
          \    Description string\n    \n    // Conditions across signals\n    Conditions []AlertCondition\n    Logic    \
          \  string  // \"all\" or \"any\"\n    \n    For         time.Duration\n    Annotations map[string]string\n}\n\n\
          type AlertCondition struct {\n    Signal    string  // \"logs\", \"metrics\", \"traces\"\n    Query     string\n\
          \    Threshold string  // e.g., \"> 100\", \"exists\"\n}\n\n// Example: Alert when high error rate AND error logs\
          \ appear\n// {\n//   \"name\": \"ServiceDegraded\",\n//   \"conditions\": [\n//     { \"signal\": \"metrics\", \"\
          query\": \"rate(http_errors_total[5m])\", \"threshold\": \"> 0.01\" },\n//     { \"signal\": \"logs\", \"query\"\
          : \"level=error\", \"threshold\": \"count > 10\" },\n//     { \"signal\": \"traces\", \"query\": \"status=ERROR\"\
          , \"threshold\": \"ratio > 0.05\" }\n//   ],\n//   \"logic\": \"any\",\n//   \"for\": \"5m\"\n// }\n\n// Anomaly\
          \ detection\ntype AnomalyDetector struct {\n    history map[string]*MetricHistory\n}\n\ntype MetricHistory struct\
          \ {\n    values []float64\n    times  []time.Time\n}\n\nfunc (d *AnomalyDetector) Detect(metric string, value float64)\
          \ *Anomaly {\n    hist := d.history[metric]\n    if hist == nil || len(hist.values) < 100 {\n        return nil\
          \  // Not enough history\n    }\n    \n    // Calculate statistics\n    mean := stat.Mean(hist.values, nil)\n  \
          \  stddev := stat.StdDev(hist.values, nil)\n    \n    // Z-score\n    zscore := (value - mean) / stddev\n    \n\
          \    if math.Abs(zscore) > 3 {\n        return &Anomaly{\n            Metric:    metric,\n            Value:   \
          \  value,\n            Expected:  mean,\n            ZScore:    zscore,\n            Timestamp: time.Now(),\n  \
          \      }\n    }\n    return nil\n}\n\n// Alert correlation - group related alerts\ntype AlertCorrelator struct {\n\
          \    window   time.Duration\n    alerts   []*Alert\n    groups   map[string]*AlertGroup\n}\n\nfunc (c *AlertCorrelator)\
          \ Correlate(alert *Alert) *AlertGroup {\n    // Find existing group by:\n    // 1. Same service\n    // 2. Related\
          \ trace IDs\n    // 3. Close in time\n    \n    for _, group := range c.groups {\n        if c.isRelated(alert,\
          \ group) {\n            group.Alerts = append(group.Alerts, alert)\n            return group\n        }\n    }\n\
          \    \n    // Create new group\n    group := &AlertGroup{\n        ID:        uuid.New().String(),\n        Alerts:\
          \    []*Alert{alert},\n        CreatedAt: time.Now(),\n    }\n    c.groups[group.ID] = group\n    return group\n\
          }\n\n// Root cause analysis\nfunc (db *ObservabilityDB) SuggestRootCause(alertGroup *AlertGroup) []RootCauseSuggestion\
          \ {\n    suggestions := []RootCauseSuggestion{}\n    \n    // Collect all trace IDs from alerts\n    traceIDs :=\
          \ collectTraceIDs(alertGroup)\n    \n    // Find common error spans\n    errorSpans := map[string]int{}  // operation\
          \ -> count\n    for _, traceID := range traceIDs {\n        trace, _ := db.traces.GetTrace(traceID)\n        for\
          \ _, span := range trace.Spans {\n            if span.Status == \"ERROR\" {\n                key := span.Service\
          \ + \"/\" + span.Name\n                errorSpans[key]++\n            }\n        }\n    }\n    \n    // Sort by\
          \ frequency\n    for op, count := range errorSpans {\n        if count > len(traceIDs)/2 {\n            suggestions\
          \ = append(suggestions, RootCauseSuggestion{\n                Type:       \"common_error_operation\",\n        \
          \        Operation:  op,\n                Confidence: float64(count) / float64(len(traceIDs)),\n               \
          \ Evidence:   fmt.Sprintf(\"%d/%d traces have errors in %s\", count, len(traceIDs), op),\n            })\n     \
          \   }\n    }\n    \n    return suggestions\n}\n```"
      pitfalls:
      - Alert fatigue
      - False positives
      - Missing root causes
      concepts:
      - Multi-signal correlation
      - Statistical anomaly detection
      - Root cause analysis
      estimated_hours: 20-30
  build-os:
    id: build-os
    name: Build Your Own OS
    description: Build a minimal operating system kernel from scratch.
    difficulty: expert
    estimated_hours: 100-200
    prerequisites:
    - Assembly language
    - C programming
    - Computer architecture
    - Memory management
    languages:
      recommended:
      - C
      - Rust
      - Zig
      also_possible:
      - Assembly
    resources:
    - type: book
      name: 'Operating Systems: Three Easy Pieces'
      url: https://pages.cs.wisc.edu/~remzi/OSTEP/
    - type: tutorial
      name: Writing an OS in Rust
      url: https://os.phil-opp.com/
    - type: wiki
      name: OSDev Wiki
      url: https://wiki.osdev.org/
    milestones:
    - id: 1
      name: Bootloader & Kernel Entry
      description: Boot from BIOS/UEFI and enter kernel code.
      acceptance_criteria:
      - Bootable image (ISO/USB)
      - Switch to protected/long mode
      - Set up GDT
      - Jump to kernel main
      hints:
        level1: Use GRUB or write a minimal bootloader. Kernel starts in real mode on x86.
        level2: Set up Global Descriptor Table (GDT) to enter 32/64-bit protected mode.
        level3: "## Bootloader Basics\n\n```asm\n; boot.asm - Minimal bootloader\n[BITS 16]\n[ORG 0x7C00]\n\nstart:\n    ;\
          \ Set up segments\n    xor ax, ax\n    mov ds, ax\n    mov es, ax\n    mov ss, ax\n    mov sp, 0x7C00\n    \n  \
          \  ; Load kernel from disk\n    mov ah, 0x02    ; BIOS read sectors\n    mov al, 10      ; Number of sectors\n \
          \   mov ch, 0       ; Cylinder\n    mov cl, 2       ; Sector (1-indexed, 1 is boot)\n    mov dh, 0       ; Head\n\
          \    mov bx, 0x1000  ; Load address\n    int 0x13\n    \n    ; Switch to protected mode\n    cli\n    lgdt [gdt_descriptor]\n\
          \    \n    mov eax, cr0\n    or eax, 1\n    mov cr0, eax\n    \n    jmp 0x08:protected_mode\n\n[BITS 32]\nprotected_mode:\n\
          \    ; Set up segment registers\n    mov ax, 0x10\n    mov ds, ax\n    mov es, ax\n    mov fs, ax\n    mov gs, ax\n\
          \    mov ss, ax\n    mov esp, 0x90000\n    \n    ; Jump to kernel\n    jmp 0x1000\n\n; GDT\ngdt_start:\n    dq 0\
          \                    ; Null descriptor\ngdt_code:\n    dw 0xFFFF, 0x0000      ; Code segment\n    db 0x00, 0x9A,\
          \ 0xCF, 0x00\ngdt_data:\n    dw 0xFFFF, 0x0000      ; Data segment\n    db 0x00, 0x92, 0xCF, 0x00\ngdt_end:\n\n\
          gdt_descriptor:\n    dw gdt_end - gdt_start - 1\n    dd gdt_start\n\ntimes 510-($-$$) db 0\ndw 0xAA55          \
          \     ; Boot signature\n```"
      pitfalls:
      - Wrong memory addresses
      - Forgetting to disable interrupts
      - GDT misconfiguration
      concepts:
      - Boot process
      - CPU modes
      - Memory layout
      estimated_hours: 15-25
    - id: 2
      name: Interrupts & Keyboard
      description: Handle hardware interrupts and keyboard input.
      acceptance_criteria:
      - Interrupt Descriptor Table (IDT)
      - PIC/APIC configuration
      - Timer interrupt
      - Keyboard driver
      hints:
        level1: Set up IDT with handlers for each interrupt. Remap PIC to avoid conflicts.
        level2: Timer (IRQ0) drives preemptive scheduling. Keyboard (IRQ1) for input.
        level3: "## Interrupt Handling\n\n```c\n// idt.c\n#include <stdint.h>\n\nstruct idt_entry {\n    uint16_t base_low;\n\
          \    uint16_t selector;\n    uint8_t  zero;\n    uint8_t  flags;\n    uint16_t base_high;\n} __attribute__((packed));\n\
          \nstruct idt_ptr {\n    uint16_t limit;\n    uint32_t base;\n} __attribute__((packed));\n\nstruct idt_entry idt[256];\n\
          struct idt_ptr idtp;\n\nvoid idt_set_gate(int num, uint32_t base, uint16_t sel, uint8_t flags) {\n    idt[num].base_low\
          \ = base & 0xFFFF;\n    idt[num].base_high = (base >> 16) & 0xFFFF;\n    idt[num].selector = sel;\n    idt[num].zero\
          \ = 0;\n    idt[num].flags = flags;\n}\n\n// Remap PIC\nvoid pic_remap() {\n    outb(0x20, 0x11);  // Init PIC1\n\
          \    outb(0xA0, 0x11);  // Init PIC2\n    outb(0x21, 0x20);  // PIC1 offset (IRQ 0-7 -> INT 32-39)\n    outb(0xA1,\
          \ 0x28);  // PIC2 offset (IRQ 8-15 -> INT 40-47)\n    outb(0x21, 0x04);  // Tell PIC1 about PIC2\n    outb(0xA1,\
          \ 0x02);  // Tell PIC2 its cascade identity\n    outb(0x21, 0x01);  // 8086 mode\n    outb(0xA1, 0x01);\n    outb(0x21,\
          \ 0x0);   // Unmask all\n    outb(0xA1, 0x0);\n}\n\n// Keyboard handler\nvoid keyboard_handler() {\n    uint8_t\
          \ scancode = inb(0x60);\n    \n    if (!(scancode & 0x80)) {  // Key press (not release)\n        char c = scancode_to_ascii(scancode);\n\
          \        if (c) {\n            terminal_putchar(c);\n        }\n    }\n    \n    // Send EOI\n    outb(0x20, 0x20);\n\
          }\n```"
      pitfalls:
      - Not sending EOI
      - Wrong PIC remapping
      - Stack corruption in handlers
      concepts:
      - Interrupts
      - Hardware I/O
      - Device drivers
      estimated_hours: 15-25
    - id: 3
      name: Memory Management
      description: Implement physical and virtual memory management.
      acceptance_criteria:
      - Physical memory manager
      - Paging setup
      - Page fault handler
      - Kernel heap allocator
      hints:
        level1: Track physical pages with a bitmap or free list. Set up page tables.
        level2: Enable paging in CR0. Handle page faults to implement demand paging.
        level3: "## Paging\n\n```c\n// Physical memory manager (bitmap)\n#define PAGE_SIZE 4096\n#define BITMAP_SIZE (total_memory\
          \ / PAGE_SIZE / 8)\n\nuint8_t *page_bitmap;\nsize_t total_pages;\n\nvoid pmm_init(size_t mem_size) {\n    total_pages\
          \ = mem_size / PAGE_SIZE;\n    page_bitmap = (uint8_t*)BITMAP_ADDR;\n    memset(page_bitmap, 0xFF, BITMAP_SIZE);\
          \  // All used initially\n    \n    // Mark available memory as free\n    // (from memory map provided by bootloader)\n\
          }\n\nvoid *pmm_alloc_page() {\n    for (size_t i = 0; i < total_pages; i++) {\n        if (!(page_bitmap[i / 8]\
          \ & (1 << (i % 8)))) {\n            page_bitmap[i / 8] |= (1 << (i % 8));  // Mark used\n            return (void*)(i\
          \ * PAGE_SIZE);\n        }\n    }\n    return NULL;  // Out of memory\n}\n\nvoid pmm_free_page(void *page) {\n \
          \   size_t idx = (size_t)page / PAGE_SIZE;\n    page_bitmap[idx / 8] &= ~(1 << (idx % 8));\n}\n\n// Page table setup\
          \ (x86)\ntypedef uint32_t page_entry_t;\n\npage_entry_t *page_directory;\npage_entry_t *page_tables[1024];\n\nvoid\
          \ paging_init() {\n    page_directory = pmm_alloc_page();\n    memset(page_directory, 0, PAGE_SIZE);\n    \n   \
          \ // Identity map first 4MB\n    page_entry_t *first_table = pmm_alloc_page();\n    for (int i = 0; i < 1024; i++)\
          \ {\n        first_table[i] = (i * PAGE_SIZE) | 3;  // Present, R/W\n    }\n    page_directory[0] = ((uint32_t)first_table)\
          \ | 3;\n    \n    // Enable paging\n    asm volatile(\n        \"mov %0, %%cr3\\n\"\n        \"mov %%cr0, %%eax\\\
          n\"\n        \"or $0x80000000, %%eax\\n\"\n        \"mov %%eax, %%cr0\\n\"\n        : : \"r\"(page_directory) :\
          \ \"eax\"\n    );\n}\n\nvoid map_page(uint32_t virt, uint32_t phys, uint32_t flags) {\n    uint32_t pd_idx = virt\
          \ >> 22;\n    uint32_t pt_idx = (virt >> 12) & 0x3FF;\n    \n    if (!(page_directory[pd_idx] & 1)) {\n        page_entry_t\
          \ *new_table = pmm_alloc_page();\n        memset(new_table, 0, PAGE_SIZE);\n        page_directory[pd_idx] = ((uint32_t)new_table)\
          \ | 3;\n    }\n    \n    page_entry_t *table = (page_entry_t*)(page_directory[pd_idx] & ~0xFFF);\n    table[pt_idx]\
          \ = phys | flags;\n}\n```"
      pitfalls:
      - TLB not flushed
      - Wrong page table structure
      - Kernel unmapped after enabling paging
      concepts:
      - Virtual memory
      - Paging
      - Memory protection
      estimated_hours: 25-40
    - id: 4
      name: Process Management
      description: Implement processes and basic scheduling.
      acceptance_criteria:
      - Process control block (PCB)
      - Context switching
      - Round-robin scheduler
      - System calls (fork, exec, exit)
      hints:
        level1: Store process state in PCB. Timer interrupt triggers scheduler.
        level2: Context switch saves/restores registers. Each process has own page table.
        level3: "## Process Management\n\n```c\ntypedef struct {\n    uint32_t eax, ebx, ecx, edx;\n    uint32_t esi, edi,\
          \ ebp, esp;\n    uint32_t eip, eflags;\n    uint32_t cr3;  // Page directory\n} cpu_state_t;\n\ntypedef struct process\
          \ {\n    int pid;\n    enum { READY, RUNNING, BLOCKED, ZOMBIE } state;\n    cpu_state_t regs;\n    uint32_t *page_directory;\n\
          \    struct process *next;\n} process_t;\n\nprocess_t *current_process;\nprocess_t *ready_queue;\n\nvoid schedule()\
          \ {\n    if (!ready_queue) return;\n    \n    // Save current process state\n    if (current_process && current_process->state\
          \ == RUNNING) {\n        current_process->state = READY;\n        // Add to end of ready queue\n        process_t\
          \ *p = ready_queue;\n        while (p->next) p = p->next;\n        p->next = current_process;\n        current_process->next\
          \ = NULL;\n    }\n    \n    // Get next process\n    current_process = ready_queue;\n    ready_queue = ready_queue->next;\n\
          \    current_process->state = RUNNING;\n    \n    // Switch to process\n    switch_context(&current_process->regs);\n\
          }\n\n// Context switch (assembly)\nextern void switch_context(cpu_state_t *state);\n\n// Timer interrupt calls scheduler\n\
          void timer_handler() {\n    // ... update time ...\n    schedule();\n    outb(0x20, 0x20);  // EOI\n}\n\nint sys_fork()\
          \ {\n    process_t *child = kmalloc(sizeof(process_t));\n    child->pid = next_pid++;\n    child->state = READY;\n\
          \    \n    // Copy page directory (copy-on-write would be better)\n    child->page_directory = clone_page_directory(current_process->page_directory);\n\
          \    \n    // Copy registers, set child return value to 0\n    child->regs = current_process->regs;\n    child->regs.eax\
          \ = 0;\n    \n    // Add to ready queue\n    child->next = ready_queue;\n    ready_queue = child;\n    \n    return\
          \ child->pid;  // Parent returns child PID\n}\n```"
      pitfalls:
      - Stack corruption during switch
      - Not saving all registers
      - Deadlocks in scheduling
      concepts:
      - Context switching
      - Scheduling algorithms
      - Process isolation
      estimated_hours: 30-50
  build-raft:
    id: build-raft
    name: Build Your Own Raft
    description: Implement the Raft consensus algorithm for distributed agreement. Learn leader election, log replication,
      and safety.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - Replicated log
    - Leader election
    - Distributed systems basics
    languages:
      recommended:
      - Go
      - Rust
      - Java
      also_possible:
      - Python
      - C++
    resources:
    - type: paper
      name: In Search of an Understandable Consensus Algorithm
      url: https://raft.github.io/raft.pdf
    - type: visualization
      name: Raft Visualization
      url: https://raft.github.io/
    milestones:
    - id: 1
      name: Leader Election
      description: Implement Raft leader election with terms and voting.
      acceptance_criteria:
      - Term management
      - Election timeout randomization
      - RequestVote RPC
      - Vote granting rules
      hints:
        level1: Follower times out -> becomes candidate -> requests votes. Majority wins.
        level2: Random timeout (150-300ms) prevents split votes. Only vote once per term.
        level3: "import random\nimport asyncio\nfrom enum import Enum\n\nclass State(Enum):\n    FOLLOWER = 'follower'\n \
          \   CANDIDATE = 'candidate'\n    LEADER = 'leader'\n\nclass RaftNode:\n    def __init__(self, node_id, peers):\n\
          \        self.id = node_id\n        self.peers = peers\n        \n        # Persistent state\n        self.current_term\
          \ = 0\n        self.voted_for = None\n        self.log = []  # List of (term, command)\n        \n        # Volatile\
          \ state\n        self.state = State.FOLLOWER\n        self.commit_index = 0\n        self.last_applied = 0\n   \
          \     \n        # Leader state\n        self.next_index = {}   # peer -> next log index to send\n        self.match_index\
          \ = {}  # peer -> highest replicated index\n        \n        self.election_timeout = self._random_timeout()\n \
          \       self.last_heartbeat = time.time()\n    \n    def _random_timeout(self):\n        return random.uniform(0.15,\
          \ 0.3)  # 150-300ms\n    \n    async def election_timer(self):\n        while True:\n            await asyncio.sleep(0.05)\
          \  # Check every 50ms\n            \n            if self.state == State.LEADER:\n                continue\n    \
          \        \n            if time.time() - self.last_heartbeat > self.election_timeout:\n                await self.start_election()\n\
          \    \n    async def start_election(self):\n        self.state = State.CANDIDATE\n        self.current_term += 1\n\
          \        self.voted_for = self.id\n        self.election_timeout = self._random_timeout()\n        self.last_heartbeat\
          \ = time.time()\n        \n        votes = 1  # Vote for self\n        last_log_index = len(self.log) - 1\n    \
          \    last_log_term = self.log[-1][0] if self.log else 0\n        \n        # Request votes from all peers\n    \
          \    tasks = []\n        for peer in self.peers:\n            tasks.append(self.send_request_vote(peer, last_log_index,\
          \ last_log_term))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        votes\
          \ += sum(1 for r in results if r is True)\n        \n        # Check if won\n        if votes > (len(self.peers)\
          \ + 1) // 2:\n            self.become_leader()\n    \n    def handle_request_vote(self, term, candidate_id, last_log_index,\
          \ last_log_term):\n        if term < self.current_term:\n            return False, self.current_term\n        \n\
          \        if term > self.current_term:\n            self.current_term = term\n            self.voted_for = None\n\
          \            self.state = State.FOLLOWER\n        \n        # Check if log is up-to-date\n        my_last_term =\
          \ self.log[-1][0] if self.log else 0\n        my_last_index = len(self.log) - 1\n        \n        log_ok = (last_log_term\
          \ > my_last_term or \n                  (last_log_term == my_last_term and last_log_index >= my_last_index))\n \
          \       \n        if (self.voted_for is None or self.voted_for == candidate_id) and log_ok:\n            self.voted_for\
          \ = candidate_id\n            self.last_heartbeat = time.time()\n            return True, self.current_term\n  \
          \      \n        return False, self.current_term"
      pitfalls:
      - Split vote scenarios
      - Term number handling
      - Log up-to-date check
      concepts:
      - Leader election
      - Distributed voting
      - Failure detection
      estimated_hours: 12-18
    - id: 2
      name: Log Replication
      description: Implement log replication from leader to followers.
      acceptance_criteria:
      - AppendEntries RPC
      - Log consistency check
      - Follower log repair
      - Commit index advancement
      hints:
        level1: Leader sends heartbeats and log entries. Include prev log index/term for consistency.
        level2: On conflict, follower rejects. Leader decrements nextIndex and retries.
        level3: "async def send_append_entries(self, peer):\n    prev_log_index = self.next_index[peer] - 1\n    prev_log_term\
          \ = self.log[prev_log_index][0] if prev_log_index >= 0 else 0\n    \n    entries = self.log[self.next_index[peer]:]\n\
          \    \n    success, term = await peer.append_entries(\n        term=self.current_term,\n        leader_id=self.id,\n\
          \        prev_log_index=prev_log_index,\n        prev_log_term=prev_log_term,\n        entries=entries,\n      \
          \  leader_commit=self.commit_index\n    )\n    \n    if term > self.current_term:\n        self.current_term = term\n\
          \        self.state = State.FOLLOWER\n        return\n    \n    if success:\n        self.next_index[peer] = len(self.log)\n\
          \        self.match_index[peer] = len(self.log) - 1\n        self.advance_commit_index()\n    else:\n        # Decrement\
          \ and retry\n        self.next_index[peer] = max(0, self.next_index[peer] - 1)\n\ndef handle_append_entries(self,\
          \ term, leader_id, prev_log_index, prev_log_term, entries, leader_commit):\n    self.last_heartbeat = time.time()\n\
          \    \n    if term < self.current_term:\n        return False, self.current_term\n    \n    if term > self.current_term:\n\
          \        self.current_term = term\n        self.voted_for = None\n    \n    self.state = State.FOLLOWER\n    \n\
          \    # Check log consistency\n    if prev_log_index >= 0:\n        if prev_log_index >= len(self.log):\n       \
          \     return False, self.current_term\n        if self.log[prev_log_index][0] != prev_log_term:\n            # Delete\
          \ conflicting entries\n            self.log = self.log[:prev_log_index]\n            return False, self.current_term\n\
          \    \n    # Append new entries\n    for i, entry in enumerate(entries):\n        idx = prev_log_index + 1 + i\n\
          \        if idx < len(self.log):\n            if self.log[idx][0] != entry[0]:\n                self.log = self.log[:idx]\n\
          \                self.log.append(entry)\n        else:\n            self.log.append(entry)\n    \n    # Update commit\
          \ index\n    if leader_commit > self.commit_index:\n        self.commit_index = min(leader_commit, len(self.log)\
          \ - 1)\n    \n    return True, self.current_term\n\ndef advance_commit_index(self):\n    # Find N such that majority\
          \ have match_index >= N\n    for n in range(len(self.log) - 1, self.commit_index, -1):\n        if self.log[n][0]\
          \ != self.current_term:\n            continue\n        \n        count = 1  # Self\n        for peer in self.peers:\n\
          \            if self.match_index.get(peer, 0) >= n:\n                count += 1\n        \n        if count > (len(self.peers)\
          \ + 1) // 2:\n            self.commit_index = n\n            break"
      pitfalls:
      - Log index off-by-one
      - Commit index from previous term
      - Handling gaps
      concepts:
      - Log replication
      - Consistency
      - Quorum
      estimated_hours: 15-25
    - id: 3
      name: Safety Properties
      description: Ensure Raft safety guarantees are maintained.
      acceptance_criteria:
      - Election safety
      - Leader append-only
      - Log matching
      - Leader completeness
      hints:
        level1: Never commit entries from previous terms directly. Only commit by replicating current term entry.
        level2: Leader never overwrites its log. Followers may truncate conflicting entries.
        level3: "# Safety invariant checks\n\nclass SafetyChecker:\n    \"\"\"Debug helper to verify Raft safety properties\"\
          \"\"\n    \n    @staticmethod\n    def check_election_safety(nodes):\n        \"\"\"At most one leader per term\"\
          \"\"\n        leaders_by_term = {}\n        for node in nodes:\n            if node.state == State.LEADER:\n   \
          \             term = node.current_term\n                if term in leaders_by_term:\n                    raise SafetyViolation(\n\
          \                        f'Multiple leaders in term {term}: '\n                        f'{leaders_by_term[term]}\
          \ and {node.id}'\n                    )\n                leaders_by_term[term] = node.id\n    \n    @staticmethod\n\
          \    def check_log_matching(nodes):\n        \"\"\"If two logs contain entry with same index and term,\n       \
          \    logs are identical up to that index\"\"\"\n        for i, n1 in enumerate(nodes):\n            for n2 in nodes[i+1:]:\n\
          \                for idx in range(min(len(n1.log), len(n2.log))):\n                    if n1.log[idx][0] == n2.log[idx][0]:\
          \  # Same term\n                        # All previous entries must match\n                        for j in range(idx):\n\
          \                            if n1.log[j] != n2.log[j]:\n                                raise SafetyViolation(\n\
          \                                    f'Log mismatch at {j} between {n1.id} and {n2.id}'\n                      \
          \          )\n    \n    @staticmethod\n    def check_leader_completeness(committed_entries, leader):\n        \"\
          \"\"Any committed entry must be in leader's log\"\"\"\n        for entry in committed_entries:\n            if entry\
          \ not in leader.log:\n                raise SafetyViolation(\n                    f'Committed entry {entry} not\
          \ in leader log'\n                )\n\n# In RaftNode, ensure we only commit from current term\ndef advance_commit_index(self):\n\
          \    for n in range(len(self.log) - 1, self.commit_index, -1):\n        # CRITICAL: Only commit entries from current\
          \ term\n        # This ensures Leader Completeness property\n        if self.log[n][0] != self.current_term:\n \
          \           continue  # Can't directly commit old term entries\n        \n        count = 1\n        for peer in\
          \ self.peers:\n            if self.match_index.get(peer, 0) >= n:\n                count += 1\n        \n      \
          \  if count > (len(self.peers) + 1) // 2:\n            self.commit_index = n\n            break"
      pitfalls:
      - Committing old term entries
      - Figure 8 scenario
      - Network partition handling
      concepts:
      - Safety properties
      - Formal verification
      - Invariants
      estimated_hours: 15-25
    - id: 4
      name: Cluster Membership Changes
      description: Implement dynamic cluster membership changes.
      acceptance_criteria:
      - Joint consensus
      - Single-server changes
      - Configuration log entries
      - Leader transfer
      hints:
        level1: 'Single-server changes are simpler: add/remove one at a time.'
        level2: New server must catch up before counting for quorum. Leader steps down if removed.
        level3: "class RaftNode:\n    def __init__(self, ...):\n        # ...\n        self.config = set(peers) | {self.id}\n\
          \        self.pending_config = None\n    \n    async def add_server(self, new_server):\n        if self.state !=\
          \ State.LEADER:\n            raise NotLeaderError()\n        \n        if self.pending_config:\n            raise\
          \ ConfigChangeInProgress()\n        \n        # First, catch up new server\n        await self.catch_up_server(new_server)\n\
          \        \n        # Append config change to log\n        new_config = self.config | {new_server}\n        entry\
          \ = (self.current_term, ConfigChange(new_config))\n        self.log.append(entry)\n        \n        self.pending_config\
          \ = new_config\n        self.peers.add(new_server)\n        self.next_index[new_server] = len(self.log)\n      \
          \  self.match_index[new_server] = 0\n        \n        # Replicate and wait for commit\n        await self.replicate_and_commit()\n\
          \        \n        self.config = new_config\n        self.pending_config = None\n    \n    async def remove_server(self,\
          \ server):\n        if self.state != State.LEADER:\n            raise NotLeaderError()\n        \n        if server\
          \ not in self.config:\n            raise ServerNotInConfig()\n        \n        new_config = self.config - {server}\n\
          \        entry = (self.current_term, ConfigChange(new_config))\n        self.log.append(entry)\n        \n     \
          \   self.pending_config = new_config\n        \n        await self.replicate_and_commit()\n        \n        self.config\
          \ = new_config\n        self.pending_config = None\n        \n        if server == self.id:\n            # Leader\
          \ is being removed\n            self.state = State.FOLLOWER\n            # Transfer leadership\n            await\
          \ self.transfer_leadership()\n    \n    async def catch_up_server(self, server, timeout=30):\n        \"\"\"Replicate\
          \ log to new server until caught up\"\"\"\n        start = time.time()\n        while time.time() - start < timeout:\n\
          \            await self.send_append_entries(server)\n            if self.match_index.get(server, 0) >= len(self.log)\
          \ - 1:\n                return\n            await asyncio.sleep(0.1)\n        raise CatchUpTimeout()"
      pitfalls:
      - Disjoint majorities
      - Leader in transition
      - Stuck configurations
      concepts:
      - Membership changes
      - Joint consensus
      - Availability during changes
      estimated_hours: 18-32
  build-raytracer:
    id: build-raytracer
    name: Build Your Own Ray Tracer
    description: Build a path tracing renderer following Ray Tracing in One Weekend. Renders photorealistic images with reflections,
      refractions, and soft shadows.
    difficulty: advanced
    estimated_hours: 20-40
    prerequisites:
    - Linear algebra (vectors, matrices)
    - Basic geometry
    - Some physics (light, optics)
    languages:
      recommended:
      - C++
      - Rust
      - Go
      also_possible:
      - Python
      - JavaScript
    resources:
    - name: Ray Tracing in One Weekend
      url: https://raytracing.github.io/books/RayTracingInOneWeekend.html
      type: book
    - name: 'Ray Tracing: The Next Week'
      url: https://raytracing.github.io/books/RayTracingTheNextWeek.html
      type: book
    - name: Physically Based Rendering (PBRT)
      url: https://pbr-book.org/
      type: reference
    milestones:
    - id: 1
      name: Output an Image
      description: Generate a simple PPM image with gradient colors.
      acceptance_criteria:
      - Outputs valid PPM image file
      - Shows color gradient
      - Correct image dimensions
      - Opens in image viewer
      hints:
        level1: 'PPM format: P3 header, width height, max color, then R G B values.'
        level2: Iterate y from top to bottom, x from left to right.
        level3: "# PPM image output - simple format for raytracing\nclass Color:\n    def __init__(self, r, g, b):\n     \
          \   self.r = r\n        self.g = g\n        self.b = b\n\n    def __add__(self, other):\n        return Color(self.r\
          \ + other.r, self.g + other.g, self.b + other.b)\n\n    def __mul__(self, scalar):\n        return Color(self.r\
          \ * scalar, self.g * scalar, self.b * scalar)\n\n    def clamp(self):\n        return Color(\n            max(0.0,\
          \ min(1.0, self.r)),\n            max(0.0, min(1.0, self.g)),\n            max(0.0, min(1.0, self.b))\n        )\n\
          \n    def to_ppm(self, samples_per_pixel=1):\n        scale = 1.0 / samples_per_pixel\n        r = int(256 * max(0.0,\
          \ min(0.999, self.r * scale)))\n        g = int(256 * max(0.0, min(0.999, self.g * scale)))\n        b = int(256\
          \ * max(0.0, min(0.999, self.b * scale)))\n        return f\"{r} {g} {b}\"\n\ndef write_ppm(filename, width, height,\
          \ pixels):\n    with open(filename, 'w') as f:\n        f.write(f\"P3\\n{width} {height}\\n255\\n\")\n        for\
          \ row in pixels:\n            for color in row:\n                f.write(color.to_ppm() + \"\\n\")\n\n# Example:\
          \ gradient image\nwidth, height = 256, 256\npixels = []\nfor j in range(height):\n    row = []\n    for i in range(width):\n\
          \        r = i / (width - 1)\n        g = (height - 1 - j) / (height - 1)\n        b = 0.25\n        row.append(Color(r,\
          \ g, b))\n    pixels.append(row)\nwrite_ppm(\"gradient.ppm\", width, height, pixels)"
      concepts:
      - Image file formats
      - Color representation
      - Coordinate systems
      estimated_hours: '1'
      pitfalls:
      - RGB values must be clamped to 0-255 range
      - Forgetting newline between pixel values in PPM
      - Integer overflow when color values exceed 1.0
      - 'Y-axis direction: most formats have origin at top-left, not bottom-left'
    - id: 2
      name: Ray Class and Background
      description: Define ray class and render background gradient.
      acceptance_criteria:
      - Ray has origin and direction
      - point_at_t(t) returns position along ray
      - Background gradient based on ray direction
      - Camera shoots rays through pixel centers
      hints:
        level1: 'Ray: P(t) = origin + t * direction. t=0 at origin, t=1 at origin+direction.'
        level2: 'Background: lerp between white and blue based on y component of unit direction.'
        level3: "Color ray_color(Ray& r) {\n  Vec3 unit_dir = normalize(r.direction);\n  float t = 0.5 * (unit_dir.y + 1.0);\n\
          \  return (1-t)*white + t*blue;\n}"
      concepts:
      - Ray representation
      - Linear interpolation (lerp)
      - Camera model basics
      estimated_hours: 1-2
      pitfalls:
      - Rays with zero-length direction vectors cause NaN
      - Forgetting to normalize direction vector affects calculations
      - Using int instead of float for coordinates loses precision
      - Background color calculation sensitive to ray direction normalization
    - id: 3
      name: Sphere Intersection
      description: Add a sphere and test ray-sphere intersection.
      acceptance_criteria:
      - Sphere defined by center and radius
      - Ray-sphere intersection using quadratic formula
      - Returns closest positive hit
      - Sphere visible in rendered image
      hints:
        level1: 'Sphere equation: |P - C|^2 = r^2. Substitute ray equation, solve quadratic.'
        level2: at^2 + bt + c = 0 where a=d.d, b=2*d.(o-c), c=(o-c).(o-c)-r^2
        level3: 'discriminant = b*b - 4*a*c

          if (discriminant < 0) return false;

          t = (-b - sqrt(discriminant)) / (2*a);

          if (t < 0) t = (-b + sqrt(discriminant)) / (2*a);'
      pitfalls:
      - Choosing correct root (closest positive)
      - Numeric precision issues
      - Sphere behind camera
      concepts:
      - Ray-sphere intersection
      - Quadratic formula
      - Hit detection
      estimated_hours: 1-2
    - id: 4
      name: Surface Normals and Multiple Objects
      description: Compute normals and support multiple objects with closest hit.
      acceptance_criteria:
      - Normal at hit point computed correctly
      - Normals visualized as colors
      - Multiple spheres in scene
      - Finds closest intersection
      hints:
        level1: 'Normal at hit point on sphere: (hit_point - center) / radius'
        level2: Hittable interface with hit() method. Scene is list of hittables.
        level3: "# Hit detection with surface normals\nimport math\nfrom dataclasses import dataclass\n\n@dataclass\nclass\
          \ Vec3:\n    x: float\n    y: float\n    z: float\n\n    def __neg__(self): return Vec3(-self.x, -self.y, -self.z)\n\
          \    def __add__(self, o): return Vec3(self.x+o.x, self.y+o.y, self.z+o.z)\n    def __sub__(self, o): return Vec3(self.x-o.x,\
          \ self.y-o.y, self.z-o.z)\n    def __mul__(self, t): return Vec3(self.x*t, self.y*t, self.z*t)\n    def dot(self,\
          \ o): return self.x*o.x + self.y*o.y + self.z*o.z\n    def length(self): return math.sqrt(self.dot(self))\n    def\
          \ unit(self): return self * (1.0 / self.length())\n\n@dataclass\nclass HitRecord:\n    p: Vec3          # Hit point\n\
          \    normal: Vec3     # Surface normal (always outward)\n    t: float         # Ray parameter\n    front_face: bool\
          \ # Are we hitting front or back?\n\nclass Sphere:\n    def __init__(self, center, radius, material):\n        self.center\
          \ = center\n        self.radius = radius\n        self.material = material\n\n    def hit(self, ray, t_min, t_max):\n\
          \        oc = ray.origin - self.center\n        a = ray.direction.dot(ray.direction)\n        half_b = oc.dot(ray.direction)\n\
          \        c = oc.dot(oc) - self.radius * self.radius\n        discriminant = half_b*half_b - a*c\n\n        if discriminant\
          \ < 0:\n            return None\n\n        sqrtd = math.sqrt(discriminant)\n        root = (-half_b - sqrtd) / a\n\
          \        if root < t_min or root > t_max:\n            root = (-half_b + sqrtd) / a\n            if root < t_min\
          \ or root > t_max:\n                return None\n\n        p = ray.at(root)\n        outward_normal = (p - self.center)\
          \ * (1.0/self.radius)\n        front_face = ray.direction.dot(outward_normal) < 0\n        normal = outward_normal\
          \ if front_face else -outward_normal\n\n        return HitRecord(p, normal, root, front_face)\n\nclass HittableList:\n\
          \    def __init__(self):\n        self.objects = []\n\n    def hit(self, ray, t_min, t_max):\n        closest =\
          \ None\n        closest_t = t_max\n        for obj in self.objects:\n            rec = obj.hit(ray, t_min, closest_t)\n\
          \            if rec:\n                closest = rec\n                closest_t = rec.t\n        return closest"
      concepts:
      - Surface normals
      - Object-oriented design
      - Closest hit algorithm
      estimated_hours: 2-3
      pitfalls:
      - Normal must point outward from surface (flip if inside)
      - t_min should be small positive (0.001) not zero to avoid self-intersection
      - 'Floating point precision: comparing t == 0 fails'
      - Forgetting to return closest hit, not first hit
    - id: 5
      name: Antialiasing
      description: Add antialiasing by shooting multiple rays per pixel.
      acceptance_criteria:
      - Multiple samples per pixel
      - Random offset within pixel
      - Average color of all samples
      - Smooth edges on spheres
      hints:
        level1: For each pixel, shoot N rays with random offset. Average results.
        level2: 'Offset: (x + random()) / width instead of x / width'
        level3: "for (int s = 0; s < samples_per_pixel; s++) {\n  float u = (x + random_float()) / (width - 1);\n  float v\
          \ = (y + random_float()) / (height - 1);\n  color += ray_color(camera.get_ray(u, v));\n}\ncolor /= samples_per_pixel;"
      concepts:
      - Antialiasing
      - Monte Carlo sampling
      - Random number generation
      estimated_hours: 1-2
      pitfalls:
      - Not using random offsets within pixel causes visible patterns
      - Too few samples causes noisy images
      - Forgetting to average samples produces overly bright images
      - Random number generator state affects reproducibility
    - id: 6
      name: Diffuse Materials
      description: Implement Lambertian (diffuse) material.
      acceptance_criteria:
      - Scattered ray goes in random hemisphere direction
      - Color attenuates with each bounce
      - Maximum bounce depth
      - Soft shadows from diffuse surfaces
      hints:
        level1: On hit, spawn new ray in random direction in hemisphere around normal.
        level2: 'Lambertian: direction = normal + random_unit_vector()'
        level3: "Color ray_color(Ray& r, int depth) {\n  if (depth <= 0) return black;\n  if (world.hit(r, hit_record)) {\n\
          \    Vec3 target = hit_point + normal + random_unit_vector();\n    return 0.5 * ray_color(Ray(hit_point, target\
          \ - hit_point), depth-1);\n  }\n  return background;\n}"
      pitfalls:
      - Infinite recursion without depth limit
      - Shadow acne (self-intersection)
      - Correct hemisphere sampling
      concepts:
      - Lambertian reflection
      - Recursive ray tracing
      - Material scattering
      estimated_hours: 2-3
    - id: 7
      name: Metal and Reflections
      description: Implement metallic (reflective) materials with fuzz.
      acceptance_criteria:
      - Perfect reflections (mirror)
      - Fuzzy reflections with adjustable roughness
      - 'Reflection formula: r = d - 2*(d.n)*n'
      - Material assigned per object
      hints:
        level1: Reflect direction around normal. Incident = incoming ray direction.
        level2: 'Fuzz: add random vector scaled by fuzziness to reflected direction.'
        level3: "Vec3 reflect(Vec3& v, Vec3& n) {\n  return v - 2*dot(v, n)*n;\n}\nVec3 scattered = reflect(ray.dir, normal)\
          \ + fuzz*random_in_unit_sphere();"
      concepts:
      - Specular reflection
      - Roughness/fuzz
      - Material system
      estimated_hours: 2-3
      pitfalls:
      - Reflected ray pointing into surface (dot product check)
      - Infinite recursion without max depth limit
      - Fuzz parameter > 1 causes rays to pass through surface
      - Not attenuating color with each bounce produces unrealistic brightness
    - id: 8
      name: Dielectrics (Glass)
      description: Implement glass with refraction and Fresnel effects.
      acceptance_criteria:
      - Snell's law for refraction
      - Total internal reflection at critical angle
      - Schlick approximation for Fresnel
      - Realistic glass appearance
      hints:
        level1: 'Snell''s law: n1*sin(theta1) = n2*sin(theta2). Glass IOR ~ 1.5'
        level2: Total internal reflection when sin(theta2) > 1. Just reflect.
        level3: 'Schlick approximation for reflectance:

          r0 = ((n1-n2)/(n1+n2))^2

          reflectance = r0 + (1-r0)*(1-cos(theta))^5'
      pitfalls:
      - Getting the IOR ratio right
      - Hollow glass spheres (negative radius trick)
      - Total internal reflection check
      concepts:
      - Refraction (Snell's law)
      - Total internal reflection
      - Fresnel equations
      estimated_hours: 3-4
    - id: 9
      name: Positionable Camera
      description: Implement camera with position, look-at, and field of view.
      acceptance_criteria:
      - Camera positioned anywhere in scene
      - Points at specified target
      - Adjustable field of view
      - Up vector for camera orientation
      hints:
        level1: 'Camera basis vectors: w (back), u (right), v (up).'
        level2: vfov = vertical field of view. viewport_height = 2 * tan(vfov/2)
        level3: 'w = normalize(lookfrom - lookat);

          u = normalize(cross(vup, w));

          v = cross(w, u);

          horizontal = viewport_width * u;

          vertical = viewport_height * v;

          lower_left = origin - horizontal/2 - vertical/2 - w;'
      concepts:
      - Camera coordinate system
      - Field of view
      - Look-at transformation
      estimated_hours: 2-3
      pitfalls:
      - Field of view in radians vs degrees confusion
      - Up vector parallel to look direction crashes
      - Aspect ratio calculation wrong leads to stretched images
      - Viewport height/width off by one pixel
    - id: 10
      name: Depth of Field
      description: Add defocus blur (depth of field) effect.
      acceptance_criteria:
      - Adjustable aperture size
      - Focus distance setting
      - Objects at focus distance are sharp
      - Objects closer/farther are blurred
      hints:
        level1: Random origin on lens disk, ray still goes through focus point.
        level2: Larger aperture = more blur. Aperture = 0 = pinhole camera.
        level3: 'Vec3 rd = lens_radius * random_in_unit_disk();

          Vec3 offset = u * rd.x + v * rd.y;

          return Ray(origin + offset, lower_left + s*horizontal + t*vertical - origin - offset);'
      concepts:
      - Thin lens model
      - Depth of field
      - Aperture and focus
      estimated_hours: 2-3
      pitfalls:
      - Aperture too large causes everything to be blurry
      - Focus distance wrong makes subject blurry
      - Random disk sampling bias causes visible artifacts
      - Thin lens approximation breaks at extreme apertures
  build-react:
    id: build-react
    name: Build Your Own React
    description: Build a React-like library with Virtual DOM, reconciliation, hooks, and fiber architecture.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - DOM manipulation
    - JavaScript advanced concepts
    - Tree data structures
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible: []
    resources:
    - type: article
      name: Build your own React
      url: https://pomb.us/build-your-own-react/
    - type: talk
      name: React Fiber Architecture
      url: https://github.com/acdlite/react-fiber-architecture
    milestones:
    - id: 1
      name: Virtual DOM
      description: Create virtual DOM representation and rendering.
      acceptance_criteria:
      - createElement function
      - Virtual node structure
      - Render to real DOM
      - Handle text nodes
      hints:
        level1: 'VNode is just an object: { type, props, children }. Recursively create DOM elements.'
        level2: Handle primitives (string/number) as text nodes. props.children can be array or single element.
        level3: "// Virtual DOM element\nfunction createElement(type, props, ...children) {\n    return {\n        type,\n\
          \        props: {\n            ...props,\n            children: children.flat().map(child =>\n                typeof\
          \ child === 'object' ? child : createTextElement(child)\n            )\n        }\n    };\n}\n\nfunction createTextElement(text)\
          \ {\n    return {\n        type: 'TEXT_ELEMENT',\n        props: {\n            nodeValue: text,\n            children:\
          \ []\n        }\n    };\n}\n\n// Render vnode to DOM\nfunction render(element, container) {\n    const dom = element.type\
          \ === 'TEXT_ELEMENT'\n        ? document.createTextNode('')\n        : document.createElement(element.type);\n \
          \   \n    // Set properties\n    Object.keys(element.props)\n        .filter(key => key !== 'children')\n      \
          \  .forEach(name => {\n            if (name.startsWith('on')) {\n                const eventType = name.toLowerCase().substring(2);\n\
          \                dom.addEventListener(eventType, element.props[name]);\n            } else {\n                dom[name]\
          \ = element.props[name];\n            }\n        });\n    \n    // Render children\n    element.props.children.forEach(child\
          \ => render(child, dom));\n    \n    container.appendChild(dom);\n}"
      pitfalls:
      - Handling null/undefined children
      - Event listener naming
      - SVG namespace
      concepts:
      - Virtual DOM
      - Declarative UI
      - Tree representation
      estimated_hours: 8-12
    - id: 2
      name: Reconciliation (Diffing)
      description: Implement efficient DOM updates through reconciliation.
      acceptance_criteria:
      - Diff old and new trees
      - Update only changed nodes
      - Handle additions/deletions
      - Key-based reconciliation
      hints:
        level1: Compare type first. Same type = update props. Different = replace entire subtree.
        level2: For lists, use keys to match elements. Reorder instead of recreate.
        level3: "function reconcile(dom, oldVNode, newVNode) {\n    // Addition\n    if (!oldVNode) {\n        return render(newVNode,\
          \ dom);\n    }\n    \n    // Deletion\n    if (!newVNode) {\n        dom.remove();\n        return null;\n    }\n\
          \    \n    // Replace entire subtree if type changed\n    if (oldVNode.type !== newVNode.type) {\n        const\
          \ newDom = createDom(newVNode);\n        dom.replaceWith(newDom);\n        return newDom;\n    }\n    \n    // Same\
          \ type - update props\n    updateProps(dom, oldVNode.props, newVNode.props);\n    \n    // Reconcile children\n\
          \    reconcileChildren(dom, oldVNode.props.children, newVNode.props.children);\n    \n    return dom;\n}\n\nfunction\
          \ reconcileChildren(dom, oldChildren, newChildren) {\n    const oldKeyed = new Map();\n    const newKeyed = new\
          \ Map();\n    \n    // Index by key\n    oldChildren.forEach((child, i) => {\n        const key = child.props?.key\
          \ ?? i;\n        oldKeyed.set(key, { vnode: child, dom: dom.childNodes[i] });\n    });\n    \n    newChildren.forEach((child,\
          \ i) => {\n        const key = child.props?.key ?? i;\n        newKeyed.set(key, child);\n    });\n    \n    //\
          \ Remove deleted\n    for (const [key, { dom: childDom }] of oldKeyed) {\n        if (!newKeyed.has(key)) {\n  \
          \          childDom.remove();\n        }\n    }\n    \n    // Update or add\n    newChildren.forEach((newChild,\
          \ i) => {\n        const key = newChild.props?.key ?? i;\n        const old = oldKeyed.get(key);\n        \n   \
          \     if (old) {\n            reconcile(old.dom, old.vnode, newChild);\n        } else {\n            render(newChild,\
          \ dom);\n        }\n    });\n}"
      pitfalls:
      - Key stability
      - Index as key problems
      - DOM node reference tracking
      concepts:
      - Tree diffing
      - Minimal updates
      - Keyed reconciliation
      estimated_hours: 12-18
    - id: 3
      name: Fiber Architecture
      description: Implement interruptible rendering with fiber nodes.
      acceptance_criteria:
      - Fiber node structure
      - Work loop with requestIdleCallback
      - Commit phase separation
      - Prioritization
      hints:
        level1: Fiber = work unit with parent/child/sibling pointers. Process one fiber, then yield.
        level2: Separate render (can interrupt) from commit (must complete). Use double buffering.
        level3: "// Fiber node\nlet nextUnitOfWork = null;\nlet wipRoot = null;  // Work in progress root\nlet currentRoot\
          \ = null;  // Committed root\nlet deletions = [];\n\nfunction createFiber(element, parent) {\n    return {\n   \
          \     type: element.type,\n        props: element.props,\n        parent,\n        dom: null,\n        child: null,\n\
          \        sibling: null,\n        alternate: null,  // Previous fiber\n        effectTag: null   // PLACEMENT, UPDATE,\
          \ DELETION\n    };\n}\n\nfunction workLoop(deadline) {\n    let shouldYield = false;\n    \n    while (nextUnitOfWork\
          \ && !shouldYield) {\n        nextUnitOfWork = performUnitOfWork(nextUnitOfWork);\n        shouldYield = deadline.timeRemaining()\
          \ < 1;\n    }\n    \n    // Commit when all work is done\n    if (!nextUnitOfWork && wipRoot) {\n        commitRoot();\n\
          \    }\n    \n    requestIdleCallback(workLoop);\n}\n\nrequestIdleCallback(workLoop);\n\nfunction performUnitOfWork(fiber)\
          \ {\n    // Create DOM node\n    if (!fiber.dom) {\n        fiber.dom = createDom(fiber);\n    }\n    \n    // Create\
          \ fibers for children\n    reconcileChildren(fiber, fiber.props.children);\n    \n    // Return next unit of work\
          \ (depth-first)\n    if (fiber.child) return fiber.child;\n    \n    let nextFiber = fiber;\n    while (nextFiber)\
          \ {\n        if (nextFiber.sibling) return nextFiber.sibling;\n        nextFiber = nextFiber.parent;\n    }\n  \
          \  return null;\n}\n\nfunction commitRoot() {\n    deletions.forEach(commitWork);\n    commitWork(wipRoot.child);\n\
          \    currentRoot = wipRoot;\n    wipRoot = null;\n}\n\nfunction commitWork(fiber) {\n    if (!fiber) return;\n \
          \   \n    const parentDom = fiber.parent.dom;\n    \n    if (fiber.effectTag === 'PLACEMENT' && fiber.dom) {\n \
          \       parentDom.appendChild(fiber.dom);\n    } else if (fiber.effectTag === 'DELETION') {\n        parentDom.removeChild(fiber.dom);\n\
          \    } else if (fiber.effectTag === 'UPDATE' && fiber.dom) {\n        updateDom(fiber.dom, fiber.alternate.props,\
          \ fiber.props);\n    }\n    \n    commitWork(fiber.child);\n    commitWork(fiber.sibling);\n}"
      pitfalls:
      - Effect ordering
      - Interrupted state
      - Memory leaks in alternate
      concepts:
      - Cooperative scheduling
      - Incremental rendering
      - Work units
      estimated_hours: 15-25
    - id: 4
      name: Hooks
      description: Implement useState and useEffect hooks.
      acceptance_criteria:
      - useState for local state
      - useEffect for side effects
      - Hook ordering rules
      - Cleanup functions
      hints:
        level1: Track hooks in array per fiber. Index determines which hook. Reset index each render.
        level2: useEffect runs after commit. Compare deps to decide if effect runs.
        level3: "let wipFiber = null;\nlet hookIndex = null;\n\nfunction useState(initial) {\n    const oldHook = wipFiber.alternate?.hooks?.[hookIndex];\n\
          \    \n    const hook = {\n        state: oldHook ? oldHook.state : initial,\n        queue: []\n    };\n    \n\
          \    // Process queued setState calls\n    const actions = oldHook ? oldHook.queue : [];\n    actions.forEach(action\
          \ => {\n        hook.state = typeof action === 'function' ? action(hook.state) : action;\n    });\n    \n    const\
          \ setState = action => {\n        hook.queue.push(action);\n        // Trigger re-render\n        wipRoot = {\n\
          \            dom: currentRoot.dom,\n            props: currentRoot.props,\n            alternate: currentRoot\n\
          \        };\n        nextUnitOfWork = wipRoot;\n        deletions = [];\n    };\n    \n    wipFiber.hooks.push(hook);\n\
          \    hookIndex++;\n    return [hook.state, setState];\n}\n\nfunction useEffect(callback, deps) {\n    const oldHook\
          \ = wipFiber.alternate?.hooks?.[hookIndex];\n    \n    const hasChanged = !oldHook || \n        !deps ||\n     \
          \   deps.some((dep, i) => dep !== oldHook.deps[i]);\n    \n    const hook = {\n        deps,\n        cleanup: oldHook?.cleanup,\n\
          \        effect: hasChanged ? callback : null\n    };\n    \n    wipFiber.hooks.push(hook);\n    hookIndex++;\n\
          }\n\n// In commit phase\nfunction commitEffects(fiber) {\n    if (!fiber) return;\n    \n    fiber.hooks?.forEach(hook\
          \ => {\n        if (hook.cleanup) hook.cleanup();\n        if (hook.effect) {\n            hook.cleanup = hook.effect();\n\
          \        }\n    });\n    \n    commitEffects(fiber.child);\n    commitEffects(fiber.sibling);\n}\n\n// Function\
          \ component handling\nfunction updateFunctionComponent(fiber) {\n    wipFiber = fiber;\n    hookIndex = 0;\n   \
          \ wipFiber.hooks = [];\n    \n    const children = [fiber.type(fiber.props)];\n    reconcileChildren(fiber, children);\n\
          }"
      pitfalls:
      - Conditional hooks
      - Stale closures
      - Effect cleanup timing
      concepts:
      - Hooks pattern
      - State management
      - Side effects
      estimated_hours: 25-45
  build-redis:
    id: build-redis
    name: Build Your Own Redis
    description: Build an in-memory data structure store that implements the Redis protocol. You'll learn about TCP servers,
      protocol parsing, data structures, and persistence.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - TCP/IP networking basics
    - Hash tables and data structures
    - Concurrency fundamentals
    - File I/O
    languages:
      recommended:
      - Go
      - Rust
      - C
      also_possible:
      - Python
      - Java
      - TypeScript
    resources:
    - name: CodeCrafters Redis Challenge
      url: https://app.codecrafters.io/courses/redis/overview
      type: interactive
    - name: Build Your Own Redis (Book)
      url: https://build-your-own.org/redis/
      type: book
    - name: Redis Protocol Specification (RESP)
      url: https://redis.io/docs/reference/protocol-spec/
      type: documentation
    milestones:
    - id: 1
      name: TCP Server + RESP Protocol
      description: Create a TCP server that listens on port 6379 and responds to PING with +PONG. Implement basic RESP parsing.
      acceptance_criteria:
      - Server binds to port 6379
      - Accepts TCP connections
      - Responds to PING with +PONG\r\n
      - Handles multiple sequential commands
      - Clean shutdown on SIGINT
      hints:
        level1: Start with a simple TCP server using your language's net library. Accept connections in a loop.
        level2: RESP Simple Strings start with '+', errors with '-'. PING expects '+PONG\r\n' response.
        level3: 'RESP format examples:

          - Simple String: +OK\r\n

          - Error: -ERR unknown command\r\n

          - Integer: :1000\r\n

          - Bulk String: $5\r\nhello\r\n

          - Array: *2\r\n$4\r\nPING\r\n$4\r\nPONG\r\n'
      pitfalls:
      - Forgetting \r\n line endings (CRLF, not just LF)
      - Not handling partial reads (TCP is a stream)
      - Blocking the main thread on single client
      concepts:
      - TCP sockets and server lifecycle
      - RESP protocol encoding/decoding
      - Basic network I/O
      estimated_hours: 3-4
    - id: 2
      name: GET/SET/DEL Commands
      description: Implement basic key-value operations. Store data in an in-memory hash map.
      acceptance_criteria:
      - SET key value stores the key-value pair
      - GET key returns the value or nil
      - DEL key removes the key and returns 1 (or 0)
      - Keys are case-sensitive
      - Values can be any string (including binary)
      hints:
        level1: Use a hash map (dict/map) to store key-value pairs.
        level2: Parse RESP arrays for commands. SET comes as *3\r\n$3\r\nSET\r\n$3\r\nkey\r\n$5\r\nvalue\r\n
        level3: "# Redis-like command implementation\nclass RedisStore:\n    def __init__(self):\n        self.data = {}\n\
          \        self.expiry = {}  # key -> expiry timestamp\n\n    def set(self, key, value, ex=None, px=None, nx=False,\
          \ xx=False):\n        # NX: only set if key doesn't exist\n        if nx and key in self.data:\n            return\
          \ None\n        # XX: only set if key exists\n        if xx and key not in self.data:\n            return None\n\
          \n        self.data[key] = value\n\n        if ex:  # Expire in seconds\n            self.expiry[key] = time.time()\
          \ + ex\n        elif px:  # Expire in milliseconds\n            self.expiry[key] = time.time() + px / 1000.0\n \
          \       elif key in self.expiry:\n            del self.expiry[key]\n\n        return \"OK\"\n\n    def get(self,\
          \ key):\n        if not self._check_expiry(key):\n            return None\n        return self.data.get(key)\n\n\
          \    def delete(self, *keys):\n        count = 0\n        for key in keys:\n            if key in self.data:\n \
          \               del self.data[key]\n                self.expiry.pop(key, None)\n                count += 1\n   \
          \     return count\n\n    def _check_expiry(self, key):\n        if key in self.expiry:\n            if time.time()\
          \ >= self.expiry[key]:\n                del self.data[key]\n                del self.expiry[key]\n             \
          \   return False\n        return key in self.data\n\n# RESP protocol parser\ndef parse_resp(data):\n    if data[0:1]\
          \ == b'+':  # Simple string\n        return data[1:data.index(b'\\r\\n')].decode()\n    elif data[0:1] == b'-':\
          \  # Error\n        return Exception(data[1:data.index(b'\\r\\n')].decode())\n    elif data[0:1] == b':':  # Integer\n\
          \        return int(data[1:data.index(b'\\r\\n')])\n    elif data[0:1] == b'$':  # Bulk string\n        length =\
          \ int(data[1:data.index(b'\\r\\n')])\n        if length == -1:\n            return None\n        start = data.index(b'\\\
          r\\n') + 2\n        return data[start:start+length]\n    elif data[0:1] == b'*':  # Array\n        count = int(data[1:data.index(b'\\\
          r\\n')])\n        # Recursively parse elements...\n        pass"
      pitfalls:
      - Not handling binary-safe strings
      - Forgetting null response for non-existent keys
      - Case sensitivity issues
      concepts:
      - Hash table implementation
      - RESP array parsing
      - Command dispatch pattern
      estimated_hours: 2-3
    - id: 3
      name: Expiration (TTL)
      description: Add key expiration support. Keys can be set with PX (milliseconds) or EX (seconds) options.
      acceptance_criteria:
      - SET key value PX milliseconds
      - SET key value EX seconds
      - GET returns nil for expired keys
      - TTL key returns remaining time
      - Expired keys are cleaned up
      hints:
        level1: Store expiration timestamp alongside each value.
        level2: Check expiration on GET (lazy deletion). Optionally run background cleanup.
        level3: 'Two deletion strategies:

          1. Lazy: Check on access, delete if expired

          2. Active: Background goroutine periodically scans

          Redis uses both.'
      pitfalls:
      - Using relative time instead of absolute timestamp
      - Not handling clock drift
      - Memory leaks from never-accessed expired keys
      concepts:
      - TTL and expiration strategies
      - Lazy vs active deletion
      - Time handling and precision
      estimated_hours: 2-3
    - id: 4
      name: Data Structures (List, Set, Hash)
      description: Implement Redis data structure commands beyond simple strings.
      acceptance_criteria:
      - LPUSH/RPUSH adds elements to list
      - LPOP/RPOP removes and returns elements
      - LRANGE returns range
      - SADD/SMEMBERS for sets
      - HSET/HGET for hashes
      hints:
        level1: 'Use native data structures: arrays for lists, sets for sets, nested maps for hashes.'
        level2: Lists in Redis are doubly-linked for O(1) push/pop at both ends.
        level3: "# Redis data structure implementations\nclass RedisList:\n    def __init__(self):\n        self.items = []\n\
          \n    def lpush(self, *values):\n        for v in reversed(values):\n            self.items.insert(0, v)\n     \
          \   return len(self.items)\n\n    def rpush(self, *values):\n        self.items.extend(values)\n        return len(self.items)\n\
          \n    def lpop(self, count=1):\n        result = self.items[:count]\n        self.items = self.items[count:]\n \
          \       return result[0] if count == 1 else result\n\n    def lrange(self, start, stop):\n        # Redis uses inclusive\
          \ stop, Python uses exclusive\n        if stop < 0:\n            stop = len(self.items) + stop + 1\n        else:\n\
          \            stop += 1\n        return self.items[start:stop]\n\nclass RedisSet:\n    def __init__(self):\n    \
          \    self.members = set()\n\n    def sadd(self, *members):\n        added = sum(1 for m in members if m not in self.members)\n\
          \        self.members.update(members)\n        return added\n\n    def srem(self, *members):\n        removed =\
          \ sum(1 for m in members if m in self.members)\n        self.members -= set(members)\n        return removed\n\n\
          \    def sismember(self, member):\n        return 1 if member in self.members else 0\n\n    def smembers(self):\n\
          \        return list(self.members)\n\nclass RedisHash:\n    def __init__(self):\n        self.fields = {}\n\n  \
          \  def hset(self, field, value):\n        is_new = field not in self.fields\n        self.fields[field] = value\n\
          \        return 1 if is_new else 0\n\n    def hget(self, field):\n        return self.fields.get(field)\n\n    def\
          \ hgetall(self):\n        result = []\n        for k, v in self.fields.items():\n            result.extend([k, v])\n\
          \        return result\n\n    def hincrby(self, field, increment):\n        self.fields[field] = int(self.fields.get(field,\
          \ 0)) + increment\n        return self.fields[field]"
      pitfalls:
      - Using array for list (O(n) insert at head)
      - Not handling type errors
      - LRANGE negative indices
      concepts:
      - Linked lists vs arrays
      - Set data structure
      - Skip lists (for sorted sets)
      estimated_hours: 4-6
    - id: 5
      name: Persistence (RDB Snapshots)
      description: Implement point-in-time snapshots using RDB format. SAVE blocks, BGSAVE forks.
      acceptance_criteria:
      - SAVE command creates RDB file synchronously
      - BGSAVE forks child process
      - Server loads RDB on startup
      - RDB file format is binary with checksums
      hints:
        level1: Start with custom binary format. Later match Redis RDB format.
        level2: BGSAVE uses fork() - child inherits memory snapshot via copy-on-write.
        level3: 'RDB file structure:

          - Magic: "REDIS" + version

          - Database selector

          - Key-value pairs with type byte

          - EOF marker

          - CRC64 checksum'
      pitfalls:
      - Blocking main thread during save
      - Not using fork() for BGSAVE
      - Corrupted RDB from incomplete writes
      concepts:
      - Binary serialization
      - Process forking and copy-on-write
      - Atomic file operations
      estimated_hours: 5-8
    - id: 6
      name: Persistence (AOF)
      description: Implement Append-Only File logging for durability.
      acceptance_criteria:
      - All write commands appended to AOF file
      - AOF file contains RESP-formatted commands
      - Server replays AOF on startup
      - BGREWRITEAOF compacts the file
      - Configurable fsync policy
      hints:
        level1: Simply append each write command to a file in RESP format.
        level2: 'BGREWRITEAOF: fork, iterate current state, write minimal commands.'
        level3: 'fsync strategies:

          - always: fsync after every write (safest, slowest)

          - everysec: fsync once per second (good balance)

          - no: let OS decide (fastest, riskier)'
      pitfalls:
      - AOF growing unbounded without rewrite
      - Concurrent writes during BGREWRITEAOF
      - Data loss from buffered writes
      concepts:
      - Write-ahead logging
      - fsync and durability guarantees
      - Log compaction
      estimated_hours: 4-6
    - id: 7
      name: Pub/Sub
      description: Implement publish/subscribe messaging pattern.
      acceptance_criteria:
      - SUBSCRIBE channel subscribes client
      - PUBLISH channel message sends to all
      - UNSUBSCRIBE removes subscription
      - Pattern subscriptions (PSUBSCRIBE) optional
      hints:
        level1: Maintain map of channel -> list of subscriber connections.
        level2: Subscribed clients enter special mode - only pub/sub commands allowed.
        level3: "# Redis Pub/Sub implementation\nimport asyncio\nfrom collections import defaultdict\n\nclass PubSubManager:\n\
          \    def __init__(self):\n        self.subscribers = defaultdict(set)  # channel -> set of clients\n        self.patterns\
          \ = defaultdict(set)      # pattern -> set of clients\n\n    def subscribe(self, client, *channels):\n        for\
          \ channel in channels:\n            self.subscribers[channel].add(client)\n            client.send_message(['subscribe',\
          \ channel, len(client.subscriptions)])\n            client.subscriptions.add(channel)\n\n    def unsubscribe(self,\
          \ client, *channels):\n        if not channels:\n            channels = list(client.subscriptions)\n        for\
          \ channel in channels:\n            self.subscribers[channel].discard(client)\n            client.subscriptions.discard(channel)\n\
          \            client.send_message(['unsubscribe', channel, len(client.subscriptions)])\n\n    def publish(self, channel,\
          \ message):\n        count = 0\n        # Direct subscribers\n        for client in self.subscribers[channel]:\n\
          \            client.send_message(['message', channel, message])\n            count += 1\n        # Pattern subscribers\n\
          \        for pattern, clients in self.patterns.items():\n            if self._match_pattern(pattern, channel):\n\
          \                for client in clients:\n                    client.send_message(['pmessage', pattern, channel,\
          \ message])\n                    count += 1\n        return count\n\n    def _match_pattern(self, pattern, channel):\n\
          \        # Simple glob matching: * matches any sequence\n        import fnmatch\n        return fnmatch.fnmatch(channel,\
          \ pattern)\n\n# Async client handling for pub/sub\nclass RedisClient:\n    def __init__(self, reader, writer):\n\
          \        self.reader = reader\n        self.writer = writer\n        self.subscriptions = set()\n        self.message_queue\
          \ = asyncio.Queue()\n\n    async def handle_pubsub_mode(self):\n        # In pub/sub mode, client can only send\
          \ SUBSCRIBE/UNSUBSCRIBE\n        while self.subscriptions:\n            msg = await self.message_queue.get()\n \
          \           self.writer.write(encode_resp_array(msg))\n            await self.writer.drain()"
      pitfalls:
      - Not blocking other commands in subscribed state
      - Memory leaks from disconnected subscribers
      - Race conditions in publish
      concepts:
      - Pub/Sub pattern
      - Observer pattern
      - Connection state management
      estimated_hours: 3-4
    - id: 8
      name: Cluster Mode (Sharding)
      description: Implement horizontal scaling with hash slot based sharding.
      acceptance_criteria:
      - 16384 hash slots distributed across nodes
      - CLUSTER SLOTS returns distribution
      - -MOVED redirect for keys on other nodes
      - Hash tags for multi-key operations
      hints:
        level1: CRC16(key) % 16384 gives the slot. Each node owns a range.
        level2: On wrong node, return -MOVED slot ip:port. Client should retry.
        level3: "# Redis Cluster sharding implementation\nimport hashlib\n\ndef crc16(data):\n    '''CRC16 XMODEM - used by\
          \ Redis Cluster'''\n    crc = 0\n    for byte in data:\n        crc ^= byte << 8\n        for _ in range(8):\n \
          \           if crc & 0x8000:\n                crc = (crc << 1) ^ 0x1021\n            else:\n                crc\
          \ <<= 1\n            crc &= 0xFFFF\n    return crc\n\ndef key_slot(key):\n    '''Calculate hash slot for a key (0-16383)'''\n\
          \    # Handle hash tags: {tag}key uses only \"tag\" for hashing\n    start = key.find(b'{')\n    if start >= 0:\n\
          \        end = key.find(b'}', start + 1)\n        if end > start + 1:\n            key = key[start + 1:end]\n  \
          \  return crc16(key) % 16384\n\nclass ClusterNode:\n    def __init__(self, node_id, host, port):\n        self.id\
          \ = node_id\n        self.host = host\n        self.port = port\n        self.slots = set()  # Set of slot numbers\
          \ this node owns\n        self.replicas = []  # Replica nodes\n\nclass Cluster:\n    def __init__(self):\n     \
          \   self.nodes = {}  # node_id -> ClusterNode\n        self.slot_map = [None] * 16384  # slot -> node_id\n\n   \
          \ def assign_slots(self, node_id, start_slot, end_slot):\n        node = self.nodes[node_id]\n        for slot in\
          \ range(start_slot, end_slot + 1):\n            self.slot_map[slot] = node_id\n            node.slots.add(slot)\n\
          \n    def get_node_for_key(self, key):\n        slot = key_slot(key if isinstance(key, bytes) else key.encode())\n\
          \        node_id = self.slot_map[slot]\n        return self.nodes[node_id]\n\n    def handle_moved(self, slot, new_node):\n\
          \        '''Handle MOVED response during resharding'''\n        return f\"MOVED {slot} {new_node.host}:{new_node.port}\"\
          \n\n# Client-side routing\nclass ClusterClient:\n    def __init__(self, startup_nodes):\n        self.cluster =\
          \ Cluster()\n        self.refresh_slots(startup_nodes)\n\n    def execute(self, *args):\n        key = args[1] \
          \ # Most commands have key as second arg\n        node = self.cluster.get_node_for_key(key)\n        try:\n    \
          \        return node.execute(*args)\n        except MovedError as e:\n            self.refresh_slots()\n       \
          \     return self.execute(*args)"
      pitfalls:
      - Not handling key migration
      - Cross-slot operations
      - Network partitions and split brain
      concepts:
      - Consistent hashing
      - Hash slots and key routing
      - Distributed systems fundamentals
      estimated_hours: 8-12
  build-regex:
    id: build-regex
    name: Build Your Own Regex Engine
    description: Build a regex engine using NFA/DFA and Thompson's construction.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - Automata theory
    - Graph algorithms
    - Recursion
    - C/Rust/Go
    languages:
      recommended:
      - C
      - Rust
      - Go
      - Python
      also_possible:
      - JavaScript
    resources:
    - type: article
      name: Regular Expression Matching
      url: https://swtch.com/~rsc/regexp/regexp1.html
    - type: book
      name: Introduction to Automata Theory
      url: https://www.amazon.com/Introduction-Automata-Theory-Languages-Computation/dp/0321455363
    milestones:
    - id: 1
      name: Regex Parser
      description: Parse regex pattern into AST.
      acceptance_criteria:
      - Literal characters
      - Alternation (|)
      - Concatenation
      - Quantifiers (*, +, ?)
      - Character classes ([a-z])
      - Escape sequences
      hints:
        level1: 'Use recursive descent. Handle operator precedence: () > * > concat > |'
        level2: Build AST nodes for each regex construct.
        level3: "## Regex Parser\n\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\n\
          class Literal:\n    char: str\n\n@dataclass\nclass Concat:\n    left: 'Node'\n    right: 'Node'\n\n@dataclass\n\
          class Alternation:\n    left: 'Node'\n    right: 'Node'\n\n@dataclass\nclass Star:\n    child: 'Node'\n\n@dataclass\n\
          class Plus:\n    child: 'Node'\n\n@dataclass\nclass Optional:\n    child: 'Node'\n\n@dataclass\nclass CharClass:\n\
          \    chars: set\n    negated: bool = False\n\nNode = Literal | Concat | Alternation | Star | Plus | Optional | CharClass\n\
          \nclass RegexParser:\n    def __init__(self, pattern: str):\n        self.pattern = pattern\n        self.pos =\
          \ 0\n    \n    def parse(self) -> Node:\n        return self.parseAlternation()\n    \n    def parseAlternation(self)\
          \ -> Node:\n        left = self.parseConcat()\n        while self.match('|'):\n            right = self.parseConcat()\n\
          \            left = Alternation(left, right)\n        return left\n    \n    def parseConcat(self) -> Node:\n  \
          \      nodes = []\n        while self.pos < len(self.pattern) and self.peek() not in '|)':\n            nodes.append(self.parseQuantified())\n\
          \        \n        if not nodes:\n            return Literal('')\n        \n        result = nodes[0]\n        for\
          \ node in nodes[1:]:\n            result = Concat(result, node)\n        return result\n    \n    def parseQuantified(self)\
          \ -> Node:\n        base = self.parseAtom()\n        \n        if self.match('*'):\n            return Star(base)\n\
          \        elif self.match('+'):\n            return Plus(base)\n        elif self.match('?'):\n            return\
          \ Optional(base)\n        \n        return base\n    \n    def parseAtom(self) -> Node:\n        if self.match('('):\n\
          \            node = self.parseAlternation()\n            self.expect(')')\n            return node\n        \n \
          \       if self.match('['):\n            return self.parseCharClass()\n        \n        if self.match('\\\\'):\n\
          \            return self.parseEscape()\n        \n        if self.match('.'):\n            return CharClass(set(),\
          \ negated=True)  # Match any\n        \n        char = self.advance()\n        return Literal(char)\n    \n    def\
          \ parseCharClass(self) -> Node:\n        negated = self.match('^')\n        chars = set()\n        \n        while\
          \ not self.match(']'):\n            c = self.advance()\n            if self.match('-') and self.peek() != ']':\n\
          \                end = self.advance()\n                for i in range(ord(c), ord(end) + 1):\n                 \
          \   chars.add(chr(i))\n            else:\n                chars.add(c)\n        \n        return CharClass(chars,\
          \ negated)\n    \n    # Helper methods\n    def peek(self) -> str:\n        return self.pattern[self.pos] if self.pos\
          \ < len(self.pattern) else ''\n    \n    def advance(self) -> str:\n        c = self.peek()\n        self.pos +=\
          \ 1\n        return c\n    \n    def match(self, c: str) -> bool:\n        if self.peek() == c:\n            self.pos\
          \ += 1\n            return True\n        return False\n```"
      pitfalls:
      - Operator precedence wrong
      - Escape handling
      - Empty alternation branches
      concepts:
      - Parsing
      - AST construction
      - Regex syntax
      estimated_hours: 8-12
    - id: 2
      name: Thompson's Construction (NFA)
      description: Convert regex AST to NFA using Thompson's construction.
      acceptance_criteria:
      - NFA state representation
      - Epsilon transitions
      - Construction for each regex operator
      - Connect sub-NFAs
      hints:
        level1: Each regex construct becomes a small NFA fragment with start/accept states.
        level2: 'Concatenation: connect first''s accept to second''s start. Alternation: split with epsilon.'
        level3: "## Thompson's Construction\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Set,\
          \ Dict, List\n\n@dataclass\nclass NFAState:\n    id: int\n    epsilon: List['NFAState'] = field(default_factory=list)\n\
          \    transitions: Dict[str, List['NFAState']] = field(default_factory=dict)\n    is_accept: bool = False\n\nclass\
          \ NFAFragment:\n    def __init__(self, start: NFAState, accept: NFAState):\n        self.start = start\n       \
          \ self.accept = accept\n\nclass NFABuilder:\n    def __init__(self):\n        self.state_count = 0\n    \n    def\
          \ new_state(self) -> NFAState:\n        state = NFAState(id=self.state_count)\n        self.state_count += 1\n \
          \       return state\n    \n    def build(self, node: Node) -> NFAFragment:\n        if isinstance(node, Literal):\n\
          \            start = self.new_state()\n            accept = self.new_state()\n            if node.char:\n      \
          \          start.transitions[node.char] = [accept]\n            else:\n                start.epsilon.append(accept)\n\
          \            return NFAFragment(start, accept)\n        \n        elif isinstance(node, Concat):\n            left\
          \ = self.build(node.left)\n            right = self.build(node.right)\n            left.accept.epsilon.append(right.start)\n\
          \            return NFAFragment(left.start, right.accept)\n        \n        elif isinstance(node, Alternation):\n\
          \            left = self.build(node.left)\n            right = self.build(node.right)\n            start = self.new_state()\n\
          \            accept = self.new_state()\n            start.epsilon.extend([left.start, right.start])\n          \
          \  left.accept.epsilon.append(accept)\n            right.accept.epsilon.append(accept)\n            return NFAFragment(start,\
          \ accept)\n        \n        elif isinstance(node, Star):\n            child = self.build(node.child)\n        \
          \    start = self.new_state()\n            accept = self.new_state()\n            start.epsilon.extend([child.start,\
          \ accept])\n            child.accept.epsilon.extend([child.start, accept])\n            return NFAFragment(start,\
          \ accept)\n        \n        elif isinstance(node, Plus):\n            child = self.build(node.child)\n        \
          \    start = self.new_state()\n            accept = self.new_state()\n            start.epsilon.append(child.start)\n\
          \            child.accept.epsilon.extend([child.start, accept])\n            return NFAFragment(start, accept)\n\
          \        \n        elif isinstance(node, Optional):\n            child = self.build(node.child)\n            start\
          \ = self.new_state()\n            accept = self.new_state()\n            start.epsilon.extend([child.start, accept])\n\
          \            child.accept.epsilon.append(accept)\n            return NFAFragment(start, accept)\n        \n    \
          \    elif isinstance(node, CharClass):\n            start = self.new_state()\n            accept = self.new_state()\n\
          \            # Add transitions for each char in class\n            for c in node.chars:\n                start.transitions.setdefault(c,\
          \ []).append(accept)\n            return NFAFragment(start, accept)\n\ndef compile_regex(pattern: str) -> NFAFragment:\n\
          \    parser = RegexParser(pattern)\n    ast = parser.parse()\n    builder = NFABuilder()\n    nfa = builder.build(ast)\n\
          \    nfa.accept.is_accept = True\n    return nfa\n```"
      pitfalls:
      - Wrong epsilon connections
      - Not marking accept state
      - Memory leaks from circular refs
      concepts:
      - NFAs
      - Thompson's construction
      - Epsilon transitions
      estimated_hours: 10-15
    - id: 3
      name: NFA Simulation
      description: Match strings using NFA simulation.
      acceptance_criteria:
      - Epsilon closure computation
      - Simultaneous state tracking
      - Match detection
      - Greedy vs non-greedy
      hints:
        level1: Track set of current states. For each input char, compute next states.
        level2: 'Epsilon closure: follow all epsilon transitions from a state set.'
        level3: "## NFA Simulation\n\n```python\ndef epsilon_closure(states: Set[NFAState]) -> Set[NFAState]:\n    \"\"\"\
          Find all states reachable via epsilon transitions.\"\"\"\n    closure = set(states)\n    stack = list(states)\n\
          \    \n    while stack:\n        state = stack.pop()\n        for next_state in state.epsilon:\n            if next_state\
          \ not in closure:\n                closure.add(next_state)\n                stack.append(next_state)\n    \n   \
          \ return closure\n\ndef move(states: Set[NFAState], char: str) -> Set[NFAState]:\n    \"\"\"Find states reachable\
          \ via the given character.\"\"\"\n    next_states = set()\n    for state in states:\n        if char in state.transitions:\n\
          \            next_states.update(state.transitions[char])\n    return next_states\n\ndef nfa_match(nfa: NFAFragment,\
          \ text: str) -> bool:\n    \"\"\"Check if NFA matches the entire text.\"\"\"\n    current = epsilon_closure({nfa.start})\n\
          \    \n    for char in text:\n        current = epsilon_closure(move(current, char))\n        if not current:\n\
          \            return False\n    \n    return any(state.is_accept for state in current)\n\ndef nfa_search(nfa: NFAFragment,\
          \ text: str) -> Optional[tuple]:\n    \"\"\"Find first match in text, return (start, end) indices.\"\"\"\n    for\
          \ start in range(len(text)):\n        current = epsilon_closure({nfa.start})\n        \n        for end in range(start,\
          \ len(text) + 1):\n            if any(state.is_accept for state in current):\n                return (start, end)\n\
          \            \n            if end < len(text):\n                current = epsilon_closure(move(current, text[end]))\n\
          \                if not current:\n                    break\n    \n    return None\n\ndef nfa_findall(nfa: NFAFragment,\
          \ text: str) -> List[str]:\n    \"\"\"Find all non-overlapping matches.\"\"\"\n    matches = []\n    pos = 0\n \
          \   \n    while pos < len(text):\n        match = nfa_search_from(nfa, text, pos)\n        if match:\n         \
          \   start, end = match\n            matches.append(text[start:end])\n            pos = end if end > pos else pos\
          \ + 1\n        else:\n            pos += 1\n    \n    return matches\n```"
      pitfalls:
      - Forgetting epsilon closure
      - Infinite loop on empty match
      - Wrong match boundaries
      concepts:
      - NFA simulation
      - State sets
      - Pattern matching
      estimated_hours: 8-12
    - id: 4
      name: DFA Conversion & Optimization
      description: Convert NFA to DFA for faster matching.
      acceptance_criteria:
      - Subset construction
      - DFA state minimization
      - Lazy DFA construction
      - Caching compiled patterns
      hints:
        level1: Each DFA state = set of NFA states. Build on demand (lazy).
        level2: Minimize DFA by merging equivalent states (Hopcroft's algorithm).
        level3: "## NFA to DFA Conversion\n\n```python\nclass DFAState:\n    def __init__(self, nfa_states: frozenset):\n\
          \        self.nfa_states = nfa_states\n        self.transitions: Dict[str, 'DFAState'] = {}\n        self.is_accept\
          \ = any(s.is_accept for s in nfa_states)\n\nclass DFA:\n    def __init__(self, nfa: NFAFragment):\n        self.start_nfa_states\
          \ = frozenset(epsilon_closure({nfa.start}))\n        self.states: Dict[frozenset, DFAState] = {}\n        self.start\
          \ = self.get_or_create_state(self.start_nfa_states)\n    \n    def get_or_create_state(self, nfa_states: frozenset)\
          \ -> DFAState:\n        if nfa_states not in self.states:\n            self.states[nfa_states] = DFAState(nfa_states)\n\
          \        return self.states[nfa_states]\n    \n    def get_transition(self, state: DFAState, char: str) -> Optional[DFAState]:\n\
          \        if char in state.transitions:\n            return state.transitions[char]\n        \n        # Lazy construction\n\
          \        next_nfa_states = frozenset(epsilon_closure(move(state.nfa_states, char)))\n        if not next_nfa_states:\n\
          \            return None\n        \n        next_state = self.get_or_create_state(next_nfa_states)\n        state.transitions[char]\
          \ = next_state\n        return next_state\n    \n    def match(self, text: str) -> bool:\n        current = self.start\n\
          \        for char in text:\n            current = self.get_transition(current, char)\n            if current is\
          \ None:\n                return False\n        return current.is_accept\n\n# DFA Minimization (Hopcroft's algorithm)\n\
          def minimize_dfa(dfa: DFA) -> DFA:\n    # Partition states into accept and non-accept\n    accept = {s for s in\
          \ dfa.states.values() if s.is_accept}\n    non_accept = {s for s in dfa.states.values() if not s.is_accept}\n  \
          \  \n    partitions = [accept, non_accept] if non_accept else [accept]\n    worklist = list(partitions)\n    \n\
          \    alphabet = set()\n    for state in dfa.states.values():\n        alphabet.update(state.transitions.keys())\n\
          \    \n    while worklist:\n        A = worklist.pop()\n        for c in alphabet:\n            # States that transition\
          \ to A on c\n            X = {s for s in dfa.states.values() \n                 if s.transitions.get(c) in A}\n\
          \            \n            new_partitions = []\n            for Y in partitions:\n                intersection =\
          \ Y & X\n                difference = Y - X\n                \n                if intersection and difference:\n\
          \                    new_partitions.extend([intersection, difference])\n                    if Y in worklist:\n\
          \                        worklist.remove(Y)\n                        worklist.extend([intersection, difference])\n\
          \                    else:\n                        worklist.append(min(intersection, difference, key=len))\n  \
          \              else:\n                    new_partitions.append(Y)\n            \n            partitions = new_partitions\n\
          \    \n    # Build minimized DFA from partitions\n    # ...\n```"
      pitfalls:
      - Exponential state blowup
      - Not handling dead states
      - Alphabet enumeration
      concepts:
      - Subset construction
      - DFA minimization
      - Lazy evaluation
      estimated_hours: 12-18
  build-shell:
    id: build-shell
    name: Build Your Own Shell
    description: Build a Unix shell that can execute commands, handle pipes, redirections, and job control. A fundamental
      systems programming project.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - C programming
    - Unix process model (fork, exec)
    - File descriptors
    - Signal handling basics
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible: []
    resources:
    - name: Write a Shell in C
      url: https://brennan.io/2015/01/16/write-a-shell-in-c/
      type: blog
    - name: CodeCrafters Shell Challenge
      url: https://app.codecrafters.io/courses/shell/overview
      type: interactive
    - name: GNU Implementing a Shell
      url: https://www.gnu.org/software/libc/manual/html_node/Implementing-a-Shell.html
      type: documentation
    milestones:
    - id: 1
      name: Basic REPL and Command Execution
      description: Read input, parse into command and arguments, execute with fork/exec.
      acceptance_criteria:
      - Displays prompt and reads user input
      - Parses command and arguments
      - Forks child process to execute command
      - Parent waits for child to complete
      - Handles command not found errors
      hints:
        level1: Use fgets() to read input, strtok() to split by spaces.
        level2: fork() returns 0 in child. In child, call execvp(cmd, args). Parent calls waitpid().
        level3: "Basic loop:\nwhile (1) {\n  printf(\"> \");\n  fgets(line, sizeof(line), stdin);\n  char *args[] = parse(line);\n\
          \  pid_t pid = fork();\n  if (pid == 0) execvp(args[0], args);\n  else waitpid(pid, &status, 0);\n}"
      pitfalls:
      - Forgetting to null-terminate args array
      - Not handling newline from fgets()
      - Zombie processes
      concepts:
      - REPL pattern
      - fork() and exec() family
      - Process creation and waiting
      estimated_hours: 3-4
    - id: 2
      name: Built-in Commands
      description: Implement cd, exit, pwd, export, and other built-in commands.
      acceptance_criteria:
      - cd changes current directory
      - cd with no args goes to $HOME
      - exit terminates shell
      - pwd prints working directory
      - export sets environment variables
      hints:
        level1: Built-ins don't fork - they modify shell state directly.
        level2: Use chdir() for cd, getcwd() for pwd, setenv() for export.
        level3: "# Shell built-in commands implementation\nimport os\nimport sys\n\nclass Shell:\n    def __init__(self):\n\
          \        self.builtins = {\n            'cd': self.builtin_cd,\n            'pwd': self.builtin_pwd,\n         \
          \   'export': self.builtin_export,\n            'exit': self.builtin_exit,\n            'echo': self.builtin_echo,\n\
          \            'type': self.builtin_type,\n            'history': self.builtin_history,\n            'alias': self.builtin_alias,\n\
          \        }\n        self.aliases = {}\n        self.history = []\n        self.env = dict(os.environ)\n\n    def\
          \ builtin_cd(self, args):\n        if not args:\n            path = self.env.get('HOME', '/')\n        elif args[0]\
          \ == '-':\n            path = self.env.get('OLDPWD', '.')\n        elif args[0].startswith('~'):\n            path\
          \ = os.path.expanduser(args[0])\n        else:\n            path = args[0]\n\n        try:\n            old_pwd\
          \ = os.getcwd()\n            os.chdir(path)\n            self.env['OLDPWD'] = old_pwd\n            self.env['PWD']\
          \ = os.getcwd()\n            return 0\n        except FileNotFoundError:\n            print(f\"cd: {path}: No such\
          \ file or directory\", file=sys.stderr)\n            return 1\n\n    def builtin_export(self, args):\n        for\
          \ arg in args:\n            if '=' in arg:\n                name, value = arg.split('=', 1)\n                self.env[name]\
          \ = value\n                os.environ[name] = value\n            else:\n                # Export existing variable\n\
          \                if arg in self.env:\n                    os.environ[arg] = self.env[arg]\n        return 0\n\n\
          \    def builtin_type(self, args):\n        for name in args:\n            if name in self.builtins:\n         \
          \       print(f\"{name} is a shell builtin\")\n            elif name in self.aliases:\n                print(f\"\
          {name} is aliased to `{self.aliases[name]}'\")\n            else:\n                path = self.find_executable(name)\n\
          \                if path:\n                    print(f\"{name} is {path}\")\n                else:\n           \
          \         print(f\"{name}: not found\")\n                    return 1\n        return 0\n\n    def find_executable(self,\
          \ name):\n        for dir in self.env.get('PATH', '').split(':'):\n            path = os.path.join(dir, name)\n\
          \            if os.path.isfile(path) and os.access(path, os.X_OK):\n                return path\n        return\
          \ None"
      pitfalls:
      - Trying to cd in child process (only affects child)
      - Not expanding ~ to HOME
      - Environment variables not inherited
      concepts:
      - Built-in vs external commands
      - Working directory
      - Environment variables
      estimated_hours: 2-3
    - id: 3
      name: I/O Redirection
      description: Implement input (<), output (>), and append (>>) redirection.
      acceptance_criteria:
      - cmd > file redirects stdout to file
      - cmd >> file appends to file
      - cmd < file reads stdin from file
      - cmd 2> file redirects stderr
      - Combinations work (cmd < in > out)
      hints:
        level1: Use open() to get file descriptor, dup2() to redirect.
        level2: dup2(fd, STDOUT_FILENO) makes stdout point to fd. Do this in child before exec.
        level3: 'In child, before exec:

          int fd = open(filename, O_WRONLY | O_CREAT | O_TRUNC, 0644);

          dup2(fd, STDOUT_FILENO);

          close(fd);

          execvp(cmd, args);'
      pitfalls:
      - Forgetting to close original fd after dup2
      - Wrong open() flags
      - File permissions on created files
      concepts:
      - File descriptors
      - dup2() system call
      - Unix I/O model
      estimated_hours: 2-3
    - id: 4
      name: Pipes
      description: Implement command pipelines (cmd1 | cmd2 | cmd3).
      acceptance_criteria:
      - Two command pipeline works (ls | grep foo)
      - Multi-stage pipelines work
      - Proper cleanup of file descriptors
      - All pipeline stages run concurrently
      hints:
        level1: pipe() creates fd pair. pipe[0] for reading, pipe[1] for writing.
        level2: Fork for each command. Connect stdout of cmd1 to stdin of cmd2 via pipe.
        level3: "For cmd1 | cmd2:\nint pipefd[2];\npipe(pipefd);\nif (fork() == 0) {  // cmd1\n  dup2(pipefd[1], STDOUT_FILENO);\n\
          \  close(pipefd[0]); close(pipefd[1]);\n  exec(cmd1);\n}\nif (fork() == 0) {  // cmd2\n  dup2(pipefd[0], STDIN_FILENO);\n\
          \  close(pipefd[0]); close(pipefd[1]);\n  exec(cmd2);\n}\nclose(pipefd[0]); close(pipefd[1]);\nwait(); wait();"
      pitfalls:
      - Not closing unused pipe ends (causes hang)
      - Parent not closing pipe fds
      - Order of fork/close operations
      concepts:
      - Unix pipes
      - Process communication
      - File descriptor inheritance
      estimated_hours: 4-6
    - id: 5
      name: Background Jobs
      description: Run commands in background with &, implement job listing.
      acceptance_criteria:
      - cmd & runs in background, shell returns immediately
      - jobs command lists background processes
      - Shell notifies when background job completes
      - Background jobs have job numbers [1], [2], etc.
      hints:
        level1: Don't waitpid() immediately for background jobs. Store PID in job list.
        level2: Use SIGCHLD handler to detect when background jobs finish.
        level3: "# Job control for background processes\nimport os\nimport signal\nfrom enum import Enum\n\nclass JobState(Enum):\n\
          \    RUNNING = \"Running\"\n    STOPPED = \"Stopped\"\n    DONE = \"Done\"\n\nclass Job:\n    def __init__(self,\
          \ job_id, pgid, command, processes):\n        self.id = job_id\n        self.pgid = pgid  # Process group ID\n \
          \       self.command = command\n        self.processes = processes  # List of PIDs\n        self.state = JobState.RUNNING\n\
          \n    def __str__(self):\n        return f\"[{self.id}]  {self.state.value}  {self.command}\"\n\nclass JobController:\n\
          \    def __init__(self):\n        self.jobs = {}\n        self.next_id = 1\n        self.foreground_pgid = None\n\
          \        signal.signal(signal.SIGCHLD, self.sigchld_handler)\n\n    def launch_job(self, command, background=False):\n\
          \        pid = os.fork()\n        if pid == 0:\n            # Child: create new process group\n            os.setpgid(0,\
          \ 0)\n            if not background:\n                # Give terminal to foreground job\n                os.tcsetpgrp(0,\
          \ os.getpgrp())\n            signal.signal(signal.SIGINT, signal.SIG_DFL)\n            signal.signal(signal.SIGTSTP,\
          \ signal.SIG_DFL)\n            os.execvp(command[0], command)\n        else:\n            # Parent\n           \
          \ pgid = pid\n            os.setpgid(pid, pgid)\n            job = Job(self.next_id, pgid, ' '.join(command), [pid])\n\
          \            self.jobs[self.next_id] = job\n            self.next_id += 1\n\n            if background:\n      \
          \          print(f\"[{job.id}] {pid}\")\n            else:\n                self.foreground_pgid = pgid\n      \
          \          os.tcsetpgrp(0, pgid)\n                self.wait_for_job(job)\n                os.tcsetpgrp(0, os.getpgrp())\n\
          \n    def wait_for_job(self, job):\n        while job.state == JobState.RUNNING:\n            pid, status = os.waitpid(-job.pgid,\
          \ os.WUNTRACED)\n            if os.WIFSTOPPED(status):\n                job.state = JobState.STOPPED\n         \
          \       print(f\"\\n{job}\")\n            elif os.WIFEXITED(status) or os.WIFSIGNALED(status):\n               \
          \ job.state = JobState.DONE\n\n    def builtin_fg(self, args):\n        job_id = int(args[0]) if args else max(self.jobs.keys())\n\
          \        job = self.jobs.get(job_id)\n        if job:\n            print(job.command)\n            os.killpg(job.pgid,\
          \ signal.SIGCONT)\n            job.state = JobState.RUNNING\n            os.tcsetpgrp(0, job.pgid)\n           \
          \ self.wait_for_job(job)\n            os.tcsetpgrp(0, os.getpgrp())\n\n    def builtin_bg(self, args):\n       \
          \ job_id = int(args[0]) if args else max(self.jobs.keys())\n        job = self.jobs.get(job_id)\n        if job\
          \ and job.state == JobState.STOPPED:\n            os.killpg(job.pgid, signal.SIGCONT)\n            job.state = JobState.RUNNING\n\
          \            print(f\"[{job.id}]+ {job.command} &\")"
      pitfalls:
      - Zombie processes from background jobs
      - Race condition in SIGCHLD handler
      - Reaping wrong child
      concepts:
      - Background execution
      - Asynchronous process management
      - Zombie processes
      estimated_hours: 3-4
    - id: 6
      name: Job Control (fg, bg, Ctrl+Z)
      description: Full job control with suspend, resume, foreground/background.
      acceptance_criteria:
      - Ctrl+Z suspends foreground job
      - fg brings job to foreground
      - bg resumes suspended job in background
      - Proper terminal control group management
      hints:
        level1: Handle SIGTSTP (Ctrl+Z). Suspended jobs need to be resumed with SIGCONT.
        level2: Use setpgid() to put job in its own process group. Use tcsetpgrp() for foreground.
        level3: 'Job control flow:

          1. Create job, put in new process group

          2. If foreground: tcsetpgrp(terminal, job_pgid)

          3. On Ctrl+Z: shell regains terminal, job is stopped

          4. fg: tcsetpgrp to job, send SIGCONT, wait

          5. bg: send SIGCONT, don''t wait'
      pitfalls:
      - Terminal control group confusion
      - Orphaned process groups
      - Signal handling race conditions
      concepts:
      - Process groups
      - Terminal control
      - Signal handling (SIGTSTP, SIGCONT)
      estimated_hours: 5-8
  build-spreadsheet:
    id: build-spreadsheet
    name: Build Your Own Spreadsheet
    description: Build an Excel-like spreadsheet application with formulas, cell references, and recalculation.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - DOM manipulation
    - Graph algorithms
    - Expression parsing
    - Event handling
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible:
      - Python
      - Rust
    resources:
    - type: article
      name: Building a Spreadsheet Engine
      url: https://leanrada.com/notes/spreadsheet-engine/
    - type: video
      name: How Excel Recalculates
      url: https://www.youtube.com/watch?v=R0hhDgvWbUU
    milestones:
    - id: 1
      name: Grid & Cell Rendering
      description: Build the spreadsheet grid with editable cells.
      acceptance_criteria:
      - Virtual scrolling for large grids
      - Cell selection and editing
      - Column/row headers (A, B, C... and 1, 2, 3...)
      - Cell formatting (width, alignment)
      hints:
        level1: Use a virtualized grid - only render visible cells plus buffer.
        level2: Track selection state, handle keyboard navigation (arrow keys, Tab, Enter).
        level3: "## Grid Implementation\n\n```javascript\nclass SpreadsheetGrid {\n  constructor(container, rows = 1000, cols\
          \ = 26) {\n    this.rows = rows;\n    this.cols = cols;\n    this.cellData = new Map(); // 'A1' -> { value, formula,\
          \ formatted }\n    this.selection = { row: 0, col: 0 };\n    this.viewportStart = { row: 0, col: 0 };\n    \n  \
          \  this.rowHeight = 24;\n    this.defaultColWidth = 100;\n    this.colWidths = new Map();\n    \n    this.setupDOM(container);\n\
          \    this.render();\n  }\n  \n  getCellId(row, col) {\n    return String.fromCharCode(65 + col) + (row + 1);\n \
          \ }\n  \n  parseCellId(id) {\n    const match = id.match(/^([A-Z]+)(\\d+)$/);\n    if (!match) return null;\n  \
          \  const col = match[1].split('').reduce((acc, c) => acc * 26 + c.charCodeAt(0) - 64, 0) - 1;\n    const row = parseInt(match[2])\
          \ - 1;\n    return { row, col };\n  }\n  \n  render() {\n    const visibleRows = Math.ceil(this.container.clientHeight\
          \ / this.rowHeight) + 2;\n    const visibleCols = this.getVisibleCols();\n    \n    // Only render visible cells\n\
          \    this.gridBody.innerHTML = '';\n    for (let r = this.viewportStart.row; r < this.viewportStart.row + visibleRows;\
          \ r++) {\n      const rowEl = document.createElement('div');\n      rowEl.className = 'row';\n      rowEl.style.top\
          \ = r * this.rowHeight + 'px';\n      \n      for (const col of visibleCols) {\n        const cell = this.createCell(r,\
          \ col);\n        rowEl.appendChild(cell);\n      }\n      this.gridBody.appendChild(rowEl);\n    }\n  }\n  \n  handleKeydown(e)\
          \ {\n    switch(e.key) {\n      case 'ArrowUp': this.moveSelection(-1, 0); break;\n      case 'ArrowDown': this.moveSelection(1,\
          \ 0); break;\n      case 'ArrowLeft': this.moveSelection(0, -1); break;\n      case 'ArrowRight': this.moveSelection(0,\
          \ 1); break;\n      case 'Tab': this.moveSelection(0, e.shiftKey ? -1 : 1); e.preventDefault(); break;\n      case\
          \ 'Enter': this.startEditing(); break;\n    }\n  }\n}\n```"
      pitfalls:
      - Rendering too many cells
      - Slow scrolling
      - Focus management issues
      concepts:
      - Virtual scrolling
      - DOM optimization
      - Keyboard navigation
      estimated_hours: 10-15
    - id: 2
      name: Formula Parser
      description: Parse and evaluate spreadsheet formulas.
      acceptance_criteria:
      - Arithmetic expressions (+, -, *, /, ^)
      - Cell references (A1, B2)
      - Range references (A1:B5)
      - Built-in functions (SUM, AVERAGE, IF, etc.)
      hints:
        level1: Use recursive descent parsing. Formulas start with '='.
        level2: Build an AST, then evaluate. Handle operator precedence.
        level3: "## Formula Parser\n\n```javascript\nclass FormulaParser {\n  constructor(getCellValue) {\n    this.getCellValue\
          \ = getCellValue;\n  }\n  \n  parse(formula) {\n    this.tokens = this.tokenize(formula.substring(1)); // Remove\
          \ '='\n    this.pos = 0;\n    return this.parseExpression();\n  }\n  \n  tokenize(str) {\n    const tokens = [];\n\
          \    const regex = /([A-Z]+\\d+:[A-Z]+\\d+|[A-Z]+\\d+|\\d+\\.?\\d*|[+\\-*/^(),]|[A-Z]+(?=\\()|\"[^\"]*\")/gi;\n\
          \    let match;\n    while ((match = regex.exec(str)) !== null) {\n      tokens.push(match[0]);\n    }\n    return\
          \ tokens;\n  }\n  \n  parseExpression() {\n    let left = this.parseTerm();\n    while (this.current() === '+' ||\
          \ this.current() === '-') {\n      const op = this.consume();\n      const right = this.parseTerm();\n      left\
          \ = { type: 'binary', op, left, right };\n    }\n    return left;\n  }\n  \n  parseTerm() {\n    let left = this.parseFactor();\n\
          \    while (this.current() === '*' || this.current() === '/') {\n      const op = this.consume();\n      const right\
          \ = this.parseFactor();\n      left = { type: 'binary', op, left, right };\n    }\n    return left;\n  }\n  \n \
          \ parseFactor() {\n    const token = this.current();\n    \n    if (token === '(') {\n      this.consume();\n  \
          \    const expr = this.parseExpression();\n      this.consume(); // ')'\n      return expr;\n    }\n    \n    if\
          \ (/^[A-Z]+$/i.test(token) && this.peek() === '(') {\n      return this.parseFunction();\n    }\n    \n    if (/^[A-Z]+\\\
          d+:[A-Z]+\\d+$/i.test(token)) {\n      this.consume();\n      return { type: 'range', range: token };\n    }\n \
          \   \n    if (/^[A-Z]+\\d+$/i.test(token)) {\n      this.consume();\n      return { type: 'cell', ref: token };\n\
          \    }\n    \n    if (/^\\d/.test(token)) {\n      this.consume();\n      return { type: 'number', value: parseFloat(token)\
          \ };\n    }\n    \n    throw new Error(`Unexpected token: ${token}`);\n  }\n  \n  parseFunction() {\n    const name\
          \ = this.consume();\n    this.consume(); // '('\n    const args = [];\n    while (this.current() !== ')') {\n  \
          \    args.push(this.parseExpression());\n      if (this.current() === ',') this.consume();\n    }\n    this.consume();\
          \ // ')'\n    return { type: 'function', name: name.toUpperCase(), args };\n  }\n}\n```"
      pitfalls:
      - Operator precedence errors
      - Not handling negative numbers
      - String vs number coercion
      concepts:
      - Lexical analysis
      - Recursive descent parsing
      - AST construction
      estimated_hours: 12-18
    - id: 3
      name: Dependency Graph & Recalculation
      description: Track cell dependencies and recalculate in correct order.
      acceptance_criteria:
      - Dependency tracking
      - Topological sort for recalc order
      - Circular reference detection
      - Incremental recalculation
      hints:
        level1: When a cell is edited, find all cells that depend on it and recalculate.
        level2: Build a dependency graph. Use topological sort to determine recalc order.
        level3: "## Dependency Graph\n\n```javascript\nclass DependencyGraph {\n  constructor() {\n    this.dependencies =\
          \ new Map();  // cell -> Set of cells it depends on\n    this.dependents = new Map();    // cell -> Set of cells\
          \ that depend on it\n  }\n  \n  setDependencies(cell, deps) {\n    // Remove old dependencies\n    const oldDeps\
          \ = this.dependencies.get(cell) || new Set();\n    for (const dep of oldDeps) {\n      this.dependents.get(dep)?.delete(cell);\n\
          \    }\n    \n    // Set new dependencies\n    this.dependencies.set(cell, new Set(deps));\n    for (const dep of\
          \ deps) {\n      if (!this.dependents.has(dep)) {\n        this.dependents.set(dep, new Set());\n      }\n     \
          \ this.dependents.get(dep).add(cell);\n    }\n  }\n  \n  getCellsToRecalculate(changedCell) {\n    // Get all cells\
          \ affected by this change\n    const affected = new Set();\n    const queue = [changedCell];\n    \n    while (queue.length\
          \ > 0) {\n      const cell = queue.shift();\n      const deps = this.dependents.get(cell) || new Set();\n      for\
          \ (const dep of deps) {\n        if (!affected.has(dep)) {\n          affected.add(dep);\n          queue.push(dep);\n\
          \        }\n      }\n    }\n    \n    // Topological sort\n    return this.topologicalSort(affected);\n  }\n  \n\
          \  topologicalSort(cells) {\n    const visited = new Set();\n    const result = [];\n    \n    const visit = (cell)\
          \ => {\n      if (visited.has(cell)) return;\n      visited.add(cell);\n      \n      const deps = this.dependencies.get(cell)\
          \ || new Set();\n      for (const dep of deps) {\n        if (cells.has(dep)) visit(dep);\n      }\n      result.push(cell);\n\
          \    };\n    \n    for (const cell of cells) {\n      visit(cell);\n    }\n    \n    return result;\n  }\n  \n \
          \ detectCircular(cell, newDeps) {\n    // DFS to check if adding these deps would create a cycle\n    const visited\
          \ = new Set();\n    const stack = [...newDeps];\n    \n    while (stack.length > 0) {\n      const current = stack.pop();\n\
          \      if (current === cell) return true;  // Circular!\n      if (visited.has(current)) continue;\n      visited.add(current);\n\
          \      \n      const deps = this.dependencies.get(current) || new Set();\n      for (const dep of deps) {\n    \
          \    stack.push(dep);\n      }\n    }\n    return false;\n  }\n}\n```"
      pitfalls:
      - Infinite loops from circular refs
      - Recalculating too much
      - Wrong recalc order
      concepts:
      - Directed graphs
      - Topological sorting
      - Cycle detection
      estimated_hours: 12-18
    - id: 4
      name: Advanced Features
      description: Add formatting, copy/paste, and undo/redo.
      acceptance_criteria:
      - Cell formatting (bold, colors, borders)
      - Copy/paste with formula adjustment
      - Undo/redo stack
      - Import/export (CSV, JSON)
      hints:
        level1: For copy/paste, adjust relative cell references based on destination.
        level2: Use command pattern for undo/redo. Each action is a command that can be reversed.
        level3: "## Copy/Paste & Undo\n\n```javascript\n// Adjust cell references when copying\nfunction adjustFormula(formula,\
          \ rowDelta, colDelta) {\n  return formula.replace(/([A-Z]+)(\\d+)/g, (match, col, row) => {\n    // Check if reference\
          \ is absolute ($A$1)\n    const newCol = String.fromCharCode(col.charCodeAt(0) + colDelta);\n    const newRow =\
          \ parseInt(row) + rowDelta;\n    return newCol + newRow;\n  });\n}\n\n// Command pattern for undo/redo\nclass Command\
          \ {\n  execute() { throw new Error('Not implemented'); }\n  undo() { throw new Error('Not implemented'); }\n}\n\n\
          class SetCellCommand extends Command {\n  constructor(spreadsheet, cellId, newValue) {\n    super();\n    this.spreadsheet\
          \ = spreadsheet;\n    this.cellId = cellId;\n    this.newValue = newValue;\n    this.oldValue = spreadsheet.getRawValue(cellId);\n\
          \  }\n  \n  execute() {\n    this.spreadsheet.setCellValue(this.cellId, this.newValue, false);\n  }\n  \n  undo()\
          \ {\n    this.spreadsheet.setCellValue(this.cellId, this.oldValue, false);\n  }\n}\n\nclass UndoManager {\n  constructor()\
          \ {\n    this.undoStack = [];\n    this.redoStack = [];\n  }\n  \n  execute(command) {\n    command.execute();\n\
          \    this.undoStack.push(command);\n    this.redoStack = [];  // Clear redo stack\n  }\n  \n  undo() {\n    if (this.undoStack.length\
          \ === 0) return;\n    const command = this.undoStack.pop();\n    command.undo();\n    this.redoStack.push(command);\n\
          \  }\n  \n  redo() {\n    if (this.redoStack.length === 0) return;\n    const command = this.redoStack.pop();\n\
          \    command.execute();\n    this.undoStack.push(command);\n  }\n}\n```"
      pitfalls:
      - Memory leaks in undo stack
      - Not handling absolute references ($A$1)
      - Paste overwriting formulas
      concepts:
      - Command pattern
      - Reference adjustment
      - State management
      estimated_hours: 15-20
  build-sqlite:
    id: build-sqlite
    name: Build Your Own SQLite
    description: Build an embedded SQL database that stores data in a single file. You'll learn about SQL parsing, B-trees,
      query execution, and transactions.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - B-tree data structure
    - SQL basics
    - File I/O and binary formats
    - Basic compiler concepts
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
      - Java
    resources:
    - name: Let's Build a Simple Database
      url: https://cstack.github.io/db_tutorial/
      type: tutorial
    - name: CodeCrafters SQLite Challenge
      url: https://app.codecrafters.io/courses/sqlite/overview
      type: interactive
    - name: SQLite File Format
      url: https://www.sqlite.org/fileformat2.html
      type: documentation
    - name: CMU 15-445 Database Systems
      url: https://15445.courses.cs.cmu.edu
      type: course
    milestones:
    - id: 1
      name: SQL Tokenizer
      description: Build a lexer that converts SQL text into tokens.
      acceptance_criteria:
      - Tokenizes keywords (SELECT, FROM, WHERE, INSERT, etc.)
      - Handles identifiers (table names, column names)
      - Parses string and numeric literals
      - Handles operators
      - Ignores whitespace and comments
      hints:
        level1: Use a state machine or regular expressions for each token type.
        level2: SQL keywords are case-insensitive. Normalize to uppercase.
        level3: 'Token types to implement:

          - KEYWORD: SELECT, FROM, WHERE, INSERT, UPDATE, DELETE, CREATE

          - IDENTIFIER: table/column names

          - STRING: ''hello''

          - NUMBER: 123, 3.14

          - OPERATOR: =, <>, <, >

          - PUNCTUATION: (, ), ,, ;'
      pitfalls:
      - Not handling escaped quotes in strings ('it''s')
      - Case sensitivity
      - Unicode identifiers
      concepts:
      - Lexical analysis
      - Finite state machines
      - Token representation
      estimated_hours: 3-4
    - id: 2
      name: SQL Parser (AST)
      description: Build a parser that converts tokens into an Abstract Syntax Tree.
      acceptance_criteria:
      - Parses SELECT statements with columns, FROM, WHERE
      - Parses INSERT statements
      - Parses CREATE TABLE with column definitions
      - Handles expressions
      - Reports syntax errors with line/column
      hints:
        level1: Use recursive descent parsing - one function per grammar rule.
        level2: For expressions, use Pratt parsing or precedence climbing for operators.
        level3: 'Grammar sketch:

          select := SELECT columns FROM table [WHERE expr]

          columns := * | column (, column)*

          expr := term ((AND|OR) term)*

          term := value (op value)?'
      pitfalls:
      - Left recursion in grammar
      - Operator precedence (AND vs OR)
      - Not handling parentheses
      concepts:
      - Recursive descent parsing
      - AST design
      - Operator precedence
      estimated_hours: 5-8
    - id: 3
      name: B-tree Page Format
      description: Implement the on-disk B-tree page structure.
      acceptance_criteria:
      - Fixed page size (4096 bytes default)
      - Page header with cell count, free space pointer
      - Cell pointer array for variable-size cells
      - Interior nodes store keys + child page pointers
      - Leaf nodes store keys + values
      hints:
        level1: Each page is a node in the B-tree. Start with leaf nodes only.
        level2: 'Page layout:

          [Header (8-12 bytes)]

          [Cell Pointer Array (2 bytes each)]

          [Free space]

          [Cells (grow from bottom)]'
        level3: "# SQLite-like B-tree page format\nimport struct\nfrom enum import IntEnum\n\nclass PageType(IntEnum):\n \
          \   INTERIOR_INDEX = 2\n    INTERIOR_TABLE = 5\n    LEAF_INDEX = 10\n    LEAF_TABLE = 13\n\nclass BTreePage:\n \
          \   '''\n    Page format (4096 bytes default):\n    - Header (8-12 bytes)\n    - Cell pointer array\n    - Unallocated\
          \ space\n    - Cell content area (grows from end)\n\n    Header format:\n    - 1 byte: page type\n    - 2 bytes:\
          \ first freeblock offset (0 if none)\n    - 2 bytes: number of cells\n    - 2 bytes: start of cell content area\n\
          \    - 1 byte: fragmented free bytes\n    - 4 bytes: right-most pointer (interior pages only)\n    '''\n\n    def\
          \ __init__(self, page_num, page_type=PageType.LEAF_TABLE):\n        self.page_num = page_num\n        self.page_type\
          \ = page_type\n        self.cells = []\n        self.right_ptr = None  # For interior pages\n        self.page_size\
          \ = 4096\n\n    def serialize(self):\n        # Build cell content from the end\n        cell_data = b''\n     \
          \   cell_pointers = []\n        content_start = self.page_size\n\n        for cell in self.cells:\n            cell_bytes\
          \ = cell.serialize()\n            content_start -= len(cell_bytes)\n            cell_data = cell_bytes + cell_data\n\
          \            cell_pointers.append(content_start)\n\n        # Header\n        header_size = 12 if self.page_type\
          \ in (PageType.INTERIOR_INDEX, PageType.INTERIOR_TABLE) else 8\n        header = struct.pack('>BHHHB',\n       \
          \     self.page_type,\n            0,  # first freeblock\n            len(self.cells),\n            content_start,\n\
          \            0   # fragmented bytes\n        )\n        if header_size == 12:\n            header += struct.pack('>I',\
          \ self.right_ptr or 0)\n\n        # Cell pointers\n        pointers = b''.join(struct.pack('>H', p) for p in cell_pointers)\n\
          \n        # Assemble page\n        used = header_size + len(pointers)\n        unallocated = self.page_size - used\
          \ - len(cell_data)\n        return header + pointers + (b'\\x00' * unallocated) + cell_data\n\n    @classmethod\n\
          \    def deserialize(cls, page_num, data):\n        page_type = data[0]\n        num_cells = struct.unpack('>H',\
          \ data[3:5])[0]\n\n        page = cls(page_num, PageType(page_type))\n        header_size = 12 if page_type in (2,\
          \ 5) else 8\n\n        # Read cell pointers\n        for i in range(num_cells):\n            offset = struct.unpack('>H',\
          \ data[header_size + i*2:header_size + i*2 + 2])[0]\n            # Parse cell at offset...\n        return page"
      pitfalls:
      - Cell overflow (value too large for page)
      - Page fragmentation after deletions
      - Endianness
      concepts:
      - B-tree structure
      - Page-based storage
      - Variable-length records
      estimated_hours: 6-10
    - id: 4
      name: Table Storage
      description: Store table rows in B-tree leaves with rowid as key.
      acceptance_criteria:
      - Each table has its own B-tree
      - Rowid is auto-incrementing primary key
      - Rows stored as serialized column values
      - CREATE TABLE creates new B-tree root page
      - Table schema stored in sqlite_master table
      hints:
        level1: 'Simplest: rowid -> serialized row bytes. Key is rowid, value is row.'
        level2: 'sqlite_master stores schema: CREATE TABLE statements as text.'
        level3: 'Record format (simplified):

          [Header size][Type1][Type2]...[Value1][Value2]...

          Types: NULL, INT8/16/32/64, FLOAT, BLOB, TEXT'
      pitfalls:
      - Not handling NULL values
      - Rowid gaps after deletes
      - Variable-length integer encoding
      concepts:
      - Row storage formats
      - Schema management
      - Type serialization
      estimated_hours: 4-6
    - id: 5
      name: SELECT Execution (Table Scan)
      description: Execute SELECT queries by scanning all rows.
      acceptance_criteria:
      - SELECT * FROM table returns all rows
      - SELECT col1, col2 returns specific columns
      - Rows returned in rowid order
      - Handle non-existent tables with error
      hints:
        level1: Iterate B-tree leaves from leftmost to rightmost.
        level2: Deserialize each row, project requested columns.
        level3: "# SELECT query execution with table scan\nfrom dataclasses import dataclass\nfrom typing import List, Any,\
          \ Optional\n\n@dataclass\nclass Column:\n    name: str\n    type: str\n    primary_key: bool = False\n\n@dataclass\n\
          class Table:\n    name: str\n    columns: List[Column]\n    root_page: int\n\nclass QueryExecutor:\n    def __init__(self,\
          \ pager, schema):\n        self.pager = pager\n        self.schema = schema\n\n    def execute_select(self, stmt):\n\
          \        table = self.schema.get_table(stmt.table_name)\n        if not table:\n            raise RuntimeError(f\"\
          no such table: {stmt.table_name}\")\n\n        # Full table scan\n        results = []\n        for row in self.scan_table(table):\n\
          \            # Apply WHERE filter\n            if stmt.where_clause:\n                if not self.evaluate_where(row,\
          \ stmt.where_clause, table):\n                    continue\n\n            # Project columns\n            if stmt.columns\
          \ == ['*']:\n                results.append(row)\n            else:\n                projected = []\n          \
          \      for col_name in stmt.columns:\n                    idx = self.get_column_index(table, col_name)\n       \
          \             projected.append(row[idx])\n                results.append(tuple(projected))\n\n        return results\n\
          \n    def scan_table(self, table):\n        '''Iterate all rows via B-tree traversal'''\n        def scan_page(page_num):\n\
          \            page = self.pager.get_page(page_num)\n\n            if page.is_leaf():\n                for cell in\
          \ page.cells:\n                    yield cell.payload  # (rowid, col1, col2, ...)\n            else:\n         \
          \       # Interior page: traverse children\n                for cell in page.cells:\n                    yield from\
          \ scan_page(cell.left_child)\n                if page.right_ptr:\n                    yield from scan_page(page.right_ptr)\n\
          \n        yield from scan_page(table.root_page)\n\n    def evaluate_where(self, row, where, table):\n        '''Evaluate\
          \ WHERE clause against a row'''\n        if where.op == 'AND':\n            return (self.evaluate_where(row, where.left,\
          \ table) and\n                    self.evaluate_where(row, where.right, table))\n        elif where.op == 'OR':\n\
          \            return (self.evaluate_where(row, where.left, table) or\n                    self.evaluate_where(row,\
          \ where.right, table))\n        else:\n            # Comparison: column op value\n            col_idx = self.get_column_index(table,\
          \ where.column)\n            col_value = row[col_idx]\n\n            if where.op == '=':\n                return\
          \ col_value == where.value\n            elif where.op == '<':\n                return col_value < where.value\n\
          \            elif where.op == '>':\n                return col_value > where.value\n            # ... other operators"
      pitfalls:
      - Column name case sensitivity
      - NULL handling in output
      - Memory management for large result sets
      concepts:
      - B-tree traversal
      - Projection operator
      - Cursor pattern
      estimated_hours: 3-4
    - id: 6
      name: INSERT/UPDATE/DELETE
      description: Implement data modification operations.
      acceptance_criteria:
      - INSERT INTO table VALUES (...) adds row
      - INSERT with column list
      - UPDATE table SET col=val WHERE condition
      - DELETE FROM table WHERE condition
      - Proper rowid assignment for inserts
      hints:
        level1: 'INSERT: serialize row, insert into B-tree with next rowid.'
        level2: 'DELETE: find matching rows, remove from B-tree, handle rebalancing.'
        level3: "# Data modification operations\nclass QueryExecutor:\n    def execute_insert(self, stmt):\n        table\
          \ = self.schema.get_table(stmt.table_name)\n\n        # Get next rowid\n        rowid = self.get_next_rowid(table)\n\
          \n        # Build record\n        values = [rowid]\n        for i, col in enumerate(table.columns):\n          \
          \  if col.name in stmt.columns:\n                idx = stmt.columns.index(col.name)\n                values.append(stmt.values[idx])\n\
          \            else:\n                values.append(None)  # Default value\n\n        # Insert into B-tree\n     \
          \   record = self.encode_record(values)\n        self.btree_insert(table.root_page, rowid, record)\n\n        return\
          \ rowid\n\n    def execute_update(self, stmt):\n        table = self.schema.get_table(stmt.table_name)\n       \
          \ updated_count = 0\n\n        # Scan and update matching rows\n        for row in self.scan_table(table):\n   \
          \         if stmt.where_clause and not self.evaluate_where(row, stmt.where_clause, table):\n                continue\n\
          \n            rowid = row[0]\n            new_values = list(row)\n\n            # Apply SET clauses\n          \
          \  for col_name, new_value in stmt.set_clauses:\n                idx = self.get_column_index(table, col_name)\n\
          \                new_values[idx] = new_value\n\n            # Update in B-tree (delete + insert)\n            self.btree_delete(table.root_page,\
          \ rowid)\n            record = self.encode_record(new_values)\n            self.btree_insert(table.root_page, rowid,\
          \ record)\n            updated_count += 1\n\n        return updated_count\n\n    def execute_delete(self, stmt):\n\
          \        table = self.schema.get_table(stmt.table_name)\n        deleted_count = 0\n\n        # Collect rowids to\
          \ delete (can't modify during scan)\n        to_delete = []\n        for row in self.scan_table(table):\n      \
          \      if stmt.where_clause and not self.evaluate_where(row, stmt.where_clause, table):\n                continue\n\
          \            to_delete.append(row[0])  # rowid\n\n        # Delete from B-tree\n        for rowid in to_delete:\n\
          \            self.btree_delete(table.root_page, rowid)\n            deleted_count += 1\n\n        return deleted_count\n\
          \n    def encode_record(self, values):\n        '''SQLite record format: header + body'''\n        header = []\n\
          \        body = b''\n        for val in values:\n            if val is None:\n                header.append(0)\n\
          \            elif isinstance(val, int):\n                if val == 0: header.append(8)\n                elif val\
          \ == 1: header.append(9)\n                else: header.append(1); body += struct.pack('>b', val)\n            elif\
          \ isinstance(val, str):\n                encoded = val.encode('utf-8')\n                header.append(13 + len(encoded)\
          \ * 2)\n                body += encoded\n        return self.encode_varint_header(header) + body"
      pitfalls:
      - B-tree rebalancing after delete
      - Updating primary key
      - Constraint violations
      concepts:
      - B-tree insertion and deletion
      - Record modification
      - Constraint checking
      estimated_hours: 5-8
    - id: 7
      name: WHERE Clause and Indexes
      description: Implement filtering and secondary indexes.
      acceptance_criteria:
      - WHERE with comparison operators
      - WHERE with AND/OR
      - CREATE INDEX creates secondary B-tree
      - Query uses index when beneficial
      - EXPLAIN shows query plan
      hints:
        level1: 'Full table scan with filter: scan all, test predicate, output matches.'
        level2: 'Index B-tree: key=indexed column, value=rowid. Lookup, then fetch row.'
        level3: "# Index-based query optimization\nfrom bisect import bisect_left\n\nclass Index:\n    def __init__(self,\
          \ name, table_name, columns, root_page):\n        self.name = name\n        self.table_name = table_name\n     \
          \   self.columns = columns  # List of column names\n        self.root_page = root_page\n\nclass QueryOptimizer:\n\
          \    def __init__(self, schema, executor):\n        self.schema = schema\n        self.executor = executor\n\n \
          \   def optimize_select(self, stmt):\n        '''Choose best execution strategy'''\n        table = self.schema.get_table(stmt.table_name)\n\
          \n        if not stmt.where_clause:\n            return TableScan(table)\n\n        # Look for usable indexes\n\
          \        index = self.find_usable_index(stmt.where_clause, table)\n        if index:\n            return IndexScan(table,\
          \ index, stmt.where_clause)\n\n        return TableScan(table)\n\n    def find_usable_index(self, where, table):\n\
          \        '''Find index that covers WHERE clause'''\n        if where.op in ('=', '<', '>', '<=', '>='):\n      \
          \      for idx in self.schema.get_indexes(table.name):\n                if idx.columns[0] == where.column:\n   \
          \                 return idx\n        return None\n\nclass IndexScan:\n    def __init__(self, table, index, where):\n\
          \        self.table = table\n        self.index = index\n        self.where = where\n\n    def execute(self, executor):\n\
          \        '''Use index to find matching rowids, then fetch rows'''\n        if self.where.op == '=':\n          \
          \  # Point lookup\n            rowids = executor.index_lookup(self.index, self.where.value)\n        elif self.where.op\
          \ in ('<', '<='):\n            # Range scan from start\n            rowids = executor.index_range(self.index, None,\
          \ self.where.value,\n                                         include_end=(self.where.op == '<='))\n        elif\
          \ self.where.op in ('>', '>='):\n            # Range scan to end\n            rowids = executor.index_range(self.index,\
          \ self.where.value, None,\n                                         include_start=(self.where.op == '>='))\n\n \
          \       # Fetch actual rows\n        for rowid in rowids:\n            yield executor.fetch_row(self.table, rowid)\n\
          \nclass QueryExecutor:\n    def index_lookup(self, index, key):\n        '''B-tree lookup for exact match'''\n \
          \       page = self.pager.get_page(index.root_page)\n        while True:\n            if page.is_leaf():\n     \
          \           for cell in page.cells:\n                    if cell.key == key:\n                        yield cell.rowid\n\
          \                return\n            else:\n                # Find child to descend\n                child_page\
          \ = self.find_child(page, key)\n                page = self.pager.get_page(child_page)"
      pitfalls:
      - Index not used for non-equality predicates
      - Maintaining index on INSERT/UPDATE/DELETE
      - Choosing between index scan vs table scan
      concepts:
      - Secondary indexes
      - Query planning basics
      - Filter predicates
      estimated_hours: 6-10
    - id: 8
      name: Query Planner
      description: Implement cost-based query optimization.
      acceptance_criteria:
      - Estimates row counts for tables
      - Chooses between table scan and index scan
      - Handles simple JOINs
      - EXPLAIN QUERY PLAN shows chosen plan
      hints:
        level1: Store row count per table. Estimate selectivity of predicates.
        level2: 'Cost model: table scan = N rows, index lookup = log(N) + matching rows.'
        level3: "# Query planner with cost estimation\nfrom dataclasses import dataclass\nfrom typing import List\nimport\
          \ math\n\n@dataclass\nclass Plan:\n    cost: float\n    rows: int\n    description: str\n\nclass QueryPlanner:\n\
          \    def __init__(self, schema, stats):\n        self.schema = schema\n        self.stats = stats  # Table statistics\n\
          \n    def plan_select(self, stmt):\n        table = self.schema.get_table(stmt.table_name)\n        table_stats\
          \ = self.stats.get(table.name)\n\n        plans = []\n\n        # Option 1: Full table scan\n        scan_cost =\
          \ table_stats.row_count * table_stats.avg_row_size\n        plans.append(Plan(\n            cost=scan_cost,\n  \
          \          rows=self.estimate_rows(stmt.where_clause, table_stats),\n            description=f\"SCAN TABLE {table.name}\"\
          \n        ))\n\n        # Option 2: Index scans\n        for index in self.schema.get_indexes(table.name):\n   \
          \         usability = self.analyze_index_usability(index, stmt.where_clause)\n            if usability:\n      \
          \          idx_stats = self.stats.get_index(index.name)\n\n                # Cost = index lookup + row fetches\n\
          \                selectivity = self.estimate_selectivity(usability, idx_stats)\n                index_cost = math.log2(idx_stats.entries)\
          \ + (selectivity * table_stats.row_count)\n\n                plans.append(Plan(\n                    cost=index_cost,\n\
          \                    rows=int(selectivity * table_stats.row_count),\n                    description=f\"SEARCH {table.name}\
          \ USING INDEX {index.name}\"\n                ))\n\n        # Choose lowest cost plan\n        return min(plans,\
          \ key=lambda p: p.cost)\n\n    def estimate_selectivity(self, condition, stats):\n        '''Estimate fraction of\
          \ rows matching condition'''\n        if condition.op == '=':\n            # Assume uniform distribution\n     \
          \       return 1.0 / stats.distinct_values\n        elif condition.op in ('<', '>', '<=', '>='):\n            #\
          \ Assume 1/3 of range\n            return 0.33\n        elif condition.op == 'BETWEEN':\n            return 0.25\n\
          \        return 0.5  # Default\n\n    def explain(self, stmt):\n        '''Generate EXPLAIN output'''\n        plan\
          \ = self.plan_select(stmt)\n        return [\n            f\"--  --  --  --  Detail\",\n            f\"0   0   0\
          \   EXECUTE\",\n            f\"1   0   0   {plan.description}\",\n            f\"\",\n            f\"Estimated rows:\
          \ {plan.rows}\",\n            f\"Estimated cost: {plan.cost:.2f}\"\n        ]\n\n@dataclass\nclass TableStats:\n\
          \    row_count: int\n    avg_row_size: int\n    page_count: int\n\n@dataclass\nclass IndexStats:\n    entries: int\n\
          \    distinct_values: int\n    depth: int"
      pitfalls:
      - Stale statistics
      - Wrong cardinality estimates
      - Exponential plan space for many JOINs
      concepts:
      - Cost-based optimization
      - Cardinality estimation
      - Join algorithms
      estimated_hours: 8-12
    - id: 9
      name: Transactions (BEGIN/COMMIT/ROLLBACK)
      description: Implement ACID transactions.
      acceptance_criteria:
      - BEGIN starts transaction
      - COMMIT makes changes permanent
      - ROLLBACK undoes all changes since BEGIN
      - Changes not visible to other connections until commit
      - Crash recovery maintains consistency
      hints:
        level1: 'Shadow paging: copy pages before modify, commit = swap pointers.'
        level2: 'Or WAL: write changes to separate log, replay on recovery.'
        level3: 'ACID properties:

          - Atomicity: all or nothing (rollback support)

          - Consistency: constraints always valid

          - Isolation: concurrent txns don''t interfere

          - Durability: committed = permanent (fsync)'
      pitfalls:
      - Partial writes (torn pages)
      - Lock ordering deadlocks
      - Long-running transactions blocking others
      concepts:
      - ACID properties
      - Shadow paging vs WAL
      - Crash recovery
      estimated_hours: 8-12
    - id: 10
      name: WAL Mode
      description: Implement Write-Ahead Logging for better concurrency.
      acceptance_criteria:
      - WAL file stores changes before applying to main DB
      - Readers don't block writers
      - Checkpointing merges WAL into main DB
      - Crash recovery replays committed WAL entries
      hints:
        level1: 'WAL append-only: each commit appends frames with changed pages.'
        level2: 'Read: check WAL first (most recent), fallback to main DB.'
        level3: 'WAL advantages over rollback journal:

          - Readers and writer concurrent

          - Faster commits (sequential writes)

          - Better crash recovery'
      pitfalls:
      - WAL growing unbounded without checkpoint
      - Readers pinning old WAL frames
      - WAL file corruption detection
      concepts:
      - Write-ahead logging
      - MVCC basics
      - Checkpointing
      estimated_hours: 8-12
  build-tcp-stack:
    id: build-tcp-stack
    name: Build Your Own TCP/IP Stack
    description: Implement a TCP/IP network stack from scratch.
    difficulty: expert
    estimated_hours: 60-100
    prerequisites:
    - Networking fundamentals
    - C programming
    - Packet analysis
    - State machines
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible: []
    resources:
    - type: book
      name: TCP/IP Illustrated Vol 1
      url: https://www.amazon.com/TCP-Illustrated-Vol-Addison-Wesley-Professional/dp/0201633469
    - type: rfc
      name: RFC 793 - TCP
      url: https://tools.ietf.org/html/rfc793
    - type: tutorial
      name: Let's code a TCP/IP stack
      url: https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/
    milestones:
    - id: 1
      name: Ethernet & ARP
      description: Handle Ethernet frames and ARP protocol.
      acceptance_criteria:
      - Raw socket/TAP device access
      - Ethernet frame parsing
      - ARP request/reply
      - MAC address table
      hints:
        level1: Use TAP device or raw sockets to send/receive Ethernet frames.
        level2: ARP resolves IP to MAC. Cache results in ARP table.
        level3: "## Ethernet & ARP\n\n```c\n#include <linux/if_tun.h>\n#include <net/if.h>\n\n// Ethernet header\nstruct eth_hdr\
          \ {\n    uint8_t  dst[6];\n    uint8_t  src[6];\n    uint16_t ethertype;\n} __attribute__((packed));\n\n#define\
          \ ETH_P_ARP  0x0806\n#define ETH_P_IP   0x0800\n\n// ARP header\nstruct arp_hdr {\n    uint16_t hwtype;\n    uint16_t\
          \ protype;\n    uint8_t  hwsize;\n    uint8_t  prosize;\n    uint16_t opcode;\n    uint8_t  sender_mac[6];\n   \
          \ uint32_t sender_ip;\n    uint8_t  target_mac[6];\n    uint32_t target_ip;\n} __attribute__((packed));\n\n#define\
          \ ARP_REQUEST 1\n#define ARP_REPLY   2\n\n// ARP cache\nstruct arp_entry {\n    uint32_t ip;\n    uint8_t mac[6];\n\
          \    time_t expires;\n};\n\nstruct arp_entry arp_cache[256];\n\nvoid handle_arp(struct eth_hdr *eth, struct arp_hdr\
          \ *arp) {\n    if (ntohs(arp->opcode) == ARP_REQUEST) {\n        if (arp->target_ip == our_ip) {\n            //\
          \ Send ARP reply\n            struct arp_hdr reply = {\n                .hwtype = htons(1),\n                .protype\
          \ = htons(ETH_P_IP),\n                .hwsize = 6,\n                .prosize = 4,\n                .opcode = htons(ARP_REPLY),\n\
          \                .sender_ip = our_ip,\n                .target_ip = arp->sender_ip,\n            };\n          \
          \  memcpy(reply.sender_mac, our_mac, 6);\n            memcpy(reply.target_mac, arp->sender_mac, 6);\n          \
          \  \n            send_ethernet(arp->sender_mac, ETH_P_ARP, &reply, sizeof(reply));\n        }\n    } else if (ntohs(arp->opcode)\
          \ == ARP_REPLY) {\n        // Update ARP cache\n        arp_cache_add(arp->sender_ip, arp->sender_mac);\n    }\n\
          }\n\nvoid arp_resolve(uint32_t ip, void (*callback)(uint8_t *mac)) {\n    // Check cache first\n    struct arp_entry\
          \ *entry = arp_cache_lookup(ip);\n    if (entry) {\n        callback(entry->mac);\n        return;\n    }\n    \n\
          \    // Send ARP request\n    struct arp_hdr req = {\n        .hwtype = htons(1),\n        .protype = htons(ETH_P_IP),\n\
          \        .hwsize = 6,\n        .prosize = 4,\n        .opcode = htons(ARP_REQUEST),\n        .sender_ip = our_ip,\n\
          \        .target_ip = ip,\n    };\n    memcpy(req.sender_mac, our_mac, 6);\n    memset(req.target_mac, 0, 6);\n\
          \    \n    uint8_t broadcast[6] = {0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF};\n    send_ethernet(broadcast, ETH_P_ARP,\
          \ &req, sizeof(req));\n}\n```"
      pitfalls:
      - Byte order (network vs host)
      - ARP cache poisoning
      - Broadcast handling
      concepts:
      - Layer 2 networking
      - Address resolution
      - Frame parsing
      estimated_hours: 10-15
    - id: 2
      name: IP & ICMP
      description: Implement IP routing and ICMP (ping).
      acceptance_criteria:
      - IP header parsing/construction
      - Checksum calculation
      - Basic routing
      - ICMP echo (ping)
      hints:
        level1: IP header has checksum over header only. ICMP rides on IP.
        level2: Route packets based on destination IP. Handle TTL expiry.
        level3: "## IP & ICMP\n\n```c\nstruct ip_hdr {\n    uint8_t  ihl:4, version:4;\n    uint8_t  tos;\n    uint16_t len;\n\
          \    uint16_t id;\n    uint16_t frag_offset;\n    uint8_t  ttl;\n    uint8_t  protocol;\n    uint16_t checksum;\n\
          \    uint32_t src;\n    uint32_t dst;\n} __attribute__((packed));\n\n#define IP_PROTO_ICMP 1\n#define IP_PROTO_TCP\
          \  6\n#define IP_PROTO_UDP  17\n\nstruct icmp_hdr {\n    uint8_t  type;\n    uint8_t  code;\n    uint16_t checksum;\n\
          \    uint16_t id;\n    uint16_t seq;\n} __attribute__((packed));\n\n#define ICMP_ECHO_REQUEST 8\n#define ICMP_ECHO_REPLY\
          \   0\n\nuint16_t checksum(void *data, int len) {\n    uint32_t sum = 0;\n    uint16_t *ptr = data;\n    \n    while\
          \ (len > 1) {\n        sum += *ptr++;\n        len -= 2;\n    }\n    if (len) {\n        sum += *(uint8_t*)ptr;\n\
          \    }\n    \n    while (sum >> 16) {\n        sum = (sum & 0xFFFF) + (sum >> 16);\n    }\n    \n    return ~sum;\n\
          }\n\nvoid handle_ip(struct eth_hdr *eth, struct ip_hdr *ip) {\n    // Verify checksum\n    if (checksum(ip, ip->ihl\
          \ * 4) != 0) {\n        return;  // Bad checksum\n    }\n    \n    // Check if for us\n    if (ip->dst != our_ip)\
          \ {\n        // Forward or drop\n        return;\n    }\n    \n    switch (ip->protocol) {\n        case IP_PROTO_ICMP:\n\
          \            handle_icmp(ip, (struct icmp_hdr*)((uint8_t*)ip + ip->ihl * 4));\n            break;\n        case\
          \ IP_PROTO_TCP:\n            handle_tcp(ip, (struct tcp_hdr*)((uint8_t*)ip + ip->ihl * 4));\n            break;\n\
          \    }\n}\n\nvoid handle_icmp(struct ip_hdr *ip, struct icmp_hdr *icmp) {\n    if (icmp->type == ICMP_ECHO_REQUEST)\
          \ {\n        // Send echo reply\n        send_icmp_reply(ip->src, icmp->id, icmp->seq,\n                       (uint8_t*)(icmp\
          \ + 1),\n                       ntohs(ip->len) - ip->ihl * 4 - sizeof(struct icmp_hdr));\n    }\n}\n```"
      pitfalls:
      - Checksum calculation errors
      - Fragmentation handling
      - TTL not decremented
      concepts:
      - Layer 3 networking
      - Routing
      - Error handling
      estimated_hours: 12-18
    - id: 3
      name: TCP Connection Management
      description: Implement TCP 3-way handshake and state machine.
      acceptance_criteria:
      - TCP header parsing
      - Connection state machine
      - 3-way handshake
      - Connection termination
      hints:
        level1: 'TCP has many states: LISTEN, SYN_SENT, ESTABLISHED, etc.'
        level2: 'Handshake: SYN -> SYN-ACK -> ACK. Track sequence numbers.'
        level3: "## TCP State Machine\n\n```c\nstruct tcp_hdr {\n    uint16_t src_port;\n    uint16_t dst_port;\n    uint32_t\
          \ seq;\n    uint32_t ack;\n    uint8_t  reserved:4, data_offset:4;\n    uint8_t  flags;\n    uint16_t window;\n\
          \    uint16_t checksum;\n    uint16_t urgent;\n} __attribute__((packed));\n\n#define TCP_FIN 0x01\n#define TCP_SYN\
          \ 0x02\n#define TCP_RST 0x04\n#define TCP_PSH 0x08\n#define TCP_ACK 0x10\n\nenum tcp_state {\n    CLOSED, LISTEN,\
          \ SYN_SENT, SYN_RECEIVED,\n    ESTABLISHED, FIN_WAIT_1, FIN_WAIT_2,\n    CLOSE_WAIT, CLOSING, LAST_ACK, TIME_WAIT\n\
          };\n\nstruct tcp_conn {\n    uint32_t local_ip, remote_ip;\n    uint16_t local_port, remote_port;\n    enum tcp_state\
          \ state;\n    uint32_t snd_una;  // Send unacknowledged\n    uint32_t snd_nxt;  // Send next\n    uint32_t rcv_nxt;\
          \  // Receive next\n    uint16_t rcv_wnd;  // Receive window\n    // ... buffers, timers ...\n};\n\nvoid tcp_input(struct\
          \ ip_hdr *ip, struct tcp_hdr *tcp) {\n    struct tcp_conn *conn = find_connection(ip, tcp);\n    \n    if (!conn)\
          \ {\n        if (tcp->flags & TCP_SYN) {\n            // New connection attempt\n            conn = create_connection(ip,\
          \ tcp);\n            conn->state = SYN_RECEIVED;\n            conn->rcv_nxt = ntohl(tcp->seq) + 1;\n           \
          \ conn->snd_nxt = generate_isn();\n            \n            // Send SYN-ACK\n            send_tcp(conn, TCP_SYN\
          \ | TCP_ACK, NULL, 0);\n            conn->snd_nxt++;\n        }\n        return;\n    }\n    \n    switch (conn->state)\
          \ {\n        case SYN_SENT:\n            if ((tcp->flags & (TCP_SYN | TCP_ACK)) == (TCP_SYN | TCP_ACK)) {\n    \
          \            conn->rcv_nxt = ntohl(tcp->seq) + 1;\n                conn->snd_una = ntohl(tcp->ack);\n          \
          \      conn->state = ESTABLISHED;\n                send_tcp(conn, TCP_ACK, NULL, 0);\n            }\n          \
          \  break;\n            \n        case SYN_RECEIVED:\n            if (tcp->flags & TCP_ACK) {\n                conn->snd_una\
          \ = ntohl(tcp->ack);\n                conn->state = ESTABLISHED;\n            }\n            break;\n          \
          \  \n        case ESTABLISHED:\n            if (tcp->flags & TCP_FIN) {\n                conn->rcv_nxt++;\n    \
          \            conn->state = CLOSE_WAIT;\n                send_tcp(conn, TCP_ACK, NULL, 0);\n            } else if\
          \ (tcp->flags & TCP_ACK) {\n                // Handle data\n                process_tcp_data(conn, tcp);\n     \
          \       }\n            break;\n        // ... other states ...\n    }\n}\n```"
      pitfalls:
      - Wrong state transitions
      - Sequence number wraparound
      - Not handling RST
      concepts:
      - State machines
      - Connection management
      - Reliability
      estimated_hours: 18-25
    - id: 4
      name: TCP Data Transfer & Flow Control
      description: Implement reliable data transfer with flow control.
      acceptance_criteria:
      - Sliding window
      - Retransmission
      - Congestion control (slow start, congestion avoidance)
      - Out-of-order handling
      hints:
        level1: Window size controls how much unacknowledged data can be in flight.
        level2: Retransmit on timeout. Implement fast retransmit on 3 duplicate ACKs.
        level3: "## Sliding Window & Retransmission\n\n```c\nstruct tcp_conn {\n    // ... previous fields ...\n    \n   \
          \ // Send buffer\n    uint8_t *snd_buf;\n    size_t snd_buf_size;\n    \n    // Receive buffer (for out-of-order)\n\
          \    uint8_t *rcv_buf;\n    uint32_t rcv_buf_start;  // seq of first byte\n    \n    // Congestion control\n   \
          \ uint32_t cwnd;           // Congestion window\n    uint32_t ssthresh;       // Slow start threshold\n    \n  \
          \  // RTT estimation\n    uint32_t srtt;           // Smoothed RTT\n    uint32_t rttvar;         // RTT variance\n\
          \    uint32_t rto;            // Retransmission timeout\n    \n    // Retransmission\n    struct timer *retx_timer;\n\
          \    int dup_ack_count;\n};\n\nvoid tcp_send(struct tcp_conn *conn, const void *data, size_t len) {\n    // Copy\
          \ to send buffer\n    memcpy(conn->snd_buf + conn->snd_nxt - conn->snd_una, data, len);\n    \n    // Send what\
          \ window allows\n    size_t can_send = MIN(conn->cwnd, conn->rcv_wnd) - (conn->snd_nxt - conn->snd_una);\n    can_send\
          \ = MIN(can_send, len);\n    \n    if (can_send > 0) {\n        send_tcp(conn, TCP_ACK | TCP_PSH, data, can_send);\n\
          \        conn->snd_nxt += can_send;\n        \n        // Start retransmission timer\n        if (!timer_active(conn->retx_timer))\
          \ {\n            timer_start(conn->retx_timer, conn->rto);\n        }\n    }\n}\n\nvoid tcp_ack_received(struct\
          \ tcp_conn *conn, uint32_t ack_num) {\n    if (ack_num > conn->snd_una) {\n        // New data acknowledged\n  \
          \      size_t acked = ack_num - conn->snd_una;\n        conn->snd_una = ack_num;\n        \n        // Update congestion\
          \ window\n        if (conn->cwnd < conn->ssthresh) {\n            // Slow start: exponential growth\n          \
          \  conn->cwnd += MSS;\n        } else {\n            // Congestion avoidance: linear growth\n            conn->cwnd\
          \ += MSS * MSS / conn->cwnd;\n        }\n        \n        // Reset retransmission timer\n        timer_stop(conn->retx_timer);\n\
          \        if (conn->snd_nxt > conn->snd_una) {\n            timer_start(conn->retx_timer, conn->rto);\n        }\n\
          \        \n        conn->dup_ack_count = 0;\n    } else {\n        // Duplicate ACK\n        conn->dup_ack_count++;\n\
          \        if (conn->dup_ack_count == 3) {\n            // Fast retransmit\n            conn->ssthresh = conn->cwnd\
          \ / 2;\n            conn->cwnd = conn->ssthresh + 3 * MSS;\n            retransmit(conn);\n        }\n    }\n}\n\
          \nvoid retransmit_timeout(struct tcp_conn *conn) {\n    // Timeout: retransmit and back off\n    conn->ssthresh\
          \ = conn->cwnd / 2;\n    conn->cwnd = MSS;  // Reset to slow start\n    conn->rto *= 2;    // Exponential backoff\n\
          \    retransmit(conn);\n}\n```"
      pitfalls:
      - Timer management bugs
      - Window calculation errors
      - Sequence wraparound
      concepts:
      - Flow control
      - Congestion control
      - Reliable delivery
      estimated_hours: 20-30
  build-test-framework:
    id: build-test-framework
    name: Build Your Own Test Framework
    description: Build a test framework like pytest or jest.
    difficulty: expert
    estimated_hours: 40-60
    prerequisites:
    - Reflection/metaprogramming
    - Assertion libraries
    - CLI development
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - Rust
      - Java
    resources:
    - type: code
      name: pytest source
      url: https://github.com/pytest-dev/pytest
    - type: article
      name: Building a Test Framework
      url: https://www.destroyallsoftware.com/screencasts/catalog/building-a-test-framework
    milestones:
    - id: 1
      name: Test Discovery & Execution
      description: Discover and run test functions.
      acceptance_criteria:
      - Find test files (test_*.py)
      - Find test functions (test_*)
      - Run tests with isolation
      - Collect results
      hints:
        level1: Walk directory tree, import test modules, find functions starting with 'test_'.
        level2: Run each test in isolation (fresh module state). Catch exceptions.
        level3: "## Test Discovery\n\n```python\nimport importlib.util\nimport sys\nimport os\nimport traceback\nfrom dataclasses\
          \ import dataclass\nfrom typing import Callable, List\nfrom enum import Enum\n\nclass TestResult(Enum):\n    PASSED\
          \ = 'passed'\n    FAILED = 'failed'\n    ERROR = 'error'\n    SKIPPED = 'skipped'\n\n@dataclass\nclass TestCase:\n\
          \    name: str\n    module: str\n    func: Callable\n\n@dataclass\nclass TestOutcome:\n    test: TestCase\n    result:\
          \ TestResult\n    duration: float\n    error: Exception = None\n    traceback: str = None\n\nclass TestCollector:\n\
          \    def __init__(self, path='.'):\n        self.path = path\n        self.tests: List[TestCase] = []\n    \n  \
          \  def collect(self):\n        for root, dirs, files in os.walk(self.path):\n            # Skip hidden and __pycache__\
          \ directories\n            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n       \
          \     \n            for file in files:\n                if file.startswith('test_') and file.endswith('.py'):\n\
          \                    filepath = os.path.join(root, file)\n                    self.collect_from_file(filepath)\n\
          \        \n        return self.tests\n    \n    def collect_from_file(self, filepath):\n        module_name = os.path.splitext(os.path.basename(filepath))[0]\n\
          \        \n        spec = importlib.util.spec_from_file_location(module_name, filepath)\n        module = importlib.util.module_from_spec(spec)\n\
          \        sys.modules[module_name] = module\n        \n        try:\n            spec.loader.exec_module(module)\n\
          \        except Exception as e:\n            print(f\"Error loading {filepath}: {e}\")\n            return\n   \
          \     \n        for name in dir(module):\n            if name.startswith('test_'):\n                obj = getattr(module,\
          \ name)\n                if callable(obj):\n                    self.tests.append(TestCase(\n                  \
          \      name=name,\n                        module=module_name,\n                        func=obj\n             \
          \       ))\n\nclass TestRunner:\n    def __init__(self):\n        self.outcomes: List[TestOutcome] = []\n    \n\
          \    def run(self, tests: List[TestCase]):\n        for test in tests:\n            outcome = self.run_one(test)\n\
          \            self.outcomes.append(outcome)\n            self.report_one(outcome)\n        \n        return self.outcomes\n\
          \    \n    def run_one(self, test: TestCase) -> TestOutcome:\n        import time\n        start = time.time()\n\
          \        \n        try:\n            test.func()\n            result = TestResult.PASSED\n            error = None\n\
          \            tb = None\n        except AssertionError as e:\n            result = TestResult.FAILED\n          \
          \  error = e\n            tb = traceback.format_exc()\n        except Exception as e:\n            result = TestResult.ERROR\n\
          \            error = e\n            tb = traceback.format_exc()\n        \n        duration = time.time() - start\n\
          \        return TestOutcome(test, result, duration, error, tb)\n    \n    def report_one(self, outcome: TestOutcome):\n\
          \        symbol = {\n            TestResult.PASSED: '.',\n            TestResult.FAILED: 'F',\n            TestResult.ERROR:\
          \ 'E',\n            TestResult.SKIPPED: 's'\n        }[outcome.result]\n        print(symbol, end='', flush=True)\n\
          ```"
      pitfalls:
      - Module import side effects
      - Test isolation
      - Path handling
      concepts:
      - Reflection
      - Module loading
      - Test isolation
      estimated_hours: 10-15
    - id: 2
      name: Assertions & Matchers
      description: Implement rich assertion library.
      acceptance_criteria:
      - Basic assertions (assertEqual, assertTrue)
      - Collection assertions
      - Exception assertions
      - Diff output for failures
      hints:
        level1: Assertions raise AssertionError with helpful messages.
        level2: Show diff for string/collection comparisons. Pretty-print values.
        level3: "## Assertions\n\n```python\nimport difflib\nfrom typing import Any, Type, Callable\n\nclass AssertionHelper:\n\
          \    @staticmethod\n    def assertEqual(actual: Any, expected: Any, msg: str = None):\n        if actual != expected:\n\
          \            diff = AssertionHelper._format_diff(actual, expected)\n            message = msg or f\"\\nExpected:\
          \ {expected!r}\\nActual:   {actual!r}\\n{diff}\"\n            raise AssertionError(message)\n    \n    @staticmethod\n\
          \    def assertTrue(value: Any, msg: str = None):\n        if not value:\n            raise AssertionError(msg or\
          \ f\"Expected truthy value, got {value!r}\")\n    \n    @staticmethod\n    def assertFalse(value: Any, msg: str\
          \ = None):\n        if value:\n            raise AssertionError(msg or f\"Expected falsy value, got {value!r}\"\
          )\n    \n    @staticmethod\n    def assertIn(item: Any, container: Any, msg: str = None):\n        if item not in\
          \ container:\n            raise AssertionError(msg or f\"{item!r} not found in {container!r}\")\n    \n    @staticmethod\n\
          \    def assertRaises(exc_type: Type[Exception]):\n        return _ExceptionContext(exc_type)\n    \n    @staticmethod\n\
          \    def assertAlmostEqual(actual: float, expected: float, places: int = 7):\n        if round(abs(actual - expected),\
          \ places) != 0:\n            raise AssertionError(f\"{actual} != {expected} within {places} places\")\n    \n  \
          \  @staticmethod\n    def _format_diff(actual: Any, expected: Any) -> str:\n        if isinstance(actual, str) and\
          \ isinstance(expected, str):\n            diff = difflib.unified_diff(\n                expected.splitlines(keepends=True),\n\
          \                actual.splitlines(keepends=True),\n                fromfile='expected',\n                tofile='actual'\n\
          \            )\n            return ''.join(diff)\n        \n        if isinstance(actual, (list, tuple)) and isinstance(expected,\
          \ (list, tuple)):\n            diff_lines = []\n            for i, (a, e) in enumerate(zip(actual, expected)):\n\
          \                if a != e:\n                    diff_lines.append(f\"  Index {i}: expected {e!r}, got {a!r}\")\n\
          \            if len(actual) != len(expected):\n                diff_lines.append(f\"  Length: expected {len(expected)},\
          \ got {len(actual)}\")\n            return '\\n'.join(diff_lines)\n        \n        return ''\n\nclass _ExceptionContext:\n\
          \    def __init__(self, exc_type: Type[Exception]):\n        self.exc_type = exc_type\n        self.exception =\
          \ None\n    \n    def __enter__(self):\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n\
          \        if exc_type is None:\n            raise AssertionError(f\"Expected {self.exc_type.__name__} to be raised\"\
          )\n        \n        if not issubclass(exc_type, self.exc_type):\n            raise AssertionError(\n          \
          \      f\"Expected {self.exc_type.__name__}, got {exc_type.__name__}\"\n            )\n        \n        self.exception\
          \ = exc_val\n        return True  # Suppress the exception\n\n# Usage convenience\nassert_equal = AssertionHelper.assertEqual\n\
          assert_true = AssertionHelper.assertTrue\nassert_raises = AssertionHelper.assertRaises\n```"
      pitfalls:
      - Unhelpful error messages
      - Float comparison
      - Exception context
      concepts:
      - Assertions
      - Diff algorithms
      - Context managers
      estimated_hours: 8-12
    - id: 3
      name: Fixtures & Setup/Teardown
      description: Implement test fixtures for setup and cleanup.
      acceptance_criteria:
      - Function-level setup/teardown
      - Module-level setup/teardown
      - Fixture dependencies
      - Fixture scope (function, module, session)
      hints:
        level1: Fixtures are functions that provide test dependencies.
        level2: Use generators for setup/teardown. Scope controls lifetime.
        level3: "## Fixtures\n\n```python\nfrom typing import Dict, Any, Generator\nfrom functools import wraps\nimport inspect\n\
          \n_fixtures: Dict[str, 'Fixture'] = {}\n\nclass Fixture:\n    def __init__(self, func: Callable, scope: str = 'function'):\n\
          \        self.func = func\n        self.scope = scope  # 'function', 'module', 'session'\n        self.name = func.__name__\n\
          \        self._cached_value = None\n        self._finalized = True\n    \n    def get_value(self, context: 'FixtureContext'):\n\
          \        if self.scope != 'function' and not self._finalized:\n            return self._cached_value\n        \n\
          \        # Resolve fixture dependencies\n        sig = inspect.signature(self.func)\n        kwargs = {}\n     \
          \   for param in sig.parameters:\n            if param in _fixtures:\n                kwargs[param] = _fixtures[param].get_value(context)\n\
          \        \n        result = self.func(**kwargs)\n        \n        if inspect.isgenerator(result):\n           \
          \ # Setup/teardown fixture\n            value = next(result)\n            context.add_finalizer(lambda: self._finalize(result))\n\
          \        else:\n            value = result\n        \n        self._cached_value = value\n        self._finalized\
          \ = False\n        return value\n    \n    def _finalize(self, gen: Generator):\n        try:\n            next(gen)\n\
          \        except StopIteration:\n            pass\n        self._finalized = True\n\ndef fixture(scope: str = 'function'):\n\
          \    def decorator(func: Callable):\n        fix = Fixture(func, scope)\n        _fixtures[func.__name__] = fix\n\
          \        return func\n    return decorator\n\nclass FixtureContext:\n    def __init__(self):\n        self.finalizers\
          \ = []\n    \n    def add_finalizer(self, func: Callable):\n        self.finalizers.append(func)\n    \n    def\
          \ teardown(self):\n        for finalizer in reversed(self.finalizers):\n            try:\n                finalizer()\n\
          \            except Exception as e:\n                print(f\"Finalizer error: {e}\")\n        self.finalizers.clear()\n\
          \n# Updated TestRunner\nclass TestRunner:\n    def run_one(self, test: TestCase) -> TestOutcome:\n        context\
          \ = FixtureContext()\n        \n        try:\n            # Inject fixtures into test function\n            sig\
          \ = inspect.signature(test.func)\n            kwargs = {}\n            for param in sig.parameters:\n          \
          \      if param in _fixtures:\n                    kwargs[param] = _fixtures[param].get_value(context)\n       \
          \     \n            test.func(**kwargs)\n            result = TestResult.PASSED\n            error = None\n    \
          \    except AssertionError as e:\n            result = TestResult.FAILED\n            error = e\n        finally:\n\
          \            context.teardown()\n        \n        return TestOutcome(test, result, 0, error)\n\n# Example fixtures\n\
          @fixture(scope='function')\ndef temp_dir():\n    import tempfile\n    import shutil\n    d = tempfile.mkdtemp()\n\
          \    yield d\n    shutil.rmtree(d)\n\n@fixture(scope='session')\ndef database():\n    db = connect_to_test_db()\n\
          \    yield db\n    db.close()\n```"
      pitfalls:
      - Fixture cleanup on error
      - Circular dependencies
      - Scope leaks
      concepts:
      - Dependency injection
      - Resource management
      - Generators
      estimated_hours: 12-18
    - id: 4
      name: Reporting & CLI
      description: Generate test reports and provide CLI interface.
      acceptance_criteria:
      - Console output with colors
      - JUnit XML output
      - Code coverage integration
      - Parallel execution
      - Watch mode
      hints:
        level1: Color-code pass/fail. Show failure details at end.
        level2: JUnit XML is standard format for CI integration.
        level3: "## Reporting & CLI\n\n```python\nimport argparse\nimport xml.etree.ElementTree as ET\nfrom typing import\
          \ List\nimport sys\n\nclass ConsoleReporter:\n    def __init__(self, verbose: bool = False):\n        self.verbose\
          \ = verbose\n        self.colors = sys.stdout.isatty()\n    \n    def _color(self, text: str, color: str) -> str:\n\
          \        if not self.colors:\n            return text\n        colors = {'green': '\\033[92m', 'red': '\\033[91m',\
          \ 'yellow': '\\033[93m', 'reset': '\\033[0m'}\n        return f\"{colors.get(color, '')}{text}{colors['reset']}\"\
          \n    \n    def report_start(self, tests: List[TestCase]):\n        print(f\"\\nCollected {len(tests)} tests\\n\"\
          )\n    \n    def report_test(self, outcome: TestOutcome):\n        if self.verbose:\n            status = {\n  \
          \              TestResult.PASSED: self._color('PASSED', 'green'),\n                TestResult.FAILED: self._color('FAILED',\
          \ 'red'),\n                TestResult.ERROR: self._color('ERROR', 'red'),\n                TestResult.SKIPPED: self._color('SKIPPED',\
          \ 'yellow')\n            }[outcome.result]\n            print(f\"{outcome.test.module}::{outcome.test.name} {status}\"\
          )\n        else:\n            symbol = {\n                TestResult.PASSED: self._color('.', 'green'),\n      \
          \          TestResult.FAILED: self._color('F', 'red'),\n                TestResult.ERROR: self._color('E', 'red'),\n\
          \                TestResult.SKIPPED: self._color('s', 'yellow')\n            }[outcome.result]\n            print(symbol,\
          \ end='', flush=True)\n    \n    def report_summary(self, outcomes: List[TestOutcome]):\n        passed = sum(1\
          \ for o in outcomes if o.result == TestResult.PASSED)\n        failed = sum(1 for o in outcomes if o.result == TestResult.FAILED)\n\
          \        errors = sum(1 for o in outcomes if o.result == TestResult.ERROR)\n        total_time = sum(o.duration\
          \ for o in outcomes)\n        \n        print(f\"\\n\\n{'='*60}\")\n        \n        # Show failures\n        for\
          \ outcome in outcomes:\n            if outcome.result in (TestResult.FAILED, TestResult.ERROR):\n              \
          \  print(f\"\\n{self._color('FAILED', 'red')} {outcome.test.module}::{outcome.test.name}\")\n                print(outcome.traceback)\n\
          \        \n        print(f\"\\n{passed} passed, {failed} failed, {errors} errors in {total_time:.2f}s\")\n\nclass\
          \ JUnitReporter:\n    def generate(self, outcomes: List[TestOutcome], output_file: str):\n        testsuite = ET.Element('testsuite')\n\
          \        testsuite.set('tests', str(len(outcomes)))\n        testsuite.set('failures', str(sum(1 for o in outcomes\
          \ if o.result == TestResult.FAILED)))\n        testsuite.set('errors', str(sum(1 for o in outcomes if o.result ==\
          \ TestResult.ERROR)))\n        \n        for outcome in outcomes:\n            testcase = ET.SubElement(testsuite,\
          \ 'testcase')\n            testcase.set('classname', outcome.test.module)\n            testcase.set('name', outcome.test.name)\n\
          \            testcase.set('time', str(outcome.duration))\n            \n            if outcome.result == TestResult.FAILED:\n\
          \                failure = ET.SubElement(testcase, 'failure')\n                failure.set('message', str(outcome.error))\n\
          \                failure.text = outcome.traceback\n            elif outcome.result == TestResult.ERROR:\n      \
          \          error = ET.SubElement(testcase, 'error')\n                error.set('message', str(outcome.error))\n\
          \                error.text = outcome.traceback\n        \n        tree = ET.ElementTree(testsuite)\n        tree.write(output_file,\
          \ encoding='unicode', xml_declaration=True)\n\n# CLI\ndef main():\n    parser = argparse.ArgumentParser(description='Test\
          \ Framework')\n    parser.add_argument('path', nargs='?', default='.', help='Test directory')\n    parser.add_argument('-v',\
          \ '--verbose', action='store_true')\n    parser.add_argument('--junit-xml', help='Output JUnit XML file')\n    parser.add_argument('-k',\
          \ '--filter', help='Filter tests by name')\n    args = parser.parse_args()\n    \n    collector = TestCollector(args.path)\n\
          \    tests = collector.collect()\n    \n    if args.filter:\n        tests = [t for t in tests if args.filter in\
          \ t.name]\n    \n    reporter = ConsoleReporter(verbose=args.verbose)\n    reporter.report_start(tests)\n    \n\
          \    runner = TestRunner()\n    outcomes = runner.run(tests)\n    \n    reporter.report_summary(outcomes)\n    \n\
          \    if args.junit_xml:\n        JUnitReporter().generate(outcomes, args.junit_xml)\n    \n    sys.exit(0 if all(o.result\
          \ == TestResult.PASSED for o in outcomes) else 1)\n\nif __name__ == '__main__':\n    main()\n```"
      pitfalls:
      - Exit codes
      - Terminal detection
      - XML escaping
      concepts:
      - CLI design
      - Reporting formats
      - CI integration
      estimated_hours: 10-15
  build-text-editor:
    id: build-text-editor
    name: Build Your Own Text Editor
    description: Build a terminal-based text editor from scratch. Based on the kilo editor (~1000 lines of C).
    difficulty: advanced
    estimated_hours: 20-30
    prerequisites:
    - Terminal I/O
    - C or systems language
    - Basic data structures
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible: []
    resources:
    - name: Build Your Own Text Editor
      url: https://viewsourcecode.org/snaptoken/kilo/
      type: tutorial
    - name: antirez/kilo source
      url: https://github.com/antirez/kilo
      type: reference
    - name: Hecto (Rust version)
      url: https://philippflenker.com/hecto/
      type: tutorial
    milestones:
    - id: 1
      name: Raw Mode and Input
      description: Put terminal in raw mode and read keypresses.
      acceptance_criteria:
      - Disables canonical mode (line buffering)
      - Disables echo
      - Reads individual keypresses
      - Handles Ctrl+Q to quit
      - Restores terminal state on exit
      hints:
        level1: Use tcgetattr/tcsetattr to modify terminal settings.
        level2: Disable ICANON and ECHO flags. Save original termios to restore later.
        level3: 'struct termios raw;

          tcgetattr(STDIN_FILENO, &raw);

          raw.c_lflag &= ~(ECHO | ICANON | ISIG | IEXTEN);

          raw.c_iflag &= ~(IXON | ICRNL | BRKINT | INPCK | ISTRIP);

          raw.c_oflag &= ~(OPOST);

          tcsetattr(STDIN_FILENO, TCSAFLUSH, &raw);'
      pitfalls:
      - Not restoring terminal on crash
      - Ctrl+C killing before cleanup
      - Different terminal emulators
      concepts:
      - Terminal modes (canonical vs raw)
      - termios structure
      - Signal handling for cleanup
      estimated_hours: 2-3
    - id: 2
      name: Screen Refresh
      description: Clear screen and position cursor using escape sequences.
      acceptance_criteria:
      - Clears screen on refresh
      - Draws rows with ~ for empty lines
      - Positions cursor correctly
      - Uses escape sequences (VT100)
      hints:
        level1: \x1b[2J clears screen. \x1b[H positions cursor at top-left.
        level2: \x1b[K clears line from cursor. Build screen in buffer, write once.
        level3: 'Common escape sequences:

          \x1b[2J    - Clear entire screen

          \x1b[H     - Move cursor to 1,1

          \x1b[{r};{c}H - Move cursor to row,col

          \x1b[K     - Clear line from cursor

          \x1b[?25l  - Hide cursor

          \x1b[?25h  - Show cursor'
      pitfalls:
      - Flickering (write small chunks vs one big write)
      - Off-by-one in cursor positioning (1-indexed)
      - Screen size detection
      concepts:
      - VT100 escape sequences
      - Screen buffers
      - Terminal graphics
      estimated_hours: 2-3
    - id: 3
      name: File Viewing
      description: Load and display file contents with scrolling.
      acceptance_criteria:
      - Opens file from command line argument
      - Displays file contents
      - Scrolls with arrow keys
      - Shows filename in status bar
      - Handles files larger than screen
      hints:
        level1: Store lines in dynamic array. Track row offset for scrolling.
        level2: Only render visible rows (offset to offset+screen_rows).
        level3: "typedef struct {\n  char *chars;\n  int size;\n} Row;\nRow *rows;\nint numrows;\nint rowoff;  // scroll offset"
      pitfalls:
      - Memory allocation for lines
      - Horizontal scrolling (long lines)
      - Tabs rendering
      concepts:
      - File I/O
      - Dynamic arrays
      - Viewport scrolling
      estimated_hours: 3-4
    - id: 4
      name: Text Editing
      description: Insert and delete characters, handle Enter and Backspace.
      acceptance_criteria:
      - Insert character at cursor
      - Delete with Backspace and Delete keys
      - Enter creates new line
      - Track 'dirty' state (modified)
      hints:
        level1: 'Insert: shift chars right, insert at cursor. Delete: shift chars left.'
        level2: 'Enter: split current line at cursor, insert new row.'
        level3: "void insertChar(Row *row, int at, int c) {\n  row->chars = realloc(row->chars, row->size + 2);\n  memmove(&row->chars[at\
          \ + 1], &row->chars[at], row->size - at + 1);\n  row->chars[at] = c;\n  row->size++;\n}"
      pitfalls:
      - Cursor at end of line edge cases
      - Deleting at beginning of line (join with previous)
      - Memory reallocation
      concepts:
      - Text buffer operations
      - Gap buffer or simple array
      - Change tracking
      estimated_hours: 4-6
    - id: 5
      name: Save and Undo
      description: Save file to disk and implement undo functionality.
      acceptance_criteria:
      - Ctrl+S saves file
      - Prompts for filename if new file
      - Confirms before quit with unsaved changes
      - Basic undo with Ctrl+Z
      hints:
        level1: Convert rows back to string with newlines, write to file.
        level2: 'Undo: keep stack of operations. Each edit pushes inverse operation.'
        level3: 'Undo approaches:

          1. Memento: save entire state (memory heavy)

          2. Command pattern: save operations and inverse

          3. Piece table: inherently supports undo'
      pitfalls:
      - Handling write errors
      - Undo across multiple edits
      - Memory for undo history
      concepts:
      - File writing
      - Undo architectures
      - Command pattern
      estimated_hours: 3-4
    - id: 6
      name: Search
      description: Implement incremental search functionality.
      acceptance_criteria:
      - Ctrl+F enters search mode
      - Highlights matches as you type
      - Enter goes to next match
      - Escape cancels search
      hints:
        level1: Simple strstr() for each line.
        level2: 'Incremental: search on each keypress, restore cursor on cancel.'
        level3: 'For incremental search:

          - Save original cursor position

          - On each char: search forward from current position

          - If found: move cursor, highlight

          - On Enter: stay at match

          - On Escape: restore original position'
      pitfalls:
      - Search wrapping at end of file
      - Case sensitivity
      - Restoring state on cancel
      concepts:
      - Text searching
      - Incremental search UX
      - State management
      estimated_hours: 2-3
    - id: 7
      name: Syntax Highlighting
      description: Add syntax highlighting for common languages.
      acceptance_criteria:
      - Highlights keywords in different color
      - Highlights strings and comments
      - Different rules per file type
      - Multi-line comment handling
      hints:
        level1: For each row, track highlighting state per character.
        level2: 'State machine: normal, string, comment. Different color per state.'
        level3: 'Highlight types:

          - HL_NORMAL

          - HL_KEYWORD1 (if, while, for)

          - HL_KEYWORD2 (int, char, void)

          - HL_STRING

          - HL_COMMENT

          - HL_MLCOMMENT

          - HL_NUMBER'
      pitfalls:
      - Multi-line strings and comments
      - Escape sequences in strings
      - Performance on large files
      concepts:
      - Syntax highlighting algorithms
      - State machines
      - ANSI color codes
      estimated_hours: 4-6
  build-tls:
    id: build-tls
    name: Build Your Own TLS
    description: Implement TLS 1.3 handshake and record layer. Learn modern cryptographic protocols.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - HTTPS client
    - AES implementation
    - Elliptic curve basics
    languages:
      recommended:
      - Rust
      - Go
      - C
      also_possible:
      - Python
      - Java
    resources:
    - type: spec
      name: RFC 8446 - TLS 1.3
      url: https://datatracker.ietf.org/doc/html/rfc8446
    - type: book
      name: Illustrated TLS 1.3
      url: https://tls13.xargs.org/
    milestones:
    - id: 1
      name: Record Layer
      description: Implement TLS record protocol for fragmenting and encrypting data.
      acceptance_criteria:
      - Record header parsing
      - Fragmentation handling
      - Content type handling
      - Record size limits
      hints:
        level1: 'Records have 5-byte header: type(1), version(2), length(2). Max payload 16KB.'
        level2: In TLS 1.3, actual content type is encrypted. Outer shows 'application_data'.
        level3: "class TLSRecord:\n    MAX_PAYLOAD = 16384  # 2^14\n    \n    # Content types\n    CHANGE_CIPHER_SPEC = 20\n\
          \    ALERT = 21\n    HANDSHAKE = 22\n    APPLICATION_DATA = 23\n    \n    def __init__(self, content_type, payload):\n\
          \        self.content_type = content_type\n        self.payload = payload\n    \n    def encode(self):\n       \
          \ # TLS 1.3 uses legacy version 0x0303 (TLS 1.2)\n        return bytes([\n            self.content_type,\n     \
          \       0x03, 0x03,  # Legacy version\n            len(self.payload) >> 8,\n            len(self.payload) & 0xFF\n\
          \        ]) + self.payload\n    \n    @classmethod\n    def parse(cls, data):\n        if len(data) < 5:\n     \
          \       return None, data\n        \n        content_type = data[0]\n        version = (data[1] << 8) | data[2]\n\
          \        length = (data[3] << 8) | data[4]\n        \n        if len(data) < 5 + length:\n            return None,\
          \ data\n        \n        payload = data[5:5+length]\n        return cls(content_type, payload), data[5+length:]\n\
          \nclass RecordLayer:\n    def __init__(self):\n        self.read_cipher = None\n        self.write_cipher = None\n\
          \        self.read_seq = 0\n        self.write_seq = 0\n    \n    def encrypt_record(self, content_type, plaintext):\n\
          \        if self.write_cipher is None:\n            return TLSRecord(content_type, plaintext)\n        \n      \
          \  # TLS 1.3: append real content type to plaintext\n        inner_plaintext = plaintext + bytes([content_type])\n\
          \        \n        # Encrypt with AEAD\n        nonce = self._build_nonce(self.write_seq, self.write_iv)\n     \
          \   ciphertext = self.write_cipher.encrypt(nonce, inner_plaintext, b'')\n        \n        self.write_seq += 1\n\
          \        \n        # Outer record always shows APPLICATION_DATA\n        return TLSRecord(TLSRecord.APPLICATION_DATA,\
          \ ciphertext)"
      pitfalls:
      - Version field confusion
      - Padding in TLS 1.3
      - Sequence number overflow
      concepts:
      - Record protocol
      - AEAD encryption
      - Protocol layering
      estimated_hours: 8-12
    - id: 2
      name: Key Exchange
      description: Implement ECDHE key exchange with X25519.
      acceptance_criteria:
      - X25519 scalar multiplication
      - Key share generation
      - Shared secret derivation
      - Key schedule
      hints:
        level1: 'X25519: generate 32-byte private key, compute public key. Shared secret from ECDH.'
        level2: HKDF-Extract then HKDF-Expand for key derivation. Different keys for client/server.
        level3: "from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PrivateKey\nfrom cryptography.hazmat.primitives.kdf.hkdf\
          \ import HKDFExpand, HKDF\nfrom cryptography.hazmat.primitives import hashes\n\nclass KeyExchange:\n    def __init__(self):\n\
          \        self.private_key = X25519PrivateKey.generate()\n        self.public_key = self.private_key.public_key()\n\
          \    \n    def get_public_bytes(self):\n        return self.public_key.public_bytes_raw()\n    \n    def compute_shared_secret(self,\
          \ peer_public_bytes):\n        from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PublicKey\n  \
          \      peer_key = X25519PublicKey.from_public_bytes(peer_public_bytes)\n        return self.private_key.exchange(peer_key)\n\
          \nclass KeySchedule:\n    def __init__(self, shared_secret, transcript_hash):\n        # Early secret (no PSK)\n\
          \        self.early_secret = self._hkdf_extract(b'\\x00' * 32, b'')\n        \n        # Handshake secret\n    \
          \    derived = self._derive_secret(self.early_secret, b'derived', b'')\n        self.handshake_secret = self._hkdf_extract(derived,\
          \ shared_secret)\n        \n        # Traffic secrets\n        self.client_handshake_secret = self._derive_secret(\n\
          \            self.handshake_secret, b'c hs traffic', transcript_hash)\n        self.server_handshake_secret = self._derive_secret(\n\
          \            self.handshake_secret, b's hs traffic', transcript_hash)\n    \n    def _hkdf_extract(self, salt, ikm):\n\
          \        import hmac\n        return hmac.new(salt, ikm, 'sha256').digest()\n    \n    def _derive_secret(self,\
          \ secret, label, context):\n        return self._hkdf_expand_label(secret, label, context, 32)\n    \n    def _hkdf_expand_label(self,\
          \ secret, label, context, length):\n        # TLS 1.3 specific label format\n        full_label = b'tls13 ' + label\n\
          \        hkdf_label = (\n            length.to_bytes(2, 'big') +\n            bytes([len(full_label)]) + full_label\
          \ +\n            bytes([len(context)]) + context\n        )\n        hkdf = HKDFExpand(hashes.SHA256(), length,\
          \ hkdf_label)\n        return hkdf.derive(secret)"
      pitfalls:
      - Endianness in key encoding
      - Transcript hash timing
      - Label format details
      concepts:
      - ECDH
      - Key derivation
      - Forward secrecy
      estimated_hours: 12-18
    - id: 3
      name: Handshake Protocol
      description: Implement TLS 1.3 full handshake flow.
      acceptance_criteria:
      - ClientHello construction
      - ServerHello parsing
      - EncryptedExtensions
      - Finished message verification
      hints:
        level1: 'ClientHello: version, random, cipher suites, extensions (key_share, supported_versions).'
        level2: After ServerHello, switch to encrypted handshake. Verify Finished with HMAC.
        level3: "class Handshake:\n    # Handshake types\n    CLIENT_HELLO = 1\n    SERVER_HELLO = 2\n    ENCRYPTED_EXTENSIONS\
          \ = 8\n    CERTIFICATE = 11\n    CERTIFICATE_VERIFY = 15\n    FINISHED = 20\n    \n    def __init__(self):\n   \
          \     self.transcript = b''  # All handshake messages\n        self.key_exchange = KeyExchange()\n    \n    def\
          \ create_client_hello(self):\n        # Random\n        client_random = os.urandom(32)\n        \n        # Cipher\
          \ suites (TLS_AES_128_GCM_SHA256)\n        cipher_suites = bytes([0x00, 0x02, 0x13, 0x01])\n        \n        #\
          \ Extensions\n        extensions = b''\n        \n        # supported_versions\n        extensions += self._extension(43,\
          \ bytes([0x02, 0x03, 0x04]))  # TLS 1.3\n        \n        # key_share (X25519)\n        key_share_data = (\n  \
          \          bytes([0x00, 0x1d]) +  # X25519\n            bytes([0x00, 0x20]) +  # Key length\n            self.key_exchange.get_public_bytes()\n\
          \        )\n        extensions += self._extension(51, bytes([0x00, len(key_share_data)]) + key_share_data)\n   \
          \     \n        # signature_algorithms\n        extensions += self._extension(13, bytes([0x00, 0x04, 0x04, 0x03,\
          \ 0x08, 0x04]))  # ECDSA, RSA-PSS\n        \n        # Build ClientHello\n        body = (\n            bytes([0x03,\
          \ 0x03]) +  # Legacy version (TLS 1.2)\n            client_random +\n            bytes([0x00]) +  # Session ID length\n\
          \            cipher_suites +\n            bytes([0x01, 0x00]) +  # Compression (null)\n            len(extensions).to_bytes(2,\
          \ 'big') + extensions\n        )\n        \n        msg = bytes([self.CLIENT_HELLO]) + len(body).to_bytes(3, 'big')\
          \ + body\n        self.transcript += msg\n        return msg\n    \n    def process_server_hello(self, data):\n\
          \        # Parse and extract key_share\n        # ... update transcript\n        # Derive handshake keys\n     \
          \   pass\n    \n    def verify_finished(self, received_verify_data):\n        # Compute expected verify_data\n \
          \       transcript_hash = hashlib.sha256(self.transcript).digest()\n        finished_key = self.key_schedule._hkdf_expand_label(\n\
          \            self.key_schedule.server_handshake_secret,\n            b'finished', b'', 32\n        )\n        expected\
          \ = hmac.new(finished_key, transcript_hash, 'sha256').digest()\n        return hmac.compare_digest(received_verify_data,\
          \ expected)"
      pitfalls:
      - Extension ordering
      - Transcript hash calculation
      - Cipher suite negotiation
      concepts:
      - Protocol negotiation
      - Message authentication
      - State machine
      estimated_hours: 15-25
    - id: 4
      name: Certificate Verification
      description: Verify server certificate chain and signature.
      acceptance_criteria:
      - X.509 parsing
      - Chain building
      - Signature verification
      - Certificate Verify message
      hints:
        level1: Parse certificates from DER. Verify chain up to trusted root.
        level2: CertificateVerify signs transcript hash. Check signature algorithm.
        level3: "from cryptography import x509\nfrom cryptography.hazmat.primitives.asymmetric import padding, ec\n\nclass\
          \ CertificateVerifier:\n    def __init__(self, trusted_roots):\n        self.trusted_roots = trusted_roots  # List\
          \ of CA certs\n    \n    def verify_chain(self, cert_chain, hostname):\n        if not cert_chain:\n           \
          \ raise TLSError('Empty certificate chain')\n        \n        # Parse certificates\n        certs = [x509.load_der_x509_certificate(c)\
          \ for c in cert_chain]\n        \n        # Verify hostname\n        leaf = certs[0]\n        self._verify_hostname(leaf,\
          \ hostname)\n        \n        # Build and verify chain\n        for i in range(len(certs) - 1):\n            issuer\
          \ = certs[i + 1]\n            self._verify_signature(certs[i], issuer)\n        \n        # Check root is trusted\n\
          \        root = certs[-1]\n        if not self._is_trusted(root):\n            raise TLSError('Untrusted root certificate')\n\
          \        \n        return leaf.public_key()\n    \n    def _verify_hostname(self, cert, hostname):\n        # Check\
          \ SAN extension\n        try:\n            san = cert.extensions.get_extension_for_class(x509.SubjectAlternativeName)\n\
          \            names = san.value.get_values_for_type(x509.DNSName)\n            if hostname not in names and not self._matches_wildcard(hostname,\
          \ names):\n                raise TLSError(f'Hostname {hostname} not in certificate')\n        except x509.ExtensionNotFound:\n\
          \            # Fall back to CN\n            cn = cert.subject.get_attributes_for_oid(x509.oid.NameOID.COMMON_NAME)\n\
          \            if not cn or cn[0].value != hostname:\n                raise TLSError('Hostname mismatch')\n    \n\
          \    def verify_certificate_verify(self, signature, algorithm, transcript_hash, public_key):\n        # TLS 1.3\
          \ signature context\n        context = b' ' * 64 + b'TLS 1.3, server CertificateVerify' + b'\\x00' + transcript_hash\n\
          \        \n        if isinstance(public_key, ec.EllipticCurvePublicKey):\n            public_key.verify(signature,\
          \ context, ec.ECDSA(hashes.SHA256()))\n        else:\n            public_key.verify(signature, context, padding.PSS(...))"
      pitfalls:
      - Chain validation order
      - Wildcard matching
      - Signature algorithm selection
      concepts:
      - PKI
      - X.509
      - Certificate validation
      estimated_hours: 15-25
  build-transformer:
    id: build-transformer
    name: Build Your Own Transformer
    description: Implement a GPT-style transformer from scratch.
    difficulty: expert
    estimated_hours: 50-80
    prerequisites:
    - Neural networks
    - Attention mechanism basics
    - NLP fundamentals
    - PyTorch/TensorFlow
    languages:
      recommended:
      - Python
      also_possible:
      - Julia
      - Rust
    resources:
    - type: paper
      name: Attention Is All You Need
      url: https://arxiv.org/abs/1706.03762
    - type: video
      name: Let's build GPT by Karpathy
      url: https://www.youtube.com/watch?v=kCc8FmEb1nY
    - type: code
      name: minGPT
      url: https://github.com/karpathy/minGPT
    milestones:
    - id: 1
      name: Self-Attention
      description: Implement scaled dot-product attention.
      acceptance_criteria:
      - Query, Key, Value projections
      - Attention scores computation
      - Softmax normalization
      - Causal masking for autoregressive
      hints:
        level1: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V
        level2: 'For autoregressive: mask future positions with -inf before softmax.'
        level3: "## Self-Attention\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport\
          \ math\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n\
          \        assert embed_dim % num_heads == 0\n        \n        self.embed_dim = embed_dim\n        self.num_heads\
          \ = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.q_proj = nn.Linear(embed_dim,\
          \ embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim,\
          \ embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n    \n    def forward(self, x, mask=None):\n\
          \        batch_size, seq_len, _ = x.shape\n        \n        # Project to Q, K, V\n        Q = self.q_proj(x)  #\
          \ (batch, seq, embed)\n        K = self.k_proj(x)\n        V = self.v_proj(x)\n        \n        # Reshape for multi-head:\
          \ (batch, heads, seq, head_dim)\n        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,\
          \ 2)\n        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size,\
          \ seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Scaled dot-product attention\n  \
          \      scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        # Apply causal\
          \ mask\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n      \
          \  \n        attn_weights = F.softmax(scores, dim=-1)\n        \n        # Apply attention to values\n        context\
          \ = torch.matmul(attn_weights, V)  # (batch, heads, seq, head_dim)\n        \n        # Concatenate heads\n    \
          \    context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n        \n      \
          \  return self.out_proj(context)\n\ndef create_causal_mask(seq_len):\n    # Lower triangular matrix (1s where we\
          \ can attend)\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask.unsqueeze(0).unsqueeze(0)  #\
          \ (1, 1, seq, seq)\n```"
      pitfalls:
      - Wrong attention dimension
      - Forgetting to scale
      - Mask applied after softmax
      concepts:
      - Attention mechanism
      - Multi-head attention
      - Masking
      estimated_hours: 10-15
    - id: 2
      name: Transformer Block
      description: Build a complete transformer block with FFN and normalization.
      acceptance_criteria:
      - Feed-forward network
      - Layer normalization
      - Residual connections
      - Dropout
      hints:
        level1: 'Block: x -> LayerNorm -> Attention -> Residual -> LayerNorm -> FFN -> Residual'
        level2: FFN typically expands 4x then projects back. Use GELU activation.
        level3: "## Transformer Block\n\n```python\nclass FeedForward(nn.Module):\n    def __init__(self, embed_dim, hidden_dim,\
          \ dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n        self.fc2\
          \ = nn.Linear(hidden_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n\
          \        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return\
          \ x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n\
          \        super().__init__()\n        self.attention = SelfAttention(embed_dim, num_heads)\n        self.ffn = FeedForward(embed_dim,\
          \ ff_hidden_dim, dropout)\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n\
          \        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n    \n    def forward(self,\
          \ x, mask=None):\n        # Pre-norm architecture (GPT-2 style)\n        attn_out = self.attention(self.ln1(x),\
          \ mask)\n        x = x + self.dropout1(attn_out)\n        \n        ffn_out = self.ffn(self.ln2(x))\n        x =\
          \ x + self.dropout2(ffn_out)\n        \n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, vocab_size,\
          \ embed_dim, num_heads, num_layers, max_seq_len, dropout=0.1):\n        super().__init__()\n        self.embed_dim\
          \ = embed_dim\n        self.max_seq_len = max_seq_len\n        \n        self.token_embedding = nn.Embedding(vocab_size,\
          \ embed_dim)\n        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\
          \        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, embed_dim *\
          \ 4, dropout)\n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(embed_dim)\n\
          \        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n        \n        # Weight tying\n       \
          \ self.lm_head.weight = self.token_embedding.weight\n    \n    def forward(self, idx):\n        batch_size, seq_len\
          \ = idx.shape\n        \n        # Embeddings\n        tok_emb = self.token_embedding(idx)\n        pos = torch.arange(0,\
          \ seq_len, device=idx.device)\n        pos_emb = self.position_embedding(pos)\n        x = self.dropout(tok_emb\
          \ + pos_emb)\n        \n        # Causal mask\n        mask = create_causal_mask(seq_len).to(idx.device)\n     \
          \   \n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x, mask)\n        \n\
          \        x = self.ln_final(x)\n        logits = self.lm_head(x)\n        \n        return logits\n```"
      pitfalls:
      - Pre-norm vs post-norm confusion
      - Missing residual connections
      - Wrong FFN expansion ratio
      concepts:
      - Transformer architecture
      - Layer normalization
      - Residual learning
      estimated_hours: 12-18
    - id: 3
      name: Training Pipeline
      description: Implement training with language modeling objective.
      acceptance_criteria:
      - Cross-entropy loss for next token
      - Gradient clipping
      - Learning rate warmup & decay
      - Mixed precision training
      hints:
        level1: 'Shift labels by 1: predict token[i+1] from token[0:i].'
        level2: Use cosine learning rate schedule with warmup.
        level3: "## Training\n\n```python\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\n\n\
          class GPTTrainer:\n    def __init__(self, model, train_data, config):\n        self.model = model\n        self.train_data\
          \ = train_data\n        self.config = config\n        \n        self.optimizer = optim.AdamW(\n            model.parameters(),\n\
          \            lr=config.learning_rate,\n            betas=(0.9, 0.95),\n            weight_decay=config.weight_decay\n\
          \        )\n        \n        self.scaler = GradScaler()  # For mixed precision\n    \n    def get_lr(self, step):\n\
          \        # Linear warmup then cosine decay\n        warmup_steps = self.config.warmup_steps\n        max_steps =\
          \ self.config.max_steps\n        min_lr = self.config.learning_rate / 10\n        \n        if step < warmup_steps:\n\
          \            return self.config.learning_rate * step / warmup_steps\n        elif step > max_steps:\n          \
          \  return min_lr\n        else:\n            decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n\
          \            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n            return min_lr + coeff * (self.config.learning_rate\
          \ - min_lr)\n    \n    def train_step(self, batch, step):\n        # Update learning rate\n        lr = self.get_lr(step)\n\
          \        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        \n       \
          \ self.model.train()\n        x, y = batch  # x: input tokens, y: target tokens (shifted by 1)\n        \n     \
          \   with autocast():  # Mixed precision\n            logits = self.model(x)\n            loss = F.cross_entropy(\n\
          \                logits.view(-1, logits.size(-1)),\n                y.view(-1),\n                ignore_index=-100\
          \  # Padding token\n            )\n        \n        self.optimizer.zero_grad()\n        self.scaler.scale(loss).backward()\n\
          \        \n        # Gradient clipping\n        self.scaler.unscale_(self.optimizer)\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(),\
          \ self.config.grad_clip)\n        \n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n   \
          \     \n        return loss.item()\n    \n    def train(self):\n        step = 0\n        for epoch in range(self.config.epochs):\n\
          \            for batch in self.train_data:\n                loss = self.train_step(batch, step)\n              \
          \  step += 1\n                \n                if step % 100 == 0:\n                    print(f\"Step {step}, Loss:\
          \ {loss:.4f}, LR: {self.get_lr(step):.6f}\")\n                \n                if step >= self.config.max_steps:\n\
          \                    return\n```"
      pitfalls:
      - Label shifting wrong
      - No gradient clipping causing instability
      - Wrong loss computation
      concepts:
      - Language modeling
      - Learning rate scheduling
      - Mixed precision
      estimated_hours: 12-18
    - id: 4
      name: Text Generation
      description: Implement autoregressive text generation.
      acceptance_criteria:
      - Greedy decoding
      - Temperature sampling
      - Top-k sampling
      - Top-p (nucleus) sampling
      - KV cache for efficient generation
      hints:
        level1: Generate one token at a time, append to input, repeat.
        level2: 'KV cache: store key/value from previous tokens to avoid recomputation.'
        level3: "## Text Generation\n\n```python\nclass GPT(nn.Module):\n    # ... previous code ...\n    \n    @torch.no_grad()\n\
          \    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None):\n        for _ in range(max_new_tokens):\n\
          \            # Crop to max sequence length\n            idx_cond = idx if idx.size(1) <= self.max_seq_len else idx[:,\
          \ -self.max_seq_len:]\n            \n            # Forward pass\n            logits = self(idx_cond)\n         \
          \   logits = logits[:, -1, :] / temperature  # Last position only\n            \n            # Apply top-k\n   \
          \         if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n      \
          \          logits[logits < v[:, [-1]]] = float('-inf')\n            \n            # Apply top-p (nucleus sampling)\n\
          \            if top_p is not None:\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n\
          \                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n                \n \
          \               # Remove tokens with cumulative probability above threshold\n                sorted_indices_to_remove\
          \ = cumulative_probs > top_p\n                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n\
          \                sorted_indices_to_remove[:, 0] = 0\n                \n                indices_to_remove = sorted_indices_to_remove.scatter(1,\
          \ sorted_indices, sorted_indices_to_remove)\n                logits[indices_to_remove] = float('-inf')\n       \
          \     \n            # Sample\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs,\
          \ num_samples=1)\n            \n            idx = torch.cat((idx, idx_next), dim=1)\n        \n        return idx\n\
          \n# KV Cache for efficient generation\nclass CachedSelfAttention(nn.Module):\n    def __init__(self, embed_dim,\
          \ num_heads):\n        super().__init__()\n        # ... same as SelfAttention ...\n    \n    def forward(self,\
          \ x, kv_cache=None):\n        batch_size, seq_len, _ = x.shape\n        \n        Q = self.q_proj(x)\n        K\
          \ = self.k_proj(x)\n        V = self.v_proj(x)\n        \n        if kv_cache is not None:\n            # Append\
          \ new K, V to cache\n            cached_k, cached_v = kv_cache\n            K = torch.cat([cached_k, K], dim=1)\n\
          \            V = torch.cat([cached_v, V], dim=1)\n        \n        new_cache = (K, V)\n        \n        # Reshape\
          \ for multi-head\n        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K =\
          \ K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads,\
          \ self.head_dim).transpose(1, 2)\n        \n        # Attention (Q only attends to available K, V)\n        scores\
          \ = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attn_weights = F.softmax(scores, dim=-1)\n\
          \        context = torch.matmul(attn_weights, V)\n        \n        context = context.transpose(1, 2).contiguous().view(batch_size,\
          \ -1, self.embed_dim)\n        return self.out_proj(context), new_cache\n```"
      pitfalls:
      - Repetitive text without sampling
      - KV cache dimension mismatch
      - Temperature of 0 causing division
      concepts:
      - Autoregressive generation
      - Sampling strategies
      - KV caching
      estimated_hours: 12-18
  build-web-framework:
    id: build-web-framework
    name: Build Your Own Web Framework
    description: Build a web framework like Express or Django with routing, middleware, and templating.
    difficulty: expert
    estimated_hours: 40-70
    prerequisites:
    - HTTP server
    - Request/response handling
    - Basic routing
    languages:
      recommended:
      - JavaScript/Node.js
      - Python
      - Go
      also_possible:
      - Ruby
      - Rust
    resources:
    - type: code
      name: Express.js source
      url: https://github.com/expressjs/express
    - type: article
      name: Build Express from Scratch
      url: https://www.freecodecamp.org/news/express-explained-with-examples-installation-routing-middleware-and-more/
    milestones:
    - id: 1
      name: Routing
      description: Implement URL routing with parameters and methods.
      acceptance_criteria:
      - Method-based routing (GET, POST, etc.)
      - Path parameters (/users/:id)
      - Query string parsing
      - Route matching
      hints:
        level1: Store routes as array of { method, pattern, handler }. Match in order.
        level2: Convert path patterns to regex. Capture groups for parameters.
        level3: "class Router {\n    constructor() {\n        this.routes = [];\n    }\n    \n    addRoute(method, path, handler)\
          \ {\n        const { pattern, paramNames } = this.compilePath(path);\n        this.routes.push({ method, pattern,\
          \ paramNames, handler });\n    }\n    \n    compilePath(path) {\n        const paramNames = [];\n        \n    \
          \    // Convert /users/:id/posts/:postId to regex\n        const pattern = path.replace(/:([^/]+)/g, (_, name) =>\
          \ {\n            paramNames.push(name);\n            return '([^/]+)';\n        });\n        \n        return {\n\
          \            pattern: new RegExp(`^${pattern}$`),\n            paramNames\n        };\n    }\n    \n    match(method,\
          \ url) {\n        const [path, queryString] = url.split('?');\n        \n        for (const route of this.routes)\
          \ {\n            if (route.method !== method && route.method !== 'ALL') continue;\n            \n            const\
          \ match = path.match(route.pattern);\n            if (match) {\n                // Extract params\n            \
          \    const params = {};\n                route.paramNames.forEach((name, i) => {\n                    params[name]\
          \ = decodeURIComponent(match[i + 1]);\n                });\n                \n                // Parse query string\n\
          \                const query = this.parseQuery(queryString);\n                \n                return { handler:\
          \ route.handler, params, query };\n            }\n        }\n        \n        return null;\n    }\n    \n    parseQuery(queryString)\
          \ {\n        if (!queryString) return {};\n        \n        return queryString.split('&').reduce((acc, pair) =>\
          \ {\n            const [key, value] = pair.split('=').map(decodeURIComponent);\n            acc[key] = value;\n\
          \            return acc;\n        }, {});\n    }\n    \n    // Convenience methods\n    get(path, handler) { this.addRoute('GET',\
          \ path, handler); }\n    post(path, handler) { this.addRoute('POST', path, handler); }\n    put(path, handler) {\
          \ this.addRoute('PUT', path, handler); }\n    delete(path, handler) { this.addRoute('DELETE', path, handler); }\n\
          }"
      pitfalls:
      - Route ordering matters
      - URL encoding
      - Trailing slashes
      concepts:
      - URL routing
      - Pattern matching
      - HTTP methods
      estimated_hours: 6-10
    - id: 2
      name: Middleware
      description: Implement middleware pipeline for request processing.
      acceptance_criteria:
      - Middleware chain
      - next() function
      - Error handling middleware
      - Async middleware support
      hints:
        level1: Middleware is function(req, res, next). Call next() to continue chain.
        level2: 'Error middleware has 4 params: (err, req, res, next). Skip to error handlers on throw.'
        level3: "class Application {\n    constructor() {\n        this.middleware = [];\n        this.router = new Router();\n\
          \    }\n    \n    use(pathOrFn, fn) {\n        if (typeof pathOrFn === 'function') {\n            this.middleware.push({\
          \ path: '/', handler: pathOrFn });\n        } else {\n            this.middleware.push({ path: pathOrFn, handler:\
          \ fn });\n        }\n    }\n    \n    async handleRequest(req, res) {\n        // Enhance request/response\n   \
          \     req.params = {};\n        req.query = {};\n        res.json = (data) => {\n            res.setHeader('Content-Type',\
          \ 'application/json');\n            res.end(JSON.stringify(data));\n        };\n        res.status = (code) => {\
          \ res.statusCode = code; return res; };\n        \n        // Collect applicable middleware\n        const stack\
          \ = [\n            ...this.middleware.filter(m => req.url.startsWith(m.path)),\n            { handler: this.routeHandler.bind(this)\
          \ }\n        ];\n        \n        let index = 0;\n        let error = null;\n        \n        const next = async\
          \ (err) => {\n            if (err) error = err;\n            \n            while (index < stack.length) {\n    \
          \            const layer = stack[index++];\n                const handler = layer.handler;\n                \n \
          \               try {\n                    if (error) {\n                        // Error handler has 4 params\n\
          \                        if (handler.length === 4) {\n                            await handler(error, req, res,\
          \ next);\n                            error = null;\n                            return;\n                     \
          \   }\n                    } else {\n                        if (handler.length < 4) {\n                       \
          \     await handler(req, res, next);\n                            return;\n                        }\n         \
          \           }\n                } catch (e) {\n                    error = e;\n                }\n            }\n\
          \            \n            // Unhandled error\n            if (error) {\n                res.statusCode = 500;\n\
          \                res.end('Internal Server Error');\n            }\n        };\n        \n        await next();\n\
          \    }\n    \n    routeHandler(req, res, next) {\n        const match = this.router.match(req.method, req.url);\n\
          \        if (match) {\n            req.params = match.params;\n            req.query = match.query;\n          \
          \  match.handler(req, res, next);\n        } else {\n            res.statusCode = 404;\n            res.end('Not\
          \ Found');\n        }\n    }\n}"
      pitfalls:
      - Calling next multiple times
      - Async error handling
      - Middleware ordering
      concepts:
      - Middleware pattern
      - Pipeline
      - Error propagation
      estimated_hours: 10-15
    - id: 3
      name: Request/Response Enhancement
      description: Add convenience methods and body parsing.
      acceptance_criteria:
      - Body parsing (JSON, form)
      - Response helpers (json, redirect, send)
      - Cookie handling
      - Headers management
      hints:
        level1: Body comes as stream. Buffer chunks, parse based on Content-Type.
        level2: Add methods to res object for common patterns. Handle content negotiation.
        level3: "// Body parser middleware\nfunction bodyParser() {\n    return async (req, res, next) => {\n        const\
          \ contentType = req.headers['content-type'] || '';\n        \n        // Collect body\n        const chunks = [];\n\
          \        for await (const chunk of req) {\n            chunks.push(chunk);\n        }\n        const body = Buffer.concat(chunks).toString();\n\
          \        \n        if (contentType.includes('application/json')) {\n            try {\n                req.body\
          \ = JSON.parse(body);\n            } catch (e) {\n                req.body = {};\n            }\n        } else\
          \ if (contentType.includes('application/x-www-form-urlencoded')) {\n            req.body = Object.fromEntries(new\
          \ URLSearchParams(body));\n        } else {\n            req.body = body;\n        }\n        \n        next();\n\
          \    };\n}\n\n// Response enhancements\nfunction enhanceResponse(res) {\n    res.json = function(data) {\n     \
          \   this.setHeader('Content-Type', 'application/json');\n        this.end(JSON.stringify(data));\n        return\
          \ this;\n    };\n    \n    res.send = function(data) {\n        if (typeof data === 'object') {\n            return\
          \ this.json(data);\n        }\n        this.setHeader('Content-Type', 'text/html');\n        this.end(String(data));\n\
          \        return this;\n    };\n    \n    res.redirect = function(url, status = 302) {\n        this.statusCode =\
          \ status;\n        this.setHeader('Location', url);\n        this.end();\n        return this;\n    };\n    \n \
          \   res.cookie = function(name, value, options = {}) {\n        let cookie = `${name}=${encodeURIComponent(value)}`;\n\
          \        if (options.maxAge) cookie += `; Max-Age=${options.maxAge}`;\n        if (options.httpOnly) cookie += ';\
          \ HttpOnly';\n        if (options.secure) cookie += '; Secure';\n        if (options.path) cookie += `; Path=${options.path}`;\n\
          \        this.setHeader('Set-Cookie', cookie);\n        return this;\n    };\n}\n\n// Cookie parser\nfunction cookieParser()\
          \ {\n    return (req, res, next) => {\n        req.cookies = {};\n        const cookieHeader = req.headers.cookie;\n\
          \        if (cookieHeader) {\n            cookieHeader.split(';').forEach(cookie => {\n                const [name,\
          \ value] = cookie.trim().split('=');\n                req.cookies[name] = decodeURIComponent(value);\n         \
          \   });\n        }\n        next();\n    };\n}"
      pitfalls:
      - Large body handling
      - Content-Type edge cases
      - Cookie security
      concepts:
      - Request parsing
      - Response helpers
      - HTTP cookies
      estimated_hours: 8-12
    - id: 4
      name: Template Engine
      description: Implement a simple template engine for HTML rendering.
      acceptance_criteria:
      - Variable interpolation
      - Control flow (if, for)
      - Includes/partials
      - Escaping
      hints:
        level1: Replace {{ variable }} with values. Compile template to function for performance.
        level2: Parse control structures into AST. Generate JavaScript code to build string.
        level3: "class TemplateEngine {\n    constructor(options = {}) {\n        this.cache = new Map();\n        this.viewsDir\
          \ = options.views || './views';\n    }\n    \n    compile(template) {\n        // Convert template to function\n\
          \        let code = 'let __output = [];\\n';\n        let cursor = 0;\n        \n        // Match {{ }}, {% %},\
          \ {# #}\n        const regex = /\\{\\{([\\s\\S]+?)\\}\\}|\\{%([\\s\\S]+?)%\\}|\\{#[\\s\\S]+?#\\}/g;\n        let\
          \ match;\n        \n        while ((match = regex.exec(template)) !== null) {\n            // Add literal text before\
          \ match\n            if (match.index > cursor) {\n                const text = template.slice(cursor, match.index);\n\
          \                code += `__output.push(${JSON.stringify(text)});\\n`;\n            }\n            \n          \
          \  if (match[1]) {\n                // {{ expression }} - output with escaping\n                const expr = match[1].trim();\n\
          \                code += `__output.push(__escape(${expr}));\\n`;\n            } else if (match[2]) {\n         \
          \       // {% statement %}\n                const stmt = match[2].trim();\n                \n                if\
          \ (stmt.startsWith('if ')) {\n                    code += `if (${stmt.slice(3)}) {\\n`;\n                } else\
          \ if (stmt === 'endif') {\n                    code += '}\\n';\n                } else if (stmt.startsWith('for\
          \ ')) {\n                    // {% for item in items %}\n                    const forMatch = stmt.match(/for (\\\
          w+) in (\\w+)/);\n                    if (forMatch) {\n                        code += `for (const ${forMatch[1]}\
          \ of ${forMatch[2]}) {\\n`;\n                    }\n                } else if (stmt === 'endfor') {\n          \
          \          code += '}\\n';\n                } else if (stmt.startsWith('include ')) {\n                    const\
          \ includePath = stmt.slice(8).trim().replace(/['\"`]/g, '');\n                    code += `__output.push(__include('${includePath}',\
          \ __context));\\n`;\n                }\n            }\n            // {# comments #} are ignored\n            \n\
          \            cursor = match.index + match[0].length;\n        }\n        \n        // Add remaining text\n     \
          \   if (cursor < template.length) {\n            code += `__output.push(${JSON.stringify(template.slice(cursor))});\\\
          n`;\n        }\n        \n        code += 'return __output.join(\"\");';\n        \n        // Create function\n\
          \        return new Function('__context', '__escape', '__include', `\n            with (__context) {\n         \
          \       ${code}\n            }\n        `);\n    }\n    \n    render(templatePath, context = {}) {\n        if (!this.cache.has(templatePath))\
          \ {\n            const fullPath = path.join(this.viewsDir, templatePath);\n            const template = fs.readFileSync(fullPath,\
          \ 'utf-8');\n            this.cache.set(templatePath, this.compile(template));\n        }\n        \n        const\
          \ fn = this.cache.get(templatePath);\n        \n        const escape = (str) => String(str)\n            .replace(/&/g,\
          \ '&amp;')\n            .replace(/</g, '&lt;')\n            .replace(/>/g, '&gt;')\n            .replace(/\"/g,\
          \ '&quot;');\n        \n        const include = (path, ctx) => this.render(path, ctx);\n        \n        return\
          \ fn(context, escape, include);\n    }\n}"
      pitfalls:
      - XSS prevention
      - Template injection
      - Performance with large templates
      concepts:
      - Template compilation
      - Code generation
      - HTML escaping
      estimated_hours: 16-33
  bytecode-compiler:
    id: bytecode-compiler
    name: Bytecode Compiler
    description: Compile AST to bytecode for a stack-based VM. Learn code generation, instruction encoding, and optimization.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - AST builder
    - Bytecode VM
    - Stack-based execution
    languages:
      recommended:
      - Python
      - Java
      - Rust
      also_possible:
      - C
      - Go
      - TypeScript
    resources:
    - type: book
      name: Crafting Interpreters - Compiling Expressions
      url: https://craftinginterpreters.com/compiling-expressions.html
    - type: article
      name: A Python Interpreter Written in Python
      url: https://aosabook.org/en/500L/a-python-interpreter-written-in-python.html
    milestones:
    - id: 1
      name: Expression Compilation
      description: Compile arithmetic and boolean expressions to bytecode.
      acceptance_criteria:
      - Emit opcodes for literals
      - Binary operations
      - Unary operations
      - Handle operator precedence via AST
      hints:
        level1: 'Post-order traversal: compile children first, then emit operator.'
        level2: Literals push to stack. Binary ops pop 2, push 1 result.
        level3: "class Compiler:\n    def __init__(self):\n        self.code = []  # bytecode\n        self.constants = []\
          \  # constant pool\n    \n    def compile(self, node):\n        method = f'compile_{type(node).__name__}'\n    \
          \    return getattr(self, method)(node)\n    \n    def compile_Number(self, node):\n        idx = self.add_constant(node.value)\n\
          \        self.emit(OpCode.CONST, idx)\n    \n    def compile_BinaryOp(self, node):\n        self.compile(node.left)\n\
          \        self.compile(node.right)\n        \n        ops = {\n            '+': OpCode.ADD,\n            '-': OpCode.SUB,\n\
          \            '*': OpCode.MUL,\n            '/': OpCode.DIV,\n            '<': OpCode.LT,\n            '>': OpCode.GT,\n\
          \            '==': OpCode.EQ,\n        }\n        self.emit(ops[node.op])\n    \n    def compile_UnaryOp(self, node):\n\
          \        self.compile(node.operand)\n        if node.op == '-':\n            self.emit(OpCode.NEG)\n        elif\
          \ node.op == '!':\n            self.emit(OpCode.NOT)\n    \n    def emit(self, opcode, operand=None):\n        self.code.append(opcode)\n\
          \        if operand is not None:\n            self.code.append(operand)\n    \n    def add_constant(self, value):\n\
          \        self.constants.append(value)\n        return len(self.constants) - 1"
      pitfalls:
      - Wrong operand order for non-commutative ops
      - Constant pool indexing
      - Forgetting to handle all operators
      concepts:
      - Tree traversal
      - Stack-based code generation
      - Constant pools
      estimated_hours: 4-6
    - id: 2
      name: Variables and Assignment
      description: Compile variable declarations, assignments, and references.
      acceptance_criteria:
      - Local variable slots
      - Variable resolution
      - Assignment expressions
      - Scoped variable shadowing
      hints:
        level1: Track variables in a symbol table mapping name to slot index.
        level2: Use LOAD_LOCAL/STORE_LOCAL with slot index. Handle nested scopes.
        level3: "class Compiler:\n    def __init__(self):\n        self.code = []\n        self.locals = []  # Stack of scope\
          \ dicts\n        self.push_scope()\n    \n    def push_scope(self):\n        self.locals.append({})\n    \n    def\
          \ pop_scope(self):\n        scope = self.locals.pop()\n        # Emit pops for local variables\n        for _ in\
          \ scope:\n            self.emit(OpCode.POP)\n    \n    def define_local(self, name):\n        scope = self.locals[-1]\n\
          \        slot = sum(len(s) for s in self.locals) - 1 + len(scope)\n        scope[name] = slot\n        return slot\n\
          \    \n    def resolve_local(self, name):\n        for scope in reversed(self.locals):\n            if name in scope:\n\
          \                return scope[name]\n        return None  # Global or undefined\n    \n    def compile_VarDecl(self,\
          \ node):\n        if node.initializer:\n            self.compile(node.initializer)\n        else:\n            self.emit(OpCode.NIL)\n\
          \        self.define_local(node.name)\n    \n    def compile_Variable(self, node):\n        slot = self.resolve_local(node.name)\n\
          \        if slot is not None:\n            self.emit(OpCode.LOAD_LOCAL, slot)\n        else:\n            idx =\
          \ self.add_constant(node.name)\n            self.emit(OpCode.LOAD_GLOBAL, idx)\n    \n    def compile_Assignment(self,\
          \ node):\n        self.compile(node.value)\n        slot = self.resolve_local(node.name)\n        if slot is not\
          \ None:\n            self.emit(OpCode.STORE_LOCAL, slot)\n        else:\n            idx = self.add_constant(node.name)\n\
          \            self.emit(OpCode.STORE_GLOBAL, idx)"
      pitfalls:
      - Scope lifetime management
      - Variable shadowing bugs
      - Uninitialized variables
      concepts:
      - Symbol tables
      - Variable resolution
      - Scope management
      estimated_hours: 5-8
    - id: 3
      name: Control Flow
      description: Compile if statements, while loops, and logical operators.
      acceptance_criteria:
      - Conditional jumps
      - Jump targets/patching
      - Short-circuit evaluation
      - Loop compilation
      hints:
        level1: Emit jump with placeholder, then patch address after compiling body.
        level2: 'Short-circuit: ''and'' jumps to false if left is false, ''or'' jumps to true if left is true.'
        level3: "def compile_If(self, node):\n    self.compile(node.condition)\n    \n    # Jump to else if false\n    jump_to_else\
          \ = self.emit_jump(OpCode.JUMP_IF_FALSE)\n    self.emit(OpCode.POP)  # Pop condition\n    \n    self.compile(node.then_branch)\n\
          \    \n    # Jump over else\n    jump_to_end = self.emit_jump(OpCode.JUMP)\n    \n    self.patch_jump(jump_to_else)\n\
          \    self.emit(OpCode.POP)  # Pop condition\n    \n    if node.else_branch:\n        self.compile(node.else_branch)\n\
          \    \n    self.patch_jump(jump_to_end)\n\ndef compile_While(self, node):\n    loop_start = len(self.code)\n   \
          \ \n    self.compile(node.condition)\n    exit_jump = self.emit_jump(OpCode.JUMP_IF_FALSE)\n    self.emit(OpCode.POP)\n\
          \    \n    self.compile(node.body)\n    self.emit_loop(loop_start)\n    \n    self.patch_jump(exit_jump)\n    self.emit(OpCode.POP)\n\
          \ndef emit_jump(self, opcode):\n    self.emit(opcode)\n    self.emit(0xFF)  # Placeholder\n    self.emit(0xFF)\n\
          \    return len(self.code) - 2\n\ndef patch_jump(self, offset):\n    jump = len(self.code) - offset - 2\n    self.code[offset]\
          \ = (jump >> 8) & 0xFF\n    self.code[offset + 1] = jump & 0xFF\n\ndef emit_loop(self, loop_start):\n    self.emit(OpCode.LOOP)\n\
          \    offset = len(self.code) - loop_start + 2\n    self.emit((offset >> 8) & 0xFF)\n    self.emit(offset & 0xFF)"
      pitfalls:
      - Jump offset calculation
      - Forgetting to pop condition
      - Break/continue in nested loops
      concepts:
      - Jump patching
      - Backpatching
      - Short-circuit evaluation
      estimated_hours: 6-10
    - id: 4
      name: Functions
      description: Compile function definitions and calls.
      acceptance_criteria:
      - Function objects with bytecode
      - Parameter passing
      - Return statements
      - Closures (optional)
      hints:
        level1: Each function has its own bytecode chunk. Compile separately.
        level2: Parameters are just the first N local slots. CALL pushes frame.
        level3: "class FunctionObject:\n    def __init__(self, name, arity):\n        self.name = name\n        self.arity\
          \ = arity\n        self.code = []\n        self.constants = []\n\ndef compile_FunctionDecl(self, node):\n    func\
          \ = FunctionObject(node.name, len(node.params))\n    \n    # Save current compiler state\n    enclosing_code = self.code\n\
          \    enclosing_constants = self.constants\n    enclosing_locals = self.locals\n    \n    # Setup for function\n\
          \    self.code = func.code\n    self.constants = func.constants\n    self.locals = [{}]\n    \n    # Parameters\
          \ become first locals\n    for param in node.params:\n        self.define_local(param)\n    \n    # Compile body\n\
          \    self.compile(node.body)\n    \n    # Implicit return nil\n    self.emit(OpCode.NIL)\n    self.emit(OpCode.RETURN)\n\
          \    \n    # Restore state\n    self.code = enclosing_code\n    self.constants = enclosing_constants\n    self.locals\
          \ = enclosing_locals\n    \n    # Emit function as constant\n    idx = self.add_constant(func)\n    self.emit(OpCode.CONST,\
          \ idx)\n    \n    # Define in current scope\n    self.define_local(node.name)\n\ndef compile_Call(self, node):\n\
          \    self.compile(node.callee)  # Push function\n    \n    for arg in node.arguments:\n        self.compile(arg)\
          \  # Push args\n    \n    self.emit(OpCode.CALL, len(node.arguments))\n\ndef compile_Return(self, node):\n    if\
          \ node.value:\n        self.compile(node.value)\n    else:\n        self.emit(OpCode.NIL)\n    self.emit(OpCode.RETURN)"
      pitfalls:
      - Frame pointer management
      - Argument count validation
      - Stack cleanup on return
      concepts:
      - Call frames
      - Parameter binding
      - Return addresses
      estimated_hours: 10-16
  bytecode-vm:
    id: bytecode-vm
    name: Bytecode Virtual Machine
    description: Build a stack-based virtual machine that executes bytecode. Learn about instruction sets and execution models.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Basic assembly concepts
    - Stack data structure
    - Binary representation
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
      - Java
    resources:
    - name: Crafting Interpreters - Bytecode VM
      url: https://craftinginterpreters.com/a-bytecode-virtual-machine.html
      type: book
    - name: Writing a Simple VM
      url: https://felix.engineer/blogs/virtual-machine-in-c
      type: article
    milestones:
    - id: 1
      name: Instruction Set Design
      description: Define opcodes and bytecode format.
      acceptance_criteria:
      - Define opcode enum
      - Design instruction encoding
      - Support operand encoding
      - Document instruction format
      hints:
        level1: 'Keep it simple: 1 byte opcode, optional operands follow.'
        level2: 'Common opcodes: PUSH, POP, ADD, SUB, JUMP, CALL, RETURN.'
        level3: "from enum import IntEnum\n\nclass OpCode(IntEnum):\n    # Stack operations\n    CONST = 0x01      # Push\
          \ constant: CONST <index>\n    POP = 0x02        # Pop top of stack\n    DUP = 0x03        # Duplicate top\n   \
          \ \n    # Arithmetic\n    ADD = 0x10\n    SUB = 0x11\n    MUL = 0x12\n    DIV = 0x13\n    NEG = 0x14\n    \n   \
          \ # Comparison\n    EQ = 0x20\n    LT = 0x21\n    GT = 0x22\n    \n    # Control flow\n    JUMP = 0x30       # JUMP\
          \ <offset>\n    JUMP_IF_FALSE = 0x31\n    \n    # Variables\n    LOAD_LOCAL = 0x40  # LOAD_LOCAL <slot>\n    STORE_LOCAL\
          \ = 0x41\n    LOAD_GLOBAL = 0x42\n    STORE_GLOBAL = 0x43\n    \n    # Functions\n    CALL = 0x50       # CALL <arg_count>\n\
          \    RETURN = 0x51\n    \n    # Special\n    HALT = 0xFF\n\nclass Chunk:\n    '''Bytecode container'''\n    def\
          \ __init__(self):\n        self.code = bytearray()\n        self.constants = []\n        self.lines = []  # For\
          \ error reporting\n    \n    def write(self, byte, line):\n        self.code.append(byte)\n        self.lines.append(line)\n\
          \    \n    def add_constant(self, value):\n        self.constants.append(value)\n        return len(self.constants)\
          \ - 1"
      pitfalls:
      - Too complex instruction set
      - Forgetting HALT
      - Operand encoding issues
      concepts:
      - Opcodes
      - Instruction encoding
      - Bytecode format
      estimated_hours: 2-3
    - id: 2
      name: Stack-Based Execution
      description: Implement the execution loop with value stack.
      acceptance_criteria:
      - Value stack for operands
      - Instruction pointer
      - Fetch-decode-execute cycle
      - Arithmetic operations
      hints:
        level1: 'Stack machine: operands pushed, operators pop and push result.'
        level2: IP points to next instruction. advance() returns byte and increments IP.
        level3: "class VM:\n    STACK_MAX = 256\n    \n    def __init__(self):\n        self.stack = []\n        self.ip =\
          \ 0\n        self.chunk = None\n    \n    def push(self, value):\n        if len(self.stack) >= self.STACK_MAX:\n\
          \            raise RuntimeError('Stack overflow')\n        self.stack.append(value)\n    \n    def pop(self):\n\
          \        if not self.stack:\n            raise RuntimeError('Stack underflow')\n        return self.stack.pop()\n\
          \    \n    def read_byte(self):\n        byte = self.chunk.code[self.ip]\n        self.ip += 1\n        return byte\n\
          \    \n    def run(self, chunk):\n        self.chunk = chunk\n        self.ip = 0\n        self.stack = []\n   \
          \     \n        while True:\n            op = self.read_byte()\n            \n            if op == OpCode.CONST:\n\
          \                index = self.read_byte()\n                self.push(self.chunk.constants[index])\n            \n\
          \            elif op == OpCode.ADD:\n                b = self.pop()\n                a = self.pop()\n          \
          \      self.push(a + b)\n            \n            elif op == OpCode.SUB:\n                b = self.pop()\n    \
          \            a = self.pop()\n                self.push(a - b)\n            \n            elif op == OpCode.MUL:\n\
          \                b = self.pop()\n                a = self.pop()\n                self.push(a * b)\n            \n\
          \            elif op == OpCode.DIV:\n                b = self.pop()\n                a = self.pop()\n          \
          \      self.push(a / b)\n            \n            elif op == OpCode.NEG:\n                self.push(-self.pop())\n\
          \            \n            elif op == OpCode.HALT:\n                return self.pop() if self.stack else None"
      pitfalls:
      - Stack over/underflow
      - Off-by-one in IP
      - Order of operands for SUB/DIV
      concepts:
      - Stack machines
      - Instruction pointer
      - Fetch-decode-execute
      estimated_hours: 4-5
    - id: 3
      name: Control Flow
      description: Add jumps and conditionals.
      acceptance_criteria:
      - Unconditional JUMP
      - Conditional JUMP_IF_FALSE
      - Comparison operators
      - Handle forward and backward jumps
      hints:
        level1: JUMP modifies IP directly. Jump offset can be relative or absolute.
        level2: 'JUMP_IF_FALSE: pop condition, jump if falsy, else continue.'
        level3: "# In VM.run():\n\nelif op == OpCode.EQ:\n    b = self.pop()\n    a = self.pop()\n    self.push(a == b)\n\n\
          elif op == OpCode.LT:\n    b = self.pop()\n    a = self.pop()\n    self.push(a < b)\n\nelif op == OpCode.GT:\n \
          \   b = self.pop()\n    a = self.pop()\n    self.push(a > b)\n\nelif op == OpCode.JUMP:\n    offset = self.read_short()\
          \  # 2-byte offset\n    self.ip = offset\n\nelif op == OpCode.JUMP_IF_FALSE:\n    offset = self.read_short()\n \
          \   if not self.peek_stack():  # Check without popping\n        self.ip = offset\n    self.pop()  # Pop condition\
          \ after check\n\ndef read_short(self):\n    '''Read 2-byte offset'''\n    high = self.read_byte()\n    low = self.read_byte()\n\
          \    return (high << 8) | low\n\ndef peek_stack(self):\n    return self.stack[-1] if self.stack else None\n\n# Example:\
          \ if-else compilation\n# if (x < 10) { a } else { b }\n# Compiles to:\n#   LOAD x\n#   CONST 10\n#   LT\n#   JUMP_IF_FALSE\
          \ else_label\n#   <a code>\n#   JUMP end_label\n# else_label:\n#   <b code>\n# end_label:"
      pitfalls:
      - Jump offset calculation
      - Forgetting to pop condition
      - Infinite loops
      concepts:
      - Control flow
      - Jumps
      - Conditional execution
      estimated_hours: 3-4
    - id: 4
      name: Variables and Functions
      description: Add local variables and function calls.
      acceptance_criteria:
      - Local variable slots on stack
      - CALL pushes return address
      - RETURN pops frame
      - Pass arguments via stack
      hints:
        level1: 'Locals: use stack slots relative to frame pointer.'
        level2: 'Call frame: save IP, base pointer. Return: restore them.'
        level3: "class CallFrame:\n    def __init__(self, function, ip, base):\n        self.function = function  # Function\
          \ being called\n        self.ip = ip              # Return address\n        self.base = base          # Stack base\
          \ for locals\n\nclass VM:\n    def __init__(self):\n        self.stack = []\n        self.frames = []  # Call stack\n\
          \    \n    @property\n    def frame(self):\n        return self.frames[-1]\n    \n    def run(self, main_function):\n\
          \        self.frames = [CallFrame(main_function, 0, 0)]\n        \n        while self.frames:\n            op =\
          \ self.read_byte()\n            \n            if op == OpCode.LOAD_LOCAL:\n                slot = self.read_byte()\n\
          \                self.push(self.stack[self.frame.base + slot])\n            \n            elif op == OpCode.STORE_LOCAL:\n\
          \                slot = self.read_byte()\n                self.stack[self.frame.base + slot] = self.peek_stack()\n\
          \            \n            elif op == OpCode.CALL:\n                arg_count = self.read_byte()\n             \
          \   function = self.stack[-arg_count - 1]  # Function is below args\n                frame = CallFrame(\n      \
          \              function,\n                    self.frame.ip,\n                    len(self.stack) - arg_count -\
          \ 1\n                )\n                self.frames.append(frame)\n                self.frame.ip = 0\n         \
          \   \n            elif op == OpCode.RETURN:\n                result = self.pop()\n                # Discard frame\n\
          \                old_frame = self.frames.pop()\n                # Pop locals and function\n                while\
          \ len(self.stack) > old_frame.base:\n                    self.pop()\n                self.push(result)\n       \
          \         \n                if not self.frames:\n                    return result"
      pitfalls:
      - Frame pointer calculation
      - Argument passing order
      - Return value handling
      concepts:
      - Call frames
      - Local variables
      - Function calls
      estimated_hours: 5-7
  calculator:
    id: calculator
    name: Calculator
    description: Build a functional calculator with a clean UI. Learn about UI state management, event handling, and handling
      edge cases in user input.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - HTML/CSS basics
    - JavaScript fundamentals
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible:
      - Python
      - React
      - Vue
    resources:
    - name: Build a Calculator - The Odin Project
      url: https://www.theodinproject.com/lessons/foundations-calculator
      type: tutorial
    milestones:
    - id: 1
      name: UI Layout
      description: Create the calculator interface with buttons and display.
      acceptance_criteria:
      - Display area for input and result
      - Number buttons (0-9)
      - Operator buttons (+, -, *, /)
      - Equals and clear buttons
      - Responsive grid layout
      hints:
        level1: Use CSS Grid for button layout - perfect for calculators.
        level2: Use data attributes to store button values.
        level3: ".calculator {\n  display: grid;\n  grid-template-columns: repeat(4, 1fr);\n  gap: 4px;\n}\n.display { grid-column:\
          \ 1 / -1; }\n.btn-zero { grid-column: span 2; }"
      pitfalls:
      - Buttons too small for touch
      - Display overflow for long numbers
      - Not aligning numbers to the right
      concepts:
      - CSS Grid
      - Button styling
      - Layout design
      estimated_hours: 2-3
    - id: 2
      name: Basic Operations
      description: Implement basic arithmetic operations.
      acceptance_criteria:
      - Numbers append to display when clicked
      - Operators store current value and operation
      - Equals performs calculation
      - Clear resets calculator
      - Only one decimal point allowed
      hints:
        level1: 'Track: currentValue, previousValue, currentOperator.'
        level2: Calculate when equals pressed or new operator selected.
        level3: "function calculate(a, b, operator) {\n  switch(operator) {\n    case '+': return a + b;\n    case '-': return\
          \ a - b;\n    case '*': return a * b;\n    case '/': return b !== 0 ? a / b : 'Error';\n  }\n}"
      pitfalls:
      - Division by zero
      - Multiple decimal points
      - Floating point precision (0.1 + 0.2)
      concepts:
      - State management
      - Arithmetic operations
      - Edge case handling
      estimated_hours: 2-3
    - id: 3
      name: Chained Operations
      description: Allow chaining multiple operations (e.g., 5 + 3 * 2).
      acceptance_criteria:
      - Pressing operator after operator changes operation
      - Result shown after each operator press
      - Can continue calculating without equals
      - Backspace/delete functionality
      hints:
        level1: When new operator pressed, first calculate previous if exists.
        level2: 'Track display mode: ''input'' vs ''result'' to know when to clear.'
        level3: "function handleOperator(op) {\n  if (previousValue && currentOperator && displayMode === 'input') {\n   \
          \ currentValue = calculate(previousValue, parseFloat(display), currentOperator);\n    updateDisplay(currentValue);\n\
          \  }\n  previousValue = parseFloat(display);\n  currentOperator = op;\n  displayMode = 'result';\n}"
      pitfalls:
      - Losing previous value when changing operators
      - Not updating display after intermediate calculation
      - Order of operations (this calculator is left-to-right)
      concepts:
      - State transitions
      - Calculator logic
      - User experience
      estimated_hours: 2-3
    - id: 4
      name: Keyboard Support & Polish
      description: Add keyboard input and polish the calculator.
      acceptance_criteria:
      - Number keys type numbers
      - Operator keys work (+, -, *, /)
      - Enter = equals, Escape = clear
      - Backspace deletes last digit
      - Visual feedback on button press
      hints:
        level1: Listen for keydown events, map key to button action.
        level2: 'Add visual feedback: briefly highlight button on keypress.'
        level3: "document.addEventListener('keydown', (e) => {\n  if (e.key >= '0' && e.key <= '9') handleNumber(e.key);\n\
          \  else if (['+', '-', '*', '/'].includes(e.key)) handleOperator(e.key);\n  else if (e.key === 'Enter') handleEquals();\n\
          \  else if (e.key === 'Escape') handleClear();\n  else if (e.key === 'Backspace') handleBackspace();\n});"
      pitfalls:
      - Preventing default for some keys (like backspace navigating away)
      - Different key codes for numpad vs main keys
      - Focus management
      concepts:
      - Keyboard events
      - Event key codes
      - Visual feedback
      estimated_hours: 2-3
  calculator-parser:
    id: calculator-parser
    name: Calculator Parser
    description: Build a calculator that parses and evaluates arithmetic expressions. Learn expression parsing and operator
      precedence.
    difficulty: beginner
    estimated_hours: 6-10
    prerequisites:
    - Basic programming
    - Understanding of precedence
    languages:
      recommended:
      - Python
      - JavaScript
      - C
      also_possible:
      - Go
      - Rust
    resources:
    - name: Recursive Descent Parsing
      url: https://craftinginterpreters.com/parsing-expressions.html
      type: book
    - name: Pratt Parsing
      url: https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html
      type: article
    milestones:
    - id: 1
      name: Basic Arithmetic
      description: Evaluate simple arithmetic expressions.
      acceptance_criteria:
      - Parse numbers
      - Handle + - * /
      - Correct precedence (* / before + -)
      - Handle parentheses
      hints:
        level1: 'One function per precedence level: expr() calls term() calls factor().'
        level2: factor handles numbers and parentheses, term handles * /, expr handles + -.
        level3: "class Calculator:\n    def __init__(self, text):\n        self.text = text\n        self.pos = 0\n    \n\
          \    def parse(self):\n        return self.expr()\n    \n    def expr(self):\n        result = self.term()\n   \
          \     while self.pos < len(self.text) and self.text[self.pos] in '+-':\n            op = self.text[self.pos]\n \
          \           self.pos += 1\n            if op == '+':\n                result += self.term()\n            else:\n\
          \                result -= self.term()\n        return result\n    \n    def term(self):\n        result = self.factor()\n\
          \        while self.pos < len(self.text) and self.text[self.pos] in '*/':\n            op = self.text[self.pos]\n\
          \            self.pos += 1\n            if op == '*':\n                result *= self.factor()\n            else:\n\
          \                result /= self.factor()\n        return result\n    \n    def factor(self):\n        self.skip_whitespace()\n\
          \        if self.text[self.pos] == '(':\n            self.pos += 1\n            result = self.expr()\n         \
          \   self.pos += 1  # skip ')'\n            return result\n        return self.number()"
      pitfalls:
      - Left vs right associativity
      - Division by zero
      - Whitespace handling
      concepts:
      - Recursive descent
      - Operator precedence
      - Expression trees
      estimated_hours: 2-3
    - id: 2
      name: Unary and Power
      description: Add unary operators and exponentiation.
      acceptance_criteria:
      - 'Unary minus: -5'
      - 'Unary plus: +5'
      - 'Exponentiation: 2^3'
      - Correct right-associativity for ^
      hints:
        level1: Unary goes in factor(). Power needs its own level.
        level2: 'Power is right-associative: 2^3^2 = 2^(3^2) = 512.'
        level3: "def factor(self):\n    self.skip_whitespace()\n    if self.text[self.pos] in '+-':\n        op = self.text[self.pos]\n\
          \        self.pos += 1\n        value = self.factor()  # Recursive for --5\n        return -value if op == '-' else\
          \ value\n    return self.power()\n\ndef power(self):\n    base = self.primary()\n    if self.pos < len(self.text)\
          \ and self.text[self.pos] == '^':\n        self.pos += 1\n        exp = self.factor()  # Right-associative: recurse\
          \ up\n        return base ** exp\n    return base"
      pitfalls:
      - --5 (double negative)
      - Power precedence vs unary
      - Right associativity
      concepts:
      - Unary operators
      - Associativity
      - Precedence climbing
      estimated_hours: 2-3
    - id: 3
      name: Variables and Functions
      description: Add variables and built-in functions.
      acceptance_criteria:
      - 'Variable assignment: x = 5'
      - 'Variable reference: x + 2'
      - 'Functions: sin(x), sqrt(x)'
      - Error on undefined variable
      hints:
        level1: Store variables in a dict. Look up on identifier.
        level2: Check if identifier followed by '(' for function call.
        level3: "class Calculator:\n    def __init__(self):\n        self.variables = {}\n        self.functions = {\n   \
          \         'sin': math.sin,\n            'cos': math.cos,\n            'sqrt': math.sqrt,\n            'abs': abs\n\
          \        }\n    \n    def statement(self):\n        self.skip_whitespace()\n        # Check for assignment\n   \
          \     if self.peek_identifier():\n            name = self.identifier()\n            self.skip_whitespace()\n   \
          \         if self.pos < len(self.text) and self.text[self.pos] == '=':\n                self.pos += 1\n        \
          \        value = self.expr()\n                self.variables[name] = value\n                return value\n     \
          \       # Put back and parse as expression\n            self.pos -= len(name)\n        return self.expr()\n    \n\
          \    def primary(self):\n        self.skip_whitespace()\n        if self.text[self.pos].isalpha():\n           \
          \ name = self.identifier()\n            if self.text[self.pos] == '(':  # Function call\n                self.pos\
          \ += 1\n                arg = self.expr()\n                self.pos += 1  # skip ')'\n                if name not\
          \ in self.functions:\n                    raise ValueError(f'Unknown function: {name}')\n                return\
          \ self.functions[name](arg)\n            if name not in self.variables:\n                raise ValueError(f'Undefined\
          \ variable: {name}')\n            return self.variables[name]\n        return self.number()"
      pitfalls:
      - Function vs variable ambiguity
      - Assignment in expression
      - Scope issues
      concepts:
      - Symbol tables
      - Function calls
      - Variable binding
      estimated_hours: 2-3
  cat-clone:
    id: cat-clone
    name: Cat Clone
    description: Build your own version of the Unix cat command. Learn file I/O, command-line argument parsing, and basic
      systems programming.
    difficulty: beginner
    estimated_hours: 4-6
    prerequisites:
    - Basic C/Rust/Go knowledge
    - Understanding of file systems
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
      - Zig
    resources:
    - name: Cat Man Page
      url: https://man7.org/linux/man-pages/man1/cat.1.html
      type: documentation
    - name: File I/O in C
      url: https://www.geeksforgeeks.org/basics-file-handling-c/
      type: tutorial
    milestones:
    - id: 1
      name: Basic File Reading
      description: Read a single file and output to stdout.
      acceptance_criteria:
      - Accept filename as argument
      - Read file contents
      - Print to stdout
      - Handle file not found error
      hints:
        level1: Use fopen/fread in C or File::open in Rust.
        level2: Read in chunks using a buffer, not entire file at once.
        level3: "#include <stdio.h>\n\nint main(int argc, char *argv[]) {\n    if (argc < 2) {\n        fprintf(stderr, \"\
          Usage: cat <file>\\n\");\n        return 1;\n    }\n    FILE *f = fopen(argv[1], \"r\");\n    if (!f) { perror(\"\
          cat\"); return 1; }\n    char buf[4096];\n    size_t n;\n    while ((n = fread(buf, 1, sizeof(buf), f)) > 0) {\n\
          \        fwrite(buf, 1, n, stdout);\n    }\n    fclose(f);\n    return 0;\n}"
      pitfalls:
      - Not checking if file exists
      - Reading entire file into memory
      - Forgetting to close file
      concepts:
      - File descriptors
      - Buffered I/O
      - Error handling
      estimated_hours: 1-2
    - id: 2
      name: Multiple Files & Stdin
      description: Handle multiple files and reading from stdin.
      acceptance_criteria:
      - Concatenate multiple files
      - Read from stdin when no args
      - Handle - as stdin specifier
      - Continue on error for remaining files
      hints:
        level1: Loop through argv[1] to argv[argc-1].
        level2: Use stdin when filename is "-" or no arguments.
        level3: "for (int i = 1; i < argc; i++) {\n    FILE *f;\n    if (strcmp(argv[i], \"-\") == 0) {\n        f = stdin;\n\
          \    } else {\n        f = fopen(argv[i], \"r\");\n        if (!f) { perror(argv[i]); continue; }\n    }\n    //\
          \ ... read and write ...\n    if (f != stdin) fclose(f);\n}"
      pitfalls:
      - Closing stdin
      - Not continuing after error
      - Order of file processing
      concepts:
      - stdin/stdout
      - Command-line conventions
      - Error recovery
      estimated_hours: 1-2
    - id: 3
      name: Options and Flags
      description: Implement common cat options.
      acceptance_criteria:
      - -n to number lines
      - -b to number non-blank lines only
      - -s to squeeze blank lines
      - -E to show $ at end of lines
      hints:
        level1: Parse options before processing files.
        level2: Track line number as state across files.
        level3: "Use getopt() for argument parsing:\nint opt;\nwhile ((opt = getopt(argc, argv, \"nbsE\")) != -1) {\n    switch\
          \ (opt) {\n        case 'n': number_lines = 1; break;\n        case 'b': number_nonblank = 1; break;\n        case\
          \ 's': squeeze_blank = 1; break;\n        case 'E': show_ends = 1; break;\n    }\n}"
      pitfalls:
      - Option parsing edge cases
      - Line counting across files
      - Blank line detection
      concepts:
      - getopt parsing
      - State machines
      - Character-by-character processing
      estimated_hours: 2-3
  cd-deployment:
    id: cd-deployment
    name: CD with Blue-Green Deployment
    description: Implement continuous deployment with zero-downtime blue-green deployment strategy.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - CI pipeline basics
    - Docker/containers
    - Load balancer concepts
    - Shell scripting
    languages:
      recommended:
      - Bash
      - Python
      - Go
      also_possible:
      - JavaScript
      - Ruby
    resources:
    - type: article
      name: Blue-Green Deployments on AWS
      url: https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/welcome.html
    - type: documentation
      name: Kubernetes Rolling Updates
      url: https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/
    - type: article
      name: Martin Fowler on Blue-Green
      url: https://martinfowler.com/bliki/BlueGreenDeployment.html
    milestones:
    - id: 1
      name: Dual Environment Setup
      description: Set up blue and green environments that can run simultaneously.
      acceptance_criteria:
      - Two identical environments
      - Independent deployability
      - Shared database/persistence
      - Health check endpoints
      hints:
        level1: Use Docker Compose or similar to define both environments.
        level2: Each environment needs its own port/address. Use environment variables for configuration.
        level3: "## Dual Environment Architecture\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  blue:\n\
          \    build: .\n    environment:\n      - ENV_COLOR=blue\n      - PORT=3001\n    ports:\n      - \"3001:3000\"\n\
          \    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/health\"]\n      interval: 10s\n\
          \      timeout: 5s\n      retries: 3\n\n  green:\n    build: .\n    environment:\n      - ENV_COLOR=green\n    \
          \  - PORT=3002\n    ports:\n      - \"3002:3000\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"\
          http://localhost:3000/health\"]\n      interval: 10s\n      timeout: 5s\n      retries: 3\n\n  nginx:\n    image:\
          \ nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n   \
          \ depends_on:\n      - blue\n      - green\n```\n\n```javascript\n// Health check endpoint\napp.get('/health', (req,\
          \ res) => {\n  const health = {\n    status: 'healthy',\n    version: process.env.APP_VERSION,\n    color: process.env.ENV_COLOR,\n\
          \    uptime: process.uptime(),\n    timestamp: Date.now()\n  };\n  res.json(health);\n});\n```"
      pitfalls:
      - Forgetting database migrations
      - Environment config drift
      - Not testing both environments equally
      concepts:
      - Environment isolation
      - Infrastructure as code
      - Health checks
      estimated_hours: 4-6
    - id: 2
      name: Load Balancer Switching
      description: Implement traffic switching between blue and green environments.
      acceptance_criteria:
      - Nginx/HAProxy configuration
      - Dynamic upstream switching
      - No dropped connections
      - Rollback capability
      hints:
        level1: Use Nginx upstream directive to define backends.
        level2: Update nginx config and reload (not restart) for zero-downtime.
        level3: "## Load Balancer Configuration\n\n```nginx\n# nginx.conf\nupstream backend {\n    # Active environment (switched\
          \ during deployment)\n    server blue:3000;\n    # Standby (commented out)\n    # server green:3000;\n}\n\nserver\
          \ {\n    listen 80;\n    \n    location / {\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n\
          \        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header\
          \ Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_cache_bypass $http_upgrade;\n   \
          \ }\n    \n    location /health {\n        proxy_pass http://backend/health;\n    }\n}\n```\n\n```bash\n#!/bin/bash\n\
          # switch-traffic.sh\n\nTARGET=$1  # blue or green\n\nif [ \"$TARGET\" != \"blue\" ] && [ \"$TARGET\" != \"green\"\
          \ ]; then\n    echo \"Usage: $0 <blue|green>\"\n    exit 1\nfi\n\n# Generate new nginx config\ncat > /etc/nginx/conf.d/upstream.conf\
          \ << EOF\nupstream backend {\n    server ${TARGET}:3000;\n}\nEOF\n\n# Test config\nnginx -t\nif [ $? -ne 0 ]; then\n\
          \    echo \"Nginx config test failed!\"\n    exit 1\nfi\n\n# Reload nginx (graceful, no dropped connections)\nnginx\
          \ -s reload\n\necho \"Traffic switched to $TARGET\"\n```"
      pitfalls:
      - Using restart instead of reload
      - Not testing config before applying
      - Forgetting to drain connections
      concepts:
      - Reverse proxy
      - Graceful reload
      - Connection draining
      estimated_hours: 4-6
    - id: 3
      name: Deployment Automation
      description: Automate the deployment process with pre-deployment checks.
      acceptance_criteria:
      - Automated deployment script
      - Pre-deployment health checks
      - Smoke tests after switch
      - Deployment logging
      hints:
        level1: 'Write a deployment script that: builds, deploys to standby, tests, switches traffic.'
        level2: Add timeouts and failure handling. Log everything for debugging.
        level3: "## Deployment Script\n\n```bash\n#!/bin/bash\n# deploy.sh - Blue-Green Deployment Script\n\nset -e  # Exit\
          \ on error\n\n# Configuration\nDEPLOY_TIMEOUT=300\nHEALTH_CHECK_RETRIES=30\nHEALTH_CHECK_INTERVAL=5\n\n# Determine\
          \ current and target environments\nCURRENT=$(curl -s http://localhost/health | jq -r '.color')\nif [ \"$CURRENT\"\
          \ == \"blue\" ]; then\n    TARGET=\"green\"\nelse\n    TARGET=\"blue\"\nfi\n\necho \"Current: $CURRENT, Deploying\
          \ to: $TARGET\"\n\n# Step 1: Build and deploy to target\necho \"Building new version...\"\ndocker-compose build\
          \ $TARGET\n\necho \"Starting $TARGET environment...\"\ndocker-compose up -d $TARGET\n\n# Step 2: Wait for health\
          \ check\necho \"Waiting for $TARGET to be healthy...\"\nfor i in $(seq 1 $HEALTH_CHECK_RETRIES); do\n    if curl\
          \ -sf \"http://${TARGET}:3000/health\" > /dev/null; then\n        echo \"$TARGET is healthy!\"\n        break\n\
          \    fi\n    if [ $i -eq $HEALTH_CHECK_RETRIES ]; then\n        echo \"Health check failed after $HEALTH_CHECK_RETRIES\
          \ attempts\"\n        exit 1\n    fi\n    echo \"Attempt $i/$HEALTH_CHECK_RETRIES - waiting...\"\n    sleep $HEALTH_CHECK_INTERVAL\n\
          done\n\n# Step 3: Run smoke tests\necho \"Running smoke tests on $TARGET...\"\n./smoke-tests.sh \"http://${TARGET}:3000\"\
          \nif [ $? -ne 0 ]; then\n    echo \"Smoke tests failed!\"\n    exit 1\nfi\n\n# Step 4: Switch traffic\necho \"Switching\
          \ traffic to $TARGET...\"\n./switch-traffic.sh $TARGET\n\n# Step 5: Verify production\necho \"Verifying production...\"\
          \nsleep 5\nPROD_COLOR=$(curl -s http://localhost/health | jq -r '.color')\nif [ \"$PROD_COLOR\" != \"$TARGET\" ];\
          \ then\n    echo \"Traffic switch verification failed!\"\n    exit 1\nfi\n\necho \"Deployment successful! Now running:\
          \ $TARGET\"\necho \"Previous environment ($CURRENT) is still available for rollback\"\n```"
      pitfalls:
      - No timeout handling
      - Missing error handling
      - Not verifying after switch
      concepts:
      - Deployment automation
      - Smoke testing
      - Idempotent deployments
      estimated_hours: 5-8
    - id: 4
      name: Rollback & Database Migrations
      description: Handle rollbacks and database migrations safely.
      acceptance_criteria:
      - Instant rollback capability
      - Backward-compatible migrations
      - Migration verification
      - Database version tracking
      hints:
        level1: Rollback is just switching back to the previous environment.
        level2: Database migrations must be backward-compatible (expand-contract pattern).
        level3: "## Rollback & Migrations\n\n```bash\n#!/bin/bash\n# rollback.sh - Instant rollback to previous environment\n\
          \nCURRENT=$(curl -s http://localhost/health | jq -r '.color')\nif [ \"$CURRENT\" == \"blue\" ]; then\n    PREVIOUS=\"\
          green\"\nelse\n    PREVIOUS=\"blue\"\nfi\n\necho \"Rolling back from $CURRENT to $PREVIOUS...\"\n\n# Verify previous\
          \ environment is still running\nif ! curl -sf \"http://${PREVIOUS}:3000/health\" > /dev/null; then\n    echo \"\
          Previous environment is not available!\"\n    exit 1\nfi\n\n# Switch traffic back\n./switch-traffic.sh $PREVIOUS\n\
          \necho \"Rollback complete. Now running: $PREVIOUS\"\n```\n\n### Database Migration Strategy\n\n```\nExpand-Contract\
          \ Pattern:\n\n1. EXPAND: Add new column (nullable or with default)\n   - Both old and new code can work\n   \n2.\
          \ MIGRATE: Deploy new code that writes to both\n   - Backfill existing data\n   \n3. CONTRACT: Remove old column/code\
          \ (after verification)\n   - Only when sure rollback isn't needed\n```\n\n```javascript\n// migrations/20240115_add_user_email.js\n\
          \n// EXPAND phase - backward compatible\nexports.up = async (db) => {\n  // Add column as nullable (old code ignores\
          \ it)\n  await db.query(`\n    ALTER TABLE users \n    ADD COLUMN email VARCHAR(255) NULL\n  `);\n};\n\nexports.down\
          \ = async (db) => {\n  await db.query(`\n    ALTER TABLE users \n    DROP COLUMN email\n  `);\n};\n\n// CONTRACT\
          \ phase (separate migration, run later)\nexports.up = async (db) => {\n  // Only run after all instances use new\
          \ code\n  await db.query(`\n    ALTER TABLE users \n    ALTER COLUMN email SET NOT NULL\n  `);\n};\n```\n\n```bash\n\
          # deployment with migrations\n./run-migrations.sh  # Run expand migrations first\n./deploy.sh          # Deploy\
          \ new code\n# ... wait for stability ...\n./run-migrations.sh --contract  # Run contract migrations\n```"
      pitfalls:
      - Breaking migrations that prevent rollback
      - Running contract too early
      - Not testing rollback regularly
      concepts:
      - Expand-contract pattern
      - Backward compatibility
      - Database versioning
      estimated_hours: 5-8
  chat-app:
    id: chat-app
    name: Real-time Chat
    description: Build a real-time chat application using WebSockets. Learn about bi-directional communication, presence,
      and message persistence.
    difficulty: intermediate
    estimated_hours: 25-35
    prerequisites:
    - JavaScript/Node.js
    - Basic HTML/CSS
    - Understanding of HTTP
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible:
      - Go
      - Python
      - Rust
    resources:
    - name: Socket.io Chat Tutorial
      url: https://socket.io/get-started/chat
      type: tutorial
    - name: WebSocket API - MDN
      url: https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API
      type: documentation
    milestones:
    - id: 1
      name: WebSocket Server Setup
      description: Set up WebSocket server and basic connection handling.
      acceptance_criteria:
      - Server accepts WebSocket connections
      - Clients can connect and disconnect
      - Server logs connection events
      - Basic error handling
      hints:
        level1: Use ws library (Node.js) or gorilla/websocket (Go).
        level2: Track connected clients in a Map or Set.
        level3: "const WebSocket = require('ws');\nconst wss = new WebSocket.Server({ port: 8080 });\nconst clients = new\
          \ Set();\n\nwss.on('connection', (ws) => {\n  clients.add(ws);\n  console.log('Client connected', clients.size);\n\
          \  ws.on('close', () => {\n    clients.delete(ws);\n    console.log('Client disconnected', clients.size);\n  });\n\
          });"
      pitfalls:
      - Not handling connection errors
      - Memory leaks from not cleaning up closed connections
      - Not implementing heartbeat/ping-pong
      concepts:
      - WebSocket protocol
      - Connection lifecycle
      - Event-driven architecture
      estimated_hours: 2-3
    - id: 2
      name: Message Broadcasting
      description: Implement sending and receiving messages.
      acceptance_criteria:
      - Client can send text messages
      - Server broadcasts to all connected clients
      - Messages include sender and timestamp
      - Message format is JSON
      hints:
        level1: Parse incoming messages as JSON, broadcast to all clients.
        level2: Don't send message back to sender (unless you want echo).
        level3: "ws.on('message', (data) => {\n  const message = JSON.parse(data);\n  const broadcast = JSON.stringify({\n\
          \    type: 'message',\n    user: message.user,\n    text: message.text,\n    timestamp: Date.now()\n  });\n  clients.forEach(client\
          \ => {\n    if (client !== ws && client.readyState === WebSocket.OPEN) {\n      client.send(broadcast);\n    }\n\
          \  });\n});"
      pitfalls:
      - Not checking readyState before sending
      - Invalid JSON crashing server
      - Missing message validation
      concepts:
      - Broadcasting pattern
      - JSON message protocol
      - Error handling
      estimated_hours: 3-4
    - id: 3
      name: Chat Rooms
      description: Implement multiple chat rooms/channels.
      acceptance_criteria:
      - Users can join/leave rooms
      - Messages scoped to room
      - List available rooms
      - Create new rooms
      - Show users in current room
      hints:
        level1: 'Use a Map: roomName -> Set of client connections.'
        level2: Each message includes room name, only broadcast to room members.
        level3: "const rooms = new Map();\n\nfunction joinRoom(ws, roomName) {\n  if (!rooms.has(roomName)) {\n    rooms.set(roomName,\
          \ new Set());\n  }\n  rooms.get(roomName).add(ws);\n  ws.currentRoom = roomName;\n}\n\nfunction broadcastToRoom(roomName,\
          \ message, sender) {\n  const room = rooms.get(roomName);\n  if (!room) return;\n  room.forEach(client => {\n  \
          \  if (client !== sender && client.readyState === WebSocket.OPEN) {\n      client.send(JSON.stringify(message));\n\
          \    }\n  });\n}"
      pitfalls:
      - Not removing user from room on disconnect
      - Room names not sanitized
      - Empty rooms accumulating
      concepts:
      - Room-based messaging
      - Namespaces
      - User presence
      estimated_hours: 4-5
    - id: 4
      name: User Authentication & Persistence
      description: Add user authentication and message history.
      acceptance_criteria:
      - Users authenticate before connecting
      - Messages stored in database
      - Load message history on join
      - User typing indicators
      - Online/offline status
      hints:
        level1: Send JWT token on WebSocket connect, verify before accepting.
        level2: Store last N messages per room, load on join.
        level3: "// Typing indicator with debounce\nws.on('message', (data) => {\n  const msg = JSON.parse(data);\n  if (msg.type\
          \ === 'typing') {\n    broadcastToRoom(ws.currentRoom, { type: 'typing', user: ws.user.name }, ws);\n    clearTimeout(ws.typingTimeout);\n\
          \    ws.typingTimeout = setTimeout(() => {\n      broadcastToRoom(ws.currentRoom, { type: 'stopped_typing', user:\
          \ ws.user.name }, ws);\n    }, 3000);\n  }\n});"
      pitfalls:
      - Loading too much history (pagination needed)
      - Typing indicator spam
      - Race conditions with presence
      concepts:
      - WebSocket authentication
      - Message persistence
      - Presence system
      estimated_hours: 6-8
  chip8-emulator:
    id: chip8-emulator
    name: CHIP-8 Emulator
    description: Build a CHIP-8 virtual machine emulator. Learn CPU emulation, instruction decoding, and graphics rendering
      through this classic 1970s VM architecture.
    difficulty: intermediate
    estimated_hours: 20-30
    prerequisites:
    - Binary/hex manipulation
    - Basic graphics
    - State machines
    languages:
      recommended:
      - C
      - Rust
      - Python
      also_possible:
      - JavaScript
      - Go
    resources:
    - name: Tobias V. Langhoff's CHIP-8 Guide
      url: https://tobiasvl.github.io/blog/write-a-chip-8-emulator/
      type: tutorial
    - name: CHIP-8 Technical Reference
      url: http://devernay.free.fr/hacks/chip8/C8TECH10.HTM
      type: reference
    - name: freeCodeCamp CHIP-8 Tutorial
      url: https://www.freecodecamp.org/news/creating-your-very-own-chip-8-emulator/
      type: tutorial
    milestones:
    - id: 1
      name: Memory and Registers
      description: Initialize CHIP-8 memory (4KB), registers (V0-VF), index register, program counter, and stack.
      acceptance_criteria:
      - 4096 bytes of RAM initialized
      - 16 general-purpose 8-bit registers (V0-VF)
      - 16-bit index register (I)
      - 16-bit program counter starting at 0x200
      - Stack with 16 levels for subroutine calls
      - Font sprites loaded at 0x000-0x1FF
      hints:
        level1: CHIP-8 programs start at address 0x200. The first 512 bytes are reserved for the interpreter and font data.
        level2: VF register is special - it's used as a flag for carry, borrow, and collision detection. Don't use it for
          general storage.
        level3: "struct Chip8 {\n    uint8_t memory[4096];      // 4KB RAM\n    uint8_t V[16];             // V0-VF registers\n\
          \    uint16_t I;                // Index register\n    uint16_t pc;               // Program counter\n    uint16_t\
          \ stack[16];        // Stack\n    uint8_t sp;                // Stack pointer\n    uint8_t delay_timer;       //\
          \ Delay timer (60Hz)\n    uint8_t sound_timer;       // Sound timer (60Hz)\n    uint8_t display[64 * 32];  // 64x32\
          \ monochrome display\n    uint8_t keypad[16];        // 16-key hexadecimal keypad\n};\n\nvoid init_chip8(Chip8*\
          \ chip) {\n    memset(chip, 0, sizeof(Chip8));\n    chip->pc = 0x200;  // Programs start here\n\n    // Load font\
          \ sprites (0-F) at 0x000\n    uint8_t fontset[80] = {\n        0xF0, 0x90, 0x90, 0x90, 0xF0, // 0\n        0x20,\
          \ 0x60, 0x20, 0x20, 0x70, // 1\n        // ... etc for 0-F\n    };\n    memcpy(chip->memory, fontset, 80);\n}"
      pitfalls:
      - Forgetting that programs start at 0x200, not 0x000
      - Not initializing font sprites in reserved memory
      - Using VF as a general register (it's used for flags)
      concepts:
      - Memory-mapped I/O
      - Register architecture
      - Stack operations
      estimated_hours: 3-4
    - id: 2
      name: Fetch-Decode-Execute Cycle
      description: Implement the main emulation loop and instruction fetching. CHIP-8 instructions are 2 bytes (big-endian).
      acceptance_criteria:
      - Fetch 2-byte opcode from memory[PC]
      - Increment PC by 2 after fetch
      - Extract opcode components (nibbles)
      - Main loop runs at ~500-700 instructions/second
      hints:
        level1: 'CHIP-8 uses big-endian byte order. Combine two bytes: (memory[pc] << 8) | memory[pc+1]'
        level2: 'Extract nibbles: first = (opcode >> 12), X = (opcode >> 8) & 0xF, Y = (opcode >> 4) & 0xF, N = opcode & 0xF'
        level3: "void cycle(Chip8* chip) {\n    // Fetch\n    uint16_t opcode = (chip->memory[chip->pc] << 8) | chip->memory[chip->pc\
          \ + 1];\n    chip->pc += 2;\n\n    // Decode - extract components\n    uint8_t first_nibble = (opcode >> 12) & 0xF;\n\
          \    uint8_t X = (opcode >> 8) & 0xF;   // Second nibble (register)\n    uint8_t Y = (opcode >> 4) & 0xF;   // Third\
          \ nibble (register)\n    uint8_t N = opcode & 0xF;          // Fourth nibble\n    uint8_t NN = opcode & 0xFF;  \
          \      // Last byte\n    uint16_t NNN = opcode & 0xFFF;     // Last 12 bits (address)\n\n    // Execute (switch\
          \ on first_nibble)\n    switch (first_nibble) {\n        case 0x0:\n            if (opcode == 0x00E0) clear_display(chip);\n\
          \            else if (opcode == 0x00EE) return_from_subroutine(chip);\n            break;\n        case 0x1: chip->pc\
          \ = NNN; break;  // Jump\n        case 0x2: call_subroutine(chip, NNN); break;\n        // ... more cases\n    }\n\
          }"
      pitfalls:
      - Wrong byte order when fetching opcode
      - Not incrementing PC after fetch (or incrementing wrong amount)
      - Running too fast or too slow (should be ~500-700 Hz)
      concepts:
      - CPU fetch-decode-execute cycle
      - Big-endian byte order
      - Instruction decoding
      estimated_hours: 4-5
    - id: 3
      name: Core Instructions
      description: Implement the 35 CHIP-8 instructions including arithmetic, logic, jumps, and subroutines.
      acceptance_criteria:
      - All 35 standard opcodes implemented
      - 'Arithmetic: ADD, SUB, SHR, SHL with proper flag handling'
      - 'Logic: AND, OR, XOR'
      - 'Flow control: JP, CALL, RET, SE, SNE'
      - 'Register operations: LD, RND'
      hints:
        level1: Group instructions by first nibble. Most 8xxx instructions are arithmetic/logic between VX and VY.
        level2: For subtraction, VF=1 means NO borrow. For addition, VF=1 means carry. This is counterintuitive!
        level3: "// Key instruction implementations\ncase 0x8:  // 8XYN - Arithmetic/Logic\n    switch (N) {\n        case\
          \ 0x0: chip->V[X] = chip->V[Y]; break;           // LD\n        case 0x1: chip->V[X] |= chip->V[Y]; break;     \
          \     // OR\n        case 0x2: chip->V[X] &= chip->V[Y]; break;          // AND\n        case 0x3: chip->V[X] ^=\
          \ chip->V[Y]; break;          // XOR\n        case 0x4: {  // ADD with carry\n            uint16_t sum = chip->V[X]\
          \ + chip->V[Y];\n            chip->V[0xF] = (sum > 255) ? 1 : 0;\n            chip->V[X] = sum & 0xFF;\n       \
          \     break;\n        }\n        case 0x5: {  // SUB (VF=1 if NO borrow)\n            chip->V[0xF] = (chip->V[X]\
          \ >= chip->V[Y]) ? 1 : 0;\n            chip->V[X] -= chip->V[Y];\n            break;\n        }\n        case 0x6:\
          \ {  // SHR (shift right, VF = bit shifted out)\n            chip->V[0xF] = chip->V[X] & 0x1;\n            chip->V[X]\
          \ >>= 1;\n            break;\n        }\n        case 0xE: {  // SHL (shift left)\n            chip->V[0xF] = (chip->V[X]\
          \ >> 7) & 0x1;\n            chip->V[X] <<= 1;\n            break;\n        }\n    }\n    break;\n\ncase 0xC:  //\
          \ CXNN - Random\n    chip->V[X] = (rand() % 256) & NN;\n    break;"
      pitfalls:
      - Getting VF flag logic backwards for SUB/SUBN
      - Forgetting that VF is modified AFTER the operation (store temp first)
      - BNNN jump should use V0, not VX (ambiguous in old docs)
      concepts:
      - ALU operations
      - Flag registers
      - Conditional branching
      estimated_hours: 5-6
    - id: 4
      name: Graphics and Display
      description: Implement the 64x32 monochrome display and DXYN draw instruction with XOR sprite drawing.
      acceptance_criteria:
      - 64x32 pixel display buffer
      - 00E0 clears the screen
      - DXYN draws N-byte sprite at (VX, VY)
      - XOR drawing with collision detection (VF=1 if pixel turned off)
      - Sprites wrap around screen edges
      hints:
        level1: Each sprite row is 8 pixels wide (1 byte). DXYN draws N rows starting from memory[I].
        level2: 'XOR means: if pixel is on and sprite bit is 1, pixel turns off (collision). If off and bit is 1, turns on.'
        level3: "void draw_sprite(Chip8* chip, uint8_t X, uint8_t Y, uint8_t N) {\n    uint8_t x_pos = chip->V[X] % 64;  //\
          \ Wrap around\n    uint8_t y_pos = chip->V[Y] % 32;\n    chip->V[0xF] = 0;  // Reset collision flag\n\n    for (int\
          \ row = 0; row < N; row++) {\n        uint8_t sprite_byte = chip->memory[chip->I + row];\n\n        for (int col\
          \ = 0; col < 8; col++) {\n            uint8_t sprite_pixel = (sprite_byte >> (7 - col)) & 1;\n\n            int\
          \ screen_x = (x_pos + col) % 64;\n            int screen_y = (y_pos + row) % 32;\n            int index = screen_y\
          \ * 64 + screen_x;\n\n            if (sprite_pixel) {\n                if (chip->display[index]) {\n           \
          \         chip->V[0xF] = 1;  // Collision!\n                }\n                chip->display[index] ^= 1;  // XOR\n\
          \            }\n        }\n    }\n}"
      pitfalls:
      - Not wrapping coordinates around screen edges
      - Setting VF before checking all pixels (should be set if ANY collision)
      - Drawing bit order wrong (MSB is leftmost pixel)
      concepts:
      - Framebuffer
      - XOR graphics
      - Sprite rendering
      - Collision detection
      estimated_hours: 4-5
    - id: 5
      name: Input and Timers
      description: Implement 16-key hexadecimal keypad input and 60Hz delay/sound timers.
      acceptance_criteria:
      - 16 keys (0-9, A-F) mapped to keyboard
      - EX9E/EXA1 skip if key pressed/not pressed
      - FX0A waits for key press (blocking)
      - Delay timer decrements at 60Hz
      - Sound timer plays tone when > 0
      hints:
        level1: 'Map keyboard keys to CHIP-8''s 4x4 keypad. Common mapping: 1234/QWER/ASDF/ZXCV â†’ 123C/456D/789E/A0BF'
        level2: FX0A is blocking - the emulator should pause until a key is pressed, then store key in VX.
        level3: "// Timer update (call at 60Hz, separate from instruction cycle)\nvoid update_timers(Chip8* chip) {\n    if\
          \ (chip->delay_timer > 0) chip->delay_timer--;\n    if (chip->sound_timer > 0) {\n        if (chip->sound_timer\
          \ == 1) play_beep();\n        chip->sound_timer--;\n    }\n}\n\n// Key instructions\ncase 0xE:\n    if (NN == 0x9E)\
          \ {  // Skip if key VX pressed\n        if (chip->keypad[chip->V[X]]) chip->pc += 2;\n    } else if (NN == 0xA1)\
          \ {  // Skip if key VX not pressed\n        if (!chip->keypad[chip->V[X]]) chip->pc += 2;\n    }\n    break;\n\n\
          case 0xF:\n    switch (NN) {\n        case 0x07: chip->V[X] = chip->delay_timer; break;\n        case 0x15: chip->delay_timer\
          \ = chip->V[X]; break;\n        case 0x18: chip->sound_timer = chip->V[X]; break;\n        case 0x0A: {  // Wait\
          \ for key press\n            bool pressed = false;\n            for (int i = 0; i < 16; i++) {\n               \
          \ if (chip->keypad[i]) {\n                    chip->V[X] = i;\n                    pressed = true;\n           \
          \         break;\n                }\n            }\n            if (!pressed) chip->pc -= 2;  // Repeat this instruction\n\
          \            break;\n        }\n    }\n    break;"
      pitfalls:
      - Running timers at instruction speed instead of 60Hz
      - Not handling FX0A blocking correctly
      - Key mapping confusion (CHIP-8 keys != keyboard layout)
      concepts:
      - Hardware timers
      - Input polling
      - Blocking I/O
      estimated_hours: 4-5
  ci-pipeline:
    id: ci-pipeline
    name: CI Pipeline Setup
    description: Set up continuous integration pipelines. Automate testing and build verification.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - Git
    - Unit testing
    - Command line
    languages:
      recommended:
      - YAML
      - Bash
      also_possible: []
    resources:
    - name: GitHub Actions Documentation
      url: https://docs.github.com/en/actions
      type: documentation
    - name: Jenkins Pipeline
      url: https://www.jenkins.io/doc/book/pipeline/
      type: documentation
    milestones:
    - id: 1
      name: Basic GitHub Actions
      description: Set up basic CI with GitHub Actions.
      acceptance_criteria:
      - Create workflow YAML file
      - Trigger on push and PR
      - Run tests
      - Show status badge
      hints:
        level1: Workflow files go in .github/workflows/
        level2: Use actions/checkout and actions/setup-* for language.
        level3: "# .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches:\
          \ [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n\
          \      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\
          \      \n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n       \
          \   pip install pytest\n      \n      - name: Run tests\n        run: pytest\n\n# Badge in README:\n# ![CI](https://github.com/user/repo/actions/workflows/ci.yml/badge.svg)"
      pitfalls:
      - YAML indentation
      - Not caching dependencies
      - Secrets in logs
      concepts:
      - CI/CD basics
      - Workflow syntax
      - Job runners
      estimated_hours: 2-3
    - id: 2
      name: Multi-Job Pipeline
      description: Create pipeline with multiple jobs and stages.
      acceptance_criteria:
      - Lint job
      - Test job
      - Build job
      - Job dependencies
      - Matrix builds
      hints:
        level1: Use 'needs' to order jobs.
        level2: Matrix strategy for multiple versions.
        level3: "jobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: pip\
          \ install flake8 && flake8 .\n  \n  test:\n    needs: lint\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n\
          \        python-version: ['3.9', '3.10', '3.11']\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n\
          \        with:\n          python-version: ${{ matrix.python-version }}\n      - run: pip install -r requirements.txt\
          \ && pytest\n  \n  build:\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\
          \      - run: docker build -t myapp .\n      - uses: actions/upload-artifact@v3\n        with:\n          name:\
          \ docker-image\n          path: ./image.tar"
      pitfalls:
      - Circular dependencies
      - Long-running matrices
      - Artifact retention costs
      concepts:
      - Job dependencies
      - Matrix builds
      - Artifacts
      estimated_hours: 2-3
    - id: 3
      name: Advanced CI Features
      description: Add caching, secrets, and conditional jobs.
      acceptance_criteria:
      - Cache dependencies
      - Use secrets safely
      - Conditional execution
      - Reusable workflows
      hints:
        level1: Cache pip/npm packages to speed up builds.
        level2: Never echo secrets, they're masked.
        level3: "jobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n     \
          \ - name: Cache pip packages\n        uses: actions/cache@v3\n        with:\n          path: ~/.cache/pip\n    \
          \      key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}\n          restore-keys: |\n            ${{\
          \ runner.os }}-pip-\n      \n      - name: Run tests\n        run: pytest\n      \n      - name: Deploy to staging\n\
          \        if: github.ref == 'refs/heads/main'\n        env:\n          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }}\n  \
          \      run: ./deploy.sh staging\n\n  # Conditional job\n  deploy-prod:\n    if: github.event_name == 'release'\n\
          \    needs: test\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - run: ./deploy.sh\
          \ production"
      pitfalls:
      - Cache invalidation
      - Secret rotation
      - Conditional syntax
      concepts:
      - Caching
      - Secrets management
      - Environments
      estimated_hours: 3-4
  code-review-practice:
    id: code-review-practice
    name: Code Review Practice
    description: Learn effective code review techniques by reviewing real pull requests.
    difficulty: intermediate
    estimated_hours: 10-20
    prerequisites:
    - Programming experience
    - Version control (Git)
    languages:
      recommended:
      - Any
      also_possible: []
    resources:
    - type: article
      name: Google's Code Review Guidelines
      url: https://google.github.io/eng-practices/review/
    - type: book
      name: The Art of Readable Code
      url: https://www.oreilly.com/library/view/the-art-of/9781449318482/
    milestones:
    - id: 1
      name: Review Fundamentals
      description: Learn what to look for in code reviews.
      acceptance_criteria:
      - Correctness checks
      - Style consistency
      - Performance considerations
      - Security review
      hints:
        level1: 'Start with: Does it work? Then: Is it maintainable? Finally: Is it optimal?'
        level2: Check edge cases, error handling, and test coverage. Look for security issues.
        level3: '## Code Review Checklist


          ### Correctness

          - [ ] Does the code do what the PR description says?

          - [ ] Are edge cases handled?

          - [ ] Are errors handled appropriately?

          - [ ] Do the tests actually test the functionality?


          ### Design

          - [ ] Is the code in the right place (right file, right layer)?

          - [ ] Does it follow existing patterns in the codebase?

          - [ ] Is it over-engineered or under-engineered?

          - [ ] Are there any obvious performance issues?


          ### Readability

          - [ ] Are variable/function names clear and descriptive?

          - [ ] Is the code self-documenting or properly commented?

          - [ ] Is the logic easy to follow?

          - [ ] Are there magic numbers that should be constants?


          ### Security (especially for web apps)

          - [ ] Is user input validated and sanitized?

          - [ ] Are there any SQL injection vulnerabilities?

          - [ ] Is authentication/authorization checked?

          - [ ] Are secrets hardcoded?


          ### Testing

          - [ ] Are there tests for new functionality?

          - [ ] Do tests cover edge cases?

          - [ ] Are tests readable and maintainable?

          - [ ] Is test coverage adequate?'
      pitfalls:
      - Nitpicking style over substance
      - Missing the forest for trees
      - Being too harsh
      concepts:
      - Code quality
      - Defensive programming
      - Best practices
      estimated_hours: 2-4
    - id: 2
      name: Giving Feedback
      description: Practice giving constructive, actionable feedback.
      acceptance_criteria:
      - Constructive tone
      - Specific suggestions
      - Distinguishing blockers from suggestions
      - Asking questions vs making demands
      hints:
        level1: Be kind. Critique the code, not the person. Offer alternatives.
        level2: Prefix with 'nit:', 'suggestion:', 'blocking:' to indicate severity.
        level3: "## Feedback Examples\n\n### Bad Feedback\nâŒ \"This is wrong.\"\nâŒ \"Why would you do it this way?\"\nâŒ \"\
          This code is confusing.\"\n\n### Good Feedback\nâœ… \"This will throw if `user` is null. Consider adding a null check:\n\
          \   ```javascript\n   if (!user) return null;\n   ```\"\n\nâœ… \"nit: This could be simplified using optional chaining:\n\
          \   ```javascript\n   // Instead of\n   user && user.profile && user.profile.name\n   // Consider\n   user?.profile?.name\n\
          \   ```\"\n\nâœ… \"question: I'm not familiar with this pattern - could you explain \n   why we're caching here instead\
          \ of at the service layer?\"\n\nâœ… \"suggestion: This loop runs O(nÂ²). For large lists, we might want\n   to use\
          \ a Set for O(n) lookup. Not blocking since our current\n   data is small, but worth considering for the future.\"\
          \n\n### Severity Prefixes\n- **blocking**: Must fix before merge\n- **suggestion**: Recommended but optional\n-\
          \ **nit**: Minor style/preference (totally optional)\n- **question**: Seeking clarification\n- **praise**: Highlighting\
          \ good work! \U0001F389"
      pitfalls:
      - Being overly negative
      - Vague feedback
      - Not explaining why
      concepts:
      - Communication
      - Empathy
      - Teaching
      estimated_hours: 3-5
    - id: 3
      name: Review Exercises
      description: Practice reviewing real code with common issues.
      acceptance_criteria:
      - Find bugs in provided code
      - Suggest improvements
      - Identify security issues
      - Evaluate test quality
      hints:
        level1: Look at open source projects' PRs. Many have good review discussions.
        level2: Practice with intentionally buggy code. Time yourself to build speed.
        level3: "## Exercise: Review This Code\n\n```javascript\n// User authentication endpoint\napp.post('/login', async\
          \ (req, res) => {\n  const { email, password } = req.body;\n  \n  const user = await db.query(\n    `SELECT * FROM\
          \ users WHERE email = '${email}'`\n  );\n  \n  if (user && password === user.password) {\n    const token = jwt.sign({\
          \ id: user.id }, 'secret123');\n    res.json({ token });\n  } else {\n    res.status(401).json({ error: 'Invalid\
          \ credentials' });\n  }\n});\n```\n\n### Issues to Find:\n1. **SQL Injection** (Critical): Using string interpolation\
          \ in query\n   - Fix: Use parameterized queries\n   \n2. **Plaintext passwords** (Critical): Comparing passwords\
          \ directly\n   - Fix: Use bcrypt.compare()\n   \n3. **Hardcoded secret** (High): JWT secret in code\n   - Fix: Use\
          \ environment variable\n   \n4. **Timing attack** (Medium): Different response times for user-exists vs wrong-password\n\
          \   - Fix: Always do password comparison\n   \n5. **No input validation** (Medium): email/password not validated\n\
          \   - Fix: Validate format and length\n   \n6. **Missing rate limiting** (Medium): Allows brute force\n   - Fix:\
          \ Add rate limiter middleware"
      pitfalls:
      - Missing subtle bugs
      - Focusing only on obvious issues
      - Not considering context
      concepts:
      - Bug patterns
      - Security awareness
      - Code smells
      estimated_hours: 3-6
    - id: 4
      name: Receiving Feedback
      description: Learn to respond professionally to code review feedback.
      acceptance_criteria:
      - Accepting valid criticism
      - Discussing disagreements constructively
      - Learning from reviews
      - Knowing when to push back
      hints:
        level1: Assume good intent. Reviewers want to help. Ask for clarification if needed.
        level2: It's okay to disagree, but explain your reasoning. Be open to being wrong.
        level3: '## Responding to Reviews


          ### Good Responses


          **To valid feedback:**

          > "Good catch! Fixed in abc123."


          **To suggestions you disagree with:**

          > "I considered that approach, but went with X because [reason].

          > Happy to change if you feel strongly - what do you think?"


          **To feedback you don''t understand:**

          > "Could you elaborate on this? I''m not sure I understand

          > the concern with the current approach."


          **To nitpicks you''ll skip:**

          > "Noted for future PRs! Keeping as-is for now to limit

          > scope of this change."


          ### Handling Disagreements


          1. **Assume good intent** - They''re trying to help

          2. **Seek to understand** - Ask questions

          3. **Explain your reasoning** - Share context they might lack

          4. **Find common ground** - What do you both agree on?

          5. **Escalate gracefully** - Get a third opinion if stuck


          ### When to Push Back


          - Reviewer is enforcing personal preference, not team standard

          - Suggested change would introduce bugs or regressions

          - Change is out of scope for this PR

          - You have context the reviewer doesn''t


          Always push back respectfully with reasoning!'
      pitfalls:
      - Taking feedback personally
      - Being defensive
      - Blindly accepting all feedback
      concepts:
      - Professional growth
      - Collaboration
      - Ego management
      estimated_hours: 2-5
  config-parser:
    id: config-parser
    name: Config File Parser
    description: Build a multi-format configuration file parser supporting INI, TOML, and YAML-subset. Learn recursive descent
      parsing and data structure mapping.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - String manipulation
    - Data structures
    - Recursive thinking
    languages:
      recommended:
      - Python
      - Go
      - Rust
      also_possible:
      - JavaScript
      - C
    resources:
    - name: TOML Specification
      url: https://toml.io/en/v1.0.0
      type: specification
    - name: INI File Format
      url: https://en.wikipedia.org/wiki/INI_file
      type: reference
    - name: Writing a Parser in Python
      url: https://www.freecodecamp.org/news/how-to-write-a-parser-in-python/
      type: tutorial
    milestones:
    - id: 1
      name: INI Parser
      description: Parse INI files with sections, key-value pairs, and comments.
      acceptance_criteria:
      - Parse [section] headers
      - 'Parse key=value and key: value pairs'
      - 'Handle ; and # comments'
      - Support quoted strings with escapes
      - Return nested dictionary structure
      hints:
        level1: 'INI is line-based. Process line by line: check if it''s a section, comment, or key-value.'
        level2: 'Use regex or simple string operations. Section: ^\[(.+)\]$, Comment: ^[;#], Key-value: split on first = or
          :'
        level3: "import re\n\ndef parse_ini(text: str) -> dict:\n    result = {}\n    current_section = None\n\n    for line_num,\
          \ line in enumerate(text.splitlines(), 1):\n        line = line.strip()\n\n        # Skip empty lines and comments\n\
          \        if not line or line.startswith(';') or line.startswith('#'):\n            continue\n\n        # Section\
          \ header\n        if line.startswith('[') and line.endswith(']'):\n            current_section = line[1:-1].strip()\n\
          \            if current_section not in result:\n                result[current_section] = {}\n            continue\n\
          \n        # Key-value pair\n        match = re.match(r'^([^=:]+)[=:](.*)$', line)\n        if match:\n         \
          \   key = match.group(1).strip()\n            value = match.group(2).strip()\n\n            # Remove quotes\n  \
          \          if (value.startswith('\"') and value.endswith('\"')) or \\\n               (value.startswith(\"'\") and\
          \ value.endswith(\"'\")):\n                value = value[1:-1]\n\n            # Type coercion\n            if value.lower()\
          \ in ('true', 'yes', 'on'):\n                value = True\n            elif value.lower() in ('false', 'no', 'off'):\n\
          \                value = False\n            elif value.isdigit():\n                value = int(value)\n\n      \
          \      target = result[current_section] if current_section else result\n            target[key] = value\n      \
          \  else:\n            raise ValueError(f\"Invalid line {line_num}: {line}\")\n\n    return result"
      pitfalls:
      - Not handling keys outside of sections (global keys)
      - Forgetting to handle inline comments after values
      - Breaking on = inside quoted strings
      concepts:
      - Line-based parsing
      - Regular expressions
      - Type coercion
      estimated_hours: 3-4
    - id: 2
      name: TOML Tokenizer
      description: Build a lexer/tokenizer for TOML format.
      acceptance_criteria:
      - 'Tokenize: brackets, dots, equals, strings, numbers'
      - Handle basic strings, literal strings, multiline strings
      - Recognize integers, floats, booleans, dates
      - Track line/column for error messages
      hints:
        level1: TOML has more complex string types than INI. Basic strings use ", literal strings use '.
        level2: Create Token class with type, value, line, column. Use a Lexer class that tracks position.
        level3: "from enum import Enum\nfrom dataclasses import dataclass\n\nclass TokenType(Enum):\n    LBRACKET = '['\n\
          \    RBRACKET = ']'\n    LBRACE = '{'\n    RBRACE = '}'\n    DOT = '.'\n    EQUALS = '='\n    COMMA = ','\n    STRING\
          \ = 'STRING'\n    INTEGER = 'INTEGER'\n    FLOAT = 'FLOAT'\n    BOOLEAN = 'BOOLEAN'\n    DATETIME = 'DATETIME'\n\
          \    BARE_KEY = 'BARE_KEY'\n    NEWLINE = 'NEWLINE'\n    EOF = 'EOF'\n\n@dataclass\nclass Token:\n    type: TokenType\n\
          \    value: any\n    line: int\n    column: int\n\nclass Lexer:\n    def __init__(self, text: str):\n        self.text\
          \ = text\n        self.pos = 0\n        self.line = 1\n        self.column = 1\n\n    def peek(self) -> str:\n \
          \       return self.text[self.pos] if self.pos < len(self.text) else ''\n\n    def advance(self) -> str:\n     \
          \   ch = self.peek()\n        self.pos += 1\n        if ch == '\\n':\n            self.line += 1\n            self.column\
          \ = 1\n        else:\n            self.column += 1\n        return ch\n\n    def read_string(self, quote: str) ->\
          \ str:\n        # Handle basic string (\") with escapes\n        # Handle literal string (') without escapes\n \
          \       # Handle multiline (''' or \"\"\")\n        pass\n\n    def tokenize(self) -> list[Token]:\n        tokens\
          \ = []\n        while self.pos < len(self.text):\n            ch = self.peek()\n            if ch in ' \\t':\n \
          \               self.advance()\n            elif ch == '\\n':\n                tokens.append(Token(TokenType.NEWLINE,\
          \ None, self.line, self.column))\n                self.advance()\n            elif ch == '#':\n                while\
          \ self.peek() and self.peek() != '\\n':\n                    self.advance()\n            # ... handle other token\
          \ types\n        return tokens"
      pitfalls:
      - TOML multiline strings have complex rules for leading newlines
      - Literal strings don't process escapes (backslash is literal)
      - 'Integer underscores are allowed: 1_000_000'
      concepts:
      - Lexical analysis
      - State machines
      - Unicode handling
      estimated_hours: 4-5
    - id: 3
      name: TOML Parser
      description: Build recursive descent parser for TOML tables and arrays.
      acceptance_criteria:
      - Parse [table] and [table.subtable] headers
      - Parse [[array.of.tables]]
      - Handle inline tables { key = value }
      - Handle inline arrays [ 1, 2, 3 ]
      - 'Dotted keys: physical.color = ''orange'''
      hints:
        level1: 'TOML keys can define nested structure: a.b.c = 1 creates {a: {b: {c: 1}}}'
        level2: '[[array]] appends to an array of tables. Each [[array]] block is a new element.'
        level3: "class Parser:\n    def __init__(self, tokens: list[Token]):\n        self.tokens = tokens\n        self.pos\
          \ = 0\n        self.result = {}\n        self.current_table = self.result\n\n    def parse_key(self) -> list[str]:\n\
          \        '''Parse dotted key like a.b.c into [\"a\", \"b\", \"c\"]'''\n        keys = []\n        while True:\n\
          \            tok = self.expect(TokenType.BARE_KEY, TokenType.STRING)\n            keys.append(tok.value)\n     \
          \       if not self.match(TokenType.DOT):\n                break\n        return keys\n\n    def set_nested(self,\
          \ keys: list[str], value: any):\n        '''Set value at nested key path'''\n        target = self.current_table\n\
          \        for key in keys[:-1]:\n            if key not in target:\n                target[key] = {}\n          \
          \  target = target[key]\n        target[keys[-1]] = value\n\n    def parse_table_header(self):\n        '''Parse\
          \ [table] or [[array.of.tables]]'''\n        is_array = self.match(TokenType.LBRACKET)  # Second [\n        keys\
          \ = self.parse_key()\n        self.expect(TokenType.RBRACKET)\n        if is_array:\n            self.expect(TokenType.RBRACKET)\n\
          \n        # Navigate to (or create) the table\n        target = self.result\n        for i, key in enumerate(keys):\n\
          \            if key not in target:\n                target[key] = [] if (is_array and i == len(keys)-1) else {}\n\
          \            target = target[key]\n            if isinstance(target, list):\n                if i == len(keys) -\
          \ 1:\n                    target.append({})\n                target = target[-1]\n\n        self.current_table =\
          \ target"
      pitfalls:
      - Can't redefine a key that already exists
      - Tables can be defined implicitly by dotted keys
      - Array of tables vs array value have different syntax
      concepts:
      - Recursive descent parsing
      - Symbol tables
      - Nested data structures
      estimated_hours: 5-6
    - id: 4
      name: YAML Subset Parser
      description: 'Parse a subset of YAML: indentation-based nesting, mappings, sequences.'
      acceptance_criteria:
      - Indentation-based block structure
      - 'Key: value mappings'
      - Sequences with - prefix
      - Quoted and unquoted strings
      - 'Flow style: [list] and {map}'
      hints:
        level1: Track indentation level. Deeper indent = nested structure. Same indent = sibling.
        level2: Build a stack of (indent_level, container). When indent decreases, pop back to matching level.
        level3: "def parse_yaml(text: str) -> dict:\n    lines = text.splitlines()\n    result = {}\n    stack = [(0, result)]\
          \  # (indent_level, container)\n\n    for line in lines:\n        stripped = line.lstrip()\n        if not stripped\
          \ or stripped.startswith('#'):\n            continue\n\n        indent = len(line) - len(stripped)\n\n        #\
          \ Pop stack until we find appropriate parent\n        while stack and stack[-1][0] >= indent:\n            stack.pop()\n\
          \n        parent = stack[-1][1] if stack else result\n\n        if stripped.startswith('- '):\n            # Sequence\
          \ item\n            value = stripped[2:].strip()\n            if isinstance(parent, dict):\n                # First\
          \ item, convert to list (need parent key)\n                pass\n            parent.append(parse_value(value) if\
          \ value else {})\n            if not value:\n                stack.append((indent + 2, parent[-1]))\n        elif\
          \ ':' in stripped:\n            # Mapping\n            key, _, value = stripped.partition(':')\n            key\
          \ = key.strip()\n            value = value.strip()\n\n            if value:\n                parent[key] = parse_value(value)\n\
          \            else:\n                parent[key] = {}\n                stack.append((indent + 2, parent[key]))\n\n\
          \    return result\n\ndef parse_value(s: str):\n    if s.startswith('['):\n        return parse_flow_sequence(s)\n\
          \    if s.startswith('{'):\n        return parse_flow_mapping(s)\n    if s.lower() in ('true', 'yes'):\n       \
          \ return True\n    if s.lower() in ('false', 'no'):\n        return False\n    if s.lower() == 'null':\n       \
          \ return None\n    try:\n        return int(s)\n    except:\n        try:\n            return float(s)\n       \
          \ except:\n            return s.strip('\"\\''')"
      pitfalls:
      - Tabs vs spaces (YAML forbids tabs for indentation)
      - Implicit type detection can be surprising (yes = true, 1.0 = float)
      - Multiline strings have multiple syntaxes (|, >, etc.)
      concepts:
      - Indentation-sensitive parsing
      - Implicit typing
      - Stack-based parsing
      estimated_hours: 5-6
  container-basic:
    id: container-basic
    name: Container (Basic)
    description: Build a simple container using Linux namespaces. Learn process isolation and cgroups basics.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Linux system calls
    - Process management
    - Filesystem basics
    languages:
      recommended:
      - C
      - Go
      - Rust
      also_possible:
      - Python
    resources:
    - name: Linux Namespaces
      url: https://man7.org/linux/man-pages/man7/namespaces.7.html
      type: documentation
    - name: Containers from Scratch
      url: https://ericchiang.github.io/post/containers-from-scratch/
      type: article
    milestones:
    - id: 1
      name: Process Namespace
      description: Isolate process tree with PID namespace.
      acceptance_criteria:
      - Create new PID namespace
      - Child sees itself as PID 1
      - Parent sees real PID
      - Handle zombie processes
      hints:
        level1: clone() with CLONE_NEWPID creates new PID namespace.
        level2: First process in namespace is PID 1, must reap children.
        level3: "#define _GNU_SOURCE\n#include <sched.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include\
          \ <sys/wait.h>\n\nstatic int child_fn(void *arg) {\n    printf(\"Child PID: %d\\n\", getpid());  // Will print 1\n\
          \    \n    // Mount proc for accurate /proc\n    mount(\"proc\", \"/proc\", \"proc\", 0, NULL);\n    \n    // Execute\
          \ the command\n    char *argv[] = {\"/bin/sh\", NULL};\n    execv(argv[0], argv);\n    return 1;\n}\n\nint main()\
          \ {\n    #define STACK_SIZE (1024 * 1024)\n    char *stack = malloc(STACK_SIZE);\n    \n    pid_t pid = clone(\n\
          \        child_fn,\n        stack + STACK_SIZE,  // Stack grows down\n        CLONE_NEWPID | SIGCHLD,\n        NULL\n\
          \    );\n    \n    printf(\"Parent sees child PID: %d\\n\", pid);  // Real PID\n    waitpid(pid, NULL, 0);\n   \
          \ \n    return 0;\n}"
      pitfalls:
      - Stack direction
      - PID 1 responsibilities
      - Zombie processes
      concepts:
      - PID namespaces
      - clone()
      - Process isolation
      estimated_hours: 3-4
    - id: 2
      name: Mount Namespace
      description: Isolate filesystem mounts.
      acceptance_criteria:
      - New mount namespace
      - Private mount propagation
      - Pivot root to new filesystem
      - Mount essential filesystems
      hints:
        level1: CLONE_NEWNS isolates mounts. pivot_root changes root.
        level2: Need to mount /proc, /sys, /dev inside container.
        level3: "void setup_mount_namespace(const char *rootfs) {\n    // Make all mounts private\n    mount(NULL, \"/\",\
          \ NULL, MS_REC | MS_PRIVATE, NULL);\n    \n    // Mount the new root filesystem\n    mount(rootfs, rootfs, \"bind\"\
          , MS_BIND | MS_REC, NULL);\n    \n    // Create mount points\n    char path[256];\n    snprintf(path, sizeof(path),\
          \ \"%s/proc\", rootfs);\n    mkdir(path, 0755);\n    snprintf(path, sizeof(path), \"%s/sys\", rootfs);\n    mkdir(path,\
          \ 0755);\n    snprintf(path, sizeof(path), \"%s/dev\", rootfs);\n    mkdir(path, 0755);\n    \n    // Mount essential\
          \ filesystems\n    snprintf(path, sizeof(path), \"%s/proc\", rootfs);\n    mount(\"proc\", path, \"proc\", 0, NULL);\n\
          \    \n    snprintf(path, sizeof(path), \"%s/sys\", rootfs);\n    mount(\"sysfs\", path, \"sysfs\", 0, NULL);\n\
          \    \n    // Pivot root\n    char old_root[256];\n    snprintf(old_root, sizeof(old_root), \"%s/.old_root\", rootfs);\n\
          \    mkdir(old_root, 0755);\n    \n    if (pivot_root(rootfs, old_root) < 0) {\n        perror(\"pivot_root\");\n\
          \        exit(1);\n    }\n    \n    chdir(\"/\");\n    \n    // Unmount old root\n    umount2(\"/.old_root\", MNT_DETACH);\n\
          \    rmdir(\"/.old_root\");\n}"
      pitfalls:
      - Mount propagation
      - pivot_root requirements
      - Device nodes
      concepts:
      - Mount namespaces
      - pivot_root
      - Filesystem isolation
      estimated_hours: 4-5
    - id: 3
      name: Network Namespace
      description: Isolate network stack.
      acceptance_criteria:
      - Create network namespace
      - Set up veth pair
      - Configure IP addresses
      - Enable container networking
      hints:
        level1: CLONE_NEWNET gives isolated network stack. veth connects to host.
        level2: 'veth pair: one end in host, one in container namespace.'
        level3: "// Note: This typically requires root and netlink or ip commands\n\nvoid setup_network(pid_t child_pid) {\n\
          \    char cmd[256];\n    \n    // Create veth pair\n    snprintf(cmd, sizeof(cmd), \n             \"ip link add\
          \ veth0 type veth peer name veth1\");\n    system(cmd);\n    \n    // Move veth1 to container namespace\n    snprintf(cmd,\
          \ sizeof(cmd),\n             \"ip link set veth1 netns %d\", child_pid);\n    system(cmd);\n    \n    // Configure\
          \ host side\n    system(\"ip addr add 10.0.0.1/24 dev veth0\");\n    system(\"ip link set veth0 up\");\n    \n \
          \   // Enable IP forwarding and NAT\n    system(\"echo 1 > /proc/sys/net/ipv4/ip_forward\");\n    system(\"iptables\
          \ -t nat -A POSTROUTING -s 10.0.0.0/24 -j MASQUERADE\");\n}\n\nvoid setup_network_inside_container() {\n    // Run\
          \ inside container namespace\n    system(\"ip addr add 10.0.0.2/24 dev veth1\");\n    system(\"ip link set veth1\
          \ up\");\n    system(\"ip link set lo up\");\n    system(\"ip route add default via 10.0.0.1\");\n}"
      pitfalls:
      - Namespace timing
      - veth cleanup
      - NAT configuration
      concepts:
      - Network namespaces
      - veth pairs
      - Container networking
      estimated_hours: 4-5
    - id: 4
      name: Cgroups (Resource Limits)
      description: Limit CPU, memory, and other resources.
      acceptance_criteria:
      - Create cgroup for container
      - Set memory limit
      - Set CPU quota
      - Clean up cgroup on exit
      hints:
        level1: 'cgroups v2: create directory in /sys/fs/cgroup, write limits.'
        level2: 'Memory limit: memory.max. CPU: cpu.max (quota period).'
        level3: "void setup_cgroups(pid_t pid, int memory_limit_mb, int cpu_percent) {\n    char path[256];\n    char value[64];\n\
          \    \n    // Create cgroup directory (cgroups v2)\n    snprintf(path, sizeof(path), \"/sys/fs/cgroup/container_%d\"\
          , pid);\n    mkdir(path, 0755);\n    \n    // Set memory limit\n    snprintf(path, sizeof(path), \n            \
          \ \"/sys/fs/cgroup/container_%d/memory.max\", pid);\n    snprintf(value, sizeof(value), \"%d\", memory_limit_mb\
          \ * 1024 * 1024);\n    write_file(path, value);\n    \n    // Set CPU quota (e.g., 50% = 50000 100000)\n    snprintf(path,\
          \ sizeof(path),\n             \"/sys/fs/cgroup/container_%d/cpu.max\", pid);\n    snprintf(value, sizeof(value),\
          \ \"%d 100000\", cpu_percent * 1000);\n    write_file(path, value);\n    \n    // Add process to cgroup\n    snprintf(path,\
          \ sizeof(path),\n             \"/sys/fs/cgroup/container_%d/cgroup.procs\", pid);\n    snprintf(value, sizeof(value),\
          \ \"%d\", pid);\n    write_file(path, value);\n}\n\nvoid cleanup_cgroups(pid_t pid) {\n    char path[256];\n   \
          \ snprintf(path, sizeof(path), \"/sys/fs/cgroup/container_%d\", pid);\n    rmdir(path);  // Only works if empty\
          \ (process exited)\n}"
      pitfalls:
      - cgroups v1 vs v2
      - Controller availability
      - Cleanup order
      concepts:
      - cgroups
      - Resource limits
      - Container resources
      estimated_hours: 4-5
  diff-tool:
    id: diff-tool
    name: Diff Tool
    description: Build a text diff tool using the Longest Common Subsequence algorithm and Myers' diff algorithm. Learn dynamic
      programming and edit distance concepts.
    difficulty: beginner
    estimated_hours: 12-18
    prerequisites:
    - Dynamic programming basics
    - File I/O
    - String manipulation
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - C
      - Rust
      - Java
    resources:
    - name: Myers' Diff Algorithm Tutorial
      url: http://simplygenius.net/Article/DiffTutorial1
      type: tutorial
    - name: The Myers Difference Algorithm
      url: https://nathaniel.ai/myers-diff/
      type: article
    - name: Wikipedia - LCS
      url: https://en.wikipedia.org/wiki/Longest_common_subsequence
      type: reference
    milestones:
    - id: 1
      name: Line Tokenization
      description: Read two files and split into line arrays for comparison.
      acceptance_criteria:
      - Read files handling different encodings
      - Split by newlines preserving empty lines
      - Handle different line endings (\n, \r\n, \r)
      - Report line counts for each file
      hints:
        level1: Use splitlines() in Python or split by regex /\r?\n/ in JavaScript.
        level2: Consider normalizing line endings first, or preserve them for accurate diff.
        level3: "def read_lines(filepath: str) -> list[str]:\n    '''Read file and return list of lines'''\n    with open(filepath,\
          \ 'r', encoding='utf-8') as f:\n        content = f.read()\n\n    # Normalize line endings\n    content = content.replace('\\\
          r\\n', '\\n').replace('\\r', '\\n')\n\n    # Split into lines (keeping empty lines)\n    lines = content.split('\\\
          n')\n\n    # Remove trailing empty line if file ends with newline\n    if lines and lines[-1] == '':\n        lines\
          \ = lines[:-1]\n\n    return lines\n\ndef main():\n    file1_lines = read_lines(sys.argv[1])\n    file2_lines =\
          \ read_lines(sys.argv[2])\n\n    print(f\"File 1: {len(file1_lines)} lines\")\n    print(f\"File 2: {len(file2_lines)}\
          \ lines\")"
      pitfalls:
      - Binary files will cause encoding errors
      - Large files can exhaust memory
      - Trailing newline handling varies between tools
      concepts:
      - File encoding
      - Line endings
      - Text normalization
      estimated_hours: 2-3
    - id: 2
      name: LCS Algorithm
      description: Implement Longest Common Subsequence using dynamic programming.
      acceptance_criteria:
      - Build LCS length matrix
      - Backtrack to find actual LCS
      - Handle sequences of different lengths
      - O(mn) time and space complexity
      hints:
        level1: 'LCS matrix: if items match, dp[i][j] = dp[i-1][j-1] + 1. Otherwise, max of left or up.'
        level2: 'Backtrack from dp[m][n]: if match, include item and go diagonal. Otherwise, go to larger neighbor.'
        level3: "def lcs_matrix(seq1: list, seq2: list) -> list[list[int]]:\n    '''Build LCS length matrix'''\n    m, n =\
          \ len(seq1), len(seq2)\n\n    # dp[i][j] = length of LCS of seq1[:i] and seq2[:j]\n    dp = [[0] * (n + 1) for _\
          \ in range(m + 1)]\n\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if seq1[i-1]\
          \ == seq2[j-1]:\n                dp[i][j] = dp[i-1][j-1] + 1\n            else:\n                dp[i][j] = max(dp[i-1][j],\
          \ dp[i][j-1])\n\n    return dp\n\ndef backtrack_lcs(dp: list[list[int]], seq1: list, seq2: list) -> list:\n    '''Backtrack\
          \ to find LCS'''\n    lcs = []\n    i, j = len(seq1), len(seq2)\n\n    while i > 0 and j > 0:\n        if seq1[i-1]\
          \ == seq2[j-1]:\n            lcs.append((i-1, j-1, seq1[i-1]))  # (idx1, idx2, item)\n            i -= 1\n     \
          \       j -= 1\n        elif dp[i-1][j] > dp[i][j-1]:\n            i -= 1\n        else:\n            j -= 1\n\n\
          \    lcs.reverse()\n    return lcs"
      pitfalls:
      - Off-by-one errors in matrix indexing
      - Not handling empty sequences
      - Memory explosion for large files (optimize with Hirschberg)
      concepts:
      - Dynamic programming
      - 2D matrices
      - Backtracking
      estimated_hours: 4-5
    - id: 3
      name: Diff Generation
      description: Convert LCS result into diff hunks with context.
      acceptance_criteria:
      - Mark lines as unchanged, added, or deleted
      - Generate unified diff format (-/+ prefixes)
      - Group changes into hunks with context lines
      - Include @@ line range markers
      hints:
        level1: Walk both sequences. If in LCS, it's unchanged. Otherwise, if only in seq1, deleted. Only in seq2, added.
        level2: 'Unified format: context lines have '' '' prefix, deletions ''-'', additions ''+''.'
        level3: "def generate_diff(lines1: list[str], lines2: list[str], context: int = 3):\n    '''Generate unified diff\
          \ output'''\n    lcs_indices = set()\n    for idx1, idx2, _ in backtrack_lcs(lcs_matrix(lines1, lines2), lines1,\
          \ lines2):\n        lcs_indices.add(('1', idx1))\n        lcs_indices.add(('2', idx2))\n\n    # Build diff operations\n\
          \    ops = []\n    i, j = 0, 0\n\n    while i < len(lines1) or j < len(lines2):\n        if ('1', i) in lcs_indices\
          \ and ('2', j) in lcs_indices:\n            ops.append((' ', lines1[i]))\n            i += 1\n            j += 1\n\
          \        elif ('1', i) not in lcs_indices and i < len(lines1):\n            ops.append(('-', lines1[i]))\n     \
          \       i += 1\n        elif ('2', j) not in lcs_indices and j < len(lines2):\n            ops.append(('+', lines2[j]))\n\
          \            j += 1\n\n    # Group into hunks with context\n    hunks = []\n    current_hunk = []\n    # ... group\
          \ changes separated by more than context*2 unchanged lines\n\n    # Output\n    for hunk in hunks:\n        print(f\"\
          @@ -{hunk.start1},{hunk.len1} +{hunk.start2},{hunk.len2} @@\")\n        for op, line in hunk.lines:\n          \
          \  print(f\"{op}{line}\")"
      pitfalls:
      - Off-by-one in line numbers (diff format is 1-indexed)
      - Forgetting to handle files with no common lines
      - Context overlapping between hunks
      concepts:
      - Diff algorithms
      - Unified diff format
      - Hunk generation
      estimated_hours: 4-5
    - id: 4
      name: CLI and Color Output
      description: Build command-line interface with colored output and options.
      acceptance_criteria:
      - Accept two file paths as arguments
      - 'Color output: red for deletions, green for additions'
      - --no-color flag for plain output
      - --context N to set context lines
      - Exit code 0 if same, 1 if different
      hints:
        level1: 'Use ANSI escape codes: \033[31m for red, \033[32m for green, \033[0m to reset.'
        level2: Check if stdout is a TTY before using colors. Use --no-color to force plain.
        level3: "import argparse\nimport sys\n\nclass Colors:\n    RED = '\\033[31m'\n    GREEN = '\\033[32m'\n    CYAN =\
          \ '\\033[36m'\n    RESET = '\\033[0m'\n\n    @classmethod\n    def disable(cls):\n        cls.RED = cls.GREEN =\
          \ cls.CYAN = cls.RESET = ''\n\ndef main():\n    parser = argparse.ArgumentParser(description='Compare two files')\n\
          \    parser.add_argument('file1', help='First file')\n    parser.add_argument('file2', help='Second file')\n   \
          \ parser.add_argument('-u', '--unified', type=int, default=3,\n                        help='Number of context lines\
          \ (default: 3)')\n    parser.add_argument('--no-color', action='store_true',\n                        help='Disable\
          \ colored output')\n    args = parser.parse_args()\n\n    if args.no_color or not sys.stdout.isatty():\n       \
          \ Colors.disable()\n\n    lines1 = read_lines(args.file1)\n    lines2 = read_lines(args.file2)\n\n    if lines1\
          \ == lines2:\n        sys.exit(0)\n\n    # Print header\n    print(f\"{Colors.CYAN}--- {args.file1}{Colors.RESET}\"\
          )\n    print(f\"{Colors.CYAN}+++ {args.file2}{Colors.RESET}\")\n\n    for hunk in generate_hunks(lines1, lines2,\
          \ args.unified):\n        print(f\"{Colors.CYAN}@@ ... @@{Colors.RESET}\")\n        for op, line in hunk:\n    \
          \        if op == '-':\n                print(f\"{Colors.RED}-{line}{Colors.RESET}\")\n            elif op == '+':\n\
          \                print(f\"{Colors.GREEN}+{line}{Colors.RESET}\")\n            else:\n                print(f\" {line}\"\
          )\n\n    sys.exit(1)"
      pitfalls:
      - ANSI codes break when piped to file
      - Windows needs special handling for colors
      - Exit codes are important for scripting
      concepts:
      - CLI argument parsing
      - ANSI colors
      - Exit codes
      - TTY detection
      estimated_hours: 3-4
  disassembler:
    id: disassembler
    name: x86 Disassembler
    description: Build an x86/x64 instruction disassembler. Learn machine code encoding, instruction formats, and binary parsing.
    difficulty: advanced
    estimated_hours: 35-50
    prerequisites:
    - x86 assembly basics
    - Binary file handling
    - Bitwise operations
    languages:
      recommended:
      - C
      - Rust
      - Python
      also_possible:
      - Go
      - C++
    resources:
    - name: Intel x86 Manual Vol. 2
      url: https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html
      type: reference
    - name: Medium - Building x86-64 Disassembler
      url: https://medium.com/@Koukyosyumei/learning-x86-64-machine-language-and-assembly-by-implementing-a-disassembler-dccc736ae85f
      type: tutorial
    - name: x86 Instruction Encoding
      url: http://wiki.osdev.org/X86-64_Instruction_Encoding
      type: reference
    milestones:
    - id: 1
      name: Binary File Loading
      description: Load and parse ELF or PE executable headers to find code sections.
      acceptance_criteria:
      - Parse ELF header (or PE for Windows)
      - Locate .text section
      - Extract code bytes and base address
      - Handle 32-bit and 64-bit binaries
      hints:
        level1: 'ELF magic: 0x7F ''E'' ''L'' ''F''. Check byte 5 for 32/64-bit. Section headers list all sections.'
        level2: e_shoff points to section header table. e_shstrndx is section name string table index.
        level3: "from dataclasses import dataclass\nfrom struct import unpack\n\n@dataclass\nclass Section:\n    name: str\n\
          \    addr: int\n    offset: int\n    size: int\n    data: bytes\n\ndef parse_elf(data: bytes) -> tuple[int, list[Section]]:\n\
          \    '''Parse ELF and return (bitness, sections)'''\n    if data[:4] != b'\\x7fELF':\n        raise ValueError(\"\
          Not an ELF file\")\n\n    is_64bit = data[4] == 2\n    is_little = data[5] == 1\n    endian = '<' if is_little else\
          \ '>'\n\n    if is_64bit:\n        # 64-bit header\n        e_shoff = unpack(f'{endian}Q', data[40:48])[0]\n   \
          \     e_shentsize = unpack(f'{endian}H', data[58:60])[0]\n        e_shnum = unpack(f'{endian}H', data[60:62])[0]\n\
          \        e_shstrndx = unpack(f'{endian}H', data[62:64])[0]\n    else:\n        # 32-bit header\n        e_shoff\
          \ = unpack(f'{endian}I', data[32:36])[0]\n        e_shentsize = unpack(f'{endian}H', data[46:48])[0]\n        e_shnum\
          \ = unpack(f'{endian}H', data[48:50])[0]\n        e_shstrndx = unpack(f'{endian}H', data[50:52])[0]\n\n    # Parse\
          \ section headers\n    sections = []\n    # ... read section names from shstrtab, extract .text\n\n    return (64\
          \ if is_64bit else 32, sections)"
      pitfalls:
      - Endianness varies (most x86 is little-endian)
      - Virtual address != file offset
      - Some binaries are stripped (no symbol table)
      concepts:
      - Executable formats
      - Binary parsing
      - Memory mapping
      estimated_hours: 6-8
    - id: 2
      name: Instruction Prefixes
      description: 'Decode x86 instruction prefixes: legacy, REX, VEX.'
      acceptance_criteria:
      - Detect legacy prefixes (66, 67, F0, F2, F3, segment overrides)
      - Decode REX prefix (64-bit mode)
      - Extract REX.W, REX.R, REX.X, REX.B bits
      - Handle prefix ordering
      hints:
        level1: 'REX prefix: 0x40-0x4F in 64-bit mode. Format: 0100WRXB.'
        level2: 66h = operand size override, 67h = address size override. They change default sizes.
        level3: "@dataclass\nclass Prefixes:\n    rex: int = 0        # Full REX byte (0 if none)\n    rex_w: bool = False\
          \ # 64-bit operand\n    rex_r: bool = False # Extends ModRM.reg\n    rex_x: bool = False # Extends SIB.index\n \
          \   rex_b: bool = False # Extends ModRM.rm or SIB.base\n    operand_size: bool = False  # 66h prefix\n    address_size:\
          \ bool = False  # 67h prefix\n    rep: int = 0        # F2/F3 prefix\n    lock: bool = False  # F0 prefix\n    segment:\
          \ int = 0    # Segment override\n\ndef decode_prefixes(code: bytes, offset: int, is_64bit: bool) -> tuple[Prefixes,\
          \ int]:\n    '''Decode prefixes, return (Prefixes, bytes_consumed)'''\n    prefixes = Prefixes()\n    pos = offset\n\
          \n    while pos < len(code):\n        byte = code[pos]\n\n        if byte == 0x66:\n            prefixes.operand_size\
          \ = True\n        elif byte == 0x67:\n            prefixes.address_size = True\n        elif byte == 0xF0:\n   \
          \         prefixes.lock = True\n        elif byte in (0xF2, 0xF3):\n            prefixes.rep = byte\n        elif\
          \ byte in (0x2E, 0x3E, 0x26, 0x64, 0x65, 0x36):\n            prefixes.segment = byte\n        elif is_64bit and\
          \ 0x40 <= byte <= 0x4F:\n            prefixes.rex = byte\n            prefixes.rex_w = bool(byte & 0x08)\n     \
          \       prefixes.rex_r = bool(byte & 0x04)\n            prefixes.rex_x = bool(byte & 0x02)\n            prefixes.rex_b\
          \ = bool(byte & 0x01)\n        else:\n            break  # Not a prefix\n\n        pos += 1\n\n    return prefixes,\
          \ pos - offset"
      pitfalls:
      - REX must be immediately before opcode
      - Some prefix combinations are invalid or have special meaning
      - VEX/EVEX prefixes in modern code need separate handling
      concepts:
      - Prefix encoding
      - Mode-dependent behavior
      - Bit field extraction
      estimated_hours: 6-8
    - id: 3
      name: Opcode Tables
      description: Build opcode lookup tables for instruction identification.
      acceptance_criteria:
      - One-byte opcode table
      - Two-byte opcode table (0F xx)
      - Handle opcode extensions via ModRM.reg
      - Map opcodes to mnemonics
      hints:
        level1: Many opcodes share byte but differ by ModRM.reg field. E.g., 0x80 is ADD/OR/ADC/SBB/AND/SUB/XOR/CMP.
        level2: 'Create separate tables: one_byte_opcodes, two_byte_opcodes, extension_groups.'
        level3: "# Opcode table entry\n@dataclass\nclass OpcodeEntry:\n    mnemonic: str\n    operands: str  # e.g., \"Ev,Gv\"\
          \ or \"rAX,Iz\"\n    flags: int = 0\n    extension_group: int = 0  # 0 = no extension, 1-17 = group number\n\n#\
          \ One-byte opcode table (partial)\nONE_BYTE = {\n    0x00: OpcodeEntry(\"ADD\", \"Eb,Gb\"),\n    0x01: OpcodeEntry(\"\
          ADD\", \"Ev,Gv\"),\n    0x02: OpcodeEntry(\"ADD\", \"Gb,Eb\"),\n    0x03: OpcodeEntry(\"ADD\", \"Gv,Ev\"),\n   \
          \ 0x04: OpcodeEntry(\"ADD\", \"AL,Ib\"),\n    0x05: OpcodeEntry(\"ADD\", \"rAX,Iz\"),\n    # ...\n    0x50: OpcodeEntry(\"\
          PUSH\", \"rAX\"),  # 50-57 are PUSH r16/r64\n    # ...\n    0x80: OpcodeEntry(\"\", \"Eb,Ib\", extension_group=1),\
          \  # Group 1\n    0x81: OpcodeEntry(\"\", \"Ev,Iz\", extension_group=1),\n    # ...\n    0x0F: OpcodeEntry(\"\"\
          , \"\", flags=TWO_BYTE),  # Escape to two-byte table\n}\n\n# Extension group 1: ADD/OR/ADC/SBB/AND/SUB/XOR/CMP\n\
          GROUP_1 = [\"ADD\", \"OR\", \"ADC\", \"SBB\", \"AND\", \"SUB\", \"XOR\", \"CMP\"]\n\ndef lookup_opcode(byte: int,\
          \ modrm_reg: int = 0) -> OpcodeEntry:\n    entry = ONE_BYTE.get(byte)\n    if entry and entry.extension_group:\n\
          \        mnemonic = GROUPS[entry.extension_group][modrm_reg]\n        return OpcodeEntry(mnemonic, entry.operands)\n\
          \    return entry"
      pitfalls:
      - Opcode tables are large and error-prone to type manually
      - Some opcodes are invalid in certain modes
      - Three-byte opcodes exist (0F 38 xx, 0F 3A xx)
      concepts:
      - Lookup tables
      - Instruction encoding
      - Opcode extensions
      estimated_hours: 8-10
    - id: 4
      name: ModRM and SIB Decoding
      description: Decode ModRM and SIB bytes for operand addressing.
      acceptance_criteria:
      - 'Parse ModRM: mod, reg, rm fields'
      - Handle SIB when rm=100b
      - Decode all 16 addressing modes
      - Handle RIP-relative addressing (64-bit)
      hints:
        level1: ModRM = mod(2) | reg(3) | rm(3). mod=11 means register, otherwise memory.
        level2: SIB = scale(2) | index(3) | base(3). Address = base + index * scale + disp.
        level3: "def decode_modrm(code: bytes, offset: int, prefixes: Prefixes, is_64bit: bool) -> tuple[str, int]:\n    '''Decode\
          \ ModRM, return (operand_string, bytes_consumed)'''\n    modrm = code[offset]\n    mod = (modrm >> 6) & 0x3\n  \
          \  reg = (modrm >> 3) & 0x7\n    rm = modrm & 0x7\n\n    # Apply REX extensions\n    if prefixes.rex_r:\n      \
          \  reg |= 0x8\n    if prefixes.rex_b:\n        rm |= 0x8\n\n    consumed = 1\n\n    if mod == 0b11:\n        # Register\
          \ direct\n        return get_register(rm, prefixes), consumed\n\n    # Memory addressing\n    if rm == 0b100 and\
          \ not is_64bit:\n        # SIB follows (32-bit) or rm=100 with REX.B (64-bit)\n        sib = code[offset + 1]\n\
          \        consumed += 1\n        scale = 1 << ((sib >> 6) & 0x3)\n        index = (sib >> 3) & 0x7\n        base\
          \ = sib & 0x7\n\n        if prefixes.rex_x:\n            index |= 0x8\n        if prefixes.rex_b:\n            base\
          \ |= 0x8\n\n        # Build address string\n        # ...\n\n    if mod == 0b00 and rm == 0b101:\n        if is_64bit:\n\
          \            # RIP-relative\n            disp = unpack('<i', code[offset+consumed:offset+consumed+4])[0]\n     \
          \       consumed += 4\n            return f\"[rip + {disp:#x}]\", consumed\n        else:\n            # 32-bit\
          \ displacement only\n            disp = unpack('<i', code[offset+consumed:offset+consumed+4])[0]\n            consumed\
          \ += 4\n            return f\"[{disp:#x}]\", consumed\n\n    # ... handle other mod values"
      pitfalls:
      - RIP-relative is 64-bit only, in 32-bit mod=00 rm=101 is disp32
      - SIB with index=100 means no index (unless REX.X set)
      - REX extends registers to r8-r15
      concepts:
      - Addressing modes
      - Register encoding
      - Memory operands
      estimated_hours: 8-10
    - id: 5
      name: Output Formatting
      description: Format disassembly output with addresses, bytes, and mnemonics.
      acceptance_criteria:
      - Show address, hex bytes, mnemonic, operands
      - Intel and AT&T syntax options
      - Label jumps to known addresses
      - Handle undefined/invalid opcodes gracefully
      hints:
        level1: 'Intel: op dst, src. AT&T: op src, dst (with size suffixes and % for registers).'
        level2: Track branch targets to insert labels. Relative jumps need base address calculation.
        level3: "class Disassembler:\n    def __init__(self, code: bytes, base_addr: int, is_64bit: bool):\n        self.code\
          \ = code\n        self.base = base_addr\n        self.is_64bit = is_64bit\n        self.labels = {}  # addr -> label\
          \ name\n\n    def disassemble(self) -> list[str]:\n        # First pass: find branch targets\n        offset = 0\n\
          \        while offset < len(self.code):\n            instr = self.decode_instruction(offset)\n            if instr.is_branch:\n\
          \                target = instr.branch_target\n                if target not in self.labels:\n                 \
          \   self.labels[target] = f\"loc_{target:x}\"\n            offset += instr.length\n\n        # Second pass: generate\
          \ output\n        output = []\n        offset = 0\n        while offset < len(self.code):\n            addr = self.base\
          \ + offset\n\n            # Insert label if this is a branch target\n            if addr in self.labels:\n     \
          \           output.append(f\"\\n{self.labels[addr]}:\")\n\n            instr = self.decode_instruction(offset)\n\
          \            hex_bytes = self.code[offset:offset+instr.length].hex()\n\n            output.append(f\"{addr:08x}:\
          \  {hex_bytes:<20}  {instr}\")\n            offset += instr.length\n\n        return output\n\n    def format_intel(self,\
          \ mnemonic: str, operands: list[str]) -> str:\n        return f\"{mnemonic} {', '.join(operands)}\"\n\n    def format_att(self,\
          \ mnemonic: str, operands: list[str]) -> str:\n        # Reverse operands, add suffixes\n        return f\"{mnemonic}\
          \ {', '.join(reversed(operands))}\"\n\n# Output example:\n# 00401000:  55                    push rbp\n# 00401001:\
          \  48 89 e5              mov rbp, rsp\n# 00401004:  e8 17 00 00 00        call loc_401020"
      pitfalls:
      - Hex bytes should be padded/aligned for readability
      - Relative addresses need base address to calculate target
      - Invalid opcodes should print as .byte or db
      concepts:
      - Assembly syntax
      - Address calculation
      - Output formatting
      estimated_hours: 6-8
  distributed-cache:
    id: distributed-cache
    name: Distributed Cache
    description: Build a distributed cache with consistent hashing.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - Hash tables
    - Networking basics
    - Consistent hashing concept
    languages:
      recommended:
      - Go
      - Java
      - Python
      also_possible:
      - Rust
      - JavaScript
    resources:
    - type: article
      name: Consistent Hashing
      url: https://www.toptal.com/big-data/consistent-hashing
    - type: paper
      name: Dynamo Paper
      url: https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf
    milestones:
    - id: 1
      name: Consistent Hash Ring
      description: Implement consistent hashing for key distribution.
      acceptance_criteria:
      - Hash ring implementation
      - Virtual nodes
      - Node addition/removal
      - Key lookup
      hints:
        level1: Hash both nodes and keys to positions on a ring (0 to 2^32-1).
        level2: 'Virtual nodes: each physical node gets multiple positions for better distribution.'
        level3: "## Consistent Hash Ring\n\n```go\npackage cache\n\nimport (\n    \"hash/crc32\"\n    \"sort\"\n    \"sync\"\
          \n)\n\ntype ConsistentHash struct {\n    mu           sync.RWMutex\n    ring         []uint32          // Sorted\
          \ hash values\n    nodes        map[uint32]string // Hash -> node ID\n    virtualNodes int               // Virtual\
          \ nodes per physical node\n}\n\nfunc NewConsistentHash(virtualNodes int) *ConsistentHash {\n    return &ConsistentHash{\n\
          \        nodes:        make(map[uint32]string),\n        virtualNodes: virtualNodes,\n    }\n}\n\nfunc (c *ConsistentHash)\
          \ hash(key string) uint32 {\n    return crc32.ChecksumIEEE([]byte(key))\n}\n\nfunc (c *ConsistentHash) AddNode(nodeID\
          \ string) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    for i := 0; i < c.virtualNodes; i++ {\n       \
          \ virtualKey := fmt.Sprintf(\"%s-%d\", nodeID, i)\n        h := c.hash(virtualKey)\n        c.ring = append(c.ring,\
          \ h)\n        c.nodes[h] = nodeID\n    }\n    \n    sort.Slice(c.ring, func(i, j int) bool {\n        return c.ring[i]\
          \ < c.ring[j]\n    })\n}\n\nfunc (c *ConsistentHash) RemoveNode(nodeID string) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n\
          \    \n    newRing := make([]uint32, 0)\n    for _, h := range c.ring {\n        if c.nodes[h] != nodeID {\n   \
          \         newRing = append(newRing, h)\n        } else {\n            delete(c.nodes, h)\n        }\n    }\n   \
          \ c.ring = newRing\n}\n\nfunc (c *ConsistentHash) GetNode(key string) string {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n\
          \    \n    if len(c.ring) == 0 {\n        return \"\"\n    }\n    \n    h := c.hash(key)\n    \n    // Binary search\
          \ for first node >= hash\n    idx := sort.Search(len(c.ring), func(i int) bool {\n        return c.ring[i] >= h\n\
          \    })\n    \n    if idx == len(c.ring) {\n        idx = 0  // Wrap around\n    }\n    \n    return c.nodes[c.ring[idx]]\n\
          }\n\n// Get N nodes for replication\nfunc (c *ConsistentHash) GetNodes(key string, n int) []string {\n    c.mu.RLock()\n\
          \    defer c.mu.RUnlock()\n    \n    if len(c.ring) == 0 {\n        return nil\n    }\n    \n    h := c.hash(key)\n\
          \    idx := sort.Search(len(c.ring), func(i int) bool {\n        return c.ring[i] >= h\n    })\n    \n    seen :=\
          \ make(map[string]bool)\n    result := make([]string, 0, n)\n    \n    for i := 0; len(result) < n && i < len(c.ring);\
          \ i++ {\n        nodeID := c.nodes[c.ring[(idx+i)%len(c.ring)]]\n        if !seen[nodeID] {\n            seen[nodeID]\
          \ = true\n            result = append(result, nodeID)\n        }\n    }\n    \n    return result\n}\n```"
      pitfalls:
      - Poor hash distribution
      - Not enough virtual nodes
      - Race conditions
      concepts:
      - Consistent hashing
      - Load balancing
      - Ring topology
      estimated_hours: 6-10
    - id: 2
      name: Cache Node Implementation
      description: Implement a cache node with LRU eviction.
      acceptance_criteria:
      - Get/Set/Delete operations
      - LRU eviction
      - TTL support
      - Memory limit
      hints:
        level1: Use hashmap + doubly linked list for O(1) LRU operations.
        level2: Track memory usage, evict LRU items when limit exceeded.
        level3: "## LRU Cache Node\n\n```go\ntype CacheEntry struct {\n    key        string\n    value      []byte\n    expireAt\
          \   time.Time\n    prev, next *CacheEntry\n}\n\ntype CacheNode struct {\n    mu          sync.RWMutex\n    data\
          \        map[string]*CacheEntry\n    head, tail  *CacheEntry  // LRU list\n    maxMemory   int64\n    usedMemory\
          \  int64\n}\n\nfunc NewCacheNode(maxMemory int64) *CacheNode {\n    c := &CacheNode{\n        data:      make(map[string]*CacheEntry),\n\
          \        maxMemory: maxMemory,\n    }\n    // Initialize sentinel nodes\n    c.head = &CacheEntry{}\n    c.tail\
          \ = &CacheEntry{}\n    c.head.next = c.tail\n    c.tail.prev = c.head\n    return c\n}\n\nfunc (c *CacheNode) moveToFront(entry\
          \ *CacheEntry) {\n    // Remove from current position\n    entry.prev.next = entry.next\n    entry.next.prev = entry.prev\n\
          \    \n    // Add to front\n    entry.next = c.head.next\n    entry.prev = c.head\n    c.head.next.prev = entry\n\
          \    c.head.next = entry\n}\n\nfunc (c *CacheNode) Get(key string) ([]byte, bool) {\n    c.mu.Lock()\n    defer\
          \ c.mu.Unlock()\n    \n    entry, ok := c.data[key]\n    if !ok {\n        return nil, false\n    }\n    \n    //\
          \ Check expiration\n    if !entry.expireAt.IsZero() && time.Now().After(entry.expireAt) {\n        c.removeEntry(entry)\n\
          \        return nil, false\n    }\n    \n    c.moveToFront(entry)\n    return entry.value, true\n}\n\nfunc (c *CacheNode)\
          \ Set(key string, value []byte, ttl time.Duration) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    entrySize\
          \ := int64(len(key) + len(value))\n    \n    // Evict if necessary\n    for c.usedMemory + entrySize > c.maxMemory\
          \ && c.tail.prev != c.head {\n        c.removeEntry(c.tail.prev)\n    }\n    \n    entry := &CacheEntry{\n     \
          \   key:   key,\n        value: value,\n    }\n    if ttl > 0 {\n        entry.expireAt = time.Now().Add(ttl)\n\
          \    }\n    \n    // Remove old entry if exists\n    if old, ok := c.data[key]; ok {\n        c.removeEntry(old)\n\
          \    }\n    \n    // Add new entry\n    c.data[key] = entry\n    c.usedMemory += entrySize\n    \n    // Add to\
          \ front of LRU list\n    entry.next = c.head.next\n    entry.prev = c.head\n    c.head.next.prev = entry\n    c.head.next\
          \ = entry\n}\n\nfunc (c *CacheNode) removeEntry(entry *CacheEntry) {\n    entry.prev.next = entry.next\n    entry.next.prev\
          \ = entry.prev\n    \n    c.usedMemory -= int64(len(entry.key) + len(entry.value))\n    delete(c.data, entry.key)\n\
          }\n```"
      pitfalls:
      - Memory accounting errors
      - TTL cleanup overhead
      - Lock contention
      concepts:
      - LRU cache
      - Memory management
      - TTL expiration
      estimated_hours: 6-10
    - id: 3
      name: Cluster Communication
      description: Implement inter-node communication and routing.
      acceptance_criteria:
      - Node discovery
      - Request routing
      - Health checks
      - Cluster membership
      hints:
        level1: Each node needs to know about others. Route requests to correct node.
        level2: Use gossip or central coordinator for membership. Health check regularly.
        level3: "## Cluster Communication\n\n```go\ntype ClusterNode struct {\n    ID       string\n    Address  string\n\
          \    Healthy  bool\n    LastSeen time.Time\n}\n\ntype CacheCluster struct {\n    localNode   *CacheNode\n    nodeID\
          \      string\n    hashRing    *ConsistentHash\n    nodes       map[string]*ClusterNode\n    mu          sync.RWMutex\n\
          }\n\nfunc (c *CacheCluster) Get(key string) ([]byte, error) {\n    targetNode := c.hashRing.GetNode(key)\n    \n\
          \    if targetNode == c.nodeID {\n        // Local\n        val, ok := c.localNode.Get(key)\n        if !ok {\n\
          \            return nil, ErrNotFound\n        }\n        return val, nil\n    }\n    \n    // Remote\n    return\
          \ c.forwardGet(targetNode, key)\n}\n\nfunc (c *CacheCluster) forwardGet(nodeID, key string) ([]byte, error) {\n\
          \    c.mu.RLock()\n    node, ok := c.nodes[nodeID]\n    c.mu.RUnlock()\n    \n    if !ok || !node.Healthy {\n  \
          \      return nil, ErrNodeUnavailable\n    }\n    \n    // HTTP request to remote node\n    resp, err := http.Get(fmt.Sprintf(\"\
          http://%s/cache/%s\", node.Address, key))\n    if err != nil {\n        return nil, err\n    }\n    defer resp.Body.Close()\n\
          \    \n    if resp.StatusCode == http.StatusNotFound {\n        return nil, ErrNotFound\n    }\n    \n    return\
          \ io.ReadAll(resp.Body)\n}\n\n// Health checker\nfunc (c *CacheCluster) healthCheck() {\n    ticker := time.NewTicker(5\
          \ * time.Second)\n    for range ticker.C {\n        c.mu.Lock()\n        for id, node := range c.nodes {\n     \
          \       if id == c.nodeID {\n                continue\n            }\n            \n            resp, err := http.Get(fmt.Sprintf(\"\
          http://%s/health\", node.Address))\n            if err != nil || resp.StatusCode != http.StatusOK {\n          \
          \      node.Healthy = false\n                if time.Since(node.LastSeen) > 30*time.Second {\n                 \
          \   // Remove dead node\n                    c.hashRing.RemoveNode(id)\n                    delete(c.nodes, id)\n\
          \                }\n            } else {\n                node.Healthy = true\n                node.LastSeen = time.Now()\n\
          \            }\n        }\n        c.mu.Unlock()\n    }\n}\n```"
      pitfalls:
      - Split brain scenarios
      - Network partition handling
      - Stale membership info
      concepts:
      - Distributed systems
      - Health checking
      - Service discovery
      estimated_hours: 8-12
    - id: 4
      name: Replication & Consistency
      description: Add data replication for fault tolerance.
      acceptance_criteria:
      - Configurable replication factor
      - Read/write quorums
      - Conflict resolution
      - Anti-entropy
      hints:
        level1: 'Replicate to N nodes. Use quorums: W + R > N for consistency.'
        level2: Last-write-wins or vector clocks for conflict resolution.
        level3: "## Replication\n\n```go\ntype ReplicatedCache struct {\n    cluster         *CacheCluster\n    replicationFactor\
          \ int  // N\n    writeQuorum     int  // W\n    readQuorum      int  // R\n}\n\nfunc (r *ReplicatedCache) Set(key\
          \ string, value []byte, ttl time.Duration) error {\n    nodes := r.cluster.hashRing.GetNodes(key, r.replicationFactor)\n\
          \    \n    // Write to all replicas, wait for quorum\n    results := make(chan error, len(nodes))\n    for _, nodeID\
          \ := range nodes {\n        go func(id string) {\n            results <- r.cluster.writeToNode(id, key, value, ttl)\n\
          \        }(nodeID)\n    }\n    \n    successCount := 0\n    var lastErr error\n    for i := 0; i < len(nodes); i++\
          \ {\n        if err := <-results; err == nil {\n            successCount++\n            if successCount >= r.writeQuorum\
          \ {\n                return nil  // Quorum achieved\n            }\n        } else {\n            lastErr = err\n\
          \        }\n    }\n    \n    return fmt.Errorf(\"write quorum not achieved: %v\", lastErr)\n}\n\nfunc (r *ReplicatedCache)\
          \ Get(key string) ([]byte, error) {\n    nodes := r.cluster.hashRing.GetNodes(key, r.replicationFactor)\n    \n\
          \    type readResult struct {\n        value     []byte\n        timestamp int64\n        err       error\n    }\n\
          \    results := make(chan readResult, len(nodes))\n    \n    for _, nodeID := range nodes {\n        go func(id\
          \ string) {\n            val, ts, err := r.cluster.readFromNode(id, key)\n            results <- readResult{val,\
          \ ts, err}\n        }(nodeID)\n    }\n    \n    var values []readResult\n    for i := 0; i < len(nodes); i++ {\n\
          \        res := <-results\n        if res.err == nil {\n            values = append(values, res)\n            if\
          \ len(values) >= r.readQuorum {\n                break\n            }\n        }\n    }\n    \n    if len(values)\
          \ < r.readQuorum {\n        return nil, ErrReadQuorumNotMet\n    }\n    \n    // Return most recent value (last-write-wins)\n\
          \    latest := values[0]\n    for _, v := range values[1:] {\n        if v.timestamp > latest.timestamp {\n    \
          \        latest = v\n        }\n    }\n    \n    return latest.value, nil\n}\n```"
      pitfalls:
      - Stale reads
      - Write conflicts
      - Replica divergence
      concepts:
      - Quorum systems
      - Eventual consistency
      - Conflict resolution
      estimated_hours: 8-12
  distributed-tracing:
    id: distributed-tracing
    name: Distributed Tracing System
    description: Build a distributed tracing system to track requests across microservices.
    difficulty: advanced
    estimated_hours: 40-60
    prerequisites:
    - Microservices
    - Networking
    - Data storage
    languages:
      recommended:
      - Go
      - Java
      also_possible:
      - Rust
      - Python
    resources:
    - name: OpenTelemetry
      url: https://opentelemetry.io/docs/
      type: documentation
    - name: Jaeger Architecture
      url: https://www.jaegertracing.io/docs/architecture/
      type: documentation
    milestones:
    - id: 1
      name: Trace Context & Propagation
      description: Implement trace/span IDs and context propagation.
      acceptance_criteria:
      - Generate unique trace and span IDs
      - Propagate via HTTP headers (W3C Trace Context)
      - Propagate via gRPC metadata
      - Parent-child span relationships
      - Context injection and extraction
      hints:
        level1: Trace ID = request ID across services. Span ID = operation within service.
        level2: 'W3C traceparent: version-traceid-spanid-flags (00-{32hex}-{16hex}-01).'
        level3: "import (\n    \"crypto/rand\"\n    \"encoding/hex\"\n    \"context\"\n    \"net/http\"\n)\n\ntype TraceID\
          \ [16]byte\ntype SpanID [8]byte\n\nfunc (t TraceID) String() string { return hex.EncodeToString(t[:]) }\nfunc (s\
          \ SpanID) String() string  { return hex.EncodeToString(s[:]) }\n\nfunc NewTraceID() TraceID {\n    var id TraceID\n\
          \    rand.Read(id[:])\n    return id\n}\n\nfunc NewSpanID() SpanID {\n    var id SpanID\n    rand.Read(id[:])\n\
          \    return id\n}\n\ntype SpanContext struct {\n    TraceID    TraceID\n    SpanID     SpanID\n    ParentID   SpanID\n\
          \    TraceFlags byte\n}\n\n// W3C Trace Context format\nconst (\n    traceparentHeader = \"traceparent\"\n    tracestateHeader\
          \  = \"tracestate\"\n)\n\ntype Propagator struct{}\n\nfunc (p *Propagator) Inject(ctx context.Context, carrier http.Header)\
          \ {\n    sc := SpanContextFromContext(ctx)\n    if sc == nil {\n        return\n    }\n\n    // Format: 00-{trace-id}-{span-id}-{trace-flags}\n\
          \    traceparent := fmt.Sprintf(\"00-%s-%s-%02x\",\n        sc.TraceID.String(),\n        sc.SpanID.String(),\n\
          \        sc.TraceFlags,\n    )\n\n    carrier.Set(traceparentHeader, traceparent)\n}\n\nfunc (p *Propagator) Extract(ctx\
          \ context.Context, carrier http.Header) context.Context {\n    traceparent := carrier.Get(traceparentHeader)\n \
          \   if traceparent == \"\" {\n        return ctx\n    }\n\n    // Parse: 00-{trace-id}-{span-id}-{trace-flags}\n\
          \    parts := strings.Split(traceparent, \"-\")\n    if len(parts) != 4 || parts[0] != \"00\" {\n        return\
          \ ctx\n    }\n\n    traceID, _ := hex.DecodeString(parts[1])\n    spanID, _ := hex.DecodeString(parts[2])\n    flags,\
          \ _ := strconv.ParseUint(parts[3], 16, 8)\n\n    var tid TraceID\n    var sid SpanID\n    copy(tid[:], traceID)\n\
          \    copy(sid[:], spanID)\n\n    sc := &SpanContext{\n        TraceID:    tid,\n        ParentID:   sid,  // Incoming\
          \ span becomes parent\n        SpanID:     NewSpanID(),  // Create new span for this service\n        TraceFlags:\
          \ byte(flags),\n    }\n\n    return ContextWithSpanContext(ctx, sc)\n}\n\n// Context helpers\ntype spanContextKey\
          \ struct{}\n\nfunc ContextWithSpanContext(ctx context.Context, sc *SpanContext) context.Context {\n    return context.WithValue(ctx,\
          \ spanContextKey{}, sc)\n}\n\nfunc SpanContextFromContext(ctx context.Context) *SpanContext {\n    sc, _ := ctx.Value(spanContextKey{}).(*SpanContext)\n\
          \    return sc\n}\n\n// HTTP middleware\nfunc TracingMiddleware(next http.Handler) http.Handler {\n    propagator\
          \ := &Propagator{}\n\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        // Extract\
          \ or create trace context\n        ctx := propagator.Extract(r.Context(), r.Header)\n\n        if SpanContextFromContext(ctx)\
          \ == nil {\n            // No incoming trace, start new one\n            sc := &SpanContext{\n                TraceID:\
          \    NewTraceID(),\n                SpanID:     NewSpanID(),\n                TraceFlags: 0x01,  // Sampled\n  \
          \          }\n            ctx = ContextWithSpanContext(ctx, sc)\n        }\n\n        r = r.WithContext(ctx)\n \
          \       next.ServeHTTP(w, r)\n    })\n}"
      pitfalls:
      - Trace ID not propagated to async operations
      - Invalid trace context parsing crashes
      - Generating new trace ID when should continue existing
      - Not handling malformed traceparent
      concepts:
      - Trace context
      - W3C Trace Context spec
      - Context propagation
      - Span hierarchy
      estimated_hours: 8-12
    - id: 2
      name: Span Recording
      description: Record span data with timing, tags, and logs.
      acceptance_criteria:
      - Start/end spans with timing
      - Add tags/attributes to spans
      - Add log events within spans
      - Record errors and exceptions
      - Span status (OK, Error)
      hints:
        level1: Span = name + start time + end time + attributes + events.
        level2: Buffer spans in memory, flush to collector periodically or on threshold.
        level3: "type Span struct {\n    TraceID     TraceID\n    SpanID      SpanID\n    ParentID    SpanID\n    Name   \
          \     string\n    StartTime   time.Time\n    EndTime     time.Time\n    Status      SpanStatus\n    Attributes \
          \ map[string]interface{}\n    Events      []SpanEvent\n    mu          sync.Mutex\n}\n\ntype SpanStatus struct {\n\
          \    Code    StatusCode\n    Message string\n}\n\ntype StatusCode int\n\nconst (\n    StatusUnset StatusCode = iota\n\
          \    StatusOK\n    StatusError\n)\n\ntype SpanEvent struct {\n    Name       string\n    Timestamp  time.Time\n\
          \    Attributes map[string]interface{}\n}\n\ntype Tracer struct {\n    serviceName string\n    exporter    SpanExporter\n\
          \    spans       chan *Span\n}\n\nfunc NewTracer(serviceName string, exporter SpanExporter) *Tracer {\n    t :=\
          \ &Tracer{\n        serviceName: serviceName,\n        exporter:    exporter,\n        spans:       make(chan *Span,\
          \ 1000),\n    }\n\n    // Background exporter\n    go t.exportLoop()\n\n    return t\n}\n\nfunc (t *Tracer) StartSpan(ctx\
          \ context.Context, name string) (context.Context, *Span) {\n    parentSC := SpanContextFromContext(ctx)\n\n    span\
          \ := &Span{\n        Name:       name,\n        StartTime:  time.Now(),\n        Attributes: make(map[string]interface{}),\n\
          \    }\n\n    if parentSC != nil {\n        span.TraceID = parentSC.TraceID\n        span.ParentID = parentSC.SpanID\n\
          \    } else {\n        span.TraceID = NewTraceID()\n    }\n    span.SpanID = NewSpanID()\n\n    // Add default attributes\n\
          \    span.SetAttribute(\"service.name\", t.serviceName)\n\n    sc := &SpanContext{\n        TraceID:  span.TraceID,\n\
          \        SpanID:   span.SpanID,\n        ParentID: span.ParentID,\n    }\n\n    return ContextWithSpanContext(ctx,\
          \ sc), span\n}\n\nfunc (s *Span) SetAttribute(key string, value interface{}) {\n    s.mu.Lock()\n    s.Attributes[key]\
          \ = value\n    s.mu.Unlock()\n}\n\nfunc (s *Span) AddEvent(name string, attrs map[string]interface{}) {\n    s.mu.Lock()\n\
          \    s.Events = append(s.Events, SpanEvent{\n        Name:       name,\n        Timestamp:  time.Now(),\n      \
          \  Attributes: attrs,\n    })\n    s.mu.Unlock()\n}\n\nfunc (s *Span) RecordError(err error) {\n    s.mu.Lock()\n\
          \    s.Status = SpanStatus{Code: StatusError, Message: err.Error()}\n    s.Events = append(s.Events, SpanEvent{\n\
          \        Name:      \"exception\",\n        Timestamp: time.Now(),\n        Attributes: map[string]interface{}{\n\
          \            \"exception.type\":    fmt.Sprintf(\"%T\", err),\n            \"exception.message\": err.Error(),\n\
          \        },\n    })\n    s.mu.Unlock()\n}\n\nfunc (s *Span) End() {\n    s.mu.Lock()\n    s.EndTime = time.Now()\n\
          \    s.mu.Unlock()\n\n    // Send to exporter\n    tracer.spans <- s\n}\n\nfunc (t *Tracer) exportLoop() {\n   \
          \ batch := make([]*Span, 0, 100)\n    ticker := time.NewTicker(5 * time.Second)\n\n    for {\n        select {\n\
          \        case span := <-t.spans:\n            batch = append(batch, span)\n            if len(batch) >= 100 {\n\
          \                t.exporter.ExportSpans(batch)\n                batch = batch[:0]\n            }\n\n        case\
          \ <-ticker.C:\n            if len(batch) > 0 {\n                t.exporter.ExportSpans(batch)\n                batch\
          \ = batch[:0]\n            }\n        }\n    }\n}"
      pitfalls:
      - Span End() never called (memory leak)
      - Too many attributes causes large payloads
      - High cardinality attribute values
      - Blocking on export channel
      concepts:
      - Span recording
      - Attributes and events
      - Error recording
      - Batch export
      estimated_hours: 8-12
    - id: 3
      name: Collector & Storage
      description: Build collector to receive, process, and store traces.
      acceptance_criteria:
      - Receive spans via HTTP/gRPC
      - Process and enrich spans
      - Store in time-series database
      - Sampling strategies
      - Tail-based sampling
      hints:
        level1: Collector receives spans, buffers, writes to storage. Use Cassandra/ClickHouse for traces.
        level2: 'Tail sampling: wait for trace completion, sample based on latency/errors.'
        level3: "// Collector service\ntype Collector struct {\n    storage   TraceStorage\n    processor SpanProcessor\n\
          \    sampler   Sampler\n}\n\ntype SpanProcessor interface {\n    Process(span *Span) *Span\n}\n\ntype EnrichmentProcessor\
          \ struct {\n    geoIP   *geoip.DB\n}\n\nfunc (p *EnrichmentProcessor) Process(span *Span) *Span {\n    // Add derived\
          \ attributes\n    if ip, ok := span.Attributes[\"http.client_ip\"].(string); ok {\n        if location, err := p.geoIP.Lookup(ip);\
          \ err == nil {\n            span.SetAttribute(\"geo.country\", location.Country)\n            span.SetAttribute(\"\
          geo.city\", location.City)\n        }\n    }\n\n    // Calculate duration\n    span.SetAttribute(\"duration_ms\"\
          , span.EndTime.Sub(span.StartTime).Milliseconds())\n\n    return span\n}\n\n// Sampling\ntype Sampler interface\
          \ {\n    ShouldSample(span *Span) bool\n}\n\ntype ProbabilisticSampler struct {\n    rate float64\n}\n\nfunc (s\
          \ *ProbabilisticSampler) ShouldSample(span *Span) bool {\n    // Use trace ID for deterministic sampling\n    hash\
          \ := fnv.New64a()\n    hash.Write(span.TraceID[:])\n    return float64(hash.Sum64()%10000)/10000 < s.rate\n}\n\n\
          // Tail-based sampling - sample complete traces based on their properties\ntype TailSampler struct {\n    pending\
          \     map[TraceID]*pendingTrace\n    sampleRules []SampleRule\n    mu          sync.Mutex\n}\n\ntype pendingTrace\
          \ struct {\n    spans       []*Span\n    firstSeen   time.Time\n    rootSpan    *Span\n}\n\ntype SampleRule struct\
          \ {\n    Name      string\n    Condition func(trace *pendingTrace) bool\n    Rate      float64\n}\n\nfunc (s *TailSampler)\
          \ AddSpan(span *Span) {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n\n    pt, ok := s.pending[span.TraceID]\n   \
          \ if !ok {\n        pt = &pendingTrace{firstSeen: time.Now()}\n        s.pending[span.TraceID] = pt\n    }\n\n \
          \   pt.spans = append(pt.spans, span)\n\n    if span.ParentID == (SpanID{}) {\n        pt.rootSpan = span\n    }\n\
          }\n\nfunc (s *TailSampler) Evaluate() []*Span {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n\n    var sampled []*Span\n\
          \n    for traceID, pt := range s.pending {\n        // Wait for trace to complete (no new spans for 10s)\n     \
          \   if time.Since(pt.firstSeen) < 10*time.Second {\n            continue\n        }\n\n        // Evaluate sampling\
          \ rules\n        shouldSample := false\n        for _, rule := range s.sampleRules {\n            if rule.Condition(pt)\
          \ && rand.Float64() < rule.Rate {\n                shouldSample = true\n                break\n            }\n \
          \       }\n\n        if shouldSample {\n            sampled = append(sampled, pt.spans...)\n        }\n\n      \
          \  delete(s.pending, traceID)\n    }\n\n    return sampled\n}\n\n// Example rules\nfunc ErrorTraceRule(pt *pendingTrace)\
          \ bool {\n    for _, span := range pt.spans {\n        if span.Status.Code == StatusError {\n            return\
          \ true\n        }\n    }\n    return false\n}\n\nfunc SlowTraceRule(threshold time.Duration) func(*pendingTrace)\
          \ bool {\n    return func(pt *pendingTrace) bool {\n        if pt.rootSpan == nil {\n            return false\n\
          \        }\n        return pt.rootSpan.EndTime.Sub(pt.rootSpan.StartTime) > threshold\n    }\n}"
      pitfalls:
      - Memory exhaustion holding pending traces
      - Sampling bias toward short traces
      - Clock skew affecting duration calculations
      - Hot partition in storage by trace ID
      concepts:
      - Span collection
      - Enrichment
      - Head vs tail sampling
      - Trace storage
      estimated_hours: 12-18
    - id: 4
      name: Query & Visualization
      description: Query traces and visualize in timeline view.
      acceptance_criteria:
      - Search traces by service, operation, tags
      - Time range queries
      - Trace timeline visualization
      - Service dependency graph
      - Latency percentiles
      hints:
        level1: Index by service, operation, and time. Use ClickHouse for fast queries.
        level2: 'Dependency graph: aggregate parent -> child service pairs from spans.'
        level3: "// Query API\ntype TraceQuery struct {\n    Service     string\n    Operation   string\n    Tags        map[string]string\n\
          \    MinDuration time.Duration\n    MaxDuration time.Duration\n    StartTime   time.Time\n    EndTime     time.Time\n\
          \    Limit       int\n}\n\ntype TraceQueryService struct {\n    storage TraceStorage\n}\n\nfunc (s *TraceQueryService)\
          \ Search(q TraceQuery) ([]*Trace, error) {\n    // Build query\n    query := `\n        SELECT trace_id, span_id,\
          \ parent_id, name, start_time, end_time, attributes\n        FROM spans\n        WHERE start_time >= ? AND start_time\
          \ <= ?\n    `\n    args := []interface{}{q.StartTime, q.EndTime}\n\n    if q.Service != \"\" {\n        query +=\
          \ \" AND service_name = ?\"\n        args = append(args, q.Service)\n    }\n\n    if q.Operation != \"\" {\n   \
          \     query += \" AND name = ?\"\n        args = append(args, q.Operation)\n    }\n\n    if q.MinDuration > 0 {\n\
          \        query += \" AND duration_ms >= ?\"\n        args = append(args, q.MinDuration.Milliseconds())\n    }\n\n\
          \    for key, value := range q.Tags {\n        query += fmt.Sprintf(\" AND attributes['%s'] = ?\", key)\n      \
          \  args = append(args, value)\n    }\n\n    query += \" ORDER BY start_time DESC LIMIT ?\"\n    args = append(args,\
          \ q.Limit)\n\n    // Execute and group by trace\n    rows, _ := s.storage.Query(query, args...)\n    return s.groupSpansToTraces(rows)\n\
          }\n\n// Dependency graph\ntype ServiceDependency struct {\n    Parent      string\n    Child       string\n    CallCount\
          \   int64\n    ErrorCount  int64\n    AvgDuration float64\n}\n\nfunc (s *TraceQueryService) GetDependencies(startTime,\
          \ endTime time.Time) ([]ServiceDependency, error) {\n    query := `\n        SELECT\n            parent.service_name\
          \ as parent,\n            child.service_name as child,\n            count(*) as call_count,\n            countIf(child.status_code\
          \ = 'ERROR') as error_count,\n            avg(child.duration_ms) as avg_duration\n        FROM spans child\n   \
          \     JOIN spans parent ON child.parent_id = parent.span_id\n            AND child.trace_id = parent.trace_id\n\
          \        WHERE child.start_time >= ? AND child.start_time <= ?\n            AND parent.service_name != child.service_name\n\
          \        GROUP BY parent.service_name, child.service_name\n    `\n\n    // Return for D3.js force-directed graph\n\
          \    return s.storage.Query(query, startTime, endTime)\n}\n\n// Timeline data for visualization\ntype TimelineSpan\
          \ struct {\n    SpanID    string        `json:\"spanId\"`\n    ParentID  string        `json:\"parentId\"`\n   \
          \ Name      string        `json:\"name\"`\n    Service   string        `json:\"service\"`\n    StartTime int64 \
          \        `json:\"startTime\"` // microseconds from trace start\n    Duration  int64         `json:\"duration\"`\
          \  // microseconds\n    Status    string        `json:\"status\"`\n    Tags      []TagPair     `json:\"tags\"`\n\
          \    Logs      []LogEntry    `json:\"logs\"`\n    Depth     int           `json:\"depth\"`\n    Children  []*TimelineSpan\
          \ `json:\"children,omitempty\"`\n}\n\nfunc (s *TraceQueryService) GetTraceTimeline(traceID string) (*TimelineSpan,\
          \ error) {\n    spans, _ := s.storage.GetSpans(traceID)\n\n    // Build tree\n    spanMap := make(map[string]*TimelineSpan)\n\
          \    var root *TimelineSpan\n\n    for _, span := range spans {\n        ts := &TimelineSpan{\n            SpanID:\
          \   span.SpanID.String(),\n            ParentID: span.ParentID.String(),\n            Name:     span.Name,\n   \
          \         Service:  span.Attributes[\"service.name\"].(string),\n            Duration: span.EndTime.Sub(span.StartTime).Microseconds(),\n\
          \            Status:   span.Status.Code.String(),\n        }\n        spanMap[ts.SpanID] = ts\n\n        if span.ParentID\
          \ == (SpanID{}) {\n            root = ts\n        }\n    }\n\n    // Link parents\n    for _, ts := range spanMap\
          \ {\n        if ts.ParentID != \"\" {\n            if parent, ok := spanMap[ts.ParentID]; ok {\n               \
          \ parent.Children = append(parent.Children, ts)\n            }\n        }\n    }\n\n    // Calculate relative times\
          \ and depths\n    if root != nil {\n        s.calculateDepths(root, 0, root.StartTime)\n    }\n\n    return root,\
          \ nil\n}"
      pitfalls:
      - Query without time range scans entire database
      - Deep traces cause stack overflow in tree building
      - Timezone issues in timestamp display
      - Missing spans leave gaps in visualization
      concepts:
      - Trace querying
      - Service dependencies
      - Timeline visualization
      - Aggregations
      estimated_hours: 12-16
  documentation-project:
    id: documentation-project
    name: Documentation Project
    description: Learn to write effective documentation including README files, API docs, and code comments.
    difficulty: beginner
    estimated_hours: 8-15
    prerequisites:
    - Basic programming
    - Markdown
    languages:
      recommended:
      - Markdown
      - Any programming language
      also_possible: []
    resources:
    - type: guide
      name: Make a README
      url: https://www.makeareadme.com/
    - type: article
      name: How to Write Good Documentation
      url: https://documentation.divio.com/
    milestones:
    - id: 1
      name: README Essentials
      description: Create a comprehensive README for a project.
      acceptance_criteria:
      - Project title and description
      - Installation instructions
      - Usage examples
      - License and contribution guidelines
      hints:
        level1: Start with what the project does, then how to use it. Add badges for build status.
        level2: Include screenshots for visual projects. Add table of contents for long READMEs.
        level3: '# Project Name


          ![Build Status](https://img.shields.io/badge/build-passing-green)

          ![License](https://img.shields.io/badge/license-MIT-blue)


          One-paragraph description of what this project does and why it exists.


          ## Features


          - Feature 1: Brief description

          - Feature 2: Brief description

          - Feature 3: Brief description


          ## Installation


          ```bash

          # Clone the repository

          git clone https://github.com/user/project.git

          cd project


          # Install dependencies

          npm install


          # Set up environment

          cp .env.example .env

          ```


          ## Quick Start


          ```javascript

          const project = require(''project'');


          // Basic usage example

          const result = project.doSomething(''input'');

          console.log(result);

          ```


          ## Configuration


          | Option | Type | Default | Description |

          |--------|------|---------|-------------|

          | `timeout` | number | 5000 | Request timeout in ms |

          | `retries` | number | 3 | Number of retry attempts |


          ## Contributing


          See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.


          ## License


          MIT - see [LICENSE](LICENSE)'
      pitfalls:
      - Outdated installation steps
      - Missing prerequisites
      - Assumed knowledge
      concepts:
      - Technical writing
      - User empathy
      - Information architecture
      estimated_hours: 2-3
    - id: 2
      name: API Documentation
      description: Document an API with endpoints, parameters, and examples.
      acceptance_criteria:
      - Endpoint descriptions
      - Request/response formats
      - Authentication details
      - Error codes
      hints:
        level1: Use consistent format for each endpoint. Include curl examples.
        level2: Document all possible error responses. Show request/response bodies.
        level3: "# API Documentation\n\n## Authentication\n\nAll API requests require a Bearer token:\n\n```\nAuthorization:\
          \ Bearer <your-token>\n```\n\n## Endpoints\n\n### Get User\n\n```\nGET /api/users/:id\n```\n\nRetrieve a user by\
          \ their ID.\n\n**Parameters**\n\n| Name | Type | In | Description |\n|------|------|-----|-------------|\n| id |\
          \ string | path | User ID (required) |\n| include | string | query | Comma-separated relations to include |\n\n\
          **Response**\n\n```json\n{\n  \"id\": \"123\",\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\",\n\
          \  \"createdAt\": \"2024-01-15T10:30:00Z\"\n}\n```\n\n**Errors**\n\n| Status | Code | Description |\n|--------|------|-------------|\n\
          | 404 | USER_NOT_FOUND | User does not exist |\n| 401 | UNAUTHORIZED | Missing or invalid token |\n\n**Example**\n\
          \n```bash\ncurl -X GET 'https://api.example.com/api/users/123' \\\n  -H 'Authorization: Bearer token123'\n```"
      pitfalls:
      - Inconsistent formatting
      - Missing error cases
      - Stale examples
      concepts:
      - API design
      - REST conventions
      - Developer experience
      estimated_hours: 2-4
    - id: 3
      name: Code Comments
      description: Write effective code comments and docstrings.
      acceptance_criteria:
      - Function/method documentation
      - Complex logic explanation
      - TODO/FIXME usage
      - Avoiding obvious comments
      hints:
        level1: Comment WHY, not WHAT. The code shows what, comments explain reasoning.
        level2: Use docstrings for public APIs. Keep comments updated with code.
        level3: "// BAD: Obvious comment\n// Increment counter by 1\ncounter++;\n\n// GOOD: Explains why\n// Use post-increment\
          \ to return old value before updating\nreturn counter++;\n\n// BAD: Just restates the code\n// Check if user is\
          \ admin\nif (user.role === 'admin') { ... }\n\n// GOOD: Explains business logic\n// Admins can bypass rate limiting\
          \ to handle support escalations\nif (user.role === 'admin') { ... }\n\n/**\n * Calculates the optimal batch size\
          \ for database operations.\n * \n * Uses an adaptive algorithm that considers:\n * - Current memory pressure\n *\
          \ - Network latency to database\n * - Historical query performance\n * \n * @param {Object} metrics - Current system\
          \ metrics\n * @param {number} metrics.memoryUsage - Memory usage percentage (0-100)\n * @param {number} metrics.avgLatency\
          \ - Average DB latency in ms\n * @returns {number} Optimal batch size between 100 and 10000\n * \n * @example\n\
          \ * const size = calculateBatchSize({ memoryUsage: 70, avgLatency: 50 });\n * // Returns ~2500 for moderate load\n\
          \ */\nfunction calculateBatchSize(metrics) {\n    // Start with base size, adjust based on conditions\n    let size\
          \ = 1000;\n    \n    // Reduce batch size under memory pressure to prevent OOM\n    // Threshold of 80% is based\
          \ on production incidents in Q3 2023\n    if (metrics.memoryUsage > 80) {\n        size = Math.floor(size * 0.5);\n\
          \    }\n    \n    // TODO(perf): Consider adding connection pool size to calculation\n    // FIXME: This doesn't\
          \ account for concurrent batch operations\n    \n    return Math.max(100, Math.min(size, 10000));\n}"
      pitfalls:
      - Comment rot
      - Over-commenting
      - Misleading comments
      concepts:
      - Self-documenting code
      - Documentation debt
      - Maintainability
      estimated_hours: 2-4
    - id: 4
      name: Architecture Documentation
      description: Document system architecture and design decisions.
      acceptance_criteria:
      - System overview diagram
      - Component descriptions
      - Data flow documentation
      - ADRs (Architecture Decision Records)
      hints:
        level1: Start with high-level diagram. Then detail each component.
        level2: Use ADRs to record why decisions were made. Include alternatives considered.
        level3: "# Architecture Overview\n\n## System Diagram\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
          â”‚   Client    â”‚â”€â”€â”€â”€â–¶â”‚   API GW    â”‚â”€â”€â”€â”€â–¶â”‚  Services   â”‚\nâ”‚  (React)    â”‚     â”‚  (Kong)     â”‚     â”‚  (Node.js)  â”‚\n\
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                                                â”‚\n   \
          \                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n                    â”‚    Redis    â”‚â—€â”€â”€â”€â”€â”‚  PostgreSQL â”‚\n\
          \                    â”‚   (Cache)   â”‚     â”‚    (DB)     â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
          ```\n\n## Components\n\n### API Gateway\n- **Technology**: Kong\n- **Responsibility**: Rate limiting, authentication,\
          \ routing\n- **Scaling**: Horizontal, 2-4 instances\n\n### Service Layer\n- **Technology**: Node.js with Express\n\
          - **Responsibility**: Business logic, data validation\n- **Communication**: REST between services\n\n---\n\n# ADR-001:\
          \ Use PostgreSQL for Primary Database\n\n## Status\nAccepted\n\n## Context\nWe need a primary database for user\
          \ data, transactions, and product catalog.\n\n## Decision\nWe will use PostgreSQL.\n\n## Alternatives Considered\n\
          1. **MySQL**: Familiar, but JSON support is weaker\n2. **MongoDB**: Flexible schema, but we need ACID transactions\n\
          \n## Consequences\n- Good: Strong consistency, excellent JSON support with JSONB\n- Good: Mature ecosystem, easy\
          \ to find developers\n- Bad: Schema migrations require more planning\n- Bad: Horizontal scaling requires additional\
          \ tools (Citus)"
      pitfalls:
      - Diagrams becoming outdated
      - Too much detail
      - Missing rationale
      concepts:
      - System design
      - Technical communication
      - Decision documentation
      estimated_hours: 2-4
  ecommerce-basic:
    id: ecommerce-basic
    name: E-commerce Store (Basic)
    description: Build a basic online store with product catalog, cart, and checkout. Learn full-stack development patterns.
    difficulty: beginner
    estimated_hours: 20-35
    prerequisites:
    - HTML/CSS/JS
    - REST API basics
    - Database basics
    languages:
      recommended:
      - JavaScript/Node.js
      - Python/Flask
      - Ruby/Rails
      also_possible:
      - PHP
      - Go
      - Java/Spring
    resources:
    - type: tutorial
      name: Full Stack E-commerce
      url: https://www.freecodecamp.org/news/how-to-build-an-e-commerce-app/
    - type: video
      name: Build an Online Store
      url: https://www.youtube.com/results?search_query=build+ecommerce+site+tutorial
    milestones:
    - id: 1
      name: Product Catalog
      description: Display products with images, prices, and descriptions.
      acceptance_criteria:
      - Product listing page
      - Product detail page
      - Category filtering
      - Search functionality
      hints:
        level1: Start with static JSON data. Add database later.
        level2: Use grid layout for products. Store images as URLs initially.
        level3: "// Product schema\nconst productSchema = {\n    id: 'string',\n    name: 'string',\n    description: 'string',\n\
          \    price: 'number',  // in cents\n    imageUrl: 'string',\n    category: 'string',\n    stock: 'number'\n};\n\n\
          // API endpoints\napp.get('/api/products', (req, res) => {\n    let products = [...allProducts];\n    \n    if (req.query.category)\
          \ {\n        products = products.filter(p => \n            p.category === req.query.category\n        );\n    }\n\
          \    \n    if (req.query.search) {\n        const term = req.query.search.toLowerCase();\n        products = products.filter(p\
          \ =>\n            p.name.toLowerCase().includes(term) ||\n            p.description.toLowerCase().includes(term)\n\
          \        );\n    }\n    \n    res.json(products);\n});\n\napp.get('/api/products/:id', (req, res) => {\n    const\
          \ product = allProducts.find(p => p.id === req.params.id);\n    if (!product) return res.status(404).json({error:\
          \ 'Not found'});\n    res.json(product);\n});"
      pitfalls:
      - Price precision (use cents)
      - Large image handling
      - N+1 queries
      concepts:
      - CRUD operations
      - Filtering/search
      - Data modeling
      estimated_hours: 4-6
    - id: 2
      name: Shopping Cart
      description: Implement add to cart, update quantities, and remove items.
      acceptance_criteria:
      - Add/remove items
      - Update quantities
      - Cart persistence
      - Price calculations
      hints:
        level1: Store cart in session/localStorage for guests, database for logged-in users.
        level2: Cart items reference product IDs. Calculate totals server-side.
        level3: "// Cart in localStorage (guest)\nconst Cart = {\n    items: [],\n    \n    load() {\n        const saved\
          \ = localStorage.getItem('cart');\n        this.items = saved ? JSON.parse(saved) : [];\n    },\n    \n    save()\
          \ {\n        localStorage.setItem('cart', JSON.stringify(this.items));\n    },\n    \n    add(productId, quantity\
          \ = 1) {\n        const existing = this.items.find(i => i.productId === productId);\n        if (existing) {\n \
          \           existing.quantity += quantity;\n        } else {\n            this.items.push({ productId, quantity\
          \ });\n        }\n        this.save();\n    },\n    \n    update(productId, quantity) {\n        const item = this.items.find(i\
          \ => i.productId === productId);\n        if (item) {\n            if (quantity <= 0) {\n                this.remove(productId);\n\
          \            } else {\n                item.quantity = quantity;\n            }\n        }\n        this.save();\n\
          \    },\n    \n    remove(productId) {\n        this.items = this.items.filter(i => i.productId !== productId);\n\
          \        this.save();\n    },\n    \n    async getTotal() {\n        // Fetch current prices from server\n     \
          \   const response = await fetch('/api/cart/calculate', {\n            method: 'POST',\n            body: JSON.stringify({\
          \ items: this.items })\n        });\n        return response.json();\n    }\n};"
      pitfalls:
      - Price changes between add and checkout
      - Stock validation
      - Session expiry
      concepts:
      - State management
      - Session storage
      - Data synchronization
      estimated_hours: 4-6
    - id: 3
      name: User Authentication
      description: Implement user registration, login, and profile management.
      acceptance_criteria:
      - Registration with validation
      - Login/logout
      - Password hashing
      - Session management
      hints:
        level1: Use bcrypt for passwords. Never store plaintext.
        level2: JWT for API auth, session cookies for web. Add email validation.
        level3: "// Password hashing\nconst bcrypt = require('bcrypt');\n\nasync function register(email, password) {\n  \
          \  // Validate\n    if (!email || !password) throw new Error('Required');\n    if (password.length < 8) throw new\
          \ Error('Password too short');\n    \n    // Check existing\n    const existing = await db.users.findOne({ email\
          \ });\n    if (existing) throw new Error('Email exists');\n    \n    // Hash password\n    const hash = await bcrypt.hash(password,\
          \ 12);\n    \n    // Create user\n    const user = await db.users.create({\n        email,\n        passwordHash:\
          \ hash,\n        createdAt: new Date()\n    });\n    \n    return { id: user.id, email: user.email };\n}\n\nasync\
          \ function login(email, password) {\n    const user = await db.users.findOne({ email });\n    if (!user) throw new\
          \ Error('Invalid credentials');\n    \n    const valid = await bcrypt.compare(password, user.passwordHash);\n  \
          \  if (!valid) throw new Error('Invalid credentials');\n    \n    // Create session/token\n    const token = jwt.sign(\n\
          \        { userId: user.id },\n        process.env.JWT_SECRET,\n        { expiresIn: '7d' }\n    );\n    \n    return\
          \ { token, user: { id: user.id, email: user.email } };\n}"
      pitfalls:
      - Timing attacks on login
      - Password requirements
      - Token invalidation
      concepts:
      - Authentication
      - Password security
      - Session management
      estimated_hours: 5-8
    - id: 4
      name: Checkout Process
      description: Implement checkout flow with order creation.
      acceptance_criteria:
      - Address collection
      - Order summary
      - Order creation
      - Inventory updates
      hints:
        level1: Validate stock before creating order. Use transactions for consistency.
        level2: Create order in 'pending' state. Update to 'confirmed' after payment.
        level3: "async function createOrder(userId, cartItems, shippingAddress) {\n    // Start transaction\n    const session\
          \ = await db.startTransaction();\n    \n    try {\n        // Fetch and validate products\n        const products\
          \ = await db.products.find({\n            id: { $in: cartItems.map(i => i.productId) }\n        });\n        \n\
          \        // Check stock and calculate total\n        let total = 0;\n        const orderItems = [];\n        \n\
          \        for (const cartItem of cartItems) {\n            const product = products.find(p => p.id === cartItem.productId);\n\
          \            if (!product) throw new Error(`Product ${cartItem.productId} not found`);\n            if (product.stock\
          \ < cartItem.quantity) {\n                throw new Error(`Insufficient stock for ${product.name}`);\n         \
          \   }\n            \n            orderItems.push({\n                productId: product.id,\n                name:\
          \ product.name,\n                price: product.price,\n                quantity: cartItem.quantity\n          \
          \  });\n            \n            total += product.price * cartItem.quantity;\n            \n            // Decrement\
          \ stock\n            await db.products.updateOne(\n                { id: product.id },\n                { $inc:\
          \ { stock: -cartItem.quantity } }\n            );\n        }\n        \n        // Create order\n        const order\
          \ = await db.orders.create({\n            userId,\n            items: orderItems,\n            total,\n        \
          \    shippingAddress,\n            status: 'pending',\n            createdAt: new Date()\n        });\n        \n\
          \        await session.commit();\n        return order;\n    } catch (error) {\n        await session.rollback();\n\
          \        throw error;\n    }\n}"
      pitfalls:
      - Race conditions on stock
      - Abandoned checkouts
      - Partial order failures
      concepts:
      - Transactions
      - Inventory management
      - Order lifecycle
      estimated_hours: 7-15
  ecs-arch:
    id: ecs-arch
    name: ECS Architecture
    description: Implement an Entity-Component-System architecture. Learn data-oriented design for games.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - Game programming basics
    - Data structures
    - Performance concepts
    languages:
      recommended:
      - C++
      - Rust
      - C
      also_possible:
      - C#
      - Go
    resources:
    - name: ECS FAQ
      url: https://github.com/SanderMertens/ecs-faq
      type: article
    - name: Overwatch GDC Talk
      url: https://www.youtube.com/watch?v=W3aieHjyNvw
      type: video
    milestones:
    - id: 1
      name: Entity Manager
      description: Create entity ID management system.
      acceptance_criteria:
      - Generate unique entity IDs
      - Track alive/dead entities
      - Recycle deleted IDs
      - Generation counter for ID reuse safety
      hints:
        level1: Entity = just an ID (integer). Components stored separately.
        level2: Use generation to detect stale references.
        level3: "struct EntityId {\n    uint32_t index;\n    uint32_t generation;\n};\n\nclass EntityManager {\n    std::vector<uint32_t>\
          \ generations;\n    std::queue<uint32_t> free_indices;\n    \npublic:\n    EntityId create() {\n        uint32_t\
          \ index;\n        if (!free_indices.empty()) {\n            index = free_indices.front();\n            free_indices.pop();\n\
          \        } else {\n            index = generations.size();\n            generations.push_back(0);\n        }\n \
          \       return {index, generations[index]};\n    }\n    \n    void destroy(EntityId id) {\n        if (is_alive(id))\
          \ {\n            generations[id.index]++;\n            free_indices.push(id.index);\n        }\n    }\n    \n  \
          \  bool is_alive(EntityId id) {\n        return id.index < generations.size() && \n               generations[id.index]\
          \ == id.generation;\n    }\n};"
      pitfalls:
      - Stale entity references
      - Index overflow
      - Not recycling IDs
      concepts:
      - Entity IDs
      - Generation counters
      - ID recycling
      estimated_hours: 3-4
    - id: 2
      name: Component Storage
      description: Implement cache-friendly component storage.
      acceptance_criteria:
      - Contiguous array storage
      - Sparse set for entity->component mapping
      - Add/remove components dynamically
      - Type-safe component access
      hints:
        level1: 'Sparse set: sparse[entity] = dense_index, dense[index] = component.'
        level2: Dense array is contiguous for cache efficiency.
        level3: "template<typename T>\nclass ComponentArray {\n    std::vector<T> dense;           // Contiguous component\
          \ data\n    std::vector<uint32_t> sparse;   // Entity ID -> dense index\n    std::vector<uint32_t> dense_to_entity;\
          \  // dense index -> entity ID\n    \npublic:\n    void add(uint32_t entity, T component) {\n        if (entity\
          \ >= sparse.size())\n            sparse.resize(entity + 1, INVALID);\n        sparse[entity] = dense.size();\n \
          \       dense_to_entity.push_back(entity);\n        dense.push_back(component);\n    }\n    \n    void remove(uint32_t\
          \ entity) {\n        uint32_t index = sparse[entity];\n        uint32_t last = dense.size() - 1;\n        \n   \
          \     // Swap with last\n        std::swap(dense[index], dense[last]);\n        std::swap(dense_to_entity[index],\
          \ dense_to_entity[last]);\n        \n        // Update sparse\n        sparse[dense_to_entity[index]] = index;\n\
          \        sparse[entity] = INVALID;\n        \n        dense.pop_back();\n        dense_to_entity.pop_back();\n \
          \   }\n    \n    T* get(uint32_t entity) {\n        if (entity >= sparse.size() || sparse[entity] == INVALID)\n\
          \            return nullptr;\n        return &dense[sparse[entity]];\n    }\n};"
      pitfalls:
      - Swap-remove ordering
      - Sparse array growth
      - Invalid index sentinel
      concepts:
      - Sparse sets
      - Data-oriented design
      - Cache efficiency
      estimated_hours: 5-7
    - id: 3
      name: System Interface
      description: Create system execution framework.
      acceptance_criteria:
      - Systems operate on component sets
      - Query entities with specific components
      - System registration and ordering
      - Delta time passing
      hints:
        level1: System = function that iterates entities with certain components.
        level2: 'Query: find all entities with Position AND Velocity.'
        level3: "class World {\n    std::unordered_map<std::type_index, std::unique_ptr<ComponentArrayBase>> components;\n\
          \    \n    template<typename... Components>\n    class View {\n        World& world;\n    public:\n        class\
          \ Iterator {\n            // Iterate entities that have all Components\n        };\n        \n        void each(std::function<void(EntityId,\
          \ Components&...)> func) {\n            // For each entity with all components, call func\n            for (auto\
          \ entity : get_matching_entities()) {\n                func(entity, *world.get<Components>(entity)...);\n      \
          \      }\n        }\n    };\n    \n    template<typename... Components>\n    View<Components...> view() {\n    \
          \    return View<Components...>(*this);\n    }\n};\n\n// Usage:\nvoid movement_system(World& world, float dt) {\n\
          \    world.view<Position, Velocity>().each([dt](EntityId e, Position& pos, Velocity& vel) {\n        pos.x += vel.x\
          \ * dt;\n        pos.y += vel.y * dt;\n    });\n}"
      pitfalls:
      - Component iteration invalidation
      - System ordering dependencies
      - Thread safety
      concepts:
      - System execution
      - Component queries
      - Iteration patterns
      estimated_hours: 5-7
    - id: 4
      name: Archetypes (Optional Advanced)
      description: Implement archetype-based storage for better performance.
      acceptance_criteria:
      - Group entities by component combination
      - Move entities between archetypes
      - Archetype graph for fast lookup
      - Chunk-based storage
      hints:
        level1: Archetype = unique combination of component types.
        level2: Entities with same components stored together.
        level3: "struct Archetype {\n    std::set<std::type_index> component_types;\n    std::vector<void*> component_arrays;\
          \  // One array per type\n    std::vector<EntityId> entities;\n    \n    // All entities in same archetype have\
          \ identical component layout\n};\n\nclass ArchetypeWorld {\n    std::unordered_map<ComponentMask, Archetype> archetypes;\n\
          \    std::unordered_map<EntityId, ArchetypeLocation> entity_locations;\n    \n    void add_component(EntityId entity,\
          \ Component c) {\n        // 1. Find current archetype\n        // 2. Find or create target archetype (current +\
          \ new component)\n        // 3. Move entity data to new archetype\n    }\n};"
      pitfalls:
      - Archetype explosion
      - Move overhead
      - Complex implementation
      concepts:
      - Archetypes
      - Data locality
      - Component masks
      estimated_hours: 6-10
  file-copy:
    id: file-copy
    name: File Copy (cp clone)
    description: Build a file copy utility. Learn file I/O, permissions, and error handling.
    difficulty: beginner
    estimated_hours: 4-8
    prerequisites:
    - Basic file I/O
    - Command-line arguments
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
    resources:
    - name: POSIX File Operations
      url: https://pubs.opengroup.org/onlinepubs/9699919799/functions/read.html
      type: documentation
    milestones:
    - id: 1
      name: Basic File Copy
      description: Copy a single file to a destination.
      acceptance_criteria:
      - Open source file for reading
      - Create destination file
      - Copy contents in chunks
      - Close files properly
      hints:
        level1: Open source (read-only), open/create dest (write-only), loop read/write.
        level2: Use buffer size of 4KB-64KB for efficiency. Check return values.
        level3: "#include <stdio.h>\n#include <stdlib.h>\n#include <fcntl.h>\n#include <unistd.h>\n\n#define BUF_SIZE 4096\n\
          \nint copy_file(const char *src, const char *dst) {\n    int fd_src = open(src, O_RDONLY);\n    if (fd_src < 0)\
          \ {\n        perror(\"open source\");\n        return -1;\n    }\n    \n    int fd_dst = open(dst, O_WRONLY | O_CREAT\
          \ | O_TRUNC, 0644);\n    if (fd_dst < 0) {\n        perror(\"open dest\");\n        close(fd_src);\n        return\
          \ -1;\n    }\n    \n    char buf[BUF_SIZE];\n    ssize_t bytes_read;\n    \n    while ((bytes_read = read(fd_src,\
          \ buf, BUF_SIZE)) > 0) {\n        ssize_t bytes_written = write(fd_dst, buf, bytes_read);\n        if (bytes_written\
          \ != bytes_read) {\n            perror(\"write\");\n            close(fd_src);\n            close(fd_dst);\n   \
          \         return -1;\n        }\n    }\n    \n    close(fd_src);\n    close(fd_dst);\n    return 0;\n}"
      pitfalls:
      - Not handling partial writes
      - Forgetting to close files
      - Wrong file permissions
      concepts:
      - File descriptors
      - Buffered I/O
      - Error handling
      estimated_hours: 1-2
    - id: 2
      name: Preserve Permissions
      description: Copy file permissions and metadata.
      acceptance_criteria:
      - Preserve file mode (rwx)
      - Copy timestamps
      - Handle special files gracefully
      hints:
        level1: Use stat() to get source file info, then set on destination.
        level2: chmod() for permissions, utimes() for timestamps.
        level3: "#include <sys/stat.h>\n#include <sys/time.h>\n\nint preserve_metadata(const char *src, const char *dst) {\n\
          \    struct stat st;\n    if (stat(src, &st) < 0) {\n        perror(\"stat\");\n        return -1;\n    }\n    \n\
          \    // Preserve permissions\n    if (chmod(dst, st.st_mode) < 0) {\n        perror(\"chmod\");\n        return\
          \ -1;\n    }\n    \n    // Preserve timestamps\n    struct timeval times[2];\n    times[0].tv_sec = st.st_atime;\
          \  // Access time\n    times[0].tv_usec = 0;\n    times[1].tv_sec = st.st_mtime;  // Modification time\n    times[1].tv_usec\
          \ = 0;\n    \n    if (utimes(dst, times) < 0) {\n        perror(\"utimes\");\n        return -1;\n    }\n    \n\
          \    return 0;\n}"
      pitfalls:
      - Forgetting executable bit
      - Not checking stat return
      - Platform differences
      concepts:
      - File metadata
      - Permissions
      - Timestamps
      estimated_hours: 1-2
    - id: 3
      name: Directory Copy
      description: Recursively copy directories.
      acceptance_criteria:
      - Detect if source is directory
      - Create destination directory
      - Recursively copy contents
      - Handle symbolic links option
      hints:
        level1: Check S_ISDIR(st.st_mode). Use opendir/readdir to iterate.
        level2: Skip . and .. entries. Handle -r flag for recursive.
        level3: "#include <dirent.h>\n#include <string.h>\n\nint copy_directory(const char *src, const char *dst) {\n    struct\
          \ stat st;\n    stat(src, &st);\n    \n    // Create destination directory\n    if (mkdir(dst, st.st_mode) < 0 &&\
          \ errno != EEXIST) {\n        perror(\"mkdir\");\n        return -1;\n    }\n    \n    DIR *dir = opendir(src);\n\
          \    if (!dir) {\n        perror(\"opendir\");\n        return -1;\n    }\n    \n    struct dirent *entry;\n   \
          \ while ((entry = readdir(dir)) != NULL) {\n        // Skip . and ..\n        if (strcmp(entry->d_name, \".\") ==\
          \ 0 || strcmp(entry->d_name, \"..\") == 0)\n            continue;\n        \n        // Build full paths\n     \
          \   char src_path[PATH_MAX], dst_path[PATH_MAX];\n        snprintf(src_path, sizeof(src_path), \"%s/%s\", src, entry->d_name);\n\
          \        snprintf(dst_path, sizeof(dst_path), \"%s/%s\", dst, entry->d_name);\n        \n        struct stat entry_st;\n\
          \        if (lstat(src_path, &entry_st) < 0) continue;\n        \n        if (S_ISDIR(entry_st.st_mode)) {\n   \
          \         copy_directory(src_path, dst_path);  // Recurse\n        } else {\n            copy_file(src_path, dst_path);\n\
          \        }\n    }\n    \n    closedir(dir);\n    return 0;\n}"
      pitfalls:
      - Infinite recursion with symlinks
      - Path buffer overflow
      - Permission denied errors
      concepts:
      - Directory traversal
      - Recursion
      - Path handling
      estimated_hours: 2-3
  gan:
    id: gan
    name: Generative Adversarial Network
    description: Implement a GAN from scratch. Learn adversarial training, generator/discriminator architecture, and image
      generation.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - Neural networks
    - CNNs
    - PyTorch/TensorFlow basics
    - Linear algebra
    languages:
      recommended:
      - Python
      also_possible:
      - Julia
    resources:
    - name: Original GAN Paper
      url: https://arxiv.org/abs/1406.2661
      type: paper
    - name: GAN Hacks
      url: https://github.com/soumith/ganhacks
      type: article
    - name: PyTorch DCGAN Tutorial
      url: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html
      type: tutorial
    milestones:
    - id: 1
      name: Discriminator Network
      description: Build the discriminator that classifies real vs fake images.
      acceptance_criteria:
      - CNN architecture for image classification
      - Binary output (real/fake)
      - Leaky ReLU activations
      - Batch normalization
      hints:
        level1: 'Discriminator: Image -> probability it''s real.'
        level2: Use strided convolutions instead of pooling. LeakyReLU prevents dead neurons.
        level3: "import torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels=1, features_d=64):\n\
          \        super().__init__()\n        self.disc = nn.Sequential(\n            # Input: N x img_channels x 64 x 64\n\
          \            nn.Conv2d(img_channels, features_d, 4, 2, 1),  # 32x32\n            nn.LeakyReLU(0.2),\n          \
          \  self._block(features_d, features_d * 2, 4, 2, 1),  # 16x16\n            self._block(features_d * 2, features_d\
          \ * 4, 4, 2, 1),  # 8x8\n            self._block(features_d * 4, features_d * 8, 4, 2, 1),  # 4x4\n            nn.Conv2d(features_d\
          \ * 8, 1, 4, 2, 0),  # 1x1\n            nn.Sigmoid()\n        )\n    \n    def _block(self, in_channels, out_channels,\
          \ kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels,\
          \ kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2)\n\
          \        )\n    \n    def forward(self, x):\n        return self.disc(x).view(-1)"
      pitfalls:
      - Using ReLU instead of LeakyReLU
      - Missing batch norm
      - Wrong output size
      concepts:
      - CNNs for classification
      - Strided convolutions
      - LeakyReLU
      estimated_hours: 4-6
    - id: 2
      name: Generator Network
      description: Build the generator that creates images from noise.
      acceptance_criteria:
      - Transposed convolutions for upsampling
      - Takes random noise as input
      - Output matches image dimensions
      - Tanh activation for output
      hints:
        level1: 'Generator: Random noise -> fake image.'
        level2: Use transposed convolutions (deconvolutions) to upsample.
        level3: "class Generator(nn.Module):\n    def __init__(self, z_dim=100, img_channels=1, features_g=64):\n        super().__init__()\n\
          \        self.gen = nn.Sequential(\n            # Input: N x z_dim x 1 x 1\n            self._block(z_dim, features_g\
          \ * 16, 4, 1, 0),  # 4x4\n            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # 8x8\n          \
          \  self._block(features_g * 8, features_g * 4, 4, 2, 1),  # 16x16\n            self._block(features_g * 4, features_g\
          \ * 2, 4, 2, 1),  # 32x32\n            nn.ConvTranspose2d(features_g * 2, img_channels, 4, 2, 1),  # 64x64\n   \
          \         nn.Tanh()  # Output in [-1, 1]\n        )\n    \n    def _block(self, in_channels, out_channels, kernel_size,\
          \ stride, padding):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\
          \ stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\
          \    \n    def forward(self, x):\n        return self.gen(x)"
      pitfalls:
      - Checkerboard artifacts
      - Wrong noise dimension
      - Not using Tanh
      concepts:
      - Transposed convolutions
      - Upsampling
      - Latent space
      estimated_hours: 4-6
    - id: 3
      name: Adversarial Training Loop
      description: Implement the minimax training procedure.
      acceptance_criteria:
      - Alternate D and G training
      - Use appropriate loss functions
      - Separate optimizers for D and G
      - Track training progress
      hints:
        level1: D tries to maximize log(D(real)) + log(1-D(G(z))). G tries to minimize log(1-D(G(z))).
        level2: In practice, train G to maximize log(D(G(z))) for better gradients.
        level3: "def train_step(real_images, D, G, opt_d, opt_g, criterion, z_dim, device):\n    batch_size = real_images.size(0)\n\
          \    real_images = real_images.to(device)\n    \n    # Train Discriminator: max log(D(real)) + log(1 - D(G(z)))\n\
          \    noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n    fake = G(noise)\n    \n    disc_real = D(real_images)\n\
          \    disc_fake = D(fake.detach())\n    \n    loss_d_real = criterion(disc_real, torch.ones_like(disc_real))\n  \
          \  loss_d_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n    loss_d = (loss_d_real + loss_d_fake) / 2\n\
          \    \n    opt_d.zero_grad()\n    loss_d.backward()\n    opt_d.step()\n    \n    # Train Generator: max log(D(G(z)))\
          \ <-> min -log(D(G(z)))\n    output = D(fake)\n    loss_g = criterion(output, torch.ones_like(output))\n    \n \
          \   opt_g.zero_grad()\n    loss_g.backward()\n    opt_g.step()\n    \n    return loss_d.item(), loss_g.item()"
      pitfalls:
      - Training D too much vs G
      - Mode collapse
      - Forgetting detach()
      concepts:
      - Adversarial training
      - Minimax game
      - Loss functions
      estimated_hours: 5-8
    - id: 4
      name: Training Stability & Results
      description: Stabilize training and generate quality images.
      acceptance_criteria:
      - Implement label smoothing
      - Add noise to discriminator inputs
      - Track FID or visual quality
      - Save generated samples during training
      hints:
        level1: Training instability is common. Use tricks from GAN Hacks.
        level2: 'Label smoothing: use 0.9 instead of 1.0 for real labels.'
        level3: "# Training improvements\n\n# 1. Label smoothing\nreal_label = 0.9  # instead of 1.0\nfake_label = 0.0\n\n\
          # 2. Add noise to discriminator inputs\ndef add_noise(images, std=0.1):\n    return images + torch.randn_like(images)\
          \ * std\n\n# 3. Learning rates\nlr_d = 0.0002\nlr_g = 0.0002\nbetas = (0.5, 0.999)  # Adam betas\n\n# 4. Track progress\n\
          def save_samples(G, fixed_noise, epoch, path):\n    G.eval()\n    with torch.no_grad():\n        fake = G(fixed_noise)\n\
          \        # Denormalize from [-1,1] to [0,1]\n        fake = (fake + 1) / 2\n        save_image(fake, f'{path}/epoch_{epoch}.png',\
          \ nrow=8)\n    G.train()\n\n# 5. Fixed noise for consistent comparison\nfixed_noise = torch.randn(64, z_dim, 1,\
          \ 1, device=device)"
      pitfalls:
      - Mode collapse (G produces same image)
      - D becomes too strong
      - Vanishing gradients
      concepts:
      - Training stability
      - Label smoothing
      - Progress tracking
      estimated_hours: 6-10
  git-workflow:
    id: git-workflow
    name: Git Workflow Mastery
    description: Master advanced Git workflows. Learn branching strategies, rebasing, and collaborative patterns.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - Basic Git commands
    - Version control concepts
    languages:
      recommended:
      - Bash
      also_possible: []
    resources:
    - name: Pro Git Book
      url: https://git-scm.com/book/en/v2
      type: book
    - name: Git Flight Rules
      url: https://github.com/k88hudson/git-flight-rules
      type: article
    milestones:
    - id: 1
      name: Branching Strategies
      description: Implement and practice branching strategies.
      acceptance_criteria:
      - Feature branch workflow
      - Git Flow (develop, feature, release, hotfix)
      - Trunk-based development
      - Understand trade-offs of each
      hints:
        level1: 'Feature branches: branch per feature, merge to main.'
        level2: 'Git Flow: main (releases), develop (integration), feature/* (work).'
        level3: '# Feature Branch Workflow

          git checkout -b feature/user-auth main

          # ... work ...

          git push -u origin feature/user-auth

          # Create PR, review, merge


          # Git Flow

          git checkout -b develop main

          git checkout -b feature/login develop

          # ... work ...

          git checkout develop && git merge --no-ff feature/login


          # Release

          git checkout -b release/1.0 develop

          # ... fix bugs ...

          git checkout main && git merge --no-ff release/1.0

          git tag -a v1.0

          git checkout develop && git merge --no-ff release/1.0


          # Hotfix

          git checkout -b hotfix/critical-bug main

          # ... fix ...

          git checkout main && git merge --no-ff hotfix/critical-bug

          git checkout develop && git merge --no-ff hotfix/critical-bug'
      pitfalls:
      - Long-lived branches
      - Merge conflicts
      - Choosing wrong strategy
      concepts:
      - Branching strategies
      - Git Flow
      - Trunk-based development
      estimated_hours: 2-3
    - id: 2
      name: Rebasing and History
      description: Master rebasing and history rewriting.
      acceptance_criteria:
      - Interactive rebase for cleanup
      - Squash commits before merge
      - Rebase vs merge trade-offs
      - Handle rebase conflicts
      hints:
        level1: 'Rebase: replay commits on new base. Cleaner history.'
        level2: 'Interactive rebase: pick, squash, reword, edit, drop.'
        level3: '# Rebase feature branch onto updated main

          git checkout feature/my-feature

          git fetch origin

          git rebase origin/main


          # Interactive rebase to clean up last 5 commits

          git rebase -i HEAD~5


          # In editor:

          # pick abc123 Add user model

          # squash def456 Fix typo

          # squash ghi789 Add tests

          # reword jkl012 Implement auth <- change commit message

          # drop mno345 WIP commit


          # Handle conflicts during rebase

          # 1. Fix conflicts in files

          git add <resolved-files>

          git rebase --continue

          # Or abort

          git rebase --abort


          # Squash merge (alternative to rebasing)

          git checkout main

          git merge --squash feature/my-feature

          git commit -m ''Add user authentication (#123)'''
      pitfalls:
      - Rebasing public branches
      - Lost commits
      - Complex conflicts
      concepts:
      - Rebasing
      - Interactive rebase
      - History rewriting
      estimated_hours: 2-3
    - id: 3
      name: Advanced Operations
      description: Learn advanced Git operations and recovery.
      acceptance_criteria:
      - Cherry-pick specific commits
      - Bisect to find bugs
      - Stash management
      - Recover lost commits with reflog
      hints:
        level1: reflog shows all HEAD movements. Commits aren't truly lost.
        level2: 'bisect: binary search through history to find bug introduction.'
        level3: '# Cherry-pick commit from another branch

          git cherry-pick abc123

          git cherry-pick abc123..def456  # Range


          # Bisect to find bug

          git bisect start

          git bisect bad  # Current commit is bad

          git bisect good v1.0  # This version was good

          # Git checks out middle commit

          # Test and mark:

          git bisect good  # or

          git bisect bad

          # Repeat until found

          git bisect reset


          # Stash management

          git stash push -m ''WIP: auth changes''

          git stash list

          git stash apply stash@{0}

          git stash pop  # Apply and remove

          git stash drop stash@{1}


          # Recover ''lost'' commits with reflog

          git reflog

          # Shows:

          # abc123 HEAD@{0}: commit: Latest

          # def456 HEAD@{1}: reset: moving to HEAD~1

          # ghi789 HEAD@{2}: commit: The ''lost'' commit

          git checkout ghi789

          # Or

          git branch recovered ghi789'
      pitfalls:
      - Cherry-pick conflicts
      - Stash corruption
      - Not using reflog in time
      concepts:
      - Cherry-pick
      - Bisect
      - Reflog
      - Stash
      estimated_hours: 2-3
  gossip-protocol:
    id: gossip-protocol
    name: Gossip Protocol
    description: Implement a gossip-based protocol for eventually consistent data dissemination. Learn epidemic algorithms.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Networking basics
    - Distributed systems concepts
    - Probability
    languages:
      recommended:
      - Go
      - Rust
      - Python
      also_possible:
      - Java
      - Erlang
    resources:
    - name: Gossip Protocol Explained
      url: https://www.cs.cornell.edu/home/rvr/papers/flowgossip.pdf
      type: paper
    - name: SWIM Paper
      url: https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf
      type: paper
    milestones:
    - id: 1
      name: Peer Management
      description: Manage cluster membership and peer list.
      acceptance_criteria:
      - Maintain list of known peers
      - Random peer selection
      - Handle peer join/leave
      - Periodic peer list exchange
      hints:
        level1: Each node maintains list of known peers. Gossip updates this list.
        level2: 'Random selection: pick k random peers to gossip with each round.'
        level3: "import random\nimport threading\nfrom dataclasses import dataclass\nfrom typing import Dict, Set\nimport\
          \ time\n\n@dataclass\nclass Peer:\n    address: str\n    port: int\n    last_seen: float\n    state: str = 'alive'\
          \  # alive, suspected, dead\n\nclass PeerManager:\n    def __init__(self, self_addr: str, self_port: int):\n   \
          \     self.self_id = f'{self_addr}:{self_port}'\n        self.peers: Dict[str, Peer] = {}\n        self.lock = threading.Lock()\n\
          \        self.fanout = 3  # Number of peers to gossip with\n    \n    def add_peer(self, addr: str, port: int):\n\
          \        peer_id = f'{addr}:{port}'\n        with self.lock:\n            if peer_id != self.self_id and peer_id\
          \ not in self.peers:\n                self.peers[peer_id] = Peer(addr, port, time.time())\n    \n    def select_random_peers(self,\
          \ k: int = None) -> list:\n        '''Select k random alive peers'''\n        k = k or self.fanout\n        with\
          \ self.lock:\n            alive = [p for p in self.peers.values() if p.state == 'alive']\n            return random.sample(alive,\
          \ min(k, len(alive)))\n    \n    def mark_alive(self, peer_id: str):\n        with self.lock:\n            if peer_id\
          \ in self.peers:\n                self.peers[peer_id].last_seen = time.time()\n                self.peers[peer_id].state\
          \ = 'alive'\n    \n    def check_timeouts(self, timeout: float = 30.0):\n        '''Mark peers as dead if not seen\
          \ recently'''\n        now = time.time()\n        with self.lock:\n            for peer in self.peers.values():\n\
          \                if now - peer.last_seen > timeout:\n                    peer.state = 'dead'"
      pitfalls:
      - Not handling own address
      - Thread safety
      - Stale peer info
      concepts:
      - Membership
      - Random selection
      - Peer state
      estimated_hours: 3-4
    - id: 2
      name: Push Gossip
      description: Implement push-based gossip dissemination.
      acceptance_criteria:
      - Periodic gossip rounds
      - Push updates to random peers
      - Version/timestamp for updates
      - Infection-style spreading
      hints:
        level1: 'Push: send your data to random peers. They forward to others.'
        level2: Version numbers prevent old data from overwriting new.
        level3: "import json\nimport socket\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass GossipMessage:\n\
          \    sender: str\n    msg_type: str  # 'push', 'pull', 'ack'\n    key: str\n    value: str\n    version: int\n \
          \   \n    def to_json(self):\n        return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls,\
          \ data):\n        return cls(**json.loads(data))\n\nclass GossipNode:\n    def __init__(self, addr, port):\n   \
          \     self.addr = addr\n        self.port = port\n        self.peer_manager = PeerManager(addr, port)\n        self.data:\
          \ Dict[str, tuple] = {}  # key -> (value, version)\n        self.lock = threading.Lock()\n    \n    def set(self,\
          \ key: str, value: str):\n        '''Set local value and prepare for gossip'''\n        with self.lock:\n      \
          \      current = self.data.get(key, (None, 0))\n            self.data[key] = (value, current[1] + 1)\n    \n   \
          \ def gossip_round(self):\n        '''Push all data to random peers'''\n        peers = self.peer_manager.select_random_peers()\n\
          \        \n        with self.lock:\n            items = list(self.data.items())\n        \n        for peer in peers:\n\
          \            for key, (value, version) in items:\n                msg = GossipMessage(\n                    sender=f'{self.addr}:{self.port}',\n\
          \                    msg_type='push',\n                    key=key,\n                    value=value,\n        \
          \            version=version\n                )\n                self.send_to_peer(peer, msg)\n    \n    def handle_push(self,\
          \ msg: GossipMessage):\n        '''Receive pushed data'''\n        with self.lock:\n            current = self.data.get(msg.key,\
          \ (None, 0))\n            if msg.version > current[1]:\n                self.data[msg.key] = (msg.value, msg.version)\n\
          \        \n        self.peer_manager.mark_alive(msg.sender)"
      pitfalls:
      - Version conflicts
      - Infinite propagation
      - Network floods
      concepts:
      - Push gossip
      - Versioning
      - Epidemic spread
      estimated_hours: 4-5
    - id: 3
      name: Pull Gossip & Anti-Entropy
      description: Add pull-based reconciliation for consistency.
      acceptance_criteria:
      - Pull mechanism to request missing data
      - 'Anti-entropy: periodic full sync'
      - Combine push-pull for efficiency
      - Handle network partitions
      hints:
        level1: 'Pull: request data you''re missing. Anti-entropy: compare full state.'
        level2: 'Push-pull: push digest, peer responds with missing items.'
        level3: "def create_digest(self) -> Dict[str, int]:\n    '''Create digest of local data (key -> version)'''\n    with\
          \ self.lock:\n        return {k: v[1] for k, v in self.data.items()}\n\ndef pull_from_peer(self, peer: Peer):\n\
          \    '''Request peer's digest and pull missing/newer items'''\n    # Send our digest\n    our_digest = self.create_digest()\n\
          \    msg = GossipMessage(\n        sender=f'{self.addr}:{self.port}',\n        msg_type='pull_request',\n      \
          \  key='__digest__',\n        value=json.dumps(our_digest),\n        version=0\n    )\n    self.send_to_peer(peer,\
          \ msg)\n\ndef handle_pull_request(self, msg: GossipMessage):\n    '''Respond with items peer is missing or has old\
          \ versions of'''\n    their_digest = json.loads(msg.value)\n    our_digest = self.create_digest()\n    \n    # Find\
          \ items to send\n    to_send = []\n    with self.lock:\n        for key, (value, version) in self.data.items():\n\
          \            their_version = their_digest.get(key, 0)\n            if version > their_version:\n               \
          \ to_send.append((key, value, version))\n    \n    # Send missing items\n    for key, value, version in to_send:\n\
          \        response = GossipMessage(\n            sender=f'{self.addr}:{self.port}',\n            msg_type='pull_response',\n\
          \            key=key,\n            value=value,\n            version=version\n        )\n        # Send to requester\n\
          \ndef anti_entropy(self):\n    '''Periodic full state synchronization'''\n    peer = self.peer_manager.select_random_peers(1)\n\
          \    if peer:\n        self.pull_from_peer(peer[0])"
      pitfalls:
      - Digest size
      - Sync storms
      - Stale data during partition
      concepts:
      - Pull gossip
      - Anti-entropy
      - State reconciliation
      estimated_hours: 4-5
    - id: 4
      name: Failure Detection
      description: Implement gossip-based failure detection (SWIM-style).
      acceptance_criteria:
      - Probe random peers periodically
      - Indirect probing through other peers
      - Suspicion mechanism before declaring dead
      - Disseminate membership changes
      hints:
        level1: 'SWIM: ping peer, if no response, ask others to ping (indirect probe).'
        level2: 'Suspicion: don''t immediately mark dead. Wait for confirmation.'
        level3: "def failure_detection_round(self):\n    '''SWIM-style failure detection'''\n    peer = self.peer_manager.select_random_peers(1)\n\
          \    if not peer:\n        return\n    \n    target = peer[0]\n    \n    # Direct probe\n    if self.ping(target):\n\
          \        self.peer_manager.mark_alive(f'{target.address}:{target.port}')\n        return\n    \n    # Direct probe\
          \ failed, try indirect\n    helpers = self.peer_manager.select_random_peers(3)\n    responses = []\n    \n    for\
          \ helper in helpers:\n        # Ask helper to ping target\n        msg = GossipMessage(\n            sender=f'{self.addr}:{self.port}',\n\
          \            msg_type='ping_req',\n            key='target',\n            value=f'{target.address}:{target.port}',\n\
          \            version=0\n        )\n        if self.send_and_wait(helper, msg, timeout=2.0):\n            responses.append(True)\n\
          \    \n    if any(responses):\n        # Target reachable through helper\n        self.peer_manager.mark_alive(f'{target.address}:{target.port}')\n\
          \    else:\n        # Start suspicion\n        self.suspect_peer(target)\n\ndef suspect_peer(self, peer: Peer):\n\
          \    '''Mark peer as suspected, wait before declaring dead'''\n    peer_id = f'{peer.address}:{peer.port}'\n   \
          \ with self.lock:\n        if peer_id in self.peers:\n            self.peers[peer_id].state = 'suspected'\n    \n\
          \    # Schedule dead declaration if not refuted\n    threading.Timer(10.0, lambda: self.confirm_dead(peer_id)).start()\n\
          \ndef confirm_dead(self, peer_id: str):\n    '''Confirm death if still suspected'''\n    with self.lock:\n     \
          \   if peer_id in self.peers and self.peers[peer_id].state == 'suspected':\n            self.peers[peer_id].state\
          \ = 'dead'\n            # Gossip membership change\n            self.broadcast_membership_change(peer_id, 'dead')"
      pitfalls:
      - False positives
      - Suspicion timeout tuning
      - Split brain
      concepts:
      - SWIM protocol
      - Failure detection
      - Suspicion
      estimated_hours: 4-6
  graph-algos:
    id: graph-algos
    name: Graph Algorithms
    description: Implement fundamental graph algorithms. Learn BFS, DFS, shortest paths, and spanning trees.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Basic data structures
    - Recursion
    - Algorithm complexity
    languages:
      recommended:
      - Python
      - Java
      - C++
      also_possible:
      - Go
      - Rust
    resources:
    - name: CLRS - Introduction to Algorithms
      url: https://mitpress.mit.edu/books/introduction-algorithms-fourth-edition
      type: book
    - name: Visualgo Graphs
      url: https://visualgo.net/en/dfsbfs
      type: interactive
    milestones:
    - id: 1
      name: Graph Representation
      description: Implement adjacency list and matrix representations.
      acceptance_criteria:
      - Adjacency list for sparse graphs
      - Adjacency matrix for dense graphs
      - Add/remove vertices and edges
      - Support weighted and directed graphs
      hints:
        level1: 'Adjacency list: dict of vertex -> list of neighbors.'
        level2: 'Adjacency matrix: 2D array, matrix[i][j] = weight or 0.'
        level3: "class Graph:\n    def __init__(self, directed=False):\n        self.adj = {}  # vertex -> [(neighbor, weight),\
          \ ...]\n        self.directed = directed\n    \n    def add_vertex(self, v):\n        if v not in self.adj:\n  \
          \          self.adj[v] = []\n    \n    def add_edge(self, u, v, weight=1):\n        self.add_vertex(u)\n       \
          \ self.add_vertex(v)\n        self.adj[u].append((v, weight))\n        if not self.directed:\n            self.adj[v].append((u,\
          \ weight))\n    \n    def neighbors(self, v):\n        return self.adj.get(v, [])\n    \n    def vertices(self):\n\
          \        return self.adj.keys()"
      pitfalls:
      - Forgetting undirected edge goes both ways
      - Self-loops
      - Parallel edges
      concepts:
      - Graph representation
      - Space complexity
      - Edge weights
      estimated_hours: 2-3
    - id: 2
      name: BFS and DFS
      description: Implement breadth-first and depth-first search.
      acceptance_criteria:
      - BFS with queue
      - DFS with recursion or stack
      - Track visited vertices
      - Return traversal order or path
      hints:
        level1: BFS uses queue (FIFO), DFS uses stack (LIFO).
        level2: Mark visited BEFORE adding to queue/stack to avoid duplicates.
        level3: "from collections import deque\n\ndef bfs(graph, start):\n    visited = {start}\n    queue = deque([start])\n\
          \    order = []\n    \n    while queue:\n        v = queue.popleft()\n        order.append(v)\n        for neighbor,\
          \ _ in graph.neighbors(v):\n            if neighbor not in visited:\n                visited.add(neighbor)\n   \
          \             queue.append(neighbor)\n    \n    return order\n\ndef dfs(graph, start):\n    visited = set()\n  \
          \  order = []\n    \n    def visit(v):\n        if v in visited:\n            return\n        visited.add(v)\n \
          \       order.append(v)\n        for neighbor, _ in graph.neighbors(v):\n            visit(neighbor)\n    \n   \
          \ visit(start)\n    return order"
      pitfalls:
      - Infinite loop without visited check
      - Stack overflow in DFS
      - Disconnected components
      concepts:
      - Graph traversal
      - BFS vs DFS
      - Visited tracking
      estimated_hours: 2-3
    - id: 3
      name: Shortest Path (Dijkstra)
      description: Implement Dijkstra's algorithm for shortest paths.
      acceptance_criteria:
      - Priority queue for efficiency
      - Handle weighted edges
      - Reconstruct shortest path
      - Detect unreachable vertices
      hints:
        level1: 'Greedy: always expand closest unvisited vertex.'
        level2: 'Use heap: (distance, vertex). Update distances when shorter found.'
        level3: "import heapq\n\ndef dijkstra(graph, start):\n    dist = {v: float('inf') for v in graph.vertices()}\n   \
          \ dist[start] = 0\n    prev = {v: None for v in graph.vertices()}\n    pq = [(0, start)]\n    visited = set()\n\
          \    \n    while pq:\n        d, u = heapq.heappop(pq)\n        if u in visited:\n            continue\n       \
          \ visited.add(u)\n        \n        for v, weight in graph.neighbors(u):\n            if v not in visited:\n   \
          \             new_dist = dist[u] + weight\n                if new_dist < dist[v]:\n                    dist[v] =\
          \ new_dist\n                    prev[v] = u\n                    heapq.heappush(pq, (new_dist, v))\n    \n    return\
          \ dist, prev\n\ndef reconstruct_path(prev, start, end):\n    path = []\n    current = end\n    while current is\
          \ not None:\n        path.append(current)\n        current = prev[current]\n    return path[::-1] if path[-1] ==\
          \ start else []"
      pitfalls:
      - Negative weights (use Bellman-Ford)
      - Duplicate entries in heap
      - Path reconstruction direction
      concepts:
      - Greedy algorithms
      - Priority queues
      - Shortest paths
      estimated_hours: 3-4
    - id: 4
      name: Minimum Spanning Tree
      description: Implement Kruskal's or Prim's MST algorithm.
      acceptance_criteria:
      - Sort edges by weight (Kruskal)
      - Union-Find for cycle detection
      - Build MST edges
      - Handle disconnected graphs
      hints:
        level1: 'Kruskal: sort edges, add if doesn''t create cycle.'
        level2: 'Union-Find: disjoint set with union by rank, path compression.'
        level3: "class UnionFind:\n    def __init__(self, n):\n        self.parent = list(range(n))\n        self.rank = [0]\
          \ * n\n    \n    def find(self, x):\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\
          \  # Path compression\n        return self.parent[x]\n    \n    def union(self, x, y):\n        px, py = self.find(x),\
          \ self.find(y)\n        if px == py:\n            return False  # Already connected\n        if self.rank[px] <\
          \ self.rank[py]:\n            px, py = py, px\n        self.parent[py] = px\n        if self.rank[px] == self.rank[py]:\n\
          \            self.rank[px] += 1\n        return True\n\ndef kruskal(graph):\n    # Get all edges\n    edges = []\n\
          \    for u in graph.vertices():\n        for v, w in graph.neighbors(u):\n            if u < v:  # Avoid duplicates\
          \ in undirected\n                edges.append((w, u, v))\n    edges.sort()\n    \n    vertices = list(graph.vertices())\n\
          \    uf = UnionFind(len(vertices))\n    v_to_i = {v: i for i, v in enumerate(vertices)}\n    \n    mst = []\n  \
          \  for w, u, v in edges:\n        if uf.union(v_to_i[u], v_to_i[v]):\n            mst.append((u, v, w))\n    \n\
          \    return mst"
      pitfalls:
      - Undirected edge duplication
      - Union-Find without optimization
      - Disconnected graph check
      concepts:
      - Minimum spanning tree
      - Union-Find
      - Greedy algorithms
      estimated_hours: 4-5
  grep-clone:
    id: grep-clone
    name: Grep Clone
    description: Build a pattern matching utility. Learn regular expressions, text search, and efficient file scanning.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - Basic programming
    - Understanding of regular expressions
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
    resources:
    - name: Grep Man Page
      url: https://man7.org/linux/man-pages/man1/grep.1.html
      type: documentation
    - name: Regex Tutorial
      url: https://regexone.com/
      type: tutorial
    milestones:
    - id: 1
      name: Simple String Matching
      description: Match literal strings in files.
      acceptance_criteria:
      - Search for literal string
      - Print matching lines
      - Support multiple files
      - Show filename with multiple files
      hints:
        level1: Use strstr() for substring matching.
        level2: Read line by line with fgets() or getline().
        level3: "char line[4096];\nwhile (fgets(line, sizeof(line), f)) {\n    if (strstr(line, pattern)) {\n        if (show_filename)\
          \ printf(\"%s:\", filename);\n        printf(\"%s\", line);\n    }\n}"
      pitfalls:
      - Lines longer than buffer
      - Missing newline at end
      - Binary files
      concepts:
      - String searching
      - Line-oriented processing
      - Output formatting
      estimated_hours: 2-3
    - id: 2
      name: Common Options
      description: Add essential grep options.
      acceptance_criteria:
      - -i case insensitive
      - -v invert match
      - -n show line numbers
      - -c count matches only
      - -l files with matches only
      hints:
        level1: For case insensitive, convert both pattern and line to lowercase.
        level2: Use strcasestr() or implement case-folding.
        level3: "int matches = 0;\nint line_num = 0;\nwhile (fgets(line, sizeof(line), f)) {\n    line_num++;\n    int match\
          \ = (opt_i ? strcasestr(line, pattern) : strstr(line, pattern)) != NULL;\n    if (opt_v) match = !match;\n    if\
          \ (match) {\n        matches++;\n        if (opt_l) { printf(\"%s\\n\", filename); break; }\n        if (!opt_c)\
          \ {\n            if (opt_n) printf(\"%d:\", line_num);\n            printf(\"%s\", line);\n        }\n    }\n}\n\
          if (opt_c) printf(\"%d\\n\", matches);"
      pitfalls:
      - Case folding for non-ASCII
      - Combining -v with -c
      - -l should stop after first match
      concepts:
      - Case insensitive matching
      - Inverted logic
      - Counting vs printing
      estimated_hours: 2-3
    - id: 3
      name: Basic Regex Support
      description: Add regular expression matching.
      acceptance_criteria:
      - . matches any char
      - '* matches zero or more'
      - ^ and $ anchors
      - Character classes [abc]
      - -E for extended regex
      hints:
        level1: Use POSIX regex library (regex.h) or implement simple patterns.
        level2: regcomp() to compile, regexec() to match.
        level3: "#include <regex.h>\n\nregex_t regex;\nint flags = REG_NOSUB | (opt_i ? REG_ICASE : 0) | (opt_E ? REG_EXTENDED\
          \ : 0);\nif (regcomp(&regex, pattern, flags) != 0) {\n    fprintf(stderr, \"Invalid pattern\\n\");\n    return 1;\n\
          }\n// Then use:\nint match = regexec(&regex, line, 0, NULL, 0) == 0;\nregfree(&regex);"
      pitfalls:
      - Regex compilation errors
      - BRE vs ERE differences
      - Newline in regex
      concepts:
      - Regular expressions
      - POSIX regex API
      - Pattern compilation
      estimated_hours: 3-4
  hash-impl:
    id: hash-impl
    name: Hash Function (SHA-256)
    description: Implement SHA-256 hash function from the NIST specification. Learn about cryptographic primitives and bitwise
      operations.
    difficulty: beginner
    estimated_hours: 10-15
    prerequisites:
    - Binary and hexadecimal representation
    - Bitwise operations
    - Basic understanding of cryptography
    languages:
      recommended:
      - Python
      - JavaScript
      - C
      also_possible:
      - Rust
      - Go
      - Java
    resources:
    - name: SHA-256 Step by Step
      url: https://blog.boot.dev/cryptography/how-sha-2-works-step-by-step-sha-256/
      type: tutorial
    - name: NIST SHA-256 Specification
      url: https://csrc.nist.gov/publications/detail/fips/180/4/final
      type: specification
    milestones:
    - id: 1
      name: Message Preprocessing
      description: Implement message padding and parsing.
      acceptance_criteria:
      - Convert message to binary
      - Append '1' bit
      - Pad with zeros to 448 mod 512
      - Append original length as 64-bit integer
      - Parse into 512-bit blocks
      hints:
        level1: Message length in bits must end at 64 bits from 512 boundary.
        level2: 'Padding: 1 + zeros + 64-bit length. Total = multiple of 512.'
        level3: "def preprocess(message):\n    if isinstance(message, str):\n        message = message.encode()\n    original_bit_len\
          \ = len(message) * 8\n    message += b'\\x80'\n    while (len(message) % 64) != 56:\n        message += b'\\x00'\n\
          \    message += original_bit_len.to_bytes(8, 'big')\n    return [message[i:i+64] for i in range(0, len(message),\
          \ 64)]"
      pitfalls:
      - Wrong bit/byte conversion
      - Endianness errors
      - Off-by-one in padding calculation
      concepts:
      - Bit padding
      - Message blocks
      - Binary representation
      estimated_hours: 2-3
    - id: 2
      name: Message Schedule
      description: Generate the message schedule from each 512-bit block.
      acceptance_criteria:
      - Parse 512-bit block into 16 words (32-bit each)
      - Extend to 64 words using SHA-256 schedule
      - Implement Ïƒ0 and Ïƒ1 functions
      - Words stored as 32-bit unsigned integers
      hints:
        level1: 'First 16 words: direct from message block (big-endian).'
        level2: 'Words 16-63: W[i] = Ïƒ1(W[i-2]) + W[i-7] + Ïƒ0(W[i-15]) + W[i-16].'
        level3: "def rotr(x, n):\n    return ((x >> n) | (x << (32 - n))) & 0xffffffff\n\ndef sigma0(x):\n    return rotr(x,\
          \ 7) ^ rotr(x, 18) ^ (x >> 3)\n\ndef sigma1(x):\n    return rotr(x, 17) ^ rotr(x, 19) ^ (x >> 10)\n\ndef message_schedule(block):\n\
          \    W = [int.from_bytes(block[i*4:(i+1)*4], 'big') for i in range(16)]\n    for i in range(16, 64):\n        W.append((sigma1(W[i-2])\
          \ + W[i-7] + sigma0(W[i-15]) + W[i-16]) & 0xffffffff)\n    return W"
      pitfalls:
      - Not masking to 32 bits
      - Right rotate vs right shift
      - Wrong Ïƒ function parameters
      concepts:
      - Rotate operations
      - XOR operations
      - Word expansion
      estimated_hours: 2-3
    - id: 3
      name: Compression Function
      description: Implement the main compression function.
      acceptance_criteria:
      - Initialize working variables (a-h) from hash values
      - 64 rounds of compression
      - Implement Î£0, Î£1, Ch, Maj functions
      - Use correct K constants
      - Update hash values after compression
      hints:
        level1: 'Each round: T1 = h + Î£1(e) + Ch(e,f,g) + K[i] + W[i].'
        level2: Ch(x,y,z) = (x AND y) XOR (NOT x AND z). Maj(x,y,z) = (x AND y) XOR (x AND z) XOR (y AND z).
        level3: "def compress(H, W):\n    a, b, c, d, e, f, g, h = H\n    for i in range(64):\n        S1 = rotr(e, 6) ^ rotr(e,\
          \ 11) ^ rotr(e, 25)\n        ch = (e & f) ^ (~e & g)\n        T1 = (h + S1 + ch + K[i] + W[i]) & 0xffffffff\n  \
          \      S0 = rotr(a, 2) ^ rotr(a, 13) ^ rotr(a, 22)\n        maj = (a & b) ^ (a & c) ^ (b & c)\n        T2 = (S0\
          \ + maj) & 0xffffffff\n        h, g, f, e, d, c, b, a = g, f, e, (d + T1) & 0xffffffff, c, b, a, (T1 + T2) & 0xffffffff\n\
          \    return [(H[i] + v) & 0xffffffff for i, v in enumerate([a,b,c,d,e,f,g,h])]"
      pitfalls:
      - Mixing up Î£ and Ïƒ functions
      - Wrong K constant values
      - Not masking intermediate results
      concepts:
      - Compression function
      - Round functions
      - Hash state
      estimated_hours: 4-5
    - id: 4
      name: Final Hash Output
      description: Produce the final 256-bit hash output.
      acceptance_criteria:
      - Process all message blocks
      - Concatenate final hash values
      - Output as 64-character hex string
      - Test against known vectors
      - Handle empty input correctly
      hints:
        level1: Process blocks sequentially, each updates hash state.
        level2: 'Final hash: concatenate all 8 hash values as big-endian.'
        level3: "def sha256(message):\n    blocks = preprocess(message)\n    H = initial_hash_values.copy()\n    for block\
          \ in blocks:\n        W = message_schedule(block)\n        H = compress(H, W)\n    return ''.join(h.to_bytes(4,\
          \ 'big').hex() for h in H)\n\n# Test vectors\nassert sha256('') == 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n\
          assert sha256('abc') == 'ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad'"
      pitfalls:
      - Wrong hash for empty string
      - Endianness in output
      - Not resetting state between calls
      concepts:
      - Hash finalization
      - Test vectors
      - Hex encoding
      estimated_hours: 2-3
  hash-table:
    id: hash-table
    name: Hash Table
    description: Implement a hash table from scratch. Learn hash functions, collision resolution, and performance trade-offs.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - Arrays
    - Understanding of hashing
    languages:
      recommended:
      - C
      - Python
      - Java
      also_possible:
      - Go
      - Rust
    resources:
    - name: Hash Table - Wikipedia
      url: https://en.wikipedia.org/wiki/Hash_table
      type: article
    milestones:
    - id: 1
      name: Chaining Implementation
      description: Implement hash table with separate chaining.
      acceptance_criteria:
      - put(key, value)
      - get(key) returns value
      - remove(key)
      - Linked list for collisions
      - Good hash function
      hints:
        level1: Each bucket is a linked list of key-value pairs.
        level2: Use djb2 or FNV hash for strings.
        level3: "class Entry:\n    def __init__(self, key, value):\n        self.key = key\n        self.value = value\n \
          \       self.next = None\n\nclass HashTable:\n    def __init__(self, capacity=16):\n        self.capacity = capacity\n\
          \        self.size = 0\n        self.buckets = [None] * capacity\n    \n    def _hash(self, key):\n        h = 5381\n\
          \        for c in key:\n            h = ((h << 5) + h) + ord(c)\n        return h % self.capacity\n    \n    def\
          \ put(self, key, value):\n        idx = self._hash(key)\n        entry = self.buckets[idx]\n        while entry:\n\
          \            if entry.key == key:\n                entry.value = value\n                return\n            entry\
          \ = entry.next\n        new_entry = Entry(key, value)\n        new_entry.next = self.buckets[idx]\n        self.buckets[idx]\
          \ = new_entry\n        self.size += 1\n    \n    def get(self, key):\n        idx = self._hash(key)\n        entry\
          \ = self.buckets[idx]\n        while entry:\n            if entry.key == key:\n                return entry.value\n\
          \            entry = entry.next\n        return None"
      pitfalls:
      - Poor hash function
      - Not updating existing keys
      - Memory leaks on remove
      concepts:
      - Hash functions
      - Collision resolution
      - Linked lists
      estimated_hours: 2-3
    - id: 2
      name: Open Addressing
      description: Implement hash table with open addressing.
      acceptance_criteria:
      - Linear probing
      - Quadratic probing (optional)
      - Double hashing (optional)
      - Proper deletion (tombstones)
      - Load factor monitoring
      hints:
        level1: On collision, try next slot. Wrap around.
        level2: Deleted entries need tombstones to not break search.
        level3: "DELETED = object()  # Tombstone sentinel\n\nclass OpenHashTable:\n    def __init__(self, capacity=16):\n\
          \        self.keys = [None] * capacity\n        self.values = [None] * capacity\n        self.capacity = capacity\n\
          \        self.size = 0\n    \n    def put(self, key, value):\n        if self.size >= self.capacity * 0.7:\n   \
          \         self._resize()\n        idx = self._hash(key)\n        while self.keys[idx] is not None and self.keys[idx]\
          \ is not DELETED:\n            if self.keys[idx] == key:\n                self.values[idx] = value\n           \
          \     return\n            idx = (idx + 1) % self.capacity\n        self.keys[idx] = key\n        self.values[idx]\
          \ = value\n        self.size += 1\n    \n    def get(self, key):\n        idx = self._hash(key)\n        start =\
          \ idx\n        while self.keys[idx] is not None:\n            if self.keys[idx] == key:\n                return\
          \ self.values[idx]\n            idx = (idx + 1) % self.capacity\n            if idx == start:\n                break\n\
          \        return None\n    \n    def remove(self, key):\n        idx = self._hash(key)\n        while self.keys[idx]\
          \ is not None:\n            if self.keys[idx] == key:\n                self.keys[idx] = DELETED\n              \
          \  self.values[idx] = None\n                self.size -= 1\n                return True\n            idx = (idx\
          \ + 1) % self.capacity\n        return False"
      pitfalls:
      - Infinite loop without tombstones
      - Clustering
      - High load factor performance
      concepts:
      - Open addressing
      - Tombstones
      - Probing sequences
      estimated_hours: 3-4
    - id: 3
      name: Dynamic Resizing
      description: Implement automatic resizing for performance.
      acceptance_criteria:
      - Grow when load factor > 0.75
      - Shrink when load factor < 0.25 (optional)
      - Rehash all entries
      - Amortized O(1) operations
      hints:
        level1: Double capacity and reinsert all entries.
        level2: Rehash needed because bucket index depends on capacity.
        level3: "def _resize(self, new_capacity=None):\n    if new_capacity is None:\n        new_capacity = self.capacity\
          \ * 2\n    old_buckets = self.buckets\n    self.capacity = new_capacity\n    self.buckets = [None] * new_capacity\n\
          \    self.size = 0\n    \n    for bucket in old_buckets:\n        entry = bucket\n        while entry:\n       \
          \     self.put(entry.key, entry.value)\n            entry = entry.next\n\ndef put(self, key, value):\n    if self.size\
          \ >= self.capacity * 0.75:\n        self._resize()\n    # ... rest of put ..."
      pitfalls:
      - Rehashing during resize
      - Not updating size correctly
      - Memory during resize
      concepts:
      - Amortized analysis
      - Load factor
      - Rehashing
      estimated_hours: 2-3
  hexdump:
    id: hexdump
    name: Hexdump Utility
    description: Build a hexdump utility to display binary file contents in hexadecimal and ASCII. Learn binary file handling
      and formatted output.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - File I/O
    - Binary vs text
    - Formatted output
    languages:
      recommended:
      - C
      - Python
      - Rust
      also_possible:
      - Go
      - JavaScript
    resources:
    - name: Let's Build a Hexdump Utility in C
      url: http://www.dmulholl.com/blog/lets-build-a-hexdump-utility.html
      type: tutorial
    - name: Hexdump man page
      url: https://man7.org/linux/man-pages/man1/hexdump.1.html
      type: reference
    milestones:
    - id: 1
      name: Basic Hex Output
      description: Read binary file and output bytes in hexadecimal format.
      acceptance_criteria:
      - Read file in binary mode
      - Output 16 bytes per line
      - Show offset at start of each line
      - Handle files of any size
      hints:
        level1: Read file in chunks of 16 bytes. Use format specifier %02x for hex output.
        level2: Track offset, incrementing by 16 each line. Handle partial last line.
        level3: "def hexdump_basic(filepath: str):\n    offset = 0\n    with open(filepath, 'rb') as f:\n        while True:\n\
          \            chunk = f.read(16)\n            if not chunk:\n                break\n\n            # Format: offset:\
          \ hex bytes\n            hex_str = ' '.join(f'{b:02x}' for b in chunk)\n            print(f'{offset:08x}: {hex_str}')\n\
          \n            offset += len(chunk)\n\n# Output:\n# 00000000: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00\n#\
          \ 00000010: 03 00 3e 00 01 00 00 00 40 10 40 00 00 00 00 00"
      pitfalls:
      - Opening file in text mode mangles binary data
      - Last chunk may have fewer than 16 bytes
      - Large files should be streamed, not loaded entirely
      concepts:
      - Binary file I/O
      - Hexadecimal formatting
      - Chunked reading
      estimated_hours: 2-3
    - id: 2
      name: ASCII Column
      description: Add ASCII representation alongside hex output.
      acceptance_criteria:
      - Show printable ASCII characters
      - Replace non-printable with '.'
      - Align ASCII column properly
      - Handle partial lines
      hints:
        level1: 'Printable ASCII: 0x20-0x7E. Use chr() to convert, ''.'' for others.'
        level2: Pad hex section with spaces for short lines to keep ASCII aligned.
        level3: "def hexdump_with_ascii(filepath: str):\n    offset = 0\n    with open(filepath, 'rb') as f:\n        while\
          \ True:\n            chunk = f.read(16)\n            if not chunk:\n                break\n\n            # Hex part\n\
          \            hex_parts = [f'{b:02x}' for b in chunk]\n            hex_str = ' '.join(hex_parts)\n\n            #\
          \ Pad for alignment (each byte = 3 chars: \"xx \")\n            padding = '   ' * (16 - len(chunk))\n\n        \
          \    # ASCII part\n            ascii_str = ''.join(\n                chr(b) if 0x20 <= b <= 0x7e else '.'\n    \
          \            for b in chunk\n            )\n\n            print(f'{offset:08x}: {hex_str}{padding}  |{ascii_str}|')\n\
          \            offset += len(chunk)\n\n# Output:\n# 00000000: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00  |.ELF............|\n\
          # 00000010: 03 00 3e 00 01 00 00 00 40 10 40 00 00 00 00 00  |..>.....@.@.....|"
      pitfalls:
      - Don't print control characters (newlines, tabs corrupt output)
      - Alignment breaks on partial lines without padding
      - Unicode handling varies by terminal
      concepts:
      - ASCII encoding
      - String formatting
      - Column alignment
      estimated_hours: 2-3
    - id: 3
      name: Grouped Output
      description: Group hex bytes (2, 4, or 8 byte groupings) for better readability.
      acceptance_criteria:
      - Support -g option for group size (1, 2, 4, 8)
      - Add extra space between groups
      - Default to 2-byte groups (like xxd)
      - Handle endianness display option
      hints:
        level1: 'Split 16-byte line into groups. For 2-byte groups: 8 groups per line.'
        level2: Join bytes within group without space, separate groups with space.
        level3: "def hexdump_grouped(filepath: str, group_size: int = 2):\n    offset = 0\n    with open(filepath, 'rb') as\
          \ f:\n        while True:\n            chunk = f.read(16)\n            if not chunk:\n                break\n\n\
          \            # Group bytes\n            groups = []\n            for i in range(0, len(chunk), group_size):\n  \
          \              group = chunk[i:i+group_size]\n                groups.append(''.join(f'{b:02x}' for b in group))\n\
          \n            hex_str = ' '.join(groups)\n\n            # Calculate padding (each group = group_size*2 + 1 for space)\n\
          \            full_groups = 16 // group_size\n            actual_groups = (len(chunk) + group_size - 1) // group_size\n\
          \            padding = ' ' * ((full_groups - actual_groups) * (group_size * 2 + 1))\n\n            ascii_str = ''.join(\n\
          \                chr(b) if 0x20 <= b <= 0x7e else '.'\n                for b in chunk\n            )\n\n       \
          \     print(f'{offset:08x}: {hex_str}{padding}  {ascii_str}')\n            offset += len(chunk)\n\n# With group_size=2:\n\
          # 00000000: 7f45 4c46 0201 0100 0000 0000 0000 0000  .ELF............\n# 00000010: 0300 3e00 0100 0000 4010 4000\
          \ 0000 0000  ..>.....@.@....."
      pitfalls:
      - Group boundaries at end of file need special handling
      - Endianness affects display for multi-byte groups
      - Different tools use different default groupings
      concepts:
      - Data grouping
      - Byte order
      - Flexible formatting
      estimated_hours: 2-3
    - id: 4
      name: CLI Options
      description: 'Add command-line options: offset, length, output format.'
      acceptance_criteria:
      - '-s/--skip: start offset'
      - '-n/--length: number of bytes'
      - '-C: canonical format (like hexdump -C)'
      - Read from stdin if no file specified
      hints:
        level1: Use argparse (Python) or getopt (C). Seek to offset before reading.
        level2: Handle - as stdin. Count bytes for length limit.
        level3: "import argparse\nimport sys\n\ndef main():\n    parser = argparse.ArgumentParser(description='Hexadecimal\
          \ file dump')\n    parser.add_argument('file', nargs='?', default='-',\n                        help='File to dump\
          \ (default: stdin)')\n    parser.add_argument('-C', '--canonical', action='store_true',\n                      \
          \  help='Canonical hex+ASCII display')\n    parser.add_argument('-g', '--group', type=int, default=2,\n        \
          \                help='Bytes per group (default: 2)')\n    parser.add_argument('-s', '--skip', type=int, default=0,\n\
          \                        help='Skip bytes at start')\n    parser.add_argument('-n', '--length', type=int, default=None,\n\
          \                        help='Dump only N bytes')\n    args = parser.parse_args()\n\n    # Open file or stdin\n\
          \    if args.file == '-':\n        f = sys.stdin.buffer\n    else:\n        f = open(args.file, 'rb')\n\n    try:\n\
          \        # Skip to offset\n        if args.skip and args.file != '-':\n            f.seek(args.skip)\n        elif\
          \ args.skip:\n            f.read(args.skip)  # Can't seek stdin\n\n        bytes_read = 0\n        while True:\n\
          \            remaining = None\n            if args.length:\n                remaining = args.length - bytes_read\n\
          \                if remaining <= 0:\n                    break\n\n            chunk_size = min(16, remaining) if\
          \ remaining else 16\n            chunk = f.read(chunk_size)\n            if not chunk:\n                break\n\n\
          \            # Output...\n            bytes_read += len(chunk)\n    finally:\n        if f != sys.stdin.buffer:\n\
          \            f.close()"
      pitfalls:
      - Can't seek on stdin (must read and discard)
      - Length limit interacts with offset
      - Binary mode required for stdin
      concepts:
      - CLI design
      - Standard input
      - Argument parsing
      estimated_hours: 2-3
  http-server-basic:
    id: http-server-basic
    name: HTTP Server (Basic)
    description: Build a static file HTTP server. Learn TCP networking, HTTP protocol, and concurrent request handling.
    difficulty: intermediate
    estimated_hours: 12-20
    prerequisites:
    - TCP/IP basics
    - Socket programming
    - File I/O
    languages:
      recommended:
      - C
      - Go
      - Rust
      also_possible:
      - Python
      - Java
    resources:
    - name: HTTP/1.1 Specification (RFC 7230)
      url: https://tools.ietf.org/html/rfc7230
      type: specification
    - name: Beej's Guide to Network Programming
      url: https://beej.us/guide/bgnet/
      type: tutorial
    milestones:
    - id: 1
      name: TCP Server Basics
      description: Create a listening TCP server.
      acceptance_criteria:
      - Bind to port 8080
      - Accept connections
      - Read request data
      - Send hardcoded response
      - Close connection
      hints:
        level1: socket(), bind(), listen(), accept() sequence.
        level2: Set SO_REUSEADDR to avoid 'address already in use'.
        level3: "int server_fd = socket(AF_INET, SOCK_STREAM, 0);\nint opt = 1;\nsetsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR,\
          \ &opt, sizeof(opt));\nstruct sockaddr_in addr = { .sin_family = AF_INET, .sin_addr.s_addr = INADDR_ANY, .sin_port\
          \ = htons(8080) };\nbind(server_fd, (struct sockaddr*)&addr, sizeof(addr));\nlisten(server_fd, 10);\nwhile (1) {\n\
          \    int client_fd = accept(server_fd, NULL, NULL);\n    char buf[1024];\n    read(client_fd, buf, sizeof(buf));\n\
          \    char *resp = \"HTTP/1.1 200 OK\\r\\nContent-Length: 5\\r\\n\\r\\nHello\";\n    write(client_fd, resp, strlen(resp));\n\
          \    close(client_fd);\n}"
      pitfalls:
      - Byte order (htons)
      - Not handling partial reads
      - Forgetting to close client fd
      concepts:
      - Sockets
      - TCP connection lifecycle
      - Bind/Listen/Accept
      estimated_hours: 2-3
    - id: 2
      name: HTTP Request Parsing
      description: Parse HTTP request line and headers.
      acceptance_criteria:
      - Parse method, path, version
      - Parse headers into key-value pairs
      - Handle GET requests
      - Extract Host header
      hints:
        level1: 'Request line: GET /path HTTP/1.1\r\n'
        level2: Headers end at \r\n\r\n (blank line).
        level3: "// Parse request line\nchar method[16], path[256], version[16];\nsscanf(request, \"%s %s %s\", method, path,\
          \ version);\n\n// Parse headers\nchar *line = strstr(request, \"\\r\\n\") + 2;\nwhile (line && strncmp(line, \"\\\
          r\\n\", 2) != 0) {\n    char *colon = strchr(line, ':');\n    if (colon) {\n        *colon = '\\0';\n        char\
          \ *value = colon + 1;\n        while (*value == ' ') value++;\n        // Store header: line -> value\n    }\n \
          \   line = strstr(line, \"\\r\\n\");\n    if (line) line += 2;\n}"
      pitfalls:
      - CRLF vs LF line endings
      - Whitespace around header values
      - Buffer overflow on long lines
      concepts:
      - HTTP message format
      - Request parsing
      - Header handling
      estimated_hours: 2-3
    - id: 3
      name: Static File Serving
      description: Serve files from a directory.
      acceptance_criteria:
      - Map URL path to file system
      - Detect and set Content-Type
      - Send file contents
      - Handle 404 Not Found
      - Prevent directory traversal
      hints:
        level1: Concatenate document root with URL path.
        level2: Use realpath() to resolve and validate path.
        level3: "char filepath[512];\nsnprintf(filepath, sizeof(filepath), \"%s%s\", docroot, path);\nchar resolved[PATH_MAX];\n\
          if (!realpath(filepath, resolved) || strncmp(resolved, docroot, strlen(docroot)) != 0) {\n    send_404(client_fd);\n\
          \    return;\n}\nFILE *f = fopen(resolved, \"rb\");\nif (!f) { send_404(client_fd); return; }\nfseek(f, 0, SEEK_END);\n\
          long size = ftell(f);\nfseek(f, 0, SEEK_SET);\nchar *mime = get_mime_type(resolved);\ndprintf(client_fd, \"HTTP/1.1\
          \ 200 OK\\r\\nContent-Type: %s\\r\\nContent-Length: %ld\\r\\n\\r\\n\", mime, size);\nchar buf[8192];\nsize_t n;\n\
          while ((n = fread(buf, 1, sizeof(buf), f)) > 0) {\n    write(client_fd, buf, n);\n}"
      pitfalls:
      - Directory traversal (../)
      - Missing Content-Type
      - Binary file handling
      concepts:
      - Path resolution
      - MIME types
      - Security validation
      estimated_hours: 3-4
    - id: 4
      name: Concurrent Connections
      description: Handle multiple clients concurrently.
      acceptance_criteria:
      - Thread-per-connection model
      - Thread pool (optional)
      - Non-blocking with select/poll (optional)
      - Graceful shutdown
      hints:
        level1: 'Simple: fork() per connection or spawn thread.'
        level2: 'Better: thread pool with work queue.'
        level3: "// Thread per connection\nwhile (1) {\n    int client_fd = accept(server_fd, NULL, NULL);\n    pthread_t\
          \ thread;\n    int *arg = malloc(sizeof(int));\n    *arg = client_fd;\n    pthread_create(&thread, NULL, handle_client,\
          \ arg);\n    pthread_detach(thread);\n}\n\nvoid *handle_client(void *arg) {\n    int client_fd = *(int*)arg;\n \
          \   free(arg);\n    // ... handle request ...\n    close(client_fd);\n    return NULL;\n}"
      pitfalls:
      - Thread safety
      - File descriptor leaks
      - Resource exhaustion
      concepts:
      - Concurrency models
      - Threading
      - Connection handling
      estimated_hours: 4-6
  http2-server:
    id: http2-server
    name: HTTP/2 Server
    description: Build an HTTP/2 server with multiplexing. Learn binary framing, HPACK compression, and stream management.
    difficulty: advanced
    estimated_hours: 30-50
    prerequisites:
    - HTTP/1.1 server
    - TLS
    - Binary protocols
    languages:
      recommended:
      - Go
      - Rust
      - C
      also_possible:
      - Java
      - Python
    resources:
    - type: spec
      name: RFC 7540 - HTTP/2
      url: https://httpwg.org/specs/rfc7540.html
    - type: spec
      name: RFC 7541 - HPACK
      url: https://httpwg.org/specs/rfc7541.html
    milestones:
    - id: 1
      name: Binary Framing
      description: Parse and emit HTTP/2 frames.
      acceptance_criteria:
      - Frame header parsing (9 bytes)
      - Frame type handling
      - Frame flags
      - Payload extraction
      hints:
        level1: 'All frames start with 9-byte header: length(3), type(1), flags(1), stream_id(4).'
        level2: Length is 24-bit big-endian. Stream ID has reserved high bit.
        level3: "class Frame:\n    def __init__(self, type, flags, stream_id, payload=b''):\n        self.type = type\n  \
          \      self.flags = flags\n        self.stream_id = stream_id\n        self.payload = payload\n\ndef parse_frame(data):\n\
          \    if len(data) < 9:\n        return None, data  # Need more data\n    \n    length = int.from_bytes(data[0:3],\
          \ 'big')\n    type_ = data[3]\n    flags = data[4]\n    stream_id = int.from_bytes(data[5:9], 'big') & 0x7FFFFFFF\n\
          \    \n    if len(data) < 9 + length:\n        return None, data  # Need more data\n    \n    payload = data[9:9+length]\n\
          \    remaining = data[9+length:]\n    \n    return Frame(type_, flags, stream_id, payload), remaining\n\ndef encode_frame(frame):\n\
          \    header = (\n        frame.length.to_bytes(3, 'big') +\n        bytes([frame.type, frame.flags]) +\n       \
          \ (frame.stream_id & 0x7FFFFFFF).to_bytes(4, 'big')\n    )\n    return header + frame.payload\n\n# Frame types\n\
          DATA = 0x0\nHEADERS = 0x1\nPRIORITY = 0x2\nRST_STREAM = 0x3\nSETTINGS = 0x4\nPUSH_PROMISE = 0x5\nPING = 0x6\nGOAWAY\
          \ = 0x7\nWINDOW_UPDATE = 0x8\nCONTINUATION = 0x9"
      pitfalls:
      - Endianness issues
      - Frame length limits
      - Reserved bit handling
      concepts:
      - Binary protocols
      - Frame-based messaging
      - Protocol parsing
      estimated_hours: 5-8
    - id: 2
      name: HPACK Compression
      description: Implement header compression with static/dynamic tables.
      acceptance_criteria:
      - Static table lookup
      - Dynamic table management
      - Huffman decoding
      - Integer encoding
      hints:
        level1: HPACK uses indexed references + Huffman. Static table has 61 entries.
        level2: Dynamic table is FIFO with size limit. Integers use 5/6/7-bit prefixes.
        level3: "STATIC_TABLE = [\n    (None, None),  # Index 0 unused\n    (':authority', ''),\n    (':method', 'GET'),\n\
          \    (':method', 'POST'),\n    (':path', '/'),\n    (':path', '/index.html'),\n    (':scheme', 'http'),\n    (':scheme',\
          \ 'https'),\n    (':status', '200'),\n    # ... 61 entries total\n]\n\nclass HPACKDecoder:\n    def __init__(self,\
          \ max_size=4096):\n        self.dynamic_table = []\n        self.max_size = max_size\n        self.size = 0\n  \
          \  \n    def decode(self, data):\n        headers = []\n        i = 0\n        \n        while i < len(data):\n\
          \            byte = data[i]\n            \n            if byte & 0x80:  # Indexed header (7-bit index)\n       \
          \         idx, i = self.decode_int(data, i, 7)\n                name, value = self.get_indexed(idx)\n          \
          \      headers.append((name, value))\n            \n            elif byte & 0x40:  # Literal with indexing\n   \
          \             idx, i = self.decode_int(data, i, 6)\n                if idx > 0:\n                    name, _ = self.get_indexed(idx)\n\
          \                else:\n                    name, i = self.decode_string(data, i)\n                value, i = self.decode_string(data,\
          \ i)\n                self.add_to_dynamic(name, value)\n                headers.append((name, value))\n        \
          \    \n            # ... handle other cases\n        \n        return headers\n    \n    def get_indexed(self, idx):\n\
          \        if idx <= 61:\n            return STATIC_TABLE[idx]\n        return self.dynamic_table[idx - 62]"
      pitfalls:
      - Table size eviction
      - Index off-by-one errors
      - Huffman boundary handling
      concepts:
      - Header compression
      - Table-based encoding
      - Huffman coding
      estimated_hours: 8-12
    - id: 3
      name: Stream Management
      description: Handle multiplexed streams with proper state machines.
      acceptance_criteria:
      - Stream states (idle, open, closed, etc.)
      - Stream ID allocation
      - Priority handling
      - Concurrent stream limits
      hints:
        level1: Client streams are odd, server (push) are even. Each has 5 states.
        level2: HEADERS opens stream, END_STREAM half-closes. Track dependencies for priority.
        level3: "from enum import Enum\n\nclass StreamState(Enum):\n    IDLE = 'idle'\n    RESERVED_LOCAL = 'reserved_local'\n\
          \    RESERVED_REMOTE = 'reserved_remote'\n    OPEN = 'open'\n    HALF_CLOSED_LOCAL = 'half_closed_local'\n    HALF_CLOSED_REMOTE\
          \ = 'half_closed_remote'\n    CLOSED = 'closed'\n\nclass Stream:\n    def __init__(self, stream_id):\n        self.id\
          \ = stream_id\n        self.state = StreamState.IDLE\n        self.window = 65535  # Initial flow control window\n\
          \        self.request_headers = None\n        self.request_body = b''\n        self.response_headers = None\n  \
          \      self.response_body = b''\n    \n    def recv_headers(self, end_stream):\n        if self.state == StreamState.IDLE:\n\
          \            self.state = StreamState.OPEN\n        \n        if end_stream:\n            if self.state == StreamState.OPEN:\n\
          \                self.state = StreamState.HALF_CLOSED_REMOTE\n    \n    def send_headers(self, end_stream):\n  \
          \      if end_stream:\n            if self.state == StreamState.OPEN:\n                self.state = StreamState.HALF_CLOSED_LOCAL\n\
          \            elif self.state == StreamState.HALF_CLOSED_REMOTE:\n                self.state = StreamState.CLOSED\n\
          \nclass Connection:\n    def __init__(self):\n        self.streams = {}\n        self.next_server_stream_id = 2\n\
          \        self.settings = {...}\n    \n    def get_or_create_stream(self, stream_id):\n        if stream_id not in\
          \ self.streams:\n            self.streams[stream_id] = Stream(stream_id)\n        return self.streams[stream_id]"
      pitfalls:
      - State machine violations
      - Stream ID exhaustion
      - Dependency cycles
      concepts:
      - Multiplexing
      - State machines
      - Concurrency
      estimated_hours: 8-12
    - id: 4
      name: Flow Control
      description: Implement connection and stream-level flow control.
      acceptance_criteria:
      - Window sizes
      - WINDOW_UPDATE frames
      - Connection-level windows
      - Backpressure handling
      hints:
        level1: Each stream AND connection has a window. Both must have space to send.
        level2: WINDOW_UPDATE increments window. Sender blocks when window hits 0.
        level3: "class FlowController:\n    def __init__(self, initial_window=65535):\n        self.connection_window = initial_window\n\
          \        self.stream_windows = {}  # stream_id -> window\n        self.pending_data = {}  # stream_id -> [(data,\
          \ callback)]\n    \n    def can_send(self, stream_id, size):\n        stream_window = self.stream_windows.get(stream_id,\
          \ 65535)\n        return self.connection_window >= size and stream_window >= size\n    \n    def send_data(self,\
          \ stream_id, data):\n        size = len(data)\n        if not self.can_send(stream_id, size):\n            # Queue\
          \ for later\n            if stream_id not in self.pending_data:\n                self.pending_data[stream_id] =\
          \ []\n            self.pending_data[stream_id].append(data)\n            return False\n        \n        # Decrement\
          \ windows\n        self.connection_window -= size\n        self.stream_windows[stream_id] -= size\n        return\
          \ True\n    \n    def recv_window_update(self, stream_id, increment):\n        if stream_id == 0:\n            self.connection_window\
          \ += increment\n        else:\n            self.stream_windows[stream_id] = \\\n                self.stream_windows.get(stream_id,\
          \ 65535) + increment\n        \n        # Check if we can now send pending data\n        self.flush_pending()\n\
          \    \n    def send_window_update(self, stream_id, increment):\n        frame = Frame(\n            type=WINDOW_UPDATE,\n\
          \            flags=0,\n            stream_id=stream_id,\n            payload=increment.to_bytes(4, 'big')\n    \
          \    )\n        return encode_frame(frame)"
      pitfalls:
      - Window underflow/overflow
      - Deadlocks from exhausted windows
      - Forgetting connection window
      concepts:
      - Flow control
      - Backpressure
      - Resource management
      estimated_hours: 9-18
  https-client:
    id: https-client
    name: HTTPS Client
    description: Build an HTTPS client that performs TLS handshake. Learn certificate validation and encrypted communication.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - TCP sockets
    - Cryptography basics
    - X.509 certificates
    languages:
      recommended:
      - Python
      - Go
      - Rust
      also_possible:
      - C
      - Java
    resources:
    - name: TLS 1.3 RFC 8446
      url: https://datatracker.ietf.org/doc/html/rfc8446
      type: specification
    - name: Illustrated TLS 1.3
      url: https://tls13.xargs.org/
      type: tutorial
    milestones:
    - id: 1
      name: TCP Socket & Record Layer
      description: Establish TCP connection and implement TLS record layer.
      acceptance_criteria:
      - TCP connect to port 443
      - TLS record format (type, version, length, data)
      - Fragment handling
      - Record layer parsing
      hints:
        level1: 'TLS record: 1 byte type, 2 bytes version, 2 bytes length, then payload.'
        level2: 'Content types: 20=ChangeCipherSpec, 21=Alert, 22=Handshake, 23=Application.'
        level3: "import socket\nimport struct\n\nclass TLSRecord:\n    CHANGE_CIPHER_SPEC = 20\n    ALERT = 21\n    HANDSHAKE\
          \ = 22\n    APPLICATION_DATA = 23\n\nclass TLSConnection:\n    def __init__(self, host, port=443):\n        self.sock\
          \ = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.connect((host, port))\n        self.host\
          \ = host\n    \n    def send_record(self, content_type, data):\n        # TLS 1.2 version for compatibility (actual\
          \ version in handshake)\n        version = (3, 3)\n        header = struct.pack('!BHH', content_type, (version[0]\
          \ << 8) | version[1], len(data))\n        self.sock.sendall(header + data)\n    \n    def recv_record(self):\n \
          \       header = self.sock.recv(5)\n        if len(header) < 5:\n            raise ConnectionError('Connection closed')\n\
          \        \n        content_type, version, length = struct.unpack('!BHH', header)\n        data = b''\n        while\
          \ len(data) < length:\n            chunk = self.sock.recv(length - len(data))\n            if not chunk:\n     \
          \           raise ConnectionError('Connection closed')\n            data += chunk\n        \n        return content_type,\
          \ data"
      pitfalls:
      - Endianness
      - Partial reads
      - Record fragmentation
      concepts:
      - TLS records
      - Network protocols
      - Binary parsing
      estimated_hours: 3-4
    - id: 2
      name: ClientHello
      description: Send ClientHello with supported cipher suites.
      acceptance_criteria:
      - Random bytes generation
      - Session ID
      - Cipher suite list
      - SNI extension for hostname
      hints:
        level1: ClientHello starts handshake. Include TLS version, random, extensions.
        level2: SNI (Server Name Indication) tells server which hostname you want.
        level3: "import os\n\ndef build_client_hello(hostname):\n    # Random (32 bytes)\n    client_random = os.urandom(32)\n\
          \    \n    # Session ID (empty for new connection)\n    session_id = b''\n    \n    # Cipher suites (TLS 1.3)\n\
          \    cipher_suites = struct.pack('!H', 2)  # Length\n    cipher_suites += struct.pack('!H', 0x1301)  # TLS_AES_128_GCM_SHA256\n\
          \    \n    # Compression methods (null only)\n    compression = struct.pack('!B', 1) + struct.pack('!B', 0)\n  \
          \  \n    # Extensions\n    extensions = build_sni_extension(hostname)\n    extensions += build_supported_versions_extension()\n\
          \    extensions += build_key_share_extension()\n    \n    # Build handshake message\n    body = struct.pack('!H',\
          \ 0x0303)  # Legacy version TLS 1.2\n    body += client_random\n    body += struct.pack('!B', len(session_id)) +\
          \ session_id\n    body += cipher_suites\n    body += compression\n    body += struct.pack('!H', len(extensions))\
          \ + extensions\n    \n    # Handshake header: type (1) + length (3)\n    handshake = struct.pack('!B', 1)  # ClientHello\n\
          \    handshake += struct.pack('!I', len(body))[1:]  # 3-byte length\n    handshake += body\n    \n    return handshake,\
          \ client_random\n\ndef build_sni_extension(hostname):\n    hostname_bytes = hostname.encode()\n    # SNI list entry\n\
          \    entry = struct.pack('!B', 0)  # DNS hostname type\n    entry += struct.pack('!H', len(hostname_bytes))\n  \
          \  entry += hostname_bytes\n    # SNI list\n    sni_list = struct.pack('!H', len(entry)) + entry\n    # Extension\n\
          \    ext = struct.pack('!H', 0)  # SNI extension type\n    ext += struct.pack('!H', len(sni_list)) + sni_list\n\
          \    return ext"
      pitfalls:
      - Extension ordering
      - Length field sizes
      - Version negotiation
      concepts:
      - TLS handshake
      - Cipher negotiation
      - Extensions
      estimated_hours: 4-6
    - id: 3
      name: Key Exchange
      description: Perform ECDHE key exchange and derive keys.
      acceptance_criteria:
      - Generate ephemeral key pair
      - Parse ServerHello and key share
      - ECDH shared secret computation
      - Key derivation (HKDF)
      hints:
        level1: 'ECDHE: both sides generate key pairs, share public keys, compute shared secret.'
        level2: TLS 1.3 uses HKDF for key derivation from shared secret.
        level3: "from cryptography.hazmat.primitives.asymmetric import x25519\nfrom cryptography.hazmat.primitives import\
          \ hashes\nfrom cryptography.hazmat.primitives.kdf.hkdf import HKDFExpand, HKDF\nimport hashlib\n\nclass KeyExchange:\n\
          \    def __init__(self):\n        self.private_key = x25519.X25519PrivateKey.generate()\n        self.public_key\
          \ = self.private_key.public_key()\n    \n    def get_public_bytes(self):\n        return self.public_key.public_bytes_raw()\n\
          \    \n    def compute_shared_secret(self, peer_public_bytes):\n        peer_public = x25519.X25519PublicKey.from_public_bytes(peer_public_bytes)\n\
          \        return self.private_key.exchange(peer_public)\n\ndef derive_keys(shared_secret, client_hello_hash, server_hello_hash):\n\
          \    # Early secret\n    early_secret = hkdf_extract(b'\\x00' * 32, b'\\x00' * 32)\n    \n    # Handshake secret\n\
          \    derived = hkdf_expand_label(early_secret, b'derived', b'', 32)\n    handshake_secret = hkdf_extract(derived,\
          \ shared_secret)\n    \n    # Traffic secrets\n    messages_hash = hashlib.sha256(client_hello_hash + server_hello_hash).digest()\n\
          \    \n    client_handshake_traffic_secret = hkdf_expand_label(\n        handshake_secret, b'c hs traffic', messages_hash,\
          \ 32\n    )\n    server_handshake_traffic_secret = hkdf_expand_label(\n        handshake_secret, b's hs traffic',\
          \ messages_hash, 32\n    )\n    \n    # Derive keys and IVs\n    client_key = hkdf_expand_label(client_handshake_traffic_secret,\
          \ b'key', b'', 16)\n    client_iv = hkdf_expand_label(client_handshake_traffic_secret, b'iv', b'', 12)\n    \n \
          \   return client_key, client_iv"
      pitfalls:
      - Curve negotiation
      - Key derivation order
      - Hash transcript
      concepts:
      - ECDHE
      - Key derivation
      - Forward secrecy
      estimated_hours: 6-8
    - id: 4
      name: Encrypted Communication
      description: Encrypt/decrypt application data with derived keys.
      acceptance_criteria:
      - AEAD encryption (AES-GCM)
      - Sequence number handling
      - Finish message verification
      - Send/receive application data
      hints:
        level1: TLS 1.3 uses AEAD. Nonce = IV XOR sequence number.
        level2: Record header is additional authenticated data (AAD).
        level3: "from cryptography.hazmat.primitives.ciphers.aead import AESGCM\n\nclass TLS13Cipher:\n    def __init__(self,\
          \ key, iv):\n        self.aesgcm = AESGCM(key)\n        self.iv = iv\n        self.seq_num = 0\n    \n    def _compute_nonce(self):\n\
          \        # XOR IV with sequence number (padded to IV length)\n        seq_bytes = self.seq_num.to_bytes(len(self.iv),\
          \ 'big')\n        nonce = bytes(a ^ b for a, b in zip(self.iv, seq_bytes))\n        self.seq_num += 1\n        return\
          \ nonce\n    \n    def encrypt(self, plaintext, content_type):\n        # Add content type byte (inner plaintext)\n\
          \        inner = plaintext + bytes([content_type])\n        \n        # Build AAD (record header with encrypted\
          \ content type)\n        aad = struct.pack('!BHH', 23, 0x0303, len(inner) + 16)  # +16 for auth tag\n        \n\
          \        nonce = self._compute_nonce()\n        ciphertext = self.aesgcm.encrypt(nonce, inner, aad)\n        \n\
          \        return aad + ciphertext\n    \n    def decrypt(self, record_data):\n        # AAD is the record header\n\
          \        aad = record_data[:5]\n        ciphertext = record_data[5:]\n        \n        nonce = self._compute_nonce()\n\
          \        plaintext = self.aesgcm.decrypt(nonce, ciphertext, aad)\n        \n        # Last byte is real content\
          \ type\n        content_type = plaintext[-1]\n        data = plaintext[:-1].rstrip(b'\\x00')  # Remove padding\n\
          \        \n        return content_type, data"
      pitfalls:
      - Nonce reuse
      - Padding removal
      - Alert handling
      concepts:
      - AEAD encryption
      - Sequence numbers
      - TLS records
      estimated_hours: 6-8
  integration-testing:
    id: integration-testing
    name: Integration Testing Suite
    description: Build comprehensive integration tests. Learn to test components working together with real dependencies.
    difficulty: advanced
    estimated_hours: 12-20
    prerequisites:
    - Unit testing
    - Docker basics
    - Database knowledge
    languages:
      recommended:
      - Python
      - JavaScript
      - Java
      also_possible:
      - Go
      - Ruby
    resources:
    - name: Testcontainers
      url: https://www.testcontainers.org/
      type: documentation
    milestones:
    - id: 1
      name: Test Database Setup
      description: Set up isolated database for testing.
      acceptance_criteria:
      - Spin up test database (Docker)
      - Migrations run before tests
      - Clean state between tests
      - Tear down after test suite
      hints:
        level1: Use Docker Compose or Testcontainers.
        level2: Transaction rollback for fast cleanup.
        level3: "import pytest\nimport docker\n\n@pytest.fixture(scope='session')\ndef postgres_container():\n    client =\
          \ docker.from_env()\n    container = client.containers.run(\n        'postgres:15',\n        environment={'POSTGRES_PASSWORD':\
          \ 'test'},\n        ports={'5432/tcp': None},\n        detach=True\n    )\n    # Wait for ready\n    import time\n\
          \    time.sleep(3)\n    port = container.ports['5432/tcp'][0]['HostPort']\n    yield f'postgresql://postgres:test@localhost:{port}/postgres'\n\
          \    container.stop()\n    container.remove()\n\n@pytest.fixture\ndef db_session(postgres_container):\n    engine\
          \ = create_engine(postgres_container)\n    # Run migrations\n    Base.metadata.create_all(engine)\n    session =\
          \ Session(engine)\n    yield session\n    session.rollback()  # Clean up\n    session.close()"
      pitfalls:
      - Port conflicts
      - Container startup time
      - Data leaking between tests
      concepts:
      - Test isolation
      - Containers in testing
      - Database fixtures
      estimated_hours: 3-4
    - id: 2
      name: API Integration Tests
      description: Test API endpoints with real HTTP requests.
      acceptance_criteria:
      - Start test server
      - Make real HTTP requests
      - Verify response codes and bodies
      - Test authentication flow
      hints:
        level1: Use requests library or test client.
        level2: Fixtures for authenticated users.
        level3: "import pytest\nfrom fastapi.testclient import TestClient\nfrom app import app, get_db\n\n@pytest.fixture\n\
          def client(db_session):\n    def override_get_db():\n        yield db_session\n    app.dependency_overrides[get_db]\
          \ = override_get_db\n    yield TestClient(app)\n    app.dependency_overrides.clear()\n\n@pytest.fixture\ndef auth_client(client,\
          \ db_session):\n    # Create test user\n    user = User(email='test@example.com')\n    user.set_password('password')\n\
          \    db_session.add(user)\n    db_session.commit()\n    \n    # Login and get token\n    response = client.post('/auth/login',\
          \ json={\n        'email': 'test@example.com',\n        'password': 'password'\n    })\n    token = response.json()['access_token']\n\
          \    client.headers['Authorization'] = f'Bearer {token}'\n    return client\n\ndef test_create_post(auth_client):\n\
          \    response = auth_client.post('/posts', json={'title': 'Test', 'content': 'Content'})\n    assert response.status_code\
          \ == 201\n    assert response.json()['title'] == 'Test'"
      pitfalls:
      - Test order dependencies
      - Shared state
      - Slow tests
      concepts:
      - API testing
      - Test clients
      - Authentication in tests
      estimated_hours: 4-5
    - id: 3
      name: External Service Mocking
      description: Mock external APIs while testing real internal integration.
      acceptance_criteria:
      - Mock HTTP responses for external APIs
      - Verify external API was called correctly
      - Simulate error responses
      - Test retry logic
      hints:
        level1: Use responses (Python) or nock (Node.js) to mock HTTP.
        level2: Record real responses for realistic mocks.
        level3: "import responses\n\n@responses.activate\ndef test_payment_integration(auth_client):\n    # Mock Stripe API\n\
          \    responses.add(\n        responses.POST,\n        'https://api.stripe.com/v1/charges',\n        json={'id':\
          \ 'ch_123', 'status': 'succeeded'},\n        status=200\n    )\n    \n    # Make request that triggers Stripe call\n\
          \    response = auth_client.post('/orders/123/pay', json={\n        'card_token': 'tok_visa'\n    })\n    \n   \
          \ assert response.status_code == 200\n    assert response.json()['payment_status'] == 'paid'\n    \n    # Verify\
          \ Stripe was called correctly\n    assert len(responses.calls) == 1\n    assert 'amount' in responses.calls[0].request.body\n\
          \n@responses.activate\ndef test_payment_failure_handling(auth_client):\n    # Mock Stripe failure\n    responses.add(\n\
          \        responses.POST,\n        'https://api.stripe.com/v1/charges',\n        json={'error': {'message': 'Card\
          \ declined'}},\n        status=402\n    )\n    \n    response = auth_client.post('/orders/123/pay', json={'card_token':\
          \ 'tok_bad'})\n    assert response.status_code == 400\n    assert 'declined' in response.json()['error']"
      pitfalls:
      - Missing mock causes real API call
      - Mock doesn't match real API
      - Order of mock setup
      concepts:
      - HTTP mocking
      - Service virtualization
      - Error simulation
      estimated_hours: 3-4
  json-db:
    id: json-db
    name: JSON File Database
    description: Build a simple file-based database using JSON. Learn data persistence, CRUD operations, and basic query patterns.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - JSON knowledge
    - File I/O
    - Basic programming
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - Ruby
      - PHP
    resources:
    - name: JSON Specification
      url: https://www.json.org
      type: documentation
    milestones:
    - id: 1
      name: Basic CRUD Operations
      description: Implement Create, Read, Update, Delete.
      acceptance_criteria:
      - Create/insert new records
      - Read by ID
      - Update existing records
      - Delete records
      - Persist to JSON file
      hints:
        level1: Load entire file into memory, modify, write back.
        level2: Use UUIDs or auto-increment for IDs.
        level3: "import json\nimport uuid\n\nclass JsonDB:\n    def __init__(self, filepath):\n        self.filepath = filepath\n\
          \        self.data = self._load()\n    \n    def _load(self):\n        try:\n            with open(self.filepath,\
          \ 'r') as f:\n                return json.load(f)\n        except FileNotFoundError:\n            return {'records':\
          \ {}}\n    \n    def _save(self):\n        with open(self.filepath, 'w') as f:\n            json.dump(self.data,\
          \ f, indent=2)\n    \n    def create(self, record):\n        id = str(uuid.uuid4())\n        record['id'] = id\n\
          \        self.data['records'][id] = record\n        self._save()\n        return id\n    \n    def read(self, id):\n\
          \        return self.data['records'].get(id)\n    \n    def update(self, id, updates):\n        if id in self.data['records']:\n\
          \            self.data['records'][id].update(updates)\n            self._save()\n            return True\n     \
          \   return False\n    \n    def delete(self, id):\n        if id in self.data['records']:\n            del self.data['records'][id]\n\
          \            self._save()\n            return True\n        return False"
      pitfalls:
      - Race conditions with concurrent access
      - Data loss on crash during write
      - Memory issues with large files
      concepts:
      - CRUD operations
      - Data persistence
      - File-based storage
      estimated_hours: 2-3
    - id: 2
      name: Querying
      description: Add simple query capabilities.
      acceptance_criteria:
      - Find all matching a condition
      - Support multiple conditions (AND)
      - Sort results
      - Limit and offset
      hints:
        level1: Filter records by iterating and checking conditions.
        level2: Use lambda functions or dicts for conditions.
        level3: "def query(self, conditions=None, sort_by=None, limit=None, offset=0):\n    results = list(self.data['records'].values())\n\
          \    \n    if conditions:\n        for key, value in conditions.items():\n            results = [r for r in results\
          \ if r.get(key) == value]\n    \n    if sort_by:\n        reverse = sort_by.startswith('-')\n        key = sort_by.lstrip('-')\n\
          \        results.sort(key=lambda r: r.get(key, ''), reverse=reverse)\n    \n    if offset:\n        results = results[offset:]\n\
          \    if limit:\n        results = results[:limit]\n    \n    return results"
      pitfalls:
      - Inefficient full scans
      - Sorting nulls/missing fields
      - Type comparison issues
      concepts:
      - Query filtering
      - Sorting
      - Pagination
      estimated_hours: 2-3
    - id: 3
      name: Collections & Indexing
      description: Support multiple collections and basic indexes.
      acceptance_criteria:
      - Multiple named collections
      - Create index on field
      - Use index for queries
      - Maintain index on writes
      hints:
        level1: Store collections as top-level keys.
        level2: Index is a map from field value to list of record IDs.
        level3: "class Collection:\n    def __init__(self, name, db):\n        self.name = name\n        self.db = db\n  \
          \      self.indexes = {}  # field -> {value -> [ids]}\n    \n    def create_index(self, field):\n        index =\
          \ {}\n        for id, record in self.db.data[self.name].items():\n            value = record.get(field)\n      \
          \      if value not in index:\n                index[value] = []\n            index[value].append(id)\n        self.indexes[field]\
          \ = index\n    \n    def query(self, conditions):\n        # Try to use index\n        for field, value in conditions.items():\n\
          \            if field in self.indexes:\n                ids = self.indexes[field].get(value, [])\n             \
          \   results = [self.db.data[self.name][id] for id in ids]\n                # Filter by remaining conditions\n  \
          \              return [r for r in results if all(r.get(k) == v for k, v in conditions.items())]\n        # Fall\
          \ back to full scan\n        return [r for r in self.db.data[self.name].values() if all(r.get(k) == v for k, v in\
          \ conditions.items())]"
      pitfalls:
      - Index staleness
      - Memory overhead
      - Index not used
      concepts:
      - Indexing
      - Query optimization
      - Collection design
      estimated_hours: 3-4
  json-parser:
    id: json-parser
    name: JSON Parser
    description: Build a JSON parser using recursive descent parsing. Learn about tokenization and AST construction.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - Basic programming
    - Understanding of JSON format
    languages:
      recommended:
      - Python
      - JavaScript
      - C
      also_possible:
      - Rust
      - Go
    resources:
    - name: JSON Specification
      url: https://www.json.org/json-en.html
      type: specification
    - name: Crafting Interpreters
      url: https://craftinginterpreters.com/
      type: book
    milestones:
    - id: 1
      name: Tokenizer
      description: Build a lexer that converts JSON string into tokens.
      acceptance_criteria:
      - Tokenize strings, numbers, booleans, null
      - Tokenize punctuation ({, }, [, ], :, ,)
      - Skip whitespace
      - Handle escape sequences in strings
      hints:
        level1: Read char by char, emit tokens based on current char.
        level2: 'Handle escape sequences in strings: \n, \t, \\, etc.'
        level3: "class Tokenizer:\n    def __init__(self, text):\n        self.text = text\n        self.pos = 0\n\n    def\
          \ tokenize(self):\n        tokens = []\n        while self.pos < len(self.text):\n            char = self.text[self.pos]\n\
          \            if char.isspace():\n                self.pos += 1\n            elif char == '\"':\n               \
          \ tokens.append(self.read_string())\n            elif char.isdigit() or char == '-':\n                tokens.append(self.read_number())\n\
          \            elif char in '{}[]:,':\n                tokens.append(('PUNCT', char))\n                self.pos +=\
          \ 1\n            elif self.text[self.pos:self.pos+4] == 'true':\n                tokens.append(('BOOL', True))\n\
          \                self.pos += 4\n        return tokens"
      pitfalls:
      - Escape sequences in strings
      - Negative numbers
      - Scientific notation
      concepts:
      - Lexical analysis
      - Token types
      - State machine
      estimated_hours: 2-3
    - id: 2
      name: Parser
      description: Parse tokens into native data structures.
      acceptance_criteria:
      - Parse objects and arrays
      - Handle nested structures
      - Return native data types
      - Handle syntax errors gracefully
      hints:
        level1: 'Recursive descent: parse_value calls parse_object/parse_array.'
        level2: 'Object: {key: value, ...}. Array: [value, ...].'
        level3: "class Parser:\n    def parse_value(self):\n        token = self.current()\n        if token[0] == 'PUNCT'\
          \ and token[1] == '{':\n            return self.parse_object()\n        elif token[0] == 'PUNCT' and token[1] ==\
          \ '[':\n            return self.parse_array()\n        elif token[0] == 'STRING':\n            self.advance()\n\
          \            return token[1]\n        elif token[0] == 'NUMBER':\n            self.advance()\n            return\
          \ token[1]\n        elif token[0] == 'BOOL':\n            self.advance()\n            return token[1]\n\n    def\
          \ parse_object(self):\n        self.expect('{')\n        obj = {}\n        while self.current()[1] != '}':\n   \
          \         key = self.expect_string()\n            self.expect(':')\n            value = self.parse_value()\n   \
          \         obj[key] = value\n            if self.current()[1] == ',':\n                self.advance()\n        self.expect('}')\n\
          \        return obj"
      pitfalls:
      - Trailing comma handling
      - Deep nesting stack overflow
      - Empty object/array edge case
      concepts:
      - Recursive descent
      - Grammar rules
      - AST construction
      estimated_hours: 3-4
    - id: 3
      name: Error Handling & Edge Cases
      description: Add robust error handling and edge case support.
      acceptance_criteria:
      - Meaningful error messages with position
      - Handle all JSON types correctly
      - Validate number format
      - Unicode string support
      hints:
        level1: Track line and column for error messages.
        level2: 'Number format: optional minus, integer or decimal, optional exponent.'
        level3: "class JSONError(Exception):\n    def __init__(self, message, line, col):\n        super().__init__(f'{message}\
          \ at line {line}, column {col}')\n        self.line = line\n        self.col = col\n\ndef read_number(self):\n \
          \   start = self.pos\n    if self.current_char() == '-':\n        self.advance()\n    if self.current_char() ==\
          \ '0':\n        self.advance()\n    else:\n        while self.current_char().isdigit():\n            self.advance()\n\
          \    if self.current_char() == '.':\n        self.advance()\n        while self.current_char().isdigit():\n    \
          \        self.advance()\n    if self.current_char() in 'eE':\n        self.advance()\n        if self.current_char()\
          \ in '+-':\n            self.advance()\n        while self.current_char().isdigit():\n            self.advance()\n\
          \    return float(self.text[start:self.pos])"
      pitfalls:
      - Numbers with leading zeros (invalid in JSON)
      - Unicode escapes \u0000
      - Duplicate keys in objects
      concepts:
      - Error recovery
      - JSON specification compliance
      - Unicode handling
      estimated_hours: 2-3
  jwt-impl:
    id: jwt-impl
    name: JWT Library
    description: Implement JSON Web Token signing and verification. Learn about authentication tokens and cryptographic signatures.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - JSON
    - Base64
    - HMAC basics
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - Java
      - Rust
    resources:
    - name: JWT Specification (RFC 7519)
      url: https://tools.ietf.org/html/rfc7519
      type: specification
    - name: JWT.io Debugger
      url: https://jwt.io/
      type: tool
    milestones:
    - id: 1
      name: JWT Structure
      description: Implement JWT encoding without signing.
      acceptance_criteria:
      - Create header with algorithm
      - Encode payload claims
      - Base64url encoding
      - Combine header.payload.signature
      hints:
        level1: JWT = base64url(header).base64url(payload).signature
        level2: 'Base64url: base64 with + -> -, / -> _, no padding.'
        level3: "import json\nimport base64\n\ndef base64url_encode(data: bytes) -> str:\n    return base64.urlsafe_b64encode(data).rstrip(b'=').decode('ascii')\n\
          \ndef base64url_decode(data: str) -> bytes:\n    padding = 4 - len(data) % 4\n    if padding != 4:\n        data\
          \ += '=' * padding\n    return base64.urlsafe_b64decode(data)\n\ndef encode_jwt(payload: dict, secret: str, algorithm='HS256')\
          \ -> str:\n    header = {'alg': algorithm, 'typ': 'JWT'}\n    header_b64 = base64url_encode(json.dumps(header).encode())\n\
          \    payload_b64 = base64url_encode(json.dumps(payload).encode())\n    \n    message = f'{header_b64}.{payload_b64}'\n\
          \    signature = sign(message, secret, algorithm)\n    signature_b64 = base64url_encode(signature)\n    \n    return\
          \ f'{message}.{signature_b64}'"
      pitfalls:
      - Regular base64 vs base64url
      - Padding removal/addition
      - JSON key ordering
      concepts:
      - JWT structure
      - Base64url
      - Token format
      estimated_hours: 2-3
    - id: 2
      name: HMAC Signing
      description: Implement HS256 signing and verification.
      acceptance_criteria:
      - Sign with HMAC-SHA256
      - Verify signature matches
      - Constant-time comparison
      - Handle invalid tokens
      hints:
        level1: 'HMAC-SHA256: hmac.new(key, message, sha256).'
        level2: Compare signatures with constant-time function.
        level3: "import hmac\nimport hashlib\n\ndef sign(message: str, secret: str, algorithm: str) -> bytes:\n    if algorithm\
          \ != 'HS256':\n        raise ValueError(f'Unsupported algorithm: {algorithm}')\n    return hmac.new(\n        secret.encode(),\n\
          \        message.encode(),\n        hashlib.sha256\n    ).digest()\n\ndef verify_jwt(token: str, secret: str) ->\
          \ dict:\n    parts = token.split('.')\n    if len(parts) != 3:\n        raise ValueError('Invalid token format')\n\
          \    \n    header_b64, payload_b64, signature_b64 = parts\n    \n    # Decode header to get algorithm\n    header\
          \ = json.loads(base64url_decode(header_b64))\n    algorithm = header.get('alg')\n    \n    # Compute expected signature\n\
          \    message = f'{header_b64}.{payload_b64}'\n    expected_sig = sign(message, secret, algorithm)\n    actual_sig\
          \ = base64url_decode(signature_b64)\n    \n    # Constant-time comparison\n    if not hmac.compare_digest(expected_sig,\
          \ actual_sig):\n        raise ValueError('Invalid signature')\n    \n    return json.loads(base64url_decode(payload_b64))"
      pitfalls:
      - Timing attacks in comparison
      - Algorithm confusion attacks
      - Key encoding
      concepts:
      - HMAC
      - Digital signatures
      - Token verification
      estimated_hours: 2-3
    - id: 3
      name: Claims Validation
      description: Implement standard JWT claim validation.
      acceptance_criteria:
      - 'exp: token expiration'
      - 'nbf: not before'
      - 'iat: issued at'
      - 'iss: issuer'
      - 'aud: audience'
      hints:
        level1: exp and nbf are Unix timestamps.
        level2: Add clock skew tolerance for exp/nbf.
        level3: "import time\n\ndef validate_claims(payload: dict, audience: str = None, issuer: str = None, clock_skew: int\
          \ = 60):\n    now = time.time()\n    \n    # Expiration\n    if 'exp' in payload:\n        if now > payload['exp']\
          \ + clock_skew:\n            raise ValueError('Token expired')\n    \n    # Not Before\n    if 'nbf' in payload:\n\
          \        if now < payload['nbf'] - clock_skew:\n            raise ValueError('Token not yet valid')\n    \n    #\
          \ Issuer\n    if issuer and payload.get('iss') != issuer:\n        raise ValueError(f'Invalid issuer: {payload.get(\"\
          iss\")}')\n    \n    # Audience\n    if audience:\n        token_aud = payload.get('aud')\n        if isinstance(token_aud,\
          \ list):\n            if audience not in token_aud:\n                raise ValueError('Invalid audience')\n    \
          \    elif token_aud != audience:\n            raise ValueError('Invalid audience')\n\ndef decode_jwt(token: str,\
          \ secret: str, audience: str = None, issuer: str = None) -> dict:\n    payload = verify_jwt(token, secret)\n   \
          \ validate_claims(payload, audience, issuer)\n    return payload"
      pitfalls:
      - Clock skew handling
      - Audience as list
      - Missing required claims
      concepts:
      - JWT claims
      - Token validation
      - Time-based security
      estimated_hours: 2-3
  knn:
    id: knn
    name: KNN Classifier
    description: Implement K-Nearest Neighbors classification algorithm. Learn about distance metrics and non-parametric models.
    difficulty: beginner
    estimated_hours: 6-10
    prerequisites:
    - Basic Python/NumPy
    - Distance formulas
    languages:
      recommended:
      - Python
      also_possible:
      - JavaScript
      - Julia
    resources:
    - name: KNN Explained
      url: https://scikit-learn.org/stable/modules/neighbors.html
      type: documentation
    - name: KNN from Scratch
      url: https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/
      type: tutorial
    milestones:
    - id: 1
      name: Distance Calculation
      description: Implement distance metrics for KNN.
      acceptance_criteria:
      - Euclidean distance
      - Manhattan distance
      - Distance to all training points
      - Efficient computation
      hints:
        level1: 'Euclidean: sqrt(sum((a-b)Â²))'
        level2: 'Manhattan: sum(|a-b|)'
        level3: "def euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b) ** 2))\n\ndef manhattan_distance(a, b):\n\
          \    return np.sum(np.abs(a - b))\n\ndef compute_distances(X_train, x_test, metric='euclidean'):\n    distances\
          \ = []\n    for x_train in X_train:\n        if metric == 'euclidean':\n            d = euclidean_distance(x_train,\
          \ x_test)\n        else:\n            d = manhattan_distance(x_train, x_test)\n        distances.append(d)\n   \
          \ return np.array(distances)"
      pitfalls:
      - Negative under square root
      - Feature scaling differences
      - Integer vs float types
      concepts:
      - Distance metrics
      - Vector operations
      - NumPy arrays
      estimated_hours: 1-2
    - id: 2
      name: K-Nearest Neighbors Classification
      description: Implement the full KNN classifier.
      acceptance_criteria:
      - Find k nearest neighbors
      - Majority voting for classification
      - Predict on new data
      - Calculate accuracy
      hints:
        level1: Sort by distance, take first k, count labels.
        level2: Use Counter for majority voting.
        level3: "from collections import Counter\n\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n\n    def\
          \ fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def predict_one(self, x):\n      \
          \  distances = compute_distances(self.X_train, x)\n        k_indices = np.argsort(distances)[:self.k]\n        k_labels\
          \ = self.y_train[k_indices]\n        most_common = Counter(k_labels).most_common(1)\n        return most_common[0][0]\n\
          \n    def predict(self, X):\n        return [self.predict_one(x) for x in X]\n\n    def score(self, X, y):\n   \
          \     predictions = self.predict(X)\n        return np.mean(predictions == y)"
      pitfalls:
      - K larger than dataset
      - Ties in voting
      - Empty neighbors
      concepts:
      - Classification
      - Majority voting
      - Accuracy metric
      estimated_hours: 2-3
    - id: 3
      name: Improvements & Evaluation
      description: Add improvements and proper evaluation.
      acceptance_criteria:
      - Weighted voting by distance
      - Cross-validation
      - Confusion matrix
      - Finding optimal k
      hints:
        level1: Weight by 1/distance for closer neighbors.
        level2: Use k-fold cross-validation to find best k.
        level3: "def weighted_predict(self, x):\n    distances = compute_distances(self.X_train, x)\n    k_indices = np.argsort(distances)[:self.k]\n\
          \    k_distances = distances[k_indices]\n    k_labels = self.y_train[k_indices]\n    # Weighted voting\n    weights\
          \ = 1 / (k_distances + 1e-8)  # Avoid div by zero\n    label_weights = {}\n    for label, weight in zip(k_labels,\
          \ weights):\n        label_weights[label] = label_weights.get(label, 0) + weight\n    return max(label_weights,\
          \ key=label_weights.get)\n\ndef find_best_k(X, y, k_range=range(1, 21)):\n    best_k, best_score = 1, 0\n    for\
          \ k in k_range:\n        knn = KNN(k=k)\n        score = cross_val_score(knn, X, y)  # Implement k-fold CV\n   \
          \     if score > best_score:\n            best_k, best_score = k, score\n    return best_k"
      pitfalls:
      - Overfitting with small k
      - Underfitting with large k
      - Computational cost
      concepts:
      - Weighted voting
      - Cross-validation
      - Hyperparameter tuning
      estimated_hours: 2-3
  kv-memory:
    id: kv-memory
    name: In-Memory Key-Value Store
    description: Build an in-memory key-value store with a hash map. Learn hash table implementation and caching concepts.
    difficulty: beginner
    estimated_hours: 6-10
    prerequisites:
    - Data structures
    - Hash functions
    languages:
      recommended:
      - C
      - Go
      - Rust
      also_possible:
      - Python
      - Java
    resources:
    - name: Hash Table Wikipedia
      url: https://en.wikipedia.org/wiki/Hash_table
      type: article
    milestones:
    - id: 1
      name: Basic Hash Map
      description: Implement core get/set/delete operations.
      acceptance_criteria:
      - Set key-value pairs
      - Get value by key
      - Delete by key
      - Handle collisions with chaining
      - Dynamic resizing
      hints:
        level1: Hash key to bucket index, store in linked list.
        level2: Resize when load factor exceeds threshold.
        level3: "typedef struct Entry {\n    char *key;\n    void *value;\n    struct Entry *next;\n} Entry;\n\ntypedef struct\
          \ HashMap {\n    Entry **buckets;\n    size_t capacity;\n    size_t size;\n} HashMap;\n\nuint32_t hash(const char\
          \ *key) {\n    uint32_t h = 5381;\n    while (*key) h = ((h << 5) + h) + *key++;\n    return h;\n}\n\nvoid *hm_get(HashMap\
          \ *m, const char *key) {\n    uint32_t idx = hash(key) % m->capacity;\n    Entry *e = m->buckets[idx];\n    while\
          \ (e) {\n        if (strcmp(e->key, key) == 0) return e->value;\n        e = e->next;\n    }\n    return NULL;\n\
          }\n\nvoid hm_set(HashMap *m, const char *key, void *value) {\n    if ((float)m->size / m->capacity > 0.75) resize(m);\n\
          \    uint32_t idx = hash(key) % m->capacity;\n    // Check existing\n    Entry *e = m->buckets[idx];\n    while\
          \ (e) {\n        if (strcmp(e->key, key) == 0) { e->value = value; return; }\n        e = e->next;\n    }\n    //\
          \ Insert new\n    Entry *new = malloc(sizeof(Entry));\n    new->key = strdup(key);\n    new->value = value;\n  \
          \  new->next = m->buckets[idx];\n    m->buckets[idx] = new;\n    m->size++;\n}"
      pitfalls:
      - Hash function quality
      - Memory leaks
      - Resizing while iterating
      concepts:
      - Hash functions
      - Collision resolution
      - Load factor
      estimated_hours: 3-4
    - id: 2
      name: TTL and Expiration
      description: Add time-to-live for keys.
      acceptance_criteria:
      - Set with expiration time
      - Get returns null for expired
      - Background cleanup (optional)
      - TTL command returns remaining time
      hints:
        level1: Store expiration timestamp with each entry.
        level2: Check expiration on get (lazy deletion).
        level3: "typedef struct Entry {\n    char *key;\n    void *value;\n    time_t expires;  // 0 = no expiration\n   \
          \ struct Entry *next;\n} Entry;\n\nvoid hm_set_ex(HashMap *m, const char *key, void *value, int ttl_secs) {\n  \
          \  // ... set entry ...\n    entry->expires = ttl_secs > 0 ? time(NULL) + ttl_secs : 0;\n}\n\nvoid *hm_get(HashMap\
          \ *m, const char *key) {\n    Entry *e = find_entry(m, key);\n    if (!e) return NULL;\n    if (e->expires && time(NULL)\
          \ > e->expires) {\n        hm_delete(m, key);  // Lazy delete\n        return NULL;\n    }\n    return e->value;\n\
          }\n\nint hm_ttl(HashMap *m, const char *key) {\n    Entry *e = find_entry(m, key);\n    if (!e || !e->expires) return\
          \ -1;\n    int remaining = e->expires - time(NULL);\n    return remaining > 0 ? remaining : -2;  // -2 = expired\n\
          }"
      pitfalls:
      - Clock skew
      - Memory not freed for expired
      - Precision of time
      concepts:
      - TTL mechanics
      - Lazy vs eager deletion
      - Time handling
      estimated_hours: 2-3
    - id: 3
      name: LRU Eviction
      description: Implement least-recently-used eviction when memory limit is reached.
      acceptance_criteria:
      - Set maximum memory limit
      - Track access order for each key
      - Evict LRU key when limit exceeded on insert
      - Update access order on get (not just set)
      - 'Handle edge case: new item larger than limit'
      hints:
        level1: Use a doubly linked list with hash map for O(1) access and removal.
        level2: Move accessed nodes to front of list. Evict from tail.
        level3: "// LRU Cache with O(1) get/set\ntypedef struct LRUNode {\n    char *key;\n    void *value;\n    size_t size;\n\
          \    struct LRUNode *prev, *next;\n} LRUNode;\n\ntypedef struct LRUCache {\n    HashMap *map;           // key ->\
          \ LRUNode*\n    LRUNode *head, *tail;   // Doubly linked list\n    size_t capacity;        // Max bytes\n    size_t\
          \ used;            // Current bytes\n} LRUCache;\n\nvoid move_to_front(LRUCache *c, LRUNode *node) {\n    if (node\
          \ == c->head) return;\n\n    // Remove from current position\n    if (node->prev) node->prev->next = node->next;\n\
          \    if (node->next) node->next->prev = node->prev;\n    if (node == c->tail) c->tail = node->prev;\n\n    // Insert\
          \ at front\n    node->prev = NULL;\n    node->next = c->head;\n    if (c->head) c->head->prev = node;\n    c->head\
          \ = node;\n    if (!c->tail) c->tail = node;\n}\n\nvoid evict_lru(LRUCache *c) {\n    if (!c->tail) return;\n\n\
          \    LRUNode *victim = c->tail;\n    c->tail = victim->prev;\n    if (c->tail) c->tail->next = NULL;\n    else c->head\
          \ = NULL;\n\n    c->used -= victim->size;\n    hm_delete(c->map, victim->key);\n    free(victim->key);\n    free(victim->value);\n\
          \    free(victim);\n}\n\nvoid lru_set(LRUCache *c, const char *key, void *value, size_t size) {\n    // Evict until\
          \ we have space\n    while (c->used + size > c->capacity && c->tail) {\n        evict_lru(c);\n    }\n\n    // Check\
          \ if updating existing\n    LRUNode *existing = hm_get(c->map, key);\n    if (existing) {\n        c->used -= existing->size;\n\
          \        free(existing->value);\n        existing->value = value;\n        existing->size = size;\n        c->used\
          \ += size;\n        move_to_front(c, existing);\n        return;\n    }\n\n    // Insert new\n    LRUNode *node\
          \ = malloc(sizeof(LRUNode));\n    node->key = strdup(key);\n    node->value = value;\n    node->size = size;\n \
          \   hm_set(c->map, key, node);\n\n    // Add to front\n    node->prev = NULL;\n    node->next = c->head;\n    if\
          \ (c->head) c->head->prev = node;\n    c->head = node;\n    if (!c->tail) c->tail = node;\n\n    c->used += size;\n\
          }"
      pitfalls:
      - Forgetting to update LRU order on reads
      - Memory accounting errors (tracked vs actual)
      - Thread safety with concurrent access
      - Evicting during iteration
      concepts:
      - LRU cache algorithm
      - Doubly linked list
      - O(1) cache operations
      - Memory management
      estimated_hours: 3-4
    - id: 4
      name: Persistence (Snapshots)
      description: Add ability to save and restore cache state from disk.
      acceptance_criteria:
      - SAVE command writes snapshot to disk
      - LOAD command restores from snapshot
      - Binary format for efficiency
      - Handle partial writes (atomic save)
      - Background save (fork-based, optional)
      hints:
        level1: Write header with count, then key-value pairs with lengths.
        level2: Write to temp file, rename atomically. Use fixed-size header.
        level3: "// Simple binary snapshot format\n// Header: magic(4) + version(4) + count(8)\n// Entry: key_len(4) + key\
          \ + value_len(4) + value + expires(8)\n\nint save_snapshot(HashMap *m, const char *path) {\n    char tmp_path[256];\n\
          \    snprintf(tmp_path, sizeof(tmp_path), \"%s.tmp\", path);\n\n    FILE *f = fopen(tmp_path, \"wb\");\n    if (!f)\
          \ return -1;\n\n    // Header\n    uint32_t magic = 0x4B564D53;  // \"KVMS\"\n    uint32_t version = 1;\n    uint64_t\
          \ count = m->size;\n    fwrite(&magic, 4, 1, f);\n    fwrite(&version, 4, 1, f);\n    fwrite(&count, 8, 1, f);\n\
          \n    // Entries\n    for (size_t i = 0; i < m->capacity; i++) {\n        Entry *e = m->buckets[i];\n        while\
          \ (e) {\n            uint32_t key_len = strlen(e->key);\n            uint32_t val_len = e->value_size;\n\n     \
          \       fwrite(&key_len, 4, 1, f);\n            fwrite(e->key, 1, key_len, f);\n            fwrite(&val_len, 4,\
          \ 1, f);\n            fwrite(e->value, 1, val_len, f);\n            fwrite(&e->expires, 8, 1, f);\n\n          \
          \  e = e->next;\n        }\n    }\n\n    fclose(f);\n\n    // Atomic rename\n    if (rename(tmp_path, path) != 0)\
          \ {\n        unlink(tmp_path);\n        return -1;\n    }\n\n    return 0;\n}\n\nint load_snapshot(HashMap *m, const\
          \ char *path) {\n    FILE *f = fopen(path, \"rb\");\n    if (!f) return -1;\n\n    // Verify header\n    uint32_t\
          \ magic, version;\n    uint64_t count;\n    fread(&magic, 4, 1, f);\n    fread(&version, 4, 1, f);\n    fread(&count,\
          \ 8, 1, f);\n\n    if (magic != 0x4B564D53 || version != 1) {\n        fclose(f);\n        return -1;\n    }\n\n\
          \    // Read entries\n    for (uint64_t i = 0; i < count; i++) {\n        uint32_t key_len, val_len;\n        fread(&key_len,\
          \ 4, 1, f);\n        char *key = malloc(key_len + 1);\n        fread(key, 1, key_len, f);\n        key[key_len]\
          \ = '\\0';\n\n        fread(&val_len, 4, 1, f);\n        void *value = malloc(val_len);\n        fread(value, 1,\
          \ val_len, f);\n\n        time_t expires;\n        fread(&expires, 8, 1, f);\n\n        hm_set_with_expiry(m, key,\
          \ value, val_len, expires);\n        free(key);\n    }\n\n    fclose(f);\n    return 0;\n}"
      pitfalls:
      - Partial writes leaving corrupt file
      - Endianness issues across platforms
      - Large snapshots blocking main thread
      - Not handling expired keys on load
      concepts:
      - Binary file formats
      - Atomic file operations
      - Serialization
      - Copy-on-write (fork)
      estimated_hours: 3-4
  leader-election:
    id: leader-election
    name: Leader Election
    description: Implement leader election algorithms. Learn distributed coordination and failure handling.
    difficulty: intermediate
    estimated_hours: 12-20
    prerequisites:
    - Distributed systems basics
    - Network programming
    - Failure detection
    languages:
      recommended:
      - Go
      - Python
      - Java
      also_possible:
      - Rust
      - Erlang
    resources:
    - name: Bully Algorithm
      url: https://en.wikipedia.org/wiki/Bully_algorithm
      type: article
    - name: Ring Election
      url: https://www.cs.colostate.edu/~cs551/CourseNotes/Synchronization/LeijdElect.html
      type: article
    milestones:
    - id: 1
      name: Node Communication
      description: Set up inter-node messaging.
      acceptance_criteria:
      - Node identity (unique ID)
      - Point-to-point messaging
      - Broadcast to all nodes
      - Handle node failures
      hints:
        level1: Each node has unique ID and knows other nodes' addresses.
        level2: Use TCP or UDP for messaging. Handle connection failures.
        level3: "import socket\nimport threading\nimport json\nfrom typing import Dict, Callable\n\nclass Node:\n    def __init__(self,\
          \ node_id: int, port: int, peers: Dict[int, tuple]):\n        self.node_id = node_id\n        self.port = port\n\
          \        self.peers = peers  # {node_id: (host, port)}\n        self.leader_id = None\n        self.handlers: Dict[str,\
          \ Callable] = {}\n        self.running = True\n    \n    def send_to(self, target_id: int, msg_type: str, data:\
          \ dict) -> bool:\n        if target_id not in self.peers:\n            return False\n        \n        host, port\
          \ = self.peers[target_id]\n        message = json.dumps({'type': msg_type, 'from': self.node_id, **data})\n    \
          \    \n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.settimeout(2.0)\n\
          \            sock.connect((host, port))\n            sock.sendall(message.encode())\n            sock.close()\n\
          \            return True\n        except:\n            return False  # Node unreachable\n    \n    def broadcast(self,\
          \ msg_type: str, data: dict):\n        for peer_id in self.peers:\n            self.send_to(peer_id, msg_type, data)\n\
          \    \n    def start_server(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.setsockopt(socket.SOL_SOCKET,\
          \ socket.SO_REUSEADDR, 1)\n        sock.bind(('0.0.0.0', self.port))\n        sock.listen(10)\n        \n      \
          \  while self.running:\n            conn, addr = sock.accept()\n            data = conn.recv(4096).decode()\n  \
          \          msg = json.loads(data)\n            \n            if msg['type'] in self.handlers:\n                self.handlers[msg['type']](msg)\n\
          \            conn.close()"
      pitfalls:
      - Message ordering
      - Partial failures
      - Network partitions
      concepts:
      - Node identity
      - Message passing
      - Failure detection
      estimated_hours: 3-4
    - id: 2
      name: Bully Algorithm
      description: Implement the bully election algorithm.
      acceptance_criteria:
      - Higher ID wins election
      - ELECTION message to higher nodes
      - OK response stops election
      - COORDINATOR announcement
      hints:
        level1: 'On timeout/failure: send ELECTION to higher IDs. If no OK, become leader.'
        level2: When receiving ELECTION from lower ID, send OK and start own election.
        level3: "class BullyElection(Node):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\
          \        self.election_in_progress = False\n        self.handlers['ELECTION'] = self.on_election\n        self.handlers['OK']\
          \ = self.on_ok\n        self.handlers['COORDINATOR'] = self.on_coordinator\n    \n    def start_election(self):\n\
          \        self.election_in_progress = True\n        higher_nodes = [nid for nid in self.peers if nid > self.node_id]\n\
          \        \n        if not higher_nodes:\n            # I'm the highest, become leader\n            self.become_leader()\n\
          \            return\n        \n        # Send ELECTION to all higher nodes\n        got_ok = False\n        for\
          \ nid in higher_nodes:\n            if self.send_to(nid, 'ELECTION', {}):\n                got_ok = True\n     \
          \   \n        if not got_ok:\n            # No higher node responded, become leader\n            self.become_leader()\n\
          \        else:\n            # Wait for COORDINATOR or timeout\n            threading.Timer(5.0, self.check_election_timeout).start()\n\
          \    \n    def on_election(self, msg):\n        sender = msg['from']\n        if sender < self.node_id:\n      \
          \      # Send OK to stop their election\n            self.send_to(sender, 'OK', {})\n            # Start my own\
          \ election\n            self.start_election()\n    \n    def on_ok(self, msg):\n        # Someone higher is alive,\
          \ wait for coordinator\n        pass\n    \n    def on_coordinator(self, msg):\n        self.leader_id = msg['leader']\n\
          \        self.election_in_progress = False\n        print(f'Node {self.node_id}: New leader is {self.leader_id}')\n\
          \    \n    def become_leader(self):\n        self.leader_id = self.node_id\n        self.election_in_progress =\
          \ False\n        self.broadcast('COORDINATOR', {'leader': self.node_id})\n        print(f'Node {self.node_id}: I\
          \ am the leader!')"
      pitfalls:
      - Split brain
      - Message loss
      - Concurrent elections
      concepts:
      - Bully algorithm
      - Leader election
      - Distributed coordination
      estimated_hours: 4-5
    - id: 3
      name: Ring Election
      description: Implement ring-based election algorithm.
      acceptance_criteria:
      - Nodes arranged in logical ring
      - Election message passed around ring
      - Collect all live node IDs
      - Highest ID becomes leader
      hints:
        level1: Pass ELECTION with list of IDs. When it returns, highest ID wins.
        level2: Each node adds its ID and forwards. When message returns to initiator, broadcast result.
        level3: "class RingElection(Node):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\
          \        self.ring = sorted(self.peers.keys())  # Ring order\n        self.handlers['ELECTION'] = self.on_election\n\
          \        self.handlers['ELECTED'] = self.on_elected\n    \n    def next_node(self):\n        '''Get next node in\
          \ ring'''\n        idx = self.ring.index(self.node_id)\n        for i in range(1, len(self.ring)):\n           \
          \ next_idx = (idx + i) % len(self.ring)\n            next_id = self.ring[next_idx]\n            if self.send_to(next_id,\
          \ 'PING', {}):\n                return next_id\n        return None  # All nodes dead\n    \n    def start_election(self):\n\
          \        # Start election message with my ID\n        self.forward_election([self.node_id], self.node_id)\n    \n\
          \    def forward_election(self, participants: list, initiator: int):\n        next_id = self.next_node()\n     \
          \   if next_id:\n            self.send_to(next_id, 'ELECTION', {\n                'participants': participants,\n\
          \                'initiator': initiator\n            })\n    \n    def on_election(self, msg):\n        participants\
          \ = msg['participants']\n        initiator = msg['initiator']\n        \n        if self.node_id in participants:\n\
          \            # Message completed the ring\n            if initiator == self.node_id:\n                # I started\
          \ this, announce winner\n                winner = max(participants)\n                self.broadcast('ELECTED', {'leader':\
          \ winner})\n                self.leader_id = winner\n        else:\n            # Add myself and forward\n     \
          \       participants.append(self.node_id)\n            self.forward_election(participants, initiator)\n    \n  \
          \  def on_elected(self, msg):\n        self.leader_id = msg['leader']\n        print(f'Node {self.node_id}: Leader\
          \ is {self.leader_id}')"
      pitfalls:
      - Ring breaks
      - Multiple elections
      - Node rejoins
      concepts:
      - Ring topology
      - Token passing
      - Distributed election
      estimated_hours: 4-5
  linear-regression:
    id: linear-regression
    name: Linear Regression
    description: Implement linear regression with gradient descent from scratch. Understand the fundamentals of machine learning
      optimization.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - Basic Python/NumPy
    - High school math (derivatives)
    languages:
      recommended:
      - Python
      also_possible:
      - JavaScript
      - Julia
    resources:
    - name: Linear Regression Tutorial
      url: https://scikit-learn.org/stable/modules/linear_model.html
      type: documentation
    - name: Gradient Descent Explained
      url: https://www.youtube.com/watch?v=sDv4f4s2SB8
      type: video
    milestones:
    - id: 1
      name: Simple Linear Regression
      description: Implement single-variable linear regression.
      acceptance_criteria:
      - Model y = mx + b
      - Fit using closed-form solution
      - Calculate predictions
      - Calculate RÂ² score
      hints:
        level1: 'Closed-form: m = cov(x,y)/var(x), b = mean(y) - m*mean(x)'
        level2: Use numpy for vectorized operations.
        level3: "def fit(X, y):\n    x_mean, y_mean = np.mean(X), np.mean(y)\n    numerator = np.sum((X - x_mean) * (y - y_mean))\n\
          \    denominator = np.sum((X - x_mean) ** 2)\n    m = numerator / denominator\n    b = y_mean - m * x_mean\n   \
          \ return m, b\n\ndef r2_score(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true\
          \ - np.mean(y_true)) ** 2)\n    return 1 - (ss_res / ss_tot)"
      pitfalls:
      - Division by zero with constant X
      - Data not being proper arrays
      - Using float instead of numpy
      concepts:
      - Linear relationship
      - Least squares
      - Model evaluation
      estimated_hours: 2-3
    - id: 2
      name: Gradient Descent
      description: Implement gradient descent optimization.
      acceptance_criteria:
      - Cost function (MSE)
      - Gradient calculation
      - Iterative parameter update
      - Convergence detection
      hints:
        level1: Cost = (1/n) * sum((y_pred - y)Â²)
        level2: 'Update: param = param - learning_rate * gradient'
        level3: "def gradient_descent(X, y, learning_rate=0.01, iterations=1000):\n    m, b = 0, 0\n    n = len(X)\n    for\
          \ _ in range(iterations):\n        y_pred = m * X + b\n        dm = -(2/n) * np.sum(X * (y - y_pred))\n        db\
          \ = -(2/n) * np.sum(y - y_pred)\n        m -= learning_rate * dm\n        b -= learning_rate * db\n    return m,\
          \ b"
      pitfalls:
      - Learning rate too high (divergence)
      - Not normalizing features
      - Stopping too early
      concepts:
      - Gradient descent
      - Learning rate
      - Cost function
      estimated_hours: 2-3
    - id: 3
      name: Multiple Linear Regression
      description: Extend to multiple input features.
      acceptance_criteria:
      - Handle multiple features
      - 'Matrix form: y = Xw'
      - Vectorized gradient descent
      - Feature normalization
      hints:
        level1: Add bias term as column of 1s in X matrix.
        level2: 'Gradient: âˆ‡J = (1/n) * X^T * (Xw - y)'
        level3: "def multi_gradient_descent(X, y, lr=0.01, iters=1000):\n    X = np.c_[np.ones(len(X)), X]  # Add bias column\n\
          \    w = np.zeros(X.shape[1])\n    n = len(y)\n    for _ in range(iters):\n        y_pred = X @ w\n        gradient\
          \ = (1/n) * (X.T @ (y_pred - y))\n        w -= lr * gradient\n    return w"
      pitfalls:
      - Features on different scales
      - Matrix dimension mismatches
      - Forgetting bias term
      concepts:
      - Matrix operations
      - Feature scaling
      - Vectorization
      estimated_hours: 2-3
  linked-list:
    id: linked-list
    name: Linked List
    description: Implement singly, doubly, and circular linked lists. Understand pointer manipulation and memory management.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - Basic programming
    - Understanding of pointers/references
    languages:
      recommended:
      - Python
      - JavaScript
      - C
      - Java
      also_possible:
      - Rust
      - Go
      - C++
    resources:
    - name: Linked List - GeeksforGeeks
      url: https://www.geeksforgeeks.org/linked-list-data-structure/
      type: tutorial
    - name: Visualgo - Linked List
      url: https://visualgo.net/en/list
      type: interactive
    milestones:
    - id: 1
      name: Singly Linked List Basics
      description: Implement a basic singly linked list with insert and traversal.
      acceptance_criteria:
      - Node class with value and next pointer
      - LinkedList class with head reference
      - Insert at beginning (prepend)
      - Insert at end (append)
      - Print/traverse list
      - Get length
      hints:
        level1: 'Node has two properties: value and next (reference to next node).'
        level2: Keep track of head. For append, traverse to end first.
        level3: "class Node:\n    def __init__(self, value):\n        self.value = value\n        self.next = None\n\nclass\
          \ LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def prepend(self, value):\n        new_node\
          \ = Node(value)\n        new_node.next = self.head\n        self.head = new_node\n\n    def append(self, value):\n\
          \        new_node = Node(value)\n        if not self.head:\n            self.head = new_node\n            return\n\
          \        current = self.head\n        while current.next:\n            current = current.next\n        current.next\
          \ = new_node"
      pitfalls:
      - Forgetting to handle empty list
      - Losing reference to head
      - Off-by-one in traversal
      concepts:
      - Node structure
      - Head pointer
      - Traversal
      estimated_hours: 2-3
    - id: 2
      name: Search & Delete
      description: Implement search and delete operations.
      acceptance_criteria:
      - Find node by value
      - Delete node by value
      - Delete at index
      - Handle not found cases
      - Handle single-element list
      hints:
        level1: To delete, keep track of previous node.
        level2: 'Special case: deleting head node.'
        level3: "def find(self, value):\n    current = self.head\n    while current:\n        if current.value == value:\n\
          \            return current\n        current = current.next\n    return None\n\ndef delete(self, value):\n    if\
          \ not self.head:\n        return False\n    if self.head.value == value:\n        self.head = self.head.next\n \
          \       return True\n    current = self.head\n    while current.next:\n        if current.next.value == value:\n\
          \            current.next = current.next.next\n            return True\n        current = current.next\n    return\
          \ False"
      pitfalls:
      - Not handling delete at head
      - Memory leak (in manual memory languages)
      - Deleting wrong node
      concepts:
      - Two-pointer technique
      - Edge cases
      - Memory management
      estimated_hours: 2-3
    - id: 3
      name: Doubly Linked List
      description: Implement a doubly linked list with bidirectional traversal.
      acceptance_criteria:
      - Node has prev and next pointers
      - Maintain both head and tail
      - Insert at beginning and end O(1)
      - Delete from both ends O(1)
      - Traverse forward and backward
      hints:
        level1: 'Each node has three fields: value, prev, next.'
        level2: Keep tail pointer for O(1) append.
        level3: "class DoublyNode:\n    def __init__(self, value):\n        self.value = value\n        self.prev = None\n\
          \        self.next = None\n\nclass DoublyLinkedList:\n    def __init__(self):\n        self.head = None\n      \
          \  self.tail = None\n\n    def append(self, value):\n        new_node = DoublyNode(value)\n        if not self.tail:\n\
          \            self.head = self.tail = new_node\n        else:\n            new_node.prev = self.tail\n          \
          \  self.tail.next = new_node\n            self.tail = new_node"
      pitfalls:
      - Not updating both prev and next
      - Forgetting to update tail
      - Inconsistent head/tail on single element
      concepts:
      - Bidirectional links
      - Head and tail pointers
      - Constant time operations
      estimated_hours: 2-3
    - id: 4
      name: Circular & Advanced Operations
      description: Implement circular linked list and advanced operations.
      acceptance_criteria:
      - Circular linked list (last points to first)
      - Reverse a linked list
      - Detect cycle (Floyd's algorithm)
      - Find middle element
      - Merge two sorted lists
      hints:
        level1: 'Circular: tail.next = head instead of null.'
        level2: 'Reverse: three pointers - prev, current, next.'
        level3: "def reverse(self):\n    prev = None\n    current = self.head\n    while current:\n        next_node = current.next\n\
          \        current.next = prev\n        prev = current\n        current = next_node\n    self.head = prev\n\ndef has_cycle(self):\n\
          \    slow = fast = self.head\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n\
          \        if slow == fast:\n            return True\n    return False\n\ndef find_middle(self):\n    slow = fast\
          \ = self.head\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n    return\
          \ slow"
      pitfalls:
      - Infinite loop in circular list traversal
      - Reverse losing nodes
      - Cycle detection on empty list
      concepts:
      - Circular structures
      - In-place reversal
      - Floyd's cycle detection
      - Two-pointer technique
      estimated_hours: 2-3
  lisp-interp:
    id: lisp-interp
    name: Lisp Interpreter
    description: Build an interpreter for a minimal Lisp. Learn S-expressions, environments, and functional programming concepts.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Recursion
    - Basic parsing
    - Functional programming concepts
    languages:
      recommended:
      - Python
      - JavaScript
      - Ruby
      also_possible:
      - Go
      - Rust
      - C
    resources:
    - name: Make a Lisp
      url: https://github.com/kanaka/mal
      type: tutorial
    - name: SICP
      url: https://mitpress.mit.edu/sites/default/files/sicp/index.html
      type: book
    milestones:
    - id: 1
      name: S-Expression Parser
      description: Parse Lisp S-expressions into data structures.
      acceptance_criteria:
      - Parse atoms (numbers, symbols)
      - Parse lists (nested parentheses)
      - Handle whitespace and comments
      - Return native data structures
      hints:
        level1: 'Tokenize first: (, ), atoms. Then parse recursively.'
        level2: ( starts list, ) ends it, atoms are leaf nodes.
        level3: "def tokenize(code):\n    tokens = []\n    i = 0\n    while i < len(code):\n        if code[i] in '()':\n\
          \            tokens.append(code[i])\n            i += 1\n        elif code[i].isspace():\n            i += 1\n \
          \       elif code[i] == ';':  # Comment\n            while i < len(code) and code[i] != '\\n':\n               \
          \ i += 1\n        else:\n            j = i\n            while j < len(code) and code[j] not in '() \\t\\n;':\n \
          \               j += 1\n            tokens.append(code[i:j])\n            i = j\n    return tokens\n\ndef parse(tokens):\n\
          \    token = tokens.pop(0)\n    if token == '(':\n        lst = []\n        while tokens[0] != ')':\n          \
          \  lst.append(parse(tokens))\n        tokens.pop(0)  # Remove ')'\n        return lst\n    elif token == ')':\n\
          \        raise SyntaxError('Unexpected )')\n    else:\n        return parse_atom(token)\n\ndef parse_atom(token):\n\
          \    try: return int(token)\n    except: pass\n    try: return float(token)\n    except: pass\n    return token\
          \  # Symbol"
      pitfalls:
      - Unbalanced parentheses
      - Quote syntax
      - Negative numbers
      concepts:
      - S-expressions
      - Tokenization
      - Recursive parsing
      estimated_hours: 2-4
    - id: 2
      name: Basic Evaluation
      description: Evaluate arithmetic and conditionals.
      acceptance_criteria:
      - Evaluate numbers to themselves
      - 'Arithmetic: +, -, *, /'
      - 'Comparison: <, >, =, <=, >='
      - 'Conditionals: if'
      hints:
        level1: 'Numbers self-evaluate. Lists: first element is operator.'
        level2: Environment maps symbols to values.
        level3: "def eval_expr(expr, env):\n    if isinstance(expr, (int, float)):\n        return expr\n    if isinstance(expr,\
          \ str):  # Symbol\n        if expr not in env:\n            raise NameError(f'Undefined: {expr}')\n        return\
          \ env[expr]\n    if isinstance(expr, list):\n        if len(expr) == 0:\n            raise SyntaxError('Empty list')\n\
          \        \n        op = expr[0]\n        \n        # Special forms\n        if op == 'if':\n            _, test,\
          \ then, else_ = expr\n            return eval_expr(then if eval_expr(test, env) else else_, env)\n        \n   \
          \     # Function call\n        func = eval_expr(op, env)\n        args = [eval_expr(arg, env) for arg in expr[1:]]\n\
          \        return func(*args)\n\n# Built-in environment\nimport operator\nglobal_env = {\n    '+': operator.add,\n\
          \    '-': operator.sub,\n    '*': operator.mul,\n    '/': operator.truediv,\n    '<': operator.lt,\n    '>': operator.gt,\n\
          \    '=': operator.eq,\n}"
      pitfalls:
      - Special forms vs functions
      - Short-circuit evaluation
      - Environment lookup
      concepts:
      - Evaluation rules
      - Environments
      - Special forms
      estimated_hours: 3-4
    - id: 3
      name: Variables and Functions
      description: Add define, lambda, and lexical scope.
      acceptance_criteria:
      - define binds variables
      - lambda creates functions
      - Lexical scoping (closures)
      - let for local bindings
      hints:
        level1: Lambda captures its defining environment.
        level2: Closure = (params, body, env).
        level3: "class Closure:\n    def __init__(self, params, body, env):\n        self.params = params\n        self.body\
          \ = body\n        self.env = env\n    \n    def __call__(self, *args):\n        # Create new env with params bound\
          \ to args\n        local_env = dict(self.env)\n        local_env.update(zip(self.params, args))\n        return\
          \ eval_expr(self.body, local_env)\n\ndef eval_expr(expr, env):\n    # ... previous code ...\n    \n    if op ==\
          \ 'define':\n        _, name, value = expr\n        env[name] = eval_expr(value, env)\n        return None\n   \
          \ \n    if op == 'lambda':\n        _, params, body = expr\n        return Closure(params, body, env.copy())\n \
          \   \n    if op == 'let':\n        # (let ((x 1) (y 2)) body)\n        _, bindings, body = expr\n        local_env\
          \ = dict(env)\n        for name, value in bindings:\n            local_env[name] = eval_expr(value, env)\n     \
          \   return eval_expr(body, local_env)"
      pitfalls:
      - Mutation vs shadowing
      - Closure capturing
      - Let vs let*
      concepts:
      - Closures
      - Lexical scope
      - Variable binding
      estimated_hours: 4-6
    - id: 4
      name: List Operations & Recursion
      description: Add list primitives and support recursion.
      acceptance_criteria:
      - cons, car, cdr operations
      - list constructor
      - null? predicate
      - Recursive functions work
      hints:
        level1: cons builds pairs, car/cdr destructure them.
        level2: For recursion, function must be in scope when called.
        level3: "# List operations\nglobal_env.update({\n    'cons': lambda a, b: [a] + (b if isinstance(b, list) else [b]),\n\
          \    'car': lambda lst: lst[0],\n    'cdr': lambda lst: lst[1:],\n    'list': lambda *args: list(args),\n    'null?':\
          \ lambda lst: lst == [],\n    'length': len,\n})\n\n# Example: factorial\n# (define fact (lambda (n) (if (= n 0)\
          \ 1 (* n (fact (- n 1))))))\n\n# Example: map\n# (define map (lambda (f lst)\n#   (if (null? lst)\n#       (list)\n\
          #       (cons (f (car lst)) (map f (cdr lst))))))"
      pitfalls:
      - Proper vs improper lists
      - Stack overflow on deep recursion
      - Empty list handling
      concepts:
      - List processing
      - Recursion
      - Higher-order functions
      estimated_hours: 3-4
  load-balancer-basic:
    id: load-balancer-basic
    name: Load Balancer (Basic)
    description: Build a basic application load balancer with round-robin distribution. Learn about reverse proxying and server
      health management.
    difficulty: intermediate
    estimated_hours: 15-20
    prerequisites:
    - HTTP protocol
    - TCP networking
    - Concurrency basics
    languages:
      recommended:
      - Go
      - Python
      - JavaScript
      also_possible:
      - Rust
      - Java
    resources:
    - name: Build Your Own Load Balancer
      url: https://codingchallenges.fyi/challenges/challenge-load-balancer/
      type: tutorial
    - name: Load Balancer in Go
      url: https://kasvith.me/posts/lets-create-a-simple-lb-go/
      type: tutorial
    milestones:
    - id: 1
      name: HTTP Proxy Foundation
      description: Build basic HTTP reverse proxy functionality.
      acceptance_criteria:
      - Accept incoming HTTP requests
      - Forward to single backend server
      - Return backend response to client
      - Handle connection errors
      - Log requests
      hints:
        level1: Read request from client, write to backend, read response, write to client.
        level2: Use HTTP library to handle request/response parsing.
        level3: "from flask import Flask, request, Response\nimport requests\n\napp = Flask(__name__)\nBACKEND = 'http://localhost:8001'\n\
          \n@app.route('/<path:path>', methods=['GET', 'POST', 'PUT', 'DELETE'])\ndef proxy(path):\n    url = f'{BACKEND}/{path}'\n\
          \    resp = requests.request(\n        method=request.method,\n        url=url,\n        headers={k: v for k, v\
          \ in request.headers if k != 'Host'},\n        data=request.get_data(),\n        allow_redirects=False\n    )\n\
          \    return Response(resp.content, status=resp.status_code, headers=dict(resp.headers))"
      pitfalls:
      - Not forwarding all headers
      - Not handling request body
      - Connection timeouts
      concepts:
      - Reverse proxy
      - HTTP forwarding
      - Request/response handling
      estimated_hours: 3-4
    - id: 2
      name: Round Robin Distribution
      description: Distribute requests across multiple backends.
      acceptance_criteria:
      - Configure list of backend servers
      - Round-robin selection algorithm
      - Even distribution across backends
      - Thread-safe counter
      - Skip unavailable backends
      hints:
        level1: Keep counter, increment mod number of backends.
        level2: Use atomic operations for thread safety.
        level3: "class RoundRobinBalancer:\n    def __init__(self, backends):\n        self.backends = backends\n        self.current\
          \ = 0\n        self.lock = threading.Lock()\n\n    def get_next(self):\n        with self.lock:\n            backend\
          \ = self.backends[self.current]\n            self.current = (self.current + 1) % len(self.backends)\n          \
          \  return backend"
      pitfalls:
      - Race condition in counter
      - Modulo with zero backends
      - Uneven distribution after backend changes
      concepts:
      - Round robin algorithm
      - Atomic operations
      - Backend pool
      estimated_hours: 2-3
    - id: 3
      name: Health Checks
      description: Implement active health checking of backends.
      acceptance_criteria:
      - Periodic health check requests
      - Mark unhealthy backends
      - Skip unhealthy backends in routing
      - Recover backends when healthy
      - Configurable check interval and threshold
      hints:
        level1: Background thread pings each backend periodically.
        level2: Track consecutive failures, mark unhealthy after threshold.
        level3: "class HealthChecker:\n    def __init__(self, backends, threshold=3):\n        self.backends = backends\n\
          \        self.healthy = {b: True for b in backends}\n        self.failures = {b: 0 for b in backends}\n        self.threshold\
          \ = threshold\n\n    def check_backend(self, backend):\n        try:\n            resp = requests.get(f'{backend}/health',\
          \ timeout=2)\n            if resp.status_code == 200:\n                self.failures[backend] = 0\n            \
          \    self.healthy[backend] = True\n                return\n        except: pass\n        self.failures[backend]\
          \ += 1\n        if self.failures[backend] >= self.threshold:\n            self.healthy[backend] = False"
      pitfalls:
      - Health checks overwhelming backends
      - No healthy backends available
      - Thundering herd on recovery
      concepts:
      - Health checking
      - Failure detection
      - Graceful degradation
      estimated_hours: 3-4
    - id: 4
      name: Additional Algorithms
      description: Implement additional load balancing algorithms.
      acceptance_criteria:
      - Weighted round robin
      - Least connections
      - IP hash (sticky sessions)
      - Random selection
      - Algorithm configurable at runtime
      hints:
        level1: 'Least connections: track active requests per backend.'
        level2: 'IP hash: hash(client_ip) % len(backends) for consistency.'
        level3: "class LeastConnectionsBalancer:\n    def __init__(self, backends):\n        self.backends = backends\n  \
          \      self.connections = {b: 0 for b in backends}\n        self.lock = threading.Lock()\n\n    def get_next(self):\n\
          \        with self.lock:\n            healthy = [b for b in self.backends if health_checker.healthy[b]]\n      \
          \      return min(healthy, key=lambda b: self.connections[b])\n\nclass IPHashBalancer:\n    def get_next(self, client_ip):\n\
          \        healthy = [b for b in self.backends if health_checker.healthy[b]]\n        return healthy[hash(client_ip)\
          \ % len(healthy)]"
      pitfalls:
      - Least connections not updating on response
      - IP hash inconsistent after backend changes
      - Weighted round robin integer overflow
      concepts:
      - Load balancing algorithms
      - Session affinity
      - Algorithm tradeoffs
      estimated_hours: 4-5
  logging-structured:
    id: logging-structured
    name: Structured Logging System
    description: Implement production-grade structured logging. Learn log levels, formats, and log aggregation patterns.
    difficulty: intermediate
    estimated_hours: 10-15
    prerequisites:
    - JSON
    - File I/O
    - Basic debugging concepts
    languages:
      recommended:
      - Python
      - Go
      - Java
      also_possible:
      - JavaScript
      - Rust
    resources:
    - name: 12 Factor App - Logs
      url: https://12factor.net/logs
      type: article
    - name: Structured Logging Guide
      url: https://www.honeycomb.io/blog/structured-logging
      type: article
    milestones:
    - id: 1
      name: Logger Core
      description: Build the core logging infrastructure.
      acceptance_criteria:
      - Log levels (DEBUG, INFO, WARN, ERROR)
      - Configurable minimum level
      - Thread-safe logging
      - Output to multiple destinations
      hints:
        level1: Logger accepts level + message + context. Filters by min level.
        level2: Use mutex/lock for thread safety. Handler pattern for outputs.
        level3: "import threading\nimport sys\nfrom enum import IntEnum\nfrom typing import Dict, Any, List, Callable\nfrom\
          \ datetime import datetime\n\nclass Level(IntEnum):\n    DEBUG = 10\n    INFO = 20\n    WARN = 30\n    ERROR = 40\n\
          \    FATAL = 50\n\nclass LogRecord:\n    def __init__(self, level: Level, message: str, **context):\n        self.timestamp\
          \ = datetime.utcnow()\n        self.level = level\n        self.message = message\n        self.context = context\n\
          \nclass Handler:\n    def emit(self, record: LogRecord):\n        raise NotImplementedError\n\nclass Logger:\n \
          \   def __init__(self, name: str = 'root', level: Level = Level.INFO):\n        self.name = name\n        self.level\
          \ = level\n        self.handlers: List[Handler] = []\n        self._lock = threading.Lock()\n    \n    def add_handler(self,\
          \ handler: Handler):\n        self.handlers.append(handler)\n    \n    def log(self, level: Level, message: str,\
          \ **context):\n        if level < self.level:\n            return\n        \n        record = LogRecord(level, message,\
          \ logger=self.name, **context)\n        \n        with self._lock:\n            for handler in self.handlers:\n\
          \                handler.emit(record)\n    \n    def debug(self, message: str, **ctx): self.log(Level.DEBUG, message,\
          \ **ctx)\n    def info(self, message: str, **ctx): self.log(Level.INFO, message, **ctx)\n    def warn(self, message:\
          \ str, **ctx): self.log(Level.WARN, message, **ctx)\n    def error(self, message: str, **ctx): self.log(Level.ERROR,\
          \ message, **ctx)"
      pitfalls:
      - Race conditions
      - Blocking I/O in hot path
      - Missing context
      concepts:
      - Log levels
      - Thread safety
      - Handler pattern
      estimated_hours: 3-4
    - id: 2
      name: Structured Output
      description: Format logs as structured JSON.
      acceptance_criteria:
      - JSON output format
      - Include timestamp, level, message, context
      - Support custom formatters
      - Pretty-print for development
      hints:
        level1: JSON format allows log aggregation tools to parse logs.
        level2: 'Include: timestamp (ISO8601), level, message, all context fields.'
        level3: "import json\n\nclass JSONHandler(Handler):\n    def __init__(self, stream=sys.stdout):\n        self.stream\
          \ = stream\n    \n    def emit(self, record: LogRecord):\n        log_dict = {\n            'timestamp': record.timestamp.isoformat()\
          \ + 'Z',\n            'level': record.level.name,\n            'message': record.message,\n            **record.context\n\
          \        }\n        line = json.dumps(log_dict, default=str) + '\\n'\n        self.stream.write(line)\n        self.stream.flush()\n\
          \nclass PrettyHandler(Handler):\n    '''Human-readable format for development'''\n    COLORS = {\n        Level.DEBUG:\
          \ '\\033[36m',  # Cyan\n        Level.INFO: '\\033[32m',   # Green\n        Level.WARN: '\\033[33m',   # Yellow\n\
          \        Level.ERROR: '\\033[31m',  # Red\n    }\n    RESET = '\\033[0m'\n    \n    def emit(self, record: LogRecord):\n\
          \        color = self.COLORS.get(record.level, '')\n        ts = record.timestamp.strftime('%H:%M:%S.%f')[:-3]\n\
          \        \n        ctx_str = ''\n        if record.context:\n            ctx_parts = [f'{k}={v!r}' for k, v in record.context.items()\
          \ if k != 'logger']\n            if ctx_parts:\n                ctx_str = f' | {\" \".join(ctx_parts)}'\n      \
          \  \n        print(f'{ts} {color}{record.level.name:5}{self.RESET} {record.message}{ctx_str}')"
      pitfalls:
      - Non-serializable values
      - Missing flush
      - Large context objects
      concepts:
      - Structured logging
      - JSON format
      - Formatters
      estimated_hours: 2-3
    - id: 3
      name: Context & Correlation
      description: Add request context and correlation IDs.
      acceptance_criteria:
      - Automatic request ID injection
      - Context propagation across functions
      - Child loggers with inherited context
      - Async-safe context
      hints:
        level1: 'Correlation ID: unique ID per request, included in all logs.'
        level2: Use thread-local or context variables for automatic injection.
        level3: "import contextvars\nimport uuid\n\n# Context variable for request context\nrequest_context: contextvars.ContextVar[Dict[str,\
          \ Any]] = contextvars.ContextVar('request_context', default={})\n\nclass ContextLogger(Logger):\n    def log(self,\
          \ level: Level, message: str, **context):\n        # Merge request context\n        ctx = {**request_context.get(),\
          \ **context}\n        super().log(level, message, **ctx)\n\ndef with_request_context(**ctx):\n    '''Decorator to\
          \ set request context'''\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            token = request_context.set({**request_context.get(),\
          \ **ctx})\n            try:\n                return func(*args, **kwargs)\n            finally:\n              \
          \  request_context.reset(token)\n        return wrapper\n    return decorator\n\n# Middleware example\nclass LoggingMiddleware:\n\
          \    def __call__(self, request, next_handler):\n        request_id = request.headers.get('X-Request-ID', str(uuid.uuid4()))\n\
          \        \n        token = request_context.set({\n            'request_id': request_id,\n            'method': request.method,\n\
          \            'path': request.path,\n        })\n        \n        try:\n            logger.info('Request started')\n\
          \            response = next_handler(request)\n            logger.info('Request completed', status=response.status)\n\
          \            return response\n        except Exception as e:\n            logger.error('Request failed', error=str(e))\n\
          \            raise\n        finally:\n            request_context.reset(token)"
      pitfalls:
      - Context not propagating
      - Memory leaks
      - Async context issues
      concepts:
      - Correlation IDs
      - Context propagation
      - Request tracing
      estimated_hours: 3-4
  markdown-renderer:
    id: markdown-renderer
    name: Markdown Renderer
    description: Build a Markdown to HTML converter. Learn text parsing, regular expressions, and document transformation.
    difficulty: beginner
    estimated_hours: 12-18
    prerequisites:
    - Regular expressions
    - String manipulation
    - HTML basics
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - Rust
      - Ruby
    resources:
    - name: CommonMark Spec
      url: https://spec.commonmark.org/
      type: specification
    - name: Building a Markdown Parser
      url: https://dev.to/kawaljain/building-my-own-markdown-parser-a-developers-journey-3b26
      type: tutorial
    - name: Sarvasv's MD Parser Notes
      url: https://sarvasvkulpati.com/mdparser
      type: tutorial
    milestones:
    - id: 1
      name: Block Elements
      description: 'Parse block-level elements: headings, paragraphs, code blocks, horizontal rules.'
      acceptance_criteria:
      - '# to ###### headings â†’ h1-h6'
      - Blank-line separated paragraphs â†’ p
      - '``` or indented code â†’ pre>code'
      - '--- or *** â†’ hr'
      hints:
        level1: Process line by line. Blank lines separate blocks. Accumulate paragraph lines.
        level2: 'Headings: /^(#{1,6})\s+(.+)$/. Code blocks: track if inside ``` fence.'
        level3: "def parse_blocks(text: str) -> list[dict]:\n    '''Parse text into block elements'''\n    lines = text.split('\\\
          n')\n    blocks = []\n    current_para = []\n    in_code = False\n    code_lines = []\n\n    def flush_para():\n\
          \        if current_para:\n            blocks.append({'type': 'paragraph', 'content': ' '.join(current_para)})\n\
          \            current_para.clear()\n\n    for line in lines:\n        # Code fence\n        if line.startswith('```'):\n\
          \            if in_code:\n                blocks.append({'type': 'code', 'content': '\\n'.join(code_lines)})\n \
          \               code_lines.clear()\n                in_code = False\n            else:\n                flush_para()\n\
          \                in_code = True\n            continue\n\n        if in_code:\n            code_lines.append(line)\n\
          \            continue\n\n        # Heading\n        match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n        if match:\n\
          \            flush_para()\n            level = len(match.group(1))\n            blocks.append({'type': f'h{level}',\
          \ 'content': match.group(2)})\n            continue\n\n        # Horizontal rule\n        if re.match(r'^([-*_])\\\
          1{2,}\\s*$', line):\n            flush_para()\n            blocks.append({'type': 'hr'})\n            continue\n\
          \n        # Blank line\n        if not line.strip():\n            flush_para()\n            continue\n\n       \
          \ # Paragraph continuation\n        current_para.append(line)\n\n    flush_para()\n    return blocks"
      pitfalls:
      - Setext headings (underline style) need lookahead
      - Indented code vs list continuation is complex
      - Fenced code can have language identifier
      concepts:
      - Block parsing
      - State machines
      - Line-by-line processing
      estimated_hours: 4-5
    - id: 2
      name: Inline Elements
      description: 'Parse inline formatting: bold, italic, code, links, images.'
      acceptance_criteria:
      - '**bold** and __bold__ â†’ strong'
      - '*italic* and _italic_ â†’ em'
      - '`code` â†’ code'
      - '[text](url) â†’ a href'
      - '![alt](url) â†’ img'
      hints:
        level1: Process inline after block parsing. Use regex or character-by-character scan.
        level2: 'Handle nested: **bold _and italic_**. Process outer markers first.'
        level3: "def parse_inline(text: str) -> str:\n    '''Convert inline markdown to HTML'''\n    # Process in order: code\
          \ (literal), then links, then emphasis\n\n    # Inline code (must be first - content is literal)\n    text = re.sub(r'`([^`]+)`',\
          \ r'<code>\\1</code>', text)\n\n    # Images (before links - similar syntax)\n    text = re.sub(r'!\\[([^\\]]*)\\\
          ]\\(([^)]+)\\)', r'<img src=\"\\2\" alt=\"\\1\">', text)\n\n    # Links\n    text = re.sub(r'\\[([^\\]]+)\\]\\(([^)]+)\\\
          )', r'<a href=\"\\2\">\\1</a>', text)\n\n    # Bold (** or __)\n    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'<strong>\\\
          1</strong>', text)\n    text = re.sub(r'__([^_]+)__', r'<strong>\\1</strong>', text)\n\n    # Italic (* or _)\n\
          \    text = re.sub(r'\\*([^*]+)\\*', r'<em>\\1</em>', text)\n    text = re.sub(r'_([^_]+)_', r'<em>\\1</em>', text)\n\
          \n    return text\n\n# Better approach: tokenize then render\ndef tokenize_inline(text: str) -> list:\n    tokens\
          \ = []\n    i = 0\n    while i < len(text):\n        if text[i:i+2] == '**':\n            # Find closing **\n  \
          \          end = text.find('**', i+2)\n            if end != -1:\n                tokens.append(('strong', text[i+2:end]))\n\
          \                i = end + 2\n                continue\n        # ... handle other cases\n        tokens.append(('text',\
          \ text[i]))\n        i += 1\n    return tokens"
      pitfalls:
      - Underscore in middle_of_words shouldn't trigger emphasis
      - 'Mismatched delimiters: **bold* is invalid'
      - 'Escaping: \* should be literal asterisk'
      concepts:
      - Inline parsing
      - Regular expressions
      - Nested formatting
      estimated_hours: 4-5
    - id: 3
      name: Lists
      description: Parse ordered and unordered lists with nesting.
      acceptance_criteria:
      - '- or * for unordered lists â†’ ul/li'
      - 1. 2. 3. for ordered lists â†’ ol/li
      - Indented items create nested lists
      - Tight vs loose lists (with blank lines)
      hints:
        level1: Track indentation level. Deeper indent = nested list. Same indent = sibling.
        level2: Use a stack to track nested lists. Push on deeper indent, pop on shallower.
        level3: "def parse_list(lines: list[str], start_idx: int) -> tuple[dict, int]:\n    '''Parse list starting at start_idx,\
          \ return (list_node, next_idx)'''\n    first_line = lines[start_idx]\n\n    # Determine list type\n    if re.match(r'^\\\
          d+\\.\\s', first_line.lstrip()):\n        list_type = 'ol'\n        pattern = r'^(\\s*)(\\d+)\\.\\s+(.*)$'\n   \
          \ else:\n        list_type = 'ul'\n        pattern = r'^(\\s*)([-*+])\\s+(.*)$'\n\n    base_indent = len(first_line)\
          \ - len(first_line.lstrip())\n    items = []\n    idx = start_idx\n\n    while idx < len(lines):\n        line =\
          \ lines[idx]\n        if not line.strip():\n            idx += 1\n            continue\n\n        match = re.match(pattern,\
          \ line)\n        if not match:\n            break\n\n        indent = len(match.group(1))\n        content = match.group(3)\n\
          \n        if indent < base_indent:\n            break\n        elif indent > base_indent:\n            # Nested\
          \ list - recursively parse\n            nested, idx = parse_list(lines, idx)\n            items[-1]['children'].append(nested)\n\
          \        else:\n            items.append({'content': content, 'children': []})\n            idx += 1\n\n    return\
          \ {'type': list_type, 'items': items}, idx"
      pitfalls:
      - Mixed list types in same level is invalid
      - Continuation lines must be indented
      - Loose lists have paragraphs in items
      concepts:
      - Recursive parsing
      - Indentation tracking
      - Tree structures
      estimated_hours: 4-5
    - id: 4
      name: HTML Generation
      description: Convert parsed structure to valid HTML output.
      acceptance_criteria:
      - Generate valid HTML5
      - Escape special characters (&, <, >)
      - Proper nesting and indentation
      - 'Optional: add wrapper template'
      hints:
        level1: Walk the parsed tree, emit HTML tags. Escape content but not generated tags.
        level2: Use html.escape() for content. Track indent level for pretty printing.
        level3: "import html\n\nclass HtmlRenderer:\n    def __init__(self, pretty: bool = True):\n        self.pretty = pretty\n\
          \        self.indent = 0\n\n    def render(self, blocks: list[dict]) -> str:\n        output = []\n        for block\
          \ in blocks:\n            output.append(self.render_block(block))\n        return '\\n'.join(output)\n\n    def\
          \ render_block(self, block: dict) -> str:\n        t = block['type']\n\n        if t == 'hr':\n            return\
          \ self._line('<hr>')\n\n        if t.startswith('h'):\n            content = self.render_inline(block['content'])\n\
          \            return self._line(f'<{t}>{content}</{t}>')\n\n        if t == 'paragraph':\n            content = self.render_inline(block['content'])\n\
          \            return self._line(f'<p>{content}</p>')\n\n        if t == 'code':\n            escaped = html.escape(block['content'])\n\
          \            return self._line(f'<pre><code>{escaped}</code></pre>')\n\n        if t in ('ul', 'ol'):\n        \
          \    return self.render_list(block)\n\n        return ''\n\n    def render_inline(self, text: str) -> str:\n   \
          \     # First escape HTML entities in the text\n        text = html.escape(text)\n        # Then apply inline markdown\
          \ (careful not to escape our generated tags)\n        # ... apply inline patterns\n        return text\n\n    def\
          \ _line(self, content: str) -> str:\n        indent = '  ' * self.indent if self.pretty else ''\n        return\
          \ f'{indent}{content}'"
      pitfalls:
      - 'Double-escaping: escape content before inline parsing'
      - 'Self-closing tags: <hr> not <hr></hr>'
      - Inline HTML in markdown should pass through
      concepts:
      - HTML generation
      - Character escaping
      - Tree traversal
      estimated_hours: 3-4
  memory-pool:
    id: memory-pool
    name: Memory Pool Allocator
    description: Build a fixed-size block memory allocator. Learn memory management, fragmentation, and allocation strategies.
    difficulty: intermediate
    estimated_hours: 10-15
    prerequisites:
    - C pointers
    - Memory layout
    - Data structures
    languages:
      recommended:
      - C
      - Rust
      - Zig
      also_possible:
      - C++
    resources:
    - name: Memory Pool Design
      url: https://en.wikipedia.org/wiki/Memory_pool
      type: article
    milestones:
    - id: 1
      name: Fixed-Size Pool
      description: Allocate a pool of fixed-size blocks.
      acceptance_criteria:
      - Initialize pool with N blocks of size S
      - Allocate returns block or NULL
      - Free returns block to pool
      - O(1) alloc and free
      hints:
        level1: 'Use free list: each free block points to next free block.'
        level2: Store next pointer in the free block itself (no extra memory).
        level3: "typedef struct Pool {\n    void *memory;\n    void *free_list;\n    size_t block_size;\n    size_t count;\n\
          } Pool;\n\nvoid pool_init(Pool *p, size_t block_size, size_t count) {\n    p->block_size = block_size < sizeof(void*)\
          \ ? sizeof(void*) : block_size;\n    p->count = count;\n    p->memory = malloc(p->block_size * count);\n    p->free_list\
          \ = p->memory;\n    // Chain all blocks\n    char *block = p->memory;\n    for (size_t i = 0; i < count - 1; i++)\
          \ {\n        *(void**)(block) = block + p->block_size;\n        block += p->block_size;\n    }\n    *(void**)block\
          \ = NULL;\n}\n\nvoid *pool_alloc(Pool *p) {\n    if (!p->free_list) return NULL;\n    void *block = p->free_list;\n\
          \    p->free_list = *(void**)block;\n    return block;\n}\n\nvoid pool_free(Pool *p, void *block) {\n    *(void**)block\
          \ = p->free_list;\n    p->free_list = block;\n}"
      pitfalls:
      - Block size smaller than pointer
      - Double free
      - Use after free
      concepts:
      - Free list
      - Pointer aliasing
      - Memory layout
      estimated_hours: 3-4
    - id: 2
      name: Pool Growing
      description: Allow pool to grow when exhausted.
      acceptance_criteria:
      - Allocate new chunk when pool empty
      - Chain chunks together
      - Track all chunks for cleanup
      - Optional max size limit
      hints:
        level1: Keep linked list of chunks.
        level2: Each chunk contains blocks, chain new blocks to free list.
        level3: "typedef struct Chunk {\n    void *memory;\n    struct Chunk *next;\n} Chunk;\n\ntypedef struct GrowablePool\
          \ {\n    Chunk *chunks;\n    void *free_list;\n    size_t block_size;\n    size_t chunk_blocks;\n} GrowablePool;\n\
          \nvoid *gpool_alloc(GrowablePool *p) {\n    if (!p->free_list) {\n        // Allocate new chunk\n        Chunk *chunk\
          \ = malloc(sizeof(Chunk));\n        chunk->memory = malloc(p->block_size * p->chunk_blocks);\n        chunk->next\
          \ = p->chunks;\n        p->chunks = chunk;\n        // Add blocks to free list\n        char *block = chunk->memory;\n\
          \        for (size_t i = 0; i < p->chunk_blocks; i++) {\n            *(void**)block = p->free_list;\n          \
          \  p->free_list = block;\n            block += p->block_size;\n        }\n    }\n    void *block = p->free_list;\n\
          \    p->free_list = *(void**)block;\n    return block;\n}"
      pitfalls:
      - Memory leak on destroy
      - Infinite growth
      - Fragmentation across chunks
      concepts:
      - Dynamic allocation
      - Chunk management
      - Resource limits
      estimated_hours: 2-3
    - id: 3
      name: Thread Safety & Debugging
      description: Add thread safety and debugging features.
      acceptance_criteria:
      - Mutex protection for alloc/free
      - Detect double free
      - Memory poisoning on free
      - Statistics tracking
      hints:
        level1: Lock around free list operations.
        level2: Use magic number to detect double free.
        level3: "#define MAGIC_FREE 0xDEADBEEF\n#define MAGIC_USED 0xBEEFCAFE\n\ntypedef struct DebugBlock {\n    uint32_t\
          \ magic;\n    char data[];\n} DebugBlock;\n\nvoid *pool_alloc_debug(Pool *p) {\n    pthread_mutex_lock(&p->lock);\n\
          \    DebugBlock *block = pool_alloc_internal(p);\n    if (block) {\n        assert(block->magic == MAGIC_FREE);\n\
          \        block->magic = MAGIC_USED;\n        memset(block->data, 0xCC, p->block_size - sizeof(DebugBlock));\n  \
          \  }\n    pthread_mutex_unlock(&p->lock);\n    return block ? block->data : NULL;\n}\n\nvoid pool_free_debug(Pool\
          \ *p, void *ptr) {\n    DebugBlock *block = (DebugBlock*)((char*)ptr - offsetof(DebugBlock, data));\n    pthread_mutex_lock(&p->lock);\n\
          \    assert(block->magic == MAGIC_USED);  // Detect double free\n    block->magic = MAGIC_FREE;\n    memset(block->data,\
          \ 0xDD, p->block_size - sizeof(DebugBlock));  // Poison\n    pool_free_internal(p, block);\n    pthread_mutex_unlock(&p->lock);\n\
          }"
      pitfalls:
      - Lock ordering deadlocks
      - Overhead of debugging
      - False positives in detection
      concepts:
      - Thread safety
      - Memory debugging
      - Defensive programming
      estimated_hours: 3-4
  metrics-dashboard:
    id: metrics-dashboard
    name: Metrics & Alerting Dashboard
    description: Build a metrics collection and visualization system with alerting capabilities.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - HTTP APIs
    - Time-series data concepts
    - Basic statistics
    - Docker
    languages:
      recommended:
      - Go
      - Python
      also_possible:
      - JavaScript
      - Rust
    resources:
    - type: documentation
      name: Prometheus Documentation
      url: https://prometheus.io/docs/introduction/overview/
    - type: documentation
      name: Grafana Documentation
      url: https://grafana.com/docs/grafana/latest/
    - type: article
      name: Google SRE Book - Monitoring
      url: https://sre.google/sre-book/monitoring-distributed-systems/
    milestones:
    - id: 1
      name: Metrics Collection
      description: Implement metrics collection with counters, gauges, and histograms.
      acceptance_criteria:
      - Counter metrics
      - Gauge metrics
      - Histogram metrics
      - Labels/dimensions
      - Prometheus exposition format
      hints:
        level1: Start with counters (always increasing) and gauges (can go up/down).
        level2: 'Implement the Prometheus text format: metric_name{label="value"} value timestamp'
        level3: "## Metrics Types Implementation\n\n```go\npackage metrics\n\nimport (\n    \"fmt\"\n    \"sort\"\n    \"\
          strings\"\n    \"sync\"\n    \"time\"\n)\n\n// Counter - only increases\ntype Counter struct {\n    mu     sync.RWMutex\n\
          \    values map[string]float64  // key: serialized labels\n    labels []string\n}\n\nfunc NewCounter(labels ...string)\
          \ *Counter {\n    return &Counter{\n        values: make(map[string]float64),\n        labels: labels,\n    }\n\
          }\n\nfunc (c *Counter) Inc(labelValues ...string) {\n    c.Add(1, labelValues...)\n}\n\nfunc (c *Counter) Add(v\
          \ float64, labelValues ...string) {\n    key := strings.Join(labelValues, \"|\")\n    c.mu.Lock()\n    c.values[key]\
          \ += v\n    c.mu.Unlock()\n}\n\n// Gauge - can increase or decrease\ntype Gauge struct {\n    mu     sync.RWMutex\n\
          \    values map[string]float64\n    labels []string\n}\n\nfunc (g *Gauge) Set(v float64, labelValues ...string)\
          \ {\n    key := strings.Join(labelValues, \"|\")\n    g.mu.Lock()\n    g.values[key] = v\n    g.mu.Unlock()\n}\n\
          \n// Histogram - distribution of values\ntype Histogram struct {\n    mu      sync.RWMutex\n    buckets []float64\n\
          \    counts  map[string][]uint64  // per-label bucket counts\n    sums    map[string]float64\n    totals  map[string]uint64\n\
          }\n\nfunc NewHistogram(buckets []float64) *Histogram {\n    sort.Float64s(buckets)\n    return &Histogram{\n   \
          \     buckets: buckets,\n        counts:  make(map[string][]uint64),\n        sums:    make(map[string]float64),\n\
          \        totals:  make(map[string]uint64),\n    }\n}\n\nfunc (h *Histogram) Observe(v float64, labelValues ...string)\
          \ {\n    key := strings.Join(labelValues, \"|\")\n    h.mu.Lock()\n    defer h.mu.Unlock()\n    \n    if _, ok :=\
          \ h.counts[key]; !ok {\n        h.counts[key] = make([]uint64, len(h.buckets))\n    }\n    \n    for i, bucket :=\
          \ range h.buckets {\n        if v <= bucket {\n            h.counts[key][i]++\n        }\n    }\n    h.sums[key]\
          \ += v\n    h.totals[key]++\n}\n```\n\n```\n# Prometheus Exposition Format\nhttp_requests_total{method=\"GET\",status=\"\
          200\"} 1234\nhttp_requests_total{method=\"POST\",status=\"201\"} 567\nhttp_request_duration_seconds_bucket{le=\"\
          0.1\"} 500\nhttp_request_duration_seconds_bucket{le=\"0.5\"} 900\nhttp_request_duration_seconds_bucket{le=\"1.0\"\
          } 990\nhttp_request_duration_seconds_bucket{le=\"+Inf\"} 1000\nhttp_request_duration_seconds_sum 450.5\nhttp_request_duration_seconds_count\
          \ 1000\n```"
      pitfalls:
      - Not using labels effectively
      - Too many unique label values (cardinality explosion)
      - Forgetting thread safety
      concepts:
      - Time-series data
      - Metric types
      - Label cardinality
      estimated_hours: 6-10
    - id: 2
      name: Storage & Querying
      description: Implement time-series storage and basic query capabilities.
      acceptance_criteria:
      - Time-series storage
      - Data retention/compaction
      - Range queries
      - Aggregation functions
      hints:
        level1: Store data points as (timestamp, value) pairs grouped by metric+labels.
        level2: Use a simple append-only log with periodic compaction.
        level3: "## Time-Series Storage\n\n```go\ntype DataPoint struct {\n    Timestamp int64\n    Value     float64\n}\n\
          \ntype TimeSeries struct {\n    mu     sync.RWMutex\n    points []DataPoint\n    maxAge time.Duration\n}\n\nfunc\
          \ (ts *TimeSeries) Append(t int64, v float64) {\n    ts.mu.Lock()\n    ts.points = append(ts.points, DataPoint{t,\
          \ v})\n    ts.mu.Unlock()\n}\n\nfunc (ts *TimeSeries) Range(start, end int64) []DataPoint {\n    ts.mu.RLock()\n\
          \    defer ts.mu.RUnlock()\n    \n    // Binary search for start\n    i := sort.Search(len(ts.points), func(i int)\
          \ bool {\n        return ts.points[i].Timestamp >= start\n    })\n    \n    result := []DataPoint{}\n    for ; i\
          \ < len(ts.points) && ts.points[i].Timestamp <= end; i++ {\n        result = append(result, ts.points[i])\n    }\n\
          \    return result\n}\n\n// Compaction - remove old data\nfunc (ts *TimeSeries) Compact() {\n    cutoff := time.Now().Add(-ts.maxAge).UnixMilli()\n\
          \    ts.mu.Lock()\n    defer ts.mu.Unlock()\n    \n    i := sort.Search(len(ts.points), func(i int) bool {\n   \
          \     return ts.points[i].Timestamp >= cutoff\n    })\n    ts.points = ts.points[i:]\n}\n```\n\n```go\n// Query\
          \ Language (simplified PromQL-like)\ntype Query struct {\n    MetricName string\n    Labels     map[string]string\n\
          \    Start      int64\n    End        int64\n    Step       int64  // resolution\n    Aggregation string // sum,\
          \ avg, max, min, rate\n}\n\nfunc (db *MetricsDB) Execute(q Query) []DataPoint {\n    series := db.findSeries(q.MetricName,\
          \ q.Labels)\n    points := series.Range(q.Start, q.End)\n    \n    switch q.Aggregation {\n    case \"rate\":\n\
          \        return calculateRate(points, q.Step)\n    case \"avg\":\n        return downsampleAvg(points, q.Step)\n\
          \    case \"sum\":\n        return downsampleSum(points, q.Step)\n    default:\n        return points\n    }\n}\n\
          \nfunc calculateRate(points []DataPoint, step int64) []DataPoint {\n    if len(points) < 2 {\n        return nil\n\
          \    }\n    \n    result := []DataPoint{}\n    for i := 1; i < len(points); i++ {\n        dt := float64(points[i].Timestamp\
          \ - points[i-1].Timestamp) / 1000\n        dv := points[i].Value - points[i-1].Value\n        result = append(result,\
          \ DataPoint{\n            Timestamp: points[i].Timestamp,\n            Value:     dv / dt,  // rate per second\n\
          \        })\n    }\n    return result\n}\n```"
      pitfalls:
      - Not implementing compaction
      - Inefficient range queries
      - Memory issues with large datasets
      concepts:
      - Time-series databases
      - Data compaction
      - Query optimization
      estimated_hours: 6-10
    - id: 3
      name: Visualization Dashboard
      description: Build a web dashboard for visualizing metrics.
      acceptance_criteria:
      - Line charts for time-series
      - Dashboard configuration
      - Auto-refresh
      - Time range selection
      hints:
        level1: Use Chart.js or similar for rendering. Fetch data via API.
        level2: Support multiple panels per dashboard, configurable via JSON.
        level3: "## Dashboard Implementation\n\n```html\n<!-- dashboard.html -->\n<div id=\"dashboard\">\n  <div class=\"\
          controls\">\n    <select id=\"timeRange\">\n      <option value=\"5m\">Last 5 minutes</option>\n      <option value=\"\
          1h\" selected>Last 1 hour</option>\n      <option value=\"24h\">Last 24 hours</option>\n    </select>\n    <button\
          \ onclick=\"refresh()\">Refresh</button>\n    <label>\n      <input type=\"checkbox\" id=\"autoRefresh\" checked>\n\
          \      Auto-refresh (30s)\n    </label>\n  </div>\n  <div id=\"panels\" class=\"panels-grid\"></div>\n</div>\n```\n\
          \n```javascript\n// Dashboard Configuration\nconst dashboardConfig = {\n  title: \"Application Metrics\",\n  refreshInterval:\
          \ 30000,\n  panels: [\n    {\n      title: \"Request Rate\",\n      query: \"rate(http_requests_total[5m])\",\n\
          \      type: \"line\",\n      yAxis: { label: \"req/s\" }\n    },\n    {\n      title: \"Error Rate\",\n      query:\
          \ \"rate(http_requests_total{status=~'5..'}[5m])\",\n      type: \"line\",\n      thresholds: [{ value: 0.01, color:\
          \ \"red\" }]\n    },\n    {\n      title: \"Response Time P99\",\n      query: \"histogram_quantile(0.99, http_request_duration_bucket)\"\
          ,\n      type: \"line\",\n      yAxis: { label: \"seconds\" }\n    }\n  ]\n};\n\nasync function renderPanel(panel,\
          \ container) {\n  const timeRange = document.getElementById('timeRange').value;\n  const { start, end } = parseTimeRange(timeRange);\n\
          \  \n  const data = await fetch(\n    `/api/query?q=${encodeURIComponent(panel.query)}&start=${start}&end=${end}`\n\
          \  ).then(r => r.json());\n  \n  const ctx = container.querySelector('canvas').getContext('2d');\n  new Chart(ctx,\
          \ {\n    type: 'line',\n    data: {\n      labels: data.map(p => new Date(p.timestamp)),\n      datasets: [{\n \
          \       label: panel.title,\n        data: data.map(p => p.value),\n        borderColor: '#3498db',\n        tension:\
          \ 0.1\n      }]\n    },\n    options: {\n      responsive: true,\n      scales: {\n        x: { type: 'time' },\n\
          \        y: { title: { text: panel.yAxis?.label || '' } }\n      }\n    }\n  });\n}\n```"
      pitfalls:
      - Not handling missing data points
      - Chart performance with many points
      - Time zone issues
      concepts:
      - Data visualization
      - Dashboard design
      - Real-time updates
      estimated_hours: 6-10
    - id: 4
      name: Alerting System
      description: Implement alerting rules with notifications.
      acceptance_criteria:
      - Alert rule definitions
      - Threshold-based alerts
      - Alert states (pending/firing/resolved)
      - Notification channels
      hints:
        level1: Poll metrics at regular intervals, evaluate rules, track state changes.
        level2: Implement hysteresis (for duration) to avoid flapping alerts.
        level3: "## Alerting System\n\n```go\ntype AlertRule struct {\n    Name        string\n    Query       string\n  \
          \  Condition   string        // e.g., \"> 0.95\"\n    For         time.Duration // must be true for this long\n\
          \    Labels      map[string]string\n    Annotations map[string]string\n}\n\ntype AlertState int\nconst (\n    AlertInactive\
          \ AlertState = iota\n    AlertPending\n    AlertFiring\n)\n\ntype Alert struct {\n    Rule       *AlertRule\n  \
          \  State      AlertState\n    ActiveAt   time.Time\n    FiredAt    time.Time\n    ResolvedAt time.Time\n    Value\
          \      float64\n}\n\ntype AlertManager struct {\n    rules    []*AlertRule\n    alerts   map[string]*Alert  // rule\
          \ name -> alert\n    notifier Notifier\n}\n\nfunc (am *AlertManager) Evaluate(db *MetricsDB) {\n    for _, rule\
          \ := range am.rules {\n        value := db.QuerySingle(rule.Query)\n        triggered := evaluateCondition(value,\
          \ rule.Condition)\n        \n        alert, exists := am.alerts[rule.Name]\n        if !exists {\n            alert\
          \ = &Alert{Rule: rule, State: AlertInactive}\n            am.alerts[rule.Name] = alert\n        }\n        \n  \
          \      switch alert.State {\n        case AlertInactive:\n            if triggered {\n                alert.State\
          \ = AlertPending\n                alert.ActiveAt = time.Now()\n                alert.Value = value\n           \
          \ }\n            \n        case AlertPending:\n            if !triggered {\n                alert.State = AlertInactive\n\
          \            } else if time.Since(alert.ActiveAt) >= rule.For {\n                alert.State = AlertFiring\n   \
          \             alert.FiredAt = time.Now()\n                am.notifier.Send(alert)\n            }\n            \n\
          \        case AlertFiring:\n            if !triggered {\n                alert.State = AlertInactive\n         \
          \       alert.ResolvedAt = time.Now()\n                am.notifier.SendResolved(alert)\n            }\n        }\n\
          \    }\n}\n```\n\n```yaml\n# alert-rules.yaml\ngroups:\n  - name: application\n    rules:\n      - alert: HighErrorRate\n\
          \        query: rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m])\n        condition:\
          \ \"> 0.01\"\n        for: 5m\n        annotations:\n          summary: \"High error rate detected\"\n         \
          \ description: \"Error rate is {{ $value | printf \\\"%.2f\\\" }}%\"\n        \n      - alert: HighLatency\n   \
          \     query: histogram_quantile(0.99, http_request_duration_bucket)\n        condition: \"> 1.0\"\n        for:\
          \ 10m\n        annotations:\n          summary: \"P99 latency above 1 second\"\n```\n\n```go\n// Notification channels\n\
          type Notifier interface {\n    Send(alert *Alert) error\n    SendResolved(alert *Alert) error\n}\n\ntype SlackNotifier\
          \ struct {\n    webhookURL string\n}\n\nfunc (s *SlackNotifier) Send(alert *Alert) error {\n    payload := map[string]interface{}{\n\
          \        \"text\": fmt.Sprintf(\":rotating_light: ALERT: %s\\n%s\\nValue: %.4f\",\n            alert.Rule.Name,\n\
          \            alert.Rule.Annotations[\"description\"],\n            alert.Value),\n    }\n    body, _ := json.Marshal(payload)\n\
          \    _, err := http.Post(s.webhookURL, \"application/json\", bytes.NewReader(body))\n    return err\n}\n```"
      pitfalls:
      - Alert fatigue from too many alerts
      - Flapping alerts
      - Missing alert resolution notifications
      concepts:
      - Alert rules
      - State machines
      - Notification routing
      estimated_hours: 7-10
  mini-shell:
    id: mini-shell
    name: Mini Shell
    description: Build a feature-rich shell with job control. Learn process groups, signals, and terminal handling.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - Process spawner
    - Signal handling
    - Unix process model
    languages:
      recommended:
      - C
      - Rust
      also_possible:
      - Go
      - Zig
    resources:
    - type: article
      name: Writing Your Own Shell
      url: https://brennan.io/2015/01/16/write-a-shell-in-c/
    - type: book
      name: Advanced Programming in Unix Environment - Ch 9
      url: https://www.apuebook.com/
    milestones:
    - id: 1
      name: Command Execution
      description: Parse and execute simple commands with arguments.
      acceptance_criteria:
      - Parse command line into tokens
      - Handle quoted strings
      - Execute external commands
      - Basic builtins (cd, exit, pwd)
      hints:
        level1: Use strtok or write custom tokenizer. Quote handling needs state machine.
        level2: Builtins must run in shell process, not forked. cd changes shell's cwd.
        level3: "char **parse_line(char *line) {\n    // Simple tokenizer (no quote handling)\n    char **tokens = malloc(64\
          \ * sizeof(char*));\n    int pos = 0;\n    char *token = strtok(line, \" \\t\\n\");\n    while (token) {\n     \
          \   tokens[pos++] = token;\n        token = strtok(NULL, \" \\t\\n\");\n    }\n    tokens[pos] = NULL;\n    return\
          \ tokens;\n}\n\nint execute_command(char **args) {\n    // Handle builtins\n    if (strcmp(args[0], \"cd\") == 0)\
          \ {\n        return chdir(args[1] ? args[1] : getenv(\"HOME\"));\n    }\n    if (strcmp(args[0], \"exit\") == 0)\
          \ exit(0);\n    \n    // External command\n    pid_t pid = fork();\n    if (pid == 0) {\n        execvp(args[0],\
          \ args);\n        perror(\"execvp\");\n        exit(1);\n    }\n    int status;\n    waitpid(pid, &status, 0);\n\
          \    return WEXITSTATUS(status);\n}"
      pitfalls:
      - Forgetting to null-terminate args
      - Not handling empty input
      - Memory leaks in tokenizer
      concepts:
      - Lexical analysis
      - fork/exec pattern
      - Process creation
      estimated_hours: 4-6
    - id: 2
      name: Pipes and Redirection
      description: Implement pipelines and I/O redirection.
      acceptance_criteria:
      - Input/output redirection (<, >)
      - Append redirection (>>)
      - Pipeline (cmd1 | cmd2 | cmd3)
      - Handle file creation modes
      hints:
        level1: Parse redirection before executing. Use dup2 to redirect fds.
        level2: Pipes need careful fd management - close unused ends. Create all pipes before forking.
        level3: "void setup_pipeline(Command *cmds, int n) {\n    int pipes[n-1][2];  // n-1 pipes for n commands\n    \n\
          \    // Create all pipes first\n    for (int i = 0; i < n-1; i++) {\n        pipe(pipes[i]);\n    }\n    \n    for\
          \ (int i = 0; i < n; i++) {\n        pid_t pid = fork();\n        if (pid == 0) {\n            // Connect to previous\
          \ pipe (read end)\n            if (i > 0) {\n                dup2(pipes[i-1][0], STDIN_FILENO);\n            }\n\
          \            // Connect to next pipe (write end)\n            if (i < n-1) {\n                dup2(pipes[i][1],\
          \ STDOUT_FILENO);\n            }\n            \n            // Close all pipe fds in child\n            for (int\
          \ j = 0; j < n-1; j++) {\n                close(pipes[j][0]);\n                close(pipes[j][1]);\n           \
          \ }\n            \n            execvp(cmds[i].args[0], cmds[i].args);\n            exit(1);\n        }\n    }\n\
          \    \n    // Parent closes all pipes\n    for (int i = 0; i < n-1; i++) {\n        close(pipes[i][0]);\n      \
          \  close(pipes[i][1]);\n    }\n    \n    // Wait for all children\n    for (int i = 0; i < n; i++) {\n        wait(NULL);\n\
          \    }\n}"
      pitfalls:
      - Fd leaks causing hangs
      - Not closing pipe ends
      - Wrong redirection precedence
      concepts:
      - File descriptors
      - dup2
      - Pipe communication
      estimated_hours: 5-8
    - id: 3
      name: Job Control
      description: Implement background jobs and job management.
      acceptance_criteria:
      - Background execution (&)
      - Job listing (jobs)
      - Foreground/background (fg, bg)
      - Process group management
      hints:
        level1: Each job needs its own process group. Use setpgid in child and parent.
        level2: Shell must give terminal to foreground job. Use tcsetpgrp.
        level3: "typedef struct Job {\n    int id;\n    pid_t pgid;\n    char *command;\n    int status;  // RUNNING, STOPPED,\
          \ DONE\n    struct Job *next;\n} Job;\n\nJob *jobs = NULL;\nint next_job_id = 1;\n\nvoid launch_job(char **args,\
          \ int background) {\n    pid_t pid = fork();\n    if (pid == 0) {\n        // Child: create new process group\n\
          \        setpgid(0, 0);\n        \n        // If foreground, take terminal\n        if (!background) {\n       \
          \     tcsetpgrp(STDIN_FILENO, getpid());\n        }\n        \n        // Reset signal handlers\n        signal(SIGINT,\
          \ SIG_DFL);\n        signal(SIGTSTP, SIG_DFL);\n        \n        execvp(args[0], args);\n        exit(1);\n   \
          \ }\n    \n    // Parent\n    setpgid(pid, pid);  // Avoid race condition\n    \n    Job *job = add_job(pid, args);\n\
          \    \n    if (background) {\n        printf(\"[%d] %d\\n\", job->id, pid);\n    } else {\n        // Wait for foreground\
          \ job\n        wait_for_job(job);\n        // Take back terminal\n        tcsetpgrp(STDIN_FILENO, getpgrp());\n\
          \    }\n}"
      pitfalls:
      - Race conditions with setpgid
      - Terminal control issues
      - Zombie processes
      concepts:
      - Process groups
      - Session management
      - Terminal control
      estimated_hours: 8-12
    - id: 4
      name: Signal Handling
      description: Handle Ctrl+C, Ctrl+Z and job notifications properly.
      acceptance_criteria:
      - SIGINT goes to foreground job only
      - SIGTSTP suspends foreground job
      - SIGCHLD for background job completion
      - Proper signal masking
      hints:
        level1: Shell ignores SIGINT/SIGTSTP, children restore defaults.
        level2: Use SIGCHLD handler to reap background jobs. Mask signals during critical sections.
        level3: "volatile sig_atomic_t sigchld_received = 0;\n\nvoid sigchld_handler(int sig) {\n    sigchld_received = 1;\n\
          }\n\nvoid reap_children() {\n    int status;\n    pid_t pid;\n    \n    while ((pid = waitpid(-1, &status, WNOHANG\
          \ | WUNTRACED | WCONTINUED)) > 0) {\n        Job *job = find_job_by_pid(pid);\n        if (!job) continue;\n   \
          \     \n        if (WIFEXITED(status) || WIFSIGNALED(status)) {\n            printf(\"\\n[%d] Done: %s\\n\", job->id,\
          \ job->command);\n            remove_job(job);\n        } else if (WIFSTOPPED(status)) {\n            job->status\
          \ = STOPPED;\n            printf(\"\\n[%d] Stopped: %s\\n\", job->id, job->command);\n        } else if (WIFCONTINUED(status))\
          \ {\n            job->status = RUNNING;\n        }\n    }\n}\n\nvoid setup_shell_signals() {\n    // Shell ignores\
          \ job-control signals\n    signal(SIGINT, SIG_IGN);\n    signal(SIGTSTP, SIG_IGN);\n    signal(SIGTTIN, SIG_IGN);\n\
          \    signal(SIGTTOU, SIG_IGN);\n    \n    // But catches SIGCHLD\n    struct sigaction sa;\n    sa.sa_handler =\
          \ sigchld_handler;\n    sigemptyset(&sa.sa_mask);\n    sa.sa_flags = SA_RESTART;\n    sigaction(SIGCHLD, &sa, NULL);\n\
          }"
      pitfalls:
      - Signal handler races
      - Async-signal-safe functions
      - Interrupted system calls
      concepts:
      - Signal masking
      - Async-signal safety
      - Job notification
      estimated_hours: 8-14
  neural-network-basic:
    id: neural-network-basic
    name: Neural Network (micrograd)
    description: Build a minimal neural network library with automatic differentiation. Inspired by Andrej Karpathy's micrograd.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Calculus (derivatives)
    - Python basics
    - Linear algebra basics
    languages:
      recommended:
      - Python
      also_possible:
      - Julia
      - JavaScript
    resources:
    - name: Micrograd Repository
      url: https://github.com/karpathy/micrograd
      type: code
    - name: Micrograd Video Tutorial
      url: https://www.youtube.com/watch?v=VMj-3S1tku0
      type: video
    milestones:
    - id: 1
      name: Value Class with Autograd
      description: Create Value class that tracks computation graph for automatic differentiation.
      acceptance_criteria:
      - Value wraps scalar number
      - Tracks children (operands)
      - Tracks operation type
      - Stores gradient
      - Supports +, *, -, /, **
      hints:
        level1: 'Value stores: data, grad, _backward function, _prev set.'
        level2: Each operation creates new Value with backward function.
        level3: "class Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad\
          \ = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n    \n\
          \    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out\
          \ = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n\
          \            other.grad += out.grad\n        out._backward = _backward\n        return out\n    \n    def __mul__(self,\
          \ other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data *\
          \ other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n  \
          \          other.grad += self.data * out.grad\n        out._backward = _backward\n        return out"
      pitfalls:
      - Forgetting += for gradients (accumulation)
      - Not handling scalar + Value
      - Backward called before forward
      concepts:
      - Computational graphs
      - Chain rule
      - Gradient accumulation
      estimated_hours: 4-6
    - id: 2
      name: Backward Pass
      description: Implement backpropagation through the computation graph.
      acceptance_criteria:
      - Topological sort of nodes
      - Backward pass in reverse order
      - Zero gradients before backward
      - Handle multiple uses of same value
      hints:
        level1: 'Topological sort: visit children before parents.'
        level2: Start with output.grad = 1, then propagate.
        level3: "def backward(self):\n    # Topological sort\n    topo = []\n    visited = set()\n    def build_topo(v):\n\
          \        if v not in visited:\n            visited.add(v)\n            for child in v._prev:\n                build_topo(child)\n\
          \            topo.append(v)\n    build_topo(self)\n    \n    # Backward pass\n    self.grad = 1\n    for v in reversed(topo):\n\
          \        v._backward()"
      pitfalls:
      - Not zeroing gradients
      - Wrong topological order
      - Gradient through constants
      concepts:
      - Backpropagation
      - Topological sort
      - Reverse-mode AD
      estimated_hours: 2-3
    - id: 3
      name: Neuron and Layer
      description: Build neural network components using Value.
      acceptance_criteria:
      - Neuron with weights and bias
      - Activation function (tanh/ReLU)
      - Layer of neurons
      - MLP (Multi-Layer Perceptron)
      hints:
        level1: 'Neuron: sum(w*x) + b, then activation.'
        level2: 'Layer: list of neurons. MLP: list of layers.'
        level3: "import random\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,\
          \ 1)) for _ in range(nin)]\n        self.b = Value(0)\n    \n    def __call__(self, x):\n        act = sum((wi*xi\
          \ for wi, xi in zip(self.w, x)), self.b)\n        return act.tanh()\n    \n    def parameters(self):\n        return\
          \ self.w + [self.b]\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for\
          \ _ in range(nout)]\n    \n    def __call__(self, x):\n        return [n(x) for n in self.neurons]\n    \n    def\
          \ parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\nclass MLP:\n    def __init__(self,\
          \ nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\
          \    \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x[0]\
          \ if len(x) == 1 else x"
      pitfalls:
      - Not initializing weights properly
      - Missing activation on last layer
      - Parameter collection incomplete
      concepts:
      - Neural network architecture
      - Activation functions
      - Parameter management
      estimated_hours: 3-4
    - id: 4
      name: Training Loop
      description: Train the neural network with gradient descent.
      acceptance_criteria:
      - Forward pass (prediction)
      - Loss calculation (MSE)
      - Backward pass
      - Gradient descent update
      - Multiple epochs
      hints:
        level1: loss = sum((ypred - ytrue)**2)
        level2: 'After backward: param.data -= learning_rate * param.grad'
        level3: "# Training data\nxs = [[2.0, 3.0], [-1.0, -2.0], [3.0, -1.0]]\nys = [1.0, -1.0, 1.0]\n\n# Training loop\n\
          model = MLP(2, [4, 4, 1])  # 2 inputs, 2 hidden layers of 4, 1 output\nlearning_rate = 0.05\n\nfor epoch in range(100):\n\
          \    # Forward pass\n    ypred = [model(x) for x in xs]\n    loss = sum((yp - yt)**2 for yp, yt in zip(ypred, ys))\n\
          \    \n    # Zero gradients\n    for p in model.parameters():\n        p.grad = 0\n    \n    # Backward pass\n \
          \   loss.backward()\n    \n    # Update weights\n    for p in model.parameters():\n        p.data -= learning_rate\
          \ * p.grad\n    \n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Loss: {loss.data}')"
      pitfalls:
      - Forgetting to zero gradients
      - Learning rate too high
      - Not enough epochs
      concepts:
      - Training loop
      - Gradient descent
      - Loss functions
      estimated_hours: 3-4
  packet-sniffer:
    id: packet-sniffer
    name: Packet Sniffer
    description: Build a network packet capture and analysis tool using raw sockets or libpcap. Learn network protocols and
      packet parsing.
    difficulty: intermediate
    estimated_hours: 20-30
    prerequisites:
    - Networking basics
    - TCP/IP model
    - Binary parsing
    languages:
      recommended:
      - C
      - Python
      - Go
      also_possible:
      - Rust
    resources:
    - name: Libpcap Programming Tutorial
      url: https://www.tcpdump.org/pcap.html
      type: tutorial
    - name: Building a Packet Sniffer from Scratch
      url: https://aidanvidal.github.io/posts/Packet_Sniffer.html
      type: tutorial
    - name: Scapy Documentation
      url: https://scapy.readthedocs.io/
      type: reference
    milestones:
    - id: 1
      name: Packet Capture Setup
      description: Set up packet capture using raw sockets or libpcap.
      acceptance_criteria:
      - List available network interfaces
      - Open interface for capture
      - Capture packets in promiscuous mode
      - Handle root/admin permissions
      hints:
        level1: 'libpcap: pcap_findalldevs() lists interfaces, pcap_open_live() opens for capture.'
        level2: Promiscuous mode captures all traffic, not just for your host. Requires root.
        level3: "# Python with scapy (simpler) or raw sockets\nfrom scapy.all import sniff, get_if_list\n\ndef list_interfaces():\n\
          \    return get_if_list()\n\ndef capture_packets(interface: str, count: int = 10):\n    '''Capture packets and return\
          \ them'''\n    packets = sniff(iface=interface, count=count, promisc=True)\n    return packets\n\n# Or with raw\
          \ sockets (Linux)\nimport socket\n\ndef raw_capture(interface: str):\n    # Create raw socket\n    sock = socket.socket(socket.AF_PACKET,\
          \ socket.SOCK_RAW, socket.ntohs(0x0003))\n    sock.bind((interface, 0))\n    sock.setblocking(False)\n\n    while\
          \ True:\n        try:\n            packet, addr = sock.recvfrom(65535)\n            yield packet\n        except\
          \ BlockingIOError:\n            continue\n\n# With libpcap (C)\n'''\npcap_t *handle;\nchar errbuf[PCAP_ERRBUF_SIZE];\n\
          handle = pcap_open_live(\"eth0\", BUFSIZ, 1, 1000, errbuf);\nif (handle == NULL) {\n    fprintf(stderr, \"Couldn't\
          \ open device: %s\\n\", errbuf);\n    return 1;\n}\n'''"
      pitfalls:
      - Requires root/admin privileges
      - Interface names vary by OS (eth0, en0, etc.)
      - Virtual interfaces may not support promiscuous mode
      concepts:
      - Raw sockets
      - Network interfaces
      - Privilege requirements
      estimated_hours: 4-5
    - id: 2
      name: Ethernet Frame Parsing
      description: Parse Ethernet frames to extract MAC addresses and protocol type.
      acceptance_criteria:
      - Extract destination MAC (6 bytes)
      - Extract source MAC (6 bytes)
      - Extract EtherType (2 bytes)
      - Format MACs as aa:bb:cc:dd:ee:ff
      hints:
        level1: 'Ethernet header is 14 bytes: dst(6) + src(6) + type(2). Type 0x0800 = IPv4.'
        level2: Use struct.unpack() for parsing. MAC is 6 bytes, format each as hex.
        level3: "from struct import unpack\n\nclass EthernetFrame:\n    def __init__(self, data: bytes):\n        # Unpack\
          \ ethernet header (14 bytes)\n        # ! = network byte order (big endian)\n        self.dst_mac = data[0:6]\n\
          \        self.src_mac = data[6:12]\n        self.ethertype = unpack('!H', data[12:14])[0]\n        self.payload\
          \ = data[14:]\n\n    @staticmethod\n    def format_mac(mac: bytes) -> str:\n        return ':'.join(f'{b:02x}' for\
          \ b in mac)\n\n    def __str__(self):\n        proto = {0x0800: 'IPv4', 0x0806: 'ARP', 0x86dd: 'IPv6'}.get(\n  \
          \          self.ethertype, f'0x{self.ethertype:04x}'\n        )\n        return (f\"Ethernet: {self.format_mac(self.src_mac)}\
          \ -> \"\n                f\"{self.format_mac(self.dst_mac)} ({proto})\")\n\n# Parse captured packet\nframe = EthernetFrame(raw_packet)\n\
          print(frame)\n# Ethernet: aa:bb:cc:dd:ee:ff -> 11:22:33:44:55:66 (IPv4)"
      pitfalls:
      - 802.1Q VLAN tags add 4 extra bytes
      - Jumbo frames can exceed 1500 bytes payload
      - Byte order is big-endian (network order)
      concepts:
      - Ethernet framing
      - MAC addresses
      - EtherType
      estimated_hours: 3-4
    - id: 3
      name: IP Header Parsing
      description: Parse IPv4 headers to extract addresses and protocol.
      acceptance_criteria:
      - Extract version and header length
      - Extract total length, TTL, protocol
      - Extract source and destination IP
      - Handle IP options (variable header length)
      hints:
        level1: 'IPv4 header: first nibble = version (4), second nibble = IHL (header length in 32-bit words).'
        level2: 'Protocol field: 1=ICMP, 6=TCP, 17=UDP. IPs are 4 bytes each at offset 12 and 16.'
        level3: "class IPv4Packet:\n    def __init__(self, data: bytes):\n        # First byte: version (4 bits) + IHL (4\
          \ bits)\n        version_ihl = data[0]\n        self.version = version_ihl >> 4\n        self.ihl = version_ihl\
          \ & 0x0F  # Header length in 32-bit words\n        self.header_length = self.ihl * 4\n\n        self.dscp_ecn =\
          \ data[1]\n        self.total_length = unpack('!H', data[2:4])[0]\n        self.identification = unpack('!H', data[4:6])[0]\n\
          \n        flags_frag = unpack('!H', data[6:8])[0]\n        self.flags = flags_frag >> 13\n        self.fragment_offset\
          \ = flags_frag & 0x1FFF\n\n        self.ttl = data[8]\n        self.protocol = data[9]\n        self.checksum =\
          \ unpack('!H', data[10:12])[0]\n\n        self.src_ip = data[12:16]\n        self.dst_ip = data[16:20]\n\n     \
          \   # Options (if any)\n        self.options = data[20:self.header_length] if self.ihl > 5 else b''\n\n        self.payload\
          \ = data[self.header_length:]\n\n    @staticmethod\n    def format_ip(ip: bytes) -> str:\n        return '.'.join(str(b)\
          \ for b in ip)\n\n    def __str__(self):\n        proto = {1: 'ICMP', 6: 'TCP', 17: 'UDP'}.get(self.protocol, str(self.protocol))\n\
          \        return (f\"IPv4: {self.format_ip(self.src_ip)} -> \"\n                f\"{self.format_ip(self.dst_ip)}\
          \ ({proto}, TTL={self.ttl})\")"
      pitfalls:
      - Header length is in 32-bit words, not bytes
      - Fragmented packets need reassembly
      - Options make header length variable
      concepts:
      - IP addressing
      - Protocol numbers
      - Header fields
      estimated_hours: 4-5
    - id: 4
      name: TCP/UDP Parsing
      description: Parse TCP and UDP headers for port information and flags.
      acceptance_criteria:
      - Extract source and destination ports
      - 'For TCP: extract flags (SYN, ACK, FIN, etc.)'
      - 'For TCP: sequence and acknowledgment numbers'
      - Identify common services by port
      hints:
        level1: 'TCP header: ports(4) + seq(4) + ack(4) + flags(2) + window(2) + checksum(2) + urgent(2) = 20+ bytes'
        level2: 'UDP is simpler: ports(4) + length(2) + checksum(2) = 8 bytes total.'
        level3: "class TCPSegment:\n    def __init__(self, data: bytes):\n        self.src_port = unpack('!H', data[0:2])[0]\n\
          \        self.dst_port = unpack('!H', data[2:4])[0]\n        self.seq_num = unpack('!I', data[4:8])[0]\n       \
          \ self.ack_num = unpack('!I', data[8:12])[0]\n\n        data_offset_flags = unpack('!H', data[12:14])[0]\n     \
          \   self.data_offset = (data_offset_flags >> 12) * 4  # In bytes\n        self.flags = data_offset_flags & 0x1FF\n\
          \n        self.window = unpack('!H', data[14:16])[0]\n        self.checksum = unpack('!H', data[16:18])[0]\n   \
          \     self.urgent = unpack('!H', data[18:20])[0]\n\n        self.payload = data[self.data_offset:]\n\n    @property\n\
          \    def flag_str(self) -> str:\n        flags = []\n        if self.flags & 0x001: flags.append('FIN')\n      \
          \  if self.flags & 0x002: flags.append('SYN')\n        if self.flags & 0x004: flags.append('RST')\n        if self.flags\
          \ & 0x008: flags.append('PSH')\n        if self.flags & 0x010: flags.append('ACK')\n        if self.flags & 0x020:\
          \ flags.append('URG')\n        return ','.join(flags) or 'none'\n\nclass UDPDatagram:\n    def __init__(self, data:\
          \ bytes):\n        self.src_port = unpack('!H', data[0:2])[0]\n        self.dst_port = unpack('!H', data[2:4])[0]\n\
          \        self.length = unpack('!H', data[4:6])[0]\n        self.checksum = unpack('!H', data[6:8])[0]\n        self.payload\
          \ = data[8:]\n\n# Common ports\nSERVICES = {80: 'HTTP', 443: 'HTTPS', 22: 'SSH', 53: 'DNS', 25: 'SMTP'}"
      pitfalls:
      - TCP data offset is in 32-bit words
      - Port numbers are unsigned 16-bit
      - Don't confuse TCP and UDP despite similar port fields
      concepts:
      - TCP flags
      - Port numbers
      - Transport layer
      estimated_hours: 4-5
    - id: 5
      name: Filtering and Output
      description: Add BPF filters and formatted output display.
      acceptance_criteria:
      - Support BPF filter expressions
      - Filter by protocol (tcp, udp, icmp)
      - Filter by port or IP
      - Formatted packet summary output
      hints:
        level1: 'BPF syntax: ''tcp port 80'', ''host 192.168.1.1'', ''udp and port 53'''
        level2: pcap_compile() and pcap_setfilter() apply BPF filters before capture.
        level3: "# Using scapy's BPF filter\nfrom scapy.all import sniff\n\ndef capture_with_filter(interface: str, bpf_filter:\
          \ str, callback):\n    '''Capture packets matching BPF filter'''\n    sniff(iface=interface, filter=bpf_filter,\
          \ prn=callback)\n\ndef packet_callback(packet):\n    '''Process and display packet'''\n    timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]\n\
          \n    # Build summary\n    summary = f\"{timestamp} \"\n\n    if packet.haslayer('Ether'):\n        eth = packet['Ether']\n\
          \n    if packet.haslayer('IP'):\n        ip = packet['IP']\n        summary += f\"{ip.src} -> {ip.dst} \"\n\n  \
          \      if packet.haslayer('TCP'):\n            tcp = packet['TCP']\n            flags = tcp.sprintf('%TCP.flags%')\n\
          \            summary += f\"TCP {tcp.sport} -> {tcp.dport} [{flags}]\"\n        elif packet.haslayer('UDP'):\n  \
          \          udp = packet['UDP']\n            summary += f\"UDP {udp.sport} -> {udp.dport}\"\n        elif packet.haslayer('ICMP'):\n\
          \            icmp = packet['ICMP']\n            summary += f\"ICMP type={icmp.type}\"\n\n    print(summary)\n\n\
          # Example filters:\n# \"tcp port 80\"       - HTTP traffic\n# \"udp port 53\"       - DNS queries\n# \"host 192.168.1.1\"\
          \  - All traffic to/from IP\n# \"tcp[tcpflags] & tcp-syn != 0\"  - TCP SYN packets"
      pitfalls:
      - BPF syntax errors are cryptic
      - High traffic can overwhelm display
      - Timestamps should be high-resolution
      concepts:
      - Berkeley Packet Filter
      - Traffic filtering
      - Packet analysis
      estimated_hours: 5-6
  password-hashing:
    id: password-hashing
    name: Password Hashing
    description: Implement secure password hashing with salt. Learn cryptographic security concepts and why plain hashing
      is insufficient.
    difficulty: beginner
    estimated_hours: 4-6
    prerequisites:
    - Basic programming
    - Understanding of hashing
    languages:
      recommended:
      - Python
      - Go
      - JavaScript
      also_possible:
      - Java
      - C#
    resources:
    - name: How to Safely Store Passwords
      url: https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html
      type: documentation
    milestones:
    - id: 1
      name: Basic Hashing with Salt
      description: Implement salted password hashing.
      acceptance_criteria:
      - Generate random salt
      - Hash password with salt
      - Store salt with hash
      - Verify password against stored hash
      hints:
        level1: Salt prevents rainbow table attacks.
        level2: Concatenate salt + password, then hash.
        level3: "import hashlib\nimport os\n\ndef hash_password(password):\n    salt = os.urandom(16)  # 16 bytes of random\
          \ salt\n    combined = salt + password.encode()\n    hash_bytes = hashlib.sha256(combined).digest()\n    # Store\
          \ salt + hash together\n    return salt + hash_bytes\n\ndef verify_password(password, stored):\n    salt = stored[:16]\n\
          \    stored_hash = stored[16:]\n    combined = salt + password.encode()\n    computed_hash = hashlib.sha256(combined).digest()\n\
          \    return computed_hash == stored_hash"
      pitfalls:
      - Using predictable salt
      - Not storing salt
      - Timing attacks in comparison
      concepts:
      - Salting
      - Rainbow tables
      - Secure random generation
      estimated_hours: 1-2
    - id: 2
      name: Key Stretching
      description: Implement iterated hashing for slow verification.
      acceptance_criteria:
      - Multiple hash iterations
      - Configurable iteration count
      - PBKDF2 algorithm
      - Constant-time comparison
      hints:
        level1: More iterations = slower brute force.
        level2: PBKDF2 is standard key derivation function.
        level3: "import hashlib\nimport hmac\n\ndef pbkdf2_hash(password, salt, iterations=100000):\n    # PBKDF2-HMAC-SHA256\n\
          \    return hashlib.pbkdf2_hmac(\n        'sha256',\n        password.encode(),\n        salt,\n        iterations\n\
          \    )\n\ndef constant_time_compare(a, b):\n    \"\"\"Prevent timing attacks\"\"\"\n    if len(a) != len(b):\n \
          \       return False\n    result = 0\n    for x, y in zip(a, b):\n        result |= x ^ y\n    return result ==\
          \ 0"
      pitfalls:
      - Too few iterations
      - Not using constant-time compare
      - iteration count not stored
      concepts:
      - Key stretching
      - PBKDF2
      - Timing attacks
      estimated_hours: 1-2
    - id: 3
      name: Modern Password Hashing
      description: Implement or use bcrypt/Argon2.
      acceptance_criteria:
      - Bcrypt format understanding
      - Work factor configuration
      - Argon2 (optional)
      - Migration strategy for old hashes
      hints:
        level1: Bcrypt includes salt and cost in output.
        level2: Argon2 is memory-hard, resistant to GPU attacks.
        level3: "# Using bcrypt library (recommended for production)\nimport bcrypt\n\ndef hash_password_bcrypt(password):\n\
          \    # Cost factor of 12 is reasonable for 2024\n    return bcrypt.hashpw(password.encode(), bcrypt.gensalt(rounds=12))\n\
          \ndef verify_password_bcrypt(password, hashed):\n    return bcrypt.checkpw(password.encode(), hashed)\n\n# Bcrypt\
          \ output format: $2b$12$salthere...hashhere\n# $2b$ = algorithm, 12 = cost factor, then 22-char salt + 31-char hash\n\
          \n# Migration strategy\ndef verify_with_migration(password, stored_hash):\n    if stored_hash.startswith('$2b$'):\n\
          \        return verify_password_bcrypt(password, stored_hash)\n    else:\n        # Old format - verify then upgrade\n\
          \        if verify_old_format(password, stored_hash):\n            return True, hash_password_bcrypt(password) \
          \ # New hash to store\n        return False, None"
      pitfalls:
      - Implementing crypto yourself
      - Too low work factor
      - Not planning for algorithm upgrades
      concepts:
      - Bcrypt
      - Argon2
      - Algorithm agility
      estimated_hours: 1-2
  platformer:
    id: platformer
    name: Platformer
    description: Build a 2D platformer with physics, jumping, and collision detection. Learn game physics and level design.
    difficulty: intermediate
    estimated_hours: 20-30
    prerequisites:
    - Basic game loop
    - 2D graphics
    - Basic physics
    languages:
      recommended:
      - JavaScript
      - Python
      - C#
      also_possible:
      - C++
      - Lua
    resources:
    - name: 2D Platformer Tutorial
      url: https://www.youtube.com/results?search_query=2d+platformer+tutorial
      type: video
    milestones:
    - id: 1
      name: Basic Movement and Gravity
      description: Implement player movement with gravity.
      acceptance_criteria:
      - Left/right movement with arrow keys
      - Gravity pulls player down
      - Ground collision stops falling
      - Velocity-based movement
      hints:
        level1: velocity.y += gravity each frame. position += velocity.
        level2: Check if player bottom > ground level to detect landing.
        level3: "class Player:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        self.vx = 0\n\
          \        self.vy = 0\n        self.width = 32\n        self.height = 48\n        self.on_ground = False\n    \n\
          \    def update(self, dt):\n        # Gravity\n        self.vy += GRAVITY * dt\n        \n        # Horizontal movement\n\
          \        if keys['left']:\n            self.vx = -MOVE_SPEED\n        elif keys['right']:\n            self.vx =\
          \ MOVE_SPEED\n        else:\n            self.vx = 0\n        \n        # Apply velocity\n        self.x += self.vx\
          \ * dt\n        self.y += self.vy * dt\n        \n        # Ground collision\n        if self.y + self.height >\
          \ GROUND_Y:\n            self.y = GROUND_Y - self.height\n            self.vy = 0\n            self.on_ground =\
          \ True"
      pitfalls:
      - Gravity too strong or weak
      - Not using delta time
      - Ground clipping
      concepts:
      - Physics simulation
      - Velocity and acceleration
      - Delta time
      estimated_hours: 3-4
    - id: 2
      name: Jumping
      description: Implement jump mechanics with variable height.
      acceptance_criteria:
      - Jump when on ground
      - Variable jump height (hold to jump higher)
      - Coyote time (jump just after leaving edge)
      - Jump buffer (press before landing)
      hints:
        level1: Jump = set negative vy. Only allow when on_ground.
        level2: Cut jump short by reducing vy when button released.
        level3: "JUMP_VELOCITY = -300\nJUMP_CUT_MULTIPLIER = 0.5\nCOYOTE_TIME = 0.1  # seconds\nJUMP_BUFFER = 0.1\n\nclass\
          \ Player:\n    def __init__(self):\n        self.coyote_timer = 0\n        self.jump_buffer_timer = 0\n    \n  \
          \  def update(self, dt):\n        # Coyote time\n        if self.on_ground:\n            self.coyote_timer = COYOTE_TIME\n\
          \        else:\n            self.coyote_timer -= dt\n        \n        # Jump buffer\n        if keys_pressed['jump']:\n\
          \            self.jump_buffer_timer = JUMP_BUFFER\n        else:\n            self.jump_buffer_timer -= dt\n   \
          \     \n        # Jump\n        can_jump = self.coyote_timer > 0\n        want_jump = self.jump_buffer_timer > 0\n\
          \        if can_jump and want_jump:\n            self.vy = JUMP_VELOCITY\n            self.coyote_timer = 0\n  \
          \          self.jump_buffer_timer = 0\n        \n        # Variable jump height\n        if keys_released['jump']\
          \ and self.vy < 0:\n            self.vy *= JUMP_CUT_MULTIPLIER"
      pitfalls:
      - Double jump without intending
      - Jump feels floaty
      - Coyote time too generous
      concepts:
      - Jump mechanics
      - Coyote time
      - Input buffering
      estimated_hours: 3-4
    - id: 3
      name: Tile-based Collision
      description: Implement collision with tile-based level.
      acceptance_criteria:
      - Load tile map
      - Collision with solid tiles
      - Separate X and Y collision resolution
      - Slope support (optional)
      hints:
        level1: Check which tiles player overlaps with.
        level2: Resolve X and Y separately to handle corners.
        level3: "def resolve_collisions(self, tilemap):\n    # Move X, resolve X collisions\n    self.x += self.vx * dt\n\
          \    for tile in tilemap.get_colliding_tiles(self.rect):\n        if self.vx > 0:  # Moving right\n            self.x\
          \ = tile.left - self.width\n        elif self.vx < 0:  # Moving left\n            self.x = tile.right\n        self.vx\
          \ = 0\n    \n    # Move Y, resolve Y collisions\n    self.y += self.vy * dt\n    self.on_ground = False\n    for\
          \ tile in tilemap.get_colliding_tiles(self.rect):\n        if self.vy > 0:  # Falling\n            self.y = tile.top\
          \ - self.height\n            self.on_ground = True\n        elif self.vy < 0:  # Jumping\n            self.y = tile.bottom\n\
          \        self.vy = 0\n\nclass Tilemap:\n    def get_colliding_tiles(self, rect):\n        tiles = []\n        x1,\
          \ y1 = int(rect.left // TILE_SIZE), int(rect.top // TILE_SIZE)\n        x2, y2 = int(rect.right // TILE_SIZE), int(rect.bottom\
          \ // TILE_SIZE)\n        for y in range(y1, y2 + 1):\n            for x in range(x1, x2 + 1):\n                if\
          \ self.is_solid(x, y):\n                    tiles.append(Rect(x * TILE_SIZE, y * TILE_SIZE, TILE_SIZE, TILE_SIZE))\n\
          \        return tiles"
      pitfalls:
      - Corner clipping
      - Tunneling at high speeds
      - Off-by-one in tile lookup
      concepts:
      - AABB collision
      - Collision resolution
      - Tile maps
      estimated_hours: 5-6
    - id: 4
      name: Enemies and Hazards
      description: Add enemies and death/respawn system.
      acceptance_criteria:
      - Simple enemy with patrol behavior
      - Player dies on enemy contact
      - Respawn at checkpoint
      - Stomping enemies (optional)
      hints:
        level1: Enemy walks left/right, reverses at edges or walls.
        level2: Check player-enemy overlap for damage.
        level3: "class Enemy:\n    def __init__(self, x, y, patrol_distance):\n        self.x = x\n        self.y = y\n  \
          \      self.start_x = x\n        self.direction = 1\n        self.patrol_distance = patrol_distance\n    \n    def\
          \ update(self, dt, tilemap):\n        self.x += ENEMY_SPEED * self.direction * dt\n        \n        # Reverse at\
          \ patrol bounds\n        if abs(self.x - self.start_x) > self.patrol_distance:\n            self.direction *= -1\n\
          \        \n        # Reverse at walls or edges\n        next_tile_x = int((self.x + self.width/2 + self.direction\
          \ * self.width/2) / TILE_SIZE)\n        floor_tile_y = int((self.y + self.height + 1) / TILE_SIZE)\n        if not\
          \ tilemap.is_solid(next_tile_x, floor_tile_y):  # No floor ahead\n            self.direction *= -1\n\nclass Player:\n\
          \    def check_enemy_collision(self, enemies):\n        for enemy in enemies:\n            if self.rect.colliderect(enemy.rect):\n\
          \                if self.vy > 0 and self.y + self.height < enemy.y + 10:\n                    # Stomped enemy\n\
          \                    enemy.die()\n                    self.vy = BOUNCE_VELOCITY\n                else:\n       \
          \             # Player takes damage\n                    self.die()"
      pitfalls:
      - Stomp detection window
      - Death during invincibility
      - Enemy stuck at patrol bounds
      concepts:
      - AI patrol behavior
      - Health systems
      - Collision types
      estimated_hours: 4-6
  pong:
    id: pong
    name: Pong
    description: Build the classic Pong game. Learn game loop fundamentals, collision detection, and basic game AI.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - Basic programming
    - HTML5 Canvas or similar graphics
    languages:
      recommended:
      - JavaScript
      - Python
      - C#
      also_possible:
      - Rust
      - Go
      - Lua
    resources:
    - name: Pong with JavaScript
      url: https://www.youtube.com/watch?v=nl0KXCa5pJk
      type: video
    - name: HTML5 Canvas Tutorial
      url: https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial
      type: documentation
    milestones:
    - id: 1
      name: Game Setup & Ball Movement
      description: Set up the game canvas and implement ball movement.
      acceptance_criteria:
      - Canvas renders at 60fps
      - Ball appears on screen
      - Ball moves in a direction
      - Ball bounces off top and bottom walls
      - Ball resets when going off sides
      hints:
        level1: Use requestAnimationFrame for smooth 60fps game loop.
        level2: 'Ball velocity: { vx, vy }. Update position: x += vx, y += vy.'
        level3: "const ball = { x: 400, y: 300, vx: 5, vy: 3, radius: 10 };\n\nfunction update() {\n  ball.x += ball.vx;\n\
          \  ball.y += ball.vy;\n  // Bounce off top/bottom\n  if (ball.y - ball.radius < 0 || ball.y + ball.radius > canvas.height)\
          \ {\n    ball.vy = -ball.vy;\n  }\n  // Reset if off sides\n  if (ball.x < 0 || ball.x > canvas.width) {\n    resetBall();\n\
          \  }\n}"
      pitfalls:
      - Ball getting stuck in walls
      - Variable frame rate affecting speed
      - Not clearing canvas each frame
      concepts:
      - Game loop
      - Velocity and position
      - Wall collision
      estimated_hours: 2-3
    - id: 2
      name: Paddles & Controls
      description: Add player paddles with keyboard controls.
      acceptance_criteria:
      - Two paddles on left and right
      - Player 1 uses W/S keys
      - Player 2 uses Arrow keys
      - Paddles stay within screen bounds
      - Smooth paddle movement
      hints:
        level1: 'Track key states: keyDown sets flag, keyUp clears it.'
        level2: Move paddles in update loop based on key states, not in event handler.
        level3: "const keys = {};\ndocument.addEventListener('keydown', e => keys[e.key] = true);\ndocument.addEventListener('keyup',\
          \ e => keys[e.key] = false);\n\nfunction updatePaddles() {\n  if (keys['w'] && paddle1.y > 0) paddle1.y -= paddle1.speed;\n\
          \  if (keys['s'] && paddle1.y < canvas.height - paddle1.height) paddle1.y += paddle1.speed;\n}"
      pitfalls:
      - Paddle going off screen
      - Jerky movement from event-based updates
      - Key repeat delay issues
      concepts:
      - Keyboard input
      - Input buffering
      - Bounds checking
      estimated_hours: 2-3
    - id: 3
      name: Paddle Collision & Scoring
      description: Implement ball-paddle collision and scoring system.
      acceptance_criteria:
      - Ball bounces off paddles
      - Angle changes based on hit position
      - Score increments when ball passes paddle
      - Score displayed on screen
      - Game resets after score
      hints:
        level1: 'AABB collision: check if ball rectangle overlaps paddle rectangle.'
        level2: 'Hit position affects angle: hit at edge = steeper angle.'
        level3: "function checkPaddleCollision(paddle) {\n  if (ball.x - ball.radius < paddle.x + paddle.width &&\n      ball.x\
          \ + ball.radius > paddle.x &&\n      ball.y > paddle.y && ball.y < paddle.y + paddle.height) {\n    ball.vx = -ball.vx;\n\
          \    const hitPos = (ball.y - paddle.y) / paddle.height;\n    const angle = (hitPos - 0.5) * Math.PI / 3;\n    const\
          \ speed = Math.sqrt(ball.vx * ball.vx + ball.vy * ball.vy);\n    ball.vy = Math.sin(angle) * speed;\n  }\n}"
      pitfalls:
      - Ball passing through paddle at high speed
      - Collision detected multiple times
      - Infinite speed increase
      concepts:
      - AABB collision
      - Hit detection
      - Score system
      estimated_hours: 2-3
    - id: 4
      name: AI Opponent & Polish
      description: Add single-player mode with AI and polish the game.
      acceptance_criteria:
      - AI controls one paddle
      - AI difficulty adjustable
      - Start/pause menu
      - Sound effects
      - Win condition (first to 10)
      hints:
        level1: 'Simple AI: move paddle toward ball Y position.'
        level2: Add reaction delay and imperfection for difficulty levels.
        level3: "function updateAI() {\n  const targetY = ball.y - paddle2.height / 2;\n  const diff = targetY - paddle2.y;\n\
          \  const difficulty = 0.8;\n  const maxSpeed = paddle2.speed * difficulty;\n  if (Math.random() > 0.9) return; //\
          \ Sometimes don't move\n  if (Math.abs(diff) > 5) {\n    paddle2.y += Math.sign(diff) * Math.min(Math.abs(diff),\
          \ maxSpeed);\n  }\n}"
      pitfalls:
      - Perfect AI is unbeatable
      - AI reacting to ball behind it
      - Sound playing too frequently
      concepts:
      - Basic game AI
      - Difficulty scaling
      - Game states
      estimated_hours: 2-3
  portfolio-site:
    id: portfolio-site
    name: Portfolio Website
    description: Build a personal portfolio site with responsive design. Learn HTML/CSS fundamentals and web design principles.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - HTML basics
    - CSS basics
    languages:
      recommended:
      - HTML
      - CSS
      - JavaScript
      also_possible: []
    resources:
    - name: MDN Web Docs
      url: https://developer.mozilla.org/en-US/docs/Learn
      type: documentation
    milestones:
    - id: 1
      name: HTML Structure
      description: Create semantic HTML structure for the portfolio.
      acceptance_criteria:
      - Header with navigation
      - About/intro section
      - Projects section
      - Contact section
      - Footer
      hints:
        level1: 'Use semantic tags: header, nav, main, section, footer.'
        level2: Add proper meta tags for SEO.
        level3: "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"\
          width=device-width, initial-scale=1.0\">\n    <meta name=\"description\" content=\"Portfolio of [Your Name] - Web\
          \ Developer\">\n    <title>Your Name - Portfolio</title>\n    <link rel=\"stylesheet\" href=\"styles.css\">\n</head>\n\
          <body>\n    <header>\n        <nav>\n            <a href=\"#\" class=\"logo\">YN</a>\n            <ul class=\"nav-links\"\
          >\n                <li><a href=\"#about\">About</a></li>\n                <li><a href=\"#projects\">Projects</a></li>\n\
          \                <li><a href=\"#contact\">Contact</a></li>\n            </ul>\n        </nav>\n    </header>\n \
          \   <main>\n        <section id=\"hero\">...</section>\n        <section id=\"about\">...</section>\n        <section\
          \ id=\"projects\">...</section>\n        <section id=\"contact\">...</section>\n    </main>\n    <footer>...</footer>\n\
          </body>\n</html>"
      pitfalls:
      - Missing viewport meta
      - Non-semantic div soup
      - Broken anchor links
      concepts:
      - Semantic HTML
      - Document structure
      - Accessibility basics
      estimated_hours: 2-3
    - id: 2
      name: CSS Styling
      description: Style the portfolio with modern CSS.
      acceptance_criteria:
      - Consistent color scheme
      - Typography hierarchy
      - Spacing and layout
      - Hover effects
      - Smooth scrolling
      hints:
        level1: Use CSS variables for colors and spacing.
        level2: Create utility classes for common patterns.
        level3: ":root {\n    --primary: #3b82f6;\n    --text: #1f2937;\n    --bg: #ffffff;\n    --spacing-sm: 0.5rem;\n \
          \   --spacing-md: 1rem;\n    --spacing-lg: 2rem;\n}\n\n* {\n    box-sizing: border-box;\n    margin: 0;\n    padding:\
          \ 0;\n}\n\nhtml {\n    scroll-behavior: smooth;\n}\n\nbody {\n    font-family: system-ui, sans-serif;\n    color:\
          \ var(--text);\n    line-height: 1.6;\n}\n\n.container {\n    max-width: 1200px;\n    margin: 0 auto;\n    padding:\
          \ 0 var(--spacing-md);\n}\n\nsection {\n    padding: var(--spacing-lg) 0;\n}\n\na {\n    color: var(--primary);\n\
          \    text-decoration: none;\n    transition: opacity 0.2s;\n}\n\na:hover {\n    opacity: 0.8;\n}"
      pitfalls:
      - Inconsistent spacing
      - Too many fonts
      - Low contrast text
      concepts:
      - CSS variables
      - Box model
      - Typography
      estimated_hours: 3-4
    - id: 3
      name: Responsive Design
      description: Make the site work on all screen sizes.
      acceptance_criteria:
      - Mobile-first approach
      - Responsive navigation
      - Flexible images
      - Grid/flexbox layouts
      - Media queries for breakpoints
      hints:
        level1: Start with mobile styles, add media queries for larger screens.
        level2: Use flexbox for navigation, CSS grid for project cards.
        level3: ".nav-links {\n    display: flex;\n    gap: var(--spacing-md);\n}\n\n.projects-grid {\n    display: grid;\n\
          \    grid-template-columns: 1fr;\n    gap: var(--spacing-lg);\n}\n\n@media (min-width: 768px) {\n    .projects-grid\
          \ {\n        grid-template-columns: repeat(2, 1fr);\n    }\n}\n\n@media (min-width: 1024px) {\n    .projects-grid\
          \ {\n        grid-template-columns: repeat(3, 1fr);\n    }\n}\n\n/* Mobile nav toggle */\n.nav-toggle {\n    display:\
          \ none;\n}\n\n@media (max-width: 767px) {\n    .nav-toggle {\n        display: block;\n    }\n    .nav-links {\n\
          \        display: none;\n        flex-direction: column;\n    }\n    .nav-links.active {\n        display: flex;\n\
          \    }\n}"
      pitfalls:
      - Fixed widths breaking layout
      - Horizontal scroll on mobile
      - Touch target too small
      concepts:
      - Responsive design
      - Flexbox/Grid
      - Media queries
      estimated_hours: 3-4
  process-spawner:
    id: process-spawner
    name: Process Spawner
    description: Build a process manager using fork/exec. Learn Unix process lifecycle and IPC.
    difficulty: intermediate
    estimated_hours: 10-15
    prerequisites:
    - Basic C programming
    - Unix basics
    - System calls
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
    resources:
    - name: fork(2) man page
      url: https://man7.org/linux/man-pages/man2/fork.2.html
      type: documentation
    - name: Advanced Programming in Unix
      url: https://www.apuebook.com/
      type: book
    milestones:
    - id: 1
      name: Basic Fork/Exec
      description: Spawn a child process to run a command.
      acceptance_criteria:
      - fork() to create child process
      - exec() to run command in child
      - Parent waits for child
      - Handle fork/exec errors
      hints:
        level1: fork() returns 0 in child, child PID in parent. exec() replaces process.
        level2: Always check return values. Child should exec or _exit.
        level3: "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/wait.h>\n\nint spawn(char *program,\
          \ char **args) {\n    pid_t pid = fork();\n    \n    if (pid < 0) {\n        perror(\"fork\");\n        return -1;\n\
          \    }\n    \n    if (pid == 0) {\n        // Child process\n        execvp(program, args);\n        // If exec\
          \ returns, it failed\n        perror(\"exec\");\n        _exit(127);  // Use _exit, not exit\n    }\n    \n    //\
          \ Parent process\n    int status;\n    if (waitpid(pid, &status, 0) < 0) {\n        perror(\"waitpid\");\n     \
          \   return -1;\n    }\n    \n    if (WIFEXITED(status)) {\n        return WEXITSTATUS(status);\n    } else if (WIFSIGNALED(status))\
          \ {\n        printf(\"Killed by signal %d\\n\", WTERMSIG(status));\n        return -1;\n    }\n    \n    return\
          \ -1;\n}\n\nint main() {\n    char *args[] = {\"ls\", \"-la\", NULL};\n    int result = spawn(\"ls\", args);\n \
          \   printf(\"Exit code: %d\\n\", result);\n    return 0;\n}"
      pitfalls:
      - Forgetting to exec
      - Using exit() instead of _exit()
      - Not handling signals
      concepts:
      - fork()
      - exec()
      - Process creation
      estimated_hours: 2-3
    - id: 2
      name: Pipe Communication
      description: Set up pipes for parent-child communication.
      acceptance_criteria:
      - Create pipe before fork
      - Redirect child stdin/stdout
      - Parent reads/writes pipe
      - Close unused pipe ends
      hints:
        level1: pipe() before fork. dup2() redirects file descriptors.
        level2: Close unused ends to avoid deadlock. EOF when all writers close.
        level3: "#include <unistd.h>\n\nint spawn_with_pipe(char *program, char **args, int *fd_in, int *fd_out) {\n    int\
          \ pipe_in[2], pipe_out[2];\n    \n    if (pipe(pipe_in) < 0 || pipe(pipe_out) < 0) {\n        perror(\"pipe\");\n\
          \        return -1;\n    }\n    \n    pid_t pid = fork();\n    if (pid < 0) {\n        perror(\"fork\");\n     \
          \   return -1;\n    }\n    \n    if (pid == 0) {\n        // Child\n        // pipe_in: parent writes, child reads\n\
          \        close(pipe_in[1]);  // Close write end\n        dup2(pipe_in[0], STDIN_FILENO);\n        close(pipe_in[0]);\n\
          \        \n        // pipe_out: child writes, parent reads\n        close(pipe_out[0]);  // Close read end\n   \
          \     dup2(pipe_out[1], STDOUT_FILENO);\n        close(pipe_out[1]);\n        \n        execvp(program, args);\n\
          \        _exit(127);\n    }\n    \n    // Parent\n    close(pipe_in[0]);   // Close read end\n    close(pipe_out[1]);\
          \  // Close write end\n    \n    *fd_in = pipe_in[1];   // Parent writes here\n    *fd_out = pipe_out[0]; // Parent\
          \ reads here\n    \n    return pid;\n}\n\n// Usage:\nint fd_in, fd_out;\npid_t pid = spawn_with_pipe(\"cat\", args,\
          \ &fd_in, &fd_out);\nwrite(fd_in, \"hello\\n\", 6);\nclose(fd_in);  // Signal EOF\nchar buf[1024];\nread(fd_out,\
          \ buf, sizeof(buf));"
      pitfalls:
      - Pipe deadlock
      - Forgetting to close ends
      - Buffer full blocking
      concepts:
      - Pipes
      - IPC
      - File descriptor redirection
      estimated_hours: 3-4
    - id: 3
      name: Process Pool
      description: Manage a pool of worker processes.
      acceptance_criteria:
      - Spawn N worker processes
      - Distribute work to workers
      - Handle worker crashes
      - Clean shutdown
      hints:
        level1: Pre-fork workers, send work via pipes or shared memory.
        level2: Parent tracks worker PIDs, respawns on SIGCHLD.
        level3: "#include <signal.h>\n\n#define MAX_WORKERS 4\n\ntypedef struct {\n    pid_t pid;\n    int fd_in;\n    int\
          \ fd_out;\n    int busy;\n} Worker;\n\nWorker workers[MAX_WORKERS];\nint num_workers = 0;\n\nvoid sigchld_handler(int\
          \ sig) {\n    int status;\n    pid_t pid;\n    while ((pid = waitpid(-1, &status, WNOHANG)) > 0) {\n        // Find\
          \ and respawn crashed worker\n        for (int i = 0; i < num_workers; i++) {\n            if (workers[i].pid ==\
          \ pid) {\n                printf(\"Worker %d (PID %d) exited\\n\", i, pid);\n                spawn_worker(i);  //\
          \ Respawn\n                break;\n            }\n        }\n    }\n}\n\nvoid spawn_worker(int idx) {\n    char\
          \ *args[] = {\"./worker\", NULL};\n    workers[idx].pid = spawn_with_pipe(\"./worker\", args,\n                \
          \                        &workers[idx].fd_in,\n                                        &workers[idx].fd_out);\n\
          \    workers[idx].busy = 0;\n}\n\nvoid init_pool() {\n    signal(SIGCHLD, sigchld_handler);\n    \n    for (int\
          \ i = 0; i < MAX_WORKERS; i++) {\n        spawn_worker(i);\n    }\n    num_workers = MAX_WORKERS;\n}\n\nWorker*\
          \ get_available_worker() {\n    for (int i = 0; i < num_workers; i++) {\n        if (!workers[i].busy) {\n     \
          \       workers[i].busy = 1;\n            return &workers[i];\n        }\n    }\n    return NULL;  // All busy\n\
          }\n\nvoid shutdown_pool() {\n    for (int i = 0; i < num_workers; i++) {\n        close(workers[i].fd_in);\n   \
          \     kill(workers[i].pid, SIGTERM);\n    }\n}"
      pitfalls:
      - Zombie processes
      - Signal handling races
      - Resource cleanup
      concepts:
      - Process pools
      - SIGCHLD
      - Worker management
      estimated_hours: 4-5
  protocol-buffer:
    id: protocol-buffer
    name: Protocol Buffer
    description: Implement a binary serialization format similar to Protocol Buffers. Learn varints, wire types, and schema-driven
      encoding.
    difficulty: advanced
    estimated_hours: 25-35
    prerequisites:
    - Binary encoding
    - Schema concepts
    - Data structures
    languages:
      recommended:
      - Python
      - Go
      - Rust
      also_possible:
      - C
      - Java
    resources:
    - name: Protocol Buffers Encoding
      url: https://protobuf.dev/programming-guides/encoding/
      type: reference
    - name: Varint Encoding
      url: https://developers.google.com/protocol-buffers/docs/encoding#varints
      type: tutorial
    milestones:
    - id: 1
      name: Varint Encoding
      description: Implement variable-length integer encoding used in Protocol Buffers.
      acceptance_criteria:
      - Encode unsigned integers as varints
      - Encode signed integers with ZigZag encoding
      - Decode varints from byte stream
      - Handle all 64-bit integer values
      hints:
        level1: Varint uses 7 bits per byte for data, MSB=1 means more bytes follow.
        level2: 'ZigZag maps signed to unsigned: 0->0, -1->1, 1->2, -2->3, etc. Formula: (n << 1) ^ (n >> 63)'
        level3: "def encode_varint(value: int) -> bytes:\n    '''Encode unsigned integer as varint'''\n    result = []\n \
          \   while value > 127:\n        result.append((value & 0x7F) | 0x80)  # 7 bits + continuation\n        value >>=\
          \ 7\n    result.append(value)\n    return bytes(result)\n\ndef decode_varint(data: bytes, offset: int = 0) -> tuple[int,\
          \ int]:\n    '''Decode varint, return (value, bytes_consumed)'''\n    result = 0\n    shift = 0\n    pos = offset\n\
          \n    while True:\n        byte = data[pos]\n        result |= (byte & 0x7F) << shift\n        pos += 1\n\n    \
          \    if not (byte & 0x80):  # No continuation bit\n            break\n        shift += 7\n\n    return result, pos\
          \ - offset\n\ndef zigzag_encode(value: int) -> int:\n    '''Encode signed int using ZigZag'''\n    return (value\
          \ << 1) ^ (value >> 63)\n\ndef zigzag_decode(value: int) -> int:\n    '''Decode ZigZag to signed int'''\n    return\
          \ (value >> 1) ^ -(value & 1)\n\n# Test\nassert encode_varint(1) == b'\\x01'\nassert encode_varint(300) == b'\\\
          xac\\x02'  # 300 = 0b100101100\nassert zigzag_encode(-1) == 1\nassert zigzag_encode(1) == 2"
      pitfalls:
      - Signed integers need ZigZag, not two's complement
      - Varint can be up to 10 bytes for 64-bit values
      - Overflow possible if not handling properly
      concepts:
      - Variable-length encoding
      - Integer representation
      - ZigZag encoding
      estimated_hours: 4-5
    - id: 2
      name: Wire Types
      description: Implement the wire type system for field encoding.
      acceptance_criteria:
      - 'Type 0: Varint (int32, int64, uint, bool, enum)'
      - 'Type 1: 64-bit (fixed64, sfixed64, double)'
      - 'Type 2: Length-delimited (string, bytes, embedded messages)'
      - 'Type 5: 32-bit (fixed32, sfixed32, float)'
      hints:
        level1: Field key = (field_number << 3) | wire_type. Wire type is in lower 3 bits.
        level2: 'Length-delimited: encode length as varint, then raw bytes.'
        level3: "from enum import IntEnum\nfrom struct import pack, unpack\n\nclass WireType(IntEnum):\n    VARINT = 0\n \
          \   FIXED64 = 1\n    LENGTH_DELIMITED = 2\n    # 3, 4 deprecated (groups)\n    FIXED32 = 5\n\ndef encode_field_key(field_number:\
          \ int, wire_type: WireType) -> bytes:\n    '''Encode field key'''\n    key = (field_number << 3) | wire_type\n \
          \   return encode_varint(key)\n\ndef decode_field_key(data: bytes, offset: int) -> tuple[int, WireType, int]:\n\
          \    '''Decode field key, return (field_number, wire_type, bytes_consumed)'''\n    key, consumed = decode_varint(data,\
          \ offset)\n    field_number = key >> 3\n    wire_type = WireType(key & 0x07)\n    return field_number, wire_type,\
          \ consumed\n\ndef encode_field(field_number: int, value, value_type: str) -> bytes:\n    '''Encode a field with\
          \ type'''\n    if value_type in ('int32', 'int64', 'uint32', 'uint64', 'bool'):\n        wire_type = WireType.VARINT\n\
          \        val_bytes = encode_varint(value if not isinstance(value, bool) else int(value))\n    elif value_type in\
          \ ('sint32', 'sint64'):\n        wire_type = WireType.VARINT\n        val_bytes = encode_varint(zigzag_encode(value))\n\
          \    elif value_type == 'double':\n        wire_type = WireType.FIXED64\n        val_bytes = pack('<d', value)\n\
          \    elif value_type == 'float':\n        wire_type = WireType.FIXED32\n        val_bytes = pack('<f', value)\n\
          \    elif value_type in ('string', 'bytes'):\n        wire_type = WireType.LENGTH_DELIMITED\n        data = value.encode()\
          \ if isinstance(value, str) else value\n        val_bytes = encode_varint(len(data)) + data\n    else:\n       \
          \ raise ValueError(f\"Unknown type: {value_type}\")\n\n    return encode_field_key(field_number, wire_type) + val_bytes"
      pitfalls:
      - Wire types 3 and 4 are deprecated (start/end group)
      - Fixed types are little-endian
      - Unknown fields should be preserved, not discarded
      concepts:
      - Wire types
      - Type-length-value encoding
      - Binary protocols
      estimated_hours: 5-6
    - id: 3
      name: Schema Parser
      description: Parse a simple schema definition to drive encoding/decoding.
      acceptance_criteria:
      - Parse message definitions
      - Parse field declarations with types and numbers
      - Support repeated fields
      - Support nested messages
      hints:
        level1: 'Schema: message Name { type name = number; }. Start simple, then add features.'
        level2: Build AST with Message nodes containing Field nodes.
        level3: "from dataclasses import dataclass\nfrom typing import Optional\nimport re\n\n@dataclass\nclass Field:\n \
          \   name: str\n    type: str\n    number: int\n    repeated: bool = False\n\n@dataclass\nclass Message:\n    name:\
          \ str\n    fields: list[Field]\n    nested: list['Message']\n\ndef parse_schema(text: str) -> list[Message]:\n \
          \   '''Parse protobuf-like schema'''\n    messages = []\n\n    # Find all message definitions\n    message_pattern\
          \ = r'message\\s+(\\w+)\\s*\\{([^}]+)\\}'\n\n    for match in re.finditer(message_pattern, text, re.DOTALL):\n \
          \       name = match.group(1)\n        body = match.group(2)\n\n        fields = []\n        # Field pattern: [repeated]\
          \ type name = number;\n        field_pattern = r'(repeated\\s+)?(\\w+)\\s+(\\w+)\\s*=\\s*(\\d+)\\s*;'\n\n      \
          \  for field_match in re.finditer(field_pattern, body):\n            repeated = bool(field_match.group(1))\n   \
          \         field_type = field_match.group(2)\n            field_name = field_match.group(3)\n            field_number\
          \ = int(field_match.group(4))\n\n            fields.append(Field(field_name, field_type, field_number, repeated))\n\
          \n        messages.append(Message(name, fields, []))\n\n    return messages\n\n# Example schema\nschema = '''\n\
          message Person {\n    string name = 1;\n    int32 age = 2;\n    repeated string emails = 3;\n}\n'''\n\nmessages\
          \ = parse_schema(schema)\n# Message(name='Person', fields=[\n#     Field(name='name', type='string', number=1),\n\
          #     Field(name='age', type='int32', number=2),\n#     Field(name='emails', type='string', number=3, repeated=True)\n\
          # ])"
      pitfalls:
      - Field numbers must be unique within a message
      - Field numbers 19000-19999 are reserved
      - Nested messages need recursive parsing
      concepts:
      - Schema languages
      - Code generation
      - Parsing
      estimated_hours: 5-6
    - id: 4
      name: Message Serialization
      description: Serialize and deserialize messages according to schema.
      acceptance_criteria:
      - Serialize dict to bytes using schema
      - Deserialize bytes to dict using schema
      - Handle missing optional fields
      - Handle repeated fields (arrays)
      hints:
        level1: Fields can appear in any order. Repeated fields appear multiple times.
        level2: During decode, collect all values for repeated fields into a list.
        level3: "class Serializer:\n    def __init__(self, messages: list[Message]):\n        self.messages = {m.name: m for\
          \ m in messages}\n\n    def serialize(self, message_name: str, data: dict) -> bytes:\n        '''Serialize dict\
          \ to protobuf bytes'''\n        message = self.messages[message_name]\n        result = b''\n\n        for field\
          \ in message.fields:\n            value = data.get(field.name)\n            if value is None:\n                continue\n\
          \n            if field.repeated:\n                for item in value:\n                    result += encode_field(field.number,\
          \ item, field.type)\n            else:\n                result += encode_field(field.number, value, field.type)\n\
          \n        return result\n\n    def deserialize(self, message_name: str, data: bytes) -> dict:\n        '''Deserialize\
          \ protobuf bytes to dict'''\n        message = self.messages[message_name]\n        field_map = {f.number: f for\
          \ f in message.fields}\n        result = {}\n\n        offset = 0\n        while offset < len(data):\n         \
          \   field_number, wire_type, consumed = decode_field_key(data, offset)\n            offset += consumed\n\n     \
          \       field = field_map.get(field_number)\n            if not field:\n                # Skip unknown field\n \
          \               offset += skip_field(data, offset, wire_type)\n                continue\n\n            value, consumed\
          \ = decode_value(data, offset, wire_type, field.type)\n            offset += consumed\n\n            if field.repeated:\n\
          \                if field.name not in result:\n                    result[field.name] = []\n                result[field.name].append(value)\n\
          \            else:\n                result[field.name] = value\n\n        return result\n\n# Usage\nserializer =\
          \ Serializer(parse_schema(schema))\ndata = {'name': 'Alice', 'age': 30, 'emails': ['a@b.com', 'c@d.com']}\nencoded\
          \ = serializer.serialize('Person', data)\ndecoded = serializer.deserialize('Person', encoded)"
      pitfalls:
      - Field order in wire format is arbitrary
      - Unknown fields should be skipped, not error
      - Empty repeated field = no entries (not encoded)
      concepts:
      - Serialization
      - Schema-driven encoding
      - Forward compatibility
      estimated_hours: 6-8
  query-optimizer:
    id: query-optimizer
    name: Query Optimizer
    description: Build a basic query optimizer. Learn cost estimation, join ordering, and query plans.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - SQL Parser
    - Database fundamentals
    - Algorithm complexity
    languages:
      recommended:
      - Python
      - Java
      - Go
      also_possible:
      - Rust
      - C++
    resources:
    - name: CMU Database Course
      url: https://15445.courses.cs.cmu.edu/
      type: course
    - name: Query Optimization Survey
      url: https://www.vldb.org/pvldb/vol14/p3025-yang.pdf
      type: paper
    milestones:
    - id: 1
      name: Query Plan Representation
      description: Define query plan tree structure.
      acceptance_criteria:
      - Plan nodes (Scan, Filter, Join, Project)
      - Tree structure for plans
      - Physical vs logical operators
      - Plan pretty-printing
      hints:
        level1: Plan = tree of operators. Each node has children and produces rows.
        level2: 'Logical: what to compute. Physical: how to compute (e.g., HashJoin vs NestedLoopJoin).'
        level3: "from dataclasses import dataclass\nfrom typing import List, Optional\nfrom abc import ABC, abstractmethod\n\
          \nclass PlanNode(ABC):\n    @abstractmethod\n    def children(self) -> List['PlanNode']:\n        pass\n    \n \
          \   @abstractmethod\n    def __str__(self) -> str:\n        pass\n\n@dataclass\nclass SeqScan(PlanNode):\n    table:\
          \ str\n    alias: str = None\n    \n    def children(self): return []\n    def __str__(self): return f'SeqScan({self.table})'\n\
          \n@dataclass\nclass IndexScan(PlanNode):\n    table: str\n    index: str\n    predicate: 'Expr'\n    \n    def children(self):\
          \ return []\n    def __str__(self): return f'IndexScan({self.table}, {self.index})'\n\n@dataclass\nclass Filter(PlanNode):\n\
          \    predicate: 'Expr'\n    child: PlanNode\n    \n    def children(self): return [self.child]\n    def __str__(self):\
          \ return f'Filter({self.predicate})'\n\n@dataclass\nclass HashJoin(PlanNode):\n    left: PlanNode\n    right: PlanNode\n\
          \    condition: 'Expr'\n    \n    def children(self): return [self.left, self.right]\n    def __str__(self): return\
          \ f'HashJoin({self.condition})'\n\n@dataclass\nclass NestedLoopJoin(PlanNode):\n    left: PlanNode\n    right: PlanNode\n\
          \    condition: 'Expr'\n    \n    def children(self): return [self.left, self.right]\n    def __str__(self): return\
          \ f'NLJoin({self.condition})'\n\n@dataclass\nclass Project(PlanNode):\n    columns: List[str]\n    child: PlanNode\n\
          \    \n    def children(self): return [self.child]\n    def __str__(self): return f'Project({self.columns})'\n\n\
          def print_plan(node: PlanNode, indent=0):\n    print(' ' * indent + str(node))\n    for child in node.children():\n\
          \        print_plan(child, indent + 2)"
      pitfalls:
      - Missing operators
      - Tree vs DAG
      - Operator semantics
      concepts:
      - Query plans
      - Operators
      - Plan trees
      estimated_hours: 3-4
    - id: 2
      name: Cost Estimation
      description: Estimate the cost of query plans.
      acceptance_criteria:
      - Table statistics (row count, distinct values)
      - Selectivity estimation
      - Join cardinality estimation
      - I/O and CPU cost model
      hints:
        level1: Cost = f(cardinality, I/O, CPU). Need statistics about data.
        level2: 'Selectivity: fraction of rows that pass a filter. Default = 0.1 for unknown.'
        level3: "@dataclass\nclass TableStats:\n    row_count: int\n    distinct_values: dict  # column -> count\n    min_values:\
          \ dict       # column -> min\n    max_values: dict       # column -> max\n\nclass CostEstimator:\n    def __init__(self,\
          \ stats: dict):\n        self.stats = stats  # table_name -> TableStats\n    \n    def estimate_cardinality(self,\
          \ node: PlanNode) -> float:\n        if isinstance(node, SeqScan):\n            return self.stats[node.table].row_count\n\
          \        \n        elif isinstance(node, Filter):\n            child_card = self.estimate_cardinality(node.child)\n\
          \            selectivity = self.estimate_selectivity(node.predicate, node.child)\n            return child_card\
          \ * selectivity\n        \n        elif isinstance(node, (HashJoin, NestedLoopJoin)):\n            left_card = self.estimate_cardinality(node.left)\n\
          \            right_card = self.estimate_cardinality(node.right)\n            selectivity = self.estimate_join_selectivity(node.condition)\n\
          \            return left_card * right_card * selectivity\n        \n        return 1000  # Default\n    \n    def\
          \ estimate_selectivity(self, predicate, source) -> float:\n        # col = constant\n        if isinstance(predicate,\
          \ Comparison) and predicate.op == '=':\n            if isinstance(predicate.left, Identifier):\n               \
          \ col = predicate.left.name\n                table = self.get_table(source)\n                if col in self.stats[table].distinct_values:\n\
          \                    return 1.0 / self.stats[table].distinct_values[col]\n        \n        # col < constant (range\
          \ query)\n        if isinstance(predicate, Comparison) and predicate.op in ('<', '>'):\n            return 0.33\
          \  # Default for range\n        \n        return 0.1  # Default selectivity\n    \n    def estimate_cost(self, node:\
          \ PlanNode) -> float:\n        cardinality = self.estimate_cardinality(node)\n        \n        if isinstance(node,\
          \ SeqScan):\n            # I/O cost = pages to read\n            pages = cardinality / 100  # Assume 100 rows per\
          \ page\n            return pages\n        \n        elif isinstance(node, HashJoin):\n            left_cost = self.estimate_cost(node.left)\n\
          \            right_cost = self.estimate_cost(node.right)\n            # Build hash table + probe\n            return\
          \ left_cost + right_cost + cardinality * 0.01\n        \n        elif isinstance(node, NestedLoopJoin):\n      \
          \      left_cost = self.estimate_cost(node.left)\n            right_cost = self.estimate_cost(node.right)\n    \
          \        left_card = self.estimate_cardinality(node.left)\n            # Nested loop: scan right for each left row\n\
          \            return left_cost + left_card * right_cost\n        \n        # Sum children costs\n        return sum(self.estimate_cost(c)\
          \ for c in node.children())"
      pitfalls:
      - Statistics staleness
      - Correlation assumptions
      - Estimation errors
      concepts:
      - Cost models
      - Selectivity
      - Cardinality estimation
      estimated_hours: 5-7
    - id: 3
      name: Join Ordering
      description: Find optimal join order for multi-table queries.
      acceptance_criteria:
      - Enumerate join orders
      - Dynamic programming for optimal order
      - Left-deep vs bushy trees
      - Prune bad plans early
      hints:
        level1: Join order matters! n tables = n! possible orders.
        level2: 'DP: build optimal plans for subsets, combine for larger sets.'
        level3: "from itertools import combinations\n\nclass JoinOrderOptimizer:\n    def __init__(self, estimator: CostEstimator):\n\
          \        self.estimator = estimator\n        self.memo = {}  # frozenset(tables) -> best_plan\n    \n    def optimize_joins(self,\
          \ tables: List[str], predicates: List['Expr']) -> PlanNode:\n        '''Find optimal join order using dynamic programming'''\n\
          \        # Base case: single tables\n        for table in tables:\n            key = frozenset([table])\n      \
          \      self.memo[key] = SeqScan(table)\n        \n        # Build up from 2 tables to all tables\n        for size\
          \ in range(2, len(tables) + 1):\n            for subset in combinations(tables, size):\n                subset_key\
          \ = frozenset(subset)\n                self.find_best_join(subset_key, predicates)\n        \n        return self.memo[frozenset(tables)]\n\
          \    \n    def find_best_join(self, tables: frozenset, predicates: List['Expr']):\n        best_plan = None\n  \
          \      best_cost = float('inf')\n        \n        # Try all ways to split tables into two non-empty subsets\n \
          \       for i in range(1, len(tables)):\n            for left_tables in combinations(tables, i):\n             \
          \   left_key = frozenset(left_tables)\n                right_key = tables - left_key\n                \n       \
          \         if left_key not in self.memo or right_key not in self.memo:\n                    continue\n          \
          \      \n                left_plan = self.memo[left_key]\n                right_plan = self.memo[right_key]\n  \
          \              \n                # Find applicable join predicate\n                join_pred = self.find_join_predicate(left_key,\
          \ right_key, predicates)\n                \n                # Try different join algorithms\n                for\
          \ join_type in [HashJoin, NestedLoopJoin]:\n                    plan = join_type(left_plan, right_plan, join_pred)\n\
          \                    cost = self.estimator.estimate_cost(plan)\n                    \n                    if cost\
          \ < best_cost:\n                        best_cost = cost\n                        best_plan = plan\n        \n \
          \       self.memo[tables] = best_plan"
      pitfalls:
      - Exponential complexity
      - Cross joins
      - Missing predicates
      concepts:
      - Join ordering
      - Dynamic programming
      - Plan enumeration
      estimated_hours: 6-8
    - id: 4
      name: Plan Selection
      description: Choose between physical operators and access methods.
      acceptance_criteria:
      - Index selection
      - Join algorithm selection
      - Predicate pushdown
      - Generate final plan
      hints:
        level1: Use index if selective enough. Push filters down before joins.
        level2: HashJoin good for large joins. NestedLoop good with index on inner.
        level3: "class QueryOptimizer:\n    def __init__(self, stats: dict, indexes: dict):\n        self.estimator = CostEstimator(stats)\n\
          \        self.indexes = indexes  # table -> [index_info]\n    \n    def optimize(self, query: SelectStmt) -> PlanNode:\n\
          \        # 1. Generate initial logical plan\n        plan = self.logical_plan(query)\n        \n        # 2. Predicate\
          \ pushdown\n        plan = self.pushdown_predicates(plan)\n        \n        # 3. Join ordering\n        if self.has_joins(plan):\n\
          \            tables = self.extract_tables(plan)\n            predicates = self.extract_join_predicates(plan)\n \
          \           plan = JoinOrderOptimizer(self.estimator).optimize_joins(tables, predicates)\n        \n        # 4.\
          \ Physical operator selection\n        plan = self.select_physical_operators(plan)\n        \n        return plan\n\
          \    \n    def select_physical_operators(self, node: PlanNode) -> PlanNode:\n        if isinstance(node, SeqScan):\n\
          \            # Check if index scan is better\n            return self.select_access_method(node)\n        \n   \
          \     elif isinstance(node, Filter):\n            child = self.select_physical_operators(node.child)\n         \
          \   return Filter(node.predicate, child)\n        \n        elif isinstance(node, (HashJoin, NestedLoopJoin)):\n\
          \            left = self.select_physical_operators(node.left)\n            right = self.select_physical_operators(node.right)\n\
          \            return self.select_join_method(left, right, node.condition)\n        \n        return node\n    \n\
          \    def select_access_method(self, scan: SeqScan) -> PlanNode:\n        # Check for applicable indexes\n      \
          \  if scan.table in self.indexes:\n            for idx in self.indexes[scan.table]:\n                # If there's\
          \ a selective predicate on indexed column\n                # return IndexScan instead\n                pass\n  \
          \      return scan\n    \n    def select_join_method(self, left, right, condition) -> PlanNode:\n        # Hash\
          \ join for equi-joins on large tables\n        # Nested loop for small tables or with index\n        left_card =\
          \ self.estimator.estimate_cardinality(left)\n        right_card = self.estimator.estimate_cardinality(right)\n \
          \       \n        if left_card > 1000 and right_card > 1000:\n            return HashJoin(left, right, condition)\n\
          \        else:\n            return NestedLoopJoin(left, right, condition)"
      pitfalls:
      - Over-optimization
      - Plan caching
      - Statistics accuracy
      concepts:
      - Physical planning
      - Index selection
      - Predicate pushdown
      estimated_hours: 5-7
  rate-limiter:
    id: rate-limiter
    name: Rate Limiter
    description: Build a rate limiter using the token bucket algorithm. Learn about request throttling and protecting services
      from abuse.
    difficulty: intermediate
    estimated_hours: 10-15
    prerequisites:
    - Basic web server knowledge
    - Concurrency basics
    - Time handling
    languages:
      recommended:
      - Python
      - Go
      - JavaScript
      also_possible:
      - Java
      - Rust
    resources:
    - name: Token Bucket Algorithm
      url: https://en.wikipedia.org/wiki/Token_bucket
      type: documentation
    - name: Rate Limiting Strategies
      url: https://blog.bytebytego.com/p/rate-limiting-fundamentals
      type: article
    milestones:
    - id: 1
      name: Token Bucket Implementation
      description: Implement the core token bucket algorithm.
      acceptance_criteria:
      - Bucket has configurable capacity
      - Tokens added at configurable rate
      - Consume tokens for requests
      - Return allow/deny decision
      - Thread-safe implementation
      hints:
        level1: Track tokens and last refill time.
        level2: Refill tokens on each request based on elapsed time.
        level3: "import time, threading\n\nclass TokenBucket:\n    def __init__(self, capacity, refill_rate):\n        self.capacity\
          \ = capacity\n        self.tokens = capacity\n        self.refill_rate = refill_rate\n        self.last_refill =\
          \ time.time()\n        self.lock = threading.Lock()\n\n    def _refill(self):\n        now = time.time()\n     \
          \   elapsed = now - self.last_refill\n        tokens_to_add = elapsed * self.refill_rate\n        self.tokens =\
          \ min(self.capacity, self.tokens + tokens_to_add)\n        self.last_refill = now\n\n    def consume(self, tokens=1):\n\
          \        with self.lock:\n            self._refill()\n            if self.tokens >= tokens:\n                self.tokens\
          \ -= tokens\n                return True\n            return False"
      pitfalls:
      - Race conditions without locking
      - Integer overflow in token calculation
      - Clock drift issues
      concepts:
      - Token bucket algorithm
      - Thread safety
      - Rate calculations
      estimated_hours: 3-4
    - id: 2
      name: Per-Client Rate Limiting
      description: Track rate limits per client (IP or API key).
      acceptance_criteria:
      - Each client has own bucket
      - Identify clients by IP or key
      - Clean up old buckets
      - Memory-efficient storage
      - Configurable per-client limits
      hints:
        level1: 'Use dict/map: client_id -> TokenBucket.'
        level2: Expire buckets not used for N minutes to save memory.
        level3: "class RateLimiter:\n    def __init__(self, capacity, refill_rate):\n        self.capacity = capacity\n  \
          \      self.refill_rate = refill_rate\n        self.buckets = {}\n        self.lock = threading.Lock()\n\n    def\
          \ is_allowed(self, client_id):\n        with self.lock:\n            if client_id not in self.buckets:\n       \
          \         self.buckets[client_id] = TokenBucket(self.capacity, self.refill_rate)\n            return self.buckets[client_id].consume()"
      pitfalls:
      - Memory leak from never cleaning buckets
      - Lock contention under load
      - Client spoofing bypassing limits
      concepts:
      - Per-client tracking
      - Memory management
      - Background cleanup
      estimated_hours: 2-3
    - id: 3
      name: HTTP Middleware Integration
      description: Integrate rate limiter as HTTP middleware.
      acceptance_criteria:
      - Middleware intercepts requests
      - Returns 429 Too Many Requests when limited
      - Includes Retry-After header
      - X-RateLimit headers show limit/remaining
      - Works with Express/Flask/etc
      hints:
        level1: Middleware checks rate limiter, proceeds or returns 429.
        level2: Calculate Retry-After from time until next token.
        level3: "# Flask middleware example\n@app.before_request\ndef rate_limit():\n    client_ip = request.remote_addr\n\
          \    if not limiter.is_allowed(client_ip):\n        response = jsonify({'error': 'Too many requests'})\n       \
          \ response.status_code = 429\n        response.headers['Retry-After'] = str(limiter.get_retry_after(client_ip))\n\
          \        response.headers['X-RateLimit-Limit'] = str(limiter.capacity)\n        response.headers['X-RateLimit-Remaining']\
          \ = '0'\n        return response"
      pitfalls:
      - Not returning proper status code
      - Missing Retry-After header
      - Rate limit headers only on 429
      concepts:
      - HTTP middleware
      - Rate limit headers
      - 429 response
      estimated_hours: 2-3
    - id: 4
      name: Distributed Rate Limiting
      description: Scale rate limiter across multiple server instances.
      acceptance_criteria:
      - Rate limit shared across instances
      - Redis-backed storage
      - Atomic operations
      - Handles Redis failures gracefully
      - Consistent under high concurrency
      hints:
        level1: Store bucket state in Redis instead of memory.
        level2: Use Redis transactions (MULTI/EXEC) or Lua scripts.
        level3: "# Redis Lua script for atomic token bucket\nSCRIPT = '''\nlocal key = KEYS[1]\nlocal capacity = tonumber(ARGV[1])\n\
          local refill_rate = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\nlocal requested = tonumber(ARGV[4])\n\nlocal\
          \ bucket = redis.call('HMGET', key, 'tokens', 'last_refill')\nlocal tokens = tonumber(bucket[1]) or capacity\nlocal\
          \ last_refill = tonumber(bucket[2]) or now\n\nlocal elapsed = now - last_refill\ntokens = math.min(capacity, tokens\
          \ + elapsed * refill_rate)\n\nlocal allowed = 0\nif tokens >= requested then\n    tokens = tokens - requested\n\
          \    allowed = 1\nend\n\nredis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)\nredis.call('EXPIRE', key,\
          \ 3600)\nreturn {allowed, tokens}\n'''"
      pitfalls:
      - Non-atomic read-modify-write
      - Redis connection failures
      - Clock sync between servers
      concepts:
      - Distributed state
      - Redis Lua scripts
      - Atomic operations
      estimated_hours: 3-4
  red-black-tree:
    id: red-black-tree
    name: Red-Black Tree
    description: Implement a self-balancing red-black tree. Learn the invariants and rotations that maintain balance.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Binary search trees
    - Tree rotations
    - Recursion
    languages:
      recommended:
      - C
      - Java
      - Python
      also_possible:
      - Rust
      - Go
    resources:
    - name: CLRS - Red-Black Trees
      url: https://mitpress.mit.edu/books/introduction-algorithms-fourth-edition
      type: book
    - name: Red-Black Tree Visualization
      url: https://www.cs.usfca.edu/~galles/visualization/RedBlack.html
      type: interactive
    milestones:
    - id: 1
      name: Tree Structure
      description: Define the red-black tree node structure.
      acceptance_criteria:
      - Node with key, value, color, parent, left, right
      - NIL sentinel node (black)
      - Root is always black
      - Implement basic BST search
      hints:
        level1: 'RB properties: 1) Nodes red or black, 2) Root black, 3) Leaves (NIL) black, 4) Red nodes have black children,
          5) Equal black height on all paths.'
        level2: Use a single NIL sentinel to simplify code.
        level3: "from enum import Enum\n\nclass Color(Enum):\n    RED = 0\n    BLACK = 1\n\nclass Node:\n    def __init__(self,\
          \ key, value=None):\n        self.key = key\n        self.value = value\n        self.color = Color.RED  # New nodes\
          \ are red\n        self.left = None\n        self.right = None\n        self.parent = None\n\nclass RedBlackTree:\n\
          \    def __init__(self):\n        self.NIL = Node(None)\n        self.NIL.color = Color.BLACK\n        self.root\
          \ = self.NIL\n    \n    def search(self, key):\n        node = self.root\n        while node != self.NIL:\n    \
          \        if key == node.key:\n                return node\n            elif key < node.key:\n                node\
          \ = node.left\n            else:\n                node = node.right\n        return None\n    \n    def minimum(self,\
          \ node):\n        while node.left != self.NIL:\n            node = node.left\n        return node"
      pitfalls:
      - Forgetting NIL sentinel
      - Null pointer errors
      - Color initialization
      concepts:
      - RB tree properties
      - Sentinel nodes
      - BST operations
      estimated_hours: 2-3
    - id: 2
      name: Rotations
      description: Implement left and right rotations.
      acceptance_criteria:
      - Left rotation around a node
      - Right rotation around a node
      - Maintain parent pointers
      - Handle root rotation
      hints:
        level1: Rotation preserves BST property. Child becomes parent, parent becomes child.
        level2: 'Update three parent pointers: rotated node, its new parent, subtree moved.'
        level3: "def left_rotate(self, x):\n    '''Rotate x down to the left'''\n    y = x.right\n    x.right = y.left\n \
          \   \n    if y.left != self.NIL:\n        y.left.parent = x\n    \n    y.parent = x.parent\n    \n    if x.parent\
          \ == self.NIL:\n        self.root = y\n    elif x == x.parent.left:\n        x.parent.left = y\n    else:\n    \
          \    x.parent.right = y\n    \n    y.left = x\n    x.parent = y\n\ndef right_rotate(self, y):\n    '''Rotate y down\
          \ to the right'''\n    x = y.left\n    y.left = x.right\n    \n    if x.right != self.NIL:\n        x.right.parent\
          \ = y\n    \n    x.parent = y.parent\n    \n    if y.parent == self.NIL:\n        self.root = x\n    elif y == y.parent.right:\n\
          \        y.parent.right = x\n    else:\n        y.parent.left = x\n    \n    x.right = y\n    y.parent = x"
      pitfalls:
      - Wrong direction
      - Missing parent updates
      - Root special case
      concepts:
      - Tree rotations
      - Pointer manipulation
      - BST preservation
      estimated_hours: 2-3
    - id: 3
      name: Insertion
      description: Insert nodes while maintaining RB properties.
      acceptance_criteria:
      - BST insert (new node is red)
      - Fix violations (red-red)
      - Handle uncle cases
      - Recolor and rotate as needed
      hints:
        level1: 'After insert: if parent red, fix. Cases depend on uncle''s color.'
        level2: 'Red uncle: recolor. Black uncle: rotate and recolor.'
        level3: "def insert(self, key, value=None):\n    node = Node(key, value)\n    node.left = self.NIL\n    node.right\
          \ = self.NIL\n    \n    # BST insert\n    parent = self.NIL\n    current = self.root\n    while current != self.NIL:\n\
          \        parent = current\n        if key < current.key:\n            current = current.left\n        else:\n  \
          \          current = current.right\n    \n    node.parent = parent\n    if parent == self.NIL:\n        self.root\
          \ = node\n    elif key < parent.key:\n        parent.left = node\n    else:\n        parent.right = node\n    \n\
          \    self._insert_fixup(node)\n\ndef _insert_fixup(self, z):\n    while z.parent.color == Color.RED:\n        if\
          \ z.parent == z.parent.parent.left:\n            uncle = z.parent.parent.right\n            if uncle.color == Color.RED:\n\
          \                # Case 1: Uncle is red - recolor\n                z.parent.color = Color.BLACK\n              \
          \  uncle.color = Color.BLACK\n                z.parent.parent.color = Color.RED\n                z = z.parent.parent\n\
          \            else:\n                if z == z.parent.right:\n                    # Case 2: Uncle black, z is right\
          \ child\n                    z = z.parent\n                    self.left_rotate(z)\n                # Case 3: Uncle\
          \ black, z is left child\n                z.parent.color = Color.BLACK\n                z.parent.parent.color =\
          \ Color.RED\n                self.right_rotate(z.parent.parent)\n        else:\n            # Mirror cases for right\
          \ side\n            uncle = z.parent.parent.left\n            # ... (symmetric)\n    \n    self.root.color = Color.BLACK"
      pitfalls:
      - Symmetric cases
      - Root color
      - Grandparent access
      concepts:
      - RB insert fixup
      - Case analysis
      - Recoloring
      estimated_hours: 5-7
    - id: 4
      name: Deletion
      description: Delete nodes while maintaining RB properties.
      acceptance_criteria:
      - BST delete (find successor if needed)
      - Handle double-black case
      - Fix violations after delete
      - Transplant helper function
      hints:
        level1: Deletion is complex. Track the 'extra black' that moves up.
        level2: Fixup cases depend on sibling and its children colors.
        level3: "def _transplant(self, u, v):\n    '''Replace subtree rooted at u with subtree rooted at v'''\n    if u.parent\
          \ == self.NIL:\n        self.root = v\n    elif u == u.parent.left:\n        u.parent.left = v\n    else:\n    \
          \    u.parent.right = v\n    v.parent = u.parent\n\ndef delete(self, key):\n    z = self.search(key)\n    if z is\
          \ None:\n        return\n    \n    y = z\n    y_original_color = y.color\n    \n    if z.left == self.NIL:\n   \
          \     x = z.right\n        self._transplant(z, z.right)\n    elif z.right == self.NIL:\n        x = z.left\n   \
          \     self._transplant(z, z.left)\n    else:\n        y = self.minimum(z.right)  # Successor\n        y_original_color\
          \ = y.color\n        x = y.right\n        if y.parent == z:\n            x.parent = y\n        else:\n         \
          \   self._transplant(y, y.right)\n            y.right = z.right\n            y.right.parent = y\n        self._transplant(z,\
          \ y)\n        y.left = z.left\n        y.left.parent = y\n        y.color = z.color\n    \n    if y_original_color\
          \ == Color.BLACK:\n        self._delete_fixup(x)\n\ndef _delete_fixup(self, x):\n    while x != self.root and x.color\
          \ == Color.BLACK:\n        if x == x.parent.left:\n            sibling = x.parent.right\n            # Case 1: Sibling\
          \ is red\n            if sibling.color == Color.RED:\n                sibling.color = Color.BLACK\n            \
          \    x.parent.color = Color.RED\n                self.left_rotate(x.parent)\n                sibling = x.parent.right\n\
          \            # Case 2: Sibling black, both children black\n            if sibling.left.color == Color.BLACK and\
          \ sibling.right.color == Color.BLACK:\n                sibling.color = Color.RED\n                x = x.parent\n\
          \            else:\n                # Case 3: Sibling black, left child red\n                if sibling.right.color\
          \ == Color.BLACK:\n                    sibling.left.color = Color.BLACK\n                    sibling.color = Color.RED\n\
          \                    self.right_rotate(sibling)\n                    sibling = x.parent.right\n                #\
          \ Case 4: Sibling black, right child red\n                sibling.color = x.parent.color\n                x.parent.color\
          \ = Color.BLACK\n                sibling.right.color = Color.BLACK\n                self.left_rotate(x.parent)\n\
          \                x = self.root\n        else:\n            # Mirror cases\n            pass\n    x.color = Color.BLACK"
      pitfalls:
      - Four delete cases
      - Double-black propagation
      - NIL parent handling
      concepts:
      - RB delete
      - Transplant
      - Case analysis
      estimated_hours: 6-8
  replicated-log:
    id: replicated-log
    name: Replicated Log
    description: Build a replicated append-only log. Learn distributed replication, consistency, and failure handling.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - RPC basics
    - Networking
    - Log-based storage
    languages:
      recommended:
      - Go
      - Rust
      - Java
      also_possible:
      - Python
      - C
    resources:
    - type: paper
      name: Viewstamped Replication
      url: https://pmg.csail.mit.edu/papers/vr-revisited.pdf
    - type: blog
      name: Distributed Log 101
      url: https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying
    milestones:
    - id: 1
      name: Log Storage
      description: Implement an append-only log with indexing.
      acceptance_criteria:
      - Append entries with sequence numbers
      - Read entries by index
      - Persist to disk
      - Handle log compaction
      hints:
        level1: Each entry has monotonic sequence number. Store entries sequentially in file.
        level2: Use index file for O(1) lookups. mmap for efficient reads.
        level3: "import struct\nimport os\n\nclass Log:\n    HEADER_SIZE = 12  # seq(8) + len(4)\n    \n    def __init__(self,\
          \ path):\n        self.path = path\n        self.data_file = open(f'{path}/data', 'ab+')\n        self.index = {}\
          \  # seq -> file_offset\n        self.next_seq = 0\n        self._rebuild_index()\n    \n    def _rebuild_index(self):\n\
          \        self.data_file.seek(0)\n        offset = 0\n        while True:\n            header = self.data_file.read(self.HEADER_SIZE)\n\
          \            if len(header) < self.HEADER_SIZE:\n                break\n            seq, length = struct.unpack('>QI',\
          \ header)\n            self.index[seq] = offset\n            self.next_seq = seq + 1\n            offset += self.HEADER_SIZE\
          \ + length\n            self.data_file.seek(offset)\n    \n    def append(self, data: bytes) -> int:\n        seq\
          \ = self.next_seq\n        self.next_seq += 1\n        \n        offset = self.data_file.seek(0, 2)  # EOF\n   \
          \     header = struct.pack('>QI', seq, len(data))\n        self.data_file.write(header + data)\n        self.data_file.flush()\n\
          \        os.fsync(self.data_file.fileno())\n        \n        self.index[seq] = offset\n        return seq\n   \
          \ \n    def read(self, seq: int) -> bytes:\n        if seq not in self.index:\n            return None\n       \
          \ offset = self.index[seq]\n        self.data_file.seek(offset)\n        header = self.data_file.read(self.HEADER_SIZE)\n\
          \        _, length = struct.unpack('>QI', header)\n        return self.data_file.read(length)"
      pitfalls:
      - Partial writes on crash
      - Index corruption
      - File handle limits
      concepts:
      - Append-only logs
      - Sequence numbers
      - Durability
      estimated_hours: 3-5
    - id: 2
      name: Replication Protocol
      description: Implement primary-backup replication with follower sync.
      acceptance_criteria:
      - Primary accepts writes
      - Replicate to followers
      - Acknowledge after quorum
      - Handle follower lag
      hints:
        level1: Primary assigns sequence, sends to followers. Wait for majority before ack.
        level2: Followers send their last seq. Primary sends missing entries.
        level3: "class Primary:\n    def __init__(self, log, followers):\n        self.log = log\n        self.followers =\
          \ followers  # List of follower RPC clients\n        self.follower_progress = {f.id: 0 for f in followers}\n   \
          \ \n    async def append(self, data: bytes) -> int:\n        # Append locally\n        seq = self.log.append(data)\n\
          \        \n        # Replicate to followers\n        acks = 1  # Self\n        tasks = []\n        for follower\
          \ in self.followers:\n            tasks.append(self._replicate_to(follower, seq, data))\n        \n        results\
          \ = await asyncio.gather(*tasks, return_exceptions=True)\n        acks += sum(1 for r in results if r is True)\n\
          \        \n        # Check quorum (majority)\n        quorum = (len(self.followers) + 1) // 2 + 1\n        if acks\
          \ >= quorum:\n            return seq\n        raise ReplicationError('Failed to reach quorum')\n    \n    async\
          \ def _replicate_to(self, follower, seq, data):\n        try:\n            # Check if follower is behind\n     \
          \       follower_seq = self.follower_progress[follower.id]\n            if follower_seq < seq - 1:\n           \
          \     # Send missing entries\n                for s in range(follower_seq + 1, seq):\n                    entry\
          \ = self.log.read(s)\n                    await follower.append(s, entry)\n            \n            # Send current\
          \ entry\n            await follower.append(seq, data)\n            self.follower_progress[follower.id] = seq\n \
          \           return True\n        except Exception as e:\n            return False\n\nclass Follower:\n    def __init__(self,\
          \ log):\n        self.log = log\n    \n    def append(self, seq, data):\n        expected = self.log.next_seq\n\
          \        if seq != expected:\n            raise OutOfOrderError(f'Expected {expected}, got {seq}')\n        self.log.append(data)\n\
          \        return True"
      pitfalls:
      - Split brain without leader election
      - Replication lag
      - Network partitions
      concepts:
      - Primary-backup
      - Quorum
      - Replication lag
      estimated_hours: 5-8
    - id: 3
      name: Failure Detection
      description: Detect node failures and handle recovery.
      acceptance_criteria:
      - Heartbeat mechanism
      - Timeout-based detection
      - Follower catch-up on recovery
      - Handle primary failure notification
      hints:
        level1: Followers send periodic heartbeats. Primary tracks last seen time.
        level2: On recovery, follower requests entries from last known seq.
        level3: "class FailureDetector:\n    def __init__(self, timeout_ms=5000):\n        self.timeout = timeout_ms / 1000\n\
          \        self.last_heartbeat = {}  # node_id -> timestamp\n        self.suspected = set()\n    \n    def heartbeat_received(self,\
          \ node_id):\n        self.last_heartbeat[node_id] = time.time()\n        if node_id in self.suspected:\n       \
          \     self.suspected.remove(node_id)\n            self.on_node_recovered(node_id)\n    \n    def check_timeouts(self):\n\
          \        now = time.time()\n        for node_id, last in self.last_heartbeat.items():\n            if now - last\
          \ > self.timeout and node_id not in self.suspected:\n                self.suspected.add(node_id)\n             \
          \   self.on_node_suspected(node_id)\n\nclass RecoveringFollower:\n    def __init__(self, log, primary_client):\n\
          \        self.log = log\n        self.primary = primary_client\n    \n    async def catch_up(self):\n        # Get\
          \ our last sequence\n        my_last = self.log.next_seq - 1 if self.log.next_seq > 0 else -1\n        \n      \
          \  # Request entries from primary\n        entries = await self.primary.get_entries_since(my_last + 1)\n       \
          \ \n        for seq, data in entries:\n            if seq != self.log.next_seq:\n                raise ConsistencyError('Gap\
          \ in log')\n            self.log.append(data)\n        \n        print(f'Caught up to seq {self.log.next_seq - 1}')"
      pitfalls:
      - False positives in detection
      - Thundering herd on recovery
      - Consistency after catch-up
      concepts:
      - Failure detection
      - Heartbeats
      - Recovery protocols
      estimated_hours: 4-6
    - id: 4
      name: Client Interface
      description: Implement a client that handles primary discovery and failover.
      acceptance_criteria:
      - Find current primary
      - Retry on failure
      - Read from followers (optional)
      - Consistent reads
      hints:
        level1: Client caches primary address. On error, re-discover.
        level2: For reads, can go to any replica if staleness is acceptable.
        level3: "class LogClient:\n    def __init__(self, cluster_nodes):\n        self.nodes = cluster_nodes\n        self.primary\
          \ = None\n        self.primary_addr = None\n    \n    async def discover_primary(self):\n        for addr in self.nodes:\n\
          \            try:\n                client = await connect(addr)\n                info = await client.get_info()\n\
          \                if info['role'] == 'primary':\n                    self.primary = client\n                    self.primary_addr\
          \ = addr\n                    return\n                elif info.get('primary_addr'):\n                    # Follower\
          \ knows who primary is\n                    self.primary = await connect(info['primary_addr'])\n               \
          \     self.primary_addr = info['primary_addr']\n                    return\n            except Exception:\n    \
          \            continue\n        raise NoPrimaryError('Cannot find primary')\n    \n    async def append(self, data:\
          \ bytes, retries=3) -> int:\n        for attempt in range(retries):\n            try:\n                if not self.primary:\n\
          \                    await self.discover_primary()\n                return await self.primary.append(data)\n   \
          \         except (ConnectionError, PrimaryChangedError):\n                self.primary = None\n                if\
          \ attempt == retries - 1:\n                    raise\n    \n    async def read(self, seq: int, allow_stale=False)\
          \ -> bytes:\n        if allow_stale:\n            # Read from any node\n            for addr in self.nodes:\n  \
          \              try:\n                    client = await connect(addr)\n                    return await client.read(seq)\n\
          \                except Exception:\n                    continue\n        \n        # Read from primary for consistency\n\
          \        if not self.primary:\n            await self.discover_primary()\n        return await self.primary.read(seq)"
      pitfalls:
      - Stale primary cache
      - Read-your-writes consistency
      - Infinite retry loops
      concepts:
      - Service discovery
      - Client-side failover
      - Consistency levels
      estimated_hours: 3-6
  rpc-basic:
    id: rpc-basic
    name: RPC Framework (Basic)
    description: Build a simple Remote Procedure Call framework. Learn serialization, network protocols, and client-server
      patterns.
    difficulty: beginner
    estimated_hours: 10-15
    prerequisites:
    - TCP sockets
    - JSON/serialization
    - Client-server architecture
    languages:
      recommended:
      - Python
      - Go
      - Java
      also_possible:
      - JavaScript
      - Rust
    resources:
    - name: gRPC Concepts
      url: https://grpc.io/docs/what-is-grpc/core-concepts/
      type: documentation
    - name: JSON-RPC Spec
      url: https://www.jsonrpc.org/specification
      type: specification
    milestones:
    - id: 1
      name: Message Protocol
      description: Define request/response message format.
      acceptance_criteria:
      - 'Request: method name, parameters, request ID'
      - 'Response: result or error, request ID'
      - JSON serialization
      - Handle different parameter types
      hints:
        level1: Request = {method, params, id}. Response = {result, error, id}.
        level2: Use JSON for simplicity. Match request ID in response.
        level3: "import json\nfrom dataclasses import dataclass, asdict\nfrom typing import Any, Optional, List\n\n@dataclass\n\
          class RPCRequest:\n    method: str\n    params: List[Any]\n    id: int\n    \n    def to_json(self) -> str:\n  \
          \      return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls, data: str) -> 'RPCRequest':\n\
          \        d = json.loads(data)\n        return cls(d['method'], d['params'], d['id'])\n\n@dataclass\nclass RPCResponse:\n\
          \    id: int\n    result: Any = None\n    error: Optional[str] = None\n    \n    def to_json(self) -> str:\n   \
          \     return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls, data: str) -> 'RPCResponse':\n\
          \        d = json.loads(data)\n        return cls(d['id'], d.get('result'), d.get('error'))"
      pitfalls:
      - ID mismatch
      - Serialization errors
      - Missing error handling
      concepts:
      - RPC protocol
      - Message serialization
      - Request-response pattern
      estimated_hours: 2-3
    - id: 2
      name: Server Implementation
      description: Build RPC server that handles method calls.
      acceptance_criteria:
      - Register callable methods
      - Parse incoming requests
      - Execute method and return result
      - Handle errors gracefully
      hints:
        level1: Server maintains registry of method_name -> function.
        level2: Receive request, lookup method, call with params, send response.
        level3: "import socket\nimport threading\n\nclass RPCServer:\n    def __init__(self, host='localhost', port=8000):\n\
          \        self.host = host\n        self.port = port\n        self.methods = {}\n    \n    def register(self, name:\
          \ str, func):\n        self.methods[name] = func\n    \n    def handle_client(self, conn, addr):\n        try:\n\
          \            while True:\n                data = conn.recv(4096)\n                if not data:\n               \
          \     break\n                \n                request = RPCRequest.from_json(data.decode())\n                response\
          \ = self.execute(request)\n                conn.sendall(response.to_json().encode())\n        finally:\n       \
          \     conn.close()\n    \n    def execute(self, request: RPCRequest) -> RPCResponse:\n        try:\n           \
          \ if request.method not in self.methods:\n                return RPCResponse(request.id, error=f'Method not found:\
          \ {request.method}')\n            \n            func = self.methods[request.method]\n            result = func(*request.params)\n\
          \            return RPCResponse(request.id, result=result)\n        except Exception as e:\n            return RPCResponse(request.id,\
          \ error=str(e))\n    \n    def serve(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\
          \        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        sock.bind((self.host, self.port))\n\
          \        sock.listen(5)\n        print(f'RPC Server listening on {self.host}:{self.port}')\n        \n        while\
          \ True:\n            conn, addr = sock.accept()\n            thread = threading.Thread(target=self.handle_client,\
          \ args=(conn, addr))\n            thread.start()"
      pitfalls:
      - Method not found
      - Parameter count mismatch
      - Blocking calls
      concepts:
      - Method registry
      - Request dispatching
      - Error handling
      estimated_hours: 3-4
    - id: 3
      name: Client Implementation
      description: Build RPC client with method proxying.
      acceptance_criteria:
      - Connect to server
      - Send requests and wait for response
      - Proxy object for natural method calls
      - Handle timeouts
      hints:
        level1: Client sends request, blocks until response with matching ID.
        level2: Use __getattr__ to proxy method calls dynamically.
        level3: "class RPCClient:\n    def __init__(self, host='localhost', port=8000):\n        self.host = host\n      \
          \  self.port = port\n        self.sock = None\n        self.request_id = 0\n    \n    def connect(self):\n     \
          \   self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.connect((self.host, self.port))\n\
          \    \n    def close(self):\n        if self.sock:\n            self.sock.close()\n    \n    def call(self, method:\
          \ str, *params, timeout=30):\n        self.request_id += 1\n        request = RPCRequest(method, list(params), self.request_id)\n\
          \        \n        self.sock.settimeout(timeout)\n        self.sock.sendall(request.to_json().encode())\n      \
          \  \n        data = self.sock.recv(4096)\n        response = RPCResponse.from_json(data.decode())\n        \n  \
          \      if response.id != self.request_id:\n            raise Exception('Response ID mismatch')\n        \n     \
          \   if response.error:\n            raise Exception(response.error)\n        \n        return response.result\n\
          \    \n    def __getattr__(self, name):\n        '''Proxy method calls: client.add(1, 2) -> client.call('add', 1,\
          \ 2)'''\n        def method(*args):\n            return self.call(name, *args)\n        return method\n\n# Usage:\n\
          client = RPCClient()\nclient.connect()\nresult = client.add(1, 2)  # Calls remote 'add' method\nprint(result)  #\
          \ 3"
      pitfalls:
      - Connection management
      - Timeout handling
      - ID tracking
      concepts:
      - RPC client
      - Method proxying
      - Connection pooling
      estimated_hours: 3-4
  service-discovery:
    id: service-discovery
    name: Service Discovery
    description: Build a service registry with health checking. Learn how microservices find and communicate with each other.
    difficulty: beginner
    estimated_hours: 10-15
    prerequisites:
    - HTTP basics
    - Networking concepts
    languages:
      recommended:
      - Go
      - Python
      - JavaScript
      also_possible:
      - Java
      - Rust
    resources:
    - name: Service Discovery Patterns
      url: https://microservices.io/patterns/service-registry.html
      type: article
    milestones:
    - id: 1
      name: Service Registry
      description: Build basic service registration and lookup.
      acceptance_criteria:
      - Register service with name, host, port
      - Deregister service
      - List services by name
      - Get all registered services
      hints:
        level1: 'Store services in a dict: name -> list of instances.'
        level2: Each instance has unique ID, host, port, metadata.
        level3: "import uuid\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n@dataclass\nclass ServiceInstance:\n\
          \    id: str\n    name: str\n    host: str\n    port: int\n    metadata: dict = None\n\nclass ServiceRegistry:\n\
          \    def __init__(self):\n        self.services: Dict[str, Dict[str, ServiceInstance]] = {}\n    \n    def register(self,\
          \ name: str, host: str, port: int, metadata: dict = None) -> str:\n        instance_id = str(uuid.uuid4())\n   \
          \     instance = ServiceInstance(instance_id, name, host, port, metadata or {})\n        if name not in self.services:\n\
          \            self.services[name] = {}\n        self.services[name][instance_id] = instance\n        return instance_id\n\
          \    \n    def deregister(self, name: str, instance_id: str):\n        if name in self.services and instance_id\
          \ in self.services[name]:\n            del self.services[name][instance_id]\n    \n    def get_instances(self, name:\
          \ str) -> List[ServiceInstance]:\n        return list(self.services.get(name, {}).values())"
      pitfalls:
      - Not handling duplicate registrations
      - Race conditions
      - Memory growth without cleanup
      concepts:
      - Service registry pattern
      - Instance identity
      - Lookup by name
      estimated_hours: 2-3
    - id: 2
      name: Health Checking
      description: Add health checks to detect failed services.
      acceptance_criteria:
      - HTTP health endpoint on services
      - Registry polls health endpoint
      - Remove unhealthy instances
      - Configurable check interval
      hints:
        level1: Each service exposes /health returning 200.
        level2: Background thread checks all instances periodically.
        level3: "import threading\nimport requests\nimport time\n\nclass ServiceRegistry:\n    def __init__(self, health_check_interval=10):\n\
          \        self.services = {}\n        self.health_check_interval = health_check_interval\n        self._start_health_checker()\n\
          \    \n    def _start_health_checker(self):\n        def check_loop():\n            while True:\n              \
          \  self._check_all_health()\n                time.sleep(self.health_check_interval)\n        thread = threading.Thread(target=check_loop,\
          \ daemon=True)\n        thread.start()\n    \n    def _check_all_health(self):\n        for name, instances in list(self.services.items()):\n\
          \            for instance_id, instance in list(instances.items()):\n                try:\n                    url\
          \ = f'http://{instance.host}:{instance.port}/health'\n                    resp = requests.get(url, timeout=5)\n\
          \                    if resp.status_code != 200:\n                        self._mark_unhealthy(name, instance_id)\n\
          \                except Exception:\n                    self._mark_unhealthy(name, instance_id)\n    \n    def _mark_unhealthy(self,\
          \ name, instance_id):\n        # Could implement retry logic or immediate removal\n        self.deregister(name,\
          \ instance_id)\n        print(f'Removed unhealthy instance: {name}/{instance_id}')"
      pitfalls:
      - Network partition false positives
      - Check timeout too short
      - Not handling transient failures
      concepts:
      - Health checking
      - Failure detection
      - Background tasks
      estimated_hours: 3-4
    - id: 3
      name: HTTP API
      description: Expose registry operations via HTTP API.
      acceptance_criteria:
      - POST /services to register
      - DELETE /services/{name}/{id} to deregister
      - GET /services/{name} to list instances
      - GET /services to list all
      hints:
        level1: Use Flask/FastAPI/Express for HTTP server.
        level2: Return JSON responses with proper status codes.
        level3: "from flask import Flask, request, jsonify\n\napp = Flask(__name__)\nregistry = ServiceRegistry()\n\n@app.route('/services',\
          \ methods=['POST'])\ndef register_service():\n    data = request.json\n    instance_id = registry.register(\n  \
          \      name=data['name'],\n        host=data['host'],\n        port=data['port'],\n        metadata=data.get('metadata')\n\
          \    )\n    return jsonify({'id': instance_id}), 201\n\n@app.route('/services/<name>/<instance_id>', methods=['DELETE'])\n\
          def deregister_service(name, instance_id):\n    registry.deregister(name, instance_id)\n    return '', 204\n\n@app.route('/services/<name>',\
          \ methods=['GET'])\ndef get_service(name):\n    instances = registry.get_instances(name)\n    return jsonify([{\n\
          \        'id': i.id,\n        'host': i.host,\n        'port': i.port,\n        'metadata': i.metadata\n    } for\
          \ i in instances])\n\n@app.route('/services', methods=['GET'])\ndef list_all_services():\n    return jsonify(list(registry.services.keys()))"
      pitfalls:
      - Missing validation
      - Not thread-safe
      - Error handling
      concepts:
      - REST API design
      - HTTP methods
      - JSON serialization
      estimated_hours: 2-3
  shell-basic:
    id: shell-basic
    name: Shell (Basic)
    description: Build a basic Unix shell with pipes and redirects. Learn process management, file descriptors, and Unix IPC.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - C programming
    - fork/exec
    - File descriptors
    languages:
      recommended:
      - C
      - Rust
      also_possible:
      - Go
      - Zig
    resources:
    - name: Writing a Shell in C
      url: https://brennan.io/2015/01/16/write-a-shell-in-c/
      type: tutorial
    - name: Shell Command Language
      url: https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html
      type: specification
    milestones:
    - id: 1
      name: REPL and Simple Commands
      description: Basic read-eval-print loop executing simple commands.
      acceptance_criteria:
      - Print prompt and read input
      - Parse command and arguments
      - Fork and exec the command
      - Wait for child and show exit status
      - Handle Ctrl+D (EOF)
      hints:
        level1: Use fork() to create child, execvp() to run command.
        level2: Split input by whitespace for arguments.
        level3: "while (1) {\n    printf(\"$ \");\n    if (!fgets(line, sizeof(line), stdin)) break;\n    char *args[64];\n\
          \    int argc = 0;\n    char *tok = strtok(line, \" \\t\\n\");\n    while (tok && argc < 63) {\n        args[argc++]\
          \ = tok;\n        tok = strtok(NULL, \" \\t\\n\");\n    }\n    args[argc] = NULL;\n    if (argc == 0) continue;\n\
          \    pid_t pid = fork();\n    if (pid == 0) {\n        execvp(args[0], args);\n        perror(args[0]);\n      \
          \  exit(127);\n    }\n    int status;\n    waitpid(pid, &status, 0);\n}"
      pitfalls:
      - Not null-terminating args array
      - Child not calling exit after exec fails
      - Not handling empty input
      concepts:
      - Process creation
      - exec family
      - Wait and status
      estimated_hours: 3-4
    - id: 2
      name: Built-in Commands
      description: Implement shell built-ins that can't be external commands.
      acceptance_criteria:
      - cd changes directory
      - pwd prints working directory
      - exit terminates shell
      - export sets environment variables
      hints:
        level1: Built-ins run in shell process, not forked.
        level2: Use chdir() for cd, getcwd() for pwd.
        level3: "if (strcmp(args[0], \"cd\") == 0) {\n    char *dir = args[1] ? args[1] : getenv(\"HOME\");\n    if (chdir(dir)\
          \ != 0) perror(\"cd\");\n    continue;  // Don't fork\n}\nif (strcmp(args[0], \"exit\") == 0) {\n    int code =\
          \ args[1] ? atoi(args[1]) : 0;\n    exit(code);\n}"
      pitfalls:
      - cd with no args should go to HOME
      - exit status from built-ins
      - Environment inheritance
      concepts:
      - Shell built-ins
      - Working directory
      - Environment variables
      estimated_hours: 2-3
    - id: 3
      name: I/O Redirection
      description: Implement input/output redirection.
      acceptance_criteria:
      - '> redirects stdout to file'
      - < redirects stdin from file
      - '>> appends to file'
      - 2> redirects stderr
      hints:
        level1: 'In child, before exec: close fd, open file, dup2 to 0/1/2.'
        level2: Parse redirections before executing.
        level3: "// In child process, before execvp:\nif (output_file) {\n    int fd = open(output_file, O_WRONLY | O_CREAT\
          \ | (append ? O_APPEND : O_TRUNC), 0644);\n    if (fd < 0) { perror(output_file); exit(1); }\n    dup2(fd, STDOUT_FILENO);\n\
          \    close(fd);\n}\nif (input_file) {\n    int fd = open(input_file, O_RDONLY);\n    if (fd < 0) { perror(input_file);\
          \ exit(1); }\n    dup2(fd, STDIN_FILENO);\n    close(fd);\n}"
      pitfalls:
      - File permissions on create
      - dup2 error handling
      - Closing original fd after dup2
      concepts:
      - File descriptors
      - dup2 system call
      - Standard streams
      estimated_hours: 3-4
    - id: 4
      name: Pipes
      description: Implement command pipelines.
      acceptance_criteria:
      - cmd1 | cmd2 pipes stdout to stdin
      - 'Multiple pipes: cmd1 | cmd2 | cmd3'
      - Error handling in pipeline
      - Exit status from last command
      hints:
        level1: Use pipe() to create fd pair. Fork twice.
        level2: 'First child: stdout -> pipe write. Second: stdin <- pipe read.'
        level3: "int pipefd[2];\npipe(pipefd);\npid_t pid1 = fork();\nif (pid1 == 0) {\n    close(pipefd[0]);\n    dup2(pipefd[1],\
          \ STDOUT_FILENO);\n    close(pipefd[1]);\n    execvp(cmd1[0], cmd1);\n    exit(127);\n}\npid_t pid2 = fork();\n\
          if (pid2 == 0) {\n    close(pipefd[1]);\n    dup2(pipefd[0], STDIN_FILENO);\n    close(pipefd[0]);\n    execvp(cmd2[0],\
          \ cmd2);\n    exit(127);\n}\nclose(pipefd[0]);\nclose(pipefd[1]);\nwaitpid(pid1, NULL, 0);\nint status;\nwaitpid(pid2,\
          \ &status, 0);"
      pitfalls:
      - Not closing all pipe ends
      - Deadlock from blocking read
      - Order of fork and close
      concepts:
      - Pipes and IPC
      - File descriptor inheritance
      - Pipeline coordination
      estimated_hours: 4-6
  signal-handler:
    id: signal-handler
    name: Signal Handler
    description: Master Unix signal handling. Learn signal safety, masks, and graceful shutdown.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - C programming
    - Unix basics
    - Process concepts
    languages:
      recommended:
      - C
      - Rust
      also_possible:
      - Go
      - Python
    resources:
    - name: signal(7) man page
      url: https://man7.org/linux/man-pages/man7/signal.7.html
      type: documentation
    - name: Signal Safety
      url: https://man7.org/linux/man-pages/man7/signal-safety.7.html
      type: documentation
    milestones:
    - id: 1
      name: Basic Signal Handling
      description: Install signal handlers for common signals.
      acceptance_criteria:
      - Handle SIGINT (Ctrl+C)
      - Handle SIGTERM
      - Use sigaction() not signal()
      - Set SA_RESTART flag
      hints:
        level1: sigaction() is portable and reliable. signal() has race conditions.
        level2: SA_RESTART makes interrupted syscalls restart automatically.
        level3: "#include <signal.h>\n#include <stdio.h>\n#include <unistd.h>\n\nvolatile sig_atomic_t running = 1;\n\nvoid\
          \ handle_signal(int sig) {\n    // Only async-signal-safe operations here!\n    if (sig == SIGINT || sig == SIGTERM)\
          \ {\n        running = 0;\n    }\n}\n\nint setup_signal_handler(int sig, void (*handler)(int)) {\n    struct sigaction\
          \ sa;\n    sa.sa_handler = handler;\n    sigemptyset(&sa.sa_mask);\n    sa.sa_flags = SA_RESTART;  // Restart interrupted\
          \ syscalls\n    \n    if (sigaction(sig, &sa, NULL) < 0) {\n        perror(\"sigaction\");\n        return -1;\n\
          \    }\n    return 0;\n}\n\nint main() {\n    setup_signal_handler(SIGINT, handle_signal);\n    setup_signal_handler(SIGTERM,\
          \ handle_signal);\n    \n    printf(\"Running... Press Ctrl+C to stop\\n\");\n    \n    while (running) {\n    \
          \    // Do work\n        sleep(1);\n        printf(\".\");\n        fflush(stdout);\n    }\n    \n    printf(\"\\\
          nGraceful shutdown\\n\");\n    return 0;\n}"
      pitfalls:
      - Non-reentrant functions in handler
      - Race conditions
      - Missing SA_RESTART
      concepts:
      - Signal handlers
      - sigaction()
      - Async-signal safety
      estimated_hours: 2-3
    - id: 2
      name: Signal Masking
      description: Block and unblock signals for critical sections.
      acceptance_criteria:
      - Block signals during critical code
      - Use sigprocmask()
      - Handle pending signals
      - Per-thread signal masks
      hints:
        level1: sigprocmask() blocks signals temporarily. Pending signals delivered when unblocked.
        level2: Block signals before modifying shared data, unblock after.
        level3: "#include <signal.h>\n\nvoid block_signals(sigset_t *oldmask) {\n    sigset_t blockmask;\n    sigemptyset(&blockmask);\n\
          \    sigaddset(&blockmask, SIGINT);\n    sigaddset(&blockmask, SIGTERM);\n    sigaddset(&blockmask, SIGCHLD);\n\
          \    \n    if (sigprocmask(SIG_BLOCK, &blockmask, oldmask) < 0) {\n        perror(\"sigprocmask\");\n    }\n}\n\n\
          void unblock_signals(sigset_t *oldmask) {\n    if (sigprocmask(SIG_SETMASK, oldmask, NULL) < 0) {\n        perror(\"\
          sigprocmask\");\n    }\n}\n\nvoid critical_section() {\n    sigset_t oldmask;\n    \n    block_signals(&oldmask);\n\
          \    \n    // Critical code - signals blocked\n    // Modify shared data structures safely\n    update_shared_state();\n\
          \    \n    unblock_signals(&oldmask);\n    // Pending signals delivered here\n}\n\n// Check for pending signals\n\
          void check_pending() {\n    sigset_t pending;\n    sigpending(&pending);\n    \n    if (sigismember(&pending, SIGINT))\
          \ {\n        printf(\"SIGINT is pending\\n\");\n    }\n}"
      pitfalls:
      - Forgetting to unblock
      - Deadlock with nested blocking
      - Thread safety
      concepts:
      - Signal masks
      - Critical sections
      - Pending signals
      estimated_hours: 2-3
    - id: 3
      name: Self-Pipe Trick
      description: Integrate signals with select/poll event loop.
      acceptance_criteria:
      - Create self-pipe
      - Write to pipe in signal handler
      - select/poll on pipe
      - Handle signal in main loop
      hints:
        level1: Signal handlers can't do much safely. Write byte to pipe, handle in event loop.
        level2: Make pipe non-blocking. select() returns when pipe readable.
        level3: "#include <fcntl.h>\n#include <sys/select.h>\n\nint signal_pipe[2];\n\nvoid signal_handler(int sig) {\n  \
          \  // Async-signal-safe: write single byte\n    int saved_errno = errno;\n    char c = sig;\n    write(signal_pipe[1],\
          \ &c, 1);\n    errno = saved_errno;\n}\n\nvoid setup_self_pipe() {\n    if (pipe(signal_pipe) < 0) {\n        perror(\"\
          pipe\");\n        exit(1);\n    }\n    \n    // Make non-blocking\n    fcntl(signal_pipe[0], F_SETFL, O_NONBLOCK);\n\
          \    fcntl(signal_pipe[1], F_SETFL, O_NONBLOCK);\n    \n    setup_signal_handler(SIGINT, signal_handler);\n    setup_signal_handler(SIGTERM,\
          \ signal_handler);\n    setup_signal_handler(SIGCHLD, signal_handler);\n}\n\nvoid event_loop() {\n    fd_set readfds;\n\
          \    \n    while (running) {\n        FD_ZERO(&readfds);\n        FD_SET(signal_pipe[0], &readfds);\n        FD_SET(client_socket,\
          \ &readfds);  // Other FDs\n        \n        int maxfd = signal_pipe[0] > client_socket ? signal_pipe[0] : client_socket;\n\
          \        \n        int ret = select(maxfd + 1, &readfds, NULL, NULL, NULL);\n        if (ret < 0 && errno != EINTR)\
          \ {\n            perror(\"select\");\n            break;\n        }\n        \n        if (FD_ISSET(signal_pipe[0],\
          \ &readfds)) {\n            // Signal received - handle safely here\n            char sig;\n            while (read(signal_pipe[0],\
          \ &sig, 1) > 0) {\n                handle_signal_safely(sig);\n            }\n        }\n        \n        if (FD_ISSET(client_socket,\
          \ &readfds)) {\n            handle_client();\n        }\n    }\n}"
      pitfalls:
      - Pipe buffer full
      - Non-blocking write
      - Multiple signals coalescing
      concepts:
      - Self-pipe trick
      - Event loops
      - Signal integration
      estimated_hours: 3-4
  simple-gc:
    id: simple-gc
    name: Simple Garbage Collector
    description: Implement a mark-sweep garbage collector. Learn memory management and object traversal.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - Memory management basics
    - Graph traversal
    - C pointers
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python (educational)
    resources:
    - name: The Garbage Collection Handbook
      url: https://gchandbook.org/
      type: book
    - name: Baby's First GC
      url: https://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/
      type: article
    milestones:
    - id: 1
      name: Object Model
      description: Define object representation with GC metadata.
      acceptance_criteria:
      - Object header with type info
      - Mark bit for GC
      - Object size tracking
      - Support different object types
      hints:
        level1: 'Every object needs header: marked flag, type, size.'
        level2: Use a common header struct that precedes object data.
        level3: "typedef enum {\n    OBJ_INT,\n    OBJ_PAIR,\n    OBJ_STRING,\n} ObjectType;\n\ntypedef struct Object {\n\
          \    ObjectType type;\n    unsigned char marked;\n    struct Object* next;  // For allocation list\n    \n    union\
          \ {\n        int int_value;\n        struct {\n            struct Object* head;\n            struct Object* tail;\n\
          \        } pair;\n        struct {\n            char* chars;\n            int length;\n        } string;\n    }\
          \ as;\n} Object;\n\ntypedef struct {\n    Object* first_object;  // Head of all allocated objects\n    Object**\
          \ stack;        // Root stack\n    int stack_size;\n    int stack_capacity;\n    int num_objects;\n    int max_objects;\
          \       // Threshold for GC\n} VM;\n\nObject* new_object(VM* vm, ObjectType type) {\n    if (vm->num_objects >=\
          \ vm->max_objects) {\n        gc(vm);\n    }\n    \n    Object* obj = malloc(sizeof(Object));\n    obj->type = type;\n\
          \    obj->marked = 0;\n    obj->next = vm->first_object;\n    vm->first_object = obj;\n    vm->num_objects++;\n\
          \    \n    return obj;\n}"
      pitfalls:
      - Header alignment
      - Type safety
      - Object size calculation
      concepts:
      - Object headers
      - Type tags
      - Allocation lists
      estimated_hours: 3-4
    - id: 2
      name: Root Discovery
      description: Identify and traverse GC roots.
      acceptance_criteria:
      - Track stack roots
      - Global variable roots
      - Register roots (if applicable)
      - Precise vs conservative roots
      hints:
        level1: Roots = live references not reachable from heap. Stack, globals.
        level2: Maintain explicit stack of roots for simplicity.
        level3: "void push_root(VM* vm, Object* obj) {\n    if (vm->stack_size >= vm->stack_capacity) {\n        vm->stack_capacity\
          \ *= 2;\n        vm->stack = realloc(vm->stack, sizeof(Object*) * vm->stack_capacity);\n    }\n    vm->stack[vm->stack_size++]\
          \ = obj;\n}\n\nObject* pop_root(VM* vm) {\n    return vm->stack[--vm->stack_size];\n}\n\nvoid mark_roots(VM* vm)\
          \ {\n    // Mark all objects on the stack\n    for (int i = 0; i < vm->stack_size; i++) {\n        mark_object(vm->stack[i]);\n\
          \    }\n    \n    // Mark globals (if any)\n    // mark_object(vm->globals);\n}\n\n// For conservative GC (scan\
          \ memory for pointers)\nvoid scan_stack_conservative(VM* vm, void* stack_top, void* stack_bottom) {\n    for (void**\
          \ p = stack_top; p < stack_bottom; p++) {\n        void* potential_ptr = *p;\n        // Check if this looks like\
          \ a valid object pointer\n        Object* obj = find_object_at_address(vm, potential_ptr);\n        if (obj) {\n\
          \            mark_object(obj);\n        }\n    }\n}"
      pitfalls:
      - Missing roots
      - Stack scanning direction
      - Interior pointers
      concepts:
      - Root set
      - Precise vs conservative
      - Stack scanning
      estimated_hours: 3-4
    - id: 3
      name: Mark Phase
      description: Traverse and mark all reachable objects.
      acceptance_criteria:
      - Recursive marking
      - Handle cycles
      - Mark bit manipulation
      - Worklist-based marking (optional)
      hints:
        level1: 'Mark: start from roots, follow pointers, set mark bit.'
        level2: Use recursion or explicit worklist to avoid stack overflow.
        level3: "void mark_object(Object* obj) {\n    if (obj == NULL) return;\n    if (obj->marked) return;  // Already visited\n\
          \    \n    obj->marked = 1;\n    \n    // Recursively mark referenced objects\n    switch (obj->type) {\n      \
          \  case OBJ_INT:\n            // No references\n            break;\n        case OBJ_PAIR:\n            mark_object(obj->as.pair.head);\n\
          \            mark_object(obj->as.pair.tail);\n            break;\n        case OBJ_STRING:\n            // No object\
          \ references (just char*)\n            break;\n    }\n}\n\n// Worklist version (avoids deep recursion)\nvoid mark_object_worklist(VM*\
          \ vm, Object* obj) {\n    if (obj == NULL || obj->marked) return;\n    \n    Object** worklist = malloc(sizeof(Object*)\
          \ * vm->num_objects);\n    int worklist_size = 0;\n    \n    worklist[worklist_size++] = obj;\n    \n    while (worklist_size\
          \ > 0) {\n        Object* current = worklist[--worklist_size];\n        if (current->marked) continue;\n       \
          \ current->marked = 1;\n        \n        if (current->type == OBJ_PAIR) {\n            if (current->as.pair.head\
          \ && !current->as.pair.head->marked)\n                worklist[worklist_size++] = current->as.pair.head;\n     \
          \       if (current->as.pair.tail && !current->as.pair.tail->marked)\n                worklist[worklist_size++]\
          \ = current->as.pair.tail;\n        }\n    }\n    \n    free(worklist);\n}"
      pitfalls:
      - Stack overflow on deep structures
      - Forgetting object types
      - Double marking
      concepts:
      - Graph traversal
      - Mark bits
      - Tri-color marking
      estimated_hours: 3-4
    - id: 4
      name: Sweep Phase
      description: Reclaim unmarked objects and reset marks.
      acceptance_criteria:
      - Walk allocation list
      - Free unmarked objects
      - Reset mark bits on live objects
      - Update allocation list
      hints:
        level1: 'Sweep: walk all objects, free unmarked, clear marks on marked.'
        level2: Update linked list as you go - skip freed objects.
        level3: "void sweep(VM* vm) {\n    Object** obj_ptr = &vm->first_object;\n    \n    while (*obj_ptr != NULL) {\n \
          \       if (!(*obj_ptr)->marked) {\n            // Unreachable - free it\n            Object* unreached = *obj_ptr;\n\
          \            *obj_ptr = unreached->next;  // Unlink\n            \n            // Free any additional memory\n \
          \           if (unreached->type == OBJ_STRING) {\n                free(unreached->as.string.chars);\n          \
          \  }\n            \n            free(unreached);\n            vm->num_objects--;\n        } else {\n           \
          \ // Reachable - clear mark for next GC\n            (*obj_ptr)->marked = 0;\n            obj_ptr = &(*obj_ptr)->next;\n\
          \        }\n    }\n}\n\nvoid gc(VM* vm) {\n    int num_before = vm->num_objects;\n    \n    // Mark phase\n    mark_roots(vm);\n\
          \    \n    // Sweep phase\n    sweep(vm);\n    \n    // Adjust threshold\n    vm->max_objects = vm->num_objects\
          \ * 2;\n    \n    printf(\"Collected %d objects, %d remaining.\\n\",\n           num_before - vm->num_objects, vm->num_objects);\n\
          }"
      pitfalls:
      - List corruption
      - Forgetting to clear marks
      - Memory leaks in objects
      concepts:
      - Memory reclamation
      - Linked list manipulation
      - GC thresholds
      estimated_hours: 3-4
  snake:
    id: snake
    name: Snake
    description: Build the classic Snake game. Learn about grid-based movement, growing arrays, and collision detection with
      self.
    difficulty: beginner
    estimated_hours: 10-14
    prerequisites:
    - Basic programming
    - Array manipulation
    - HTML5 Canvas or similar
    languages:
      recommended:
      - JavaScript
      - Python
      - C#
      also_possible:
      - Rust
      - Go
      - Lua
    resources:
    - name: Snake with JavaScript
      url: https://www.youtube.com/watch?v=7Azlj0f9vas
      type: video
    - name: Python Snake with Pygame
      url: https://realpython.com/pygame-a-primer/
      type: tutorial
    milestones:
    - id: 1
      name: Grid Setup & Snake Rendering
      description: Set up the grid and render the snake.
      acceptance_criteria:
      - Game grid drawn on canvas
      - Snake represented as array of segments
      - Snake renders as connected blocks
      - Grid-based positioning
      - Clear visual distinction for head
      hints:
        level1: Snake is array of {x, y} coordinates. Head is index 0.
        level2: Grid cell size (e.g., 20px). Snake position in grid units.
        level3: "const GRID_SIZE = 20;\nlet snake = [\n  { x: 15, y: 10 }, // head\n  { x: 14, y: 10 },\n  { x: 13, y: 10\
          \ }\n];\n\nfunction draw() {\n  ctx.clearRect(0, 0, canvas.width, canvas.height);\n  snake.forEach((segment, i)\
          \ => {\n    ctx.fillStyle = i === 0 ? 'darkgreen' : 'green';\n    ctx.fillRect(segment.x * GRID_SIZE, segment.y\
          \ * GRID_SIZE, GRID_SIZE - 1, GRID_SIZE - 1);\n  });\n}"
      pitfalls:
      - Drawing snake in wrong order
      - Off-by-one in grid positioning
      - Not leaving gap between segments
      concepts:
      - Grid-based games
      - Array representation
      - Canvas rendering
      estimated_hours: 2-3
    - id: 2
      name: Movement & Direction
      description: Implement snake movement with keyboard controls.
      acceptance_criteria:
      - Snake moves automatically in current direction
      - Arrow keys change direction
      - Cannot reverse direction (no 180Â° turns)
      - Movement updates at regular interval
      - Smooth, grid-aligned movement
      hints:
        level1: Use setInterval for fixed movement timing, not requestAnimationFrame.
        level2: Queue direction changes, apply at next movement tick.
        level3: "let direction = { x: 1, y: 0 };\nlet nextDirection = { x: 1, y: 0 };\n\ndocument.addEventListener('keydown',\
          \ e => {\n  switch(e.key) {\n    case 'ArrowUp': if (direction.y !== 1) nextDirection = {x: 0, y: -1}; break;\n\
          \    case 'ArrowDown': if (direction.y !== -1) nextDirection = {x: 0, y: 1}; break;\n    case 'ArrowLeft': if (direction.x\
          \ !== 1) nextDirection = {x: -1, y: 0}; break;\n    case 'ArrowRight': if (direction.x !== -1) nextDirection = {x:\
          \ 1, y: 0}; break;\n  }\n});\n\nfunction moveSnake() {\n  direction = nextDirection;\n  const head = { x: snake[0].x\
          \ + direction.x, y: snake[0].y + direction.y };\n  snake.unshift(head);\n  snake.pop();\n}"
      pitfalls:
      - Reversing and dying immediately
      - Multiple direction changes between ticks
      - Movement too fast or slow
      concepts:
      - Timer-based updates
      - Direction queuing
      - Movement constraints
      estimated_hours: 2-3
    - id: 3
      name: Food & Growth
      description: Add food spawning and snake growth.
      acceptance_criteria:
      - Food appears at random location
      - Food not spawned on snake
      - Eating food grows snake
      - New food spawns after eating
      - Score tracking
      hints:
        level1: 'To grow: don''t remove tail when eating food.'
        level2: Generate food position, check it's not on snake, regenerate if needed.
        level3: "function spawnFood() {\n  let pos;\n  do {\n    pos = {\n      x: Math.floor(Math.random() * GRID_WIDTH),\n\
          \      y: Math.floor(Math.random() * GRID_HEIGHT)\n    };\n  } while (snake.some(s => s.x === pos.x && s.y === pos.y));\n\
          \  return pos;\n}\n\nfunction moveSnake() {\n  const head = { x: snake[0].x + direction.x, y: snake[0].y + direction.y\
          \ };\n  snake.unshift(head);\n  if (head.x === food.x && head.y === food.y) {\n    score++;\n    food = spawnFood();\n\
          \  } else {\n    snake.pop();\n  }\n}"
      pitfalls:
      - Food spawning on snake
      - Infinite loop if snake fills grid
      - Growth happening wrong direction
      concepts:
      - Random positioning
      - Collision with objects
      - Array growth
      estimated_hours: 2-3
    - id: 4
      name: Collision & Game Over
      description: Implement collision detection and game over.
      acceptance_criteria:
      - Collision with walls ends game
      - Collision with self ends game
      - Game over screen with score
      - Restart option
      - High score tracking
      hints:
        level1: 'Wall collision: head.x < 0 || head.x >= GRID_WIDTH.'
        level2: 'Self collision: check if head matches any other segment.'
        level3: "function checkCollision() {\n  const head = snake[0];\n  // Wall collision\n  if (head.x < 0 || head.x >=\
          \ GRID_WIDTH || head.y < 0 || head.y >= GRID_HEIGHT) {\n    return true;\n  }\n  // Self collision (skip head at\
          \ index 0)\n  for (let i = 1; i < snake.length; i++) {\n    if (head.x === snake[i].x && head.y === snake[i].y)\
          \ return true;\n  }\n  return false;\n}"
      pitfalls:
      - Checking collision before movement
      - Self collision including head
      - High score not persisting
      concepts:
      - Self-collision
      - Game states
      - Local storage
      estimated_hours: 2-3
  social-network:
    id: social-network
    name: Social Network
    description: Build a social network with user profiles, feeds, followers, and notifications. Learn about complex data
      relationships and real-time features at scale.
    difficulty: advanced
    estimated_hours: 60-80
    prerequisites:
    - Full-stack web development
    - Database design
    - Caching concepts
    - Real-time systems
    languages:
      recommended:
      - JavaScript
      - Python
      - Go
      also_possible:
      - Ruby
      - Java
      - Elixir
    resources:
    - name: Feed Architecture
      url: https://www.youtube.com/watch?v=QmX2NPkJTKg
      type: video
    - name: System Design Social Network
      url: https://www.youtube.com/results?search_query=system+design+social+network
      type: video
    milestones:
    - id: 1
      name: User Profiles & Follow System
      description: Build user profiles and the follow/follower relationship.
      acceptance_criteria:
      - User profile with bio, avatar, links
      - Follow/unfollow users
      - Follower and following lists
      - Follower/following counts
      - Profile editing
      hints:
        level1: Follow is a self-referential many-to-many relationship.
        level2: Store counts denormalized for performance.
        level3: "// Follow relationship\nmodel Follow {\n  id          Int      @id @default(autoincrement())\n  followerId\
          \  Int\n  followingId Int\n  createdAt   DateTime @default(now())\n  follower    User     @relation(\"follower\"\
          , ...)\n  following   User     @relation(\"following\", ...)\n  @@unique([followerId, followingId])\n  @@index([followingId])\n\
          }\n\n// Denormalized counts on User\nmodel User {\n  followerCount  Int @default(0)\n  followingCount Int @default(0)\n\
          }"
      pitfalls:
      - Self-follow allowed
      - Count drift from denormalization
      - N+1 queries for follower lists
      concepts:
      - Self-referential relations
      - Denormalization
      - Count caching
      estimated_hours: 8-10
    - id: 2
      name: Posts & Feed (Fan-out on Write)
      description: Implement posts and the home feed using fan-out on write.
      acceptance_criteria:
      - Create text/image posts
      - User's own post list
      - Home feed (posts from followed users)
      - Feed pagination (cursor-based)
      - Post timestamps
      hints:
        level1: 'Fan-out on write: when user posts, add to each follower''s feed.'
        level2: 'Use a Feed table: { userId, postId, createdAt }.'
        level3: "async function createPost(authorId, content) {\n  const post = await Post.create({ authorId, content });\n\
          \  const followers = await Follow.findMany({\n    where: { followingId: authorId },\n    select: { followerId: true\
          \ }\n  });\n  await FeedItem.createMany({\n    data: followers.map(f => ({\n      userId: f.followerId,\n      postId:\
          \ post.id,\n      createdAt: post.createdAt\n    }))\n  });\n  return post;\n}"
      pitfalls:
      - Fan-out blocking post creation (use queue)
      - Celebrity problem (millions of followers)
      - Offset pagination performance
      concepts:
      - Fan-out on write vs read
      - Feed generation
      - Cursor pagination
      estimated_hours: 10-12
    - id: 3
      name: Likes, Comments & Interactions
      description: Add social interactions to posts.
      acceptance_criteria:
      - Like/unlike posts
      - Comment on posts
      - Like counts (real-time update)
      - Comment threads
      - Share/repost
      hints:
        level1: 'Like: simple user-post relation. Comment: text + user + post.'
        level2: Use optimistic UI updates for likes.
        level3: "async function likePost(postId) {\n  // Optimistic update\n  setLiked(true);\n  setLikeCount(prev => prev\
          \ + 1);\n  try {\n    await api.post(`/posts/${postId}/like`);\n  } catch (error) {\n    // Rollback on error\n\
          \    setLiked(false);\n    setLikeCount(prev => prev - 1);\n  }\n}"
      pitfalls:
      - Double-like race condition
      - Comment ordering (newest vs oldest)
      - Deep nested replies complexity
      concepts:
      - Social interactions
      - Optimistic updates
      - Comment systems
      estimated_hours: 8-10
    - id: 4
      name: Notifications
      description: Build a notification system for social activity.
      acceptance_criteria:
      - Notification for new follower
      - Notification for likes/comments
      - Notification badge count
      - Mark as read
      - Real-time notification delivery
      hints:
        level1: 'Notification table: { userId, type, data, read, createdAt }.'
        level2: Use WebSocket or SSE for real-time delivery.
        level3: "model Notification {\n  id        Int      @id\n  userId    Int      // recipient\n  type      String   //\
          \ 'follow', 'like', 'comment'\n  actorId   Int      // who triggered it\n  targetId  Int?     // post id for likes/comments\n\
          \  read      Boolean  @default(false)\n  createdAt DateTime @default(now())\n  @@index([userId, read, createdAt])\n\
          }\n\nasync function createLikeNotification(postId, likerId) {\n  const post = await Post.findUnique({ where: { id:\
          \ postId } });\n  if (post.authorId === likerId) return;\n  await Notification.create({\n    data: { userId: post.authorId,\
          \ type: 'like', actorId: likerId, targetId: postId }\n  });\n  notificationService.push(post.authorId, { type: 'like',\
          \ ... });\n}"
      pitfalls:
      - Notification spam (batch similar notifications)
      - Notifying yourself
      - Notification count going negative
      concepts:
      - Notification system
      - Real-time delivery
      - Batching
      estimated_hours: 8-10
    - id: 5
      name: Search & Discovery
      description: Add search and content discovery features.
      acceptance_criteria:
      - Search users by name/username
      - Search posts by content
      - Trending posts/hashtags
      - Suggested users to follow
      - Explore page
      hints:
        level1: Simple search with LIKE. Use Elasticsearch for scale.
        level2: 'Trending: count hashtags/posts in sliding window.'
        level3: "async function getSuggestedUsers(userId) {\n  // Users followed by people I follow, that I don't follow\n\
          \  const suggestions = await prisma.$queryRaw`\n    SELECT u.*, COUNT(*) as mutual\n    FROM users u\n    JOIN follows\
          \ f1 ON f1.following_id = u.id\n    JOIN follows f2 ON f2.follower_id = f1.follower_id\n    WHERE f2.following_id\
          \ = ${userId}\n      AND u.id != ${userId}\n      AND u.id NOT IN (\n        SELECT following_id FROM follows WHERE\
          \ follower_id = ${userId}\n      )\n    GROUP BY u.id\n    ORDER BY mutual DESC\n    LIMIT 10\n  `;\n  return suggestions;\n\
          }"
      pitfalls:
      - Search too slow on large datasets
      - Trending manipulation (spam)
      - Recommendation bubbles
      concepts:
      - Search implementation
      - Trending algorithms
      - Recommendation systems
      estimated_hours: 10-12
    - id: 6
      name: Performance & Scaling
      description: Optimize for performance and prepare for scale.
      acceptance_criteria:
      - Redis caching for feeds
      - Background job processing
      - CDN for media
      - Database indexing optimized
      - Load testing done
      hints:
        level1: 'Cache hot data: user profiles, feed, follower counts.'
        level2: Use background jobs for fan-out, notifications, emails.
        level3: "async function getFeed(userId, cursor) {\n  const cacheKey = `feed:${userId}`;\n  let feedIds = await redis.zrevrange(cacheKey,\
          \ 0, 19);\n  if (!feedIds.length) {\n    const feed = await FeedItem.findMany({\n      where: { userId },\n    \
          \  orderBy: { createdAt: 'desc' },\n      take: 100\n    });\n    await redis.zadd(cacheKey, ...feed.flatMap(f =>\
          \ [f.createdAt.getTime(), f.postId]));\n    feedIds = feed.map(f => f.postId).slice(0, 20);\n  }\n  return Post.findMany({\
          \ where: { id: { in: feedIds } } });\n}"
      pitfalls:
      - Cache invalidation complexity
      - Celebrity user problem
      - Hot partition on popular content
      concepts:
      - Caching strategies
      - Background jobs
      - Performance optimization
      estimated_hours: 12-15
  software-3d:
    id: software-3d
    name: Software 3D Renderer
    description: Build a 3D renderer without GPU acceleration. Learn the math behind 3D graphics from scratch.
    difficulty: advanced
    estimated_hours: 30-50
    prerequisites:
    - Linear algebra (matrices, vectors)
    - Basic trigonometry
    - 2D graphics basics
    languages:
      recommended:
      - C
      - C++
      - Rust
      also_possible:
      - Python
      - JavaScript
    resources:
    - name: tinyrenderer
      url: https://github.com/ssloy/tinyrenderer/wiki
      type: tutorial
    - name: 3D Math Primer
      url: https://gamemath.com/
      type: book
    milestones:
    - id: 1
      name: Line Drawing
      description: Implement Bresenham's line algorithm.
      acceptance_criteria:
      - Draw lines between any two points
      - Handle all octants correctly
      - Optimize for integer-only math
      - Draw to a pixel buffer
      hints:
        level1: 'Bresenham: use error accumulation instead of floats.'
        level2: Handle steep lines by swapping x/y. Handle direction with step.
        level3: "void draw_line(int x0, int y0, int x1, int y1, uint32_t *buffer, int width) {\n    int steep = abs(y1 - y0)\
          \ > abs(x1 - x0);\n    if (steep) {\n        // Swap x and y\n        int tmp = x0; x0 = y0; y0 = tmp;\n       \
          \ tmp = x1; x1 = y1; y1 = tmp;\n    }\n    if (x0 > x1) {\n        int tmp = x0; x0 = x1; x1 = tmp;\n        tmp\
          \ = y0; y0 = y1; y1 = tmp;\n    }\n    \n    int dx = x1 - x0;\n    int dy = abs(y1 - y0);\n    int error = dx /\
          \ 2;\n    int ystep = (y0 < y1) ? 1 : -1;\n    int y = y0;\n    \n    for (int x = x0; x <= x1; x++) {\n       \
          \ if (steep)\n            buffer[x * width + y] = 0xFFFFFF;\n        else\n            buffer[y * width + x] = 0xFFFFFF;\n\
          \        \n        error -= dy;\n        if (error < 0) {\n            y += ystep;\n            error += dx;\n \
          \       }\n    }\n}"
      pitfalls:
      - Integer overflow
      - Division by zero
      - Missing octants
      concepts:
      - Rasterization
      - Bresenham's algorithm
      - Pixel buffers
      estimated_hours: 3-4
    - id: 2
      name: Triangle Rasterization
      description: Fill triangles using scanline or barycentric methods.
      acceptance_criteria:
      - Fill solid triangles
      - Handle edge cases (flat top/bottom)
      - Implement barycentric coordinates
      - Anti-aliasing (optional)
      hints:
        level1: 'Barycentric: point P in triangle if all weights positive.'
        level2: Compute bounding box, test each pixel with barycentric coords.
        level3: "typedef struct { float x, y; } Vec2;\n\nfloat edge_function(Vec2 a, Vec2 b, Vec2 c) {\n    return (c.x -\
          \ a.x) * (b.y - a.y) - (c.y - a.y) * (b.x - a.x);\n}\n\nvoid draw_triangle(Vec2 v0, Vec2 v1, Vec2 v2, uint32_t color,\
          \ uint32_t *buffer, int w, int h) {\n    // Bounding box\n    int minX = max(0, (int)min(v0.x, min(v1.x, v2.x)));\n\
          \    int maxX = min(w-1, (int)max(v0.x, max(v1.x, v2.x)));\n    int minY = max(0, (int)min(v0.y, min(v1.y, v2.y)));\n\
          \    int maxY = min(h-1, (int)max(v0.y, max(v1.y, v2.y)));\n    \n    float area = edge_function(v0, v1, v2);\n\
          \    \n    for (int y = minY; y <= maxY; y++) {\n        for (int x = minX; x <= maxX; x++) {\n            Vec2\
          \ p = {x + 0.5f, y + 0.5f};\n            \n            float w0 = edge_function(v1, v2, p);\n            float w1\
          \ = edge_function(v2, v0, p);\n            float w2 = edge_function(v0, v1, p);\n            \n            // Point\
          \ inside triangle if all same sign\n            if (w0 >= 0 && w1 >= 0 && w2 >= 0) {\n                // Barycentric\
          \ coordinates\n                w0 /= area; w1 /= area; w2 /= area;\n                buffer[y * w + x] = color;\n\
          \            }\n        }\n    }\n}"
      pitfalls:
      - Winding order
      - Gaps between triangles
      - Subpixel precision
      concepts:
      - Barycentric coordinates
      - Rasterization
      - Fill rules
      estimated_hours: 5-7
    - id: 3
      name: 3D Transformations
      description: Implement model, view, and projection matrices.
      acceptance_criteria:
      - 4x4 matrix multiplication
      - Translation, rotation, scaling
      - Look-at camera matrix
      - Perspective projection
      hints:
        level1: 'MVP: Model * View * Projection. Apply to each vertex.'
        level2: 'Perspective divide: after projection, divide x,y,z by w.'
        level3: "typedef struct { float m[4][4]; } Mat4;\n\nMat4 mat4_perspective(float fov, float aspect, float near, float\
          \ far) {\n    Mat4 m = {0};\n    float tanHalfFov = tan(fov / 2.0f);\n    m.m[0][0] = 1.0f / (aspect * tanHalfFov);\n\
          \    m.m[1][1] = 1.0f / tanHalfFov;\n    m.m[2][2] = -(far + near) / (far - near);\n    m.m[2][3] = -1.0f;\n   \
          \ m.m[3][2] = -(2.0f * far * near) / (far - near);\n    return m;\n}\n\nMat4 mat4_look_at(Vec3 eye, Vec3 target,\
          \ Vec3 up) {\n    Vec3 f = vec3_normalize(vec3_sub(target, eye));\n    Vec3 r = vec3_normalize(vec3_cross(f, up));\n\
          \    Vec3 u = vec3_cross(r, f);\n    \n    Mat4 m = mat4_identity();\n    m.m[0][0] = r.x; m.m[1][0] = r.y; m.m[2][0]\
          \ = r.z;\n    m.m[0][1] = u.x; m.m[1][1] = u.y; m.m[2][1] = u.z;\n    m.m[0][2] = -f.x; m.m[1][2] = -f.y; m.m[2][2]\
          \ = -f.z;\n    m.m[3][0] = -vec3_dot(r, eye);\n    m.m[3][1] = -vec3_dot(u, eye);\n    m.m[3][2] = vec3_dot(f, eye);\n\
          \    return m;\n}\n\nVec3 project_vertex(Vec3 v, Mat4 mvp, int width, int height) {\n    // Apply MVP\n    float\
          \ w = mvp.m[0][3]*v.x + mvp.m[1][3]*v.y + mvp.m[2][3]*v.z + mvp.m[3][3];\n    float x = mvp.m[0][0]*v.x + mvp.m[1][0]*v.y\
          \ + mvp.m[2][0]*v.z + mvp.m[3][0];\n    float y = mvp.m[0][1]*v.x + mvp.m[1][1]*v.y + mvp.m[2][1]*v.z + mvp.m[3][1];\n\
          \    \n    // Perspective divide\n    x /= w; y /= w;\n    \n    // Viewport transform\n    return (Vec3){\n   \
          \     (x + 1.0f) * width / 2.0f,\n        (1.0f - y) * height / 2.0f,\n        0\n    };\n}"
      pitfalls:
      - Matrix multiplication order
      - Homogeneous coordinates
      - Y-axis flip
      concepts:
      - Linear transformations
      - Projection
      - Camera space
      estimated_hours: 6-8
    - id: 4
      name: Depth Buffer & Lighting
      description: Add z-buffering and basic shading.
      acceptance_criteria:
      - Z-buffer for hidden surface removal
      - Flat shading with surface normals
      - Gouraud shading (vertex interpolation)
      - Basic diffuse lighting
      hints:
        level1: 'Z-buffer: store depth per pixel, only draw if closer.'
        level2: Interpolate z using barycentric coordinates.
        level3: "float *zbuffer;  // Initialize to infinity\n\nvoid draw_triangle_zbuf(Vec3 v0, Vec3 v1, Vec3 v2, uint32_t\
          \ color,\n                         uint32_t *buffer, float *zbuffer, int w, int h) {\n    // ... bounding box code\
          \ ...\n    \n    for (int y = minY; y <= maxY; y++) {\n        for (int x = minX; x <= maxX; x++) {\n          \
          \  Vec2 p = {x + 0.5f, y + 0.5f};\n            \n            float w0 = edge_function(v1, v2, p) / area;\n     \
          \       float w1 = edge_function(v2, v0, p) / area;\n            float w2 = edge_function(v0, v1, p) / area;\n \
          \           \n            if (w0 >= 0 && w1 >= 0 && w2 >= 0) {\n                // Interpolate z\n             \
          \   float z = w0 * v0.z + w1 * v1.z + w2 * v2.z;\n                \n                int idx = y * w + x;\n     \
          \           if (z < zbuffer[idx]) {\n                    zbuffer[idx] = z;\n                    buffer[idx] = color;\n\
          \                }\n            }\n        }\n    }\n}\n\n// Flat shading\nVec3 compute_normal(Vec3 v0, Vec3 v1,\
          \ Vec3 v2) {\n    Vec3 edge1 = vec3_sub(v1, v0);\n    Vec3 edge2 = vec3_sub(v2, v0);\n    return vec3_normalize(vec3_cross(edge1,\
          \ edge2));\n}\n\nfloat compute_lighting(Vec3 normal, Vec3 light_dir) {\n    float intensity = vec3_dot(normal, light_dir);\n\
          \    return max(0.0f, intensity);  // Clamp to [0,1]\n}"
      pitfalls:
      - Z-fighting
      - Normal direction
      - Interpolation artifacts
      concepts:
      - Z-buffering
      - Surface normals
      - Lighting models
      estimated_hours: 8-12
  sql-parser:
    id: sql-parser
    name: SQL Parser
    description: Build a SQL parser for SELECT, INSERT, UPDATE queries. Learn query parsing and AST construction.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Tokenizer basics
    - Recursive descent parsing
    - Tree data structures
    languages:
      recommended:
      - Python
      - Go
      - Rust
      also_possible:
      - JavaScript
      - Java
    resources:
    - name: SQLite Grammar
      url: https://www.sqlite.org/lang.html
      type: documentation
    - name: Writing a SQL Parser
      url: https://blog.subnetzero.io/post/building-a-sql-parser/
      type: article
    milestones:
    - id: 1
      name: SQL Tokenizer
      description: Tokenize SQL statements into keywords, identifiers, literals.
      acceptance_criteria:
      - Recognize SQL keywords (SELECT, FROM, WHERE)
      - Handle identifiers and numbers
      - Parse string literals
      - Support operators (=, <, >, AND, OR)
      hints:
        level1: Keywords are case-insensitive. Identifiers can be quoted.
        level2: Handle both single and double quotes for strings. Whitespace separates tokens.
        level3: "import re\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\n\nclass TokenType(Enum):\n   \
          \ SELECT = auto()\n    FROM = auto()\n    WHERE = auto()\n    INSERT = auto()\n    INTO = auto()\n    VALUES = auto()\n\
          \    UPDATE = auto()\n    SET = auto()\n    DELETE = auto()\n    AND = auto()\n    OR = auto()\n    NOT = auto()\n\
          \    NULL = auto()\n    IDENTIFIER = auto()\n    NUMBER = auto()\n    STRING = auto()\n    STAR = auto()\n    COMMA\
          \ = auto()\n    LPAREN = auto()\n    RPAREN = auto()\n    EQ = auto()\n    NE = auto()\n    LT = auto()\n    GT\
          \ = auto()\n    LE = auto()\n    GE = auto()\n    EOF = auto()\n\nKEYWORDS = {\n    'SELECT': TokenType.SELECT,\
          \ 'FROM': TokenType.FROM,\n    'WHERE': TokenType.WHERE, 'AND': TokenType.AND,\n    'OR': TokenType.OR, 'INSERT':\
          \ TokenType.INSERT,\n    'INTO': TokenType.INTO, 'VALUES': TokenType.VALUES,\n    'UPDATE': TokenType.UPDATE, 'SET':\
          \ TokenType.SET,\n    'DELETE': TokenType.DELETE, 'NULL': TokenType.NULL,\n    'NOT': TokenType.NOT,\n}\n\n@dataclass\n\
          class Token:\n    type: TokenType\n    value: str\n    \nclass SQLTokenizer:\n    def __init__(self, sql: str):\n\
          \        self.sql = sql\n        self.pos = 0\n    \n    def tokenize(self) -> list:\n        tokens = []\n    \
          \    while self.pos < len(self.sql):\n            self.skip_whitespace()\n            if self.pos >= len(self.sql):\n\
          \                break\n            token = self.next_token()\n            if token:\n                tokens.append(token)\n\
          \        tokens.append(Token(TokenType.EOF, ''))\n        return tokens"
      pitfalls:
      - Case sensitivity
      - Escape sequences in strings
      - Multi-char operators
      concepts:
      - Lexical analysis
      - Token types
      - SQL syntax
      estimated_hours: 3-4
    - id: 2
      name: SELECT Parser
      description: Parse SELECT statements into AST.
      acceptance_criteria:
      - Parse column list or *
      - Parse FROM clause with table name
      - Handle multiple columns
      - Parse aliases (AS)
      hints:
        level1: SELECT columns FROM table. columns = * | column_list.
        level2: column_list = column (, column)*. column can have alias.
        level3: "@dataclass\nclass SelectColumn:\n    name: str\n    alias: str = None\n\n@dataclass\nclass SelectStmt:\n\
          \    columns: list  # [SelectColumn] or ['*']\n    table: str\n    table_alias: str = None\n    where: 'Expr' =\
          \ None\n\nclass SQLParser:\n    def __init__(self, tokens: list):\n        self.tokens = tokens\n        self.pos\
          \ = 0\n    \n    def parse(self):\n        if self.current().type == TokenType.SELECT:\n            return self.parse_select()\n\
          \        # ... other statements\n    \n    def parse_select(self):\n        self.expect(TokenType.SELECT)\n    \
          \    \n        # Parse columns\n        if self.match(TokenType.STAR):\n            columns = ['*']\n        else:\n\
          \            columns = self.parse_column_list()\n        \n        # Parse FROM\n        self.expect(TokenType.FROM)\n\
          \        table = self.expect(TokenType.IDENTIFIER).value\n        \n        # Optional alias\n        table_alias\
          \ = None\n        if self.check(TokenType.IDENTIFIER) or self.match_keyword('AS'):\n            table_alias = self.expect(TokenType.IDENTIFIER).value\n\
          \        \n        # Optional WHERE\n        where = None\n        if self.match(TokenType.WHERE):\n           \
          \ where = self.parse_expr()\n        \n        return SelectStmt(columns, table, table_alias, where)\n    \n   \
          \ def parse_column_list(self):\n        columns = [self.parse_column()]\n        while self.match(TokenType.COMMA):\n\
          \            columns.append(self.parse_column())\n        return columns\n    \n    def parse_column(self):\n  \
          \      name = self.expect(TokenType.IDENTIFIER).value\n        alias = None\n        if self.match_keyword('AS')\
          \ or self.check(TokenType.IDENTIFIER):\n            alias = self.expect(TokenType.IDENTIFIER).value\n        return\
          \ SelectColumn(name, alias)"
      pitfalls:
      - Missing commas
      - Alias without AS
      - Reserved words as identifiers
      concepts:
      - Recursive descent
      - AST construction
      - Grammar rules
      estimated_hours: 4-5
    - id: 3
      name: WHERE Clause
      description: Parse WHERE expressions with operators and logic.
      acceptance_criteria:
      - Comparison operators (=, <, >, !=)
      - Boolean operators (AND, OR, NOT)
      - Parentheses for grouping
      - NULL checks (IS NULL, IS NOT NULL)
      hints:
        level1: 'Precedence: NOT > AND > OR. Use recursive descent.'
        level2: or_expr -> and_expr (OR and_expr)*. and_expr -> not_expr (AND not_expr)*.
        level3: "@dataclass\nclass BinaryExpr:\n    left: 'Expr'\n    op: str\n    right: 'Expr'\n\n@dataclass\nclass UnaryExpr:\n\
          \    op: str\n    operand: 'Expr'\n\n@dataclass\nclass Comparison:\n    left: 'Expr'\n    op: str  # =, <, >, <=,\
          \ >=, !=\n    right: 'Expr'\n\n@dataclass\nclass IsNull:\n    expr: 'Expr'\n    negated: bool  # IS NOT NULL\n\n\
          def parse_expr(self):\n    return self.parse_or()\n\ndef parse_or(self):\n    left = self.parse_and()\n    while\
          \ self.match(TokenType.OR):\n        right = self.parse_and()\n        left = BinaryExpr(left, 'OR', right)\n  \
          \  return left\n\ndef parse_and(self):\n    left = self.parse_not()\n    while self.match(TokenType.AND):\n    \
          \    right = self.parse_not()\n        left = BinaryExpr(left, 'AND', right)\n    return left\n\ndef parse_not(self):\n\
          \    if self.match(TokenType.NOT):\n        return UnaryExpr('NOT', self.parse_not())\n    return self.parse_comparison()\n\
          \ndef parse_comparison(self):\n    left = self.parse_primary()\n    \n    # IS NULL / IS NOT NULL\n    if self.match_keyword('IS'):\n\
          \        negated = self.match(TokenType.NOT)\n        self.expect(TokenType.NULL)\n        return IsNull(left, negated)\n\
          \    \n    # Comparison operators\n    if self.current().type in (TokenType.EQ, TokenType.NE, TokenType.LT, TokenType.GT,\
          \ TokenType.LE, TokenType.GE):\n        op = self.advance().value\n        right = self.parse_primary()\n      \
          \  return Comparison(left, op, right)\n    \n    return left\n\ndef parse_primary(self):\n    if self.match(TokenType.LPAREN):\n\
          \        expr = self.parse_expr()\n        self.expect(TokenType.RPAREN)\n        return expr\n    \n    if self.check(TokenType.NUMBER):\n\
          \        return Literal(int(self.advance().value))\n    if self.check(TokenType.STRING):\n        return Literal(self.advance().value)\n\
          \    if self.check(TokenType.IDENTIFIER):\n        return Identifier(self.advance().value)\n    \n    raise ParseError(f'Unexpected\
          \ token: {self.current()}')"
      pitfalls:
      - Operator precedence
      - Unbalanced parentheses
      - NULL comparisons
      concepts:
      - Expression parsing
      - Precedence climbing
      - Boolean logic
      estimated_hours: 4-5
    - id: 4
      name: INSERT/UPDATE/DELETE
      description: Parse modification statements.
      acceptance_criteria:
      - INSERT INTO table (cols) VALUES (vals)
      - UPDATE table SET col=val WHERE...
      - DELETE FROM table WHERE...
      - Handle multiple rows in INSERT
      hints:
        level1: 'INSERT: columns optional. VALUES can have multiple tuples.'
        level2: 'UPDATE: SET clause is comma-separated assignments.'
        level3: "@dataclass\nclass InsertStmt:\n    table: str\n    columns: list  # Optional column list\n    values: list\
          \   # List of value tuples\n\n@dataclass\nclass UpdateStmt:\n    table: str\n    assignments: list  # [(column,\
          \ value), ...]\n    where: Expr = None\n\n@dataclass\nclass DeleteStmt:\n    table: str\n    where: Expr = None\n\
          \ndef parse_insert(self):\n    self.expect(TokenType.INSERT)\n    self.expect(TokenType.INTO)\n    table = self.expect(TokenType.IDENTIFIER).value\n\
          \    \n    # Optional column list\n    columns = None\n    if self.match(TokenType.LPAREN):\n        columns = []\n\
          \        columns.append(self.expect(TokenType.IDENTIFIER).value)\n        while self.match(TokenType.COMMA):\n \
          \           columns.append(self.expect(TokenType.IDENTIFIER).value)\n        self.expect(TokenType.RPAREN)\n   \
          \ \n    self.expect(TokenType.VALUES)\n    values = []\n    values.append(self.parse_value_tuple())\n    while self.match(TokenType.COMMA):\n\
          \        values.append(self.parse_value_tuple())\n    \n    return InsertStmt(table, columns, values)\n\ndef parse_update(self):\n\
          \    self.expect(TokenType.UPDATE)\n    table = self.expect(TokenType.IDENTIFIER).value\n    self.expect(TokenType.SET)\n\
          \    \n    assignments = []\n    col = self.expect(TokenType.IDENTIFIER).value\n    self.expect(TokenType.EQ)\n\
          \    val = self.parse_primary()\n    assignments.append((col, val))\n    \n    while self.match(TokenType.COMMA):\n\
          \        col = self.expect(TokenType.IDENTIFIER).value\n        self.expect(TokenType.EQ)\n        val = self.parse_primary()\n\
          \        assignments.append((col, val))\n    \n    where = None\n    if self.match(TokenType.WHERE):\n        where\
          \ = self.parse_expr()\n    \n    return UpdateStmt(table, assignments, where)"
      pitfalls:
      - Column/value count mismatch
      - Missing WHERE in DELETE
      - Type mismatches
      concepts:
      - DML parsing
      - Statement types
      - Data modification
      estimated_hours: 4-5
  stack-queue:
    id: stack-queue
    name: Stack & Queue
    description: Implement stack and queue data structures. Learn LIFO/FIFO principles and their applications.
    difficulty: beginner
    estimated_hours: 4-6
    prerequisites:
    - Basic programming
    - Arrays
    languages:
      recommended:
      - Python
      - Java
      - C
      also_possible:
      - JavaScript
      - Go
    resources:
    - name: Stack and Queue - GeeksforGeeks
      url: https://www.geeksforgeeks.org/stack-data-structure/
      type: tutorial
    milestones:
    - id: 1
      name: Array-based Stack
      description: Implement a stack using an array.
      acceptance_criteria:
      - push() adds to top
      - pop() removes and returns top
      - peek() returns top without removing
      - isEmpty() checks if empty
      - Handle stack overflow/underflow
      hints:
        level1: Track top index. Push increments, pop decrements.
        level2: Use dynamic array for unbounded stack.
        level3: "class Stack:\n    def __init__(self, capacity=100):\n        self.items = [None] * capacity\n        self.top\
          \ = -1\n        self.capacity = capacity\n    \n    def push(self, item):\n        if self.top >= self.capacity\
          \ - 1:\n            raise OverflowError('Stack overflow')\n        self.top += 1\n        self.items[self.top] =\
          \ item\n    \n    def pop(self):\n        if self.top < 0:\n            raise IndexError('Stack underflow')\n  \
          \      item = self.items[self.top]\n        self.top -= 1\n        return item\n    \n    def peek(self):\n    \
          \    if self.top < 0:\n            raise IndexError('Stack is empty')\n        return self.items[self.top]\n   \
          \ \n    def is_empty(self):\n        return self.top < 0"
      pitfalls:
      - Off-by-one errors
      - Not checking bounds
      - Memory leaks in C
      concepts:
      - LIFO principle
      - Array indexing
      - Bounds checking
      estimated_hours: 1-2
    - id: 2
      name: Array-based Queue
      description: Implement a queue using a circular array.
      acceptance_criteria:
      - enqueue() adds to back
      - dequeue() removes from front
      - front() peeks front element
      - Circular array for efficiency
      - Handle full/empty states
      hints:
        level1: Track front and rear indices. Wrap around with modulo.
        level2: 'Distinguish full from empty: track count or use sentinel.'
        level3: "class CircularQueue:\n    def __init__(self, capacity):\n        self.items = [None] * capacity\n       \
          \ self.capacity = capacity\n        self.front = 0\n        self.rear = 0\n        self.count = 0\n    \n    def\
          \ enqueue(self, item):\n        if self.count >= self.capacity:\n            raise OverflowError('Queue is full')\n\
          \        self.items[self.rear] = item\n        self.rear = (self.rear + 1) % self.capacity\n        self.count +=\
          \ 1\n    \n    def dequeue(self):\n        if self.count == 0:\n            raise IndexError('Queue is empty')\n\
          \        item = self.items[self.front]\n        self.front = (self.front + 1) % self.capacity\n        self.count\
          \ -= 1\n        return item\n    \n    def peek_front(self):\n        if self.count == 0:\n            raise IndexError('Queue\
          \ is empty')\n        return self.items[self.front]"
      pitfalls:
      - Full vs empty with same indices
      - Modulo arithmetic errors
      - Growing circular buffer
      concepts:
      - FIFO principle
      - Circular buffer
      - Modulo arithmetic
      estimated_hours: 1-2
    - id: 3
      name: Linked List Implementations
      description: Implement stack and queue using linked lists.
      acceptance_criteria:
      - Linked stack with O(1) push/pop
      - Linked queue with O(1) enqueue/dequeue
      - No fixed capacity
      - Memory efficient
      hints:
        level1: 'Stack: push/pop at head. Queue: enqueue at tail, dequeue at head.'
        level2: Keep tail pointer for O(1) enqueue.
        level3: "class Node:\n    def __init__(self, value):\n        self.value = value\n        self.next = None\n\nclass\
          \ LinkedStack:\n    def __init__(self):\n        self.top = None\n    \n    def push(self, value):\n        node\
          \ = Node(value)\n        node.next = self.top\n        self.top = node\n    \n    def pop(self):\n        if not\
          \ self.top:\n            raise IndexError('Stack is empty')\n        value = self.top.value\n        self.top =\
          \ self.top.next\n        return value\n\nclass LinkedQueue:\n    def __init__(self):\n        self.front = self.rear\
          \ = None\n    \n    def enqueue(self, value):\n        node = Node(value)\n        if self.rear:\n            self.rear.next\
          \ = node\n        self.rear = node\n        if not self.front:\n            self.front = node\n    \n    def dequeue(self):\n\
          \        if not self.front:\n            raise IndexError('Queue is empty')\n        value = self.front.value\n\
          \        self.front = self.front.next\n        if not self.front:\n            self.rear = None\n        return\
          \ value"
      pitfalls:
      - Null pointer on empty
      - Memory management
      - Updating both front and rear
      concepts:
      - Linked nodes
      - Dynamic memory
      - Pointer manipulation
      estimated_hours: 1-2
  tdd-kata:
    id: tdd-kata
    name: TDD Kata Series
    description: Practice Test-Driven Development with coding katas. Learn the red-green-refactor cycle.
    difficulty: intermediate
    estimated_hours: 10-15
    prerequisites:
    - Unit testing basics
    - Refactoring concepts
    languages:
      recommended:
      - Python
      - JavaScript
      - Java
      also_possible:
      - Ruby
      - C#
    resources:
    - name: TDD by Example - Kent Beck
      url: https://www.amazon.com/Test-Driven-Development-Kent-Beck/dp/0321146530
      type: book
    - name: Kata Catalog
      url: https://kata-log.rocks/
      type: tutorial
    milestones:
    - id: 1
      name: String Calculator Kata
      description: Classic TDD kata for beginners.
      acceptance_criteria:
      - Empty string returns 0
      - Single number returns that number
      - Two numbers comma-separated
      - Handle newlines as delimiter
      - Custom delimiters
      hints:
        level1: 'Start with simplest case: empty string.'
        level2: Add one test, make it pass, refactor. Repeat.
        level3: "# Step by step TDD\n\n# RED: Write failing test\ndef test_empty_string():\n    assert add('') == 0\n\n# GREEN:\
          \ Minimal code to pass\ndef add(numbers):\n    return 0\n\n# RED: Next test\ndef test_single_number():\n    assert\
          \ add('1') == 1\n\n# GREEN: Make it pass\ndef add(numbers):\n    if not numbers:\n        return 0\n    return int(numbers)\n\
          \n# RED: Two numbers\ndef test_two_numbers():\n    assert add('1,2') == 3\n\n# GREEN + REFACTOR\ndef add(numbers):\n\
          \    if not numbers:\n        return 0\n    return sum(int(n) for n in numbers.split(','))\n\n# Continue: newlines,\
          \ custom delimiters, negatives..."
      pitfalls:
      - Writing too much code at once
      - Skipping refactor step
      - Not running tests after each change
      concepts:
      - Red-green-refactor
      - Baby steps
      - Incremental design
      estimated_hours: 2-3
    - id: 2
      name: Bowling Game Kata
      description: Calculate bowling scores with TDD.
      acceptance_criteria:
      - Score a gutter game (all zeros)
      - Score all ones
      - Handle spare
      - Handle strike
      - Perfect game (all strikes)
      hints:
        level1: Track rolls, calculate score after game.
        level2: 'Spare: bonus = next roll. Strike: bonus = next two rolls.'
        level3: "# Test progression:\n# 1. Gutter game\ndef test_gutter_game():\n    game = BowlingGame()\n    for _ in range(20):\n\
          \        game.roll(0)\n    assert game.score() == 0\n\n# 2. All ones\ndef test_all_ones():\n    game = BowlingGame()\n\
          \    for _ in range(20):\n        game.roll(1)\n    assert game.score() == 20\n\n# 3. One spare\ndef test_one_spare():\n\
          \    game = BowlingGame()\n    game.roll(5)\n    game.roll(5)  # Spare\n    game.roll(3)\n    for _ in range(17):\n\
          \        game.roll(0)\n    assert game.score() == 16  # 10 + 3 + 3\n\n# Implementation\nclass BowlingGame:\n   \
          \ def __init__(self):\n        self.rolls = []\n    \n    def roll(self, pins):\n        self.rolls.append(pins)\n\
          \    \n    def score(self):\n        total = 0\n        roll_index = 0\n        for frame in range(10):\n      \
          \      if self._is_strike(roll_index):\n                total += 10 + self.rolls[roll_index+1] + self.rolls[roll_index+2]\n\
          \                roll_index += 1\n            elif self._is_spare(roll_index):\n                total += 10 + self.rolls[roll_index+2]\n\
          \                roll_index += 2\n            else:\n                total += self.rolls[roll_index] + self.rolls[roll_index+1]\n\
          \                roll_index += 2\n        return total"
      pitfalls:
      - Premature optimization
      - Not testing edge cases
      - Overcomplicating frame handling
      concepts:
      - Domain modeling
      - State management
      - Emergent design
      estimated_hours: 3-4
    - id: 3
      name: Roman Numerals Kata
      description: Convert numbers to/from Roman numerals.
      acceptance_criteria:
      - Convert 1-9 to I-IX
      - Convert 10-90 to X-XC
      - Convert 100-900 to C-CM
      - Handle 1-3999 range
      - Convert Roman to integer
      hints:
        level1: Start with 1=I, build up to subtraction cases.
        level2: Use lookup table for values.
        level3: "# Test-driven approach\nclass TestRomanNumerals:\n    def test_1(self): assert to_roman(1) == 'I'\n    def\
          \ test_2(self): assert to_roman(2) == 'II'\n    def test_3(self): assert to_roman(3) == 'III'\n    def test_4(self):\
          \ assert to_roman(4) == 'IV'\n    def test_5(self): assert to_roman(5) == 'V'\n    def test_9(self): assert to_roman(9)\
          \ == 'IX'\n    def test_10(self): assert to_roman(10) == 'X'\n    # ... continue\n\n# Final implementation\nVALUES\
          \ = [\n    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'),\n    (100, 'C'), (90, 'XC'), (50, 'L'), (40, 'XL'),\n\
          \    (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\n]\n\ndef to_roman(num):\n    result = ''\n    for value,\
          \ numeral in VALUES:\n        while num >= value:\n            result += numeral\n            num -= value\n   \
          \ return result\n\ndef from_roman(roman):\n    result = 0\n    i = 0\n    for value, numeral in VALUES:\n      \
          \  while roman[i:i+len(numeral)] == numeral:\n            result += value\n            i += len(numeral)\n    return\
          \ result"
      pitfalls:
      - Not handling subtraction cases
      - Off-by-one in ranges
      - Hardcoding without pattern
      concepts:
      - Lookup tables
      - Greedy algorithm
      - Bi-directional conversion
      estimated_hours: 2-3
  terminal-multiplexer:
    id: terminal-multiplexer
    name: Terminal Multiplexer
    description: Build a simple terminal multiplexer like tmux/screen. Learn PTY handling, terminal escape sequences, and
      process management.
    difficulty: advanced
    estimated_hours: 30-40
    prerequisites:
    - Unix processes
    - Terminal basics
    - File descriptors
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
    resources:
    - name: PTY Programming
      url: https://www.man7.org/linux/man-pages/man7/pty.7.html
      type: reference
    - name: ANSI Escape Codes
      url: https://en.wikipedia.org/wiki/ANSI_escape_code
      type: reference
    - name: Building a Terminal Emulator
      url: https://www.uninformativ.de/blog/postings/2018-02-24/0/POSTING-en.html
      type: tutorial
    milestones:
    - id: 1
      name: PTY Creation
      description: Create and manage pseudo-terminal pairs for running shell sessions.
      acceptance_criteria:
      - Open PTY master/slave pair
      - Fork child process with PTY as controlling terminal
      - Start shell in child process
      - Handle terminal size (TIOCSWINSZ)
      hints:
        level1: posix_openpt() or openpty() create PTY pairs. Child uses slave, parent uses master.
        level2: Child must setsid(), then open slave to make it controlling terminal.
        level3: "import os\nimport pty\nimport fcntl\nimport termios\nimport struct\n\ndef create_pty_shell() -> tuple[int,\
          \ int]:\n    '''Create PTY and spawn shell, return (master_fd, child_pid)'''\n    master_fd, slave_fd = pty.openpty()\n\
          \n    pid = os.fork()\n\n    if pid == 0:\n        # Child process\n        os.close(master_fd)\n\n        # Create\
          \ new session\n        os.setsid()\n\n        # Slave becomes controlling terminal\n        os.dup2(slave_fd, 0)\
          \  # stdin\n        os.dup2(slave_fd, 1)  # stdout\n        os.dup2(slave_fd, 2)  # stderr\n\n        if slave_fd\
          \ > 2:\n            os.close(slave_fd)\n\n        # Exec shell\n        shell = os.environ.get('SHELL', '/bin/sh')\n\
          \        os.execvp(shell, [shell])\n\n    # Parent\n    os.close(slave_fd)\n    return master_fd, pid\n\ndef set_terminal_size(fd:\
          \ int, rows: int, cols: int):\n    '''Set terminal size via ioctl'''\n    winsize = struct.pack('HHHH', rows, cols,\
          \ 0, 0)\n    fcntl.ioctl(fd, termios.TIOCSWINSZ, winsize)\n\n# C version:\n'''\nint master_fd = posix_openpt(O_RDWR\
          \ | O_NOCTTY);\ngrantpt(master_fd);\nunlockpt(master_fd);\nchar *slave_name = ptsname(master_fd);\n'''"
      pitfalls:
      - Must call setsid() in child before opening slave
      - File descriptors need careful management across fork
      - SIGCHLD handling for child termination
      concepts:
      - Pseudo-terminals
      - Process groups
      - Controlling terminals
      estimated_hours: 6-8
    - id: 2
      name: Terminal Emulation
      description: Parse and render terminal escape sequences for display.
      acceptance_criteria:
      - Handle cursor movement (up, down, left, right)
      - Handle clear screen and clear line
      - Handle text attributes (bold, colors)
      - Maintain virtual screen buffer
      hints:
        level1: ANSI escapes start with ESC[ (0x1B 0x5B). Parse parameters between ESC[ and command letter.
        level2: Maintain grid of cells with character and attributes. Render to real terminal.
        level3: "from dataclasses import dataclass\nfrom enum import IntFlag\n\nclass Attr(IntFlag):\n    NORMAL = 0\n   \
          \ BOLD = 1\n    DIM = 2\n    UNDERLINE = 4\n    REVERSE = 8\n\n@dataclass\nclass Cell:\n    char: str = ' '\n  \
          \  fg: int = 7  # White\n    bg: int = 0  # Black\n    attr: Attr = Attr.NORMAL\n\nclass Screen:\n    def __init__(self,\
          \ rows: int, cols: int):\n        self.rows = rows\n        self.cols = cols\n        self.cursor_row = 0\n    \
          \    self.cursor_col = 0\n        self.cells = [[Cell() for _ in range(cols)] for _ in range(rows)]\n        self.current_attr\
          \ = Attr.NORMAL\n        self.current_fg = 7\n        self.current_bg = 0\n\n    def write(self, data: bytes):\n\
          \        '''Process output from PTY'''\n        i = 0\n        while i < len(data):\n            if data[i] == 0x1B\
          \ and i+1 < len(data) and data[i+1] == 0x5B:\n                # ESC[ sequence\n                end, params, cmd\
          \ = self.parse_csi(data, i+2)\n                self.handle_csi(params, cmd)\n                i = end\n         \
          \   elif data[i] == ord('\\n'):\n                self.cursor_row = min(self.cursor_row + 1, self.rows - 1)\n   \
          \             i += 1\n            elif data[i] == ord('\\r'):\n                self.cursor_col = 0\n           \
          \     i += 1\n            else:\n                self.put_char(chr(data[i]))\n                i += 1\n\n    def\
          \ handle_csi(self, params: list[int], cmd: str):\n        '''Handle CSI escape sequence'''\n        if cmd == 'H':\
          \  # Cursor position\n            row = params[0] - 1 if params else 0\n            col = params[1] - 1 if len(params)\
          \ > 1 else 0\n            self.cursor_row = max(0, min(row, self.rows - 1))\n            self.cursor_col = max(0,\
          \ min(col, self.cols - 1))\n        elif cmd == 'J':  # Clear screen\n            mode = params[0] if params else\
          \ 0\n            if mode == 2:  # Clear all\n                self.cells = [[Cell() for _ in range(self.cols)]\n\
          \                              for _ in range(self.rows)]\n        elif cmd == 'm':  # SGR (text attributes)\n \
          \           self.handle_sgr(params)"
      pitfalls:
      - Many escape sequences have optional parameters
      - UTF-8 characters can span multiple bytes
      - Some sequences are terminal-specific
      concepts:
      - Terminal emulation
      - ANSI escape codes
      - Screen buffers
      estimated_hours: 8-10
    - id: 3
      name: Window Management
      description: Support multiple panes in a split-screen layout.
      acceptance_criteria:
      - Vertical and horizontal splits
      - Switch focus between panes
      - Resize panes
      - Each pane runs independent shell
      hints:
        level1: Each pane has its own PTY, screen buffer, and render region.
        level2: 'Use a tree structure: nodes are either splits (container) or panes (leaf).'
        level3: "from dataclasses import dataclass\nfrom typing import Optional, Union\n\n@dataclass\nclass Rect:\n    x:\
          \ int\n    y: int\n    width: int\n    height: int\n\n@dataclass\nclass Pane:\n    id: int\n    master_fd: int\n\
          \    pid: int\n    screen: Screen\n    rect: Rect\n\n@dataclass\nclass Split:\n    direction: str  # 'horizontal'\
          \ or 'vertical'\n    ratio: float  # 0.0-1.0, position of split\n    first: Union['Split', Pane]\n    second: Union['Split',\
          \ Pane]\n\nclass Multiplexer:\n    def __init__(self, rows: int, cols: int):\n        self.rows = rows\n       \
          \ self.cols = cols\n        self.root: Union[Split, Pane] = None\n        self.focused_pane: Optional[Pane] = None\n\
          \        self.panes: list[Pane] = []\n        self.next_pane_id = 0\n\n    def create_pane(self, rect: Rect) ->\
          \ Pane:\n        '''Create new pane with PTY'''\n        master_fd, pid = create_pty_shell()\n        screen = Screen(rect.height,\
          \ rect.width)\n        set_terminal_size(master_fd, rect.height, rect.width)\n\n        pane = Pane(self.next_pane_id,\
          \ master_fd, pid, screen, rect)\n        self.next_pane_id += 1\n        self.panes.append(pane)\n        return\
          \ pane\n\n    def split_pane(self, pane: Pane, direction: str):\n        '''Split pane horizontally or vertically'''\n\
          \        old_rect = pane.rect\n\n        if direction == 'vertical':\n            # Split left/right\n         \
          \   w1 = old_rect.width // 2\n            w2 = old_rect.width - w1 - 1  # -1 for border\n            rect1 = Rect(old_rect.x,\
          \ old_rect.y, w1, old_rect.height)\n            rect2 = Rect(old_rect.x + w1 + 1, old_rect.y, w2, old_rect.height)\n\
          \        else:\n            # Split top/bottom\n            h1 = old_rect.height // 2\n            h2 = old_rect.height\
          \ - h1 - 1\n            rect1 = Rect(old_rect.x, old_rect.y, old_rect.width, h1)\n            rect2 = Rect(old_rect.x,\
          \ old_rect.y + h1 + 1, old_rect.width, h2)\n\n        # Resize existing pane\n        pane.rect = rect1\n      \
          \  pane.screen = Screen(rect1.height, rect1.width)\n        set_terminal_size(pane.master_fd, rect1.height, rect1.width)\n\
          \n        # Create new pane\n        new_pane = self.create_pane(rect2)\n\n        # Update tree structure\n   \
          \     # ..."
      pitfalls:
      - Resize needs to update PTY size too
      - Border drawing reduces usable space
      - Focus tracking across splits is complex
      concepts:
      - Window management
      - Tree layouts
      - PTY multiplexing
      estimated_hours: 8-10
    - id: 4
      name: Key Bindings and UI
      description: Add command mode and key bindings for pane management.
      acceptance_criteria:
      - Prefix key (like Ctrl-b) enters command mode
      - Bindings for split, close, navigate, resize
      - Status bar showing pane info
      - Render all panes to terminal
      hints:
        level1: Raw terminal mode to capture all keys. Check for prefix, then command key.
        level2: After prefix, next key is command. 'v' = vertical split, 's' = horizontal, arrows = navigate.
        level3: "import select\nimport tty\nimport sys\n\nclass InputHandler:\n    def __init__(self, mux: Multiplexer):\n\
          \        self.mux = mux\n        self.prefix = b'\\x02'  # Ctrl-B\n        self.in_command_mode = False\n\n    \
          \    self.bindings = {\n            ord('v'): self.split_vertical,\n            ord('s'): self.split_horizontal,\n\
          \            ord('x'): self.close_pane,\n            ord('o'): self.next_pane,\n        }\n\n    def handle_input(self,\
          \ data: bytes):\n        '''Process keyboard input'''\n        if self.in_command_mode:\n            self.in_command_mode\
          \ = False\n            if data in self.bindings:\n                self.bindings[data]()\n            return\n\n\
          \        if data == self.prefix:\n            self.in_command_mode = True\n            return\n\n        # Forward\
          \ to focused pane\n        if self.mux.focused_pane:\n            os.write(self.mux.focused_pane.master_fd, data)\n\
          \ndef main_loop(mux: Multiplexer):\n    '''Main event loop'''\n    # Set terminal to raw mode\n    old_settings\
          \ = termios.tcgetattr(sys.stdin)\n    tty.setraw(sys.stdin)\n\n    handler = InputHandler(mux)\n\n    try:\n   \
          \     while True:\n            # Watch stdin and all PTY masters\n            fds = [sys.stdin] + [p.master_fd for\
          \ p in mux.panes]\n            readable, _, _ = select.select(fds, [], [], 0.1)\n\n            for fd in readable:\n\
          \                if fd == sys.stdin:\n                    data = os.read(sys.stdin.fileno(), 1024)\n           \
          \         handler.handle_input(data)\n                else:\n                    # Find pane and update its screen\n\
          \                    pane = next(p for p in mux.panes if p.master_fd == fd)\n                    data = os.read(fd,\
          \ 4096)\n                    if data:\n                        pane.screen.write(data)\n\n            # Render all\
          \ panes\n            mux.render()\n    finally:\n        termios.tcsetattr(sys.stdin, termios.TCSADRAIN, old_settings)"
      pitfalls:
      - Raw mode disables Ctrl-C - need explicit handling
      - Must restore terminal settings on exit
      - Rendering must be efficient to avoid flicker
      concepts:
      - Raw terminal mode
      - Event loops
      - Key binding systems
      estimated_hours: 8-10
  tetris:
    id: tetris
    name: Tetris
    description: Build the classic Tetris game. Learn about piece rotation, line clearing, and more complex game state management.
    difficulty: beginner
    estimated_hours: 15-20
    prerequisites:
    - Basic programming
    - 2D arrays
    - HTML5 Canvas or similar
    languages:
      recommended:
      - JavaScript
      - Python
      - C#
      also_possible:
      - Rust
      - Go
      - C++
    resources:
    - name: Tetris with JavaScript
      url: https://www.youtube.com/watch?v=rAUn1Lom6dw
      type: video
    - name: Coding Challenges Tetris
      url: https://codingchallenges.fyi/challenges/challenge-tetris/
      type: tutorial
    milestones:
    - id: 1
      name: Board & Tetrominoes
      description: Create the game board and define tetromino shapes.
      acceptance_criteria:
      - 10x20 game board
      - 7 standard tetromino shapes (I, O, T, S, Z, J, L)
      - Each shape has distinct color
      - Shapes defined as 2D arrays
      - Board renders empty grid
      hints:
        level1: 'Board: 2D array of cell states (0=empty, 1-7=piece colors).'
        level2: Define pieces as 2D arrays, use rotation matrices or store all rotations.
        level3: "const PIECES = {\n  I: { shape: [[1,1,1,1]], color: 'cyan' },\n  O: { shape: [[1,1],[1,1]], color: 'yellow'\
          \ },\n  T: { shape: [[0,1,0],[1,1,1]], color: 'purple' },\n  S: { shape: [[0,1,1],[1,1,0]], color: 'green' },\n\
          \  Z: { shape: [[1,1,0],[0,1,1]], color: 'red' },\n  J: { shape: [[1,0,0],[1,1,1]], color: 'blue' },\n  L: { shape:\
          \ [[0,0,1],[1,1,1]], color: 'orange' }\n};"
      pitfalls:
      - Piece definition orientation inconsistent
      - Board dimensions swapped
      - Off-by-one in grid rendering
      concepts:
      - 2D arrays
      - Piece representation
      - Grid rendering
      estimated_hours: 2-3
    - id: 2
      name: Piece Falling & Controls
      description: Implement falling pieces and player controls.
      acceptance_criteria:
      - Current piece falls automatically
      - Left/Right moves piece
      - Down accelerates fall
      - Space hard drops
      - Cannot move outside board
      hints:
        level1: Track current piece position separately from board state.
        level2: 'Validate move before applying: check bounds and collisions.'
        level3: "function isValidPosition(piece, offsetX = 0, offsetY = 0) {\n  const shape = PIECES[piece.type].shape;\n\
          \  for (let row = 0; row < shape.length; row++) {\n    for (let col = 0; col < shape[row].length; col++) {\n   \
          \   if (shape[row][col]) {\n        const newX = piece.x + col + offsetX;\n        const newY = piece.y + row +\
          \ offsetY;\n        if (newX < 0 || newX >= 10 || newY >= 20) return false;\n        if (newY >= 0 && board[newY][newX])\
          \ return false;\n      }\n    }\n  }\n  return true;\n}"
      pitfalls:
      - Negative Y during spawn
      - Piece moving into placed blocks
      - Hard drop going through floor
      concepts:
      - Position validation
      - Input handling
      - Collision detection
      estimated_hours: 3-4
    - id: 3
      name: Piece Rotation
      description: Implement piece rotation with wall kicks.
      acceptance_criteria:
      - Up arrow rotates piece clockwise
      - Rotation works for all pieces
      - Cannot rotate into walls or pieces
      - 'Wall kick: try offset positions if direct rotation fails'
      - O piece doesn't rotate
      hints:
        level1: Transpose and reverse rows to rotate 90Â° clockwise.
        level2: 'If rotation fails, try offsets: (1,0), (-1,0), (0,-1).'
        level3: "function rotateShape(shape) {\n  const rows = shape.length;\n  const cols = shape[0].length;\n  const rotated\
          \ = Array(cols).fill(null).map(() => Array(rows).fill(0));\n  for (let r = 0; r < rows; r++) {\n    for (let c =\
          \ 0; c < cols; c++) {\n      rotated[c][rows - 1 - r] = shape[r][c];\n    }\n  }\n  return rotated;\n}"
      pitfalls:
      - O piece rotation changing position
      - I piece wall kick needs special handling
      - Rotating into ceiling
      concepts:
      - Matrix rotation
      - Wall kicks
      - SRS (Super Rotation System)
      estimated_hours: 3-4
    - id: 4
      name: Line Clearing & Scoring
      description: Implement line clearing and scoring system.
      acceptance_criteria:
      - Filled rows are detected
      - Filled rows are removed
      - Rows above fall down
      - Score based on lines cleared (1/2/3/4 = 100/300/500/800)
      - Level increases with lines
      - Speed increases with level
      hints:
        level1: 'Check each row: if all cells filled, remove and shift above down.'
        level2: Track lines cleared, increase level every 10 lines.
        level3: "function clearLines() {\n  let linesCleared = 0;\n  for (let row = board.length - 1; row >= 0; row--) {\n\
          \    if (board[row].every(cell => cell !== 0)) {\n      board.splice(row, 1);\n      board.unshift(Array(10).fill(0));\n\
          \      linesCleared++;\n      row++;\n    }\n  }\n  if (linesCleared > 0) {\n    const points = [0, 100, 300, 500,\
          \ 800][linesCleared];\n    score += points * level;\n  }\n}"
      pitfalls:
      - Iterating wrong direction when removing
      - Not re-checking row after splice
      - Tetris (4 lines) scoring wrong
      concepts:
      - Row clearing
      - Score systems
      - Difficulty progression
      estimated_hours: 3-4
  todo-app:
    id: todo-app
    name: Todo App
    description: Build a classic todo application with CRUD operations. Learn fundamental web development patterns including
      state management, local storage, and DOM manipulation.
    difficulty: beginner
    estimated_hours: 8-12
    prerequisites:
    - HTML/CSS basics
    - JavaScript fundamentals
    - DOM manipulation basics
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible:
      - Python
      - React
      - Vue
    resources:
    - name: MDN Todo App Tutorial
      url: https://developer.mozilla.org/en-US/docs/Learn/Tools_and_testing/Client-side_JavaScript_frameworks/React_todo_list_beginning
      type: tutorial
    - name: JavaScript Todo App - freeCodeCamp
      url: https://www.freecodecamp.org/news/how-to-build-a-todo-app-with-javascript/
      type: tutorial
    milestones:
    - id: 1
      name: Project Setup & Basic HTML Structure
      description: Set up the project and create the basic HTML structure for the todo app.
      acceptance_criteria:
      - HTML page with input field for new todos
      - Add button to create todos
      - Empty list container for todos
      - Basic CSS styling applied
      hints:
        level1: Start with a simple HTML file with an input, button, and ul element.
        level2: 'Use semantic HTML: main, section, form elements for better structure.'
        level3: "HTML structure:\n<form id=\"todo-form\">\n  <input type=\"text\" id=\"todo-input\" placeholder=\"Add a todo...\"\
          >\n  <button type=\"submit\">Add</button>\n</form>\n<ul id=\"todo-list\"></ul>"
      pitfalls:
      - Forgetting to prevent default form submission
      - Not using semantic HTML elements
      - Hardcoding styles instead of using CSS classes
      concepts:
      - HTML forms
      - Semantic HTML
      - CSS basics
      estimated_hours: 1-2
    - id: 2
      name: Add Todo Functionality
      description: Implement the ability to add new todos to the list.
      acceptance_criteria:
      - User can type in input field
      - Pressing Enter or clicking Add creates new todo
      - New todo appears in the list
      - Input field clears after adding
      - Empty todos are not allowed
      hints:
        level1: Listen for form submit event, get input value, create list item.
        level2: Use createElement or template literals to build todo HTML dynamically.
        level3: "function addTodo(text) {\n  const li = document.createElement('li');\n  li.textContent = text;\n  todoList.appendChild(li);\n\
          }"
      pitfalls:
      - Not trimming whitespace from input
      - Memory leaks from not removing event listeners
      - XSS vulnerability if using innerHTML without sanitization
      concepts:
      - Event handling
      - DOM manipulation
      - Input validation
      estimated_hours: 1-2
    - id: 3
      name: Complete & Delete Todos
      description: Add ability to mark todos as complete and delete them.
      acceptance_criteria:
      - Clicking a todo toggles its completed state
      - Completed todos have visual distinction (strikethrough)
      - Delete button removes todo from list
      - Confirmation before delete (optional)
      hints:
        level1: Add click event to toggle a 'completed' class on the todo item.
        level2: 'Use event delegation: listen on parent, check event.target.'
        level3: "todoList.addEventListener('click', (e) => {\n  if (e.target.matches('.delete-btn')) {\n    e.target.parentElement.remove();\n\
          \  } else if (e.target.matches('li')) {\n    e.target.classList.toggle('completed');\n  }\n});"
      pitfalls:
      - Adding event listeners to each item (use delegation instead)
      - Not handling click on child elements of li
      - Removing wrong element from DOM
      concepts:
      - Event delegation
      - CSS classes for state
      - DOM element removal
      estimated_hours: 1-2
    - id: 4
      name: Local Storage Persistence
      description: Save todos to localStorage so they persist across page refreshes.
      acceptance_criteria:
      - Todos saved to localStorage on every change
      - Todos loaded from localStorage on page load
      - Completed state persists
      - Works even if localStorage is empty
      hints:
        level1: Use JSON.stringify to save array, JSON.parse to load.
        level2: Create a save function, call it after every add/delete/toggle.
        level3: "function saveTodos() {\n  const todos = Array.from(todoList.children).map(li => ({\n    text: li.textContent,\n\
          \    completed: li.classList.contains('completed')\n  }));\n  localStorage.setItem('todos', JSON.stringify(todos));\n\
          }"
      pitfalls:
      - Forgetting to handle null from localStorage.getItem
      - Not updating localStorage after every change
      - Storing DOM elements instead of data
      concepts:
      - localStorage API
      - JSON serialization
      - Data persistence
      estimated_hours: 1-2
    - id: 5
      name: Filter & Polish
      description: Add filtering options and polish the user experience.
      acceptance_criteria:
      - 'Filter buttons: All, Active, Completed'
      - Show count of remaining todos
      - Clear completed button
      - Smooth animations for add/remove
      - Responsive design
      hints:
        level1: Filter by showing/hiding items based on their completed state.
        level2: Keep track of current filter, re-render list when filter changes.
        level3: "function filterTodos(filter) {\n  const todos = todoList.querySelectorAll('li');\n  todos.forEach(todo =>\
          \ {\n    const isCompleted = todo.classList.contains('completed');\n    const show = filter === 'all' ||\n     \
          \ (filter === 'active' && !isCompleted) ||\n      (filter === 'completed' && isCompleted);\n    todo.style.display\
          \ = show ? '' : 'none';\n  });\n}"
      pitfalls:
      - Not updating count when filtering
      - CSS transitions causing layout shifts
      - Accessibility issues with filter buttons
      concepts:
      - UI state management
      - CSS transitions
      - Responsive design
      estimated_hours: 2-3
  tokenizer:
    id: tokenizer
    name: Tokenizer/Lexer
    description: Build a lexer that converts source code into tokens. Foundation for all compilers and interpreters.
    difficulty: beginner
    estimated_hours: 8-15
    prerequisites:
    - Regular expressions basics
    - String manipulation
    languages:
      recommended:
      - Python
      - JavaScript
      - Go
      also_possible:
      - Rust
      - C
    resources:
    - name: Crafting Interpreters - Scanning
      url: https://craftinginterpreters.com/scanning.html
      type: book
    - name: Let's Build a Compiler
      url: https://compilers.iecc.com/crenshaw/
      type: tutorial
    milestones:
    - id: 1
      name: Basic Token Types
      description: Define token types and basic structure.
      acceptance_criteria:
      - Token class with type and value
      - Support numbers and identifiers
      - Support operators (+, -, *, /)
      - Track position (line, column)
      hints:
        level1: Token = (type, value, position). Start simple.
        level2: Use an enum or constants for token types.
        level3: "from enum import Enum, auto\nfrom dataclasses import dataclass\n\nclass TokenType(Enum):\n    # Literals\n\
          \    NUMBER = auto()\n    IDENTIFIER = auto()\n    STRING = auto()\n    \n    # Operators\n    PLUS = auto()\n \
          \   MINUS = auto()\n    STAR = auto()\n    SLASH = auto()\n    \n    # Delimiters\n    LPAREN = auto()\n    RPAREN\
          \ = auto()\n    SEMICOLON = auto()\n    \n    # Keywords\n    IF = auto()\n    ELSE = auto()\n    WHILE = auto()\n\
          \    \n    # Special\n    EOF = auto()\n\n@dataclass\nclass Token:\n    type: TokenType\n    value: str\n    line:\
          \ int\n    column: int\n    \n    def __repr__(self):\n        return f'Token({self.type.name}, {repr(self.value)},\
          \ {self.line}:{self.column})'"
      pitfalls:
      - Forgetting EOF token
      - Not tracking position
      - Inconsistent naming
      concepts:
      - Tokens
      - Lexemes
      - Token types
      estimated_hours: 1-2
    - id: 2
      name: Scanning Logic
      description: Implement the main scanning loop.
      acceptance_criteria:
      - Iterate through source code
      - Match single-character tokens
      - Handle whitespace and newlines
      - Report lexical errors with position
      hints:
        level1: Maintain current position, peek ahead when needed.
        level2: advance() returns current char and moves forward. peek() looks ahead without moving.
        level3: "class Lexer:\n    def __init__(self, source):\n        self.source = source\n        self.pos = 0\n     \
          \   self.line = 1\n        self.column = 1\n        self.tokens = []\n    \n    def is_at_end(self):\n        return\
          \ self.pos >= len(self.source)\n    \n    def advance(self):\n        char = self.source[self.pos]\n        self.pos\
          \ += 1\n        if char == '\\n':\n            self.line += 1\n            self.column = 1\n        else:\n    \
          \        self.column += 1\n        return char\n    \n    def peek(self):\n        if self.is_at_end():\n      \
          \      return '\\0'\n        return self.source[self.pos]\n    \n    def add_token(self, type, value=''):\n    \
          \    self.tokens.append(Token(type, value, self.line, self.column))\n    \n    def scan_tokens(self):\n        while\
          \ not self.is_at_end():\n            self.scan_token()\n        self.add_token(TokenType.EOF)\n        return self.tokens\n\
          \    \n    def scan_token(self):\n        char = self.advance()\n        \n        if char in ' \\t\\r\\n':\n  \
          \          return  # Skip whitespace\n        elif char == '+':\n            self.add_token(TokenType.PLUS, char)\n\
          \        elif char == '-':\n            self.add_token(TokenType.MINUS, char)\n        # ... more cases\n      \
          \  else:\n            raise LexerError(f'Unexpected character: {char}', self.line, self.column)"
      pitfalls:
      - Off-by-one errors
      - Not handling newlines
      - Consuming too much
      concepts:
      - Scanning
      - Character stream
      - Position tracking
      estimated_hours: 2-3
    - id: 3
      name: Multi-character Tokens
      description: Handle numbers, identifiers, strings, and multi-char operators.
      acceptance_criteria:
      - Scan integers and floats
      - Scan identifiers (alphanumeric + underscore)
      - Distinguish keywords from identifiers
      - Handle ==, !=, <=, >= operators
      hints:
        level1: When you see a digit, keep consuming digits. Same for identifiers.
        level2: Use a keyword lookup table to convert identifiers to keywords.
        level3: "KEYWORDS = {\n    'if': TokenType.IF,\n    'else': TokenType.ELSE,\n    'while': TokenType.WHILE,\n    'for':\
          \ TokenType.FOR,\n    'return': TokenType.RETURN,\n    'true': TokenType.TRUE,\n    'false': TokenType.FALSE,\n\
          }\n\ndef scan_number(self):\n    start = self.pos - 1\n    while self.peek().isdigit():\n        self.advance()\n\
          \    \n    # Look for decimal part\n    if self.peek() == '.' and self.peek_next().isdigit():\n        self.advance()\
          \  # consume '.'\n        while self.peek().isdigit():\n            self.advance()\n    \n    value = self.source[start:self.pos]\n\
          \    self.add_token(TokenType.NUMBER, value)\n\ndef scan_identifier(self):\n    start = self.pos - 1\n    while\
          \ self.peek().isalnum() or self.peek() == '_':\n        self.advance()\n    \n    text = self.source[start:self.pos]\n\
          \    token_type = KEYWORDS.get(text, TokenType.IDENTIFIER)\n    self.add_token(token_type, text)\n\ndef match(self,\
          \ expected):\n    '''Consume next char if it matches expected'''\n    if self.is_at_end() or self.source[self.pos]\
          \ != expected:\n        return False\n    self.pos += 1\n    self.column += 1\n    return True\n\n# In scan_token:\n\
          if char == '=':\n    if self.match('='):\n        self.add_token(TokenType.EQUAL_EQUAL, '==')\n    else:\n     \
          \   self.add_token(TokenType.EQUAL, '=')"
      pitfalls:
      - Not handling '.' in floats correctly
      - Keywords vs identifiers
      - Multi-char operators
      concepts:
      - Maximal munch
      - Keyword tables
      - Lookahead
      estimated_hours: 3-4
    - id: 4
      name: Strings and Comments
      description: Handle string literals and comments.
      acceptance_criteria:
      - String literals with escape sequences
      - Single-line comments (//)
      - Multi-line comments (/* */)
      - Handle unterminated strings/comments
      hints:
        level1: 'Strings: consume until closing quote. Handle \n, \t, \\.'
        level2: For multi-line comments, track nesting or just find */.
        level3: "def scan_string(self):\n    start_line = self.line\n    value = ''\n    \n    while self.peek() != '\"' and\
          \ not self.is_at_end():\n        if self.peek() == '\\\\':\n            self.advance()  # consume backslash\n  \
          \          escape = self.advance()\n            if escape == 'n':\n                value += '\\n'\n            elif\
          \ escape == 't':\n                value += '\\t'\n            elif escape == '\\\\':\n                value += '\\\
          \\'\n            elif escape == '\"':\n                value += '\"'\n            else:\n                raise LexerError(f'Invalid\
          \ escape: \\\\{escape}', self.line, self.column)\n        else:\n            value += self.advance()\n    \n   \
          \ if self.is_at_end():\n        raise LexerError('Unterminated string', start_line, self.column)\n    \n    self.advance()\
          \  # closing \"\n    self.add_token(TokenType.STRING, value)\n\ndef skip_comment(self):\n    if self.peek() == '/':\n\
          \        # Single line comment\n        while self.peek() != '\\n' and not self.is_at_end():\n            self.advance()\n\
          \    elif self.peek() == '*':\n        # Multi-line comment\n        self.advance()  # consume *\n        while\
          \ True:\n            if self.is_at_end():\n                raise LexerError('Unterminated comment', self.line, self.column)\n\
          \            if self.peek() == '*' and self.peek_next() == '/':\n                self.advance()  # *\n         \
          \       self.advance()  # /\n                break\n            self.advance()"
      pitfalls:
      - Unterminated strings across lines
      - Nested comments
      - Escape sequences
      concepts:
      - String literals
      - Comments
      - Escape sequences
      estimated_hours: 2-3
  topdown-shooter:
    id: topdown-shooter
    name: Top-down Shooter
    description: Build a top-down shooter with enemies, projectiles, and waves. Learn game AI, collision systems, and game
      feel.
    difficulty: intermediate
    estimated_hours: 20-30
    prerequisites:
    - Basic game loop
    - 2D graphics
    - Vector math
    languages:
      recommended:
      - JavaScript
      - Python
      - C#
      also_possible:
      - C++
      - Lua
    resources:
    - name: Game Programming Patterns
      url: https://gameprogrammingpatterns.com/
      type: book
    milestones:
    - id: 1
      name: Player Movement & Aiming
      description: Implement player with 8-directional movement and mouse aiming.
      acceptance_criteria:
      - WASD movement (8 directions)
      - Mouse aiming (player faces cursor)
      - Smooth movement with acceleration
      - Screen bounds checking
      hints:
        level1: Normalize diagonal movement to avoid faster speed.
        level2: 'Angle to mouse: atan2(mouse.y - player.y, mouse.x - player.x).'
        level3: "class Player:\n    def update(self, dt):\n        # Input\n        dx = (keys['d'] - keys['a'])\n       \
          \ dy = (keys['s'] - keys['w'])\n        \n        # Normalize diagonal\n        if dx != 0 and dy != 0:\n      \
          \      dx *= 0.707  # 1/sqrt(2)\n            dy *= 0.707\n        \n        # Apply movement\n        self.vx =\
          \ dx * SPEED\n        self.vy = dy * SPEED\n        self.x += self.vx * dt\n        self.y += self.vy * dt\n   \
          \     \n        # Aim at mouse\n        self.angle = math.atan2(mouse_y - self.y, mouse_x - self.x)\n        \n\
          \        # Bounds\n        self.x = max(0, min(SCREEN_WIDTH, self.x))\n        self.y = max(0, min(SCREEN_HEIGHT,\
          \ self.y))"
      pitfalls:
      - Diagonal speed boost
      - Angle in wrong units
      - Jittery movement
      concepts:
      - 8-directional movement
      - Mouse aiming
      - Vector normalization
      estimated_hours: 2-3
    - id: 2
      name: Shooting & Projectiles
      description: Implement projectile system with firing and collision.
      acceptance_criteria:
      - Click to fire projectile
      - Projectile travels in aimed direction
      - Fire rate limiting
      - Projectile-enemy collision
      - Remove off-screen projectiles
      hints:
        level1: Projectile velocity = (cos(angle), sin(angle)) * speed.
        level2: Track last fire time for fire rate.
        level3: "class Projectile:\n    def __init__(self, x, y, angle, speed=500):\n        self.x = x\n        self.y =\
          \ y\n        self.vx = math.cos(angle) * speed\n        self.vy = math.sin(angle) * speed\n        self.radius =\
          \ 5\n        self.damage = 10\n    \n    def update(self, dt):\n        self.x += self.vx * dt\n        self.y +=\
          \ self.vy * dt\n    \n    def is_offscreen(self):\n        return self.x < 0 or self.x > SCREEN_WIDTH or self.y\
          \ < 0 or self.y > SCREEN_HEIGHT\n\nclass Player:\n    def __init__(self):\n        self.fire_cooldown = 0\n    \
          \    self.fire_rate = 0.1  # seconds between shots\n    \n    def shoot(self, projectiles):\n        if self.fire_cooldown\
          \ <= 0:\n            proj = Projectile(self.x, self.y, self.angle)\n            projectiles.append(proj)\n     \
          \       self.fire_cooldown = self.fire_rate\n    \n    def update(self, dt):\n        self.fire_cooldown -= dt"
      pitfalls:
      - Projectile spawn position
      - Fire rate timing
      - Memory from many projectiles
      concepts:
      - Projectile physics
      - Fire rate
      - Object pooling
      estimated_hours: 3-4
    - id: 3
      name: Enemies & AI
      description: Add enemies with simple AI behaviors.
      acceptance_criteria:
      - Enemy chases player
      - Different enemy types
      - Health and damage system
      - Death and spawning
      hints:
        level1: 'Chase: direction = normalize(player.pos - enemy.pos).'
        level2: 'Add variety: fast/weak, slow/strong, ranged.'
        level3: "class Enemy:\n    def __init__(self, x, y, enemy_type='basic'):\n        self.x = x\n        self.y = y\n\
          \        if enemy_type == 'basic':\n            self.speed = 100\n            self.health = 30\n            self.damage\
          \ = 10\n        elif enemy_type == 'fast':\n            self.speed = 200\n            self.health = 15\n       \
          \     self.damage = 5\n        elif enemy_type == 'tank':\n            self.speed = 50\n            self.health\
          \ = 100\n            self.damage = 25\n    \n    def update(self, dt, player):\n        # Chase player\n       \
          \ dx = player.x - self.x\n        dy = player.y - self.y\n        dist = math.sqrt(dx*dx + dy*dy)\n        if dist\
          \ > 0:\n            self.x += (dx/dist) * self.speed * dt\n            self.y += (dy/dist) * self.speed * dt\n \
          \   \n    def take_damage(self, amount):\n        self.health -= amount\n        return self.health <= 0  # Returns\
          \ True if dead"
      pitfalls:
      - Division by zero in normalize
      - Enemy stacking
      - Instant kill on spawn
      concepts:
      - Enemy AI
      - Health systems
      - Enemy types
      estimated_hours: 4-5
    - id: 4
      name: Waves & Scoring
      description: Implement wave system and scoring.
      acceptance_criteria:
      - Wave-based spawning
      - Difficulty increases each wave
      - Score for kills
      - High score tracking
      - Wave clear bonus
      hints:
        level1: Wave spawns N enemies. N increases each wave.
        level2: Track enemies alive, start next wave when zero.
        level3: "class WaveManager:\n    def __init__(self):\n        self.wave = 0\n        self.enemies_to_spawn = 0\n \
          \       self.spawn_timer = 0\n        self.spawn_interval = 1.0\n    \n    def start_wave(self, enemies):\n    \
          \    self.wave += 1\n        self.enemies_to_spawn = 5 + self.wave * 2\n        self.spawn_timer = 0\n    \n   \
          \ def update(self, dt, enemies):\n        if self.enemies_to_spawn > 0:\n            self.spawn_timer -= dt\n  \
          \          if self.spawn_timer <= 0:\n                self.spawn_enemy(enemies)\n                self.enemies_to_spawn\
          \ -= 1\n                self.spawn_timer = self.spawn_interval\n        elif len(enemies) == 0:\n            self.start_wave(enemies)\n\
          \    \n    def spawn_enemy(self, enemies):\n        # Spawn at random edge\n        side = random.randint(0, 3)\n\
          \        if side == 0: x, y = random.randint(0, WIDTH), 0\n        elif side == 1: x, y = WIDTH, random.randint(0,\
          \ HEIGHT)\n        # ...\n        enemy_type = random.choice(['basic', 'basic', 'fast', 'tank'])\n        enemies.append(Enemy(x,\
          \ y, enemy_type))"
      pitfalls:
      - Infinite spawn loop
      - No break between waves
      - Unfair difficulty spike
      concepts:
      - Wave systems
      - Difficulty scaling
      - Score systems
      estimated_hours: 3-4
  transformer-scratch:
    id: transformer-scratch
    name: Transformer from Scratch
    description: Implement the Transformer architecture from 'Attention Is All You Need'. Learn self-attention and modern
      NLP architectures.
    difficulty: advanced
    estimated_hours: 30-50
    prerequisites:
    - Neural networks
    - Linear algebra
    - Python/PyTorch basics
    languages:
      recommended:
      - Python
      also_possible:
      - Julia
    resources:
    - name: Attention Is All You Need Paper
      url: https://arxiv.org/abs/1706.03762
      type: paper
    - name: The Illustrated Transformer
      url: https://jalammar.github.io/illustrated-transformer/
      type: article
    - name: Annotated Transformer
      url: https://nlp.seas.harvard.edu/2018/04/03/attention.html
      type: tutorial
    milestones:
    - id: 1
      name: Scaled Dot-Product Attention
      description: Implement the core attention mechanism.
      acceptance_criteria:
      - Compute Q, K, V from input
      - 'Scaled dot-product: softmax(QK^T / sqrt(d_k))V'
      - Handle attention mask for padding/causal
      - Vectorized implementation
      hints:
        level1: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V
        level2: 'Mask: set masked positions to -inf before softmax.'
        level3: "import torch\nimport torch.nn.functional as F\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n\
          \    # Q, K, V: (batch, seq_len, d_k)\n    d_k = Q.size(-1)\n    \n    # Compute attention scores\n    scores =\
          \ torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)\n    # scores: (batch, seq_len, seq_len)\n    \n    # Apply\
          \ mask\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    \n    # Softmax and\
          \ apply to values\n    attn_weights = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn_weights, V)\n  \
          \  \n    return output, attn_weights"
      pitfalls:
      - Forgetting to scale by sqrt(d_k)
      - Wrong mask application
      - Dimension mismatches
      concepts:
      - Attention mechanism
      - Query-Key-Value
      - Masking
      estimated_hours: 4-6
    - id: 2
      name: Multi-Head Attention
      description: Implement multi-head attention with multiple parallel heads.
      acceptance_criteria:
      - Split into h heads
      - Project Q, K, V for each head
      - Parallel attention computation
      - Concatenate and project output
      hints:
        level1: Split d_model into h heads of d_k = d_model/h each.
        level2: 'Linear projections: W_q, W_k, W_v, W_o.'
        level3: "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n\
          \        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model,\
          \ d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n   \
          \     self.W_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, Q, K, V, mask=None):\n        batch_size\
          \ = Q.size(0)\n        \n        # Linear projections and reshape to (batch, heads, seq, d_k)\n        Q = self.W_q(Q).view(batch_size,\
          \ -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,\
          \ 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n       \
          \ # Attention\n        attn_output, _ = scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Concatenate\
          \ heads\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n\
          \        \n        return self.W_o(attn_output)"
      pitfalls:
      - Reshape vs view errors
      - Transpose dimensions wrong
      - Not using contiguous()
      concepts:
      - Multi-head attention
      - Parallel computation
      - Linear projections
      estimated_hours: 4-6
    - id: 3
      name: Position-wise Feed-Forward & Embeddings
      description: Implement FFN layer and positional embeddings.
      acceptance_criteria:
      - Two-layer FFN with ReLU
      - Positional encoding (sinusoidal)
      - Token embeddings
      - Embedding scaling
      hints:
        level1: 'FFN: Linear(d_model, d_ff) -> ReLU -> Linear(d_ff, d_model).'
        level2: 'Positional: PE(pos, 2i) = sin(pos/10000^(2i/d_model)).'
        level3: "class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n\
          \        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n\
          \        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        \n\
          \        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n  \
          \      \n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n    \n \
          \   def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass FeedForward(nn.Module):\n    def __init__(self,\
          \ d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n  \
          \      self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self,\
          \ x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
      pitfalls:
      - Positional encoding dimensions
      - Forgetting dropout
      - Embedding scale factor
      concepts:
      - Positional encoding
      - Feed-forward networks
      - Residual connections
      estimated_hours: 4-6
    - id: 4
      name: Encoder & Decoder Layers
      description: Combine components into encoder/decoder layers.
      acceptance_criteria:
      - 'Encoder: self-attention + FFN'
      - 'Decoder: masked self-attn + cross-attn + FFN'
      - Layer normalization
      - Residual connections
      hints:
        level1: 'Each sublayer: LayerNorm(x + Sublayer(x)).'
        level2: Decoder has extra cross-attention to encoder output.
        level3: "class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n\
          \        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model, d_ff,\
          \ dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout\
          \ = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Self attention with residual\n  \
          \      attn_out = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        # FFN\
          \ with residual\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_out))\n        return\
          \ x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n\
          \        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model,\
          \ num_heads)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n\
          \        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)"
      pitfalls:
      - Pre-norm vs post-norm
      - Causal mask in decoder
      - Cross-attention key/value source
      concepts:
      - Encoder-decoder architecture
      - Layer normalization
      - Residual connections
      estimated_hours: 5-8
    - id: 5
      name: Full Transformer & Training
      description: Assemble full transformer and train on a task.
      acceptance_criteria:
      - Stack N encoder/decoder layers
      - Output projection to vocabulary
      - Label smoothing (optional)
      - Train on translation or LM task
      hints:
        level1: Stack 6 encoder layers + 6 decoder layers (original paper).
        level2: For language modeling, decoder-only with causal mask.
        level3: "class Transformer(nn.Module):\n    def __init__(self, src_vocab, tgt_vocab, d_model=512, num_heads=8, num_layers=6,\
          \ d_ff=2048):\n        super().__init__()\n        self.encoder_embed = nn.Embedding(src_vocab, d_model)\n     \
          \   self.decoder_embed = nn.Embedding(tgt_vocab, d_model)\n        self.pos_encoding = PositionalEncoding(d_model)\n\
          \        \n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, 0.1) for _ in range(num_layers)])\n\
          \        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, 0.1) for _ in range(num_layers)])\n\
          \        \n        self.output_proj = nn.Linear(d_model, tgt_vocab)\n        self.scale = d_model ** 0.5\n    \n\
          \    def encode(self, src, src_mask):\n        x = self.pos_encoding(self.encoder_embed(src) * self.scale)\n   \
          \     for layer in self.encoder_layers:\n            x = layer(x, src_mask)\n        return x\n    \n    def decode(self,\
          \ tgt, enc_out, src_mask, tgt_mask):\n        x = self.pos_encoding(self.decoder_embed(tgt) * self.scale)\n    \
          \    for layer in self.decoder_layers:\n            x = layer(x, enc_out, src_mask, tgt_mask)\n        return self.output_proj(x)"
      pitfalls:
      - Mask generation
      - Teacher forcing
      - Learning rate schedule
      concepts:
      - Full transformer
      - Training loop
      - Inference/generation
      estimated_hours: 8-12
  type-checker:
    id: type-checker
    name: Type Checker
    description: Build a type checker for a statically typed language. Learn type inference, type rules, and semantic analysis.
    difficulty: advanced
    estimated_hours: 20-35
    prerequisites:
    - AST Builder
    - Type systems basics
    - Recursive algorithms
    languages:
      recommended:
      - OCaml
      - Haskell
      - Rust
      also_possible:
      - Python
      - TypeScript
    resources:
    - name: Types and Programming Languages
      url: https://www.cis.upenn.edu/~bcpierce/tapl/
      type: book
    - name: Write You a Haskell
      url: http://dev.stephendiehl.com/fun/
      type: tutorial
    milestones:
    - id: 1
      name: Type Representation
      description: Define type representations and type environment.
      acceptance_criteria:
      - Primitive types (int, bool, string)
      - Function types
      - Type variables for generics
      - Type environment mapping
      hints:
        level1: 'Types are data structures: Int, Bool, Function(params, return), TypeVar(name).'
        level2: 'Type environment: name -> type mapping, with scoping.'
        level3: "from dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n@dataclass\nclass Type:\n \
          \   pass\n\n@dataclass\nclass TInt(Type):\n    def __repr__(self): return 'int'\n\n@dataclass\nclass TBool(Type):\n\
          \    def __repr__(self): return 'bool'\n\n@dataclass\nclass TString(Type):\n    def __repr__(self): return 'string'\n\
          \n@dataclass\nclass TFunction(Type):\n    params: List[Type]\n    ret: Type\n    def __repr__(self): return f'({\"\
          , \".join(map(str, self.params))}) -> {self.ret}'\n\n@dataclass\nclass TVar(Type):\n    '''Type variable for polymorphism'''\n\
          \    name: str\n    def __repr__(self): return self.name\n\nclass TypeEnv:\n    def __init__(self, parent=None):\n\
          \        self.bindings: Dict[str, Type] = {}\n        self.parent = parent\n    \n    def define(self, name: str,\
          \ type: Type):\n        self.bindings[name] = type\n    \n    def lookup(self, name: str) -> Optional[Type]:\n \
          \       if name in self.bindings:\n            return self.bindings[name]\n        if self.parent:\n           \
          \ return self.parent.lookup(name)\n        return None\n    \n    def child(self):\n        return TypeEnv(parent=self)"
      pitfalls:
      - Forgetting unit/void type
      - Mutable type variables
      - Scope handling
      concepts:
      - Type representations
      - Type environments
      - Scoping
      estimated_hours: 3-4
    - id: 2
      name: Basic Type Checking
      description: Check types for expressions and statements.
      acceptance_criteria:
      - Literals have known types
      - Binary operators check operand types
      - Function calls check argument types
      - Assignments check type compatibility
      hints:
        level1: check(node, env) -> Type. Recursively check children.
        level2: 'Binary +: both operands int -> result int. ==: same types -> bool.'
        level3: "class TypeChecker:\n    def __init__(self):\n        self.errors = []\n    \n    def error(self, message,\
          \ node):\n        self.errors.append(TypeError(message, node))\n    \n    def check_expr(self, expr, env) -> Type:\n\
          \        if isinstance(expr, Literal):\n            if isinstance(expr.value, int):\n                return TInt()\n\
          \            elif isinstance(expr.value, bool):\n                return TBool()\n            elif isinstance(expr.value,\
          \ str):\n                return TString()\n        \n        elif isinstance(expr, Identifier):\n            type\
          \ = env.lookup(expr.name)\n            if type is None:\n                self.error(f'Undefined variable: {expr.name}',\
          \ expr)\n                return TInt()  # Return something to continue\n            return type\n        \n    \
          \    elif isinstance(expr, Binary):\n            left_type = self.check_expr(expr.left, env)\n            right_type\
          \ = self.check_expr(expr.right, env)\n            \n            if expr.operator in ['+', '-', '*', '/']:\n    \
          \            if not isinstance(left_type, TInt) or not isinstance(right_type, TInt):\n                    self.error(f'Operator\
          \ {expr.operator} requires int operands', expr)\n                return TInt()\n            \n            elif expr.operator\
          \ in ['<', '>', '<=', '>=']:\n                if not isinstance(left_type, TInt) or not isinstance(right_type, TInt):\n\
          \                    self.error(f'Comparison requires int operands', expr)\n                return TBool()\n   \
          \         \n            elif expr.operator in ['==', '!=']:\n                if type(left_type) != type(right_type):\n\
          \                    self.error(f'Cannot compare {left_type} and {right_type}', expr)\n                return TBool()\n\
          \        \n        elif isinstance(expr, Call):\n            callee_type = self.check_expr(expr.callee, env)\n \
          \           if not isinstance(callee_type, TFunction):\n                self.error('Cannot call non-function', expr)\n\
          \                return TInt()\n            \n            if len(expr.arguments) != len(callee_type.params):\n \
          \               self.error(f'Expected {len(callee_type.params)} arguments, got {len(expr.arguments)}', expr)\n \
          \           \n            for arg, param_type in zip(expr.arguments, callee_type.params):\n                arg_type\
          \ = self.check_expr(arg, env)\n                if not self.types_match(arg_type, param_type):\n                \
          \    self.error(f'Expected {param_type}, got {arg_type}', arg)\n            \n            return callee_type.ret"
      pitfalls:
      - Operator type rules
      - Function arity
      - Return type tracking
      concepts:
      - Type checking
      - Type rules
      - Error reporting
      estimated_hours: 5-7
    - id: 3
      name: Type Inference
      description: Infer types where not explicitly annotated.
      acceptance_criteria:
      - Infer variable types from initializers
      - Infer function return types
      - Unification algorithm
      - Handle type constraints
      hints:
        level1: 'Inference: generate constraints, then solve (unify).'
        level2: 'Unification: TVar = concrete type, or TVar = TVar.'
        level3: "class TypeInferrer:\n    def __init__(self):\n        self.type_var_counter = 0\n        self.substitution\
          \ = {}  # TVar name -> Type\n    \n    def fresh_type_var(self):\n        self.type_var_counter += 1\n        return\
          \ TVar(f't{self.type_var_counter}')\n    \n    def unify(self, t1: Type, t2: Type):\n        '''Make t1 and t2 the\
          \ same type'''\n        t1 = self.apply_substitution(t1)\n        t2 = self.apply_substitution(t2)\n        \n \
          \       if isinstance(t1, TVar):\n            if t1.name != getattr(t2, 'name', None):\n                # Occurs\
          \ check\n                if self.occurs(t1, t2):\n                    raise TypeError(f'Infinite type: {t1} = {t2}')\n\
          \                self.substitution[t1.name] = t2\n        elif isinstance(t2, TVar):\n            self.unify(t2,\
          \ t1)\n        elif type(t1) == type(t2):\n            if isinstance(t1, TFunction):\n                if len(t1.params)\
          \ != len(t2.params):\n                    raise TypeError(f'Arity mismatch')\n                for p1, p2 in zip(t1.params,\
          \ t2.params):\n                    self.unify(p1, p2)\n                self.unify(t1.ret, t2.ret)\n        else:\n\
          \            raise TypeError(f'Cannot unify {t1} with {t2}')\n    \n    def apply_substitution(self, type: Type)\
          \ -> Type:\n        if isinstance(type, TVar):\n            if type.name in self.substitution:\n               \
          \ return self.apply_substitution(self.substitution[type.name])\n            return type\n        elif isinstance(type,\
          \ TFunction):\n            return TFunction(\n                [self.apply_substitution(p) for p in type.params],\n\
          \                self.apply_substitution(type.ret)\n            )\n        return type\n    \n    def occurs(self,\
          \ tvar: TVar, type: Type) -> bool:\n        '''Check if tvar occurs in type (infinite type check)'''\n        type\
          \ = self.apply_substitution(type)\n        if isinstance(type, TVar):\n            return tvar.name == type.name\n\
          \        elif isinstance(type, TFunction):\n            return any(self.occurs(tvar, p) for p in type.params) or\
          \ self.occurs(tvar, type.ret)\n        return False"
      pitfalls:
      - Infinite types
      - Substitution application
      - Occurs check
      concepts:
      - Type inference
      - Unification
      - Constraints
      estimated_hours: 6-10
    - id: 4
      name: Polymorphism
      description: Add support for generic/polymorphic types.
      acceptance_criteria:
      - Let polymorphism
      - Generalize types at let bindings
      - Instantiate polymorphic types at use sites
      - Type schemes (forall quantification)
      hints:
        level1: 'Polymorphism: ''id'' has type forall a. a -> a.'
        level2: 'Generalize: collect free type vars. Instantiate: replace with fresh vars.'
        level3: "@dataclass\nclass Scheme:\n    '''Type scheme: forall [vars] . type'''\n    vars: List[str]  # Quantified\
          \ variables\n    type: Type\n\nclass TypeInferrer:\n    def generalize(self, type: Type, env: TypeEnv) -> Scheme:\n\
          \        '''Generalize type by quantifying free variables not in env'''\n        type = self.apply_substitution(type)\n\
          \        free_in_type = self.free_vars(type)\n        free_in_env = self.free_vars_env(env)\n        quantified\
          \ = free_in_type - free_in_env\n        return Scheme(list(quantified), type)\n    \n    def instantiate(self, scheme:\
          \ Scheme) -> Type:\n        '''Create fresh type variables for quantified vars'''\n        subst = {var: self.fresh_type_var()\
          \ for var in scheme.vars}\n        return self.apply_subst_to_type(scheme.type, subst)\n    \n    def free_vars(self,\
          \ type: Type) -> set:\n        if isinstance(type, TVar):\n            return {type.name}\n        elif isinstance(type,\
          \ TFunction):\n            result = set()\n            for p in type.params:\n                result |= self.free_vars(p)\n\
          \            result |= self.free_vars(type.ret)\n            return result\n        return set()\n\n# Algorithm\
          \ W for let-polymorphism:\n# let x = e1 in e2\n# 1. Infer type of e1\n# 2. Generalize to get scheme\n# 3. Add x:\
          \ scheme to env\n# 4. Infer type of e2 in extended env"
      pitfalls:
      - Value restriction
      - Generalization timing
      - Monomorphization
      concepts:
      - Polymorphism
      - Type schemes
      - Generalization
      estimated_hours: 6-10
  unit-testing-basics:
    id: unit-testing-basics
    name: Unit Testing Fundamentals
    description: Learn unit testing principles and practices. Write effective tests for confidence in your code.
    difficulty: beginner
    estimated_hours: 6-10
    prerequisites:
    - Basic programming
    - Functions and classes
    languages:
      recommended:
      - Python
      - JavaScript
      - Java
      also_possible:
      - Go
      - Rust
    resources:
    - name: pytest Documentation
      url: https://docs.pytest.org/
      type: documentation
    - name: Jest Documentation
      url: https://jestjs.io/docs/getting-started
      type: documentation
    milestones:
    - id: 1
      name: First Tests
      description: Write your first unit tests.
      acceptance_criteria:
      - Set up test framework
      - Write test for pure function
      - Use assertions (assertEqual, assertTrue)
      - Run tests and see results
      - Test edge cases
      hints:
        level1: Test function should start with test_.
        level2: Arrange-Act-Assert pattern.
        level3: "# calculator.py\ndef add(a, b):\n    return a + b\n\ndef divide(a, b):\n    if b == 0:\n        raise ValueError(\"\
          Cannot divide by zero\")\n    return a / b\n\n# test_calculator.py\nimport pytest\nfrom calculator import add, divide\n\
          \ndef test_add_positive_numbers():\n    # Arrange\n    a, b = 2, 3\n    # Act\n    result = add(a, b)\n    # Assert\n\
          \    assert result == 5\n\ndef test_add_negative_numbers():\n    assert add(-1, -1) == -2\n\ndef test_add_zero():\n\
          \    assert add(0, 5) == 5\n\ndef test_divide_normal():\n    assert divide(10, 2) == 5\n\ndef test_divide_by_zero():\n\
          \    with pytest.raises(ValueError):\n        divide(10, 0)"
      pitfalls:
      - Testing implementation not behavior
      - Not testing edge cases
      - Fragile assertions
      concepts:
      - Unit testing
      - Assertions
      - Test structure
      estimated_hours: 2-3
    - id: 2
      name: Test Organization
      description: Organize tests with fixtures and setup.
      acceptance_criteria:
      - Group related tests in classes
      - Use setup/teardown
      - Share fixtures between tests
      - Parameterized tests
      hints:
        level1: Fixtures provide reusable test data.
        level2: Parameterize to test multiple inputs.
        level3: "import pytest\n\n# Fixtures\n@pytest.fixture\ndef user():\n    return {'name': 'John', 'email': 'john@example.com'}\n\
          \n@pytest.fixture\ndef database():\n    db = connect_test_db()\n    yield db  # Test runs here\n    db.close() \
          \ # Cleanup\n\ndef test_user_creation(user):\n    assert user['name'] == 'John'\n\n# Parameterized tests\n@pytest.mark.parametrize('input,expected',\
          \ [\n    (1, 1),\n    (2, 4),\n    (3, 9),\n    (-2, 4),\n])\ndef test_square(input, expected):\n    assert input\
          \ ** 2 == expected\n\n# Test class for grouping\nclass TestStringMethods:\n    def test_upper(self):\n        assert\
          \ 'hello'.upper() == 'HELLO'\n    \n    def test_split(self):\n        assert 'a,b,c'.split(',') == ['a', 'b', 'c']"
      pitfalls:
      - Fixtures with side effects
      - Shared mutable state
      - Over-complicated fixtures
      concepts:
      - Fixtures
      - Parameterization
      - Test isolation
      estimated_hours: 2-3
    - id: 3
      name: Mocking and Isolation
      description: Test code with external dependencies.
      acceptance_criteria:
      - Mock external API calls
      - Mock file system operations
      - Verify mock was called
      - Patch at correct level
      hints:
        level1: Mock replaces real object with fake.
        level2: Patch where it's used, not where it's defined.
        level3: "from unittest.mock import Mock, patch\n\n# Code under test\ndef get_user_data(user_id):\n    response = requests.get(f'https://api.example.com/users/{user_id}')\n\
          \    return response.json()\n\ndef process_user(user_id):\n    data = get_user_data(user_id)\n    return data['name'].upper()\n\
          \n# Test with mocking\n@patch('mymodule.requests.get')\ndef test_process_user(mock_get):\n    # Setup mock response\n\
          \    mock_response = Mock()\n    mock_response.json.return_value = {'name': 'john', 'id': 1}\n    mock_get.return_value\
          \ = mock_response\n    \n    # Act\n    result = process_user(1)\n    \n    # Assert\n    assert result == 'JOHN'\n\
          \    mock_get.assert_called_once_with('https://api.example.com/users/1')\n\n# Mock file operations\n@patch('builtins.open',\
          \ mock_open(read_data='file content'))\ndef test_read_config():\n    result = read_config('config.txt')\n    assert\
          \ result == 'file content'"
      pitfalls:
      - Mocking too much
      - Wrong patch location
      - Not verifying interactions
      concepts:
      - Mocking
      - Dependency injection
      - Test isolation
      estimated_hours: 2-3
  vector-clocks:
    id: vector-clocks
    name: Vector Clocks
    description: Implement vector clocks for tracking causality in distributed systems. Learn about logical time and partial
      ordering.
    difficulty: intermediate
    estimated_hours: 8-12
    prerequisites:
    - Distributed systems basics
    - Understanding of causality
    languages:
      recommended:
      - Python
      - Go
      - Java
      also_possible:
      - Rust
      - JavaScript
    resources:
    - name: Vector Clocks Paper
      url: https://en.wikipedia.org/wiki/Vector_clock
      type: article
    milestones:
    - id: 1
      name: Basic Vector Clock
      description: Implement vector clock data structure and operations.
      acceptance_criteria:
      - Initialize clock for N nodes
      - Increment local clock on event
      - Merge clocks on message receive
      - Compare clocks (before, after, concurrent)
      hints:
        level1: Vector clock is dict/array of counters, one per node.
        level2: 'Merge: take max of each component.'
        level3: "class VectorClock:\n    def __init__(self, node_id, num_nodes):\n        self.node_id = node_id\n       \
          \ self.clock = [0] * num_nodes\n    \n    def increment(self):\n        \"\"\"Called on local event\"\"\"\n    \
          \    self.clock[self.node_id] += 1\n    \n    def update(self, other_clock):\n        \"\"\"Called when receiving\
          \ message\"\"\"\n        for i in range(len(self.clock)):\n            self.clock[i] = max(self.clock[i], other_clock[i])\n\
          \        self.increment()  # Receive is also an event\n    \n    def send(self):\n        \"\"\"Called when sending\
          \ message, returns clock to attach\"\"\"\n        self.increment()\n        return self.clock.copy()\n    \n   \
          \ def compare(self, other):\n        \"\"\"Returns 'before', 'after', 'concurrent', or 'equal'\"\"\"\n        less\
          \ = all(a <= b for a, b in zip(self.clock, other))\n        greater = all(a >= b for a, b in zip(self.clock, other))\n\
          \        if less and greater:\n            return 'equal'\n        if less:\n            return 'before'\n     \
          \   if greater:\n            return 'after'\n        return 'concurrent'"
      pitfalls:
      - Forgetting to increment on receive
      - Not copying clock on send
      - Comparison logic
      concepts:
      - Vector clocks
      - Causality
      - Partial ordering
      estimated_hours: 3-4
    - id: 2
      name: Conflict Detection
      description: Use vector clocks to detect conflicting updates.
      acceptance_criteria:
      - Detect concurrent writes to same key
      - Store version with each value
      - Return all concurrent versions on read
      - Last-writer-wins (optional)
      hints:
        level1: Concurrent clocks = neither happened-before the other.
        level2: Keep list of (value, clock) pairs for conflicts.
        level3: "class VersionedStore:\n    def __init__(self):\n        self.data = {}  # key -> list of (value, clock)\n\
          \    \n    def put(self, key, value, clock):\n        if key not in self.data:\n            self.data[key] = []\n\
          \        \n        # Remove versions this one supersedes\n        new_versions = []\n        for old_value, old_clock\
          \ in self.data[key]:\n            relation = self._compare(old_clock, clock)\n            if relation == 'after'\
          \ or relation == 'equal':\n                # Old version supersedes or equals new, keep it\n                new_versions.append((old_value,\
          \ old_clock))\n            elif relation == 'concurrent':\n                # Conflict! Keep both\n             \
          \   new_versions.append((old_value, old_clock))\n            # 'before' means old is superseded, don't keep\n  \
          \      \n        new_versions.append((value, clock))\n        self.data[key] = new_versions\n    \n    def get(self,\
          \ key):\n        \"\"\"Returns list of (value, clock) - multiple means conflict\"\"\"\n        return self.data.get(key,\
          \ [])"
      pitfalls:
      - Growing version lists without cleanup
      - Comparison direction
      - Lost updates
      concepts:
      - Conflict detection
      - Version vectors
      - Multi-value registers
      estimated_hours: 3-4
    - id: 3
      name: Version Pruning
      description: Implement strategies to limit unbounded growth of version history.
      acceptance_criteria:
      - Configurable max versions per key
      - Prune oldest versions when limit exceeded
      - 'Optional: prune dominated versions'
      - Garbage collection of unreferenced clocks
      - Metrics for version count
      hints:
        level1: Track timestamp with each version. Prune by age or count.
        level2: 'Dominated version: another version has higher clock values for all nodes.'
        level3: "class PrunedVersionStore:\n    def __init__(self, max_versions=10, max_age_seconds=3600):\n        self.data\
          \ = {}  # key -> list of (value, clock, timestamp)\n        self.max_versions = max_versions\n        self.max_age\
          \ = max_age_seconds\n\n    def put(self, key, value, clock):\n        import time\n        now = time.time()\n\n\
          \        if key not in self.data:\n            self.data[key] = []\n\n        versions = self.data[key]\n\n    \
          \    # Remove dominated versions\n        new_versions = []\n        for v, vc, ts in versions:\n            if\
          \ not self._is_dominated(vc, clock):\n                new_versions.append((v, vc, ts))\n\n        # Add new version\n\
          \        new_versions.append((value, clock.copy(), now))\n\n        # Prune by age\n        new_versions = [(v,\
          \ vc, ts) for v, vc, ts in new_versions\n                        if now - ts < self.max_age]\n\n        # Prune\
          \ by count (keep most recent)\n        if len(new_versions) > self.max_versions:\n            new_versions.sort(key=lambda\
          \ x: x[2])  # Sort by timestamp\n            new_versions = new_versions[-self.max_versions:]\n\n        self.data[key]\
          \ = new_versions\n\n    def _is_dominated(self, clock1, clock2):\n        '''Returns True if clock1 <= clock2 (clock2\
          \ dominates clock1)'''\n        if len(clock1) != len(clock2):\n            return False\n        return all(c1\
          \ <= c2 for c1, c2 in zip(clock1, clock2))\n\n    def get_stats(self):\n        total_versions = sum(len(v) for\
          \ v in self.data.values())\n        return {\n            'keys': len(self.data),\n            'total_versions':\
          \ total_versions,\n            'avg_versions_per_key': total_versions / max(1, len(self.data))\n        }"
      pitfalls:
      - Pruning too aggressively loses conflict info
      - Clock comparison must handle different lengths
      - Timestamp skew across nodes
      - Memory leaks from orphaned clocks
      concepts:
      - Version pruning strategies
      - Garbage collection
      - Dominated versions
      - 'Trade-off: consistency vs storage'
      estimated_hours: 2-3
    - id: 4
      name: Distributed Key-Value Store
      description: Integrate vector clocks into a working distributed key-value store.
      acceptance_criteria:
      - Multi-node setup (3+ nodes)
      - PUT/GET operations with vector clocks
      - Read-repair on conflict detection
      - Configurable conflict resolution (LWW, merge, manual)
      - Simple replication (all nodes store all keys)
      hints:
        level1: Each node maintains its own vector clock. Attach clock to every write.
        level2: On GET, collect from multiple nodes. If clocks differ, return all versions.
        level3: "import asyncio\nimport aiohttp\nfrom aiohttp import web\n\nclass DistributedKVNode:\n    def __init__(self,\
          \ node_id, peers):\n        self.node_id = node_id\n        self.peers = peers  # list of (host, port)\n       \
          \ self.store = VersionedStore()\n        self.clock = VectorClock(node_id, len(peers) + 1)\n\n    async def put(self,\
          \ key, value):\n        '''Write to local and replicate to peers'''\n        clock = self.clock.send()\n       \
          \ self.store.put(key, value, clock)\n\n        # Async replicate to peers\n        tasks = [self._replicate_to_peer(peer,\
          \ key, value, clock)\n                 for peer in self.peers]\n        await asyncio.gather(*tasks, return_exceptions=True)\n\
          \n        return {'status': 'ok', 'clock': clock}\n\n    async def _replicate_to_peer(self, peer, key, value, clock):\n\
          \        url = f'http://{peer[0]}:{peer[1]}/internal/replicate'\n        async with aiohttp.ClientSession() as session:\n\
          \            await session.post(url, json={\n                'key': key, 'value': value, 'clock': clock\n      \
          \      })\n\n    async def get(self, key, quorum=2):\n        '''Read from multiple nodes, detect conflicts'''\n\
          \        results = [self.store.get(key)]\n\n        # Query peers\n        tasks = [self._get_from_peer(peer, key)\
          \ for peer in self.peers]\n        peer_results = await asyncio.gather(*tasks, return_exceptions=True)\n\n     \
          \   for r in peer_results:\n            if not isinstance(r, Exception) and r:\n                results.append(r)\n\
          \n        # Merge all versions\n        all_versions = []\n        for versions in results:\n            all_versions.extend(versions)\n\
          \n        # Remove dominated versions\n        final = self._remove_dominated(all_versions)\n\n        if len(final)\
          \ > 1:\n            return {'status': 'conflict', 'versions': final}\n        elif len(final) == 1:\n          \
          \  return {'status': 'ok', 'value': final[0][0]}\n        else:\n            return {'status': 'not_found'}\n\n\
          \    def _remove_dominated(self, versions):\n        result = []\n        for v1, c1 in versions:\n            dominated\
          \ = False\n            for v2, c2 in versions:\n                if c1 != c2 and self._is_dominated(c1, c2):\n  \
          \                  dominated = True\n                    break\n            if not dominated:\n                result.append((v1,\
          \ c1))\n        return result\n\n# HTTP handlers\nroutes = web.RouteTableDef()\n\n@routes.post('/kv/{key}')\nasync\
          \ def handle_put(request):\n    key = request.match_info['key']\n    data = await request.json()\n    result = await\
          \ node.put(key, data['value'])\n    return web.json_response(result)\n\n@routes.get('/kv/{key}')\nasync def handle_get(request):\n\
          \    key = request.match_info['key']\n    result = await node.get(key)\n    return web.json_response(result)"
      pitfalls:
      - Network partitions causing divergence
      - Replication lag during high write load
      - Clock synchronization across restarts
      - Handling node failures during writes
      concepts:
      - Distributed replication
      - Quorum reads/writes
      - Read repair
      - Eventual consistency
      estimated_hours: 4-6
  video-streaming:
    id: video-streaming
    name: Video Streaming Platform
    description: Build a video streaming service with upload, transcoding, and adaptive playback.
    difficulty: intermediate
    estimated_hours: 30-50
    prerequisites:
    - HTTP server
    - File handling
    - Basic frontend
    languages:
      recommended:
      - Node.js
      - Python
      - Go
      also_possible:
      - Java
      - Rust
    resources:
    - type: article
      name: HLS Streaming Explained
      url: https://www.cloudflare.com/learning/video/what-is-hls-streaming/
    - type: tool
      name: FFmpeg Documentation
      url: https://ffmpeg.org/documentation.html
    milestones:
    - id: 1
      name: Video Upload
      description: Handle large video file uploads with progress tracking.
      acceptance_criteria:
      - Chunked upload support
      - Progress tracking
      - File validation
      - Storage management
      hints:
        level1: Use multipart upload for large files. Validate MIME types.
        level2: Resumable uploads with chunk tracking. Store metadata separately.
        level3: "// Chunked upload endpoint\nconst uploads = new Map();  // uploadId -> chunks\n\napp.post('/api/upload/init',\
          \ (req, res) => {\n    const { filename, fileSize, mimeType } = req.body;\n    \n    // Validate\n    if (!mimeType.startsWith('video/'))\
          \ {\n        return res.status(400).json({ error: 'Must be video' });\n    }\n    \n    const uploadId = crypto.randomUUID();\n\
          \    const chunkSize = 5 * 1024 * 1024;  // 5MB chunks\n    const totalChunks = Math.ceil(fileSize / chunkSize);\n\
          \    \n    uploads.set(uploadId, {\n        filename,\n        fileSize,\n        chunkSize,\n        totalChunks,\n\
          \        receivedChunks: new Set(),\n        tempPath: `/tmp/uploads/${uploadId}`\n    });\n    \n    fs.mkdirSync(`/tmp/uploads/${uploadId}`,\
          \ { recursive: true });\n    \n    res.json({ uploadId, chunkSize, totalChunks });\n});\n\napp.post('/api/upload/:uploadId/chunk/:chunkIndex',\
          \ async (req, res) => {\n    const { uploadId, chunkIndex } = req.params;\n    const upload = uploads.get(uploadId);\n\
          \    \n    if (!upload) return res.status(404).json({ error: 'Upload not found' });\n    \n    // Save chunk\n \
          \   const chunkPath = `${upload.tempPath}/chunk_${chunkIndex}`;\n    await pipeline(req, fs.createWriteStream(chunkPath));\n\
          \    \n    upload.receivedChunks.add(parseInt(chunkIndex));\n    \n    // Check if complete\n    if (upload.receivedChunks.size\
          \ === upload.totalChunks) {\n        // Trigger assembly\n        assembleChunks(uploadId);\n    }\n    \n    res.json({\n\
          \        received: upload.receivedChunks.size,\n        total: upload.totalChunks\n    });\n});"
      pitfalls:
      - Memory issues with large files
      - Incomplete uploads
      - Storage cleanup
      concepts:
      - Chunked transfer
      - Resumable uploads
      - File validation
      estimated_hours: 5-8
    - id: 2
      name: Video Transcoding
      description: Convert videos to streaming-friendly formats using FFmpeg.
      acceptance_criteria:
      - FFmpeg integration
      - Multiple quality levels
      - Progress monitoring
      - Background processing
      hints:
        level1: Use FFmpeg to create HLS segments. Start with 720p, add more qualities.
        level2: Run transcoding in background job queue. Track progress via FFmpeg output.
        level3: "const { spawn } = require('child_process');\n\nasync function transcodeToHLS(inputPath, outputDir, qualities)\
          \ {\n    // qualities: [{name: '720p', width: 1280, height: 720, bitrate: '3000k'}]\n    \n    for (const q of qualities)\
          \ {\n        const outputPath = `${outputDir}/${q.name}`;\n        fs.mkdirSync(outputPath, { recursive: true });\n\
          \        \n        await new Promise((resolve, reject) => {\n            const ffmpeg = spawn('ffmpeg', [\n    \
          \            '-i', inputPath,\n                '-vf', `scale=${q.width}:${q.height}`,\n                '-c:v', 'libx264',\n\
          \                '-b:v', q.bitrate,\n                '-c:a', 'aac',\n                '-b:a', '128k',\n         \
          \       '-hls_time', '10',\n                '-hls_list_size', '0',\n                '-hls_segment_filename', `${outputPath}/segment_%03d.ts`,\n\
          \                `${outputPath}/playlist.m3u8`\n            ]);\n            \n            ffmpeg.stderr.on('data',\
          \ (data) => {\n                // Parse progress from FFmpeg output\n                const match = data.toString().match(/time=(\\\
          d+:\\d+:\\d+)/);\n                if (match) {\n                    updateProgress(inputPath, q.name, match[1]);\n\
          \                }\n            });\n            \n            ffmpeg.on('close', (code) => {\n                code\
          \ === 0 ? resolve() : reject(new Error(`FFmpeg exit ${code}`));\n            });\n        });\n    }\n    \n   \
          \ // Create master playlist\n    createMasterPlaylist(outputDir, qualities);\n}\n\nfunction createMasterPlaylist(outputDir,\
          \ qualities) {\n    let content = '#EXTM3U\\n';\n    \n    for (const q of qualities) {\n        content += `#EXT-X-STREAM-INF:BANDWIDTH=${parseInt(q.bitrate)*1000},RESOLUTION=${q.width}x${q.height}\\\
          n`;\n        content += `${q.name}/playlist.m3u8\\n`;\n    }\n    \n    fs.writeFileSync(`${outputDir}/master.m3u8`,\
          \ content);\n}"
      pitfalls:
      - FFmpeg memory usage
      - Codec compatibility
      - Interrupted transcoding
      concepts:
      - Video codecs
      - HLS format
      - Background jobs
      estimated_hours: 8-12
    - id: 3
      name: Adaptive Streaming
      description: Serve HLS streams with quality adaptation.
      acceptance_criteria:
      - HLS manifest serving
      - Segment serving
      - Byte-range requests
      - CDN-friendly headers
      hints:
        level1: Serve .m3u8 as application/vnd.apple.mpegurl, .ts as video/mp2t.
        level2: Support Range requests for seeking. Set proper cache headers.
        level3: "// Serve HLS content\napp.get('/api/videos/:videoId/stream/*', async (req, res) => {\n    const { videoId\
          \ } = req.params;\n    const filePath = req.params[0];  // e.g., 'master.m3u8' or '720p/segment_001.ts'\n    \n\
          \    const fullPath = path.join(VIDEO_DIR, videoId, filePath);\n    \n    if (!fs.existsSync(fullPath)) {\n    \
          \    return res.status(404).send('Not found');\n    }\n    \n    // Set content type\n    const ext = path.extname(filePath);\n\
          \    const contentTypes = {\n        '.m3u8': 'application/vnd.apple.mpegurl',\n        '.ts': 'video/mp2t'\n  \
          \  };\n    res.setHeader('Content-Type', contentTypes[ext] || 'application/octet-stream');\n    \n    // Cache headers\n\
          \    if (ext === '.m3u8') {\n        res.setHeader('Cache-Control', 'no-cache');  // Playlist can change\n    }\
          \ else {\n        res.setHeader('Cache-Control', 'public, max-age=31536000');  // Segments immutable\n    }\n  \
          \  \n    // Handle Range requests for segments\n    const stat = fs.statSync(fullPath);\n    const range = req.headers.range;\n\
          \    \n    if (range && ext === '.ts') {\n        const parts = range.replace(/bytes=/, '').split('-');\n      \
          \  const start = parseInt(parts[0], 10);\n        const end = parts[1] ? parseInt(parts[1], 10) : stat.size - 1;\n\
          \        \n        res.status(206);\n        res.setHeader('Content-Range', `bytes ${start}-${end}/${stat.size}`);\n\
          \        res.setHeader('Content-Length', end - start + 1);\n        \n        fs.createReadStream(fullPath, { start,\
          \ end }).pipe(res);\n    } else {\n        res.setHeader('Content-Length', stat.size);\n        fs.createReadStream(fullPath).pipe(res);\n\
          \    }\n});"
      pitfalls:
      - CORS for cross-origin players
      - Playlist caching issues
      - Seeking accuracy
      concepts:
      - HLS protocol
      - HTTP range requests
      - Caching strategies
      estimated_hours: 6-10
    - id: 4
      name: Video Player Integration
      description: Build frontend player with quality switching and progress tracking.
      acceptance_criteria:
      - HLS.js integration
      - Quality selector
      - Progress bar/seeking
      - Playback analytics
      hints:
        level1: Use HLS.js library for non-Safari browsers. Safari has native HLS.
        level2: Track watch progress for resume. Send analytics events.
        level3: "// Video player component\nclass VideoPlayer {\n    constructor(container, videoId) {\n        this.video\
          \ = document.createElement('video');\n        this.video.controls = true;\n        container.appendChild(this.video);\n\
          \        \n        this.videoId = videoId;\n        this.hls = null;\n        this.qualities = [];\n        \n \
          \       this.setupHLS();\n        this.setupAnalytics();\n    }\n    \n    setupHLS() {\n        const src = `/api/videos/${this.videoId}/stream/master.m3u8`;\n\
          \        \n        if (this.video.canPlayType('application/vnd.apple.mpegurl')) {\n            // Safari native\
          \ HLS\n            this.video.src = src;\n        } else if (Hls.isSupported()) {\n            this.hls = new Hls({\n\
          \                enableWorker: true,\n                lowLatencyMode: false\n            });\n            \n   \
          \         this.hls.loadSource(src);\n            this.hls.attachMedia(this.video);\n            \n            this.hls.on(Hls.Events.MANIFEST_PARSED,\
          \ (event, data) => {\n                this.qualities = data.levels.map((level, i) => ({\n                    index:\
          \ i,\n                    height: level.height,\n                    bitrate: level.bitrate\n                }));\n\
          \                this.renderQualitySelector();\n            });\n        }\n    }\n    \n    setQuality(levelIndex)\
          \ {\n        if (this.hls) {\n            this.hls.currentLevel = levelIndex;  // -1 for auto\n        }\n    }\n\
          \    \n    setupAnalytics() {\n        let lastReport = 0;\n        \n        this.video.addEventListener('timeupdate',\
          \ () => {\n            const now = Date.now();\n            if (now - lastReport > 10000) {  // Every 10 seconds\n\
          \                this.reportProgress(this.video.currentTime);\n                lastReport = now;\n            }\n\
          \        });\n        \n        this.video.addEventListener('ended', () => {\n            this.reportComplete();\n\
          \        });\n    }\n    \n    async reportProgress(time) {\n        await fetch(`/api/videos/${this.videoId}/progress`,\
          \ {\n            method: 'POST',\n            body: JSON.stringify({ time })\n        });\n    }\n}"
      pitfalls:
      - Browser compatibility
      - Memory leaks on unmount
      - Bandwidth estimation
      concepts:
      - Adaptive bitrate
      - Media APIs
      - Analytics
      estimated_hours: 11-20
  virtual-memory-sim:
    id: virtual-memory-sim
    name: Virtual Memory Simulator
    description: Simulate virtual memory with page tables and TLB. Learn memory management and address translation.
    difficulty: advanced
    estimated_hours: 20-30
    prerequisites:
    - Memory concepts
    - Binary/hex
    - Data structures
    languages:
      recommended:
      - C
      - Rust
      - Python
      also_possible:
      - Go
      - Java
    resources:
    - type: book
      name: OSTEP - Address Translation
      url: https://pages.cs.wisc.edu/~remzi/OSTEP/vm-mechanism.pdf
    - type: book
      name: OSTEP - Paging
      url: https://pages.cs.wisc.edu/~remzi/OSTEP/vm-paging.pdf
    milestones:
    - id: 1
      name: Page Table
      description: Implement single-level page table with address translation.
      acceptance_criteria:
      - Virtual to physical translation
      - Page table entries with flags
      - Handle page faults
      - Valid/invalid bits
      hints:
        level1: Split virtual address into page number and offset. Look up frame in table.
        level2: PTE has frame number + flags (valid, dirty, accessed, protection).
        level3: "class PageTableEntry:\n    def __init__(self):\n        self.valid = False\n        self.frame_number = 0\n\
          \        self.dirty = False\n        self.accessed = False\n        self.protection = 0b111  # RWX\n\nclass PageTable:\n\
          \    def __init__(self, page_bits=12, addr_bits=32):\n        self.page_size = 1 << page_bits\n        self.offset_mask\
          \ = self.page_size - 1\n        self.page_bits = page_bits\n        self.num_pages = 1 << (addr_bits - page_bits)\n\
          \        self.entries = [PageTableEntry() for _ in range(self.num_pages)]\n    \n    def translate(self, virtual_addr,\
          \ write=False):\n        page_num = virtual_addr >> self.page_bits\n        offset = virtual_addr & self.offset_mask\n\
          \        \n        if page_num >= self.num_pages:\n            raise SegmentationFault(f'Invalid page {page_num}')\n\
          \        \n        pte = self.entries[page_num]\n        \n        if not pte.valid:\n            raise PageFault(page_num)\n\
          \        \n        # Check permissions\n        if write and not (pte.protection & 0b010):\n            raise ProtectionFault('Write\
          \ not allowed')\n        \n        # Update accessed/dirty bits\n        pte.accessed = True\n        if write:\n\
          \            pte.dirty = True\n        \n        physical_addr = (pte.frame_number << self.page_bits) | offset\n\
          \        return physical_addr\n    \n    def map_page(self, page_num, frame_num, protection=0b111):\n        pte\
          \ = self.entries[page_num]\n        pte.valid = True\n        pte.frame_number = frame_num\n        pte.protection\
          \ = protection\n        pte.dirty = False\n        pte.accessed = False"
      pitfalls:
      - Off-by-one in bit shifting
      - Forgetting offset
      - Protection check order
      concepts:
      - Address translation
      - Page tables
      - Memory protection
      estimated_hours: 4-6
    - id: 2
      name: TLB
      description: Add Translation Lookaside Buffer for faster translations.
      acceptance_criteria:
      - TLB lookup before page table
      - TLB miss handling
      - TLB eviction (LRU/random)
      - TLB flush on context switch
      hints:
        level1: TLB is small cache of recent translations. Check TLB first.
        level2: On miss, walk page table and add to TLB. Track LRU for eviction.
        level3: "from collections import OrderedDict\n\nclass TLB:\n    def __init__(self, size=64):\n        self.size =\
          \ size\n        self.entries = OrderedDict()  # page_num -> (frame_num, protection)\n        self.hits = 0\n   \
          \     self.misses = 0\n    \n    def lookup(self, page_num):\n        if page_num in self.entries:\n           \
          \ self.hits += 1\n            # Move to end (most recently used)\n            self.entries.move_to_end(page_num)\n\
          \            return self.entries[page_num]\n        self.misses += 1\n        return None\n    \n    def insert(self,\
          \ page_num, frame_num, protection):\n        if page_num in self.entries:\n            del self.entries[page_num]\n\
          \        elif len(self.entries) >= self.size:\n            # Evict LRU (first item)\n            self.entries.popitem(last=False)\n\
          \        self.entries[page_num] = (frame_num, protection)\n    \n    def invalidate(self, page_num):\n        if\
          \ page_num in self.entries:\n            del self.entries[page_num]\n    \n    def flush(self):\n        self.entries.clear()\n\
          \    \n    def hit_rate(self):\n        total = self.hits + self.misses\n        return self.hits / total if total\
          \ > 0 else 0\n\nclass MMU:\n    def __init__(self, page_table):\n        self.page_table = page_table\n        self.tlb\
          \ = TLB()\n    \n    def translate(self, virtual_addr, write=False):\n        page_num = virtual_addr >> self.page_table.page_bits\n\
          \        offset = virtual_addr & self.page_table.offset_mask\n        \n        # TLB lookup\n        tlb_entry\
          \ = self.tlb.lookup(page_num)\n        if tlb_entry:\n            frame_num, protection = tlb_entry\n          \
          \  if write and not (protection & 0b010):\n                raise ProtectionFault('Write not allowed')\n        \
          \    return (frame_num << self.page_table.page_bits) | offset\n        \n        # TLB miss - walk page table\n\
          \        physical = self.page_table.translate(virtual_addr, write)\n        \n        # Add to TLB\n        pte\
          \ = self.page_table.entries[page_num]\n        self.tlb.insert(page_num, pte.frame_number, pte.protection)\n   \
          \     \n        return physical"
      pitfalls:
      - TLB coherency with page table
      - Context switch handling
      - ASID management
      concepts:
      - Caching
      - Locality
      - TLB shootdown
      estimated_hours: 4-6
    - id: 3
      name: Multi-level Page Tables
      description: Implement hierarchical page tables to save memory.
      acceptance_criteria:
      - Two or three-level tables
      - Sparse address space handling
      - On-demand table allocation
      - Page table walks
      hints:
        level1: Split page number into multiple indices. Each level points to next.
        level2: Only allocate tables for used regions. Check valid bit at each level.
        level3: "class MultiLevelPageTable:\n    def __init__(self, levels=2, bits_per_level=10, offset_bits=12):\n      \
          \  self.levels = levels\n        self.bits_per_level = bits_per_level\n        self.offset_bits = offset_bits\n\
          \        self.entries_per_table = 1 << bits_per_level\n        self.offset_mask = (1 << offset_bits) - 1\n     \
          \   \n        # Root table (always allocated)\n        self.root = self._new_table()\n        self.tables_allocated\
          \ = 1\n    \n    def _new_table(self):\n        return [None] * self.entries_per_table\n    \n    def _extract_indices(self,\
          \ virtual_addr):\n        indices = []\n        addr = virtual_addr >> self.offset_bits\n        for _ in range(self.levels):\n\
          \            indices.append(addr & (self.entries_per_table - 1))\n            addr >>= self.bits_per_level\n   \
          \     return list(reversed(indices))\n    \n    def translate(self, virtual_addr):\n        indices = self._extract_indices(virtual_addr)\n\
          \        offset = virtual_addr & self.offset_mask\n        \n        table = self.root\n        for i, idx in enumerate(indices[:-1]):\n\
          \            entry = table[idx]\n            if entry is None:\n                raise PageFault(virtual_addr)\n\
          \            table = entry  # Next level table\n        \n        # Last level is the PTE\n        pte = table[indices[-1]]\n\
          \        if pte is None or not pte.valid:\n            raise PageFault(virtual_addr)\n        \n        return (pte.frame_number\
          \ << self.offset_bits) | offset\n    \n    def map_page(self, virtual_addr, frame_num):\n        indices = self._extract_indices(virtual_addr)\n\
          \        \n        table = self.root\n        for i, idx in enumerate(indices[:-1]):\n            if table[idx]\
          \ is None:\n                table[idx] = self._new_table()\n                self.tables_allocated += 1\n       \
          \     table = table[idx]\n        \n        # Create PTE at leaf\n        pte = PageTableEntry()\n        pte.valid\
          \ = True\n        pte.frame_number = frame_num\n        table[indices[-1]] = pte"
      pitfalls:
      - Index extraction order
      - Table pointer vs PTE confusion
      - Memory overhead calculation
      concepts:
      - Hierarchical structures
      - Sparse data
      - Memory efficiency
      estimated_hours: 5-8
    - id: 4
      name: Page Replacement
      description: Implement page replacement algorithms when memory is full.
      acceptance_criteria:
      - FIFO replacement
      - LRU replacement
      - Clock algorithm
      - Track working set
      hints:
        level1: 'FIFO: queue of page numbers, evict oldest. LRU: track last access time.'
        level2: 'Clock: circular list with reference bits. Give second chance.'
        level3: "class ClockPageReplacement:\n    def __init__(self, num_frames):\n        self.num_frames = num_frames\n\
          \        self.frames = [None] * num_frames  # page_num in each frame\n        self.reference_bits = [False] * num_frames\n\
          \        self.clock_hand = 0\n        self.page_to_frame = {}  # page_num -> frame_num\n    \n    def access(self,\
          \ page_num):\n        if page_num in self.page_to_frame:\n            # Page hit - set reference bit\n         \
          \   frame = self.page_to_frame[page_num]\n            self.reference_bits[frame] = True\n            return frame,\
          \ None  # No eviction\n        \n        # Page fault - find frame to use\n        frame, evicted = self._find_victim()\n\
          \        \n        # Update mappings\n        if evicted is not None:\n            del self.page_to_frame[evicted]\n\
          \        \n        self.frames[frame] = page_num\n        self.reference_bits[frame] = True\n        self.page_to_frame[page_num]\
          \ = frame\n        \n        return frame, evicted\n    \n    def _find_victim(self):\n        # Check for empty\
          \ frame first\n        for i in range(self.num_frames):\n            if self.frames[i] is None:\n              \
          \  return i, None\n        \n        # Clock algorithm - find victim\n        while True:\n            if not self.reference_bits[self.clock_hand]:\n\
          \                # Found victim\n                evicted = self.frames[self.clock_hand]\n                frame =\
          \ self.clock_hand\n                self.clock_hand = (self.clock_hand + 1) % self.num_frames\n                return\
          \ frame, evicted\n            \n            # Give second chance\n            self.reference_bits[self.clock_hand]\
          \ = False\n            self.clock_hand = (self.clock_hand + 1) % self.num_frames"
      pitfalls:
      - Belady's anomaly with FIFO
      - Dirty page handling
      - Thrashing
      concepts:
      - Page replacement
      - Working set
      - Thrashing
      estimated_hours: 7-10
  wal-impl:
    id: wal-impl
    name: WAL Implementation
    description: Implement Write-Ahead Logging for durability. Learn crash recovery and log-structured storage.
    difficulty: advanced
    estimated_hours: 15-25
    prerequisites:
    - File I/O
    - Database basics
    - Crash recovery concepts
    languages:
      recommended:
      - Rust
      - Go
      - C
      also_possible:
      - Python
      - Java
    resources:
    - name: ARIES Paper
      url: https://cs.stanford.edu/people/chr101/cs345/aries.pdf
      type: paper
    - name: SQLite WAL
      url: https://sqlite.org/wal.html
      type: documentation
    milestones:
    - id: 1
      name: Log Record Format
      description: Design and implement log record structure.
      acceptance_criteria:
      - LSN (Log Sequence Number)
      - Transaction ID
      - Operation type (INSERT, UPDATE, DELETE)
      - Before/after images for undo/redo
      hints:
        level1: Log record = LSN + TxID + Type + Data. Append-only file.
        level2: Include enough info to redo OR undo the operation.
        level3: "import struct\nfrom enum import IntEnum\n\nclass LogType(IntEnum):\n    BEGIN = 1\n    COMMIT = 2\n    ABORT\
          \ = 3\n    INSERT = 4\n    UPDATE = 5\n    DELETE = 6\n    CHECKPOINT = 7\n\nclass LogRecord:\n    def __init__(self,\
          \ lsn, tx_id, log_type, table=None, key=None, \n                 before_value=None, after_value=None):\n       \
          \ self.lsn = lsn\n        self.tx_id = tx_id\n        self.log_type = log_type\n        self.table = table\n   \
          \     self.key = key\n        self.before_value = before_value  # For undo\n        self.after_value = after_value\
          \    # For redo\n    \n    def serialize(self):\n        # Format: LSN(8) + TxID(4) + Type(1) + DataLen(4) + Data\n\
          \        data = b''\n        if self.table:\n            data += self.table.encode() + b'\\x00'\n        if self.key\
          \ is not None:\n            data += struct.pack('!I', self.key)\n        if self.before_value is not None:\n   \
          \         before_bytes = self.before_value.encode() if isinstance(self.before_value, str) else self.before_value\n\
          \            data += struct.pack('!I', len(before_bytes)) + before_bytes\n        if self.after_value is not None:\n\
          \            after_bytes = self.after_value.encode() if isinstance(self.after_value, str) else self.after_value\n\
          \            data += struct.pack('!I', len(after_bytes)) + after_bytes\n        \n        header = struct.pack('!QIB\
          \ I', self.lsn, self.tx_id, self.log_type, len(data))\n        return header + data"
      pitfalls:
      - Variable-length fields
      - Endianness
      - Corruption detection
      concepts:
      - Log records
      - LSN
      - Undo/redo logging
      estimated_hours: 3-4
    - id: 2
      name: Log Writer
      description: Implement append-only log file with fsync.
      acceptance_criteria:
      - Append records atomically
      - Force log to disk (fsync)
      - Handle concurrent writers
      - Log file rotation
      hints:
        level1: Append to log file, fsync before acknowledging commit.
        level2: 'Group commit: batch multiple commits into one fsync.'
        level3: "import os\nimport threading\n\nclass WALWriter:\n    def __init__(self, log_path):\n        self.log_path\
          \ = log_path\n        self.log_file = open(log_path, 'ab')\n        self.lock = threading.Lock()\n        self.current_lsn\
          \ = self._recover_lsn()\n        self.buffer = bytearray()\n        self.pending_commits = []\n    \n    def _recover_lsn(self):\n\
          \        # Read last LSN from log file\n        try:\n            size = os.path.getsize(self.log_path)\n      \
          \      if size == 0:\n                return 0\n            # Read last record's LSN\n            # ... implementation\
          \ ...\n        except FileNotFoundError:\n            return 0\n    \n    def append(self, record):\n        with\
          \ self.lock:\n            record.lsn = self.current_lsn\n            self.current_lsn += 1\n            \n     \
          \       data = record.serialize()\n            self.buffer.extend(data)\n            \n            if record.log_type\
          \ == LogType.COMMIT:\n                self.pending_commits.append(record.tx_id)\n            \n            return\
          \ record.lsn\n    \n    def flush(self):\n        '''Force buffered records to disk'''\n        with self.lock:\n\
          \            if self.buffer:\n                self.log_file.write(bytes(self.buffer))\n                self.log_file.flush()\n\
          \                os.fsync(self.log_file.fileno())\n                self.buffer.clear()\n                committed\
          \ = self.pending_commits.copy()\n                self.pending_commits.clear()\n                return committed\n\
          \            return []"
      pitfalls:
      - Partial writes
      - fsync semantics
      - Torn pages
      concepts:
      - Durability
      - fsync
      - Group commit
      estimated_hours: 4-5
    - id: 3
      name: Crash Recovery
      description: Implement ARIES-style recovery (redo then undo).
      acceptance_criteria:
      - Scan log to find transactions
      - Redo all committed changes
      - Undo incomplete transactions
      - Track active transactions
      hints:
        level1: 'Recovery: 1) Analysis - find active txs, 2) Redo - replay, 3) Undo - rollback.'
        level2: 'Redo: apply all operations. Undo: reverse uncommitted ops.'
        level3: "class WALRecovery:\n    def recover(self, wal_reader, database):\n        # Phase 1: Analysis\n        active_txs\
          \ = set()\n        committed_txs = set()\n        \n        for record in wal_reader.scan():\n            if record.log_type\
          \ == LogType.BEGIN:\n                active_txs.add(record.tx_id)\n            elif record.log_type == LogType.COMMIT:\n\
          \                active_txs.discard(record.tx_id)\n                committed_txs.add(record.tx_id)\n           \
          \ elif record.log_type == LogType.ABORT:\n                active_txs.discard(record.tx_id)\n        \n        #\
          \ Phase 2: Redo (all operations from committed transactions)\n        for record in wal_reader.scan():\n       \
          \     if record.tx_id in committed_txs:\n                if record.log_type == LogType.INSERT:\n               \
          \     database.insert(record.table, record.key, record.after_value)\n                elif record.log_type == LogType.UPDATE:\n\
          \                    database.update(record.table, record.key, record.after_value)\n                elif record.log_type\
          \ == LogType.DELETE:\n                    database.delete(record.table, record.key)\n        \n        # Phase 3:\
          \ Undo (reverse uncommitted transactions)\n        # Scan backwards\n        for record in wal_reader.scan_reverse():\n\
          \            if record.tx_id in active_txs:\n                if record.log_type == LogType.INSERT:\n           \
          \         database.delete(record.table, record.key)\n                elif record.log_type == LogType.UPDATE:\n \
          \                   database.update(record.table, record.key, record.before_value)\n                elif record.log_type\
          \ == LogType.DELETE:\n                    database.insert(record.table, record.key, record.before_value)\n     \
          \   \n        # Log abort records for incomplete transactions\n        for tx_id in active_txs:\n            wal_writer.append(LogRecord(0,\
          \ tx_id, LogType.ABORT))"
      pitfalls:
      - Idempotent operations
      - Log corruption
      - Partial recovery
      concepts:
      - ARIES recovery
      - Redo/undo
      - Crash consistency
      estimated_hours: 5-7
    - id: 4
      name: Checkpointing
      description: Implement checkpoints to speed up recovery.
      acceptance_criteria:
      - Fuzzy checkpoints (non-blocking)
      - Record dirty pages/active txs
      - Truncate old log entries
      - Restart from checkpoint
      hints:
        level1: Checkpoint records state so recovery starts from there.
        level2: 'Fuzzy checkpoint: don''t block, just record dirty pages.'
        level3: "class Checkpoint:\n    def __init__(self, lsn, active_txs, dirty_pages):\n        self.lsn = lsn\n      \
          \  self.active_txs = active_txs  # {tx_id: first_lsn}\n        self.dirty_pages = dirty_pages  # {page_id: recovery_lsn}\n\
          \nclass WALManager:\n    def create_checkpoint(self):\n        with self.lock:\n            # Get current state\n\
          \            active_txs = dict(self.transaction_table)\n            dirty_pages = dict(self.dirty_page_table)\n\
          \            \n            # Write checkpoint begin\n            begin_record = LogRecord(0, 0, LogType.CHECKPOINT_BEGIN)\n\
          \            self.wal_writer.append(begin_record)\n            \n            # Flush dirty pages in background\n\
          \            # (fuzzy - don't wait)\n            \n            # Write checkpoint end with state\n            checkpoint\
          \ = Checkpoint(\n                begin_record.lsn,\n                active_txs,\n                dirty_pages\n \
          \           )\n            end_record = LogRecord(0, 0, LogType.CHECKPOINT_END,\n                              \
          \      data=checkpoint.serialize())\n            self.wal_writer.append(end_record)\n            self.wal_writer.flush()\n\
          \            \n            # Update master record\n            self._write_master_record(begin_record.lsn)\n   \
          \ \n    def recover_from_checkpoint(self):\n        # Read master record to find checkpoint\n        checkpoint_lsn\
          \ = self._read_master_record()\n        \n        if checkpoint_lsn:\n            # Start analysis from checkpoint\n\
          \            checkpoint = self._read_checkpoint(checkpoint_lsn)\n            self.transaction_table = checkpoint.active_txs\n\
          \            self.dirty_page_table = checkpoint.dirty_pages\n            start_lsn = checkpoint_lsn\n        else:\n\
          \            start_lsn = 0\n        \n        # Continue with normal recovery from start_lsn\n        self._redo_phase(start_lsn)\n\
          \        self._undo_phase()"
      pitfalls:
      - Checkpoint consistency
      - Log truncation timing
      - Master record
      concepts:
      - Checkpointing
      - Fuzzy checkpoints
      - Log truncation
      estimated_hours: 4-5
  wasm-emitter:
    id: wasm-emitter
    name: WebAssembly Emitter
    description: Compile a simple language to WebAssembly binary. Learn WASM format and code generation.
    difficulty: advanced
    estimated_hours: 25-40
    prerequisites:
    - AST building
    - Binary formats
    - Stack machines
    languages:
      recommended:
      - Rust
      - Go
      - TypeScript
      also_possible:
      - Python
      - C
    resources:
    - type: spec
      name: WebAssembly Specification
      url: https://webassembly.github.io/spec/core/
    - type: tool
      name: WebAssembly Binary Toolkit
      url: https://github.com/WebAssembly/wabt
    milestones:
    - id: 1
      name: WASM Binary Format
      description: Understand and emit valid WASM module structure.
      acceptance_criteria:
      - Magic number and version
      - Section encoding
      - LEB128 integers
      - Type section
      hints:
        level1: WASM starts with magic (0x00 0x61 0x73 0x6d) + version (0x01 0x00 0x00 0x00).
        level2: Sections have ID, size (LEB128), content. Type section defines function signatures.
        level3: "def leb128_unsigned(n):\n    result = []\n    while True:\n        byte = n & 0x7f\n        n >>= 7\n   \
          \     if n != 0:\n            byte |= 0x80\n        result.append(byte)\n        if n == 0:\n            break\n\
          \    return bytes(result)\n\ndef leb128_signed(n):\n    result = []\n    while True:\n        byte = n & 0x7f\n\
          \        n >>= 7\n        if (n == 0 and byte & 0x40 == 0) or (n == -1 and byte & 0x40):\n            result.append(byte)\n\
          \            break\n        result.append(byte | 0x80)\n    return bytes(result)\n\nclass WasmModule:\n    MAGIC\
          \ = b'\\x00asm'\n    VERSION = b'\\x01\\x00\\x00\\x00'\n    \n    # Section IDs\n    TYPE_SECTION = 1\n    FUNC_SECTION\
          \ = 3\n    EXPORT_SECTION = 7\n    CODE_SECTION = 10\n    \n    def __init__(self):\n        self.types = []   \
          \   # Function signatures\n        self.functions = []  # Function type indices\n        self.exports = []    #\
          \ Exported items\n        self.code = []       # Function bodies\n    \n    def add_function_type(self, params,\
          \ results):\n        # params/results are lists of value types (0x7f=i32, 0x7e=i64, etc.)\n        sig = bytes([0x60,\
          \ len(params)] + params + [len(results)] + results)\n        if sig not in self.types:\n            self.types.append(sig)\n\
          \        return self.types.index(sig)\n    \n    def encode(self):\n        output = self.MAGIC + self.VERSION\n\
          \        \n        # Type section\n        if self.types:\n            content = bytes([len(self.types)]) + b''.join(self.types)\n\
          \            output += self._section(self.TYPE_SECTION, content)\n        \n        # ... other sections\n     \
          \   return output\n    \n    def _section(self, id, content):\n        return bytes([id]) + leb128_unsigned(len(content))\
          \ + content"
      pitfalls:
      - LEB128 edge cases
      - Section ordering
      - Size calculation
      concepts:
      - Binary formats
      - Variable-length encoding
      - Module structure
      estimated_hours: 5-8
    - id: 2
      name: Expression Compilation
      description: Compile arithmetic expressions to WASM instructions.
      acceptance_criteria:
      - i32 operations
      - Stack-based code generation
      - Local variables
      - Constants
      hints:
        level1: WASM is stack-based. i32.const pushes, i32.add pops 2 pushes 1.
        level2: local.get/set for variables. Locals declared at function start.
        level3: "# WASM opcodes\nI32_CONST = 0x41\nI32_ADD = 0x6a\nI32_SUB = 0x6b\nI32_MUL = 0x6c\nI32_DIV_S = 0x6d\nLOCAL_GET\
          \ = 0x20\nLOCAL_SET = 0x21\nLOCAL_TEE = 0x22  # Set and keep on stack\n\nclass CodeGen:\n    def __init__(self):\n\
          \        self.code = []\n        self.locals = {}  # name -> index\n        self.local_types = []  # type of each\
          \ local\n    \n    def compile_expr(self, node):\n        if isinstance(node, NumberLit):\n            self.code.append(I32_CONST)\n\
          \            self.code.extend(leb128_signed(node.value))\n        \n        elif isinstance(node, BinaryOp):\n \
          \           self.compile_expr(node.left)\n            self.compile_expr(node.right)\n            ops = {'+': I32_ADD,\
          \ '-': I32_SUB, '*': I32_MUL, '/': I32_DIV_S}\n            self.code.append(ops[node.op])\n        \n        elif\
          \ isinstance(node, Variable):\n            idx = self.locals[node.name]\n            self.code.append(LOCAL_GET)\n\
          \            self.code.extend(leb128_unsigned(idx))\n        \n        elif isinstance(node, Assignment):\n    \
          \        self.compile_expr(node.value)\n            idx = self.locals[node.name]\n            self.code.append(LOCAL_TEE)\
          \  # Keep value on stack\n            self.code.extend(leb128_unsigned(idx))\n    \n    def declare_local(self,\
          \ name, type=0x7f):  # 0x7f = i32\n        idx = len(self.locals)\n        self.locals[name] = idx\n        self.local_types.append(type)\n\
          \        return idx"
      pitfalls:
      - Operand order for non-commutative ops
      - Signed vs unsigned
      - Stack imbalance
      concepts:
      - Stack machines
      - Instruction encoding
      - Local variables
      estimated_hours: 5-8
    - id: 3
      name: Control Flow
      description: Compile if/else and loops to WASM structured control flow.
      acceptance_criteria:
      - Block/end structure
      - If/else/end
      - Loop/br_if
      - Break to labels
      hints:
        level1: 'WASM has structured control: block/loop/if all need end. br jumps to enclosing block.'
        level2: 'br 0 exits innermost block. loop: br goes back to start. block: br goes to end.'
        level3: "BLOCK = 0x02\nLOOP = 0x03\nIF = 0x04\nELSE = 0x05\nEND = 0x0b\nBR = 0x0c\nBR_IF = 0x0d\nRETURN = 0x0f\nVOID\
          \ = 0x40  # Empty block type\n\nclass CodeGen:\n    def __init__(self):\n        self.code = []\n        self.block_depth\
          \ = 0\n    \n    def compile_if(self, node):\n        # Condition leaves i32 on stack\n        self.compile_expr(node.condition)\n\
          \        \n        self.code.append(IF)\n        self.code.append(VOID)  # No result type\n        self.block_depth\
          \ += 1\n        \n        self.compile_stmt(node.then_branch)\n        \n        if node.else_branch:\n        \
          \    self.code.append(ELSE)\n            self.compile_stmt(node.else_branch)\n        \n        self.code.append(END)\n\
          \        self.block_depth -= 1\n    \n    def compile_while(self, node):\n        # block { loop { if !cond br 1;\
          \ body; br 0 } }\n        self.code.append(BLOCK)\n        self.code.append(VOID)\n        self.block_depth += 1\n\
          \        \n        self.code.append(LOOP)\n        self.code.append(VOID)\n        self.block_depth += 1\n     \
          \   \n        # Check condition, break if false\n        self.compile_expr(node.condition)\n        self.code.append(I32_EQZ)\
          \  # Invert\n        self.code.append(BR_IF)\n        self.code.extend(leb128_unsigned(1))  # Break to outer block\n\
          \        \n        # Body\n        self.compile_stmt(node.body)\n        \n        # Loop back\n        self.code.append(BR)\n\
          \        self.code.extend(leb128_unsigned(0))  # Back to loop start\n        \n        self.code.append(END)  #\
          \ End loop\n        self.block_depth -= 1\n        self.code.append(END)  # End block\n        self.block_depth\
          \ -= 1"
      pitfalls:
      - Label depth calculation
      - Block type annotations
      - Unreachable code after br
      concepts:
      - Structured control flow
      - Label indices
      - Block nesting
      estimated_hours: 6-10
    - id: 4
      name: Functions and Exports
      description: Compile function definitions and export them.
      acceptance_criteria:
      - Function section
      - Code section format
      - Export section
      - Function calls
      hints:
        level1: 'Separate sections: Type (signatures), Function (type refs), Code (bodies), Export.'
        level2: Code section encodes local counts then instructions. Functions called by index.
        level3: "CALL = 0x10\n\nclass WasmCompiler:\n    def __init__(self):\n        self.module = WasmModule()\n       \
          \ self.function_indices = {}  # name -> index\n    \n    def compile_function(self, func):\n        # Get/create\
          \ type signature\n        param_types = [0x7f] * len(func.params)  # All i32\n        result_types = [0x7f] if func.returns\
          \ else []\n        type_idx = self.module.add_function_type(param_types, result_types)\n        \n        # Register\
          \ function\n        func_idx = len(self.module.functions)\n        self.module.functions.append(type_idx)\n    \
          \    self.function_indices[func.name] = func_idx\n        \n        # Compile body\n        codegen = CodeGen()\n\
          \        for i, param in enumerate(func.params):\n            codegen.locals[param] = i\n        \n        for stmt\
          \ in func.body:\n            codegen.compile_stmt(stmt)\n        \n        codegen.code.append(END)\n        \n\
          \        # Encode function body\n        # Local declarations: count of (count, type) pairs\n        if codegen.local_types:\n\
          \            # Group consecutive same types\n            local_decls = self._group_locals(codegen.local_types[len(func.params):])\n\
          \        else:\n            local_decls = []\n        \n        body = bytes([len(local_decls)])\n        for count,\
          \ type in local_decls:\n            body += leb128_unsigned(count) + bytes([type])\n        body += bytes(codegen.code)\n\
          \        \n        # Wrap with size\n        self.module.code.append(leb128_unsigned(len(body)) + body)\n      \
          \  \n        return func_idx\n    \n    def export_function(self, name, func_name):\n        idx = self.function_indices[func_name]\n\
          \        # name as UTF-8 string + kind (0=func) + index\n        name_bytes = name.encode('utf-8')\n        self.module.exports.append(\n\
          \            leb128_unsigned(len(name_bytes)) + name_bytes + bytes([0]) + leb128_unsigned(idx)\n        )"
      pitfalls:
      - Parameter vs local indices
      - Body size encoding
      - Export name encoding
      concepts:
      - Module linking
      - Function ABI
      - Export mechanisms
      estimated_hours: 9-14
  wc-clone:
    id: wc-clone
    name: Wc Clone
    description: Build your own word count utility. Learn text processing, counting algorithms, and Unix conventions.
    difficulty: beginner
    estimated_hours: 4-6
    prerequisites:
    - Basic programming
    - File I/O
    languages:
      recommended:
      - C
      - Rust
      - Go
      also_possible:
      - Python
      - Zig
    resources:
    - name: Wc Man Page
      url: https://man7.org/linux/man-pages/man1/wc.1.html
      type: documentation
    milestones:
    - id: 1
      name: Basic Counting
      description: Count lines, words, and bytes in a file.
      acceptance_criteria:
      - Count newline characters (lines)
      - Count words (whitespace-separated)
      - Count total bytes
      - Output in standard wc format
      hints:
        level1: Word = sequence of non-whitespace chars. Newline = \n.
        level2: Track 'in_word' state to count word transitions.
        level3: "int lines=0, words=0, bytes=0, in_word=0;\nint c;\nwhile ((c = fgetc(f)) != EOF) {\n    bytes++;\n    if\
          \ (c == '\\n') lines++;\n    if (isspace(c)) {\n        in_word = 0;\n    } else if (!in_word) {\n        in_word\
          \ = 1;\n        words++;\n    }\n}\nprintf(\"%7d %7d %7d\\n\", lines, words, bytes);"
      pitfalls:
      - Off-by-one in word counting
      - Files without trailing newline
      - Different whitespace characters
      concepts:
      - State machines
      - Text parsing
      - Whitespace handling
      estimated_hours: 1-2
    - id: 2
      name: Options and Multiple Files
      description: Add flag options and handle multiple files.
      acceptance_criteria:
      - -l for lines only
      - -w for words only
      - -c for bytes only
      - -m for character count (UTF-8)
      - Total line for multiple files
      hints:
        level1: No flags = show all three counts.
        level2: Character count differs from byte count for UTF-8.
        level3: "// For UTF-8 character counting:\nint chars = 0;\nwhile ((c = fgetc(f)) != EOF) {\n    // Count start bytes,\
          \ not continuation bytes\n    if ((c & 0xC0) != 0x80) chars++;\n}"
      pitfalls:
      - UTF-8 multi-byte characters
      - Showing totals only with multiple files
      - Flag combination logic
      concepts:
      - UTF-8 encoding
      - Option handling
      - Output formatting
      estimated_hours: 2-3
    - id: 3
      name: Standard Input Support
      description: Read from stdin when no file is specified, like real wc.
      acceptance_criteria:
      - Read from stdin when no args
      - 'Support piping: cat file | wc'
      - 'Mix stdin with files: wc - file1 file2'
      - Dash '-' means stdin explicitly
      - Handle binary input gracefully
      hints:
        level1: If argc == 1 or filename is '-', read from stdin instead of file.
        level2: stdin doesn't have a filename to print - use empty or no label.
        level3: "// Handle both files and stdin\nint count_stream(FILE *f, const char *name, Counts *totals) {\n    Counts\
          \ c = {0, 0, 0, 0};\n    int in_word = 0;\n    int ch;\n\n    while ((ch = fgetc(f)) != EOF) {\n        c.bytes++;\n\
          \        if (ch == '\\n') c.lines++;\n        if ((unsigned char)ch < 128 || ((ch & 0xC0) != 0x80)) {\n        \
          \    c.chars++;  // Start of UTF-8 char\n        }\n        if (isspace(ch)) {\n            in_word = 0;\n     \
          \   } else if (!in_word) {\n            in_word = 1;\n            c.words++;\n        }\n    }\n\n    print_counts(&c,\
          \ name);\n\n    totals->lines += c.lines;\n    totals->words += c.words;\n    totals->bytes += c.bytes;\n    totals->chars\
          \ += c.chars;\n\n    return 0;\n}\n\nint main(int argc, char **argv) {\n    // Parse options (skip for brevity)\n\
          \    int optind = parse_options(argc, argv);\n\n    Counts totals = {0};\n    int num_files = argc - optind;\n\n\
          \    if (num_files == 0) {\n        // No files - read stdin\n        count_stream(stdin, NULL, &totals);\n    }\
          \ else {\n        for (int i = optind; i < argc; i++) {\n            if (strcmp(argv[i], \"-\") == 0) {\n      \
          \          count_stream(stdin, NULL, &totals);\n            } else {\n                FILE *f = fopen(argv[i], \"\
          rb\");\n                if (!f) {\n                    fprintf(stderr, \"wc: %s: No such file\\n\", argv[i]);\n\
          \                    continue;\n                }\n                count_stream(f, argv[i], &totals);\n        \
          \        fclose(f);\n            }\n        }\n\n        if (num_files > 1) {\n            print_counts(&totals,\
          \ \"total\");\n        }\n    }\n\n    return 0;\n}"
      pitfalls:
      - stdin can only be read once
      - Binary data in stdin (null bytes)
      - Blocking on empty stdin
      - Mixing stdin position with files
      concepts:
      - Standard streams
      - Unix filter pattern
      - Piping and redirection
      - Special filename conventions
      estimated_hours: 1-2
    - id: 4
      name: Max Line Length (-L)
      description: Add the -L option to report the maximum line length.
      acceptance_criteria:
      - -L flag shows max line length only
      - Combine with other flags
      - Handle lines without newline (last line)
      - 'UTF-8 aware: count display width, not bytes'
      - Tab expansion (optional)
      hints:
        level1: Track current line length and max seen. Reset on newline.
        level2: For UTF-8, count codepoints not bytes. Tabs count as 8-(col%8).
        level3: "// UTF-8 aware line length counting\nint max_line_length(FILE *f) {\n    int max_len = 0;\n    int cur_len\
          \ = 0;\n    int ch;\n\n    while ((ch = fgetc(f)) != EOF) {\n        if (ch == '\\n') {\n            if (cur_len\
          \ > max_len) max_len = cur_len;\n            cur_len = 0;\n        } else if (ch == '\\t') {\n            // Tab\
          \ expands to next 8-column boundary\n            cur_len = ((cur_len / 8) + 1) * 8;\n        } else if ((ch & 0xC0)\
          \ != 0x80) {\n            // Start of UTF-8 char (or ASCII)\n            // Could use wcwidth() for proper display\
          \ width\n            cur_len++;\n        }\n        // Continuation bytes (10xxxxxx) don't add to length\n    }\n\
          \n    // Handle last line without newline\n    if (cur_len > max_len) max_len = cur_len;\n\n    return max_len;\n\
          }\n\n// More accurate with wcwidth for CJK chars (double-width)\n#include <wchar.h>\n#include <locale.h>\n\nint\
          \ display_width(FILE *f) {\n    setlocale(LC_ALL, \"\");  // Enable locale for wcwidth\n\n    int max_len = 0;\n\
          \    int cur_len = 0;\n    wint_t wc;\n\n    while ((wc = fgetwc(f)) != WEOF) {\n        if (wc == L'\\n') {\n \
          \           if (cur_len > max_len) max_len = cur_len;\n            cur_len = 0;\n        } else if (wc == L'\\t')\
          \ {\n            cur_len = ((cur_len / 8) + 1) * 8;\n        } else {\n            int w = wcwidth(wc);\n      \
          \      if (w > 0) cur_len += w;  // wcwidth returns -1 for non-printable\n        }\n    }\n\n    if (cur_len >\
          \ max_len) max_len = cur_len;\n    return max_len;\n}"
      pitfalls:
      - Forgetting last line without newline
      - UTF-8 vs display width (CJK is double-width)
      - Control characters and escape sequences
      - Very long lines causing overflow
      concepts:
      - Display width vs byte length
      - Tab expansion
      - wcwidth() for Unicode
      - Terminal column counting
      estimated_hours: 1-2
  weather-app:
    id: weather-app
    name: Weather App
    description: Build a weather application that fetches real-time weather data from an API. Learn about async JavaScript,
      API consumption, and handling loading/error states.
    difficulty: beginner
    estimated_hours: 10-15
    prerequisites:
    - HTML/CSS basics
    - JavaScript fundamentals
    - Understanding of async/await
    languages:
      recommended:
      - JavaScript
      - TypeScript
      also_possible:
      - Python
      - React
      - Vue
    resources:
    - name: OpenWeatherMap API
      url: https://openweathermap.org/api
      type: documentation
    - name: Build a Weather App with Vanilla JS
      url: https://www.youtube.com/watch?v=WZNG8UomjSI
      type: video
    milestones:
    - id: 1
      name: API Setup & Basic Fetch
      description: Set up API key and make basic weather API calls.
      acceptance_criteria:
      - Sign up for OpenWeatherMap API key
      - Successfully fetch weather data for a hardcoded city
      - Log weather data to console
      - Handle API errors gracefully
      hints:
        level1: Use fetch() with your API key as a query parameter.
        level2: Store API key in a config file, not hardcoded in fetch URL.
        level3: "const API_KEY = 'your_api_key';\nasync function getWeather(city) {\n  const response = await fetch(\n   \
          \ `https://api.openweathermap.org/data/2.5/weather?q=${city}&appid=${API_KEY}&units=metric`\n  );\n  if (!response.ok)\
          \ throw new Error('City not found');\n  return response.json();\n}"
      pitfalls:
      - Exposing API key in client-side code (use env vars in production)
      - Not handling network errors
      - Forgetting units parameter (default is Kelvin)
      concepts:
      - REST APIs
      - Fetch API
      - Async/await
      - Error handling
      estimated_hours: 2-3
    - id: 2
      name: Search Functionality
      description: Allow users to search for weather by city name.
      acceptance_criteria:
      - Search input field for city name
      - Search triggers on Enter or button click
      - Loading indicator while fetching
      - Error message for invalid city
      hints:
        level1: Add event listener for form submit, get input value, call API.
        level2: Show a loading spinner while waiting for API response.
        level3: "searchForm.addEventListener('submit', async (e) => {\n  e.preventDefault();\n  const city = cityInput.value.trim();\n\
          \  if (!city) return;\n  showLoading();\n  try {\n    const data = await getWeather(city);\n    displayWeather(data);\n\
          \  } catch (error) {\n    showError(error.message);\n  } finally {\n    hideLoading();\n  }\n});"
      pitfalls:
      - Not debouncing rapid searches
      - Not clearing previous errors
      - 'Accessibility: missing form labels'
      concepts:
      - Form handling
      - Loading states
      - Error states
      estimated_hours: 2-3
    - id: 3
      name: Display Weather Data
      description: Create a beautiful UI to display weather information.
      acceptance_criteria:
      - Display city name and country
      - Show current temperature
      - Show weather condition with icon
      - Display humidity, wind speed
      - Show feels-like temperature
      hints:
        level1: Map API response fields to UI elements.
        level2: Use weather icon codes from API to show appropriate icons.
        level3: "function displayWeather(data) {\n  cityEl.textContent = `${data.name}, ${data.sys.country}`;\n  tempEl.textContent\
          \ = `${Math.round(data.main.temp)}Â°C`;\n  iconEl.src = `https://openweathermap.org/img/wn/${data.weather[0].icon}@2x.png`;\n\
          \  descEl.textContent = data.weather[0].description;\n}"
      pitfalls:
      - Not handling missing data fields
      - Icon URL format changes with API version
      - Not rounding temperature values
      concepts:
      - Data mapping
      - Dynamic content
      - API response structure
      estimated_hours: 2-3
    - id: 4
      name: Geolocation & Current Location
      description: Use browser geolocation to get weather for current location.
      acceptance_criteria:
      - Button to get current location weather
      - Request geolocation permission
      - Fetch weather using lat/lon coordinates
      - Handle permission denied gracefully
      hints:
        level1: Use navigator.geolocation.getCurrentPosition() API.
        level2: API supports lat/lon params instead of city name.
        level3: "function getCurrentLocationWeather() {\n  navigator.geolocation.getCurrentPosition(\n    async (position)\
          \ => {\n      const { latitude, longitude } = position.coords;\n      const data = await getWeatherByCoords(latitude,\
          \ longitude);\n      displayWeather(data);\n    },\n    (error) => showError('Location access denied')\n  );\n}"
      pitfalls:
      - Geolocation requires HTTPS in production
      - Not handling timeout for slow GPS
      - High accuracy option drains battery
      concepts:
      - Geolocation API
      - Permissions
      - Coordinate-based queries
      estimated_hours: 2-3
  word2vec:
    id: word2vec
    name: Word Embeddings (Word2Vec)
    description: Implement Word2Vec from scratch using Skip-gram or CBOW. Learn how words become dense vectors capturing semantic
      meaning.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Neural network basics
    - Linear algebra
    - Python/NumPy
    languages:
      recommended:
      - Python
      also_possible:
      - Julia
      - C++
    resources:
    - name: Word2Vec Paper
      url: https://arxiv.org/abs/1301.3781
      type: paper
    - name: Word2Vec Tutorial
      url: https://www.tensorflow.org/tutorials/text/word2vec
      type: tutorial
    milestones:
    - id: 1
      name: Data Preprocessing
      description: Prepare text corpus for training.
      acceptance_criteria:
      - Tokenize text into words
      - Build vocabulary with word-to-index mapping
      - Create training pairs (context, target)
      - Implement subsampling of frequent words
      hints:
        level1: 'Skip-gram: given center word, predict context words.'
        level2: 'For window size 2: ''the cat sat'' -> (cat, the), (cat, sat).'
        level3: "import re\nfrom collections import Counter\n\ndef preprocess(text, min_count=5):\n    # Tokenize\n    words\
          \ = re.findall(r'\\w+', text.lower())\n    \n    # Build vocab\n    word_counts = Counter(words)\n    vocab = {w:\
          \ i for i, (w, c) in enumerate(word_counts.items()) if c >= min_count}\n    \n    # Convert to indices\n    data\
          \ = [vocab[w] for w in words if w in vocab]\n    return data, vocab\n\ndef generate_training_pairs(data, window_size=2):\n\
          \    pairs = []\n    for i, center in enumerate(data):\n        for j in range(max(0, i-window_size), min(len(data),\
          \ i+window_size+1)):\n            if i != j:\n                pairs.append((center, data[j]))\n    return pairs"
      pitfalls:
      - Vocabulary too large
      - Not removing rare words
      - Memory issues with large corpus
      concepts:
      - Tokenization
      - Vocabulary building
      - Skip-gram pairs
      estimated_hours: 3-4
    - id: 2
      name: Skip-gram Model
      description: Implement the Skip-gram neural network.
      acceptance_criteria:
      - Embedding layer (input)
      - Output layer (context prediction)
      - Softmax over vocabulary
      - Forward pass implementation
      hints:
        level1: 'Two matrices: W_in (vocab x embed_dim), W_out (embed_dim x vocab).'
        level2: 'Forward: embed = W_in[word], scores = embed @ W_out, probs = softmax(scores).'
        level3: "import numpy as np\n\nclass SkipGram:\n    def __init__(self, vocab_size, embed_dim):\n        self.W_in\
          \ = np.random.randn(vocab_size, embed_dim) * 0.01\n        self.W_out = np.random.randn(embed_dim, vocab_size) *\
          \ 0.01\n    \n    def forward(self, center_word):\n        # Get embedding\n        self.embed = self.W_in[center_word]\
          \  # (embed_dim,)\n        # Compute scores\n        self.scores = self.embed @ self.W_out  # (vocab_size,)\n  \
          \      # Softmax\n        exp_scores = np.exp(self.scores - np.max(self.scores))\n        self.probs = exp_scores\
          \ / exp_scores.sum()\n        return self.probs"
      pitfalls:
      - Softmax numerical stability
      - Wrong matrix dimensions
      - Embedding lookup indexing
      concepts:
      - Word embeddings
      - Softmax classification
      - Neural network layers
      estimated_hours: 3-4
    - id: 3
      name: Training with Negative Sampling
      description: Implement efficient training with negative sampling.
      acceptance_criteria:
      - Cross-entropy loss
      - Negative sampling instead of full softmax
      - Gradient computation
      - SGD parameter updates
      hints:
        level1: Full softmax is O(vocab_size). Negative sampling is O(k) where k ~ 5-20.
        level2: Sample negatives according to word frequency ^ 0.75.
        level3: "def negative_sampling_loss(self, center, context, neg_samples):\n    # Positive sample\n    pos_embed = self.W_in[center]\n\
          \    pos_context = self.W_out[:, context]\n    pos_score = sigmoid(pos_embed @ pos_context)\n    pos_loss = -np.log(pos_score\
          \ + 1e-10)\n    \n    # Negative samples\n    neg_loss = 0\n    for neg in neg_samples:\n        neg_context = self.W_out[:,\
          \ neg]\n        neg_score = sigmoid(-pos_embed @ neg_context)\n        neg_loss -= np.log(neg_score + 1e-10)\n \
          \   \n    return pos_loss + neg_loss\n\ndef train_step(self, center, context, neg_samples, lr=0.01):\n    # Compute\
          \ gradients and update\n    pos_embed = self.W_in[center]\n    pos_context = self.W_out[:, context]\n    \n    #\
          \ Gradient for positive\n    pos_score = sigmoid(pos_embed @ pos_context)\n    grad_pos = (pos_score - 1) * pos_context\n\
          \    \n    # Update embeddings\n    self.W_in[center] -= lr * grad_pos\n    self.W_out[:, context] -= lr * (pos_score\
          \ - 1) * pos_embed"
      pitfalls:
      - Sampling distribution
      - Gradient computation errors
      - Learning rate too high
      concepts:
      - Negative sampling
      - Cross-entropy loss
      - Stochastic gradient descent
      estimated_hours: 4-6
    - id: 4
      name: Evaluation & Visualization
      description: Evaluate embeddings and visualize results.
      acceptance_criteria:
      - Find similar words (cosine similarity)
      - 'Analogy task: king - man + woman = queen'
      - Visualize with t-SNE/PCA
      - Save and load embeddings
      hints:
        level1: Cosine similarity = dot(a, b) / (norm(a) * norm(b)).
        level2: 'Analogy: vec(king) - vec(man) + vec(woman) â‰ˆ vec(queen).'
        level3: "def most_similar(self, word, top_k=10):\n    if word not in self.vocab:\n        return []\n    word_vec\
          \ = self.W_in[self.vocab[word]]\n    word_vec = word_vec / np.linalg.norm(word_vec)\n    \n    similarities = self.W_in\
          \ @ word_vec\n    norms = np.linalg.norm(self.W_in, axis=1)\n    similarities = similarities / (norms + 1e-10)\n\
          \    \n    top_indices = similarities.argsort()[-top_k-1:-1][::-1]\n    idx_to_word = {i: w for w, i in self.vocab.items()}\n\
          \    return [(idx_to_word[i], similarities[i]) for i in top_indices]\n\ndef analogy(self, a, b, c):\n    # a is\
          \ to b as c is to ?\n    vec = self.W_in[self.vocab[b]] - self.W_in[self.vocab[a]] + self.W_in[self.vocab[c]]\n\
          \    return self.most_similar_vec(vec)"
      pitfalls:
      - Normalizing vectors
      - Including query word in results
      - Memory for large vocab
      concepts:
      - Cosine similarity
      - Word analogies
      - Dimensionality reduction
      estimated_hours: 3-4
  chatbot-intent:
    id: chatbot-intent
    name: Intent-Based Chatbot
    description: Build a chatbot with intent classification and entity extraction. Foundation for conversational AI without
      LLMs.
    difficulty: beginner
    estimated_hours: 15-25
    prerequisites:
    - Python basics
    - Text processing
    - Basic ML concepts
    languages:
      recommended:
      - Python
      also_possible:
      - JavaScript
      - Go
    resources:
    - name: Rasa Open Source
      url: https://rasa.com/docs/rasa/
      type: documentation
    - name: Intent Classification Tutorial
      url: https://www.tensorflow.org/tutorials/text/text_classification_rnn
      type: tutorial
    milestones:
    - id: 1
      name: Intent Classification
      description: Build intent classifier from training examples.
      acceptance_criteria:
      - Load training data (intent -> examples)
      - Vectorize text (TF-IDF or embeddings)
      - Train classifier (Naive Bayes or SVM)
      - Predict intent with confidence score
      - Handle unknown/low-confidence intents
      hints:
        level1: Start with TF-IDF + Naive Bayes. Simple but effective baseline.
        level2: Use scikit-learn Pipeline for clean preprocessing + classification.
        level3: "from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom\
          \ sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\n\nclass IntentClassifier:\n\
          \    def __init__(self, confidence_threshold=0.5):\n        self.threshold = confidence_threshold\n        self.pipeline\
          \ = Pipeline([\n            ('tfidf', TfidfVectorizer(ngram_range=(1, 2))),\n            ('clf', MultinomialNB())\n\
          \        ])\n        self.intents = []\n\n    def train(self, texts, intents):\n        self.intents = list(set(intents))\n\
          \        self.pipeline.fit(texts, intents)\n\n    def predict(self, text):\n        probs = self.pipeline.predict_proba([text])[0]\n\
          \        max_idx = probs.argmax()\n        confidence = probs[max_idx]\n\n        if confidence < self.threshold:\n\
          \            return {\"intent\": \"unknown\", \"confidence\": confidence}\n\n        return {\n            \"intent\"\
          : self.pipeline.classes_[max_idx],\n            \"confidence\": float(confidence)\n        }\n\n# Training data\n\
          training_data = [\n    (\"What's the weather like?\", \"weather\"),\n    (\"Is it going to rain?\", \"weather\"\
          ),\n    (\"Set an alarm for 7am\", \"alarm\"),\n    (\"Wake me up at 6\", \"alarm\"),\n    # ... more examples\n\
          ]\ntexts, intents = zip(*training_data)\nclf = IntentClassifier()\nclf.train(texts, intents)"
      pitfalls:
      - Too few training examples per intent
      - Imbalanced classes skew predictions
      - Not handling out-of-domain inputs
      - Overfitting on small datasets
      concepts:
      - Text classification
      - TF-IDF
      - Naive Bayes
      - Confidence thresholds
      estimated_hours: 4-6
    - id: 2
      name: Entity Extraction
      description: Extract entities (names, dates, numbers) from user input.
      acceptance_criteria:
      - Define entity types for your domain
      - Rule-based extraction (regex, patterns)
      - Named Entity Recognition (NER)
      - Slot filling for intents
      - Handle multiple entities in one message
      hints:
        level1: Combine regex for structured data + spaCy NER for names/dates.
        level2: Create entity templates per intent. 'set alarm for {time}' -> extract time.
        level3: "import re\nimport spacy\nfrom dateutil import parser as date_parser\n\nclass EntityExtractor:\n    def __init__(self):\n\
          \        self.nlp = spacy.load(\"en_core_web_sm\")\n        self.patterns = {\n            'time': r'\\b(\\d{1,2}(?::\\\
          d{2})?\\s*(?:am|pm)?|\\d{1,2}\\s*o'?clock)\\b',\n            'duration': r'\\b(\\d+)\\s*(minutes?|hours?|seconds?)\\\
          b',\n            'number': r'\\b(\\d+(?:\\.\\d+)?)\\b'\n        }\n\n    def extract(self, text, intent=None):\n\
          \        entities = {}\n\n        # spaCy NER for standard entities\n        doc = self.nlp(text)\n        for ent\
          \ in doc.ents:\n            if ent.label_ in ('PERSON', 'ORG', 'GPE', 'DATE', 'TIME', 'MONEY'):\n              \
          \  entities[ent.label_.lower()] = ent.text\n\n        # Custom patterns\n        for entity_type, pattern in self.patterns.items():\n\
          \            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                entities[entity_type]\
          \ = match.group(0)\n\n        # Parse dates/times\n        if 'time' in entities:\n            try:\n          \
          \      entities['parsed_time'] = date_parser.parse(entities['time'])\n            except:\n                pass\n\
          \n        return entities\n\n# Slot filling\nclass SlotFiller:\n    def __init__(self):\n        self.intent_slots\
          \ = {\n            'set_alarm': ['time'],\n            'book_restaurant': ['date', 'time', 'party_size', 'restaurant'],\n\
          \            'send_email': ['recipient', 'subject']\n        }\n\n    def fill_slots(self, intent, entities):\n\
          \        required = self.intent_slots.get(intent, [])\n        filled = {slot: entities.get(slot) for slot in required}\n\
          \        missing = [s for s in required if not filled.get(s)]\n        return {'slots': filled, 'missing': missing}"
      pitfalls:
      - Regex too greedy matches wrong text
      - spaCy model not loaded (need python -m spacy download)
      - Date parsing ambiguity (01/02 = Jan 2 or Feb 1?)
      - Overlapping entity matches
      concepts:
      - Named Entity Recognition
      - Regex patterns
      - Slot filling
      - spaCy
      estimated_hours: 4-6
    - id: 3
      name: Dialog Management
      description: Manage multi-turn conversations with context.
      acceptance_criteria:
      - Track conversation state
      - Handle follow-up questions
      - Ask clarifying questions for missing slots
      - Support conversation reset
      - Context timeout/expiry
      hints:
        level1: Store conversation state in dict keyed by session_id.
        level2: 'State machine: INIT -> COLLECTING_SLOTS -> CONFIRMING -> EXECUTING'
        level3: "from dataclasses import dataclass, field\nfrom typing import Dict, Any, Optional\nfrom enum import Enum\n\
          import time\n\nclass DialogState(Enum):\n    INIT = \"init\"\n    COLLECTING = \"collecting\"\n    CONFIRMING =\
          \ \"confirming\"\n    EXECUTING = \"executing\"\n    COMPLETE = \"complete\"\n\n@dataclass\nclass ConversationContext:\n\
          \    session_id: str\n    state: DialogState = DialogState.INIT\n    current_intent: Optional[str] = None\n    slots:\
          \ Dict[str, Any] = field(default_factory=dict)\n    history: list = field(default_factory=list)\n    last_active:\
          \ float = field(default_factory=time.time)\n\n    def is_expired(self, timeout=300):\n        return time.time()\
          \ - self.last_active > timeout\n\nclass DialogManager:\n    def __init__(self, intent_classifier, entity_extractor,\
          \ slot_filler):\n        self.classifier = intent_classifier\n        self.extractor = entity_extractor\n      \
          \  self.filler = slot_filler\n        self.contexts: Dict[str, ConversationContext] = {}\n\n    def process(self,\
          \ session_id: str, user_input: str) -> str:\n        ctx = self._get_or_create_context(session_id)\n        ctx.last_active\
          \ = time.time()\n        ctx.history.append({'role': 'user', 'text': user_input})\n\n        # Intent classification\n\
          \        if ctx.state == DialogState.INIT:\n            result = self.classifier.predict(user_input)\n         \
          \   ctx.current_intent = result['intent']\n            ctx.state = DialogState.COLLECTING\n\n        # Entity extraction\n\
          \        entities = self.extractor.extract(user_input, ctx.current_intent)\n        ctx.slots.update(entities)\n\
          \n        # Check missing slots\n        slot_result = self.filler.fill_slots(ctx.current_intent, ctx.slots)\n\n\
          \        if slot_result['missing']:\n            response = self._ask_for_slot(slot_result['missing'][0])\n    \
          \    else:\n            ctx.state = DialogState.CONFIRMING\n            response = self._confirm_action(ctx)\n\n\
          \        ctx.history.append({'role': 'assistant', 'text': response})\n        return response\n\n    def _ask_for_slot(self,\
          \ slot_name):\n        prompts = {\n            'time': \"What time would you like?\",\n            'date': \"What\
          \ date?\",\n            'recipient': \"Who should I send it to?\"\n        }\n        return prompts.get(slot_name,\
          \ f\"Please provide {slot_name}\")"
      pitfalls:
      - Memory leak from never-expiring sessions
      - Losing context on intent change
      - Infinite loops in dialog flow
      - Not handling 'cancel' or 'start over'
      concepts:
      - State machines
      - Session management
      - Multi-turn dialog
      - Context tracking
      estimated_hours: 5-8
    - id: 4
      name: Response Generation
      description: Generate natural language responses with templates.
      acceptance_criteria:
      - Template-based responses
      - Variable substitution
      - Response variations (avoid repetition)
      - Personality/tone consistency
      - Error message handling
      hints:
        level1: Use Jinja2 templates with random.choice for variations.
        level2: Group responses by intent and state. Add personality traits.
        level3: "import random\nfrom jinja2 import Template\nfrom dataclasses import dataclass\n\n@dataclass\nclass ResponseTemplate:\n\
          \    templates: list\n\n    def render(self, **kwargs):\n        template = random.choice(self.templates)\n    \
          \    return Template(template).render(**kwargs)\n\nclass ResponseGenerator:\n    def __init__(self):\n        self.responses\
          \ = {\n            ('weather', 'success'): ResponseTemplate([\n                \"It's currently {{temp}}Â°F and {{condition}}\
          \ in {{location}}.\",\n                \"The weather in {{location}}: {{temp}}Â°F, {{condition}}.\",\n          \
          \      \"{{location}} is {{condition}} right now, {{temp}}Â°F.\"\n            ]),\n            ('alarm', 'success'):\
          \ ResponseTemplate([\n                \"Done! I've set an alarm for {{time}}.\",\n                \"Alarm set for\
          \ {{time}}. I'll wake you up!\",\n                \"Got it - {{time}} alarm is ready.\"\n            ]),\n     \
          \       ('alarm', 'confirm'): ResponseTemplate([\n                \"Just to confirm: set an alarm for {{time}}?\"\
          ,\n                \"You want me to wake you at {{time}}, correct?\"\n            ]),\n            ('error', 'not_understood'):\
          \ ResponseTemplate([\n                \"I'm not sure I understand. Could you rephrase?\",\n                \"Sorry,\
          \ I didn't catch that. Can you try again?\",\n                \"I'm having trouble understanding. What would you\
          \ like?\"\n            ]),\n            ('error', 'missing_slot'): ResponseTemplate([\n                \"I need\
          \ a bit more info. {{question}}\",\n                \"{{question}}\"\n            ])\n        }\n\n        # Personality\
          \ traits\n        self.personality = {\n            'greeting_prefix': [\"Sure!\", \"Absolutely!\", \"Of course!\"\
          , \"\"],\n            'filler_words': [\"just\", \"actually\", \"\"],\n        }\n\n    def generate(self, intent,\
          \ state, **context):\n        key = (intent, state)\n        if key not in self.responses:\n            key = ('error',\
          \ 'not_understood')\n\n        response = self.responses[key].render(**context)\n\n        # Add personality\n \
          \       if random.random() < 0.3:\n            prefix = random.choice(self.personality['greeting_prefix'])\n   \
          \         if prefix:\n                response = f\"{prefix} {response}\"\n\n        return response"
      pitfalls:
      - Templates not escaping user input (XSS)
      - Missing template variables cause errors
      - Too few variations feel robotic
      - Inconsistent tone across responses
      concepts:
      - Template engines
      - NLG basics
      - Response variation
      - Personality design
      estimated_hours: 3-5
  rag-system:
    id: rag-system
    name: RAG System (Retrieval Augmented Generation)
    description: 'Build a production RAG pipeline: document ingestion, chunking, embedding, vector search, and LLM generation
      with retrieved context.'
    difficulty: intermediate
    estimated_hours: 30-50
    prerequisites:
    - Python
    - Basic ML concepts
    - REST APIs
    - Database basics
    languages:
      recommended:
      - Python
      also_possible:
      - TypeScript
      - Go
    resources:
    - name: LangChain RAG Tutorial
      url: https://python.langchain.com/docs/tutorials/rag/
      type: tutorial
    - name: OpenAI Embeddings
      url: https://platform.openai.com/docs/guides/embeddings
      type: documentation
    - name: Pinecone Vector DB
      url: https://docs.pinecone.io/
      type: documentation
    milestones:
    - id: 1
      name: Document Ingestion & Chunking
      description: Load documents and split into optimal chunks for retrieval.
      acceptance_criteria:
      - Load PDF, Markdown, HTML documents
      - Extract text with metadata preservation
      - Implement chunking strategies (fixed, semantic, recursive)
      - Handle chunk overlap for context continuity
      - Track source document and position
      hints:
        level1: Start with fixed-size chunks (500-1000 tokens) with 100 token overlap.
        level2: 'Semantic chunking: split on paragraphs/sections, then enforce max size.'
        level3: "from dataclasses import dataclass\nfrom typing import List, Optional\nimport tiktoken\nfrom pypdf import\
          \ PdfReader\nimport markdown\nfrom bs4 import BeautifulSoup\n\n@dataclass\nclass Chunk:\n    text: str\n    metadata:\
          \ dict  # source, page, position, etc.\n    token_count: int\n\nclass DocumentLoader:\n    def load(self, path:\
          \ str) -> str:\n        if path.endswith('.pdf'):\n            return self._load_pdf(path)\n        elif path.endswith('.md'):\n\
          \            return self._load_markdown(path)\n        elif path.endswith('.html'):\n            return self._load_html(path)\n\
          \        else:\n            return open(path).read()\n\n    def _load_pdf(self, path):\n        reader = PdfReader(path)\n\
          \        pages = []\n        for i, page in enumerate(reader.pages):\n            text = page.extract_text()\n \
          \           pages.append({'text': text, 'page': i + 1})\n        return pages\n\nclass TextChunker:\n    def __init__(self,\
          \ chunk_size=500, overlap=100, model=\"gpt-4\"):\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n\
          \        self.encoder = tiktoken.encoding_for_model(model)\n\n    def chunk_text(self, text: str, metadata: dict\
          \ = None) -> List[Chunk]:\n        tokens = self.encoder.encode(text)\n        chunks = []\n        start = 0\n\n\
          \        while start < len(tokens):\n            end = start + self.chunk_size\n            chunk_tokens = tokens[start:end]\n\
          \            chunk_text = self.encoder.decode(chunk_tokens)\n\n            chunks.append(Chunk(\n              \
          \  text=chunk_text,\n                metadata={**(metadata or {}), 'start_token': start},\n                token_count=len(chunk_tokens)\n\
          \            ))\n\n            start = end - self.overlap\n\n        return chunks\n\n    def semantic_chunk(self,\
          \ text: str, metadata: dict = None) -> List[Chunk]:\n        # Split on semantic boundaries first\n        paragraphs\
          \ = text.split('\\n\\n')\n        chunks = []\n        current_chunk = []\n        current_tokens = 0\n\n      \
          \  for para in paragraphs:\n            para_tokens = len(self.encoder.encode(para))\n\n            if current_tokens\
          \ + para_tokens > self.chunk_size:\n                if current_chunk:\n                    chunks.append(self._make_chunk(current_chunk,\
          \ metadata))\n                current_chunk = [para]\n                current_tokens = para_tokens\n           \
          \ else:\n                current_chunk.append(para)\n                current_tokens += para_tokens\n\n        if\
          \ current_chunk:\n            chunks.append(self._make_chunk(current_chunk, metadata))\n\n        return chunks"
      pitfalls:
      - Chunks too small lose context
      - Chunks too large exceed LLM context window
      - PDF extraction loses formatting/tables
      - Not handling encoding issues (UTF-8)
      concepts:
      - Document parsing
      - Text chunking
      - Tokenization
      - Metadata tracking
      estimated_hours: 6-10
    - id: 2
      name: Embedding Generation
      description: Convert text chunks to vector embeddings for semantic search.
      acceptance_criteria:
      - Generate embeddings using OpenAI/local model
      - Batch processing for efficiency
      - Handle rate limits and retries
      - Cache embeddings to avoid recomputation
      - Support multiple embedding models
      hints:
        level1: Use OpenAI text-embedding-3-small for good quality/cost ratio.
        level2: Batch requests (max 2048 texts per call). Implement exponential backoff.
        level3: "import openai\nfrom tenacity import retry, wait_exponential, stop_after_attempt\nimport numpy as np\nfrom\
          \ typing import List\nimport hashlib\nimport pickle\nfrom pathlib import Path\n\nclass EmbeddingService:\n    def\
          \ __init__(self, model=\"text-embedding-3-small\", cache_dir=\".cache/embeddings\"):\n        self.model = model\n\
          \        self.client = openai.OpenAI()\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(parents=True,\
          \ exist_ok=True)\n        self.dimension = 1536  # OpenAI small model\n\n    def _cache_key(self, text: str) ->\
          \ str:\n        return hashlib.md5(f\"{self.model}:{text}\".encode()).hexdigest()\n\n    def _get_cached(self, text:\
          \ str) -> Optional[np.ndarray]:\n        cache_path = self.cache_dir / f\"{self._cache_key(text)}.pkl\"\n      \
          \  if cache_path.exists():\n            return pickle.load(open(cache_path, 'rb'))\n        return None\n\n    def\
          \ _set_cached(self, text: str, embedding: np.ndarray):\n        cache_path = self.cache_dir / f\"{self._cache_key(text)}.pkl\"\
          \n        pickle.dump(embedding, open(cache_path, 'wb'))\n\n    @retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\n\
          \    def _embed_batch(self, texts: List[str]) -> List[np.ndarray]:\n        response = self.client.embeddings.create(\n\
          \            model=self.model,\n            input=texts\n        )\n        return [np.array(e.embedding) for e\
          \ in response.data]\n\n    def embed(self, texts: List[str], batch_size=100) -> List[np.ndarray]:\n        results\
          \ = [None] * len(texts)\n        to_embed = []  # (index, text) pairs\n\n        # Check cache first\n        for\
          \ i, text in enumerate(texts):\n            cached = self._get_cached(text)\n            if cached is not None:\n\
          \                results[i] = cached\n            else:\n                to_embed.append((i, text))\n\n        #\
          \ Batch embed uncached\n        for batch_start in range(0, len(to_embed), batch_size):\n            batch = to_embed[batch_start:batch_start\
          \ + batch_size]\n            indices, batch_texts = zip(*batch) if batch else ([], [])\n\n            embeddings\
          \ = self._embed_batch(list(batch_texts))\n\n            for idx, text, emb in zip(indices, batch_texts, embeddings):\n\
          \                self._set_cached(text, emb)\n                results[idx] = emb\n\n        return results\n\n#\
          \ Local embedding alternative with sentence-transformers\nclass LocalEmbedding:\n    def __init__(self, model_name=\"\
          all-MiniLM-L6-v2\"):\n        from sentence_transformers import SentenceTransformer\n        self.model = SentenceTransformer(model_name)\n\
          \        self.dimension = self.model.get_sentence_embedding_dimension()\n\n    def embed(self, texts: List[str])\
          \ -> List[np.ndarray]:\n        return self.model.encode(texts, convert_to_numpy=True)"
      pitfalls:
      - Rate limits cause failures without retry logic
      - Large batches exceed API limits
      - Not normalizing embeddings for cosine similarity
      - Embedding dimension mismatch between models
      concepts:
      - Text embeddings
      - API rate limiting
      - Caching strategies
      - Batch processing
      estimated_hours: 4-6
    - id: 3
      name: Vector Store & Retrieval
      description: Store embeddings and perform similarity search.
      acceptance_criteria:
      - Store vectors with metadata
      - K-nearest neighbor search
      - Hybrid search (vector + keyword)
      - Filtering by metadata
      - Support Pinecone/Chroma/pgvector
      hints:
        level1: Start with Chroma (local, no setup). Graduate to Pinecone for scale.
        level2: 'Hybrid search: combine BM25 keyword score with vector similarity.'
        level3: "import chromadb\nfrom chromadb.config import Settings\nimport numpy as np\nfrom typing import List, Dict,\
          \ Optional\nfrom rank_bm25 import BM25Okapi\n\nclass VectorStore:\n    def __init__(self, collection_name=\"documents\"\
          , persist_dir=\".chroma\"):\n        self.client = chromadb.PersistentClient(path=persist_dir)\n        self.collection\
          \ = self.client.get_or_create_collection(\n            name=collection_name,\n            metadata={\"hnsw:space\"\
          : \"cosine\"}\n        )\n        self._bm25 = None\n        self._doc_texts = []\n\n    def add(self, chunks: List[Chunk],\
          \ embeddings: List[np.ndarray]):\n        ids = [f\"chunk_{i}_{hash(c.text)}\" for i, c in enumerate(chunks)]\n\n\
          \        self.collection.add(\n            ids=ids,\n            embeddings=[e.tolist() for e in embeddings],\n\
          \            documents=[c.text for c in chunks],\n            metadatas=[c.metadata for c in chunks]\n        )\n\
          \n        # Update BM25 index\n        self._doc_texts.extend([c.text for c in chunks])\n        self._rebuild_bm25()\n\
          \n    def _rebuild_bm25(self):\n        tokenized = [doc.lower().split() for doc in self._doc_texts]\n        self._bm25\
          \ = BM25Okapi(tokenized)\n\n    def search(self, query_embedding: np.ndarray, query_text: str,\n               k:\
          \ int = 5, filter: Dict = None, hybrid_alpha: float = 0.5) -> List[Dict]:\n\n        # Vector search\n        vector_results\
          \ = self.collection.query(\n            query_embeddings=[query_embedding.tolist()],\n            n_results=k *\
          \ 2,  # Get more for reranking\n            where=filter\n        )\n\n        if not self._bm25 or hybrid_alpha\
          \ == 1.0:\n            return self._format_results(vector_results)\n\n        # BM25 search\n        tokenized_query\
          \ = query_text.lower().split()\n        bm25_scores = self._bm25.get_scores(tokenized_query)\n\n        # Combine\
          \ scores\n        combined = []\n        for i, (doc_id, doc, metadata, distance) in enumerate(zip(\n          \
          \  vector_results['ids'][0],\n            vector_results['documents'][0],\n            vector_results['metadatas'][0],\n\
          \            vector_results['distances'][0]\n        )):\n            vector_score = 1 - distance  # Convert distance\
          \ to similarity\n\n            # Find BM25 score for this doc\n            try:\n                bm25_idx = self._doc_texts.index(doc)\n\
          \                bm25_score = bm25_scores[bm25_idx] / max(bm25_scores)\n            except ValueError:\n       \
          \         bm25_score = 0\n\n            combined_score = hybrid_alpha * vector_score + (1 - hybrid_alpha) * bm25_score\n\
          \            combined.append({\n                'id': doc_id,\n                'text': doc,\n                'metadata':\
          \ metadata,\n                'score': combined_score\n            })\n\n        # Sort by combined score and return\
          \ top k\n        combined.sort(key=lambda x: x['score'], reverse=True)\n        return combined[:k]"
      pitfalls:
      - Using wrong distance metric (cosine vs L2)
      - Not normalizing scores for hybrid search
      - Memory issues with large collections
      - Stale BM25 index after updates
      concepts:
      - Vector databases
      - ANN search
      - BM25
      - Hybrid retrieval
      estimated_hours: 6-8
    - id: 4
      name: LLM Integration & Prompting
      description: Generate answers using retrieved context with LLM.
      acceptance_criteria:
      - Build effective RAG prompt template
      - Handle context window limits
      - Stream responses
      - Handle LLM errors gracefully
      - Support multiple LLM providers
      hints:
        level1: 'Simple template: ''Answer based on context: {context}\n\nQuestion: {question}'''
        level2: 'Add instructions: cite sources, say ''I don''t know'' if not in context.'
        level3: "from openai import OpenAI\nfrom anthropic import Anthropic\nfrom typing import Generator, List\nimport tiktoken\n\
          \nclass LLMService:\n    def __init__(self, provider=\"openai\", model=\"gpt-4-turbo-preview\"):\n        self.provider\
          \ = provider\n        self.model = model\n        if provider == \"openai\":\n            self.client = OpenAI()\n\
          \            self.encoder = tiktoken.encoding_for_model(model)\n        elif provider == \"anthropic\":\n      \
          \      self.client = Anthropic()\n\n        self.max_context = 128000  # GPT-4 Turbo\n\n    def _build_rag_prompt(self,\
          \ question: str, contexts: List[Dict]) -> str:\n        context_text = \"\\n\\n---\\n\\n\".join([\n            f\"\
          [Source: {c['metadata'].get('source', 'unknown')}]\\n{c['text']}\"\n            for c in contexts\n        ])\n\n\
          \        return f'''You are a helpful assistant that answers questions based on the provided context.\n\nCONTEXT:\n\
          {context_text}\n\nINSTRUCTIONS:\n- Answer the question based ONLY on the context above\n- If the answer is not in\
          \ the context, say \"I don't have enough information to answer this\"\n- Cite your sources using [Source: filename]\
          \ format\n- Be concise but complete\n\nQUESTION: {question}\n\nANSWER:'''\n\n    def _truncate_contexts(self, contexts:\
          \ List[Dict], max_tokens: int) -> List[Dict]:\n        result = []\n        total_tokens = 0\n\n        for ctx\
          \ in contexts:\n            ctx_tokens = len(self.encoder.encode(ctx['text']))\n            if total_tokens + ctx_tokens\
          \ > max_tokens:\n                break\n            result.append(ctx)\n            total_tokens += ctx_tokens\n\
          \n        return result\n\n    def generate(self, question: str, contexts: List[Dict],\n                 stream:\
          \ bool = True) -> Generator[str, None, None]:\n        # Reserve tokens for question and response\n        max_context_tokens\
          \ = self.max_context - 4000\n        contexts = self._truncate_contexts(contexts, max_context_tokens)\n\n      \
          \  prompt = self._build_rag_prompt(question, contexts)\n\n        if self.provider == \"openai\":\n            response\
          \ = self.client.chat.completions.create(\n                model=self.model,\n                messages=[{\"role\"\
          : \"user\", \"content\": prompt}],\n                stream=stream,\n                temperature=0.1\n          \
          \  )\n\n            if stream:\n                for chunk in response:\n                    if chunk.choices[0].delta.content:\n\
          \                        yield chunk.choices[0].delta.content\n            else:\n                yield response.choices[0].message.content\n\
          \nclass RAGPipeline:\n    def __init__(self, embedding_service, vector_store, llm_service):\n        self.embedder\
          \ = embedding_service\n        self.store = vector_store\n        self.llm = llm_service\n\n    def query(self,\
          \ question: str, k: int = 5) -> Generator[str, None, None]:\n        # Embed question\n        query_embedding =\
          \ self.embedder.embed([question])[0]\n\n        # Retrieve relevant chunks\n        contexts = self.store.search(query_embedding,\
          \ question, k=k)\n\n        # Generate response\n        yield from self.llm.generate(question, contexts)"
      pitfalls:
      - Exceeding context window with too many chunks
      - LLM hallucinating despite 'only use context' instruction
      - Not handling streaming errors mid-response
      - Prompt injection from retrieved content
      concepts:
      - Prompt engineering
      - Context window management
      - Streaming responses
      - LLM providers
      estimated_hours: 6-8
    - id: 5
      name: Evaluation & Optimization
      description: Measure and improve RAG quality.
      acceptance_criteria:
      - Implement retrieval metrics (recall, MRR)
      - Implement generation metrics (faithfulness, relevance)
      - Create evaluation dataset
      - A/B test different configurations
      - Implement reranking
      hints:
        level1: 'Create golden dataset: questions + expected chunks + expected answers.'
        level2: Use LLM-as-judge for generation quality (faithfulness, relevance).
        level3: "from dataclasses import dataclass\nfrom typing import List, Tuple\nimport numpy as np\n\n@dataclass\nclass\
          \ EvalSample:\n    question: str\n    expected_chunks: List[str]  # Ground truth relevant chunks\n    expected_answer:\
          \ str\n\nclass RAGEvaluator:\n    def __init__(self, rag_pipeline, llm_judge):\n        self.rag = rag_pipeline\n\
          \        self.judge = llm_judge\n\n    def evaluate_retrieval(self, samples: List[EvalSample], k: int = 5) -> Dict:\n\
          \        recalls = []\n        mrrs = []\n\n        for sample in samples:\n            query_emb = self.rag.embedder.embed([sample.question])[0]\n\
          \            retrieved = self.rag.store.search(query_emb, sample.question, k=k)\n            retrieved_texts = [r['text']\
          \ for r in retrieved]\n\n            # Recall@k\n            hits = sum(1 for exp in sample.expected_chunks\n  \
          \                    if any(exp in ret for ret in retrieved_texts))\n            recall = hits / len(sample.expected_chunks)\n\
          \            recalls.append(recall)\n\n            # MRR\n            for i, ret in enumerate(retrieved_texts):\n\
          \                if any(exp in ret for exp in sample.expected_chunks):\n                    mrrs.append(1 / (i +\
          \ 1))\n                    break\n            else:\n                mrrs.append(0)\n\n        return {\n      \
          \      'recall@k': np.mean(recalls),\n            'mrr': np.mean(mrrs)\n        }\n\n    def evaluate_generation(self,\
          \ samples: List[EvalSample]) -> Dict:\n        faithfulness_scores = []\n        relevance_scores = []\n\n     \
          \   for sample in samples:\n            # Get RAG response\n            response = ''.join(self.rag.query(sample.question))\n\
          \            query_emb = self.rag.embedder.embed([sample.question])[0]\n            contexts = self.rag.store.search(query_emb,\
          \ sample.question, k=5)\n\n            # Faithfulness: is response grounded in context?\n            faithfulness\
          \ = self.judge.evaluate(\n                prompt=f'''Rate if the response is fully supported by the context.\nContext:\
          \ {[c[\"text\"] for c in contexts]}\nResponse: {response}\nScore 1-5 (5=fully grounded):''',\n                parse_fn=lambda\
          \ x: int(x.strip()) / 5\n            )\n            faithfulness_scores.append(faithfulness)\n\n            # Relevance:\
          \ does response answer the question?\n            relevance = self.judge.evaluate(\n                prompt=f'''Rate\
          \ if the response answers the question.\nQuestion: {sample.question}\nResponse: {response}\nScore 1-5 (5=fully answers):''',\n\
          \                parse_fn=lambda x: int(x.strip()) / 5\n            )\n            relevance_scores.append(relevance)\n\
          \n        return {\n            'faithfulness': np.mean(faithfulness_scores),\n            'relevance': np.mean(relevance_scores)\n\
          \        }\n\n# Reranking for better retrieval\nclass Reranker:\n    def __init__(self, model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\
          ):\n        from sentence_transformers import CrossEncoder\n        self.model = CrossEncoder(model)\n\n    def\
          \ rerank(self, query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:\n        pairs = [(query, doc['text'])\
          \ for doc in documents]\n        scores = self.model.predict(pairs)\n\n        for doc, score in zip(documents,\
          \ scores):\n            doc['rerank_score'] = float(score)\n\n        documents.sort(key=lambda x: x['rerank_score'],\
          \ reverse=True)\n        return documents[:top_k]"
      pitfalls:
      - Evaluation set too small to be statistically significant
      - LLM judge bias toward verbose responses
      - Not versioning evaluation datasets
      - Overfitting to evaluation set
      concepts:
      - Retrieval metrics
      - Generation evaluation
      - LLM-as-judge
      - Cross-encoder reranking
      estimated_hours: 8-12
  semantic-search:
    id: semantic-search
    name: Semantic Search Engine
    description: Build a semantic search engine that understands meaning, not just keywords. Core technology behind modern
      search.
    difficulty: intermediate
    estimated_hours: 25-40
    prerequisites:
    - Python
    - Embeddings basics
    - Database fundamentals
    languages:
      recommended:
      - Python
      - Go
      also_possible:
      - TypeScript
      - Rust
    resources:
    - name: Sentence Transformers
      url: https://www.sbert.net/
      type: documentation
    - name: FAISS Library
      url: https://github.com/facebookresearch/faiss
      type: documentation
    milestones:
    - id: 1
      name: Embedding Index
      description: Build efficient vector index for similarity search.
      acceptance_criteria:
      - Index millions of vectors efficiently
      - Sub-second query latency
      - Support incremental updates
      - Implement HNSW or IVF index
      - Memory-mapped for large datasets
      hints:
        level1: FAISS IVF for large scale, HNSW for accuracy. Start with Flat for small data.
        level2: 'IVF: cluster vectors, search only nearby clusters. HNSW: navigable small world graph.'
        level3: "import faiss\nimport numpy as np\nfrom typing import List, Tuple\n\nclass VectorIndex:\n    def __init__(self,\
          \ dimension: int, index_type: str = \"hnsw\"):\n        self.dimension = dimension\n        self.index_type = index_type\n\
          \        self._build_index()\n        self.id_map = {}  # internal_id -> external_id\n        self.next_id = 0\n\
          \n    def _build_index(self):\n        if self.index_type == \"flat\":\n            # Exact search, O(n)\n     \
          \       self.index = faiss.IndexFlatIP(self.dimension)\n\n        elif self.index_type == \"ivf\":\n           \
          \ # Inverted file index, faster for large datasets\n            quantizer = faiss.IndexFlatIP(self.dimension)\n\
          \            self.index = faiss.IndexIVFFlat(quantizer, self.dimension, 100)  # 100 clusters\n            self.needs_training\
          \ = True\n\n        elif self.index_type == \"hnsw\":\n            # Hierarchical Navigable Small World - best accuracy/speed\
          \ tradeoff\n            self.index = faiss.IndexHNSWFlat(self.dimension, 32)  # 32 connections per node\n      \
          \      self.index.hnsw.efConstruction = 200  # Build-time quality\n            self.index.hnsw.efSearch = 50  #\
          \ Search-time quality\n\n    def add(self, vectors: np.ndarray, ids: List[str]):\n        # Normalize for cosine\
          \ similarity\n        faiss.normalize_L2(vectors)\n\n        # Train if needed (IVF)\n        if hasattr(self, 'needs_training')\
          \ and self.needs_training:\n            self.index.train(vectors)\n            self.needs_training = False\n\n \
          \       # Map external IDs to internal\n        for ext_id in ids:\n            self.id_map[self.next_id] = ext_id\n\
          \            self.next_id += 1\n\n        self.index.add(vectors)\n\n    def search(self, query: np.ndarray, k:\
          \ int = 10) -> List[Tuple[str, float]]:\n        query = query.reshape(1, -1).astype('float32')\n        faiss.normalize_L2(query)\n\
          \n        distances, indices = self.index.search(query, k)\n\n        results = []\n        for idx, dist in zip(indices[0],\
          \ distances[0]):\n            if idx >= 0:  # -1 means not found\n                ext_id = self.id_map.get(idx,\
          \ str(idx))\n                results.append((ext_id, float(dist)))\n\n        return results\n\n    def save(self,\
          \ path: str):\n        faiss.write_index(self.index, f\"{path}.faiss\")\n        np.save(f\"{path}_idmap.npy\",\
          \ self.id_map)\n\n    def load(self, path: str):\n        self.index = faiss.read_index(f\"{path}.faiss\")\n   \
          \     self.id_map = np.load(f\"{path}_idmap.npy\", allow_pickle=True).item()"
      pitfalls:
      - Not normalizing vectors for cosine similarity
      - IVF requires training before adding
      - Memory explosion with large HNSW M parameter
      - ID mapping gets out of sync
      concepts:
      - HNSW algorithm
      - IVF indexing
      - Approximate nearest neighbors
      - Index persistence
      estimated_hours: 6-10
    - id: 2
      name: Query Processing
      description: Parse and enhance queries for better results.
      acceptance_criteria:
      - Query expansion (synonyms, related terms)
      - Query understanding (intent, entities)
      - Multi-vector queries
      - Negative queries ('without X')
      - Query caching
      hints:
        level1: Expand queries with WordNet synonyms or use LLM to generate variations.
        level2: 'Multi-vector: average embeddings of query expansion. Weight original higher.'
        level3: "from functools import lru_cache\nimport numpy as np\nfrom nltk.corpus import wordnet\n\nclass QueryProcessor:\n\
          \    def __init__(self, embedding_service):\n        self.embedder = embedding_service\n\n    @lru_cache(maxsize=10000)\n\
          \    def _get_synonyms(self, word: str) -> List[str]:\n        synonyms = set()\n        for syn in wordnet.synsets(word):\n\
          \            for lemma in syn.lemmas():\n                synonyms.add(lemma.name().replace('_', ' '))\n        return\
          \ list(synonyms)[:5]\n\n    def expand_query(self, query: str) -> List[str]:\n        words = query.lower().split()\n\
          \        expanded = [query]\n\n        for word in words:\n            synonyms = self._get_synonyms(word)\n   \
          \         for syn in synonyms[:2]:\n                expanded.append(query.replace(word, syn))\n\n        return\
          \ list(set(expanded))\n\n    def process(self, query: str) -> np.ndarray:\n        # Parse negative queries\n  \
          \      positive_query, negative_terms = self._parse_negatives(query)\n\n        # Expand query\n        expansions\
          \ = self.expand_query(positive_query)\n\n        # Embed all expansions\n        embeddings = self.embedder.embed(expansions)\n\
          \n        # Weighted average (original query weighted 2x)\n        weights = [2.0] + [1.0] * (len(embeddings) -\
          \ 1)\n        weights = np.array(weights) / sum(weights)\n        query_vector = np.average(embeddings, axis=0,\
          \ weights=weights)\n\n        # Subtract negative term vectors\n        if negative_terms:\n            neg_embeddings\
          \ = self.embedder.embed(negative_terms)\n            neg_vector = np.mean(neg_embeddings, axis=0)\n            query_vector\
          \ = query_vector - 0.5 * neg_vector\n\n        # Normalize\n        query_vector = query_vector / np.linalg.norm(query_vector)\n\
          \n        return query_vector\n\n    def _parse_negatives(self, query: str) -> Tuple[str, List[str]]:\n        #\
          \ \"python tutorial -video -beginner\"\n        parts = query.split()\n        positive = []\n        negative =\
          \ []\n\n        for part in parts:\n            if part.startswith('-'):\n                negative.append(part[1:])\n\
          \            else:\n                positive.append(part)\n\n        return ' '.join(positive), negative"
      pitfalls:
      - Over-expansion dilutes original intent
      - Synonym quality varies by domain
      - Negative subtraction can flip meaning
      - Cache invalidation on embedding model change
      concepts:
      - Query expansion
      - Semantic understanding
      - Vector arithmetic
      - Query caching
      estimated_hours: 5-8
    - id: 3
      name: Ranking & Relevance
      description: Combine signals for optimal result ranking.
      acceptance_criteria:
      - Multi-stage ranking (retrieve -> rerank)
      - Combine semantic + lexical scores
      - Personalization signals
      - Freshness/recency boost
      - Click-through rate learning
      hints:
        level1: 'First stage: fast vector search. Second stage: cross-encoder reranking on top-100.'
        level2: 'Linear combination: score = Î±*semantic + Î²*bm25 + Î³*freshness + Î´*popularity'
        level3: "from sentence_transformers import CrossEncoder\nfrom datetime import datetime\nimport math\n\nclass RankingService:\n\
          \    def __init__(self):\n        self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n\
          \        # Feature weights (learned or tuned)\n        self.weights = {\n            'semantic': 0.4,\n        \
          \    'lexical': 0.2,\n            'rerank': 0.25,\n            'freshness': 0.1,\n            'popularity': 0.05\n\
          \        }\n\n    def rank(self, query: str, candidates: List[Dict],\n             user_context: Dict = None) ->\
          \ List[Dict]:\n\n        # Stage 1: Score all candidates\n        for doc in candidates:\n            doc['scores']\
          \ = {}\n            doc['scores']['semantic'] = doc.get('vector_score', 0)\n            doc['scores']['lexical']\
          \ = doc.get('bm25_score', 0)\n            doc['scores']['freshness'] = self._freshness_score(doc)\n            doc['scores']['popularity']\
          \ = self._popularity_score(doc)\n\n        # Stage 2: Rerank top candidates with cross-encoder\n        top_candidates\
          \ = sorted(candidates,\n                                key=lambda x: x['scores']['semantic'],\n               \
          \                 reverse=True)[:100]\n\n        pairs = [(query, doc['text'][:512]) for doc in top_candidates]\n\
          \        rerank_scores = self.cross_encoder.predict(pairs)\n\n        for doc, score in zip(top_candidates, rerank_scores):\n\
          \            doc['scores']['rerank'] = float(score)\n\n        # Combine scores\n        for doc in top_candidates:\n\
          \            doc['final_score'] = sum(\n                self.weights[k] * doc['scores'].get(k, 0)\n            \
          \    for k in self.weights\n            )\n\n            # Personalization boost\n            if user_context:\n\
          \                doc['final_score'] *= self._personalization_boost(doc, user_context)\n\n        return sorted(top_candidates,\
          \ key=lambda x: x['final_score'], reverse=True)\n\n    def _freshness_score(self, doc: Dict) -> float:\n       \
          \ if 'timestamp' not in doc:\n            return 0.5\n        age_days = (datetime.now() - doc['timestamp']).days\n\
          \        # Exponential decay, half-life of 30 days\n        return math.exp(-age_days / 30)\n\n    def _popularity_score(self,\
          \ doc: Dict) -> float:\n        clicks = doc.get('click_count', 0)\n        impressions = doc.get('impression_count',\
          \ 1)\n        # Smoothed CTR\n        return (clicks + 1) / (impressions + 10)\n\n    def _personalization_boost(self,\
          \ doc: Dict, user: Dict) -> float:\n        boost = 1.0\n        # Boost docs from user's preferred categories\n\
          \        if doc.get('category') in user.get('preferred_categories', []):\n            boost *= 1.2\n        # Boost\
          \ recent user interests\n        if any(tag in user.get('recent_tags', []) for tag in doc.get('tags', [])):\n  \
          \          boost *= 1.1\n        return boost"
      pitfalls:
      - Cross-encoder too slow for all results
      - Weight tuning is data-dependent
      - Popularity bias creates filter bubbles
      - Freshness can demote evergreen content
      concepts:
      - Multi-stage ranking
      - Learning to rank
      - Feature engineering
      - Personalization
      estimated_hours: 6-10
    - id: 4
      name: Search API & UI
      description: Build production search API with instant results.
      acceptance_criteria:
      - RESTful search API
      - Typeahead/autocomplete
      - Faceted search (filters)
      - Highlighting matched terms
      - Search analytics
      hints:
        level1: 'Autocomplete: prefix trie + popular queries. Update on each keystroke.'
        level2: 'Facets: pre-compute counts per category. Update with filters.'
        level3: "from fastapi import FastAPI, Query\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport\
          \ asyncio\n\napp = FastAPI()\n\nclass SearchRequest(BaseModel):\n    query: str\n    filters: Optional[Dict] = {}\n\
          \    page: int = 1\n    page_size: int = 10\n\nclass SearchResult(BaseModel):\n    id: str\n    title: str\n   \
          \ snippet: str\n    score: float\n    highlights: List[str]\n\nclass SearchResponse(BaseModel):\n    results: List[SearchResult]\n\
          \    total: int\n    facets: Dict[str, List[Dict]]\n    query_time_ms: float\n\n@app.post(\"/search\")\nasync def\
          \ search(request: SearchRequest) -> SearchResponse:\n    start = time.time()\n\n    # Process query\n    query_vector\
          \ = query_processor.process(request.query)\n\n    # Search with filters\n    candidates = await vector_store.search_async(\n\
          \        query_vector,\n        k=request.page_size * 10,  # Over-fetch for filtering\n        filters=request.filters\n\
          \    )\n\n    # Rank\n    ranked = ranking_service.rank(request.query, candidates)\n\n    # Paginate\n    start_idx\
          \ = (request.page - 1) * request.page_size\n    page_results = ranked[start_idx:start_idx + request.page_size]\n\
          \n    # Build response with highlights\n    results = [\n        SearchResult(\n            id=doc['id'],\n    \
          \        title=doc['title'],\n            snippet=highlight_snippet(doc['text'], request.query),\n            score=doc['final_score'],\n\
          \            highlights=find_highlights(doc['text'], request.query)\n        )\n        for doc in page_results\n\
          \    ]\n\n    # Compute facets\n    facets = compute_facets(ranked, request.filters)\n\n    return SearchResponse(\n\
          \        results=results,\n        total=len(ranked),\n        facets=facets,\n        query_time_ms=(time.time()\
          \ - start) * 1000\n    )\n\n@app.get(\"/autocomplete\")\nasync def autocomplete(q: str = Query(min_length=2)) ->\
          \ List[str]:\n    # Prefix search in popular queries\n    suggestions = await trie.search_prefix(q.lower(), limit=10)\n\
          \    return suggestions\n\ndef highlight_snippet(text: str, query: str, context_chars: int = 150) -> str:\n    query_terms\
          \ = query.lower().split()\n    text_lower = text.lower()\n\n    # Find best matching window\n    best_pos = 0\n\
          \    best_count = 0\n\n    for i in range(0, len(text) - context_chars, 50):\n        window = text_lower[i:i +\
          \ context_chars]\n        count = sum(1 for term in query_terms if term in window)\n        if count > best_count:\n\
          \            best_count = count\n            best_pos = i\n\n    snippet = text[best_pos:best_pos + context_chars]\n\
          \n    # Highlight terms\n    for term in query_terms:\n        snippet = re.sub(\n            f'({re.escape(term)})',\n\
          \            r'<mark>\\1</mark>',\n            snippet,\n            flags=re.IGNORECASE\n        )\n\n    return\
          \ snippet"
      pitfalls:
      - Autocomplete latency > 100ms feels slow
      - Facet counts expensive to compute
      - Highlighting breaks on HTML entities
      - Not tracking null results for improvement
      concepts:
      - Search API design
      - Autocomplete
      - Faceted navigation
      - Result highlighting
      estimated_hours: 8-12
  ai-agent-framework:
    id: ai-agent-framework
    name: AI Agent Framework
    description: Build a framework for autonomous AI agents that can use tools, plan, and execute multi-step tasks.
    difficulty: advanced
    estimated_hours: 50-80
    prerequisites:
    - LLM APIs
    - Python async
    - System design
    languages:
      recommended:
      - Python
      also_possible:
      - TypeScript
    resources:
    - name: LangChain Agents
      url: https://python.langchain.com/docs/modules/agents/
      type: documentation
    - name: AutoGPT
      url: https://github.com/Significant-Gravitas/AutoGPT
      type: reference
    - name: ReAct Paper
      url: https://arxiv.org/abs/2210.03629
      type: paper
    milestones:
    - id: 1
      name: Tool System
      description: Build extensible tool system for agent capabilities.
      acceptance_criteria:
      - Define tool interface (name, description, parameters, execute)
      - Tool discovery and registration
      - Parameter validation
      - Error handling and retries
      - Tool result formatting for LLM
      hints:
        level1: Tools are functions with JSON schema for parameters. LLM chooses which to call.
        level2: Use Pydantic for parameter validation. Return structured results.
        level3: "from abc import ABC, abstractmethod\nfrom pydantic import BaseModel, Field\nfrom typing import Any, Dict,\
          \ Type\nimport inspect\n\nclass ToolResult(BaseModel):\n    success: bool\n    data: Any = None\n    error: str\
          \ = None\n\nclass Tool(ABC):\n    name: str\n    description: str\n    parameters_schema: Type[BaseModel]\n\n  \
          \  @abstractmethod\n    def execute(self, **kwargs) -> ToolResult:\n        pass\n\n    def to_openai_function(self)\
          \ -> Dict:\n        return {\n            \"name\": self.name,\n            \"description\": self.description,\n\
          \            \"parameters\": self.parameters_schema.model_json_schema()\n        }\n\nclass WebSearchParams(BaseModel):\n\
          \    query: str = Field(description=\"Search query\")\n    num_results: int = Field(default=5, description=\"Number\
          \ of results\")\n\nclass WebSearchTool(Tool):\n    name = \"web_search\"\n    description = \"Search the web for\
          \ current information\"\n    parameters_schema = WebSearchParams\n\n    def __init__(self, api_key: str):\n    \
          \    self.api_key = api_key\n\n    def execute(self, query: str, num_results: int = 5) -> ToolResult:\n        try:\n\
          \            results = search_api.search(query, num_results)\n            return ToolResult(success=True, data=results)\n\
          \        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n\nclass CodeExecutionParams(BaseModel):\n\
          \    code: str = Field(description=\"Python code to execute\")\n    timeout: int = Field(default=30, description=\"\
          Timeout in seconds\")\n\nclass CodeExecutionTool(Tool):\n    name = \"execute_code\"\n    description = \"Execute\
          \ Python code in a sandboxed environment\"\n    parameters_schema = CodeExecutionParams\n\n    def execute(self,\
          \ code: str, timeout: int = 30) -> ToolResult:\n        try:\n            # Run in sandbox (Docker, subprocess,\
          \ etc.)\n            result = sandbox.run(code, timeout=timeout)\n            return ToolResult(success=True, data={\n\
          \                'stdout': result.stdout,\n                'stderr': result.stderr,\n                'return_value':\
          \ result.return_value\n            })\n        except TimeoutError:\n            return ToolResult(success=False,\
          \ error=\"Code execution timed out\")\n        except Exception as e:\n            return ToolResult(success=False,\
          \ error=str(e))\n\nclass ToolRegistry:\n    def __init__(self):\n        self.tools: Dict[str, Tool] = {}\n\n  \
          \  def register(self, tool: Tool):\n        self.tools[tool.name] = tool\n\n    def get(self, name: str) -> Tool:\n\
          \        return self.tools.get(name)\n\n    def list_functions(self) -> List[Dict]:\n        return [t.to_openai_function()\
          \ for t in self.tools.values()]\n\n    def execute(self, name: str, params: Dict) -> ToolResult:\n        tool =\
          \ self.get(name)\n        if not tool:\n            return ToolResult(success=False, error=f\"Unknown tool: {name}\"\
          )\n\n        # Validate parameters\n        try:\n            validated = tool.parameters_schema(**params)\n   \
          \         return tool.execute(**validated.model_dump())\n        except Exception as e:\n            return ToolResult(success=False,\
          \ error=f\"Parameter error: {e}\")"
      pitfalls:
      - Tool descriptions too vague for LLM to choose correctly
      - Missing parameter validation causes runtime errors
      - Tool execution without timeout hangs forever
      - Sensitive tools without permission checks
      concepts:
      - Tool abstraction
      - JSON Schema
      - Sandboxed execution
      - Registry pattern
      estimated_hours: 8-12
    - id: 2
      name: ReAct Loop (Reasoning + Acting)
      description: 'Implement the core agent loop: think, act, observe, repeat.'
      acceptance_criteria:
      - Thought-Action-Observation cycle
      - Parse LLM output for actions
      - Handle action failures gracefully
      - Maximum iteration limits
      - Structured output parsing
      hints:
        level1: 'Prompt: ''Think step by step. Use Action: tool_name(params). Wait for Observation.'''
        level2: Use function calling API for reliable action parsing. Fallback to regex.
        level3: "from openai import OpenAI\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Optional,\
          \ List\n\nclass AgentState(Enum):\n    THINKING = \"thinking\"\n    ACTING = \"acting\"\n    OBSERVING = \"observing\"\
          \n    FINISHED = \"finished\"\n    ERROR = \"error\"\n\n@dataclass\nclass AgentStep:\n    thought: Optional[str]\n\
          \    action: Optional[str]\n    action_input: Optional[Dict]\n    observation: Optional[str]\n\nclass ReActAgent:\n\
          \    def __init__(self, tools: ToolRegistry, model: str = \"gpt-4-turbo-preview\"):\n        self.tools = tools\n\
          \        self.client = OpenAI()\n        self.model = model\n        self.max_iterations = 10\n\n    def run(self,\
          \ task: str) -> str:\n        messages = [\n            {\"role\": \"system\", \"content\": self._system_prompt()},\n\
          \            {\"role\": \"user\", \"content\": task}\n        ]\n\n        steps: List[AgentStep] = []\n\n     \
          \   for i in range(self.max_iterations):\n            # Get LLM response with function calling\n            response\
          \ = self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n\
          \                tools=[{\"type\": \"function\", \"function\": f}\n                       for f in self.tools.list_functions()],\n\
          \                tool_choice=\"auto\"\n            )\n\n            message = response.choices[0].message\n\n  \
          \          # Check if finished (no tool calls)\n            if not message.tool_calls:\n                return message.content\n\
          \n            # Execute tool calls\n            messages.append(message)\n\n            for tool_call in message.tool_calls:\n\
          \                step = AgentStep(\n                    thought=message.content,\n                    action=tool_call.function.name,\n\
          \                    action_input=json.loads(tool_call.function.arguments),\n                    observation=None\n\
          \                )\n\n                # Execute tool\n                result = self.tools.execute(\n           \
          \         tool_call.function.name,\n                    json.loads(tool_call.function.arguments)\n             \
          \   )\n\n                step.observation = str(result.data if result.success else result.error)\n             \
          \   steps.append(step)\n\n                # Add observation to messages\n                messages.append({\n   \
          \                 \"role\": \"tool\",\n                    \"tool_call_id\": tool_call.id,\n                   \
          \ \"content\": step.observation\n                })\n\n        return \"Max iterations reached without completing\
          \ task\"\n\n    def _system_prompt(self) -> str:\n        tool_descriptions = \"\\n\".join([\n            f\"- {t.name}:\
          \ {t.description}\"\n            for t in self.tools.tools.values()\n        ])\n\n        return f'''You are a\
          \ helpful AI assistant that can use tools to accomplish tasks.\n\nAvailable tools:\n{tool_descriptions}\n\nThink\
          \ step by step about how to accomplish the task. Use tools when needed.\nWhen you have the final answer, respond\
          \ without using any tools.'''"
      pitfalls:
      - Infinite loops when agent doesn't know when to stop
      - LLM hallucinates non-existent tools
      - Action parsing fails on malformed output
      - Not handling tool timeouts
      concepts:
      - ReAct framework
      - Function calling
      - Agent loops
      - Structured outputs
      estimated_hours: 10-15
    - id: 3
      name: Planning & Task Decomposition
      description: Add planning layer for complex multi-step tasks.
      acceptance_criteria:
      - Decompose complex tasks into subtasks
      - Create execution plan
      - Handle dependencies between steps
      - Replan on failure
      - Parallel execution where possible
      hints:
        level1: First ask LLM to create plan, then execute each step. Simpler than full planning.
        level2: DAG of tasks with dependencies. Topological sort for execution order.
        level3: "from dataclasses import dataclass, field\nfrom typing import List, Set\nimport asyncio\nfrom enum import\
          \ Enum\n\nclass TaskStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\
          \n    FAILED = \"failed\"\n\n@dataclass\nclass PlanTask:\n    id: str\n    description: str\n    dependencies: Set[str]\
          \ = field(default_factory=set)\n    status: TaskStatus = TaskStatus.PENDING\n    result: Any = None\n    error:\
          \ str = None\n\nclass TaskPlanner:\n    def __init__(self, llm_client):\n        self.llm = llm_client\n\n    def\
          \ create_plan(self, objective: str) -> List[PlanTask]:\n        prompt = f'''Create a plan to accomplish this objective:\n\
          {objective}\n\nReturn a JSON list of tasks with:\n- id: unique task identifier\n- description: what to do\n- dependencies:\
          \ list of task ids this depends on\n\nExample:\n[\n  {{\"id\": \"1\", \"description\": \"Search for information\
          \ about X\", \"dependencies\": []}},\n  {{\"id\": \"2\", \"description\": \"Analyze the search results\", \"dependencies\"\
          : [\"1\"]}},\n  {{\"id\": \"3\", \"description\": \"Write summary based on analysis\", \"dependencies\": [\"2\"\
          ]}}\n]'''\n\n        response = self.llm.generate(prompt)\n        tasks_data = json.loads(response)\n\n       \
          \ return [\n            PlanTask(\n                id=t['id'],\n                description=t['description'],\n\
          \                dependencies=set(t.get('dependencies', []))\n            )\n            for t in tasks_data\n \
          \       ]\n\nclass PlanExecutor:\n    def __init__(self, agent: ReActAgent):\n        self.agent = agent\n\n   \
          \ async def execute_plan(self, tasks: List[PlanTask]) -> Dict[str, Any]:\n        task_map = {t.id: t for t in tasks}\n\
          \        results = {}\n\n        while not all(t.status in (TaskStatus.COMPLETED, TaskStatus.FAILED) for t in tasks):\n\
          \            # Find ready tasks (dependencies met)\n            ready = [\n                t for t in tasks\n  \
          \              if t.status == TaskStatus.PENDING and\n                all(task_map[dep].status == TaskStatus.COMPLETED\n\
          \                    for dep in t.dependencies)\n            ]\n\n            if not ready:\n                # Check\
          \ for deadlock\n                pending = [t for t in tasks if t.status == TaskStatus.PENDING]\n               \
          \ if pending:\n                    raise RuntimeError(\"Deadlock detected in task dependencies\")\n            \
          \    break\n\n            # Execute ready tasks in parallel\n            async def run_task(task: PlanTask):\n \
          \               task.status = TaskStatus.RUNNING\n                try:\n                    # Include context from\
          \ dependencies\n                    context = {dep: results[dep] for dep in task.dependencies}\n               \
          \     prompt = f\"Previous results: {context}\\n\\nTask: {task.description}\"\n\n                    result = await\
          \ asyncio.to_thread(self.agent.run, prompt)\n                    task.result = result\n                    task.status\
          \ = TaskStatus.COMPLETED\n                    results[task.id] = result\n                except Exception as e:\n\
          \                    task.status = TaskStatus.FAILED\n                    task.error = str(e)\n\n            await\
          \ asyncio.gather(*[run_task(t) for t in ready])\n\n        return results\n\n    async def replan_on_failure(self,\
          \ tasks: List[PlanTask], objective: str) -> List[PlanTask]:\n        failed = [t for t in tasks if t.status == TaskStatus.FAILED]\n\
          \        completed = [t for t in tasks if t.status == TaskStatus.COMPLETED]\n\n        prompt = f'''The original\
          \ plan failed. Here's what happened:\n\nCompleted tasks: {[(t.id, t.description, t.result) for t in completed]}\n\
          Failed tasks: {[(t.id, t.description, t.error) for t in failed]}\n\nCreate a new plan to accomplish: {objective}\n\
          Consider what already succeeded and avoid the failures.'''\n\n        return self.planner.create_plan(prompt)"
      pitfalls:
      - Circular dependencies in task graph
      - Over-decomposition creates too many steps
      - Context lost between subtasks
      - Replanning loops infinitely
      concepts:
      - Task decomposition
      - DAG execution
      - Async parallel execution
      - Replanning
      estimated_hours: 12-18
    - id: 4
      name: Memory & Context Management
      description: Give agents short-term and long-term memory.
      acceptance_criteria:
      - Conversation history (short-term)
      - Working memory for current task
      - Long-term memory (vector store)
      - Memory retrieval for context
      - Memory summarization and compression
      hints:
        level1: 'Short-term: sliding window of last N messages. Long-term: embed and store in vector DB.'
        level2: Summarize old messages to save context. Retrieve relevant memories for current task.
        level3: "from datetime import datetime\nfrom typing import List, Optional\nimport numpy as np\n\n@dataclass\nclass\
          \ Memory:\n    content: str\n    type: str  # 'conversation', 'task_result', 'fact', 'reflection'\n    timestamp:\
          \ datetime\n    importance: float = 0.5\n    embedding: Optional[np.ndarray] = None\n\nclass AgentMemory:\n    def\
          \ __init__(self, embedding_service, vector_store, llm):\n        self.embedder = embedding_service\n        self.store\
          \ = vector_store\n        self.llm = llm\n        self.working_memory: List[Memory] = []\n        self.max_working_memory\
          \ = 10\n\n    def add(self, content: str, memory_type: str = 'conversation', importance: float = 0.5):\n       \
          \ embedding = self.embedder.embed([content])[0]\n\n        memory = Memory(\n            content=content,\n    \
          \        type=memory_type,\n            timestamp=datetime.now(),\n            importance=importance,\n        \
          \    embedding=embedding\n        )\n\n        # Add to working memory\n        self.working_memory.append(memory)\n\
          \        if len(self.working_memory) > self.max_working_memory:\n            # Move oldest to long-term\n      \
          \      old = self.working_memory.pop(0)\n            self._store_long_term(old)\n\n        # High importance goes\
          \ straight to long-term\n        if importance > 0.8:\n            self._store_long_term(memory)\n\n    def _store_long_term(self,\
          \ memory: Memory):\n        self.store.add(\n            ids=[f\"mem_{memory.timestamp.isoformat()}\"],\n      \
          \      embeddings=[memory.embedding],\n            metadatas=[{\n                'content': memory.content,\n  \
          \              'type': memory.type,\n                'timestamp': memory.timestamp.isoformat(),\n              \
          \  'importance': memory.importance\n            }]\n        )\n\n    def retrieve(self, query: str, k: int = 5)\
          \ -> List[Memory]:\n        query_embedding = self.embedder.embed([query])[0]\n        results = self.store.search(query_embedding,\
          \ query, k=k)\n\n        return [\n            Memory(\n                content=r['metadata']['content'],\n    \
          \            type=r['metadata']['type'],\n                timestamp=datetime.fromisoformat(r['metadata']['timestamp']),\n\
          \                importance=r['metadata']['importance']\n            )\n            for r in results\n        ]\n\
          \n    def get_context(self, current_task: str, max_tokens: int = 2000) -> str:\n        # Recent working memory\n\
          \        recent = [m.content for m in self.working_memory[-5:]]\n\n        # Relevant long-term memories\n     \
          \   relevant = self.retrieve(current_task, k=10)\n        relevant_content = [m.content for m in relevant]\n\n \
          \       context = f'''RECENT CONTEXT:\n{chr(10).join(recent)}\n\nRELEVANT MEMORIES:\n{chr(10).join(relevant_content)}'''\n\
          \n        # Summarize if too long\n        if len(context) > max_tokens * 4:  # Rough char estimate\n          \
          \  context = self._summarize(context, max_tokens)\n\n        return context\n\n    def _summarize(self, content:\
          \ str, max_tokens: int) -> str:\n        return self.llm.generate(\n            f\"Summarize the following context\
          \ in about {max_tokens} tokens, \"\n            f\"preserving the most important information:\\n\\n{content}\"\n\
          \        )\n\n    def reflect(self):\n        '''Generate insights from recent memories'''\n        recent = [m.content\
          \ for m in self.working_memory]\n\n        reflection = self.llm.generate(\n            f\"Based on these recent\
          \ events, what are key insights or patterns?\\n\\n\"\n            f\"{chr(10).join(recent)}\"\n        )\n\n   \
          \     self.add(reflection, memory_type='reflection', importance=0.9)\n        return reflection"
      pitfalls:
      - Memory grows unbounded without cleanup
      - Retrieved memories not relevant to task
      - Summarization loses critical details
      - Importance scoring is subjective
      concepts:
      - Working vs long-term memory
      - Memory retrieval
      - Context compression
      - Reflection
      estimated_hours: 10-15
    - id: 5
      name: Multi-Agent Collaboration
      description: Build systems where multiple agents work together.
      acceptance_criteria:
      - Define agent roles and capabilities
      - Message passing between agents
      - Coordinator/orchestrator agent
      - Shared context and state
      - Conflict resolution
      hints:
        level1: 'Start with simple delegation: main agent calls specialist agents as tools.'
        level2: 'Pub/sub messaging: agents subscribe to topics, broadcast results.'
        level3: "from abc import ABC\nfrom typing import Dict, List, Any\nimport asyncio\nfrom dataclasses import dataclass\n\
          \n@dataclass\nclass AgentMessage:\n    from_agent: str\n    to_agent: str  # or 'broadcast'\n    content: str\n\
          \    metadata: Dict = None\n\nclass SpecialistAgent:\n    def __init__(self, name: str, role: str, tools: ToolRegistry):\n\
          \        self.name = name\n        self.role = role\n        self.tools = tools\n        self.inbox: asyncio.Queue\
          \ = asyncio.Queue()\n\n    async def process_message(self, message: AgentMessage) -> AgentMessage:\n        # Add\
          \ role context\n        prompt = f\"You are a {self.role} specialist.\\n\\nTask: {message.content}\"\n\n       \
          \ result = self.react_agent.run(prompt)\n\n        return AgentMessage(\n            from_agent=self.name,\n   \
          \         to_agent=message.from_agent,\n            content=result\n        )\n\nclass OrchestratorAgent:\n    def\
          \ __init__(self, specialists: Dict[str, SpecialistAgent]):\n        self.specialists = specialists\n        self.message_bus:\
          \ asyncio.Queue = asyncio.Queue()\n\n    async def run(self, task: str) -> str:\n        # Plan which specialists\
          \ to involve\n        plan = self._create_delegation_plan(task)\n\n        results = {}\n\n        for step in plan:\n\
          \            if step['type'] == 'delegate':\n                specialist = self.specialists[step['agent']]\n\n  \
          \              message = AgentMessage(\n                    from_agent='orchestrator',\n                    to_agent=step['agent'],\n\
          \                    content=step['task']\n                )\n\n                response = await specialist.process_message(message)\n\
          \                results[step['agent']] = response.content\n\n            elif step['type'] == 'parallel_delegate':\n\
          \                tasks = [\n                    self.specialists[agent].process_message(\n                     \
          \   AgentMessage('orchestrator', agent, subtask)\n                    )\n                    for agent, subtask\
          \ in step['assignments'].items()\n                ]\n                responses = await asyncio.gather(*tasks)\n\
          \                for resp in responses:\n                    results[resp.from_agent] = resp.content\n\n       \
          \     elif step['type'] == 'synthesize':\n                # Combine results from specialists\n                synthesis_prompt\
          \ = f'''Combine these specialist results:\n{json.dumps(results, indent=2)}\n\nOriginal task: {task}'''\n\n     \
          \           return self.llm.generate(synthesis_prompt)\n\n        return str(results)\n\n    def _create_delegation_plan(self,\
          \ task: str) -> List[Dict]:\n        specialist_info = {\n            name: agent.role\n            for name, agent\
          \ in self.specialists.items()\n        }\n\n        prompt = f'''You are an orchestrator with these specialists:\n\
          {json.dumps(specialist_info, indent=2)}\n\nCreate a delegation plan for: {task}\n\nReturn JSON list of steps:\n\
          {{\"type\": \"delegate\", \"agent\": \"agent_name\", \"task\": \"specific task\"}}\n{{\"type\": \"parallel_delegate\"\
          , \"assignments\": {{\"agent1\": \"task1\", \"agent2\": \"task2\"}}}}\n{{\"type\": \"synthesize\"}}'''\n\n     \
          \   return json.loads(self.llm.generate(prompt))\n\n# Example usage\nspecialists = {\n    'researcher': SpecialistAgent('researcher',\
          \ 'research and information gathering', research_tools),\n    'coder': SpecialistAgent('coder', 'writing and analyzing\
          \ code', coding_tools),\n    'writer': SpecialistAgent('writer', 'writing and editing text', writing_tools)\n}\n\
          \norchestrator = OrchestratorAgent(specialists)\nresult = await orchestrator.run(\"Research best practices for API\
          \ design and create a style guide\")"
      pitfalls:
      - Agents talk past each other without shared context
      - Delegation loops (A delegates to B delegates to A)
      - Orchestrator becomes bottleneck
      - No timeout for unresponsive agents
      concepts:
      - Multi-agent systems
      - Message passing
      - Orchestration patterns
      - Role specialization
      estimated_hours: 15-20
  llm-eval-framework:
    id: llm-eval-framework
    name: LLM Evaluation Framework
    description: Build comprehensive evaluation system for LLM applications. Critical for production AI.
    difficulty: advanced
    estimated_hours: 40-60
    prerequisites:
    - LLM APIs
    - Statistics
    - Python
    languages:
      recommended:
      - Python
      also_possible:
      - TypeScript
    resources:
    - name: Anthropic Eval Best Practices
      url: https://docs.anthropic.com/en/docs/build-with-claude/develop-tests
      type: documentation
    - name: LangSmith
      url: https://docs.smith.langchain.com/
      type: documentation
    - name: Braintrust
      url: https://www.braintrustdata.com/docs
      type: documentation
    milestones:
    - id: 1
      name: Dataset Management
      description: Create and version evaluation datasets.
      acceptance_criteria:
      - Define test case schema
      - Version datasets with git-like tracking
      - Import from CSV/JSON
      - Split into train/test/validation
      - Handle golden examples
      hints:
        level1: Test case = input + expected output + metadata. Store as JSON lines.
        level2: Hash dataset content for versioning. Track lineage of derived datasets.
        level3: "from dataclasses import dataclass, asdict\nfrom typing import List, Optional, Dict, Any\nimport hashlib\n\
          import json\nfrom pathlib import Path\nfrom datetime import datetime\n\n@dataclass\nclass TestCase:\n    id: str\n\
          \    input: str\n    expected_output: Optional[str] = None\n    context: Optional[Dict] = None\n    tags: List[str]\
          \ = None\n    metadata: Dict = None\n\n    def to_dict(self):\n        return asdict(self)\n\n@dataclass\nclass\
          \ Dataset:\n    name: str\n    version: str\n    cases: List[TestCase]\n    created_at: datetime\n    parent_version:\
          \ Optional[str] = None\n\n    @property\n    def hash(self) -> str:\n        content = json.dumps([c.to_dict() for\
          \ c in self.cases], sort_keys=True)\n        return hashlib.sha256(content.encode()).hexdigest()[:12]\n\nclass DatasetManager:\n\
          \    def __init__(self, storage_path: str = \".evals/datasets\"):\n        self.storage = Path(storage_path)\n \
          \       self.storage.mkdir(parents=True, exist_ok=True)\n\n    def create(self, name: str, cases: List[TestCase])\
          \ -> Dataset:\n        dataset = Dataset(\n            name=name,\n            version=f\"v1_{datetime.now().strftime('%Y%m%d')}\"\
          ,\n            cases=cases,\n            created_at=datetime.now()\n        )\n        self._save(dataset)\n   \
          \     return dataset\n\n    def add_cases(self, dataset: Dataset, new_cases: List[TestCase]) -> Dataset:\n     \
          \   new_dataset = Dataset(\n            name=dataset.name,\n            version=f\"v{len(self._get_versions(dataset.name))\
          \ + 1}_{datetime.now().strftime('%Y%m%d')}\",\n            cases=dataset.cases + new_cases,\n            created_at=datetime.now(),\n\
          \            parent_version=dataset.version\n        )\n        self._save(new_dataset)\n        return new_dataset\n\
          \n    def _save(self, dataset: Dataset):\n        path = self.storage / dataset.name / f\"{dataset.version}.jsonl\"\
          \n        path.parent.mkdir(exist_ok=True)\n\n        with open(path, 'w') as f:\n            for case in dataset.cases:\n\
          \                f.write(json.dumps(case.to_dict()) + '\\n')\n\n        # Save metadata\n        meta_path = self.storage\
          \ / dataset.name / f\"{dataset.version}.meta.json\"\n        with open(meta_path, 'w') as f:\n            json.dump({\n\
          \                'name': dataset.name,\n                'version': dataset.version,\n                'hash': dataset.hash,\n\
          \                'case_count': len(dataset.cases),\n                'created_at': dataset.created_at.isoformat(),\n\
          \                'parent_version': dataset.parent_version\n            }, f, indent=2)\n\n    def load(self, name:\
          \ str, version: str = None) -> Dataset:\n        if version is None:\n            version = self._get_latest_version(name)\n\
          \n        path = self.storage / name / f\"{version}.jsonl\"\n        cases = []\n        with open(path) as f:\n\
          \            for line in f:\n                cases.append(TestCase(**json.loads(line)))\n\n        meta_path = self.storage\
          \ / name / f\"{version}.meta.json\"\n        meta = json.loads(meta_path.read_text())\n\n        return Dataset(\n\
          \            name=name,\n            version=version,\n            cases=cases,\n            created_at=datetime.fromisoformat(meta['created_at']),\n\
          \            parent_version=meta.get('parent_version')\n        )\n\n    def split(self, dataset: Dataset, train_ratio=0.8,\
          \ seed=42) -> tuple:\n        import random\n        random.seed(seed)\n        cases = dataset.cases.copy()\n \
          \       random.shuffle(cases)\n\n        split_idx = int(len(cases) * train_ratio)\n        train_cases = cases[:split_idx]\n\
          \        test_cases = cases[split_idx:]\n\n        return (\n            Dataset(f\"{dataset.name}_train\", dataset.version,\
          \ train_cases, datetime.now()),\n            Dataset(f\"{dataset.name}_test\", dataset.version, test_cases, datetime.now())\n\
          \        )"
      pitfalls:
      - Test set leaking into training data
      - Version conflicts with concurrent edits
      - Large datasets slow to load
      - Not tracking dataset provenance
      concepts:
      - Dataset versioning
      - Train/test splits
      - Golden examples
      - Data lineage
      estimated_hours: 6-10
    - id: 2
      name: Evaluation Metrics
      description: Implement metrics for different evaluation types.
      acceptance_criteria:
      - Exact match and fuzzy match
      - Semantic similarity
      - LLM-as-judge grading
      - Custom metric functions
      - Aggregate scoring
      hints:
        level1: Start with exact match. Add fuzzy match (Levenshtein). Then semantic similarity.
        level2: 'LLM-as-judge: ask GPT-4 to rate on criteria. Parse structured score.'
        level3: "from abc import ABC, abstractmethod\nfrom typing import Any, Dict\nimport numpy as np\nfrom difflib import\
          \ SequenceMatcher\n\nclass Metric(ABC):\n    name: str\n\n    @abstractmethod\n    def score(self, expected: Any,\
          \ actual: Any, context: Dict = None) -> float:\n        '''Return score between 0 and 1'''\n        pass\n\nclass\
          \ ExactMatch(Metric):\n    name = \"exact_match\"\n\n    def __init__(self, case_sensitive: bool = False, strip:\
          \ bool = True):\n        self.case_sensitive = case_sensitive\n        self.strip = strip\n\n    def score(self,\
          \ expected: str, actual: str, context: Dict = None) -> float:\n        if self.strip:\n            expected, actual\
          \ = expected.strip(), actual.strip()\n        if not self.case_sensitive:\n            expected, actual = expected.lower(),\
          \ actual.lower()\n        return 1.0 if expected == actual else 0.0\n\nclass FuzzyMatch(Metric):\n    name = \"\
          fuzzy_match\"\n\n    def __init__(self, threshold: float = 0.8):\n        self.threshold = threshold\n\n    def\
          \ score(self, expected: str, actual: str, context: Dict = None) -> float:\n        ratio = SequenceMatcher(None,\
          \ expected.lower(), actual.lower()).ratio()\n        return ratio\n\nclass SemanticSimilarity(Metric):\n    name\
          \ = \"semantic_similarity\"\n\n    def __init__(self, embedding_service):\n        self.embedder = embedding_service\n\
          \n    def score(self, expected: str, actual: str, context: Dict = None) -> float:\n        embeddings = self.embedder.embed([expected,\
          \ actual])\n        similarity = np.dot(embeddings[0], embeddings[1])\n        return float(similarity)\n\nclass\
          \ LLMJudge(Metric):\n    name = \"llm_judge\"\n\n    def __init__(self, llm_client, criteria: str, rubric: Dict[int,\
          \ str] = None):\n        self.llm = llm_client\n        self.criteria = criteria\n        self.rubric = rubric or\
          \ {\n            5: \"Excellent - fully meets criteria\",\n            4: \"Good - mostly meets criteria with minor\
          \ issues\",\n            3: \"Acceptable - meets basic criteria\",\n            2: \"Poor - partially meets criteria\"\
          ,\n            1: \"Unacceptable - does not meet criteria\"\n        }\n\n    def score(self, expected: str, actual:\
          \ str, context: Dict = None) -> float:\n        rubric_text = \"\\n\".join([f\"{k}: {v}\" for k, v in self.rubric.items()])\n\
          \n        prompt = f'''Evaluate the following response based on this criteria:\n{self.criteria}\n\nExpected answer:\
          \ {expected}\nActual response: {actual}\n\nScoring rubric:\n{rubric_text}\n\nProvide your score (1-5) and brief\
          \ justification.\nFormat: SCORE: [number]\nREASON: [explanation]'''\n\n        response = self.llm.generate(prompt)\n\
          \n        # Parse score\n        import re\n        match = re.search(r'SCORE:\\s*(\\d)', response)\n        if\
          \ match:\n            score = int(match.group(1))\n            return score / 5.0\n        return 0.5  # Default\
          \ if parsing fails\n\nclass MultiCriteriaJudge(Metric):\n    name = \"multi_criteria\"\n\n    def __init__(self,\
          \ llm_client, criteria: List[Dict]):\n        self.llm = llm_client\n        self.criteria = criteria  # [{'name':\
          \ 'accuracy', 'weight': 0.5, 'description': '...'}]\n\n    def score(self, expected: str, actual: str, context:\
          \ Dict = None) -> float:\n        scores = {}\n\n        for criterion in self.criteria:\n            judge = LLMJudge(self.llm,\
          \ criterion['description'])\n            scores[criterion['name']] = judge.score(expected, actual, context)\n\n\
          \        # Weighted average\n        total_weight = sum(c['weight'] for c in self.criteria)\n        weighted_sum\
          \ = sum(\n            scores[c['name']] * c['weight']\n            for c in self.criteria\n        )\n\n       \
          \ return weighted_sum / total_weight"
      pitfalls:
      - Semantic similarity not calibrated (what's 'good' score?)
      - LLM judge inconsistent across runs
      - Custom metrics not between 0-1
      - Not handling edge cases (empty strings, None)
      concepts:
      - Evaluation metrics
      - Fuzzy matching
      - Semantic similarity
      - LLM-as-judge
      estimated_hours: 8-12
    - id: 3
      name: Evaluation Runner
      description: Run evaluations efficiently with caching and parallelism.
      acceptance_criteria:
      - Batch evaluation runs
      - Parallel LLM calls
      - Result caching to avoid re-running
      - Progress tracking
      - Resume from failures
      hints:
        level1: asyncio.gather for parallel LLM calls. Cache results by (input_hash, model, prompt_version).
        level2: Checkpoint results periodically. Resume by loading checkpoint and skipping completed.
        level3: "import asyncio\nfrom dataclasses import dataclass, field\nfrom typing import Callable, List, Dict\nimport\
          \ hashlib\nimport json\nfrom tqdm.asyncio import tqdm\n\n@dataclass\nclass EvalResult:\n    case_id: str\n    input:\
          \ str\n    expected: str\n    actual: str\n    scores: Dict[str, float]\n    latency_ms: float\n    error: str =\
          \ None\n\n@dataclass\nclass EvalRun:\n    id: str\n    dataset_name: str\n    model: str\n    timestamp: datetime\n\
          \    results: List[EvalResult] = field(default_factory=list)\n    aggregate_scores: Dict[str, float] = None\n\n\
          class EvalRunner:\n    def __init__(self,\n                 model_fn: Callable[[str], str],\n                 metrics:\
          \ List[Metric],\n                 cache_dir: str = \".evals/cache\",\n                 max_parallel: int = 10):\n\
          \        self.model_fn = model_fn\n        self.metrics = metrics\n        self.cache_dir = Path(cache_dir)\n  \
          \      self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.semaphore = asyncio.Semaphore(max_parallel)\n\
          \n    def _cache_key(self, input_text: str, model_id: str) -> str:\n        content = f\"{model_id}:{input_text}\"\
          \n        return hashlib.sha256(content.encode()).hexdigest()\n\n    def _get_cached(self, cache_key: str) -> Optional[str]:\n\
          \        cache_path = self.cache_dir / f\"{cache_key}.json\"\n        if cache_path.exists():\n            return\
          \ json.loads(cache_path.read_text())['output']\n        return None\n\n    def _set_cached(self, cache_key: str,\
          \ output: str):\n        cache_path = self.cache_dir / f\"{cache_key}.json\"\n        cache_path.write_text(json.dumps({'output':\
          \ output}))\n\n    async def _evaluate_case(self, case: TestCase, model_id: str) -> EvalResult:\n        async with\
          \ self.semaphore:\n            cache_key = self._cache_key(case.input, model_id)\n            cached = self._get_cached(cache_key)\n\
          \n            start = time.time()\n\n            if cached:\n                actual = cached\n            else:\n\
          \                try:\n                    actual = await asyncio.to_thread(self.model_fn, case.input)\n       \
          \             self._set_cached(cache_key, actual)\n                except Exception as e:\n                    return\
          \ EvalResult(\n                        case_id=case.id,\n                        input=case.input,\n           \
          \             expected=case.expected_output,\n                        actual=None,\n                        scores={},\n\
          \                        latency_ms=(time.time() - start) * 1000,\n                        error=str(e)\n      \
          \              )\n\n            latency = (time.time() - start) * 1000\n\n            # Calculate scores\n     \
          \       scores = {}\n            for metric in self.metrics:\n                try:\n                    scores[metric.name]\
          \ = metric.score(\n                        case.expected_output, actual, case.context\n                    )\n \
          \               except Exception as e:\n                    scores[metric.name] = None\n\n            return EvalResult(\n\
          \                case_id=case.id,\n                input=case.input,\n                expected=case.expected_output,\n\
          \                actual=actual,\n                scores=scores,\n                latency_ms=latency\n          \
          \  )\n\n    async def run(self, dataset: Dataset, model_id: str,\n                  checkpoint_every: int = 50)\
          \ -> EvalRun:\n        run = EvalRun(\n            id=f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n   \
          \         dataset_name=dataset.name,\n            model=model_id,\n            timestamp=datetime.now()\n      \
          \  )\n\n        # Check for existing checkpoint\n        checkpoint_path = self.cache_dir / f\"{run.id}_checkpoint.json\"\
          \n        completed_ids = set()\n        if checkpoint_path.exists():\n            checkpoint = json.loads(checkpoint_path.read_text())\n\
          \            run.results = [EvalResult(**r) for r in checkpoint['results']]\n            completed_ids = {r.case_id\
          \ for r in run.results}\n\n        # Filter to incomplete cases\n        pending = [c for c in dataset.cases if\
          \ c.id not in completed_ids]\n\n        # Run with progress bar\n        tasks = [self._evaluate_case(c, model_id)\
          \ for c in pending]\n\n        for i, result in enumerate(await tqdm.gather(*tasks)):\n            run.results.append(result)\n\
          \n            # Checkpoint periodically\n            if (i + 1) % checkpoint_every == 0:\n                self._save_checkpoint(run,\
          \ checkpoint_path)\n\n        # Calculate aggregates\n        run.aggregate_scores = self._aggregate(run.results)\n\
          \n        return run\n\n    def _aggregate(self, results: List[EvalResult]) -> Dict[str, float]:\n        aggregates\
          \ = {}\n\n        for metric in self.metrics:\n            scores = [r.scores.get(metric.name) for r in results\n\
          \                     if r.scores.get(metric.name) is not None]\n            if scores:\n                aggregates[f\"\
          {metric.name}_mean\"] = np.mean(scores)\n                aggregates[f\"{metric.name}_std\"] = np.std(scores)\n \
          \               aggregates[f\"{metric.name}_p50\"] = np.percentile(scores, 50)\n                aggregates[f\"{metric.name}_p95\"\
          ] = np.percentile(scores, 95)\n\n        return aggregates"
      pitfalls:
      - Rate limiting without backoff crashes run
      - Cache invalidation when prompt changes
      - Memory issues with large result sets
      - Lost progress on crash without checkpoints
      concepts:
      - Async evaluation
      - Result caching
      - Checkpointing
      - Progress tracking
      estimated_hours: 10-15
    - id: 4
      name: Reporting & Analysis
      description: Generate insights from evaluation results.
      acceptance_criteria:
      - Score breakdown by tags/categories
      - Regression detection vs baseline
      - Failure analysis (common patterns)
      - Export to HTML/PDF reports
      - CI/CD integration (pass/fail)
      hints:
        level1: Group results by tags. Calculate per-group metrics. Flag if below threshold.
        level2: 'Compare runs: significant difference = > 2 std devs. Use bootstrap for confidence.'
        level3: "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom jinja2 import Template\n\
          \nclass EvalAnalyzer:\n    def __init__(self):\n        self.significance_threshold = 0.05\n\n    def to_dataframe(self,\
          \ run: EvalRun) -> pd.DataFrame:\n        rows = []\n        for result in run.results:\n            row = {\n \
          \               'case_id': result.case_id,\n                'latency_ms': result.latency_ms,\n                'error':\
          \ result.error is not None,\n                **result.scores\n            }\n            rows.append(row)\n    \
          \    return pd.DataFrame(rows)\n\n    def breakdown_by_tag(self, run: EvalRun, dataset: Dataset) -> Dict:\n    \
          \    df = self.to_dataframe(run)\n\n        # Add tags from dataset\n        case_tags = {c.id: c.tags or [] for\
          \ c in dataset.cases}\n\n        breakdowns = {}\n        all_tags = set()\n        for tags in case_tags.values():\n\
          \            all_tags.update(tags)\n\n        for tag in all_tags:\n            tag_cases = [cid for cid, tags in\
          \ case_tags.items() if tag in tags]\n            tag_df = df[df['case_id'].isin(tag_cases)]\n\n            breakdowns[tag]\
          \ = {\n                'count': len(tag_df),\n                'metrics': {\n                    col: tag_df[col].mean()\n\
          \                    for col in df.columns\n                    if col not in ('case_id', 'latency_ms', 'error')\n\
          \                }\n            }\n\n        return breakdowns\n\n    def compare_runs(self, baseline: EvalRun,\
          \ current: EvalRun) -> Dict:\n        baseline_df = self.to_dataframe(baseline)\n        current_df = self.to_dataframe(current)\n\
          \n        comparisons = {}\n        metric_cols = [c for c in baseline_df.columns\n                      if c not\
          \ in ('case_id', 'latency_ms', 'error')]\n\n        for metric in metric_cols:\n            baseline_scores = baseline_df[metric].dropna()\n\
          \            current_scores = current_df[metric].dropna()\n\n            # Statistical test\n            t_stat,\
          \ p_value = stats.ttest_ind(baseline_scores, current_scores)\n\n            delta = current_scores.mean() - baseline_scores.mean()\n\
          \            delta_pct = (delta / baseline_scores.mean()) * 100 if baseline_scores.mean() != 0 else 0\n\n      \
          \      comparisons[metric] = {\n                'baseline_mean': baseline_scores.mean(),\n                'current_mean':\
          \ current_scores.mean(),\n                'delta': delta,\n                'delta_pct': delta_pct,\n           \
          \     'p_value': p_value,\n                'significant': p_value < self.significance_threshold,\n             \
          \   'regression': delta < 0 and p_value < self.significance_threshold\n            }\n\n        return comparisons\n\
          \n    def failure_analysis(self, run: EvalRun, threshold: float = 0.5) -> Dict:\n        failures = [r for r in\
          \ run.results\n                   if any(s < threshold for s in r.scores.values() if s is not None)]\n\n       \
          \ # Cluster by input patterns (simple: first 50 chars)\n        patterns = {}\n        for f in failures:\n    \
          \        pattern = f.input[:50]\n            if pattern not in patterns:\n                patterns[pattern] = []\n\
          \            patterns[pattern].append(f)\n\n        # Sort by frequency\n        sorted_patterns = sorted(patterns.items(),\
          \ key=lambda x: len(x[1]), reverse=True)\n\n        return {\n            'total_failures': len(failures),\n   \
          \         'failure_rate': len(failures) / len(run.results),\n            'top_patterns': sorted_patterns[:10]\n\
          \        }\n\n    def generate_report(self, run: EvalRun, baseline: EvalRun = None) -> str:\n        template =\
          \ Template('''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Eval Report: {{ run.id }}</title>\n    <style>\n   \
          \     body { font-family: sans-serif; margin: 40px; }\n        .metric { padding: 10px; margin: 10px; border: 1px\
          \ solid #ccc; }\n        .regression { background: #ffcccc; }\n        .improvement { background: #ccffcc; }\n \
          \   </style>\n</head>\n<body>\n    <h1>Evaluation Report</h1>\n    <p>Run ID: {{ run.id }}</p>\n    <p>Dataset:\
          \ {{ run.dataset_name }}</p>\n    <p>Model: {{ run.model }}</p>\n\n    <h2>Aggregate Scores</h2>\n    {% for metric,\
          \ value in run.aggregate_scores.items() %}\n    <div class=\"metric\">{{ metric }}: {{ \"%.3f\"|format(value) }}</div>\n\
          \    {% endfor %}\n\n    {% if comparison %}\n    <h2>Comparison to Baseline</h2>\n    {% for metric, comp in comparison.items()\
          \ %}\n    <div class=\"metric {{ 'regression' if comp.regression else 'improvement' if comp.delta > 0 else '' }}\"\
          >\n        {{ metric }}: {{ \"%.3f\"|format(comp.current_mean) }}\n        ({{ \"%+.1f\"|format(comp.delta_pct)\
          \ }}% vs baseline)\n        {% if comp.significant %}*{% endif %}\n    </div>\n    {% endfor %}\n    {% endif %}\n\
          </body>\n</html>\n        ''')\n\n        comparison = self.compare_runs(baseline, run) if baseline else None\n\n\
          \        return template.render(run=run, comparison=comparison)"
      pitfalls:
      - Small sample sizes give unreliable comparisons
      - Multiple comparisons without correction
      - Ignoring variance in aggregate scores
      - Report generation fails silently
      concepts:
      - Statistical testing
      - Regression detection
      - Failure analysis
      - Reporting
      estimated_hours: 8-12
  recommendation-engine:
    id: recommendation-engine
    name: Recommendation Engine
    description: Build a production recommendation system with collaborative filtering, content-based, and hybrid approaches.
    difficulty: intermediate
    estimated_hours: 35-50
    prerequisites:
    - Python
    - Linear algebra basics
    - Database fundamentals
    languages:
      recommended:
      - Python
      also_possible:
      - Go
      - Scala
    resources:
    - name: Surprise Library
      url: https://surpriselib.com/
      type: documentation
    - name: Netflix Recommendation Paper
      url: https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-BellKor-Solution.pdf
      type: paper
    milestones:
    - id: 1
      name: Collaborative Filtering
      description: Implement user-based and item-based collaborative filtering.
      acceptance_criteria:
      - User-item rating matrix
      - User similarity calculation
      - Item similarity calculation
      - K-nearest neighbors prediction
      - Handle cold start problem
      hints:
        level1: Cosine similarity between user rating vectors. Predict = weighted average of neighbors.
        level2: Item-based is more stable than user-based (items change less than users).
        level3: "import numpy as np\nfrom scipy.sparse import csr_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\
          \nclass CollaborativeFilter:\n    def __init__(self, k_neighbors: int = 20):\n        self.k = k_neighbors\n   \
          \     self.user_matrix = None\n        self.item_matrix = None\n        self.user_similarity = None\n        self.item_similarity\
          \ = None\n\n    def fit(self, ratings: List[tuple]):  # [(user_id, item_id, rating), ...]\n        # Build sparse\
          \ matrix\n        users = list(set(r[0] for r in ratings))\n        items = list(set(r[1] for r in ratings))\n\n\
          \        self.user_idx = {u: i for i, u in enumerate(users)}\n        self.item_idx = {it: i for i, it in enumerate(items)}\n\
          \        self.idx_user = {i: u for u, i in self.user_idx.items()}\n        self.idx_item = {i: it for it, i in self.item_idx.items()}\n\
          \n        # User-item matrix\n        rows, cols, data = [], [], []\n        for user, item, rating in ratings:\n\
          \            rows.append(self.user_idx[user])\n            cols.append(self.item_idx[item])\n            data.append(rating)\n\
          \n        self.user_matrix = csr_matrix(\n            (data, (rows, cols)),\n            shape=(len(users), len(items))\n\
          \        )\n        self.item_matrix = self.user_matrix.T\n\n        # Precompute similarities\n        self.user_similarity\
          \ = cosine_similarity(self.user_matrix)\n        self.item_similarity = cosine_similarity(self.item_matrix)\n\n\
          \    def predict_user_based(self, user_id, item_id) -> float:\n        if user_id not in self.user_idx or item_id\
          \ not in self.item_idx:\n            return self._global_mean()\n\n        u_idx = self.user_idx[user_id]\n    \
          \    i_idx = self.item_idx[item_id]\n\n        # Find users who rated this item\n        item_raters = self.user_matrix[:,\
          \ i_idx].nonzero()[0]\n\n        if len(item_raters) == 0:\n            return self._user_mean(u_idx)\n\n      \
          \  # Get similarities and ratings\n        similarities = self.user_similarity[u_idx, item_raters]\n        ratings\
          \ = np.array(self.user_matrix[item_raters, i_idx].todense()).flatten()\n\n        # Top-k neighbors\n        top_k\
          \ = np.argsort(similarities)[-self.k:]\n\n        sim_sum = np.sum(np.abs(similarities[top_k]))\n        if sim_sum\
          \ == 0:\n            return self._user_mean(u_idx)\n\n        weighted_sum = np.sum(similarities[top_k] * ratings[top_k])\n\
          \        return weighted_sum / sim_sum\n\n    def predict_item_based(self, user_id, item_id) -> float:\n       \
          \ if user_id not in self.user_idx or item_id not in self.item_idx:\n            return self._global_mean()\n\n \
          \       u_idx = self.user_idx[user_id]\n        i_idx = self.item_idx[item_id]\n\n        # Find items rated by\
          \ this user\n        user_items = self.user_matrix[u_idx, :].nonzero()[1]\n\n        if len(user_items) == 0:\n\
          \            return self._item_mean(i_idx)\n\n        # Get similarities and ratings\n        similarities = self.item_similarity[i_idx,\
          \ user_items]\n        ratings = np.array(self.user_matrix[u_idx, user_items].todense()).flatten()\n\n        #\
          \ Top-k similar items\n        top_k = np.argsort(similarities)[-self.k:]\n\n        sim_sum = np.sum(np.abs(similarities[top_k]))\n\
          \        if sim_sum == 0:\n            return self._item_mean(i_idx)\n\n        weighted_sum = np.sum(similarities[top_k]\
          \ * ratings[top_k])\n        return weighted_sum / sim_sum\n\n    def recommend(self, user_id, n: int = 10) -> List[tuple]:\n\
          \        if user_id not in self.user_idx:\n            return self._popular_items(n)\n\n        u_idx = self.user_idx[user_id]\n\
          \        rated_items = set(self.user_matrix[u_idx, :].nonzero()[1])\n\n        predictions = []\n        for i_idx\
          \ in range(self.user_matrix.shape[1]):\n            if i_idx not in rated_items:\n                item_id = self.idx_item[i_idx]\n\
          \                pred = self.predict_item_based(user_id, item_id)\n                predictions.append((item_id,\
          \ pred))\n\n        predictions.sort(key=lambda x: x[1], reverse=True)\n        return predictions[:n]"
      pitfalls:
      - Sparse matrix operations are memory-intensive
      - Precomputing all similarities doesn't scale
      - 'Cold start: new users/items have no data'
      - Popularity bias in recommendations
      concepts:
      - Collaborative filtering
      - Similarity metrics
      - K-NN
      - Sparse matrices
      estimated_hours: 8-12
    - id: 2
      name: Matrix Factorization
      description: Implement SVD and ALS for latent factor models.
      acceptance_criteria:
      - SVD decomposition of rating matrix
      - ALS (Alternating Least Squares)
      - Regularization to prevent overfitting
      - Implicit feedback handling
      - Hyperparameter tuning
      hints:
        level1: Rating â‰ˆ User_vector Â· Item_vector. Learn vectors via gradient descent.
        level2: 'ALS: fix users, optimize items. Then fix items, optimize users. Repeat.'
        level3: "import numpy as np\n\nclass MatrixFactorization:\n    def __init__(self, n_factors: int = 50, learning_rate:\
          \ float = 0.01,\n                 regularization: float = 0.02, n_epochs: int = 20):\n        self.n_factors = n_factors\n\
          \        self.lr = learning_rate\n        self.reg = regularization\n        self.n_epochs = n_epochs\n\n    def\
          \ fit(self, ratings: List[tuple]):\n        users = list(set(r[0] for r in ratings))\n        items = list(set(r[1]\
          \ for r in ratings))\n\n        self.user_idx = {u: i for i, u in enumerate(users)}\n        self.item_idx = {it:\
          \ i for i, it in enumerate(items)}\n\n        n_users = len(users)\n        n_items = len(items)\n\n        # Initialize\
          \ factors randomly\n        self.user_factors = np.random.normal(0, 0.1, (n_users, self.n_factors))\n        self.item_factors\
          \ = np.random.normal(0, 0.1, (n_items, self.n_factors))\n\n        # Biases\n        self.global_mean = np.mean([r[2]\
          \ for r in ratings])\n        self.user_bias = np.zeros(n_users)\n        self.item_bias = np.zeros(n_items)\n\n\
          \        # SGD training\n        for epoch in range(self.n_epochs):\n            np.random.shuffle(ratings)\n  \
          \          total_error = 0\n\n            for user, item, rating in ratings:\n                u_idx = self.user_idx[user]\n\
          \                i_idx = self.item_idx[item]\n\n                # Prediction\n                pred = (self.global_mean\
          \ +\n                       self.user_bias[u_idx] +\n                       self.item_bias[i_idx] +\n          \
          \             np.dot(self.user_factors[u_idx], self.item_factors[i_idx]))\n\n                error = rating - pred\n\
          \                total_error += error ** 2\n\n                # Update biases\n                self.user_bias[u_idx]\
          \ += self.lr * (error - self.reg * self.user_bias[u_idx])\n                self.item_bias[i_idx] += self.lr * (error\
          \ - self.reg * self.item_bias[i_idx])\n\n                # Update factors\n                user_factor = self.user_factors[u_idx].copy()\n\
          \                self.user_factors[u_idx] += self.lr * (\n                    error * self.item_factors[i_idx] -\
          \ self.reg * user_factor\n                )\n                self.item_factors[i_idx] += self.lr * (\n         \
          \           error * user_factor - self.reg * self.item_factors[i_idx]\n                )\n\n            rmse = np.sqrt(total_error\
          \ / len(ratings))\n            print(f\"Epoch {epoch + 1}: RMSE = {rmse:.4f}\")\n\n    def predict(self, user_id,\
          \ item_id) -> float:\n        if user_id not in self.user_idx or item_id not in self.item_idx:\n            return\
          \ self.global_mean\n\n        u_idx = self.user_idx[user_id]\n        i_idx = self.item_idx[item_id]\n\n       \
          \ return (self.global_mean +\n               self.user_bias[u_idx] +\n               self.item_bias[i_idx] +\n \
          \              np.dot(self.user_factors[u_idx], self.item_factors[i_idx]))\n\n    def recommend(self, user_id, n:\
          \ int = 10, exclude_rated: set = None) -> List[tuple]:\n        if user_id not in self.user_idx:\n            return\
          \ []\n\n        u_idx = self.user_idx[user_id]\n        exclude_rated = exclude_rated or set()\n\n        # Score\
          \ all items\n        scores = (self.global_mean +\n                 self.user_bias[u_idx] +\n                 self.item_bias\
          \ +\n                 np.dot(self.item_factors, self.user_factors[u_idx]))\n\n        # Filter and sort\n      \
          \  items_scores = [\n            (self.idx_item[i], scores[i])\n            for i in range(len(scores))\n      \
          \      if self.idx_item.get(i) not in exclude_rated\n        ]\n        items_scores.sort(key=lambda x: x[1], reverse=True)\n\
          \n        return items_scores[:n]"
      pitfalls:
      - Learning rate too high causes divergence
      - Overfitting without regularization
      - Implicit data needs different loss function
      - Factor initialization affects convergence
      concepts:
      - Matrix factorization
      - SVD
      - Stochastic gradient descent
      - Regularization
      estimated_hours: 8-12
    - id: 3
      name: Content-Based Filtering
      description: Recommend based on item features and user preferences.
      acceptance_criteria:
      - Extract item features (text, categories, tags)
      - Build user preference profile
      - Content similarity matching
      - Combine with collaborative signals
      - Explain recommendations
      hints:
        level1: TF-IDF on item descriptions. User profile = weighted avg of liked item vectors.
        level2: For explainability, track which features contributed most to recommendation.
        level3: "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\
          import numpy as np\n\nclass ContentBasedRecommender:\n    def __init__(self):\n        self.tfidf = TfidfVectorizer(max_features=5000,\
          \ stop_words='english')\n        self.item_features = None\n        self.item_ids = []\n\n    def fit(self, items:\
          \ List[Dict]):\n        # items = [{'id': '1', 'title': '...', 'description': '...', 'tags': [...]}]\n        self.item_ids\
          \ = [item['id'] for item in items]\n        self.item_idx = {id: i for i, id in enumerate(self.item_ids)}\n\n  \
          \      # Combine text features\n        texts = []\n        for item in items:\n            text = f\"{item.get('title',\
          \ '')} {item.get('description', '')} {' '.join(item.get('tags', []))}\"\n            texts.append(text)\n\n    \
          \    # TF-IDF matrix\n        self.item_features = self.tfidf.fit_transform(texts)\n\n        # Precompute item\
          \ similarity\n        self.item_similarity = cosine_similarity(self.item_features)\n\n    def build_user_profile(self,\
          \ user_ratings: List[tuple]) -> np.ndarray:\n        # user_ratings = [(item_id, rating), ...]\n        if not user_ratings:\n\
          \            return np.zeros(self.item_features.shape[1])\n\n        # Weighted average of item vectors\n      \
          \  profile = np.zeros(self.item_features.shape[1])\n        total_weight = 0\n\n        for item_id, rating in user_ratings:\n\
          \            if item_id in self.item_idx:\n                idx = self.item_idx[item_id]\n                weight\
          \ = rating - 3  # Center around neutral rating\n                profile += weight * self.item_features[idx].toarray().flatten()\n\
          \                total_weight += abs(weight)\n\n        if total_weight > 0:\n            profile /= total_weight\n\
          \n        return profile\n\n    def recommend(self, user_ratings: List[tuple], n: int = 10) -> List[Dict]:\n   \
          \     user_profile = self.build_user_profile(user_ratings)\n\n        # Similarity to user profile\n        scores\
          \ = cosine_similarity([user_profile], self.item_features)[0]\n\n        # Exclude already rated\n        rated_items\
          \ = {r[0] for r in user_ratings}\n\n        recommendations = []\n        for idx in np.argsort(scores)[::-1]:\n\
          \            item_id = self.item_ids[idx]\n            if item_id not in rated_items:\n                # Get explanation\n\
          \                explanation = self._explain(idx, user_profile)\n                recommendations.append({\n    \
          \                'item_id': item_id,\n                    'score': float(scores[idx]),\n                    'explanation':\
          \ explanation\n                })\n                if len(recommendations) >= n:\n                    break\n\n\
          \        return recommendations\n\n    def _explain(self, item_idx: int, user_profile: np.ndarray, top_k: int =\
          \ 3) -> str:\n        item_vector = self.item_features[item_idx].toarray().flatten()\n\n        # Find overlapping\
          \ important features\n        feature_names = self.tfidf.get_feature_names_out()\n\n        contributions = item_vector\
          \ * user_profile\n        top_indices = np.argsort(contributions)[::-1][:top_k]\n\n        top_features = [feature_names[i]\
          \ for i in top_indices if contributions[i] > 0]\n\n        if top_features:\n            return f\"Recommended because\
          \ you liked items with: {', '.join(top_features)}\"\n        return \"Recommended based on your preferences\"\n\n\
          \    def similar_items(self, item_id: str, n: int = 10) -> List[tuple]:\n        if item_id not in self.item_idx:\n\
          \            return []\n\n        idx = self.item_idx[item_id]\n        similarities = self.item_similarity[idx]\n\
          \n        similar = []\n        for i in np.argsort(similarities)[::-1][1:n+1]:  # Skip self\n            similar.append((self.item_ids[i],\
          \ float(similarities[i])))\n\n        return similar"
      pitfalls:
      - Feature extraction misses important signals
      - User profile dominated by few items
      - 'Filter bubble: only recommend similar items'
      - Explanations don't match user perception
      concepts:
      - Content-based filtering
      - TF-IDF
      - User profiling
      - Explainability
      estimated_hours: 6-10
    - id: 4
      name: Production System
      description: Build production-ready recommendation service.
      acceptance_criteria:
      - Real-time recommendation API
      - Batch candidate generation
      - A/B testing framework
      - Metrics and monitoring
      - Model versioning
      hints:
        level1: 'Two-stage: batch generates candidates, real-time ranks them.'
        level2: Track impressions, clicks, conversions. Calculate CTR, conversion rate.
        level3: "from fastapi import FastAPI, BackgroundTasks\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\
          import asyncio\nimport random\n\napp = FastAPI()\n\nclass RecommendationRequest(BaseModel):\n    user_id: str\n\
          \    context: Optional[Dict] = {}\n    n: int = 10\n    experiment_id: Optional[str] = None\n\nclass RecommendationResponse(BaseModel):\n\
          \    items: List[Dict]\n    model_version: str\n    experiment_group: str\n\nclass RecommendationService:\n    def\
          \ __init__(self):\n        self.models = {}  # version -> model\n        self.candidate_cache = {}  # user_id ->\
          \ candidates\n        self.experiments = {}  # experiment_id -> config\n\n    def load_model(self, version: str,\
          \ model):\n        self.models[version] = model\n\n    async def get_recommendations(self, request: RecommendationRequest)\
          \ -> RecommendationResponse:\n        # Determine experiment group\n        experiment_group = self._assign_experiment_group(\n\
          \            request.user_id, request.experiment_id\n        )\n        model_version = self._get_model_for_group(experiment_group)\n\
          \n        # Get candidates (from cache or generate)\n        candidates = await self._get_candidates(request.user_id)\n\
          \n        # Rank candidates\n        model = self.models[model_version]\n        ranked = model.rank(request.user_id,\
          \ candidates, request.context)\n\n        # Apply business rules\n        final = self._apply_business_rules(ranked,\
          \ request.context)\n\n        return RecommendationResponse(\n            items=final[:request.n],\n           \
          \ model_version=model_version,\n            experiment_group=experiment_group\n        )\n\n    def _assign_experiment_group(self,\
          \ user_id: str, experiment_id: str) -> str:\n        if not experiment_id or experiment_id not in self.experiments:\n\
          \            return \"control\"\n\n        config = self.experiments[experiment_id]\n        # Deterministic assignment\
          \ based on user_id\n        hash_val = hash(f\"{user_id}_{experiment_id}\") % 100\n\n        cumulative = 0\n  \
          \      for group, percentage in config['groups'].items():\n            cumulative += percentage\n            if\
          \ hash_val < cumulative:\n                return group\n\n        return \"control\"\n\n# Metrics tracking\nclass\
          \ MetricsTracker:\n    def __init__(self):\n        self.events = []\n\n    def track_impression(self, user_id:\
          \ str, items: List[str],\n                        experiment_group: str, model_version: str):\n        self.events.append({\n\
          \            'type': 'impression',\n            'user_id': user_id,\n            'items': items,\n            'experiment_group':\
          \ experiment_group,\n            'model_version': model_version,\n            'timestamp': datetime.now()\n    \
          \    })\n\n    def track_click(self, user_id: str, item_id: str, position: int):\n        self.events.append({\n\
          \            'type': 'click',\n            'user_id': user_id,\n            'item_id': item_id,\n            'position':\
          \ position,\n            'timestamp': datetime.now()\n        })\n\n    def calculate_metrics(self, experiment_id:\
          \ str) -> Dict:\n        impressions = [e for e in self.events if e['type'] == 'impression']\n        clicks = [e\
          \ for e in self.events if e['type'] == 'click']\n\n        by_group = {}\n        for group in set(e.get('experiment_group')\
          \ for e in impressions):\n            group_impressions = [e for e in impressions if e.get('experiment_group') ==\
          \ group]\n            group_users = set(e['user_id'] for e in group_impressions)\n            group_clicks = [e\
          \ for e in clicks if e['user_id'] in group_users]\n\n            by_group[group] = {\n                'impressions':\
          \ len(group_impressions),\n                'clicks': len(group_clicks),\n                'ctr': len(group_clicks)\
          \ / max(1, len(group_impressions)),\n                'unique_users': len(group_users)\n            }\n\n       \
          \ return by_group"
      pitfalls:
      - Cold cache causes latency spikes
      - A/B test assignment not deterministic
      - Metrics delayed lose temporal correlation
      - Model drift not detected
      concepts:
      - Two-stage recommendation
      - A/B testing
      - Metrics tracking
      - Model serving
      estimated_hours: 12-16
  rest-api-design:
    id: rest-api-design
    name: Production REST API
    description: Build a production-grade REST API with authentication, validation, rate limiting, and proper error handling.
    difficulty: beginner
    estimated_hours: 20-30
    prerequisites:
    - HTTP basics
    - JSON
    - Database basics
    languages:
      recommended:
      - Go
      - Python
      - Node.js
      also_possible:
      - Rust
      - Java
    resources:
    - name: REST API Design Best Practices
      url: https://restfulapi.net/
      type: tutorial
    - name: OpenAPI Specification
      url: https://swagger.io/specification/
      type: documentation
    milestones:
    - id: 1
      name: CRUD Operations
      description: Implement Create, Read, Update, Delete operations with proper HTTP methods.
      acceptance_criteria:
      - POST /resources creates new resource, returns 201
      - GET /resources returns paginated list
      - GET /resources/:id returns single resource or 404
      - PUT/PATCH /resources/:id updates resource
      - DELETE /resources/:id removes resource
      hints:
        level1: 'Use proper HTTP methods: GET for read, POST for create, PUT for replace, PATCH for update, DELETE for remove.'
        level2: 'Return appropriate status codes: 200 OK, 201 Created, 204 No Content, 400 Bad Request, 404 Not Found.'
        level3: "from fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel\nfrom typing import List,\
          \ Optional\nfrom uuid import uuid4\n\napp = FastAPI()\n\nclass Item(BaseModel):\n    id: Optional[str] = None\n\
          \    name: str\n    description: Optional[str] = None\n    price: float\n\n# In-memory storage\nitems_db: dict[str,\
          \ Item] = {}\n\n@app.post(\"/items\", status_code=status.HTTP_201_CREATED)\ndef create_item(item: Item) -> Item:\n\
          \    item.id = str(uuid4())\n    items_db[item.id] = item\n    return item\n\n@app.get(\"/items\")\ndef list_items(skip:\
          \ int = 0, limit: int = 10) -> List[Item]:\n    items = list(items_db.values())\n    return items[skip:skip + limit]\n\
          \n@app.get(\"/items/{item_id}\")\ndef get_item(item_id: str) -> Item:\n    if item_id not in items_db:\n       \
          \ raise HTTPException(status_code=404, detail=\"Item not found\")\n    return items_db[item_id]\n\n@app.put(\"/items/{item_id}\"\
          )\ndef update_item(item_id: str, item: Item) -> Item:\n    if item_id not in items_db:\n        raise HTTPException(status_code=404,\
          \ detail=\"Item not found\")\n    item.id = item_id\n    items_db[item_id] = item\n    return item\n\n@app.delete(\"\
          /items/{item_id}\", status_code=status.HTTP_204_NO_CONTENT)\ndef delete_item(item_id: str):\n    if item_id not\
          \ in items_db:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    del items_db[item_id]"
      pitfalls:
      - Using POST for updates or GET for mutations
      - Returning 200 for resource creation instead of 201
      - Not handling concurrent updates
      - Missing Content-Type headers
      concepts:
      - REST principles
      - HTTP methods
      - Status codes
      - Resource modeling
      estimated_hours: 4-6
    - id: 2
      name: Input Validation
      description: Validate all input data and return meaningful errors.
      acceptance_criteria:
      - Validate request body against schema
      - Validate query parameters
      - Validate path parameters
      - Return detailed error messages
      - Sanitize input for security
      hints:
        level1: Use Pydantic (Python), Zod (TypeScript), or similar for schema validation.
        level2: Return 400 Bad Request with field-level errors. Include error codes for i18n.
        level3: "from pydantic import BaseModel, Field, validator, ValidationError\nfrom fastapi import FastAPI, HTTPException,\
          \ Query\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\nfrom\
          \ typing import List\nimport re\n\nclass CreateUserRequest(BaseModel):\n    email: str = Field(..., description=\"\
          User email address\")\n    password: str = Field(..., min_length=8, max_length=128)\n    username: str = Field(...,\
          \ min_length=3, max_length=50)\n\n    @validator('email')\n    def validate_email(cls, v):\n        pattern = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\
          .[a-zA-Z0-9-.]+$'\n        if not re.match(pattern, v):\n            raise ValueError('Invalid email format')\n\
          \        return v.lower()\n\n    @validator('password')\n    def validate_password(cls, v):\n        if not any(c.isupper()\
          \ for c in v):\n            raise ValueError('Password must contain uppercase letter')\n        if not any(c.isdigit()\
          \ for c in v):\n            raise ValueError('Password must contain digit')\n        return v\n\n    @validator('username')\n\
          \    def validate_username(cls, v):\n        if not re.match(r'^[a-zA-Z0-9_]+$', v):\n            raise ValueError('Username\
          \ can only contain letters, numbers, underscores')\n        return v\n\nclass ErrorDetail(BaseModel):\n    field:\
          \ str\n    message: str\n    code: str\n\nclass ErrorResponse(BaseModel):\n    error: str\n    details: List[ErrorDetail]\n\
          \n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request, exc):\n    errors\
          \ = []\n    for error in exc.errors():\n        field = '.'.join(str(loc) for loc in error['loc'][1:])  # Skip 'body'\n\
          \        errors.append(ErrorDetail(\n            field=field,\n            message=error['msg'],\n            code=f\"\
          validation.{error['type']}\"\n        ))\n\n    return JSONResponse(\n        status_code=400,\n        content=ErrorResponse(\n\
          \            error=\"Validation failed\",\n            details=errors\n        ).dict()\n    )\n\n# Query parameter\
          \ validation\n@app.get(\"/users\")\ndef list_users(\n    page: int = Query(1, ge=1, description=\"Page number\"\
          ),\n    per_page: int = Query(20, ge=1, le=100, description=\"Items per page\"),\n    sort_by: str = Query(\"created_at\"\
          , regex=\"^(created_at|name|email)$\")\n):\n    # Validated parameters are guaranteed to be valid here\n    pass"
      pitfalls:
      - Only validating types, not business rules
      - Exposing internal error details
      - Not validating query/path params
      - SQL/NoSQL injection through unvalidated input
      concepts:
      - Schema validation
      - Error handling
      - Input sanitization
      - Security
      estimated_hours: 4-6
    - id: 3
      name: Authentication & Authorization
      description: Implement JWT authentication and role-based access control.
      acceptance_criteria:
      - User registration and login
      - JWT token generation and validation
      - Token refresh mechanism
      - Role-based access control (RBAC)
      - Protected routes
      hints:
        level1: JWT = header.payload.signature. Store user ID and roles in payload.
        level2: Use short-lived access tokens (15min) + long-lived refresh tokens (7d).
        level3: "import jwt\nfrom datetime import datetime, timedelta\nfrom fastapi import Depends, HTTPException, status\n\
          from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom passlib.context import CryptContext\n\
          from functools import wraps\n\nSECRET_KEY = \"your-secret-key\"  # Use env var in production\nALGORITHM = \"HS256\"\
          \nACCESS_TOKEN_EXPIRE_MINUTES = 15\nREFRESH_TOKEN_EXPIRE_DAYS = 7\n\npwd_context = CryptContext(schemes=[\"bcrypt\"\
          ], deprecated=\"auto\")\nsecurity = HTTPBearer()\n\ndef create_access_token(user_id: str, roles: list[str]) -> str:\n\
          \    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    payload = {\n        \"sub\"\
          : user_id,\n        \"roles\": roles,\n        \"exp\": expire,\n        \"type\": \"access\"\n    }\n    return\
          \ jwt.encode(payload, SECRET_KEY, algorithm=ALGORITHM)\n\ndef create_refresh_token(user_id: str) -> str:\n    expire\
          \ = datetime.utcnow() + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)\n    payload = {\n        \"sub\": user_id,\n\
          \        \"exp\": expire,\n        \"type\": \"refresh\"\n    }\n    return jwt.encode(payload, SECRET_KEY, algorithm=ALGORITHM)\n\
          \ndef verify_token(token: str) -> dict:\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n\
          \        return payload\n    except jwt.ExpiredSignatureError:\n        raise HTTPException(status_code=401, detail=\"\
          Token expired\")\n    except jwt.InvalidTokenError:\n        raise HTTPException(status_code=401, detail=\"Invalid\
          \ token\")\n\nasync def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    payload\
          \ = verify_token(credentials.credentials)\n    if payload.get(\"type\") != \"access\":\n        raise HTTPException(status_code=401,\
          \ detail=\"Invalid token type\")\n    return payload\n\ndef require_roles(*required_roles):\n    def decorator(func):\n\
          \        @wraps(func)\n        async def wrapper(*args, current_user: dict = Depends(get_current_user), **kwargs):\n\
          \            user_roles = set(current_user.get(\"roles\", []))\n            if not user_roles.intersection(required_roles):\n\
          \                raise HTTPException(status_code=403, detail=\"Insufficient permissions\")\n            return await\
          \ func(*args, current_user=current_user, **kwargs)\n        return wrapper\n    return decorator\n\n@app.post(\"\
          /auth/login\")\ndef login(email: str, password: str):\n    user = get_user_by_email(email)\n    if not user or not\
          \ pwd_context.verify(password, user.hashed_password):\n        raise HTTPException(status_code=401, detail=\"Invalid\
          \ credentials\")\n\n    return {\n        \"access_token\": create_access_token(user.id, user.roles),\n        \"\
          refresh_token\": create_refresh_token(user.id),\n        \"token_type\": \"bearer\"\n    }\n\n@app.post(\"/auth/refresh\"\
          )\ndef refresh(refresh_token: str):\n    payload = verify_token(refresh_token)\n    if payload.get(\"type\") !=\
          \ \"refresh\":\n        raise HTTPException(status_code=401, detail=\"Invalid token type\")\n\n    user = get_user_by_id(payload[\"\
          sub\"])\n    return {\n        \"access_token\": create_access_token(user.id, user.roles),\n        \"token_type\"\
          : \"bearer\"\n    }\n\n@app.get(\"/admin/users\")\n@require_roles(\"admin\")\nasync def list_all_users(current_user:\
          \ dict = Depends(get_current_user)):\n    return get_all_users()"
      pitfalls:
      - Storing secrets in code
      - No token expiration or too long expiry
      - Not invalidating tokens on password change
      - RBAC bypass through direct object access
      concepts:
      - JWT tokens
      - Password hashing
      - RBAC
      - Token refresh
      estimated_hours: 6-8
    - id: 4
      name: Rate Limiting & Throttling
      description: Protect API from abuse with rate limiting.
      acceptance_criteria:
      - Per-user rate limits
      - Per-endpoint rate limits
      - Sliding window algorithm
      - Rate limit headers in response
      - Graceful degradation
      hints:
        level1: Token bucket or sliding window. Store counts in Redis for distributed.
        level2: Return X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset headers.
        level3: "import time\nimport redis\nfrom fastapi import Request, HTTPException\nfrom starlette.middleware.base import\
          \ BaseHTTPMiddleware\n\nclass RateLimiter:\n    def __init__(self, redis_client: redis.Redis):\n        self.redis\
          \ = redis_client\n\n    def is_allowed(self, key: str, limit: int, window_seconds: int) -> tuple[bool, dict]:\n\
          \        now = time.time()\n        window_start = now - window_seconds\n\n        pipe = self.redis.pipeline()\n\
          \n        # Remove old entries\n        pipe.zremrangebyscore(key, 0, window_start)\n\n        # Count current entries\n\
          \        pipe.zcard(key)\n\n        # Add current request\n        pipe.zadd(key, {str(now): now})\n\n        #\
          \ Set expiry\n        pipe.expire(key, window_seconds)\n\n        results = pipe.execute()\n        current_count\
          \ = results[1]\n\n        remaining = max(0, limit - current_count - 1)\n        reset_time = int(now + window_seconds)\n\
          \n        headers = {\n            \"X-RateLimit-Limit\": str(limit),\n            \"X-RateLimit-Remaining\": str(remaining),\n\
          \            \"X-RateLimit-Reset\": str(reset_time)\n        }\n\n        return current_count < limit, headers\n\
          \nclass RateLimitMiddleware(BaseHTTPMiddleware):\n    def __init__(self, app, redis_client: redis.Redis, default_limit:\
          \ int = 100):\n        super().__init__(app)\n        self.limiter = RateLimiter(redis_client)\n        self.default_limit\
          \ = default_limit\n\n        # Per-endpoint limits\n        self.endpoint_limits = {\n            \"/auth/login\"\
          : (5, 60),     # 5 requests per minute\n            \"/auth/register\": (3, 60),  # 3 requests per minute\n    \
          \        \"/api/search\": (30, 60),    # 30 requests per minute\n        }\n\n    async def dispatch(self, request:\
          \ Request, call_next):\n        # Get identifier (user ID or IP)\n        user_id = getattr(request.state, 'user_id',\
          \ None)\n        identifier = user_id or request.client.host\n\n        # Get limit for endpoint\n        path =\
          \ request.url.path\n        limit, window = self.endpoint_limits.get(path, (self.default_limit, 60))\n\n       \
          \ # Check rate limit\n        key = f\"ratelimit:{identifier}:{path}\"\n        allowed, headers = self.limiter.is_allowed(key,\
          \ limit, window)\n\n        if not allowed:\n            return JSONResponse(\n                status_code=429,\n\
          \                content={\"error\": \"Rate limit exceeded\", \"retry_after\": headers[\"X-RateLimit-Reset\"]},\n\
          \                headers=headers\n            )\n\n        response = await call_next(request)\n\n        # Add\
          \ rate limit headers\n        for header, value in headers.items():\n            response.headers[header] = value\n\
          \n        return response\n\n# Distributed rate limiting with token bucket\nclass TokenBucket:\n    def __init__(self,\
          \ redis_client: redis.Redis, capacity: int, refill_rate: float):\n        self.redis = redis_client\n        self.capacity\
          \ = capacity\n        self.refill_rate = refill_rate  # tokens per second\n\n    def consume(self, key: str, tokens:\
          \ int = 1) -> bool:\n        now = time.time()\n\n        # Lua script for atomic operation\n        script = '''\n\
          \        local key = KEYS[1]\n        local capacity = tonumber(ARGV[1])\n        local refill_rate = tonumber(ARGV[2])\n\
          \        local tokens = tonumber(ARGV[3])\n        local now = tonumber(ARGV[4])\n\n        local bucket = redis.call('HMGET',\
          \ key, 'tokens', 'last_update')\n        local current_tokens = tonumber(bucket[1]) or capacity\n        local last_update\
          \ = tonumber(bucket[2]) or now\n\n        local elapsed = now - last_update\n        local refill = elapsed * refill_rate\n\
          \        current_tokens = math.min(capacity, current_tokens + refill)\n\n        if current_tokens >= tokens then\n\
          \            current_tokens = current_tokens - tokens\n            redis.call('HMSET', key, 'tokens', current_tokens,\
          \ 'last_update', now)\n            redis.call('EXPIRE', key, 3600)\n            return 1\n        end\n        return\
          \ 0\n        '''\n\n        result = self.redis.eval(script, 1, key, self.capacity, self.refill_rate, tokens, now)\n\
          \        return result == 1"
      pitfalls:
      - Rate limits per IP bypass with proxies
      - Not handling Redis failures gracefully
      - Clock skew in distributed systems
      - Rate limit headers revealing too much info
      concepts:
      - Rate limiting algorithms
      - Sliding window
      - Token bucket
      - Redis
      estimated_hours: 5-7
  api-gateway:
    id: api-gateway
    name: API Gateway
    description: Build an API gateway that handles routing, authentication, rate limiting, and request transformation.
    difficulty: advanced
    estimated_hours: 50-70
    prerequisites:
    - REST APIs
    - Networking
    - Load balancing concepts
    languages:
      recommended:
      - Go
      - Rust
      also_possible:
      - Node.js
      - Java
    resources:
    - name: Kong Gateway
      url: https://docs.konghq.com/
      type: reference
    - name: NGINX as API Gateway
      url: https://www.nginx.com/blog/deploying-nginx-plus-as-an-api-gateway/
      type: article
    milestones:
    - id: 1
      name: Reverse Proxy & Routing
      description: Route requests to appropriate backend services.
      acceptance_criteria:
      - Path-based routing (/api/users -> user-service)
      - Host-based routing (api.example.com)
      - Load balancing across instances
      - Health checks for backends
      - Circuit breaker for failing services
      hints:
        level1: Use regex patterns for path matching. Maintain pool of healthy backends.
        level2: Round-robin for basic LB. Add weights, least-connections for advanced.
        level3: "package main\n\nimport (\n    \"net/http\"\n    \"net/http/httputil\"\n    \"net/url\"\n    \"sync\"\n  \
          \  \"time\"\n)\n\ntype Backend struct {\n    URL      *url.URL\n    Alive    bool\n    Weight   int\n    mu    \
          \   sync.RWMutex\n}\n\nfunc (b *Backend) SetAlive(alive bool) {\n    b.mu.Lock()\n    b.Alive = alive\n    b.mu.Unlock()\n\
          }\n\nfunc (b *Backend) IsAlive() bool {\n    b.mu.RLock()\n    defer b.mu.RUnlock()\n    return b.Alive\n}\n\ntype\
          \ Route struct {\n    PathPrefix string\n    Backends   []*Backend\n    current    uint64\n}\n\nfunc (r *Route)\
          \ NextBackend() *Backend {\n    for i := 0; i < len(r.Backends); i++ {\n        idx := int(atomic.AddUint64(&r.current,\
          \ 1)) % len(r.Backends)\n        if r.Backends[idx].IsAlive() {\n            return r.Backends[idx]\n        }\n\
          \    }\n    return nil\n}\n\ntype Gateway struct {\n    routes  map[string]*Route\n    mu      sync.RWMutex\n}\n\
          \nfunc (g *Gateway) AddRoute(pathPrefix string, backends []string) {\n    route := &Route{PathPrefix: pathPrefix}\n\
          \n    for _, b := range backends {\n        u, _ := url.Parse(b)\n        route.Backends = append(route.Backends,\
          \ &Backend{URL: u, Alive: true, Weight: 1})\n    }\n\n    g.mu.Lock()\n    g.routes[pathPrefix] = route\n    g.mu.Unlock()\n\
          }\n\nfunc (g *Gateway) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    // Find matching route\n    g.mu.RLock()\n\
          \    var matchedRoute *Route\n    var longestMatch int\n    for prefix, route := range g.routes {\n        if strings.HasPrefix(r.URL.Path,\
          \ prefix) && len(prefix) > longestMatch {\n            matchedRoute = route\n            longestMatch = len(prefix)\n\
          \        }\n    }\n    g.mu.RUnlock()\n\n    if matchedRoute == nil {\n        http.Error(w, \"No route found\"\
          , http.StatusNotFound)\n        return\n    }\n\n    // Get healthy backend\n    backend := matchedRoute.NextBackend()\n\
          \    if backend == nil {\n        http.Error(w, \"No healthy backends\", http.StatusServiceUnavailable)\n      \
          \  return\n    }\n\n    // Reverse proxy\n    proxy := httputil.NewSingleHostReverseProxy(backend.URL)\n\n    //\
          \ Modify request\n    r.URL.Host = backend.URL.Host\n    r.URL.Scheme = backend.URL.Scheme\n    r.Header.Set(\"\
          X-Forwarded-Host\", r.Host)\n    r.Header.Set(\"X-Real-IP\", r.RemoteAddr)\n\n    proxy.ServeHTTP(w, r)\n}\n\n//\
          \ Health checker\nfunc (g *Gateway) HealthCheck(interval time.Duration) {\n    ticker := time.NewTicker(interval)\n\
          \    for range ticker.C {\n        g.mu.RLock()\n        for _, route := range g.routes {\n            for _, backend\
          \ := range route.Backends {\n                go func(b *Backend) {\n                    resp, err := http.Get(b.URL.String()\
          \ + \"/health\")\n                    b.SetAlive(err == nil && resp.StatusCode == 200)\n                }(backend)\n\
          \            }\n        }\n        g.mu.RUnlock()\n    }\n}"
      pitfalls:
      - Not forwarding original client IP
      - Health checks blocking request handling
      - Memory leaks from unclosed connections
      - Thundering herd on backend recovery
      concepts:
      - Reverse proxy
      - Load balancing
      - Health checks
      - Service discovery
      estimated_hours: 12-18
    - id: 2
      name: Request/Response Transformation
      description: Modify requests and responses as they pass through.
      acceptance_criteria:
      - Header manipulation (add, remove, modify)
      - Request body transformation
      - Response body transformation
      - URL rewriting
      - Request aggregation (combine multiple backend calls)
      hints:
        level1: Intercept request/response streams. Use middleware pattern.
        level2: For body transformation, buffer entire body, transform, forward.
        level3: "import (\n    \"bytes\"\n    \"encoding/json\"\n    \"io\"\n)\n\ntype Transformer interface {\n    TransformRequest(r\
          \ *http.Request) error\n    TransformResponse(resp *http.Response) error\n}\n\ntype HeaderTransformer struct {\n\
          \    AddHeaders    map[string]string\n    RemoveHeaders []string\n}\n\nfunc (t *HeaderTransformer) TransformRequest(r\
          \ *http.Request) error {\n    for key, value := range t.AddHeaders {\n        r.Header.Set(key, value)\n    }\n\
          \    for _, key := range t.RemoveHeaders {\n        r.Header.Del(key)\n    }\n    return nil\n}\n\ntype BodyTransformer\
          \ struct {\n    Transform func(body []byte) ([]byte, error)\n}\n\nfunc (t *BodyTransformer) TransformRequest(r *http.Request)\
          \ error {\n    if r.Body == nil || t.Transform == nil {\n        return nil\n    }\n\n    body, err := io.ReadAll(r.Body)\n\
          \    if err != nil {\n        return err\n    }\n    r.Body.Close()\n\n    transformed, err := t.Transform(body)\n\
          \    if err != nil {\n        return err\n    }\n\n    r.Body = io.NopCloser(bytes.NewReader(transformed))\n   \
          \ r.ContentLength = int64(len(transformed))\n    return nil\n}\n\n// URL Rewriting\ntype URLRewriter struct {\n\
          \    Rules []RewriteRule\n}\n\ntype RewriteRule struct {\n    Pattern *regexp.Regexp\n    Replace string\n}\n\n\
          func (u *URLRewriter) TransformRequest(r *http.Request) error {\n    path := r.URL.Path\n    for _, rule := range\
          \ u.Rules {\n        if rule.Pattern.MatchString(path) {\n            path = rule.Pattern.ReplaceAllString(path,\
          \ rule.Replace)\n            break\n        }\n    }\n    r.URL.Path = path\n    return nil\n}\n\n// Request aggregation\n\
          type Aggregator struct {\n    Endpoints []AggregateEndpoint\n}\n\ntype AggregateEndpoint struct {\n    Name    string\n\
          \    URL     string\n    Extract func(response []byte) interface{}\n}\n\nfunc (a *Aggregator) Aggregate(ctx context.Context)\
          \ (map[string]interface{}, error) {\n    results := make(map[string]interface{})\n    var mu sync.Mutex\n    var\
          \ wg sync.WaitGroup\n\n    for _, endpoint := range a.Endpoints {\n        wg.Add(1)\n        go func(ep AggregateEndpoint)\
          \ {\n            defer wg.Done()\n\n            req, _ := http.NewRequestWithContext(ctx, \"GET\", ep.URL, nil)\n\
          \            resp, err := http.DefaultClient.Do(req)\n            if err != nil {\n                return\n    \
          \        }\n            defer resp.Body.Close()\n\n            body, _ := io.ReadAll(resp.Body)\n            extracted\
          \ := ep.Extract(body)\n\n            mu.Lock()\n            results[ep.Name] = extracted\n            mu.Unlock()\n\
          \        }(endpoint)\n    }\n\n    wg.Wait()\n    return results, nil\n}"
      pitfalls:
      - Large body buffering causes memory issues
      - Transformation errors not handled gracefully
      - Aggregation timeout causes partial responses
      - Content-Length mismatch after transformation
      concepts:
      - Request transformation
      - Response transformation
      - URL rewriting
      - API aggregation
      estimated_hours: 10-14
    - id: 3
      name: Authentication & Authorization Layer
      description: Centralized auth handling at gateway level.
      acceptance_criteria:
      - JWT validation at gateway
      - API key authentication
      - OAuth2 token introspection
      - Pass user context to backends
      - Auth caching for performance
      hints:
        level1: Validate JWT signature and expiry. Cache valid tokens briefly.
        level2: Add user info to X-User-ID, X-User-Roles headers for backends.
        level3: "type AuthMiddleware struct {\n    JWTSecret     []byte\n    APIKeyStore   APIKeyStore\n    TokenCache   \
          \ *lru.Cache // github.com/hashicorp/golang-lru\n    CacheTTL      time.Duration\n}\n\nfunc (m *AuthMiddleware)\
          \ Authenticate(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request)\
          \ {\n        var userCtx *UserContext\n\n        // Try Bearer token first\n        authHeader := r.Header.Get(\"\
          Authorization\")\n        if strings.HasPrefix(authHeader, \"Bearer \") {\n            token := strings.TrimPrefix(authHeader,\
          \ \"Bearer \")\n            userCtx = m.validateJWT(token)\n        }\n\n        // Try API key\n        if userCtx\
          \ == nil {\n            apiKey := r.Header.Get(\"X-API-Key\")\n            if apiKey != \"\" {\n               \
          \ userCtx = m.validateAPIKey(apiKey)\n            }\n        }\n\n        if userCtx == nil {\n            http.Error(w,\
          \ \"Unauthorized\", http.StatusUnauthorized)\n            return\n        }\n\n        // Add user context to headers\
          \ for backends\n        r.Header.Set(\"X-User-ID\", userCtx.UserID)\n        r.Header.Set(\"X-User-Roles\", strings.Join(userCtx.Roles,\
          \ \",\"))\n        r.Header.Set(\"X-Auth-Method\", userCtx.AuthMethod)\n\n        next.ServeHTTP(w, r)\n    })\n\
          }\n\nfunc (m *AuthMiddleware) validateJWT(token string) *UserContext {\n    // Check cache first\n    if cached,\
          \ ok := m.TokenCache.Get(token); ok {\n        return cached.(*UserContext)\n    }\n\n    // Parse and validate\n\
          \    claims := jwt.MapClaims{}\n    parsed, err := jwt.ParseWithClaims(token, claims, func(t *jwt.Token) (interface{},\
          \ error) {\n        return m.JWTSecret, nil\n    })\n\n    if err != nil || !parsed.Valid {\n        return nil\n\
          \    }\n\n    userCtx := &UserContext{\n        UserID:     claims[\"sub\"].(string),\n        Roles:      toStringSlice(claims[\"\
          roles\"]),\n        AuthMethod: \"jwt\",\n    }\n\n    // Cache it\n    m.TokenCache.Add(token, userCtx)\n\n   \
          \ return userCtx\n}\n\n// OAuth2 token introspection for external tokens\nfunc (m *AuthMiddleware) introspectToken(token\
          \ string) *UserContext {\n    // Check cache\n    if cached, ok := m.TokenCache.Get(token); ok {\n        return\
          \ cached.(*UserContext)\n    }\n\n    // Call OAuth server\n    resp, err := http.PostForm(m.IntrospectionURL, url.Values{\n\
          \        \"token\":           {token},\n        \"client_id\":       {m.ClientID},\n        \"client_secret\": \
          \  {m.ClientSecret},\n    })\n    if err != nil {\n        return nil\n    }\n    defer resp.Body.Close()\n\n  \
          \  var result struct {\n        Active bool   `json:\"active\"`\n        Sub    string `json:\"sub\"`\n        Scope\
          \  string `json:\"scope\"`\n    }\n    json.NewDecoder(resp.Body).Decode(&result)\n\n    if !result.Active {\n \
          \       return nil\n    }\n\n    userCtx := &UserContext{\n        UserID:     result.Sub,\n        Roles:     \
          \ strings.Split(result.Scope, \" \"),\n        AuthMethod: \"oauth2\",\n    }\n\n    m.TokenCache.Add(token, userCtx)\n\
          \    return userCtx\n}"
      pitfalls:
      - Caching tokens too long misses revocations
      - Not validating token audience/issuer
      - API keys in logs or error messages
      - Auth failures not rate limited (brute force)
      concepts:
      - Centralized authentication
      - Token introspection
      - Auth caching
      - Header propagation
      estimated_hours: 10-15
    - id: 4
      name: Observability & Plugins
      description: Add logging, metrics, tracing, and plugin system.
      acceptance_criteria:
      - Structured access logs
      - Prometheus metrics
      - Distributed tracing (OpenTelemetry)
      - Plugin architecture for extensibility
      - Dynamic configuration reload
      hints:
        level1: Wrap handlers with logging/metrics middleware. Use OpenTelemetry SDK.
        level2: Plugin = interface with hooks. Load plugins at startup or dynamically.
        level3: "import (\n    \"github.com/prometheus/client_golang/prometheus\"\n    \"go.opentelemetry.io/otel\"\n    \"\
          go.opentelemetry.io/otel/trace\"\n)\n\n// Metrics\nvar (\n    requestsTotal = prometheus.NewCounterVec(\n      \
          \  prometheus.CounterOpts{\n            Name: \"gateway_requests_total\",\n            Help: \"Total HTTP requests\"\
          ,\n        },\n        []string{\"method\", \"path\", \"status\"},\n    )\n\n    requestDuration = prometheus.NewHistogramVec(\n\
          \        prometheus.HistogramOpts{\n            Name:    \"gateway_request_duration_seconds\",\n            Help:\
          \    \"Request duration in seconds\",\n            Buckets: []float64{.001, .005, .01, .025, .05, .1, .25, .5, 1,\
          \ 2.5, 5, 10},\n        },\n        []string{\"method\", \"path\"},\n    )\n)\n\nfunc MetricsMiddleware(next http.Handler)\
          \ http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n\
          \n        wrapped := &statusRecorder{ResponseWriter: w, status: 200}\n        next.ServeHTTP(wrapped, r)\n\n   \
          \     duration := time.Since(start).Seconds()\n        path := sanitizePath(r.URL.Path)  // Avoid cardinality explosion\n\
          \n        requestsTotal.WithLabelValues(r.Method, path, strconv.Itoa(wrapped.status)).Inc()\n        requestDuration.WithLabelValues(r.Method,\
          \ path).Observe(duration)\n    })\n}\n\n// Tracing\nfunc TracingMiddleware(next http.Handler) http.Handler {\n \
          \   tracer := otel.Tracer(\"gateway\")\n\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request)\
          \ {\n        ctx, span := tracer.Start(r.Context(), r.URL.Path,\n            trace.WithAttributes(\n           \
          \     attribute.String(\"http.method\", r.Method),\n                attribute.String(\"http.url\", r.URL.String()),\n\
          \            ),\n        )\n        defer span.End()\n\n        // Propagate trace context to backends\n       \
          \ r = r.WithContext(ctx)\n        otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(r.Header))\n\
          \n        next.ServeHTTP(w, r)\n    })\n}\n\n// Plugin system\ntype Plugin interface {\n    Name() string\n    Init(config\
          \ map[string]interface{}) error\n    PreRequest(r *http.Request) error\n    PostResponse(resp *http.Response) error\n\
          }\n\ntype PluginManager struct {\n    plugins []Plugin\n}\n\nfunc (pm *PluginManager) Load(name string, config map[string]interface{})\
          \ error {\n    // Dynamic loading (could use go plugins or scripting)\n    var plugin Plugin\n\n    switch name\
          \ {\n    case \"cors\":\n        plugin = &CORSPlugin{}\n    case \"request-id\":\n        plugin = &RequestIDPlugin{}\n\
          \    case \"compression\":\n        plugin = &CompressionPlugin{}\n    default:\n        return fmt.Errorf(\"unknown\
          \ plugin: %s\", name)\n    }\n\n    if err := plugin.Init(config); err != nil {\n        return err\n    }\n\n \
          \   pm.plugins = append(pm.plugins, plugin)\n    return nil\n}\n\nfunc (pm *PluginManager) RunPreRequest(r *http.Request)\
          \ error {\n    for _, p := range pm.plugins {\n        if err := p.PreRequest(r); err != nil {\n            return\
          \ err\n        }\n    }\n    return nil\n}"
      pitfalls:
      - High cardinality labels in metrics
      - Logging sensitive data (passwords, tokens)
      - Trace sampling too aggressive loses important traces
      - Plugin panics crash entire gateway
      concepts:
      - Observability
      - Prometheus metrics
      - OpenTelemetry tracing
      - Plugin architecture
      estimated_hours: 12-18
  grpc-service:
    id: grpc-service
    name: gRPC Microservice
    description: Build a gRPC service with streaming, error handling, and interceptors.
    difficulty: intermediate
    estimated_hours: 25-40
    prerequisites:
    - Protocol Buffers
    - RPC concepts
    - Go or similar
    languages:
      recommended:
      - Go
      - Rust
      also_possible:
      - Java
      - Python
      - C++
    resources:
    - name: gRPC Official Docs
      url: https://grpc.io/docs/
      type: documentation
    - name: Protocol Buffers
      url: https://developers.google.com/protocol-buffers
      type: documentation
    milestones:
    - id: 1
      name: Proto Definition & Code Generation
      description: Define service contract with Protocol Buffers.
      acceptance_criteria:
      - Define message types
      - Define service methods (unary, streaming)
      - Generate server and client code
      - Version proto files properly
      - Document APIs with comments
      hints:
        level1: proto3 syntax. Use well-known types (Timestamp, Duration). Generate with protoc.
        level2: 'Package versioning: myservice.v1.MyService. Never break backwards compatibility.'
        level3: "syntax = \"proto3\";\n\npackage userservice.v1;\n\nimport \"google/protobuf/timestamp.proto\";\nimport \"\
          google/protobuf/empty.proto\";\n\noption go_package = \"github.com/example/userservice/v1;userservicev1\";\n\n//\
          \ UserService manages user accounts\nservice UserService {\n  // Creates a new user account\n  rpc CreateUser(CreateUserRequest)\
          \ returns (User);\n\n  // Gets a user by ID\n  rpc GetUser(GetUserRequest) returns (User);\n\n  // Lists users with\
          \ pagination\n  rpc ListUsers(ListUsersRequest) returns (ListUsersResponse);\n\n  // Watches for user updates (server\
          \ streaming)\n  rpc WatchUsers(WatchUsersRequest) returns (stream UserEvent);\n\n  // Bulk import users (client\
          \ streaming)\n  rpc ImportUsers(stream ImportUserRequest) returns (ImportUsersResponse);\n\n  // Chat (bidirectional\
          \ streaming)\n  rpc Chat(stream ChatMessage) returns (stream ChatMessage);\n}\n\nmessage User {\n  string id = 1;\n\
          \  string email = 2;\n  string name = 3;\n  UserStatus status = 4;\n  google.protobuf.Timestamp created_at = 5;\n\
          \  google.protobuf.Timestamp updated_at = 6;\n\n  // Nested type for profile\n  Profile profile = 7;\n\n  message\
          \ Profile {\n    string bio = 1;\n    string avatar_url = 2;\n  }\n}\n\nenum UserStatus {\n  USER_STATUS_UNSPECIFIED\
          \ = 0;\n  USER_STATUS_ACTIVE = 1;\n  USER_STATUS_SUSPENDED = 2;\n  USER_STATUS_DELETED = 3;\n}\n\nmessage CreateUserRequest\
          \ {\n  string email = 1;\n  string name = 2;\n  string password = 3;\n}\n\nmessage GetUserRequest {\n  string id\
          \ = 1;\n}\n\nmessage ListUsersRequest {\n  int32 page_size = 1;\n  string page_token = 2;\n  string filter = 3;\
          \  // e.g., \"status=active\"\n}\n\nmessage ListUsersResponse {\n  repeated User users = 1;\n  string next_page_token\
          \ = 2;\n  int32 total_count = 3;\n}\n\nmessage UserEvent {\n  enum EventType {\n    EVENT_TYPE_UNSPECIFIED = 0;\n\
          \    EVENT_TYPE_CREATED = 1;\n    EVENT_TYPE_UPDATED = 2;\n    EVENT_TYPE_DELETED = 3;\n  }\n\n  EventType type\
          \ = 1;\n  User user = 2;\n  google.protobuf.Timestamp occurred_at = 3;\n}\n\n// Generate: protoc --go_out=. --go-grpc_out=.\
          \ user.proto"
      pitfalls:
      - Changing field numbers breaks compatibility
      - Using required fields (proto3 doesn't have them)
      - Not setting default enum value to UNSPECIFIED
      - Large messages exceed gRPC size limits
      concepts:
      - Protocol Buffers
      - Service definition
      - Code generation
      - API versioning
      estimated_hours: 4-6
    - id: 2
      name: Server Implementation
      description: Implement gRPC server with all RPC types.
      acceptance_criteria:
      - Unary RPC implementation
      - Server streaming RPC
      - Client streaming RPC
      - Bidirectional streaming
      - Graceful shutdown
      hints:
        level1: Implement the generated interface. For streaming, use Send() and Recv() loops.
        level2: Handle context cancellation. Return proper gRPC status codes.
        level3: "package main\n\nimport (\n    \"context\"\n    \"io\"\n    \"sync\"\n\n    pb \"github.com/example/userservice/v1\"\
          \n    \"google.golang.org/grpc\"\n    \"google.golang.org/grpc/codes\"\n    \"google.golang.org/grpc/status\"\n\
          )\n\ntype userServer struct {\n    pb.UnimplementedUserServiceServer\n    users    map[string]*pb.User\n    mu \
          \      sync.RWMutex\n    watchers map[chan *pb.UserEvent]struct{}\n}\n\nfunc (s *userServer) CreateUser(ctx context.Context,\
          \ req *pb.CreateUserRequest) (*pb.User, error) {\n    // Validate\n    if req.Email == \"\" {\n        return nil,\
          \ status.Error(codes.InvalidArgument, \"email is required\")\n    }\n\n    s.mu.Lock()\n    defer s.mu.Unlock()\n\
          \n    // Check duplicate\n    for _, u := range s.users {\n        if u.Email == req.Email {\n            return\
          \ nil, status.Error(codes.AlreadyExists, \"email already registered\")\n        }\n    }\n\n    user := &pb.User{\n\
          \        Id:        uuid.New().String(),\n        Email:     req.Email,\n        Name:      req.Name,\n        Status:\
          \    pb.UserStatus_USER_STATUS_ACTIVE,\n        CreatedAt: timestamppb.Now(),\n    }\n\n    s.users[user.Id] = user\n\
          \n    // Notify watchers\n    s.notify(&pb.UserEvent{\n        Type:       pb.UserEvent_EVENT_TYPE_CREATED,\n  \
          \      User:       user,\n        OccurredAt: timestamppb.Now(),\n    })\n\n    return user, nil\n}\n\nfunc (s *userServer)\
          \ GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.User, error) {\n    s.mu.RLock()\n    defer s.mu.RUnlock()\n\
          \n    user, ok := s.users[req.Id]\n    if !ok {\n        return nil, status.Error(codes.NotFound, \"user not found\"\
          )\n    }\n\n    return user, nil\n}\n\n// Server streaming\nfunc (s *userServer) WatchUsers(req *pb.WatchUsersRequest,\
          \ stream pb.UserService_WatchUsersServer) error {\n    ch := make(chan *pb.UserEvent, 10)\n\n    s.mu.Lock()\n \
          \   s.watchers[ch] = struct{}{}\n    s.mu.Unlock()\n\n    defer func() {\n        s.mu.Lock()\n        delete(s.watchers,\
          \ ch)\n        s.mu.Unlock()\n        close(ch)\n    }()\n\n    for {\n        select {\n        case event := <-ch:\n\
          \            if err := stream.Send(event); err != nil {\n                return err\n            }\n        case\
          \ <-stream.Context().Done():\n            return stream.Context().Err()\n        }\n    }\n}\n\n// Client streaming\n\
          func (s *userServer) ImportUsers(stream pb.UserService_ImportUsersServer) error {\n    var count int32\n\n    for\
          \ {\n        req, err := stream.Recv()\n        if err == io.EOF {\n            return stream.SendAndClose(&pb.ImportUsersResponse{\n\
          \                ImportedCount: count,\n            })\n        }\n        if err != nil {\n            return err\n\
          \        }\n\n        // Process each user\n        _, err = s.CreateUser(stream.Context(), &pb.CreateUserRequest{\n\
          \            Email: req.Email,\n            Name:  req.Name,\n        })\n        if err == nil {\n            count++\n\
          \        }\n    }\n}\n\n// Bidirectional streaming\nfunc (s *userServer) Chat(stream pb.UserService_ChatServer)\
          \ error {\n    for {\n        msg, err := stream.Recv()\n        if err == io.EOF {\n            return nil\n  \
          \      }\n        if err != nil {\n            return err\n        }\n\n        // Echo back (in real app, broadcast\
          \ to other participants)\n        response := &pb.ChatMessage{\n            UserId:    msg.UserId,\n           \
          \ Content:   \"Received: \" + msg.Content,\n            Timestamp: timestamppb.Now(),\n        }\n\n        if err\
          \ := stream.Send(response); err != nil {\n            return err\n        }\n    }\n}\n\nfunc main() {\n    lis,\
          \ _ := net.Listen(\"tcp\", \":50051\")\n\n    srv := grpc.NewServer()\n    pb.RegisterUserServiceServer(srv, &userServer{\n\
          \        users:    make(map[string]*pb.User),\n        watchers: make(map[chan *pb.UserEvent]struct{}),\n    })\n\
          \n    // Graceful shutdown\n    go func() {\n        sigCh := make(chan os.Signal, 1)\n        signal.Notify(sigCh,\
          \ syscall.SIGINT, syscall.SIGTERM)\n        <-sigCh\n        srv.GracefulStop()\n    }()\n\n    srv.Serve(lis)\n\
          }"
      pitfalls:
      - Blocking in streaming without timeout
      - Not handling context cancellation
      - Forgetting UnimplementedServer for forward compatibility
      - Memory leaks from unclosed streams
      concepts:
      - gRPC server
      - Streaming RPCs
      - Graceful shutdown
      - Status codes
      estimated_hours: 8-12
    - id: 3
      name: Interceptors & Middleware
      description: Add cross-cutting concerns with interceptors.
      acceptance_criteria:
      - Logging interceptor
      - Authentication interceptor
      - Rate limiting
      - Error handling and recovery
      - Request/response validation
      hints:
        level1: UnaryInterceptor and StreamInterceptor. Chain multiple with grpc.ChainUnaryInterceptor.
        level2: Extract metadata (headers) with metadata.FromIncomingContext(ctx).
        level3: "import (\n    \"google.golang.org/grpc\"\n    \"google.golang.org/grpc/metadata\"\n    \"google.golang.org/grpc/codes\"\
          \n    \"google.golang.org/grpc/status\"\n)\n\n// Logging interceptor\nfunc LoggingInterceptor(ctx context.Context,\
          \ req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {\n    start := time.Now()\n\
          \n    // Extract metadata\n    md, _ := metadata.FromIncomingContext(ctx)\n    requestID := md.Get(\"x-request-id\"\
          )\n\n    log.Printf(\"[%s] %s started\", requestID, info.FullMethod)\n\n    resp, err := handler(ctx, req)\n\n \
          \   duration := time.Since(start)\n    code := codes.OK\n    if err != nil {\n        code = status.Code(err)\n\
          \    }\n\n    log.Printf(\"[%s] %s completed in %v with code %s\",\n        requestID, info.FullMethod, duration,\
          \ code)\n\n    return resp, err\n}\n\n// Auth interceptor\nfunc AuthInterceptor(ctx context.Context, req interface{},\
          \ info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {\n    // Skip auth for certain methods\n\
          \    if info.FullMethod == \"/userservice.v1.UserService/Health\" {\n        return handler(ctx, req)\n    }\n\n\
          \    md, ok := metadata.FromIncomingContext(ctx)\n    if !ok {\n        return nil, status.Error(codes.Unauthenticated,\
          \ \"missing metadata\")\n    }\n\n    tokens := md.Get(\"authorization\")\n    if len(tokens) == 0 {\n        return\
          \ nil, status.Error(codes.Unauthenticated, \"missing token\")\n    }\n\n    // Validate token\n    userID, err :=\
          \ validateToken(tokens[0])\n    if err != nil {\n        return nil, status.Error(codes.Unauthenticated, \"invalid\
          \ token\")\n    }\n\n    // Add user to context\n    ctx = context.WithValue(ctx, \"user_id\", userID)\n\n    return\
          \ handler(ctx, req)\n}\n\n// Recovery interceptor\nfunc RecoveryInterceptor(ctx context.Context, req interface{},\
          \ info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) {\n    defer func() {\n \
          \       if r := recover(); r != nil {\n            log.Printf(\"panic recovered: %v\\n%s\", r, debug.Stack())\n\
          \            err = status.Error(codes.Internal, \"internal error\")\n        }\n    }()\n\n    return handler(ctx,\
          \ req)\n}\n\n// Validation interceptor\nfunc ValidationInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo,\
          \ handler grpc.UnaryHandler) (interface{}, error) {\n    // Check if request implements Validate()\n    if v, ok\
          \ := req.(interface{ Validate() error }); ok {\n        if err := v.Validate(); err != nil {\n            return\
          \ nil, status.Error(codes.InvalidArgument, err.Error())\n        }\n    }\n\n    return handler(ctx, req)\n}\n\n\
          // Stream interceptor for server streaming\nfunc StreamLoggingInterceptor(srv interface{}, ss grpc.ServerStream,\
          \ info *grpc.StreamServerInfo, handler grpc.StreamHandler) error {\n    start := time.Now()\n    log.Printf(\"%s\
          \ stream started\", info.FullMethod)\n\n    err := handler(srv, &wrappedStream{ss})\n\n    log.Printf(\"%s stream\
          \ ended after %v\", info.FullMethod, time.Since(start))\n    return err\n}\n\ntype wrappedStream struct {\n    grpc.ServerStream\n\
          }\n\nfunc (w *wrappedStream) SendMsg(m interface{}) error {\n    log.Printf(\"Sending message: %T\", m)\n    return\
          \ w.ServerStream.SendMsg(m)\n}\n\nfunc (w *wrappedStream) RecvMsg(m interface{}) error {\n    err := w.ServerStream.RecvMsg(m)\n\
          \    log.Printf(\"Received message: %T\", m)\n    return err\n}\n\n// Server setup with chained interceptors\nfunc\
          \ main() {\n    srv := grpc.NewServer(\n        grpc.ChainUnaryInterceptor(\n            RecoveryInterceptor,\n\
          \            LoggingInterceptor,\n            AuthInterceptor,\n            ValidationInterceptor,\n        ),\n\
          \        grpc.ChainStreamInterceptor(\n            StreamLoggingInterceptor,\n        ),\n    )\n}"
      pitfalls:
      - Interceptor order matters (auth before logging?)
      - Recovery interceptor not catching all panics
      - Context values lost in interceptor chain
      - Stream interceptors more complex than unary
      concepts:
      - gRPC interceptors
      - Middleware pattern
      - Context propagation
      - Error handling
      estimated_hours: 6-10
    - id: 4
      name: Client & Testing
      description: Build robust client and comprehensive tests.
      acceptance_criteria:
      - Client with retry and backoff
      - Connection pooling
      - Deadline/timeout handling
      - Unit tests with mocks
      - Integration tests
      hints:
        level1: Use grpc.WithDefaultServiceConfig for retry policy. Set deadlines with context.
        level2: Mock server for unit tests. Bufconn for in-process testing.
        level3: "import (\n    \"google.golang.org/grpc\"\n    \"google.golang.org/grpc/credentials/insecure\"\n    \"google.golang.org/grpc/backoff\"\
          \n)\n\n// Client with retry\nfunc NewUserServiceClient(target string) (pb.UserServiceClient, error) {\n    retryPolicy\
          \ := `{\n        \"methodConfig\": [{\n            \"name\": [{\"service\": \"userservice.v1.UserService\"}],\n\
          \            \"retryPolicy\": {\n                \"maxAttempts\": 3,\n                \"initialBackoff\": \"0.1s\"\
          ,\n                \"maxBackoff\": \"1s\",\n                \"backoffMultiplier\": 2,\n                \"retryableStatusCodes\"\
          : [\"UNAVAILABLE\", \"RESOURCE_EXHAUSTED\"]\n            }\n        }]\n    }`\n\n    conn, err := grpc.Dial(target,\n\
          \        grpc.WithTransportCredentials(insecure.NewCredentials()),\n        grpc.WithDefaultServiceConfig(retryPolicy),\n\
          \        grpc.WithConnectParams(grpc.ConnectParams{\n            Backoff: backoff.Config{\n                BaseDelay:\
          \  100 * time.Millisecond,\n                Multiplier: 1.6,\n                MaxDelay:   10 * time.Second,\n  \
          \          },\n            MinConnectTimeout: 5 * time.Second,\n        }),\n    )\n    if err != nil {\n      \
          \  return nil, err\n    }\n\n    return pb.NewUserServiceClient(conn), nil\n}\n\n// Client wrapper with deadline\n\
          type UserClient struct {\n    client  pb.UserServiceClient\n    timeout time.Duration\n}\n\nfunc (c *UserClient)\
          \ GetUser(ctx context.Context, id string) (*pb.User, error) {\n    ctx, cancel := context.WithTimeout(ctx, c.timeout)\n\
          \    defer cancel()\n\n    return c.client.GetUser(ctx, &pb.GetUserRequest{Id: id})\n}\n\n// Testing with bufconn\n\
          import (\n    \"google.golang.org/grpc/test/bufconn\"\n    \"testing\"\n)\n\nconst bufSize = 1024 * 1024\n\nvar\
          \ lis *bufconn.Listener\n\nfunc init() {\n    lis = bufconn.Listen(bufSize)\n    srv := grpc.NewServer()\n    pb.RegisterUserServiceServer(srv,\
          \ &userServer{\n        users: make(map[string]*pb.User),\n    })\n    go srv.Serve(lis)\n}\n\nfunc bufDialer(context.Context,\
          \ string) (net.Conn, error) {\n    return lis.Dial()\n}\n\nfunc TestCreateUser(t *testing.T) {\n    ctx := context.Background()\n\
          \    conn, err := grpc.DialContext(ctx, \"bufnet\",\n        grpc.WithContextDialer(bufDialer),\n        grpc.WithTransportCredentials(insecure.NewCredentials()),\n\
          \    )\n    if err != nil {\n        t.Fatalf(\"Failed to dial: %v\", err)\n    }\n    defer conn.Close()\n\n  \
          \  client := pb.NewUserServiceClient(conn)\n\n    resp, err := client.CreateUser(ctx, &pb.CreateUserRequest{\n \
          \       Email: \"test@example.com\",\n        Name:  \"Test User\",\n    })\n\n    if err != nil {\n        t.Fatalf(\"\
          CreateUser failed: %v\", err)\n    }\n\n    if resp.Email != \"test@example.com\" {\n        t.Errorf(\"Expected\
          \ email test@example.com, got %s\", resp.Email)\n    }\n}\n\n// Mock for unit testing\ntype mockUserServiceClient\
          \ struct {\n    pb.UserServiceClient\n    getUser func(ctx context.Context, req *pb.GetUserRequest) (*pb.User, error)\n\
          }\n\nfunc (m *mockUserServiceClient) GetUser(ctx context.Context, req *pb.GetUserRequest, opts ...grpc.CallOption)\
          \ (*pb.User, error) {\n    return m.getUser(ctx, req)\n}\n\nfunc TestBusinessLogic(t *testing.T) {\n    mock :=\
          \ &mockUserServiceClient{\n        getUser: func(ctx context.Context, req *pb.GetUserRequest) (*pb.User, error)\
          \ {\n            return &pb.User{Id: req.Id, Name: \"Mocked\"}, nil\n        },\n    }\n\n    // Test code that\
          \ uses the client\n    service := NewMyService(mock)\n    result, err := service.DoSomething(\"user-123\")\n   \
          \ // assertions...\n}"
      pitfalls:
      - Not setting deadline causes hanging requests
      - Retry on non-idempotent methods causes duplicates
      - Connection not reused (creating new per request)
      - Tests not cleaning up connections
      concepts:
      - gRPC client
      - Retry policies
      - Connection management
      - Testing strategies
      estimated_hours: 6-10
  circuit-breaker:
    id: circuit-breaker
    name: Circuit Breaker Pattern
    description: Implement circuit breaker for resilient microservices communication.
    difficulty: intermediate
    estimated_hours: 15-25
    prerequisites:
    - Microservices basics
    - Concurrency
    languages:
      recommended:
      - Go
      - Java
      also_possible:
      - Python
      - TypeScript
    resources:
    - name: Circuit Breaker Pattern
      url: https://martinfowler.com/bliki/CircuitBreaker.html
      type: article
    - name: Hystrix
      url: https://github.com/Netflix/Hystrix/wiki
      type: reference
    milestones:
    - id: 1
      name: Basic Circuit Breaker
      description: Implement closed, open, half-open states.
      acceptance_criteria:
      - 'Closed: requests pass through normally'
      - 'Open: requests fail immediately'
      - 'Half-open: test requests allowed'
      - State transitions based on failure threshold
      - Thread-safe implementation
      hints:
        level1: Track consecutive failures. Open circuit after threshold. Reset after timeout.
        level2: 'Half-open: allow one request, if success -> closed, if fail -> open again.'
        level3: "import (\n    \"sync\"\n    \"time\"\n)\n\ntype State int\n\nconst (\n    StateClosed State = iota\n    StateOpen\n\
          \    StateHalfOpen\n)\n\ntype CircuitBreaker struct {\n    name          string\n    maxFailures   int\n    timeout\
          \       time.Duration\n    halfOpenMax   int\n\n    state         State\n    failures      int\n    successes  \
          \   int\n    lastFailure   time.Time\n    halfOpenCount int\n    mu            sync.RWMutex\n}\n\nfunc NewCircuitBreaker(name\
          \ string, maxFailures int, timeout time.Duration) *CircuitBreaker {\n    return &CircuitBreaker{\n        name:\
          \        name,\n        maxFailures: maxFailures,\n        timeout:     timeout,\n        halfOpenMax: 3,\n    \
          \    state:       StateClosed,\n    }\n}\n\nfunc (cb *CircuitBreaker) Execute(fn func() error) error {\n    if !cb.allowRequest()\
          \ {\n        return ErrCircuitOpen\n    }\n\n    err := fn()\n\n    cb.recordResult(err)\n    return err\n}\n\n\
          func (cb *CircuitBreaker) allowRequest() bool {\n    cb.mu.Lock()\n    defer cb.mu.Unlock()\n\n    switch cb.state\
          \ {\n    case StateClosed:\n        return true\n\n    case StateOpen:\n        if time.Since(cb.lastFailure) >\
          \ cb.timeout {\n            cb.state = StateHalfOpen\n            cb.halfOpenCount = 0\n            cb.successes\
          \ = 0\n            return true\n        }\n        return false\n\n    case StateHalfOpen:\n        if cb.halfOpenCount\
          \ < cb.halfOpenMax {\n            cb.halfOpenCount++\n            return true\n        }\n        return false\n\
          \    }\n\n    return false\n}\n\nfunc (cb *CircuitBreaker) recordResult(err error) {\n    cb.mu.Lock()\n    defer\
          \ cb.mu.Unlock()\n\n    if err != nil {\n        cb.onFailure()\n    } else {\n        cb.onSuccess()\n    }\n}\n\
          \nfunc (cb *CircuitBreaker) onFailure() {\n    cb.failures++\n    cb.lastFailure = time.Now()\n\n    switch cb.state\
          \ {\n    case StateClosed:\n        if cb.failures >= cb.maxFailures {\n            cb.state = StateOpen\n     \
          \   }\n    case StateHalfOpen:\n        cb.state = StateOpen\n    }\n}\n\nfunc (cb *CircuitBreaker) onSuccess()\
          \ {\n    switch cb.state {\n    case StateClosed:\n        cb.failures = 0\n    case StateHalfOpen:\n        cb.successes++\n\
          \        if cb.successes >= cb.halfOpenMax {\n            cb.state = StateClosed\n            cb.failures = 0\n\
          \        }\n    }\n}\n\nvar ErrCircuitOpen = errors.New(\"circuit breaker is open\")"
      pitfalls:
      - Race conditions without proper locking
      - Not resetting failure count on success
      - Half-open allows too many requests
      - Timer not being reset properly
      concepts:
      - Circuit breaker states
      - Failure detection
      - Recovery testing
      - Thread safety
      estimated_hours: 5-8
    - id: 2
      name: Advanced Features
      description: Add sliding window, metrics, and fallbacks.
      acceptance_criteria:
      - Sliding window for failure rate calculation
      - Configurable error classification
      - Fallback function support
      - Metrics and observability
      - Bulkhead (concurrency limit)
      hints:
        level1: Use ring buffer for sliding window. Classify errors (timeout vs 5xx).
        level2: 'Fallback: return cached/default value. Bulkhead: limit concurrent calls.'
        level3: "type SlidingWindow struct {\n    size    int\n    buckets []bucket\n    current int\n    mu      sync.Mutex\n\
          }\n\ntype bucket struct {\n    successes int\n    failures  int\n    timestamp time.Time\n}\n\nfunc NewSlidingWindow(size\
          \ int, bucketDuration time.Duration) *SlidingWindow {\n    sw := &SlidingWindow{\n        size:    size,\n     \
          \   buckets: make([]bucket, size),\n    }\n\n    // Rotate buckets periodically\n    go func() {\n        ticker\
          \ := time.NewTicker(bucketDuration)\n        for range ticker.C {\n            sw.rotate()\n        }\n    }()\n\
          \n    return sw\n}\n\nfunc (sw *SlidingWindow) rotate() {\n    sw.mu.Lock()\n    defer sw.mu.Unlock()\n\n    sw.current\
          \ = (sw.current + 1) % sw.size\n    sw.buckets[sw.current] = bucket{timestamp: time.Now()}\n}\n\nfunc (sw *SlidingWindow)\
          \ RecordSuccess() {\n    sw.mu.Lock()\n    sw.buckets[sw.current].successes++\n    sw.mu.Unlock()\n}\n\nfunc (sw\
          \ *SlidingWindow) RecordFailure() {\n    sw.mu.Lock()\n    sw.buckets[sw.current].failures++\n    sw.mu.Unlock()\n\
          }\n\nfunc (sw *SlidingWindow) FailureRate() float64 {\n    sw.mu.Lock()\n    defer sw.mu.Unlock()\n\n    var total,\
          \ failures int\n    for _, b := range sw.buckets {\n        total += b.successes + b.failures\n        failures\
          \ += b.failures\n    }\n\n    if total == 0 {\n        return 0\n    }\n    return float64(failures) / float64(total)\n\
          }\n\n// Enhanced circuit breaker\ntype EnhancedCircuitBreaker struct {\n    *CircuitBreaker\n    window      *SlidingWindow\n\
          \    fallback    func() (interface{}, error)\n    bulkhead    chan struct{}\n    classifier  func(error) bool  //\
          \ true = should count as failure\n    metrics     *Metrics\n}\n\nfunc (cb *EnhancedCircuitBreaker) ExecuteWithFallback(fn\
          \ func() (interface{}, error)) (interface{}, error) {\n    // Check bulkhead (concurrency limit)\n    select {\n\
          \    case cb.bulkhead <- struct{}{}:\n        defer func() { <-cb.bulkhead }()\n    default:\n        cb.metrics.BulkheadRejected.Inc()\n\
          \        return cb.executeFallback(ErrBulkheadFull)\n    }\n\n    if !cb.allowRequest() {\n        cb.metrics.CircuitRejected.Inc()\n\
          \        return cb.executeFallback(ErrCircuitOpen)\n    }\n\n    start := time.Now()\n    result, err := fn()\n\
          \    duration := time.Since(start)\n\n    cb.metrics.RequestDuration.Observe(duration.Seconds())\n\n    // Classify\
          \ error\n    if err != nil && cb.classifier(err) {\n        cb.window.RecordFailure()\n        cb.metrics.Failures.Inc()\n\
          \n        // Check if we should open circuit\n        if cb.window.FailureRate() > 0.5 {  // 50% threshold\n   \
          \         cb.openCircuit()\n        }\n\n        return cb.executeFallback(err)\n    }\n\n    cb.window.RecordSuccess()\n\
          \    cb.metrics.Successes.Inc()\n    return result, nil\n}\n\nfunc (cb *EnhancedCircuitBreaker) executeFallback(originalErr\
          \ error) (interface{}, error) {\n    if cb.fallback != nil {\n        cb.metrics.FallbackExecuted.Inc()\n      \
          \  return cb.fallback()\n    }\n    return nil, originalErr\n}\n\n// Error classifier\nfunc DefaultClassifier(err\
          \ error) bool {\n    // Ignore client errors, count server errors and timeouts\n    var netErr net.Error\n    if\
          \ errors.As(err, &netErr) && netErr.Timeout() {\n        return true\n    }\n\n    var httpErr *HTTPError\n    if\
          \ errors.As(err, &httpErr) {\n        return httpErr.StatusCode >= 500\n    }\n\n    return true\n}"
      pitfalls:
      - Sliding window bucket rotation timing issues
      - Fallback also failing (need fallback for fallback)
      - Bulkhead too small causes unnecessary rejections
      - Error classifier too aggressive
      concepts:
      - Sliding window
      - Fallback patterns
      - Bulkhead pattern
      - Error classification
      estimated_hours: 6-10
    - id: 3
      name: Integration & Testing
      description: Integrate with HTTP/gRPC clients and test chaos scenarios.
      acceptance_criteria:
      - HTTP client wrapper with circuit breaker
      - gRPC interceptor with circuit breaker
      - Per-service circuit breakers
      - Chaos testing (inject failures)
      - Dashboard for circuit states
      hints:
        level1: Wrap http.Client.Do(). Key circuit breaker by host or service name.
        level2: 'Chaos: randomly fail N% of requests. Test state transitions.'
        level3: "// HTTP client with circuit breaker\ntype ResilientHTTPClient struct {\n    client   *http.Client\n    breakers\
          \ map[string]*EnhancedCircuitBreaker\n    mu       sync.RWMutex\n}\n\nfunc (c *ResilientHTTPClient) getBreaker(host\
          \ string) *EnhancedCircuitBreaker {\n    c.mu.RLock()\n    cb, ok := c.breakers[host]\n    c.mu.RUnlock()\n\n  \
          \  if ok {\n        return cb\n    }\n\n    c.mu.Lock()\n    defer c.mu.Unlock()\n\n    // Double check\n    if\
          \ cb, ok = c.breakers[host]; ok {\n        return cb\n    }\n\n    cb = NewEnhancedCircuitBreaker(host, Config{\n\
          \        MaxFailures:   5,\n        Timeout:       30 * time.Second,\n        HalfOpenMax:   3,\n        BulkheadSize:\
          \  100,\n    })\n    c.breakers[host] = cb\n    return cb\n}\n\nfunc (c *ResilientHTTPClient) Do(req *http.Request)\
          \ (*http.Response, error) {\n    cb := c.getBreaker(req.Host)\n\n    result, err := cb.ExecuteWithFallback(func()\
          \ (interface{}, error) {\n        return c.client.Do(req)\n    })\n\n    if err != nil {\n        return nil, err\n\
          \    }\n    return result.(*http.Response), nil\n}\n\n// gRPC interceptor\nfunc CircuitBreakerInterceptor(breakers\
          \ map[string]*EnhancedCircuitBreaker) grpc.UnaryClientInterceptor {\n    return func(ctx context.Context, method\
          \ string, req, reply interface{},\n                cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption)\
          \ error {\n\n        // Get service name from method\n        service := strings.Split(method, \"/\")[1]\n     \
          \   cb := breakers[service]\n\n        _, err := cb.ExecuteWithFallback(func() (interface{}, error) {\n        \
          \    return nil, invoker(ctx, method, req, reply, cc, opts...)\n        })\n\n        return err\n    }\n}\n\n//\
          \ Chaos testing\ntype ChaosMiddleware struct {\n    failureRate float64\n    latencyMs   int\n    enabled     bool\n\
          }\n\nfunc (c *ChaosMiddleware) Wrap(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter,\
          \ r *http.Request) {\n        if !c.enabled {\n            next.ServeHTTP(w, r)\n            return\n        }\n\
          \n        // Inject latency\n        if c.latencyMs > 0 {\n            time.Sleep(time.Duration(c.latencyMs) * time.Millisecond)\n\
          \        }\n\n        // Inject failures\n        if rand.Float64() < c.failureRate {\n            http.Error(w,\
          \ \"Chaos: injected failure\", http.StatusInternalServerError)\n            return\n        }\n\n        next.ServeHTTP(w,\
          \ r)\n    })\n}\n\n// Tests\nfunc TestCircuitOpensOnFailures(t *testing.T) {\n    cb := NewCircuitBreaker(\"test\"\
          , 3, 1*time.Second)\n\n    // 3 failures should open circuit\n    for i := 0; i < 3; i++ {\n        cb.Execute(func()\
          \ error {\n            return errors.New(\"fail\")\n        })\n    }\n\n    // Next call should fail immediately\n\
          \    err := cb.Execute(func() error {\n        t.Fatal(\"Should not execute\")\n        return nil\n    })\n\n \
          \   if !errors.Is(err, ErrCircuitOpen) {\n        t.Errorf(\"Expected ErrCircuitOpen, got %v\", err)\n    }\n}\n\
          \nfunc TestCircuitRecovery(t *testing.T) {\n    cb := NewCircuitBreaker(\"test\", 1, 100*time.Millisecond)\n\n \
          \   // Open circuit\n    cb.Execute(func() error { return errors.New(\"fail\") })\n\n    // Wait for timeout\n \
          \   time.Sleep(150 * time.Millisecond)\n\n    // Should be half-open now, success should close\n    err := cb.Execute(func()\
          \ error { return nil })\n    if err != nil {\n        t.Errorf(\"Expected success, got %v\", err)\n    }\n}"
      pitfalls:
      - Chaos testing in production without safeguards
      - Circuit breaker per-request instead of per-service
      - Not exposing circuit state for debugging
      - Test flakiness due to timing
      concepts:
      - Client integration
      - Chaos engineering
      - Per-service isolation
      - Observability
      estimated_hours: 6-10
  websocket-server:
    name: WebSocket Server
    description: Build a WebSocket server from scratch supporting the RFC 6455 protocol with connection management, heartbeats,
      and message framing.
    why_important: WebSockets enable real-time bidirectional communication essential for chat apps, live updates, gaming,
      and collaborative tools.
    difficulty: intermediate
    tags:
    - networking
    - real-time
    - protocols
    estimated_hours: 30
    prerequisites:
    - tcp-server
    learning_outcomes:
    - Master WebSocket handshake and frame parsing
    - Implement connection lifecycle management
    - Handle binary and text message types
    - Build heartbeat/ping-pong mechanisms
    milestones:
    - name: HTTP Upgrade Handshake
      description: Implement WebSocket handshake by parsing HTTP upgrade request and computing Sec-WebSocket-Accept using
        SHA-1 and Base64.
      hints:
        level1: Parse HTTP headers and validate Upgrade request.
        level2: Concatenate client key with magic GUID, SHA-1 hash, Base64 encode.
        level3: "```python\nimport hashlib\nimport base64\n\nMAGIC_GUID = \"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\"\n\ndef\
          \ compute_accept_key(client_key: str) -> str:\n    combined = client_key.strip() + MAGIC_GUID\n    sha1_hash = hashlib.sha1(combined.encode()).digest()\n\
          \    return base64.b64encode(sha1_hash).decode()\n\ndef parse_upgrade_request(data: bytes) -> dict:\n    lines =\
          \ data.decode().split('\\r\\n')\n    headers = {}\n    for line in lines[1:]:\n        if ': ' in line:\n      \
          \      key, value = line.split(': ', 1)\n            headers[key.lower()] = value\n    return headers\n\ndef create_handshake_response(client_key:\
          \ str) -> bytes:\n    accept = compute_accept_key(client_key)\n    return (\n        \"HTTP/1.1 101 Switching Protocols\\\
          r\\n\"\n        \"Upgrade: websocket\\r\\n\"\n        \"Connection: Upgrade\\r\\n\"\n        f\"Sec-WebSocket-Accept:\
          \ {accept}\\r\\n\"\n        \"\\r\\n\"\n    ).encode()\n```"
      pitfalls:
      - Missing CRLF at end of headers causes handshake failure
      - Case-sensitive header parsing breaks with some clients
      - Not validating WebSocket version header
    - name: Frame Parsing
      description: Parse WebSocket frames including opcode, masking, and payload length handling for small and large messages.
      hints:
        level1: Read first 2 bytes for FIN, opcode, mask bit, and initial length.
        level2: Handle 7-bit, 16-bit (126), and 64-bit (127) payload lengths.
        level3: "```python\nimport struct\nfrom dataclasses import dataclass\nfrom enum import IntEnum\n\nclass Opcode(IntEnum):\n\
          \    CONTINUATION = 0x0\n    TEXT = 0x1\n    BINARY = 0x2\n    CLOSE = 0x8\n    PING = 0x9\n    PONG = 0xA\n\n@dataclass\n\
          class Frame:\n    fin: bool\n    opcode: Opcode\n    payload: bytes\n\ndef parse_frame(data: bytes) -> tuple[Frame,\
          \ int]:\n    if len(data) < 2:\n        raise ValueError(\"Incomplete frame\")\n\n    byte1, byte2 = data[0], data[1]\n\
          \    fin = bool(byte1 & 0x80)\n    opcode = Opcode(byte1 & 0x0F)\n    masked = bool(byte2 & 0x80)\n    length =\
          \ byte2 & 0x7F\n\n    offset = 2\n    if length == 126:\n        length = struct.unpack('>H', data[2:4])[0]\n  \
          \      offset = 4\n    elif length == 127:\n        length = struct.unpack('>Q', data[2:10])[0]\n        offset\
          \ = 10\n\n    if masked:\n        mask = data[offset:offset+4]\n        offset += 4\n        payload = bytes(b ^\
          \ mask[i % 4] for i, b in enumerate(data[offset:offset+length]))\n    else:\n        payload = data[offset:offset+length]\n\
          \n    return Frame(fin, opcode, payload), offset + length\n```"
      pitfalls:
      - Forgetting to unmask client frames (clients MUST mask)
      - Integer overflow with 64-bit payload lengths
      - Not handling fragmented messages (FIN=0)
    - name: Connection Management
      description: Manage multiple WebSocket connections with proper state tracking, broadcasting, and graceful disconnection.
      hints:
        level1: Track connections in a dictionary keyed by unique ID.
        level2: Use asyncio for concurrent connection handling.
        level3: "```python\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom typing import Callable\n\n@dataclass\n\
          class Connection:\n    id: str\n    writer: asyncio.StreamWriter\n    reader: asyncio.StreamReader\n    is_alive:\
          \ bool = True\n    rooms: set = field(default_factory=set)\n\nclass ConnectionManager:\n    def __init__(self):\n\
          \        self.connections: dict[str, Connection] = {}\n        self.rooms: dict[str, set[str]] = {}  # room -> connection_ids\n\
          \n    async def add(self, conn: Connection):\n        self.connections[conn.id] = conn\n\n    async def remove(self,\
          \ conn_id: str):\n        if conn := self.connections.pop(conn_id, None):\n            for room in conn.rooms:\n\
          \                self.rooms.get(room, set()).discard(conn_id)\n            conn.writer.close()\n            await\
          \ conn.writer.wait_closed()\n\n    async def broadcast(self, message: bytes, exclude: str = None):\n        for\
          \ conn_id, conn in self.connections.items():\n            if conn_id != exclude and conn.is_alive:\n           \
          \     await self.send(conn_id, message)\n\n    async def send(self, conn_id: str, message: bytes):\n        if conn\
          \ := self.connections.get(conn_id):\n            frame = create_frame(message, Opcode.TEXT)\n            conn.writer.write(frame)\n\
          \            await conn.writer.drain()\n\n    async def join_room(self, conn_id: str, room: str):\n        if conn\
          \ := self.connections.get(conn_id):\n            conn.rooms.add(room)\n            self.rooms.setdefault(room, set()).add(conn_id)\n\
          \n    async def broadcast_to_room(self, room: str, message: bytes, exclude: str = None):\n        for conn_id in\
          \ self.rooms.get(room, set()):\n            if conn_id != exclude:\n                await self.send(conn_id, message)\n\
          ```"
      pitfalls:
      - Not handling disconnections during broadcast causes cascade failures
      - Memory leak from not cleaning up closed connections
      - Race conditions when modifying connection dict during iteration
    - name: Ping/Pong Heartbeat
      description: Implement heartbeat mechanism using ping/pong frames to detect dead connections and maintain NAT mappings.
      hints:
        level1: Send ping frames periodically, expect pong responses.
        level2: Track last pong time, disconnect if no response within timeout.
        level3: "```python\nimport asyncio\nimport time\n\nclass HeartbeatManager:\n    def __init__(self, conn_manager: ConnectionManager,\n\
          \                 ping_interval: float = 30.0,\n                 pong_timeout: float = 10.0):\n        self.conn_manager\
          \ = conn_manager\n        self.ping_interval = ping_interval\n        self.pong_timeout = pong_timeout\n       \
          \ self.last_pong: dict[str, float] = {}\n        self._task = None\n\n    def start(self):\n        self._task =\
          \ asyncio.create_task(self._heartbeat_loop())\n\n    def stop(self):\n        if self._task:\n            self._task.cancel()\n\
          \n    def record_pong(self, conn_id: str):\n        self.last_pong[conn_id] = time.monotonic()\n\n    async def\
          \ _heartbeat_loop(self):\n        while True:\n            await asyncio.sleep(self.ping_interval)\n           \
          \ now = time.monotonic()\n            dead_connections = []\n\n            for conn_id, conn in self.conn_manager.connections.items():\n\
          \                last = self.last_pong.get(conn_id, now)\n                if now - last > self.ping_interval + self.pong_timeout:\n\
          \                    dead_connections.append(conn_id)\n                else:\n                    # Send ping\n\
          \                    ping_frame = create_frame(b'', Opcode.PING)\n                    try:\n                   \
          \     conn.writer.write(ping_frame)\n                        await conn.writer.drain()\n                    except\
          \ Exception:\n                        dead_connections.append(conn_id)\n\n            for conn_id in dead_connections:\n\
          \                await self.conn_manager.remove(conn_id)\n                self.last_pong.pop(conn_id, None)\n```"
      pitfalls:
      - Using wall clock time fails with system time changes
      - Too aggressive ping interval wastes bandwidth
      - Not initializing last_pong on connect causes immediate disconnect
  realtime-chat:
    name: Real-time Chat System
    description: Build a scalable real-time chat system with rooms, presence, message history, and horizontal scaling support.
    why_important: Chat systems demonstrate core real-time architecture patterns used in Slack, Discord, and messaging apps.
    difficulty: intermediate
    tags:
    - real-time
    - distributed-systems
    - backend
    estimated_hours: 40
    prerequisites:
    - websocket-server
    - redis-clone
    learning_outcomes:
    - Design scalable real-time message delivery
    - Implement presence detection and tracking
    - Handle message ordering and consistency
    - Build pub/sub for horizontal scaling
    milestones:
    - name: Message Routing
      description: Implement message routing between users with direct messages, room broadcasts, and delivery confirmation.
      hints:
        level1: 'Route messages based on type: DM, room, or broadcast.'
        level2: Use message IDs for delivery acknowledgment.
        level3: "```python\nimport uuid\nimport json\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\n\
          from enum import Enum\n\nclass MessageType(str, Enum):\n    DIRECT = \"direct\"\n    ROOM = \"room\"\n    BROADCAST\
          \ = \"broadcast\"\n    SYSTEM = \"system\"\n\n@dataclass\nclass ChatMessage:\n    id: str\n    type: MessageType\n\
          \    sender_id: str\n    content: str\n    timestamp: datetime\n    target: str = None  # user_id for DM, room_id\
          \ for room\n    metadata: dict = None\n\n    @classmethod\n    def create(cls, type: MessageType, sender_id: str,\
          \ content: str, target: str = None):\n        return cls(\n            id=str(uuid.uuid4()),\n            type=type,\n\
          \            sender_id=sender_id,\n            content=content,\n            timestamp=datetime.utcnow(),\n    \
          \        target=target\n        )\n\nclass MessageRouter:\n    def __init__(self, conn_manager: ConnectionManager,\
          \ history_store):\n        self.conn_manager = conn_manager\n        self.history = history_store\n        self.pending_acks:\
          \ dict[str, ChatMessage] = {}\n\n    async def route(self, message: ChatMessage):\n        await self.history.store(message)\n\
          \n        if message.type == MessageType.DIRECT:\n            await self._route_direct(message)\n        elif message.type\
          \ == MessageType.ROOM:\n            await self._route_room(message)\n        elif message.type == MessageType.BROADCAST:\n\
          \            await self._route_broadcast(message)\n\n    async def _route_direct(self, message: ChatMessage):\n\
          \        payload = json.dumps(asdict(message), default=str).encode()\n        # Send to recipient\n        await\
          \ self.conn_manager.send(message.target, payload)\n        # Send to sender (echo)\n        await self.conn_manager.send(message.sender_id,\
          \ payload)\n        self.pending_acks[message.id] = message\n\n    async def _route_room(self, message: ChatMessage):\n\
          \        payload = json.dumps(asdict(message), default=str).encode()\n        await self.conn_manager.broadcast_to_room(\n\
          \            message.target, payload, exclude=message.sender_id\n        )\n\n    async def acknowledge(self, message_id:\
          \ str, user_id: str):\n        if msg := self.pending_acks.get(message_id):\n            # Mark as delivered\n \
          \           await self.history.mark_delivered(message_id, user_id)\n```"
      pitfalls:
      - Not echoing messages to sender causes UI desync
      - Missing delivery confirmation loses messages silently
      - JSON serialization fails with datetime objects
    - name: Presence System
      description: Track user online/offline status with typing indicators, last seen timestamps, and efficient presence broadcasts.
      hints:
        level1: Update presence on connect/disconnect and activity.
        level2: Use heartbeats to detect ghost sessions.
        level3: "```python\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import\
          \ Enum\nimport asyncio\n\nclass PresenceStatus(str, Enum):\n    ONLINE = \"online\"\n    AWAY = \"away\"\n    OFFLINE\
          \ = \"offline\"\n    TYPING = \"typing\"\n\n@dataclass\nclass UserPresence:\n    user_id: str\n    status: PresenceStatus\n\
          \    last_active: datetime\n    current_room: str = None\n\nclass PresenceManager:\n    def __init__(self, conn_manager:\
          \ ConnectionManager):\n        self.conn_manager = conn_manager\n        self.presence: dict[str, UserPresence]\
          \ = {}\n        self.typing_timers: dict[str, asyncio.Task] = {}\n        self.away_threshold = timedelta(minutes=5)\n\
          \n    async def set_online(self, user_id: str):\n        self.presence[user_id] = UserPresence(\n            user_id=user_id,\n\
          \            status=PresenceStatus.ONLINE,\n            last_active=datetime.utcnow()\n        )\n        await\
          \ self._broadcast_presence(user_id)\n\n    async def set_offline(self, user_id: str):\n        if user_id in self.presence:\n\
          \            self.presence[user_id].status = PresenceStatus.OFFLINE\n            await self._broadcast_presence(user_id)\n\
          \n    async def set_typing(self, user_id: str, room_id: str):\n        if user_id in self.presence:\n          \
          \  self.presence[user_id].status = PresenceStatus.TYPING\n            self.presence[user_id].current_room = room_id\n\
          \            await self._broadcast_typing(user_id, room_id)\n\n            # Cancel previous timer\n           \
          \ if user_id in self.typing_timers:\n                self.typing_timers[user_id].cancel()\n\n            # Auto-clear\
          \ typing after 3 seconds\n            self.typing_timers[user_id] = asyncio.create_task(\n                self._clear_typing(user_id,\
          \ 3.0)\n            )\n\n    async def _clear_typing(self, user_id: str, delay: float):\n        await asyncio.sleep(delay)\n\
          \        if p := self.presence.get(user_id):\n            if p.status == PresenceStatus.TYPING:\n              \
          \  p.status = PresenceStatus.ONLINE\n                await self._broadcast_presence(user_id)\n\n    async def heartbeat(self,\
          \ user_id: str):\n        if p := self.presence.get(user_id):\n            p.last_active = datetime.utcnow()\n \
          \           if p.status == PresenceStatus.AWAY:\n                p.status = PresenceStatus.ONLINE\n            \
          \    await self._broadcast_presence(user_id)\n\n    def get_room_presence(self, room_id: str) -> list[UserPresence]:\n\
          \        return [p for p in self.presence.values()\n                if p.current_room == room_id and p.status !=\
          \ PresenceStatus.OFFLINE]\n```"
      pitfalls:
      - Typing indicator never clears if user disconnects while typing
      - Presence storms when many users join simultaneously
      - Race condition between away detection and activity
    - name: Message History & Sync
      description: Store and retrieve message history with pagination, gap detection, and client sync protocol.
      hints:
        level1: Store messages with timestamps, allow range queries.
        level2: Use cursor-based pagination for efficient scrolling.
        level3: "```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport json\n\n@dataclass\nclass\
          \ MessagePage:\n    messages: list[ChatMessage]\n    has_more: bool\n    cursor: str = None\n\nclass MessageHistory:\n\
          \    def __init__(self, storage):\n        self.storage = storage  # Redis, DB, etc.\n\n    async def store(self,\
          \ message: ChatMessage):\n        key = self._get_key(message)\n        score = message.timestamp.timestamp()\n\
          \        await self.storage.zadd(key, {message.id: score})\n        await self.storage.hset(f\"msg:{message.id}\"\
          , mapping=asdict(message))\n\n    async def get_history(self, room_id: str, cursor: str = None,\n              \
          \           limit: int = 50) -> MessagePage:\n        key = f\"room:{room_id}:messages\"\n\n        if cursor:\n\
          \            # Get score of cursor message\n            cursor_score = await self.storage.zscore(key, cursor)\n\
          \            max_score = cursor_score - 0.001\n        else:\n            max_score = \"+inf\"\n\n        # Get\
          \ message IDs in reverse chronological order\n        msg_ids = await self.storage.zrevrangebyscore(\n         \
          \   key, max_score, \"-inf\", start=0, num=limit + 1\n        )\n\n        has_more = len(msg_ids) > limit\n   \
          \     msg_ids = msg_ids[:limit]\n\n        # Fetch full messages\n        messages = []\n        for msg_id in msg_ids:\n\
          \            data = await self.storage.hgetall(f\"msg:{msg_id}\")\n            if data:\n                messages.append(ChatMessage(**data))\n\
          \n        return MessagePage(\n            messages=messages,\n            has_more=has_more,\n            cursor=msg_ids[-1]\
          \ if msg_ids else None\n        )\n\n    async def get_since(self, room_id: str, since_id: str) -> list[ChatMessage]:\n\
          \        \"\"\"Get all messages after a given message ID for sync.\"\"\"\n        key = f\"room:{room_id}:messages\"\
          \n        since_score = await self.storage.zscore(key, since_id)\n\n        msg_ids = await self.storage.zrangebyscore(\n\
          \            key, f\"({since_score}\", \"+inf\"\n        )\n\n        messages = []\n        for msg_id in msg_ids:\n\
          \            data = await self.storage.hgetall(f\"msg:{msg_id}\")\n            if data:\n                messages.append(ChatMessage(**data))\n\
          \        return messages\n```"
      pitfalls:
      - Offset-based pagination skips messages when new ones arrive
      - Large gaps cause client to fetch too many messages at once
      - Not handling deleted messages in sync causes errors
    - name: Horizontal Scaling with Pub/Sub
      description: Scale to multiple server instances using pub/sub for cross-instance message delivery.
      hints:
        level1: Publish messages to shared channel, subscribe on all instances.
        level2: Include instance ID to avoid echo, handle reconnection.
        level3: "```python\nimport asyncio\nimport json\nfrom dataclasses import dataclass\nfrom typing import Callable\n\
          import uuid\n\n@dataclass\nclass ClusterMessage:\n    type: str\n    source_instance: str\n    payload: dict\n\n\
          class ClusterBroker:\n    def __init__(self, redis_client, instance_id: str = None):\n        self.redis = redis_client\n\
          \        self.instance_id = instance_id or str(uuid.uuid4())[:8]\n        self.handlers: dict[str, Callable] = {}\n\
          \        self._subscriber = None\n        self._task = None\n\n    def on(self, message_type: str):\n        def\
          \ decorator(func):\n            self.handlers[message_type] = func\n            return func\n        return decorator\n\
          \n    async def start(self):\n        self._subscriber = self.redis.pubsub()\n        await self._subscriber.subscribe(\"\
          chat:cluster\")\n        self._task = asyncio.create_task(self._listen())\n\n    async def stop(self):\n       \
          \ if self._task:\n            self._task.cancel()\n        if self._subscriber:\n            await self._subscriber.unsubscribe()\n\
          \n    async def publish(self, message_type: str, payload: dict):\n        msg = ClusterMessage(\n            type=message_type,\n\
          \            source_instance=self.instance_id,\n            payload=payload\n        )\n        await self.redis.publish(\"\
          chat:cluster\", json.dumps(asdict(msg)))\n\n    async def _listen(self):\n        async for message in self._subscriber.listen():\n\
          \            if message[\"type\"] == \"message\":\n                try:\n                    data = json.loads(message[\"\
          data\"])\n                    cluster_msg = ClusterMessage(**data)\n\n                    # Skip messages from self\n\
          \                    if cluster_msg.source_instance == self.instance_id:\n                        continue\n\n \
          \                   if handler := self.handlers.get(cluster_msg.type):\n                        await handler(cluster_msg.payload)\n\
          \                except Exception as e:\n                    print(f\"Cluster message error: {e}\")\n\n# Usage in\
          \ chat server\nbroker = ClusterBroker(redis_client)\n\n@broker.on(\"chat_message\")\nasync def handle_cluster_message(payload):\n\
          \    message = ChatMessage(**payload)\n    # Deliver to local connections only\n    await local_router.deliver_local(message)\n\
          \n# When receiving a message from WebSocket\nasync def on_message(ws_message):\n    message = parse_message(ws_message)\n\
          \    # Store and publish to cluster\n    await history.store(message)\n    await broker.publish(\"chat_message\"\
          , asdict(message))\n    # Also deliver locally\n    await local_router.deliver_local(message)\n```"
      pitfalls:
      - Message duplication when publishing to self
      - Lost messages during Redis reconnection
      - Thundering herd when all instances reconnect simultaneously
  multiplayer-game-server:
    name: Multiplayer Game Server
    description: Build a real-time multiplayer game server with authoritative state, client prediction, lag compensation,
      and anti-cheat.
    why_important: Game servers push real-time systems to their limits with strict latency requirements, making this excellent
      practice for performance-critical code.
    difficulty: advanced
    tags:
    - real-time
    - networking
    - game-dev
    - performance
    estimated_hours: 60
    prerequisites:
    - websocket-server
    - realtime-chat
    learning_outcomes:
    - Implement authoritative server architecture
    - Build client-side prediction and reconciliation
    - Handle lag compensation for fair gameplay
    - Design tick-based game loops
    milestones:
    - name: Game Loop & Tick System
      description: Implement a fixed timestep game loop that processes input, updates state, and broadcasts at consistent
        intervals.
      hints:
        level1: Use fixed delta time (e.g., 60 ticks/second) for deterministic simulation.
        level2: Accumulate time and process multiple ticks if behind.
        level3: "```python\nimport asyncio\nimport time\nfrom dataclasses import dataclass\nfrom typing import Callable\n\n\
          @dataclass\nclass GameState:\n    tick: int = 0\n    entities: dict = None\n\n    def __post_init__(self):\n   \
          \     self.entities = self.entities or {}\n\nclass GameLoop:\n    def __init__(self, tick_rate: int = 60):\n   \
          \     self.tick_rate = tick_rate\n        self.tick_duration = 1.0 / tick_rate\n        self.state = GameState()\n\
          \        self.input_buffer: dict[int, list] = {}  # tick -> [inputs]\n        self.running = False\n        self._update_callbacks:\
          \ list[Callable] = []\n        self._broadcast_callback: Callable = None\n\n    def on_update(self, callback: Callable):\n\
          \        self._update_callbacks.append(callback)\n\n    def set_broadcast(self, callback: Callable):\n        self._broadcast_callback\
          \ = callback\n\n    def queue_input(self, player_id: str, input_data: dict, client_tick: int):\n        \"\"\"Queue\
          \ player input for processing at specific tick.\"\"\"\n        target_tick = max(client_tick, self.state.tick)\n\
          \        self.input_buffer.setdefault(target_tick, []).append({\n            'player_id': player_id,\n         \
          \   'input': input_data,\n            'client_tick': client_tick\n        })\n\n    async def start(self):\n   \
          \     self.running = True\n        last_time = time.perf_counter()\n        accumulator = 0.0\n\n        while self.running:\n\
          \            current_time = time.perf_counter()\n            frame_time = current_time - last_time\n           \
          \ last_time = current_time\n\n            accumulator += frame_time\n\n            # Process fixed timestep ticks\n\
          \            while accumulator >= self.tick_duration:\n                self._process_tick()\n                accumulator\
          \ -= self.tick_duration\n\n            # Broadcast state\n            if self._broadcast_callback:\n           \
          \     await self._broadcast_callback(self.state)\n\n            # Sleep to maintain tick rate\n            sleep_time\
          \ = self.tick_duration - (time.perf_counter() - current_time)\n            if sleep_time > 0:\n                await\
          \ asyncio.sleep(sleep_time)\n\n    def _process_tick(self):\n        # Process inputs for this tick\n        inputs\
          \ = self.input_buffer.pop(self.state.tick, [])\n        for input_data in inputs:\n            self._apply_input(input_data)\n\
          \n        # Run update callbacks\n        for callback in self._update_callbacks:\n            callback(self.state,\
          \ self.tick_duration)\n\n        self.state.tick += 1\n\n    def _apply_input(self, input_data: dict):\n       \
          \ player_id = input_data['player_id']\n        if entity := self.state.entities.get(player_id):\n            entity.apply_input(input_data['input'])\n\
          ```"
      pitfalls:
      - Variable delta time causes non-deterministic simulation
      - Sleeping exact tick duration ignores processing time
      - Unbounded accumulator causes spiral of death when behind
    - name: Client Prediction & Reconciliation
      description: Implement client-side prediction for responsive controls with server reconciliation to correct mispredictions.
      hints:
        level1: Client predicts movement locally, server is authoritative.
        level2: Store input history, replay from last acknowledged tick on correction.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport copy\n\n@dataclass\n\
          class InputFrame:\n    tick: int\n    input_data: dict\n    predicted_state: dict\n\n@dataclass\nclass ClientPrediction:\n\
          \    entity_id: str\n    current_state: dict\n    pending_inputs: list[InputFrame] = field(default_factory=list)\n\
          \    last_server_tick: int = 0\n\n    def apply_input(self, tick: int, input_data: dict, physics):\n        \"\"\
          \"Apply input locally and store for reconciliation.\"\"\"\n        # Predict new state\n        predicted = physics.simulate(self.current_state,\
          \ input_data)\n\n        self.pending_inputs.append(InputFrame(\n            tick=tick,\n            input_data=input_data,\n\
          \            predicted_state=copy.deepcopy(predicted)\n        ))\n\n        self.current_state = predicted\n  \
          \      return predicted\n\n    def reconcile(self, server_tick: int, server_state: dict, physics):\n        \"\"\
          \"Reconcile with server state, replay unacknowledged inputs.\"\"\"\n        # Remove acknowledged inputs\n     \
          \   self.pending_inputs = [\n            inp for inp in self.pending_inputs if inp.tick > server_tick\n        ]\n\
          \        self.last_server_tick = server_tick\n\n        # Check if prediction matches\n        if not self.pending_inputs:\n\
          \            self.current_state = server_state\n            return False  # No correction needed\n\n        # Check\
          \ first pending prediction against server\n        first_pending = self.pending_inputs[0]\n        if self._states_match(first_pending.predicted_state,\
          \ server_state):\n            return False  # Prediction was correct\n\n        # Misprediction - replay from server\
          \ state\n        self.current_state = copy.deepcopy(server_state)\n        for inp in self.pending_inputs:\n   \
          \         self.current_state = physics.simulate(\n                self.current_state, inp.input_data\n         \
          \   )\n            inp.predicted_state = copy.deepcopy(self.current_state)\n\n        return True  # Correction\
          \ applied\n\n    def _states_match(self, a: dict, b: dict, tolerance: float = 0.01) -> bool:\n        for key in\
          \ ['x', 'y', 'z']:\n            if abs(a.get(key, 0) - b.get(key, 0)) > tolerance:\n                return False\n\
          \        return True\n\n# Server-side validation\nclass AuthoritativeServer:\n    def __init__(self):\n        self.player_states:\
          \ dict[str, dict] = {}\n        self.physics = PhysicsEngine()\n\n    def process_input(self, player_id: str, input_data:\
          \ dict, client_tick: int):\n        state = self.player_states.get(player_id, {})\n\n        # Validate input (anti-cheat)\n\
          \        if not self._validate_input(input_data):\n            return None  # Reject invalid input\n\n        #\
          \ Apply authoritative simulation\n        new_state = self.physics.simulate(state, input_data)\n        self.player_states[player_id]\
          \ = new_state\n\n        return {\n            'tick': client_tick,\n            'state': new_state\n        }\n\
          ```"
      pitfalls:
      - Not storing enough input history causes rubber-banding
      - Exact float comparison fails due to precision
      - Overcorrecting causes jittery movement
    - name: Lag Compensation
      description: Implement server-side lag compensation to make hit detection fair for high-latency players.
      hints:
        level1: Store history of game states, rewind to player's perceived time.
        level2: Interpolate between stored states for accurate reconstruction.
        level3: "```python\nfrom dataclasses import dataclass\nfrom collections import deque\nimport copy\n\n@dataclass\n\
          class StateSnapshot:\n    tick: int\n    timestamp: float\n    entities: dict  # entity_id -> state\n\nclass LagCompensation:\n\
          \    def __init__(self, history_duration: float = 1.0, tick_rate: int = 60):\n        self.history: deque[StateSnapshot]\
          \ = deque()\n        self.history_duration = history_duration\n        self.tick_duration = 1.0 / tick_rate\n  \
          \      self.max_history = int(history_duration * tick_rate)\n\n    def record(self, tick: int, timestamp: float,\
          \ entities: dict):\n        snapshot = StateSnapshot(\n            tick=tick,\n            timestamp=timestamp,\n\
          \            entities=copy.deepcopy(entities)\n        )\n        self.history.append(snapshot)\n\n        # Prune\
          \ old history\n        while len(self.history) > self.max_history:\n            self.history.popleft()\n\n    def\
          \ get_state_at_time(self, target_time: float) -> dict:\n        \"\"\"Get interpolated entity states at specific\
          \ time.\"\"\"\n        if not self.history:\n            return {}\n\n        # Find surrounding snapshots\n   \
          \     before = None\n        after = None\n\n        for snapshot in self.history:\n            if snapshot.timestamp\
          \ <= target_time:\n                before = snapshot\n            else:\n                after = snapshot\n    \
          \            break\n\n        if before is None:\n            return self.history[0].entities\n        if after\
          \ is None:\n            return self.history[-1].entities\n\n        # Interpolate between snapshots\n        t =\
          \ (target_time - before.timestamp) / (after.timestamp - before.timestamp)\n        return self._interpolate_entities(before.entities,\
          \ after.entities, t)\n\n    def _interpolate_entities(self, before: dict, after: dict, t: float) -> dict:\n    \
          \    result = {}\n        for entity_id in before:\n            if entity_id in after:\n                result[entity_id]\
          \ = self._interpolate_state(\n                    before[entity_id], after[entity_id], t\n                )\n  \
          \      return result\n\n    def _interpolate_state(self, a: dict, b: dict, t: float) -> dict:\n        return {\n\
          \            'x': a['x'] + (b['x'] - a['x']) * t,\n            'y': a['y'] + (b['y'] - a['y']) * t,\n          \
          \  'z': a.get('z', 0) + (b.get('z', 0) - a.get('z', 0)) * t,\n        }\n\nclass HitDetection:\n    def __init__(self,\
          \ lag_comp: LagCompensation):\n        self.lag_comp = lag_comp\n\n    def check_hit(self, shooter_id: str, target_id:\
          \ str,\n                  shot_origin: dict, shot_direction: dict,\n                  client_timestamp: float, max_rewind:\
          \ float = 0.2):\n        \"\"\"Check if shot hits with lag compensation.\"\"\"\n        # Limit rewind to prevent\
          \ abuse\n        current_time = time.time()\n        rewind_time = min(\n            current_time - client_timestamp,\n\
          \            max_rewind\n        )\n\n        # Get world state at shooter's perceived time\n        perceived_time\
          \ = current_time - rewind_time\n        world_state = self.lag_comp.get_state_at_time(perceived_time)\n\n      \
          \  if target_id not in world_state:\n            return False\n\n        target_state = world_state[target_id]\n\
          \        return self._ray_intersects_hitbox(\n            shot_origin, shot_direction, target_state\n        )\n\
          ```"
      pitfalls:
      - Unlimited rewind allows shooting around corners
      - Memory grows unbounded without history pruning
      - Interpolation fails at entity spawn/despawn boundaries
    - name: State Synchronization
      description: Efficiently synchronize game state to clients using delta compression, interest management, and prioritization.
      hints:
        level1: Send only changed values, not full state.
        level2: Prioritize nearby entities, reduce updates for distant ones.
        level3: "```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport math\n\n@dataclass\nclass\
          \ EntityPriority:\n    entity_id: str\n    priority: float\n    last_update_tick: int\n\nclass StateSynchronizer:\n\
          \    def __init__(self, tick_rate: int = 60):\n        self.tick_rate = tick_rate\n        self.client_states: dict[str,\
          \ dict] = {}  # client_id -> last sent state per entity\n        self.update_rates: dict[float, int] = {\n     \
          \       1.0: 1,    # Priority 1.0: every tick\n            0.5: 2,    # Priority 0.5: every 2 ticks\n          \
          \  0.25: 4,   # Priority 0.25: every 4 ticks\n            0.1: 10,   # Priority 0.1: every 10 ticks\n        }\n\
          \n    def calculate_priority(self, viewer_state: dict, entity_state: dict,\n                          entity_type:\
          \ str) -> float:\n        # Distance-based priority\n        dx = viewer_state['x'] - entity_state['x']\n      \
          \  dy = viewer_state['y'] - entity_state['y']\n        distance = math.sqrt(dx*dx + dy*dy)\n\n        # Base priority\
          \ from distance\n        if distance < 10:\n            priority = 1.0\n        elif distance < 50:\n          \
          \  priority = 0.5\n        elif distance < 100:\n            priority = 0.25\n        else:\n            priority\
          \ = 0.1\n\n        # Boost for important entity types\n        if entity_type == 'player':\n            priority\
          \ = min(1.0, priority * 1.5)\n        elif entity_type == 'projectile':\n            priority = min(1.0, priority\
          \ * 2.0)\n\n        return priority\n\n    def generate_update(self, client_id: str, viewer_id: str,\n         \
          \              current_tick: int, world_state: dict) -> bytes:\n        client_last = self.client_states.setdefault(client_id,\
          \ {})\n        viewer_state = world_state.get(viewer_id, {'x': 0, 'y': 0})\n\n        updates = []\n\n        for\
          \ entity_id, entity_state in world_state.items():\n            priority = self.calculate_priority(\n           \
          \     viewer_state, entity_state, entity_state.get('type', 'generic')\n            )\n\n            # Check if should\
          \ update this tick based on priority\n            update_interval = self._get_update_interval(priority)\n      \
          \      if current_tick % update_interval != 0:\n                continue\n\n            # Delta compression\n  \
          \          last_state = client_last.get(entity_id, {})\n            delta = self._compute_delta(last_state, entity_state)\n\
          \n            if delta:\n                updates.append({\n                    'id': entity_id,\n              \
          \      'delta': delta,\n                    'full': len(delta) > len(entity_state) // 2\n                })\n  \
          \              client_last[entity_id] = entity_state.copy()\n\n        return self._encode_updates(updates)\n\n\
          \    def _compute_delta(self, old: dict, new: dict) -> dict:\n        delta = {}\n        for key, value in new.items():\n\
          \            if key not in old or old[key] != value:\n                delta[key] = value\n        return delta\n\
          \n    def _get_update_interval(self, priority: float) -> int:\n        for p, interval in sorted(self.update_rates.items(),\
          \ reverse=True):\n            if priority >= p:\n                return interval\n        return 10  # Default slow\
          \ rate\n```"
      pitfalls:
      - Sending full state every tick overwhelms bandwidth
      - Delta without baseline causes desync
      - Priority calculation per entity per client is O(n*m)
  collaborative-editor:
    name: Collaborative Text Editor
    description: Build a real-time collaborative text editor using CRDTs or Operational Transformation for conflict-free concurrent
      editing.
    why_important: Collaborative editing is used in Google Docs, Notion, Figma. Understanding CRDTs and OT is valuable for
      any real-time collaborative application.
    difficulty: advanced
    tags:
    - distributed-systems
    - real-time
    - algorithms
    estimated_hours: 50
    prerequisites:
    - realtime-chat
    learning_outcomes:
    - Implement Operational Transformation or CRDTs
    - Handle concurrent edits without conflicts
    - Build cursor presence and selection sync
    - Design efficient document synchronization
    milestones:
    - name: Operation-based CRDT
      description: Implement a sequence CRDT (like RGA or YATA) for conflict-free text insertion and deletion.
      hints:
        level1: Assign unique IDs to each character, order by ID for consistent view.
        level2: Use Lamport timestamps + site ID for globally unique, orderable IDs.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport bisect\n\n@dataclass(frozen=True,\
          \ order=True)\nclass CharId:\n    timestamp: int\n    site_id: str\n    seq: int = 0\n\n    def __str__(self):\n\
          \        return f\"{self.timestamp}.{self.site_id}.{self.seq}\"\n\n@dataclass\nclass Char:\n    id: CharId\n   \
          \ value: str\n    deleted: bool = False\n\n@dataclass\nclass RGADocument:\n    \"\"\"Replicated Growable Array -\
          \ a sequence CRDT.\"\"\"\n    site_id: str\n    chars: list[Char] = field(default_factory=list)\n    timestamp:\
          \ int = 0\n\n    def _next_id(self) -> CharId:\n        self.timestamp += 1\n        return CharId(self.timestamp,\
          \ self.site_id)\n\n    def _find_position(self, after_id: Optional[CharId]) -> int:\n        if after_id is None:\n\
          \            return 0\n        for i, char in enumerate(self.chars):\n            if char.id == after_id:\n    \
          \            return i + 1\n        return len(self.chars)\n\n    def _visible_index_to_position(self, index: int)\
          \ -> int:\n        \"\"\"Convert visible text index to internal position.\"\"\"\n        visible = 0\n        for\
          \ i, char in enumerate(self.chars):\n            if not char.deleted:\n                if visible == index:\n  \
          \                  return i\n                visible += 1\n        return len(self.chars)\n\n    def insert(self,\
          \ index: int, value: str) -> dict:\n        \"\"\"Local insert at visible index.\"\"\"\n        pos = self._visible_index_to_position(index)\n\
          \        after_id = self.chars[pos - 1].id if pos > 0 else None\n\n        char_id = self._next_id()\n        char\
          \ = Char(id=char_id, value=value)\n\n        # Find correct position respecting ID ordering\n        insert_pos\
          \ = pos\n        while insert_pos < len(self.chars):\n            if self.chars[insert_pos].id < char_id:\n    \
          \            break\n            insert_pos += 1\n\n        self.chars.insert(insert_pos, char)\n\n        return\
          \ {\n            'type': 'insert',\n            'id': char_id,\n            'after': after_id,\n            'value':\
          \ value\n        }\n\n    def delete(self, index: int) -> dict:\n        \"\"\"Local delete at visible index (tombstone).\"\
          \"\"\n        pos = self._visible_index_to_position(index)\n        self.chars[pos].deleted = True\n\n        return\
          \ {\n            'type': 'delete',\n            'id': self.chars[pos].id\n        }\n\n    def apply_remote(self,\
          \ op: dict):\n        \"\"\"Apply operation from remote site.\"\"\"\n        if op['type'] == 'insert':\n      \
          \      self._apply_remote_insert(op)\n        elif op['type'] == 'delete':\n            self._apply_remote_delete(op)\n\
          \n        # Update local timestamp\n        self.timestamp = max(self.timestamp, op['id'].timestamp)\n\n    def\
          \ _apply_remote_insert(self, op: dict):\n        char = Char(id=op['id'], value=op['value'])\n        pos = self._find_position(op['after'])\n\
          \n        # Find correct position respecting ID ordering\n        while pos < len(self.chars):\n            if self.chars[pos].id\
          \ < char.id:\n                break\n            pos += 1\n\n        self.chars.insert(pos, char)\n\n    def _apply_remote_delete(self,\
          \ op: dict):\n        for char in self.chars:\n            if char.id == op['id']:\n                char.deleted\
          \ = True\n                break\n\n    def get_text(self) -> str:\n        return ''.join(c.value for c in self.chars\
          \ if not c.deleted)\n```"
      pitfalls:
      - Timestamp collision when sites have same clock
      - Memory grows unbounded with tombstones
      - O(n) lookup for every operation is slow on large docs
    - name: Cursor Presence
      description: Synchronize cursor positions and selections across all connected editors with colored indicators.
      hints:
        level1: Track cursor as (position, selection_start, selection_end) per user.
        level2: Transform cursor positions when local/remote edits occur.
        level3: "```python\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass CursorState:\n\
          \    user_id: str\n    position: int\n    selection_start: Optional[int] = None\n    selection_end: Optional[int]\
          \ = None\n    color: str = \"#3498db\"\n    name: str = \"Anonymous\"\n\nclass CursorManager:\n    def __init__(self,\
          \ document):\n        self.document = document\n        self.cursors: dict[str, CursorState] = {}\n\n    def update_local(self,\
          \ user_id: str, position: int,\n                    selection: tuple[int, int] = None) -> dict:\n        cursor\
          \ = self.cursors.setdefault(user_id, CursorState(user_id, 0))\n        cursor.position = position\n        if selection:\n\
          \            cursor.selection_start, cursor.selection_end = selection\n        else:\n            cursor.selection_start\
          \ = cursor.selection_end = None\n\n        return {\n            'type': 'cursor',\n            'user_id': user_id,\n\
          \            'position': position,\n            'selection': selection\n        }\n\n    def apply_remote_cursor(self,\
          \ op: dict):\n        user_id = op['user_id']\n        cursor = self.cursors.setdefault(user_id, CursorState(user_id,\
          \ 0))\n        cursor.position = op['position']\n        if op.get('selection'):\n            cursor.selection_start,\
          \ cursor.selection_end = op['selection']\n\n    def transform_on_insert(self, insert_pos: int, length: int,\n  \
          \                         author_id: str):\n        \"\"\"Transform all cursors after an insert operation.\"\"\"\
          \n        for user_id, cursor in self.cursors.items():\n            # Don't transform author's cursor (they handle\
          \ it locally)\n            if user_id == author_id:\n                continue\n\n            if cursor.position\
          \ >= insert_pos:\n                cursor.position += length\n\n            if cursor.selection_start is not None:\n\
          \                if cursor.selection_start >= insert_pos:\n                    cursor.selection_start += length\n\
          \                if cursor.selection_end >= insert_pos:\n                    cursor.selection_end += length\n\n\
          \    def transform_on_delete(self, delete_pos: int, length: int,\n                           author_id: str):\n\
          \        \"\"\"Transform all cursors after a delete operation.\"\"\"\n        delete_end = delete_pos + length\n\
          \n        for user_id, cursor in self.cursors.items():\n            if user_id == author_id:\n                continue\n\
          \n            # Cursor after deleted region\n            if cursor.position >= delete_end:\n                cursor.position\
          \ -= length\n            # Cursor within deleted region\n            elif cursor.position > delete_pos:\n      \
          \          cursor.position = delete_pos\n\n            # Transform selection\n            if cursor.selection_start\
          \ is not None:\n                cursor.selection_start = self._transform_point(\n                    cursor.selection_start,\
          \ delete_pos, delete_end, length\n                )\n                cursor.selection_end = self._transform_point(\n\
          \                    cursor.selection_end, delete_pos, delete_end, length\n                )\n\n    def _transform_point(self,\
          \ point: int, del_start: int,\n                        del_end: int, length: int) -> int:\n        if point >= del_end:\n\
          \            return point - length\n        elif point > del_start:\n            return del_start\n        return\
          \ point\n```"
      pitfalls:
      - Cursor flickers when transforming on every keystroke
      - Selection can become inverted (end < start) after transform
      - Not handling cursor at document boundaries
    - name: Operational Transformation
      description: Implement OT as alternative to CRDTs, with transform functions for insert/delete operations.
      hints:
        level1: Transform concurrent operations so they can be applied in any order.
        level2: Use server to serialize operations and determine canonical order.
        level3: "```python\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom enum import Enum\n\nclass OpType(Enum):\n\
          \    INSERT = \"insert\"\n    DELETE = \"delete\"\n\n@dataclass\nclass Operation:\n    type: OpType\n    position:\
          \ int\n    text: str = \"\"  # For insert\n    length: int = 0  # For delete\n    version: int = 0\n\ndef transform(op1:\
          \ Operation, op2: Operation) -> Operation:\n    \"\"\"Transform op1 against op2 (op2 was applied first).\"\"\"\n\
          \    if op1.type == OpType.INSERT and op2.type == OpType.INSERT:\n        return transform_insert_insert(op1, op2)\n\
          \    elif op1.type == OpType.INSERT and op2.type == OpType.DELETE:\n        return transform_insert_delete(op1,\
          \ op2)\n    elif op1.type == OpType.DELETE and op2.type == OpType.INSERT:\n        return transform_delete_insert(op1,\
          \ op2)\n    else:\n        return transform_delete_delete(op1, op2)\n\ndef transform_insert_insert(op1: Operation,\
          \ op2: Operation) -> Operation:\n    if op1.position <= op2.position:\n        return op1  # No transformation needed\n\
          \    else:\n        return Operation(\n            type=OpType.INSERT,\n            position=op1.position + len(op2.text),\n\
          \            text=op1.text,\n            version=op1.version\n        )\n\ndef transform_insert_delete(op1: Operation,\
          \ op2: Operation) -> Operation:\n    if op1.position <= op2.position:\n        return op1\n    elif op1.position\
          \ >= op2.position + op2.length:\n        return Operation(\n            type=OpType.INSERT,\n            position=op1.position\
          \ - op2.length,\n            text=op1.text,\n            version=op1.version\n        )\n    else:\n        # Insert\
          \ position was in deleted region\n        return Operation(\n            type=OpType.INSERT,\n            position=op2.position,\n\
          \            text=op1.text,\n            version=op1.version\n        )\n\ndef transform_delete_insert(op1: Operation,\
          \ op2: Operation) -> Operation:\n    if op2.position >= op1.position + op1.length:\n        return op1\n    elif\
          \ op2.position <= op1.position:\n        return Operation(\n            type=OpType.DELETE,\n            position=op1.position\
          \ + len(op2.text),\n            length=op1.length,\n            version=op1.version\n        )\n    else:\n    \
          \    # Insert splits the delete\n        # Delete text before insert, then after\n        return Operation(\n  \
          \          type=OpType.DELETE,\n            position=op1.position,\n            length=op1.length + len(op2.text),\n\
          \            version=op1.version\n        )\n\ndef transform_delete_delete(op1: Operation, op2: Operation) -> Operation:\n\
          \    # Complex case: overlapping deletes\n    end1 = op1.position + op1.length\n    end2 = op2.position + op2.length\n\
          \n    if end1 <= op2.position:\n        return op1  # op1 entirely before op2\n    elif op1.position >= end2:\n\
          \        return Operation(\n            type=OpType.DELETE,\n            position=op1.position - op2.length,\n \
          \           length=op1.length,\n            version=op1.version\n        )\n    else:\n        # Overlapping deletes\n\
          \        new_pos = min(op1.position, op2.position)\n        # Calculate remaining length after op2's deletion\n\
          \        overlap_start = max(op1.position, op2.position)\n        overlap_end = min(end1, end2)\n        overlap\
          \ = max(0, overlap_end - overlap_start)\n        new_length = op1.length - overlap\n\n        if new_length <= 0:\n\
          \            return None  # op1 is completely covered by op2\n\n        return Operation(\n            type=OpType.DELETE,\n\
          \            position=new_pos,\n            length=new_length,\n            version=op1.version\n        )\n\nclass\
          \ OTServer:\n    def __init__(self):\n        self.document = \"\"\n        self.version = 0\n        self.history:\
          \ list[Operation] = []\n\n    def apply(self, op: Operation) -> Operation:\n        # Transform against all operations\
          \ since client's version\n        transformed = op\n        for historical in self.history[op.version:]:\n     \
          \       transformed = transform(transformed, historical)\n            if transformed is None:\n                return\
          \ None\n\n        # Apply to document\n        if transformed.type == OpType.INSERT:\n            self.document\
          \ = (\n                self.document[:transformed.position] +\n                transformed.text +\n            \
          \    self.document[transformed.position:]\n            )\n        else:\n            self.document = (\n       \
          \         self.document[:transformed.position] +\n                self.document[transformed.position + transformed.length:]\n\
          \            )\n\n        transformed.version = self.version\n        self.history.append(transformed)\n       \
          \ self.version += 1\n\n        return transformed\n```"
      pitfalls:
      - 'Transform functions must be composable: T(T(a,b),c) = T(a,T(b,c))'
      - Overlapping delete transforms are notoriously tricky
      - History grows unbounded without garbage collection
    - name: Undo/Redo with Collaboration
      description: Implement undo/redo that works correctly with concurrent edits from multiple users.
      hints:
        level1: Each user has their own undo stack of their operations.
        level2: Undoing means inverting the operation and transforming against subsequent ops.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n@dataclass\nclass UndoManager:\n\
          \    user_id: str\n    undo_stack: list[Operation] = field(default_factory=list)\n    redo_stack: list[Operation]\
          \ = field(default_factory=list)\n    document: 'OTDocument' = None\n\n    def record(self, op: Operation):\n   \
          \     \"\"\"Record an operation for potential undo.\"\"\"\n        self.undo_stack.append(op)\n        self.redo_stack.clear()\
          \  # Clear redo on new action\n\n    def can_undo(self) -> bool:\n        return len(self.undo_stack) > 0\n\n  \
          \  def can_redo(self) -> bool:\n        return len(self.redo_stack) > 0\n\n    def undo(self) -> Optional[Operation]:\n\
          \        if not self.can_undo():\n            return None\n\n        op = self.undo_stack.pop()\n\n        # Create\
          \ inverse operation\n        inverse = self._invert(op)\n\n        # Transform inverse against all operations that\
          \ happened after op\n        transformed = self._transform_against_history(inverse, op.version)\n\n        if transformed:\n\
          \            self.redo_stack.append(op)\n            return transformed\n        return None\n\n    def redo(self)\
          \ -> Optional[Operation]:\n        if not self.can_redo():\n            return None\n\n        op = self.redo_stack.pop()\n\
          \n        # Transform the original operation against subsequent history\n        transformed = self._transform_against_history(op,\
          \ op.version)\n\n        if transformed:\n            self.undo_stack.append(op)\n            return transformed\n\
          \        return None\n\n    def _invert(self, op: Operation) -> Operation:\n        \"\"\"Create the inverse of\
          \ an operation.\"\"\"\n        if op.type == OpType.INSERT:\n            return Operation(\n                type=OpType.DELETE,\n\
          \                position=op.position,\n                length=len(op.text),\n                version=self.document.version\n\
          \            )\n        else:\n            # For delete, we need the deleted text\n            # This requires storing\
          \ deleted text with operation\n            return Operation(\n                type=OpType.INSERT,\n            \
          \    position=op.position,\n                text=op.deleted_text,  # Need to store this\n                version=self.document.version\n\
          \            )\n\n    def _transform_against_history(self, op: Operation,\n                                   since_version:\
          \ int) -> Optional[Operation]:\n        transformed = op\n        for historical in self.document.history[since_version:]:\n\
          \            # Skip our own operations (they're in undo stack)\n            if historical.user_id == self.user_id:\n\
          \                continue\n            transformed = transform(transformed, historical)\n            if transformed\
          \ is None:\n                return None\n        return transformed\n\n    def transform_stacks_on_remote(self,\
          \ remote_op: Operation):\n        \"\"\"Transform all stacks when a remote operation arrives.\"\"\"\n        self.undo_stack\
          \ = [\n            transform(op, remote_op) for op in self.undo_stack\n            if transform(op, remote_op) is\
          \ not None\n        ]\n        self.redo_stack = [\n            transform(op, remote_op) for op in self.redo_stack\n\
          \            if transform(op, remote_op) is not None\n        ]\n```"
      pitfalls:
      - Undoing a delete requires storing the deleted text
      - Undo stack becomes invalid if user disconnects/reconnects
      - Group related operations (e.g., typing) into single undo unit
  event-sourcing:
    name: Event Sourcing System
    description: Build an event-sourced system with event store, projections, snapshots, and CQRS pattern for complex domain
      modeling.
    why_important: Event sourcing provides audit trails, temporal queries, and robust distributed systems. Used in banking,
      e-commerce, and enterprise systems.
    difficulty: advanced
    tags:
    - distributed-systems
    - architecture
    - backend
    estimated_hours: 45
    prerequisites:
    - rest-api-design
    learning_outcomes:
    - Implement append-only event store
    - Build projections from event streams
    - Handle snapshots for performance
    - Apply CQRS for read/write separation
    milestones:
    - name: Event Store
      description: Build an append-only event store with optimistic concurrency, stream versioning, and event metadata.
      hints:
        level1: Store events with stream ID, version, timestamp, and payload.
        level2: Use expected version for optimistic concurrency control.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Any,\
          \ Optional\nimport json\nimport uuid\n\n@dataclass\nclass Event:\n    id: str\n    stream_id: str\n    version:\
          \ int\n    type: str\n    data: dict\n    metadata: dict\n    timestamp: datetime = field(default_factory=datetime.utcnow)\n\
          \n    @classmethod\n    def create(cls, stream_id: str, version: int, event_type: str,\n               data: dict,\
          \ metadata: dict = None):\n        return cls(\n            id=str(uuid.uuid4()),\n            stream_id=stream_id,\n\
          \            version=version,\n            type=event_type,\n            data=data,\n            metadata=metadata\
          \ or {}\n        )\n\nclass ConcurrencyError(Exception):\n    pass\n\nclass EventStore:\n    def __init__(self,\
          \ storage):\n        self.storage = storage  # Could be PostgreSQL, EventStoreDB, etc.\n\n    async def append(self,\
          \ stream_id: str, events: list[Event],\n                    expected_version: int = None) -> int:\n        \"\"\"\
          \n        Append events to stream with optimistic concurrency.\n        Returns new stream version.\n        \"\"\
          \"\n        async with self.storage.transaction() as tx:\n            # Get current version\n            current_version\
          \ = await self._get_stream_version(tx, stream_id)\n\n            # Check expected version for concurrency\n    \
          \        if expected_version is not None:\n                if current_version != expected_version:\n           \
          \         raise ConcurrencyError(\n                        f\"Expected version {expected_version}, \"\n        \
          \                f\"but stream is at {current_version}\"\n                    )\n\n            # Append events\n\
          \            new_version = current_version\n            for event in events:\n                new_version += 1\n\
          \                event.version = new_version\n                await self._store_event(tx, event)\n\n           \
          \ return new_version\n\n    async def read_stream(self, stream_id: str,\n                         from_version:\
          \ int = 0,\n                         to_version: int = None) -> list[Event]:\n        query = \"\"\"\n         \
          \   SELECT * FROM events\n            WHERE stream_id = $1 AND version >= $2\n        \"\"\"\n        params = [stream_id,\
          \ from_version]\n\n        if to_version is not None:\n            query += \" AND version <= $3\"\n           \
          \ params.append(to_version)\n\n        query += \" ORDER BY version ASC\"\n\n        rows = await self.storage.fetch(query,\
          \ *params)\n        return [self._row_to_event(row) for row in rows]\n\n    async def read_all(self, from_position:\
          \ int = 0,\n                      batch_size: int = 1000) -> list[Event]:\n        \"\"\"Read all events across\
          \ all streams (for projections).\"\"\"\n        rows = await self.storage.fetch(\n            \"\"\"\n         \
          \   SELECT * FROM events\n            WHERE global_position > $1\n            ORDER BY global_position ASC\n   \
          \         LIMIT $2\n            \"\"\",\n            from_position, batch_size\n        )\n        return [self._row_to_event(row)\
          \ for row in rows]\n\n    async def _get_stream_version(self, tx, stream_id: str) -> int:\n        result = await\
          \ tx.fetchval(\n            \"SELECT COALESCE(MAX(version), 0) FROM events WHERE stream_id = $1\",\n           \
          \ stream_id\n        )\n        return result\n\n    async def _store_event(self, tx, event: Event):\n        await\
          \ tx.execute(\n            \"\"\"\n            INSERT INTO events (id, stream_id, version, type, data, metadata,\
          \ timestamp)\n            VALUES ($1, $2, $3, $4, $5, $6, $7)\n            \"\"\",\n            event.id, event.stream_id,\
          \ event.version, event.type,\n            json.dumps(event.data), json.dumps(event.metadata), event.timestamp\n\
          \        )\n```"
      pitfalls:
      - Gap in versions makes stream unreadable
      - Large events bloat storage without compression
      - Missing global ordering breaks cross-stream projections
    - name: Aggregate & Event Sourcing
      description: Implement domain aggregates that are reconstituted from event history with command handlers.
      hints:
        level1: Aggregate state is built by replaying events in order.
        level2: Commands validate business rules, produce events.
        level3: "```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import\
          \ TypeVar, Generic\n\nT = TypeVar('T')\n\nclass Aggregate(ABC, Generic[T]):\n    def __init__(self):\n        self.id:\
          \ str = None\n        self.version: int = 0\n        self._changes: list[Event] = []\n\n    @abstractmethod\n  \
          \  def apply(self, event: Event):\n        \"\"\"Apply event to update aggregate state.\"\"\"\n        pass\n\n\
          \    def load(self, events: list[Event]):\n        \"\"\"Reconstitute aggregate from event history.\"\"\"\n    \
          \    for event in events:\n            self.apply(event)\n            self.version = event.version\n\n    def _raise_event(self,\
          \ event_type: str, data: dict):\n        \"\"\"Record new event (to be persisted).\"\"\"\n        event = Event.create(\n\
          \            stream_id=self.id,\n            version=self.version + len(self._changes) + 1,\n            event_type=event_type,\n\
          \            data=data\n        )\n        self._changes.append(event)\n        self.apply(event)\n\n    def get_changes(self)\
          \ -> list[Event]:\n        return self._changes\n\n    def clear_changes(self):\n        self._changes = []\n\n\
          # Example: Order aggregate\n@dataclass\nclass OrderItem:\n    product_id: str\n    quantity: int\n    price: float\n\
          \nclass Order(Aggregate):\n    def __init__(self):\n        super().__init__()\n        self.customer_id: str =\
          \ None\n        self.items: list[OrderItem] = []\n        self.status: str = \"draft\"\n        self.total: float\
          \ = 0\n\n    # Commands\n    def create(self, order_id: str, customer_id: str):\n        if self.id is not None:\n\
          \            raise ValueError(\"Order already exists\")\n        self._raise_event(\"OrderCreated\", {\n       \
          \     \"order_id\": order_id,\n            \"customer_id\": customer_id\n        })\n\n    def add_item(self, product_id:\
          \ str, quantity: int, price: float):\n        if self.status != \"draft\":\n            raise ValueError(\"Cannot\
          \ modify non-draft order\")\n        self._raise_event(\"ItemAdded\", {\n            \"product_id\": product_id,\n\
          \            \"quantity\": quantity,\n            \"price\": price\n        })\n\n    def submit(self):\n      \
          \  if self.status != \"draft\":\n            raise ValueError(\"Order already submitted\")\n        if not self.items:\n\
          \            raise ValueError(\"Cannot submit empty order\")\n        self._raise_event(\"OrderSubmitted\", {\n\
          \            \"total\": self.total\n        })\n\n    # Event handlers\n    def apply(self, event: Event):\n   \
          \     if event.type == \"OrderCreated\":\n            self.id = event.data[\"order_id\"]\n            self.customer_id\
          \ = event.data[\"customer_id\"]\n        elif event.type == \"ItemAdded\":\n            item = OrderItem(\n    \
          \            product_id=event.data[\"product_id\"],\n                quantity=event.data[\"quantity\"],\n      \
          \          price=event.data[\"price\"]\n            )\n            self.items.append(item)\n            self.total\
          \ += item.quantity * item.price\n        elif event.type == \"OrderSubmitted\":\n            self.status = \"submitted\"\
          \n\nclass AggregateRepository:\n    def __init__(self, event_store: EventStore, aggregate_type: type):\n       \
          \ self.event_store = event_store\n        self.aggregate_type = aggregate_type\n\n    async def load(self, aggregate_id:\
          \ str) -> Aggregate:\n        events = await self.event_store.read_stream(aggregate_id)\n        aggregate = self.aggregate_type()\n\
          \        aggregate.load(events)\n        return aggregate\n\n    async def save(self, aggregate: Aggregate):\n \
          \       changes = aggregate.get_changes()\n        if changes:\n            await self.event_store.append(\n   \
          \             aggregate.id,\n                changes,\n                expected_version=aggregate.version\n    \
          \        )\n            aggregate.clear_changes()\n```"
      pitfalls:
      - Business logic in apply() instead of command handlers
      - Forgetting to clear changes after save causes duplicates
      - Mutable event data allows accidental modification
    - name: Projections
      description: Build read models (projections) that update in response to events for efficient querying.
      hints:
        level1: Subscribe to events, update denormalized read model.
        level2: Track checkpoint to resume from last processed position.
        level3: "```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nimport asyncio\n\n@dataclass\n\
          class Checkpoint:\n    projection_name: str\n    position: int\n\nclass Projection(ABC):\n    def __init__(self,\
          \ name: str):\n        self.name = name\n\n    @abstractmethod\n    async def handle(self, event: Event):\n    \
          \    \"\"\"Handle a single event.\"\"\"\n        pass\n\n    @abstractmethod\n    def handles(self) -> list[str]:\n\
          \        \"\"\"Return list of event types this projection handles.\"\"\"\n        pass\n\nclass ProjectionEngine:\n\
          \    def __init__(self, event_store: EventStore, checkpoint_store):\n        self.event_store = event_store\n  \
          \      self.checkpoint_store = checkpoint_store\n        self.projections: list[Projection] = []\n        self._running\
          \ = False\n\n    def register(self, projection: Projection):\n        self.projections.append(projection)\n\n  \
          \  async def start(self):\n        self._running = True\n        while self._running:\n            await self._process_batch()\n\
          \            await asyncio.sleep(0.1)  # Poll interval\n\n    async def stop(self):\n        self._running = False\n\
          \n    async def _process_batch(self, batch_size: int = 100):\n        for projection in self.projections:\n    \
          \        checkpoint = await self.checkpoint_store.get(projection.name)\n            position = checkpoint.position\
          \ if checkpoint else 0\n\n            events = await self.event_store.read_all(\n                from_position=position,\n\
          \                batch_size=batch_size\n            )\n\n            for event in events:\n                if event.type\
          \ in projection.handles():\n                    await projection.handle(event)\n                position = event.global_position\n\
          \n            if events:\n                await self.checkpoint_store.save(\n                    Checkpoint(projection.name,\
          \ position)\n                )\n\n# Example projection: Order summary read model\nclass OrderSummaryProjection(Projection):\n\
          \    def __init__(self, db):\n        super().__init__(\"order_summary\")\n        self.db = db\n\n    def handles(self)\
          \ -> list[str]:\n        return [\"OrderCreated\", \"ItemAdded\", \"OrderSubmitted\"]\n\n    async def handle(self,\
          \ event: Event):\n        if event.type == \"OrderCreated\":\n            await self.db.execute(\n             \
          \   \"\"\"\n                INSERT INTO order_summaries (id, customer_id, status, total, item_count)\n         \
          \       VALUES ($1, $2, 'draft', 0, 0)\n                \"\"\",\n                event.data[\"order_id\"], event.data[\"\
          customer_id\"]\n            )\n        elif event.type == \"ItemAdded\":\n            await self.db.execute(\n \
          \               \"\"\"\n                UPDATE order_summaries\n                SET total = total + $1, item_count\
          \ = item_count + 1\n                WHERE id = $2\n                \"\"\",\n                event.data[\"quantity\"\
          ] * event.data[\"price\"],\n                event.stream_id\n            )\n        elif event.type == \"OrderSubmitted\"\
          :\n            await self.db.execute(\n                \"\"\"\n                UPDATE order_summaries SET status\
          \ = 'submitted' WHERE id = $1\n                \"\"\",\n                event.stream_id\n            )\n\n# Projection\
          \ for customer analytics\nclass CustomerAnalyticsProjection(Projection):\n    def __init__(self, db):\n        super().__init__(\"\
          customer_analytics\")\n        self.db = db\n\n    def handles(self) -> list[str]:\n        return [\"OrderSubmitted\"\
          ]\n\n    async def handle(self, event: Event):\n        # Update customer lifetime value\n        await self.db.execute(\n\
          \            \"\"\"\n            INSERT INTO customer_stats (customer_id, order_count, total_spent)\n          \
          \  VALUES ($1, 1, $2)\n            ON CONFLICT (customer_id) DO UPDATE\n            SET order_count = customer_stats.order_count\
          \ + 1,\n                total_spent = customer_stats.total_spent + $2\n            \"\"\",\n            event.metadata.get(\"\
          customer_id\"),\n            event.data[\"total\"]\n        )\n```"
      pitfalls:
      - Processing same event twice without idempotency corrupts data
      - Checkpoint saved before event processed loses events on crash
      - Slow projection blocks all others in single-threaded engine
    - name: Snapshots
      description: Implement aggregate snapshots to avoid replaying entire event history for performance.
      hints:
        level1: Periodically save aggregate state, load snapshot + events since.
        level2: Store snapshot version to know which events to replay.
        level3: "```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport json\n\n@dataclass\nclass\
          \ Snapshot:\n    aggregate_id: str\n    aggregate_type: str\n    version: int\n    state: dict\n\nclass SnapshotStore:\n\
          \    def __init__(self, storage):\n        self.storage = storage\n\n    async def save(self, snapshot: Snapshot):\n\
          \        await self.storage.execute(\n            \"\"\"\n            INSERT INTO snapshots (aggregate_id, aggregate_type,\
          \ version, state)\n            VALUES ($1, $2, $3, $4)\n            ON CONFLICT (aggregate_id) DO UPDATE\n     \
          \       SET version = $3, state = $4\n            WHERE snapshots.version < $3\n            \"\"\",\n          \
          \  snapshot.aggregate_id, snapshot.aggregate_type,\n            snapshot.version, json.dumps(snapshot.state)\n \
          \       )\n\n    async def load(self, aggregate_id: str) -> Optional[Snapshot]:\n        row = await self.storage.fetchrow(\n\
          \            \"SELECT * FROM snapshots WHERE aggregate_id = $1\",\n            aggregate_id\n        )\n       \
          \ if row:\n            return Snapshot(\n                aggregate_id=row[\"aggregate_id\"],\n                aggregate_type=row[\"\
          aggregate_type\"],\n                version=row[\"version\"],\n                state=json.loads(row[\"state\"])\n\
          \            )\n        return None\n\nclass SnapshottingRepository:\n    def __init__(self, event_store: EventStore,\n\
          \                 snapshot_store: SnapshotStore,\n                 aggregate_type: type,\n                 snapshot_frequency:\
          \ int = 100):\n        self.event_store = event_store\n        self.snapshot_store = snapshot_store\n        self.aggregate_type\
          \ = aggregate_type\n        self.snapshot_frequency = snapshot_frequency\n\n    async def load(self, aggregate_id:\
          \ str) -> Aggregate:\n        aggregate = self.aggregate_type()\n\n        # Try to load snapshot\n        snapshot\
          \ = await self.snapshot_store.load(aggregate_id)\n\n        if snapshot:\n            # Restore from snapshot\n\
          \            aggregate.restore_from_snapshot(snapshot.state)\n            aggregate.version = snapshot.version\n\
          \n            # Load events since snapshot\n            events = await self.event_store.read_stream(\n         \
          \       aggregate_id,\n                from_version=snapshot.version + 1\n            )\n        else:\n       \
          \     # Load all events\n            events = await self.event_store.read_stream(aggregate_id)\n\n        aggregate.load(events)\n\
          \        return aggregate\n\n    async def save(self, aggregate: Aggregate):\n        changes = aggregate.get_changes()\n\
          \        if not changes:\n            return\n\n        await self.event_store.append(\n            aggregate.id,\n\
          \            changes,\n            expected_version=aggregate.version\n        )\n\n        new_version = aggregate.version\
          \ + len(changes)\n        aggregate.clear_changes()\n\n        # Check if we should create a snapshot\n        if\
          \ new_version % self.snapshot_frequency == 0:\n            await self._create_snapshot(aggregate, new_version)\n\
          \n    async def _create_snapshot(self, aggregate: Aggregate, version: int):\n        snapshot = Snapshot(\n    \
          \        aggregate_id=aggregate.id,\n            aggregate_type=type(aggregate).__name__,\n            version=version,\n\
          \            state=aggregate.get_snapshot_state()\n        )\n        await self.snapshot_store.save(snapshot)\n\
          \n# Add to Order aggregate\nclass Order(Aggregate):\n    # ... previous code ...\n\n    def get_snapshot_state(self)\
          \ -> dict:\n        return {\n            \"id\": self.id,\n            \"customer_id\": self.customer_id,\n   \
          \         \"items\": [\n                {\"product_id\": i.product_id, \"quantity\": i.quantity, \"price\": i.price}\n\
          \                for i in self.items\n            ],\n            \"status\": self.status,\n            \"total\"\
          : self.total\n        }\n\n    def restore_from_snapshot(self, state: dict):\n        self.id = state[\"id\"]\n\
          \        self.customer_id = state[\"customer_id\"]\n        self.items = [\n            OrderItem(**item) for item\
          \ in state[\"items\"]\n        ]\n        self.status = state[\"status\"]\n        self.total = state[\"total\"\
          ]\n```"
      pitfalls:
      - Snapshot schema changes break deserialization
      - Creating snapshot on every save destroys performance
      - Snapshot without version causes event replay from beginning
  ci-cd-pipeline:
    name: CI/CD Pipeline
    description: Build a continuous integration and deployment pipeline that handles build, test, artifact management, and
      deployment automation.
    why_important: CI/CD is the backbone of modern software delivery. Understanding pipeline internals makes you better at
      debugging and optimizing delivery workflows.
    difficulty: intermediate
    tags:
    - devops
    - automation
    - infrastructure
    estimated_hours: 35
    prerequisites:
    - shell
    learning_outcomes:
    - Design multi-stage pipeline workflows
    - Implement build and test automation
    - Handle artifact versioning and storage
    - Automate deployment with rollback support
    milestones:
    - name: Pipeline Definition Parser
      description: Parse YAML pipeline definitions with stages, jobs, steps, and dependencies between them.
      hints:
        level1: Use YAML to define stages with ordered steps.
        level2: Build a DAG from job dependencies for execution order.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport yaml\nfrom enum\
          \ import Enum\n\nclass StepType(str, Enum):\n    SCRIPT = \"script\"\n    CHECKOUT = \"checkout\"\n    ARTIFACT_UPLOAD\
          \ = \"artifact_upload\"\n    ARTIFACT_DOWNLOAD = \"artifact_download\"\n    DEPLOY = \"deploy\"\n\n@dataclass\n\
          class Step:\n    name: str\n    type: StepType\n    script: Optional[str] = None\n    working_dir: str = \".\"\n\
          \    env: dict = field(default_factory=dict)\n    timeout: int = 600\n    continue_on_error: bool = False\n\n@dataclass\n\
          class Job:\n    name: str\n    steps: list[Step]\n    needs: list[str] = field(default_factory=list)\n    runs_on:\
          \ str = \"default\"\n    env: dict = field(default_factory=dict)\n    condition: Optional[str] = None\n\n@dataclass\n\
          class Stage:\n    name: str\n    jobs: list[Job]\n\n@dataclass\nclass Pipeline:\n    name: str\n    stages: list[Stage]\n\
          \    triggers: dict = field(default_factory=dict)\n    env: dict = field(default_factory=dict)\n\nclass PipelineParser:\n\
          \    def parse(self, yaml_content: str) -> Pipeline:\n        data = yaml.safe_load(yaml_content)\n\n        stages\
          \ = []\n        for stage_data in data.get('stages', []):\n            jobs = []\n            for job_name, job_data\
          \ in stage_data.get('jobs', {}).items():\n                steps = [\n                    Step(\n               \
          \         name=s.get('name', f'step-{i}'),\n                        type=StepType(s.get('type', 'script')),\n  \
          \                      script=s.get('script'),\n                        working_dir=s.get('working_dir', '.'),\n\
          \                        env=s.get('env', {}),\n                        timeout=s.get('timeout', 600),\n       \
          \                 continue_on_error=s.get('continue_on_error', False)\n                    )\n                 \
          \   for i, s in enumerate(job_data.get('steps', []))\n                ]\n                jobs.append(Job(\n    \
          \                name=job_name,\n                    steps=steps,\n                    needs=job_data.get('needs',\
          \ []),\n                    runs_on=job_data.get('runs_on', 'default'),\n                    env=job_data.get('env',\
          \ {}),\n                    condition=job_data.get('if')\n                ))\n            stages.append(Stage(name=stage_data['name'],\
          \ jobs=jobs))\n\n        return Pipeline(\n            name=data.get('name', 'pipeline'),\n            stages=stages,\n\
          \            triggers=data.get('triggers', {}),\n            env=data.get('env', {})\n        )\n\n    def build_execution_graph(self,\
          \ pipeline: Pipeline) -> dict:\n        \"\"\"Build DAG for job execution order.\"\"\"\n        graph = {}\n   \
          \     for stage in pipeline.stages:\n            for job in stage.jobs:\n                graph[job.name] = {\n \
          \                   'job': job,\n                    'dependencies': job.needs,\n                    'stage': stage.name\n\
          \                }\n        return graph\n\n    def topological_sort(self, graph: dict) -> list[str]:\n        \"\
          \"\"Return jobs in valid execution order.\"\"\"\n        visited = set()\n        order = []\n\n        def visit(name):\n\
          \            if name in visited:\n                return\n            visited.add(name)\n            for dep in\
          \ graph[name]['dependencies']:\n                visit(dep)\n            order.append(name)\n\n        for name in\
          \ graph:\n            visit(name)\n        return order\n```"
      pitfalls:
      - Circular dependencies cause infinite loop
      - Missing dependency reference crashes at runtime
      - YAML anchors and aliases need special handling
    - name: Job Executor
      description: Execute jobs in isolated environments with proper logging, timeout handling, and exit code propagation.
      hints:
        level1: Run each step in subprocess, capture output.
        level2: Implement timeout and cancellation handling.
        level3: "```python\nimport asyncio\nimport subprocess\nimport os\nfrom dataclasses import dataclass\nfrom datetime\
          \ import datetime\nfrom enum import Enum\n\nclass JobStatus(str, Enum):\n    PENDING = \"pending\"\n    RUNNING\
          \ = \"running\"\n    SUCCESS = \"success\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n    SKIPPED\
          \ = \"skipped\"\n\n@dataclass\nclass StepResult:\n    name: str\n    status: JobStatus\n    exit_code: int\n   \
          \ stdout: str\n    stderr: str\n    duration: float\n    started_at: datetime\n    finished_at: datetime\n\n@dataclass\n\
          class JobResult:\n    name: str\n    status: JobStatus\n    step_results: list[StepResult]\n    duration: float\n\
          \nclass JobExecutor:\n    def __init__(self, workspace: str):\n        self.workspace = workspace\n        self._cancelled\
          \ = False\n\n    async def execute(self, job: Job, context: dict) -> JobResult:\n        results = []\n        job_start\
          \ = datetime.utcnow()\n        status = JobStatus.SUCCESS\n\n        # Check condition\n        if job.condition\
          \ and not self._evaluate_condition(job.condition, context):\n            return JobResult(job.name, JobStatus.SKIPPED,\
          \ [], 0)\n\n        # Merge environment\n        env = {**os.environ, **context.get('env', {}), **job.env}\n\n \
          \       for step in job.steps:\n            if self._cancelled:\n                status = JobStatus.CANCELLED\n\
          \                break\n\n            step_env = {**env, **step.env}\n            result = await self._execute_step(step,\
          \ step_env)\n            results.append(result)\n\n            if result.status == JobStatus.FAILED and not step.continue_on_error:\n\
          \                status = JobStatus.FAILED\n                break\n\n        duration = (datetime.utcnow() - job_start).total_seconds()\n\
          \        return JobResult(job.name, status, results, duration)\n\n    async def _execute_step(self, step: Step,\
          \ env: dict) -> StepResult:\n        started = datetime.utcnow()\n\n        try:\n            if step.type == StepType.SCRIPT:\n\
          \                result = await self._run_script(step.script, env, step.timeout, step.working_dir)\n           \
          \ elif step.type == StepType.CHECKOUT:\n                result = await self._checkout(env)\n            else:\n\
          \                result = (0, \"\", \"\")\n\n            exit_code, stdout, stderr = result\n            status\
          \ = JobStatus.SUCCESS if exit_code == 0 else JobStatus.FAILED\n\n        except asyncio.TimeoutError:\n        \
          \    exit_code, stdout, stderr = -1, \"\", \"Step timed out\"\n            status = JobStatus.FAILED\n        except\
          \ Exception as e:\n            exit_code, stdout, stderr = -1, \"\", str(e)\n            status = JobStatus.FAILED\n\
          \n        finished = datetime.utcnow()\n        return StepResult(\n            name=step.name,\n            status=status,\n\
          \            exit_code=exit_code,\n            stdout=stdout,\n            stderr=stderr,\n            duration=(finished\
          \ - started).total_seconds(),\n            started_at=started,\n            finished_at=finished\n        )\n\n\
          \    async def _run_script(self, script: str, env: dict,\n                         timeout: int, working_dir: str)\
          \ -> tuple:\n        work_path = os.path.join(self.workspace, working_dir)\n\n        proc = await asyncio.create_subprocess_shell(\n\
          \            script,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n\
          \            env=env,\n            cwd=work_path\n        )\n\n        try:\n            stdout, stderr = await\
          \ asyncio.wait_for(\n                proc.communicate(), timeout=timeout\n            )\n            return proc.returncode,\
          \ stdout.decode(), stderr.decode()\n        except asyncio.TimeoutError:\n            proc.kill()\n            raise\n\
          \n    def cancel(self):\n        self._cancelled = True\n```"
      pitfalls:
      - Zombie processes when parent killed without cleanup
      - Environment variable injection security risk
      - Large output causes memory exhaustion
    - name: Artifact Management
      description: Implement artifact upload, download, and caching between pipeline stages with versioning.
      hints:
        level1: Store artifacts with run ID and path in object storage.
        level2: Implement cache keys based on file hashes for dependency caching.
        level3: "```python\nimport hashlib\nimport os\nimport tarfile\nimport io\nfrom dataclasses import dataclass\nfrom\
          \ typing import Optional\n\n@dataclass\nclass Artifact:\n    name: str\n    path: str\n    run_id: str\n    job_name:\
          \ str\n    size: int\n    checksum: str\n\nclass ArtifactManager:\n    def __init__(self, storage, cache_storage):\n\
          \        self.storage = storage  # S3, GCS, etc.\n        self.cache = cache_storage\n\n    async def upload(self,\
          \ run_id: str, job_name: str,\n                    name: str, local_path: str) -> Artifact:\n        \"\"\"Upload\
          \ artifact from local path.\"\"\"\n        # Create tarball\n        tar_buffer = io.BytesIO()\n        with tarfile.open(fileobj=tar_buffer,\
          \ mode='w:gz') as tar:\n            tar.add(local_path, arcname=os.path.basename(local_path))\n\n        tar_data\
          \ = tar_buffer.getvalue()\n        checksum = hashlib.sha256(tar_data).hexdigest()\n\n        # Upload to storage\n\
          \        storage_path = f\"artifacts/{run_id}/{job_name}/{name}.tar.gz\"\n        await self.storage.put(storage_path,\
          \ tar_data)\n\n        return Artifact(\n            name=name,\n            path=storage_path,\n            run_id=run_id,\n\
          \            job_name=job_name,\n            size=len(tar_data),\n            checksum=checksum\n        )\n\n \
          \   async def download(self, artifact: Artifact, local_path: str):\n        \"\"\"Download and extract artifact.\"\
          \"\"\n        data = await self.storage.get(artifact.path)\n\n        # Verify checksum\n        if hashlib.sha256(data).hexdigest()\
          \ != artifact.checksum:\n            raise ValueError(\"Artifact checksum mismatch\")\n\n        tar_buffer = io.BytesIO(data)\n\
          \        with tarfile.open(fileobj=tar_buffer, mode='r:gz') as tar:\n            tar.extractall(local_path)\n\n\
          \    async def cache_save(self, key: str, paths: list[str]):\n        \"\"\"Save paths to cache with key.\"\"\"\n\
          \        tar_buffer = io.BytesIO()\n        with tarfile.open(fileobj=tar_buffer, mode='w:gz') as tar:\n       \
          \     for path in paths:\n                if os.path.exists(path):\n                    tar.add(path)\n\n      \
          \  cache_path = f\"cache/{key}.tar.gz\"\n        await self.cache.put(cache_path, tar_buffer.getvalue())\n\n   \
          \ async def cache_restore(self, key: str, fallback_keys: list[str] = None) -> bool:\n        \"\"\"Restore from\
          \ cache, try fallback keys if primary misses.\"\"\"\n        keys_to_try = [key] + (fallback_keys or [])\n\n   \
          \     for k in keys_to_try:\n            cache_path = f\"cache/{k}.tar.gz\"\n            data = await self.cache.get(cache_path)\n\
          \            if data:\n                tar_buffer = io.BytesIO(data)\n                with tarfile.open(fileobj=tar_buffer,\
          \ mode='r:gz') as tar:\n                    tar.extractall('.')\n                return True\n        return False\n\
          \n    def compute_cache_key(self, prefix: str, files: list[str]) -> str:\n        \"\"\"Compute cache key from file\
          \ contents.\"\"\"\n        hasher = hashlib.sha256()\n        hasher.update(prefix.encode())\n\n        for file_path\
          \ in sorted(files):\n            if os.path.exists(file_path):\n                with open(file_path, 'rb') as f:\n\
          \                    hasher.update(f.read())\n\n        return hasher.hexdigest()[:16]\n```"
      pitfalls:
      - Symlinks in tarball can escape extraction directory
      - Cache key collision overwrites unrelated cache
      - Large artifacts exhaust memory without streaming
    - name: Deployment Strategies
      description: Implement blue-green and canary deployment strategies with health checks and automatic rollback.
      hints:
        level1: 'Blue-green: switch traffic between two identical environments.'
        level2: 'Canary: gradually shift traffic percentage to new version.'
        level3: "```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom enum import Enum\n\
          import asyncio\n\nclass DeploymentStatus(str, Enum):\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\
          \n    SUCCESS = \"success\"\n    FAILED = \"failed\"\n    ROLLED_BACK = \"rolled_back\"\n\n@dataclass\nclass DeploymentConfig:\n\
          \    service_name: str\n    version: str\n    replicas: int\n    health_check_path: str = \"/health\"\n    health_check_interval:\
          \ int = 5\n    health_check_timeout: int = 30\n\nclass DeploymentStrategy(ABC):\n    @abstractmethod\n    async\
          \ def deploy(self, config: DeploymentConfig) -> DeploymentStatus:\n        pass\n\n    @abstractmethod\n    async\
          \ def rollback(self, config: DeploymentConfig):\n        pass\n\nclass BlueGreenDeployment(DeploymentStrategy):\n\
          \    def __init__(self, infrastructure, load_balancer):\n        self.infra = infrastructure\n        self.lb =\
          \ load_balancer\n\n    async def deploy(self, config: DeploymentConfig) -> DeploymentStatus:\n        service =\
          \ config.service_name\n\n        # Determine current (blue) and new (green) environments\n        current = await\
          \ self.lb.get_active_environment(service)\n        new_env = \"green\" if current == \"blue\" else \"blue\"\n\n\
          \        try:\n            # Deploy to inactive environment\n            await self.infra.deploy(\n            \
          \    f\"{service}-{new_env}\",\n                config.version,\n                config.replicas\n            )\n\
          \n            # Wait for health checks\n            healthy = await self._wait_healthy(\n                f\"{service}-{new_env}\"\
          ,\n                config.health_check_path,\n                config.health_check_timeout\n            )\n\n   \
          \         if not healthy:\n                await self.infra.destroy(f\"{service}-{new_env}\")\n                return\
          \ DeploymentStatus.FAILED\n\n            # Switch traffic\n            await self.lb.switch_traffic(service, new_env)\n\
          \n            # Keep old environment for quick rollback\n            # (destroy after confirmation period)\n\n \
          \           return DeploymentStatus.SUCCESS\n\n        except Exception as e:\n            await self.rollback(config)\n\
          \            return DeploymentStatus.FAILED\n\n    async def rollback(self, config: DeploymentConfig):\n       \
          \ service = config.service_name\n        current = await self.lb.get_active_environment(service)\n        previous\
          \ = \"green\" if current == \"blue\" else \"blue\"\n        await self.lb.switch_traffic(service, previous)\n\n\
          \    async def _wait_healthy(self, env: str, path: str, timeout: int) -> bool:\n        deadline = asyncio.get_event_loop().time()\
          \ + timeout\n        while asyncio.get_event_loop().time() < deadline:\n            if await self.infra.health_check(env,\
          \ path):\n                return True\n            await asyncio.sleep(5)\n        return False\n\nclass CanaryDeployment(DeploymentStrategy):\n\
          \    def __init__(self, infrastructure, load_balancer, metrics):\n        self.infra = infrastructure\n        self.lb\
          \ = load_balancer\n        self.metrics = metrics\n        self.stages = [5, 25, 50, 75, 100]  # Traffic percentages\n\
          \        self.stage_duration = 300  # 5 minutes per stage\n\n    async def deploy(self, config: DeploymentConfig)\
          \ -> DeploymentStatus:\n        service = config.service_name\n        canary = f\"{service}-canary\"\n\n      \
          \  try:\n            # Deploy canary with minimal replicas\n            await self.infra.deploy(canary, config.version,\
          \ 1)\n\n            # Wait for initial health\n            if not await self._wait_healthy(canary, config.health_check_path,\
          \ 60):\n                await self.infra.destroy(canary)\n                return DeploymentStatus.FAILED\n\n   \
          \         # Progressive traffic shift\n            for percentage in self.stages:\n                await self.lb.set_canary_weight(service,\
          \ percentage)\n\n                # Monitor for anomalies\n                healthy = await self._monitor_stage(\n\
          \                    service, canary, self.stage_duration\n                )\n\n                if not healthy:\n\
          \                    await self._auto_rollback(service, canary)\n                    return DeploymentStatus.ROLLED_BACK\n\
          \n            # Canary successful - promote to production\n            await self.infra.scale(canary, config.replicas)\n\
          \            await self.lb.promote_canary(service)\n            await self.infra.destroy(f\"{service}-stable\")\n\
          \n            return DeploymentStatus.SUCCESS\n\n        except Exception:\n            await self._auto_rollback(service,\
          \ canary)\n            return DeploymentStatus.FAILED\n\n    async def _monitor_stage(self, service: str, canary:\
          \ str,\n                            duration: int) -> bool:\n        \"\"\"Monitor canary health during traffic\
          \ stage.\"\"\"\n        end_time = asyncio.get_event_loop().time() + duration\n\n        while asyncio.get_event_loop().time()\
          \ < end_time:\n            # Check error rate\n            canary_errors = await self.metrics.get_error_rate(canary)\n\
          \            stable_errors = await self.metrics.get_error_rate(f\"{service}-stable\")\n\n            # Canary error\
          \ rate significantly higher = problem\n            if canary_errors > stable_errors * 1.5:\n                return\
          \ False\n\n            # Check latency\n            canary_p99 = await self.metrics.get_latency_p99(canary)\n  \
          \          stable_p99 = await self.metrics.get_latency_p99(f\"{service}-stable\")\n\n            if canary_p99 >\
          \ stable_p99 * 2:\n                return False\n\n            await asyncio.sleep(30)\n\n        return True\n\n\
          \    async def _auto_rollback(self, service: str, canary: str):\n        await self.lb.set_canary_weight(service,\
          \ 0)\n        await self.infra.destroy(canary)\n\n    async def rollback(self, config: DeploymentConfig):\n    \
          \    await self._auto_rollback(config.service_name, f\"{config.service_name}-canary\")\n```"
      pitfalls:
      - Database migrations incompatible between versions
      - Session affinity breaks during traffic switch
      - Metrics lag causes delayed rollback decision
  container-runtime:
    name: Container Runtime
    description: Build a minimal container runtime using Linux namespaces, cgroups, and overlay filesystem for process isolation.
    why_important: Understanding containers at the kernel level helps debug container issues, optimize performance, and secure
      deployments.
    difficulty: advanced
    tags:
    - containers
    - linux
    - systems
    estimated_hours: 50
    prerequisites:
    - shell
    learning_outcomes:
    - Use Linux namespaces for isolation
    - Implement cgroups for resource limits
    - Build overlay filesystem for images
    - Handle container networking
    milestones:
    - name: Process Isolation with Namespaces
      description: Create isolated process environment using PID, mount, network, UTS, and user namespaces.
      hints:
        level1: Use clone() with CLONE_NEWPID, CLONE_NEWNS flags.
        level2: Set up new root filesystem with pivot_root or chroot.
        level3: "```c\n#define _GNU_SOURCE\n#include <sched.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n\
          #include <sys/wait.h>\n#include <sys/mount.h>\n#include <sys/syscall.h>\n\n#define STACK_SIZE (1024 * 1024)\n\n\
          static char child_stack[STACK_SIZE];\n\ntypedef struct {\n    char *rootfs;\n    char *hostname;\n    char **argv;\n\
          } container_config;\n\nstatic int pivot_root(const char *new_root, const char *put_old) {\n    return syscall(SYS_pivot_root,\
          \ new_root, put_old);\n}\n\nstatic int setup_root(const char *rootfs) {\n    // Mount rootfs as private\n    if\
          \ (mount(NULL, \"/\", NULL, MS_REC | MS_PRIVATE, NULL) < 0) {\n        perror(\"mount private\");\n        return\
          \ -1;\n    }\n\n    // Bind mount new root\n    if (mount(rootfs, rootfs, NULL, MS_BIND | MS_REC, NULL) < 0) {\n\
          \        perror(\"mount bind rootfs\");\n        return -1;\n    }\n\n    // Create put_old directory\n    char\
          \ put_old[256];\n    snprintf(put_old, sizeof(put_old), \"%s/.pivot_root\", rootfs);\n    mkdir(put_old, 0700);\n\
          \n    // Pivot root\n    if (pivot_root(rootfs, put_old) < 0) {\n        perror(\"pivot_root\");\n        return\
          \ -1;\n    }\n\n    // Change to new root\n    if (chdir(\"/\") < 0) {\n        perror(\"chdir\");\n        return\
          \ -1;\n    }\n\n    // Unmount old root\n    if (umount2(\"/.pivot_root\", MNT_DETACH) < 0) {\n        perror(\"\
          umount old root\");\n        return -1;\n    }\n    rmdir(\"/.pivot_root\");\n\n    return 0;\n}\n\nstatic int setup_mounts(void)\
          \ {\n    // Mount proc\n    if (mount(\"proc\", \"/proc\", \"proc\", 0, NULL) < 0) {\n        perror(\"mount proc\"\
          );\n        return -1;\n    }\n\n    // Mount sysfs\n    if (mount(\"sysfs\", \"/sys\", \"sysfs\", 0, NULL) < 0)\
          \ {\n        perror(\"mount sysfs\");\n        return -1;\n    }\n\n    // Mount tmpfs for /tmp\n    if (mount(\"\
          tmpfs\", \"/tmp\", \"tmpfs\", 0, NULL) < 0) {\n        perror(\"mount tmpfs\");\n        return -1;\n    }\n\n \
          \   return 0;\n}\n\nstatic int child_fn(void *arg) {\n    container_config *config = (container_config *)arg;\n\n\
          \    // Set hostname\n    if (sethostname(config->hostname, strlen(config->hostname)) < 0) {\n        perror(\"\
          sethostname\");\n        return 1;\n    }\n\n    // Setup root filesystem\n    if (setup_root(config->rootfs) <\
          \ 0) {\n        return 1;\n    }\n\n    // Setup mounts\n    if (setup_mounts() < 0) {\n        return 1;\n    }\n\
          \n    // Execute command\n    execvp(config->argv[0], config->argv);\n    perror(\"execvp\");\n    return 1;\n}\n\
          \nint run_container(container_config *config) {\n    int flags = CLONE_NEWPID |  // New PID namespace\n        \
          \        CLONE_NEWNS |   // New mount namespace\n                CLONE_NEWUTS |  // New UTS namespace (hostname)\n\
          \                CLONE_NEWNET |  // New network namespace\n                CLONE_NEWIPC |  // New IPC namespace\n\
          \                SIGCHLD;\n\n    pid_t pid = clone(child_fn, child_stack + STACK_SIZE, flags, config);\n    if (pid\
          \ < 0) {\n        perror(\"clone\");\n        return -1;\n    }\n\n    // Wait for child\n    int status;\n    waitpid(pid,\
          \ &status, 0);\n    return WEXITSTATUS(status);\n}\n```"
      pitfalls:
      - pivot_root fails if new root is not a mount point
      - Forgetting to mount /proc breaks process tools
      - User namespace UID mapping required for unprivileged containers
    - name: Resource Limits with Cgroups
      description: Implement CPU, memory, and I/O limits using cgroups v2 for container resource control.
      hints:
        level1: Write to cgroup filesystem to set limits.
        level2: Create cgroup hierarchy per container, add process to group.
        level3: "```python\nimport os\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\n@dataclass\nclass ResourceLimits:\n\
          \    memory_limit: int = 0      # bytes, 0 = unlimited\n    memory_swap: int = 0       # bytes\n    cpu_shares:\
          \ int = 1024     # relative weight\n    cpu_quota: int = 0         # microseconds per period\n    cpu_period: int\
          \ = 100000   # microseconds\n    pids_max: int = 0          # max processes\n\nclass CgroupManager:\n    def __init__(self,\
          \ cgroup_root: str = \"/sys/fs/cgroup\"):\n        self.root = Path(cgroup_root)\n\n    def create(self, container_id:\
          \ str, limits: ResourceLimits) -> str:\n        \"\"\"Create cgroup for container and apply limits.\"\"\"\n    \
          \    cgroup_path = self.root / \"containers\" / container_id\n        cgroup_path.mkdir(parents=True, exist_ok=True)\n\
          \n        # Enable controllers for this cgroup\n        self._enable_controllers(cgroup_path.parent)\n\n       \
          \ # Apply memory limits\n        if limits.memory_limit > 0:\n            (cgroup_path / \"memory.max\").write_text(str(limits.memory_limit))\n\
          \            if limits.memory_swap >= 0:\n                swap_max = limits.memory_limit + limits.memory_swap\n\
          \                (cgroup_path / \"memory.swap.max\").write_text(str(swap_max))\n\n        # Apply CPU limits\n \
          \       if limits.cpu_quota > 0:\n            cpu_max = f\"{limits.cpu_quota} {limits.cpu_period}\"\n          \
          \  (cgroup_path / \"cpu.max\").write_text(cpu_max)\n\n        (cgroup_path / \"cpu.weight\").write_text(str(limits.cpu_shares))\n\
          \n        # Apply PID limits\n        if limits.pids_max > 0:\n            (cgroup_path / \"pids.max\").write_text(str(limits.pids_max))\n\
          \n        return str(cgroup_path)\n\n    def add_process(self, cgroup_path: str, pid: int):\n        \"\"\"Add process\
          \ to cgroup.\"\"\"\n        procs_file = Path(cgroup_path) / \"cgroup.procs\"\n        procs_file.write_text(str(pid))\n\
          \n    def get_stats(self, container_id: str) -> dict:\n        \"\"\"Get resource usage statistics.\"\"\"\n    \
          \    cgroup_path = self.root / \"containers\" / container_id\n\n        stats = {}\n\n        # Memory stats\n \
          \       memory_current = cgroup_path / \"memory.current\"\n        if memory_current.exists():\n            stats['memory_usage']\
          \ = int(memory_current.read_text().strip())\n\n        memory_stat = cgroup_path / \"memory.stat\"\n        if memory_stat.exists():\n\
          \            for line in memory_stat.read_text().splitlines():\n                key, value = line.split()\n    \
          \            stats[f'memory_{key}'] = int(value)\n\n        # CPU stats\n        cpu_stat = cgroup_path / \"cpu.stat\"\
          \n        if cpu_stat.exists():\n            for line in cpu_stat.read_text().splitlines():\n                key,\
          \ value = line.split()\n                stats[f'cpu_{key}'] = int(value)\n\n        # PID stats\n        pids_current\
          \ = cgroup_path / \"pids.current\"\n        if pids_current.exists():\n            stats['pids_current'] = int(pids_current.read_text().strip())\n\
          \n        return stats\n\n    def destroy(self, container_id: str):\n        \"\"\"Remove cgroup (processes must\
          \ be terminated first).\"\"\"\n        cgroup_path = self.root / \"containers\" / container_id\n\n        # Kill\
          \ any remaining processes\n        procs_file = cgroup_path / \"cgroup.procs\"\n        if procs_file.exists():\n\
          \            pids = procs_file.read_text().strip().split()\n            for pid in pids:\n                try:\n\
          \                    os.kill(int(pid), 9)\n                except ProcessLookupError:\n                    pass\n\
          \n        # Remove cgroup directory\n        cgroup_path.rmdir()\n\n    def _enable_controllers(self, parent_path:\
          \ Path):\n        \"\"\"Enable controllers in parent cgroup.\"\"\"\n        subtree_control = parent_path / \"cgroup.subtree_control\"\
          \n        if subtree_control.exists():\n            current = subtree_control.read_text()\n            needed =\
          \ \"+cpu +memory +pids +io\"\n            for controller in needed.split():\n                if controller[1:] not\
          \ in current:\n                    try:\n                        subtree_control.write_text(controller)\n      \
          \              except PermissionError:\n                        pass  # Controller might not be available\n```"
      pitfalls:
      - Cgroups v1 vs v2 have different interfaces
      - Can't remove cgroup with active processes
      - Memory limit without swap limit allows OOM escape
    - name: Overlay Filesystem
      description: Implement image layering using overlayfs for copy-on-write filesystem support.
      hints:
        level1: Stack read-only layers with one writable upper layer.
        level2: Handle whiteout files for deletions in upper layer.
        level3: "```python\nimport os\nimport shutil\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\n\
          import subprocess\nimport json\n\n@dataclass\nclass ImageLayer:\n    id: str\n    parent: str = None\n    diff_path:\
          \ str = \"\"\n\n@dataclass\nclass Image:\n    id: str\n    layers: list[ImageLayer] = field(default_factory=list)\n\
          \    config: dict = field(default_factory=dict)\n\nclass OverlayManager:\n    def __init__(self, storage_root: str):\n\
          \        self.root = Path(storage_root)\n        self.layers_dir = self.root / \"layers\"\n        self.images_dir\
          \ = self.root / \"images\"\n        self.containers_dir = self.root / \"containers\"\n\n        for d in [self.layers_dir,\
          \ self.images_dir, self.containers_dir]:\n            d.mkdir(parents=True, exist_ok=True)\n\n    def create_layer(self,\
          \ layer_id: str, parent_id: str = None) -> ImageLayer:\n        \"\"\"Create a new image layer.\"\"\"\n        layer_path\
          \ = self.layers_dir / layer_id\n        layer_path.mkdir(exist_ok=True)\n        (layer_path / \"diff\").mkdir(exist_ok=True)\n\
          \n        return ImageLayer(\n            id=layer_id,\n            parent=parent_id,\n            diff_path=str(layer_path\
          \ / \"diff\")\n        )\n\n    def prepare_container(self, container_id: str, image: Image) -> str:\n        \"\
          \"\"Prepare overlay mount for container.\"\"\"\n        container_path = self.containers_dir / container_id\n  \
          \      container_path.mkdir(exist_ok=True)\n\n        # Create container-specific directories\n        upper_dir\
          \ = container_path / \"upper\"\n        work_dir = container_path / \"work\"\n        merged_dir = container_path\
          \ / \"merged\"\n\n        for d in [upper_dir, work_dir, merged_dir]:\n            d.mkdir(exist_ok=True)\n\n  \
          \      # Build lower directories (image layers in order)\n        lower_dirs = []\n        for layer in reversed(image.layers):\n\
          \            lower_dirs.append(layer.diff_path)\n\n        return self._mount_overlay(\n            lower_dirs=lower_dirs,\n\
          \            upper_dir=str(upper_dir),\n            work_dir=str(work_dir),\n            merged_dir=str(merged_dir)\n\
          \        )\n\n    def _mount_overlay(self, lower_dirs: list[str], upper_dir: str,\n                      work_dir:\
          \ str, merged_dir: str) -> str:\n        \"\"\"Mount overlayfs.\"\"\"\n        lower = \":\".join(lower_dirs)\n\
          \        options = f\"lowerdir={lower},upperdir={upper_dir},workdir={work_dir}\"\n\n        subprocess.run([\n \
          \           \"mount\", \"-t\", \"overlay\", \"overlay\",\n            \"-o\", options, merged_dir\n        ], check=True)\n\
          \n        return merged_dir\n\n    def commit_container(self, container_id: str, new_image_id: str) -> Image:\n\
          \        \"\"\"Create new image from container's upper layer.\"\"\"\n        container_path = self.containers_dir\
          \ / container_id\n        upper_dir = container_path / \"upper\"\n\n        # Create new layer from upper\n    \
          \    new_layer = self.create_layer(new_image_id)\n        shutil.copytree(upper_dir, new_layer.diff_path, dirs_exist_ok=True)\n\
          \n        # Process whiteouts (files starting with .wh.)\n        self._process_whiteouts(Path(new_layer.diff_path))\n\
          \n        return Image(id=new_image_id, layers=[new_layer])\n\n    def _process_whiteouts(self, layer_path: Path):\n\
          \        \"\"\"Handle overlayfs whiteout files.\"\"\"\n        for root, dirs, files in os.walk(layer_path):\n \
          \           for f in files:\n                if f.startswith('.wh.'):\n                    whiteout_path = Path(root)\
          \ / f\n                    # Convert to OCI whiteout format or mark for deletion\n                    original_name\
          \ = f[4:]  # Remove .wh. prefix\n                    whiteout_path.rename(Path(root) / f\".wh.{original_name}\"\
          )\n\n    def cleanup_container(self, container_id: str):\n        \"\"\"Unmount and remove container filesystem.\"\
          \"\"\n        container_path = self.containers_dir / container_id\n        merged_dir = container_path / \"merged\"\
          \n\n        # Unmount overlay\n        subprocess.run([\"umount\", str(merged_dir)], check=False)\n\n        # Remove\
          \ container directory\n        shutil.rmtree(container_path, ignore_errors=True)\n```"
      pitfalls:
      - Overlayfs requires specific kernel version (3.18+)
      - Work directory must be empty and same filesystem as upper
      - Hardlinks across layers cause unexpected behavior
    - name: Container Networking
      description: Implement bridge networking for containers with port mapping and inter-container communication.
      hints:
        level1: Create veth pairs, attach one end to bridge, one to container.
        level2: Use iptables for NAT and port forwarding.
        level3: "```python\nimport subprocess\nimport ipaddress\nfrom dataclasses import dataclass\nfrom typing import Optional\n\
          \n@dataclass\nclass NetworkConfig:\n    bridge_name: str = \"container0\"\n    bridge_ip: str = \"172.17.0.1/16\"\
          \n    subnet: str = \"172.17.0.0/16\"\n\n@dataclass\nclass ContainerNetwork:\n    container_id: str\n    ip_address:\
          \ str\n    mac_address: str\n    veth_host: str\n    veth_container: str\n    port_mappings: dict = None  # {host_port:\
          \ container_port}\n\nclass NetworkManager:\n    def __init__(self, config: NetworkConfig = None):\n        self.config\
          \ = config or NetworkConfig()\n        self.subnet = ipaddress.ip_network(self.config.subnet)\n        self.allocated_ips\
          \ = set()\n        self._setup_bridge()\n\n    def _setup_bridge(self):\n        \"\"\"Create network bridge if\
          \ not exists.\"\"\"\n        bridge = self.config.bridge_name\n\n        # Create bridge\n        subprocess.run([\n\
          \            \"ip\", \"link\", \"add\", bridge, \"type\", \"bridge\"\n        ], check=False)\n\n        # Set bridge\
          \ IP\n        subprocess.run([\n            \"ip\", \"addr\", \"add\", self.config.bridge_ip, \"dev\", bridge\n\
          \        ], check=False)\n\n        # Bring up bridge\n        subprocess.run([\"ip\", \"link\", \"set\", bridge,\
          \ \"up\"], check=True)\n\n        # Enable IP forwarding\n        with open(\"/proc/sys/net/ipv4/ip_forward\", \"\
          w\") as f:\n            f.write(\"1\")\n\n        # Setup NAT for outbound traffic\n        subprocess.run([\n \
          \           \"iptables\", \"-t\", \"nat\", \"-A\", \"POSTROUTING\",\n            \"-s\", self.config.subnet, \"\
          -j\", \"MASQUERADE\"\n        ], check=False)\n\n    def connect(self, container_id: str, pid: int,\n          \
          \     port_mappings: dict = None) -> ContainerNetwork:\n        \"\"\"Connect container to bridge network.\"\"\"\
          \n        # Allocate IP\n        ip = self._allocate_ip()\n\n        # Create veth pair\n        veth_host = f\"\
          veth{container_id[:8]}\"\n        veth_container = \"eth0\"\n\n        subprocess.run([\n            \"ip\", \"\
          link\", \"add\", veth_host, \"type\", \"veth\",\n            \"peer\", \"name\", veth_container\n        ], check=True)\n\
          \n        # Attach host end to bridge\n        subprocess.run([\n            \"ip\", \"link\", \"set\", veth_host,\
          \ \"master\", self.config.bridge_name\n        ], check=True)\n        subprocess.run([\"ip\", \"link\", \"set\"\
          , veth_host, \"up\"], check=True)\n\n        # Move container end to container's network namespace\n        subprocess.run([\n\
          \            \"ip\", \"link\", \"set\", veth_container, \"netns\", str(pid)\n        ], check=True)\n\n        #\
          \ Configure container interface (run in container's netns)\n        subprocess.run([\n            \"nsenter\", \"\
          -t\", str(pid), \"-n\",\n            \"ip\", \"addr\", \"add\", f\"{ip}/16\", \"dev\", veth_container\n        ],\
          \ check=True)\n\n        subprocess.run([\n            \"nsenter\", \"-t\", str(pid), \"-n\",\n            \"ip\"\
          , \"link\", \"set\", veth_container, \"up\"\n        ], check=True)\n\n        subprocess.run([\n            \"\
          nsenter\", \"-t\", str(pid), \"-n\",\n            \"ip\", \"link\", \"set\", \"lo\", \"up\"\n        ], check=True)\n\
          \n        # Set default route\n        gateway = self.config.bridge_ip.split('/')[0]\n        subprocess.run([\n\
          \            \"nsenter\", \"-t\", str(pid), \"-n\",\n            \"ip\", \"route\", \"add\", \"default\", \"via\"\
          , gateway\n        ], check=True)\n\n        # Setup port mappings\n        if port_mappings:\n            for host_port,\
          \ container_port in port_mappings.items():\n                self._add_port_mapping(ip, host_port, container_port)\n\
          \n        return ContainerNetwork(\n            container_id=container_id,\n            ip_address=str(ip),\n  \
          \          mac_address=self._get_mac(veth_host),\n            veth_host=veth_host,\n            veth_container=veth_container,\n\
          \            port_mappings=port_mappings\n        )\n\n    def _allocate_ip(self) -> ipaddress.IPv4Address:\n  \
          \      for ip in self.subnet.hosts():\n            if ip not in self.allocated_ips and str(ip) != self.config.bridge_ip.split('/')[0]:\n\
          \                self.allocated_ips.add(ip)\n                return ip\n        raise RuntimeError(\"No available\
          \ IP addresses\")\n\n    def _add_port_mapping(self, container_ip: str, host_port: int,\n                      \
          \   container_port: int):\n        subprocess.run([\n            \"iptables\", \"-t\", \"nat\", \"-A\", \"PREROUTING\"\
          ,\n            \"-p\", \"tcp\", \"--dport\", str(host_port),\n            \"-j\", \"DNAT\", \"--to-destination\"\
          , f\"{container_ip}:{container_port}\"\n        ], check=True)\n\n    def disconnect(self, network: ContainerNetwork):\n\
          \        \"\"\"Remove container from network.\"\"\"\n        # Remove port mappings\n        if network.port_mappings:\n\
          \            for host_port, container_port in network.port_mappings.items():\n                subprocess.run([\n\
          \                    \"iptables\", \"-t\", \"nat\", \"-D\", \"PREROUTING\",\n                    \"-p\", \"tcp\"\
          , \"--dport\", str(host_port),\n                    \"-j\", \"DNAT\", \"--to-destination\",\n                  \
          \  f\"{network.ip_address}:{container_port}\"\n                ], check=False)\n\n        # Delete veth pair\n \
          \       subprocess.run([\n            \"ip\", \"link\", \"delete\", network.veth_host\n        ], check=False)\n\
          \n        # Release IP\n        ip = ipaddress.ip_address(network.ip_address)\n        self.allocated_ips.discard(ip)\n\
          \n    def _get_mac(self, interface: str) -> str:\n        result = subprocess.run(\n            [\"cat\", f\"/sys/class/net/{interface}/address\"\
          ],\n            capture_output=True, text=True\n        )\n        return result.stdout.strip()\n```"
      pitfalls:
      - Container loses network if host veth deleted before container end
      - iptables rules persist after container death
      - MTU mismatch causes packet fragmentation issues
  service-mesh:
    name: Service Mesh Sidecar
    description: Build a service mesh sidecar proxy handling service discovery, load balancing, circuit breaking, and mTLS.
    why_important: Service meshes like Istio and Linkerd are essential for microservices. Understanding the proxy layer helps
      with debugging and optimization.
    difficulty: advanced
    tags:
    - microservices
    - networking
    - distributed-systems
    estimated_hours: 55
    prerequisites:
    - api-gateway
    - circuit-breaker
    learning_outcomes:
    - Implement transparent traffic interception
    - Build service discovery integration
    - Handle mTLS between services
    - Implement advanced load balancing
    milestones:
    - name: Traffic Interception
      description: Intercept inbound and outbound traffic transparently using iptables redirect rules.
      hints:
        level1: Use iptables REDIRECT to send traffic to proxy port.
        level2: Preserve original destination using SO_ORIGINAL_DST.
        level3: "```python\nimport socket\nimport struct\nimport subprocess\nfrom dataclasses import dataclass\n\nSO_ORIGINAL_DST\
          \ = 80\n\n@dataclass\nclass InterceptConfig:\n    inbound_port: int = 15001\n    outbound_port: int = 15006\n  \
          \  proxy_uid: int = 1337\n    exclude_ports: list = None\n\nclass TrafficInterceptor:\n    def __init__(self, config:\
          \ InterceptConfig = None):\n        self.config = config or InterceptConfig()\n\n    def setup_iptables(self):\n\
          \        \"\"\"Setup iptables rules for traffic interception.\"\"\"\n        # Create PROXY_REDIRECT chain\n   \
          \     subprocess.run([\n            \"iptables\", \"-t\", \"nat\", \"-N\", \"PROXY_REDIRECT\"\n        ], check=False)\n\
          \n        # Redirect to proxy\n        subprocess.run([\n            \"iptables\", \"-t\", \"nat\", \"-A\", \"PROXY_REDIRECT\"\
          ,\n            \"-p\", \"tcp\", \"-j\", \"REDIRECT\", \"--to-port\", str(self.config.outbound_port)\n        ],\
          \ check=True)\n\n        # Create PROXY_OUTPUT chain for outbound\n        subprocess.run([\n            \"iptables\"\
          , \"-t\", \"nat\", \"-N\", \"PROXY_OUTPUT\"\n        ], check=False)\n\n        # Exclude proxy's own traffic (prevent\
          \ loops)\n        subprocess.run([\n            \"iptables\", \"-t\", \"nat\", \"-A\", \"PROXY_OUTPUT\",\n     \
          \       \"-m\", \"owner\", \"--uid-owner\", str(self.config.proxy_uid),\n            \"-j\", \"RETURN\"\n      \
          \  ], check=True)\n\n        # Exclude localhost\n        subprocess.run([\n            \"iptables\", \"-t\", \"\
          nat\", \"-A\", \"PROXY_OUTPUT\",\n            \"-d\", \"127.0.0.1/32\", \"-j\", \"RETURN\"\n        ], check=True)\n\
          \n        # Exclude specific ports\n        for port in (self.config.exclude_ports or []):\n            subprocess.run([\n\
          \                \"iptables\", \"-t\", \"nat\", \"-A\", \"PROXY_OUTPUT\",\n                \"-p\", \"tcp\", \"--dport\"\
          , str(port), \"-j\", \"RETURN\"\n            ], check=True)\n\n        # Send everything else to proxy\n       \
          \ subprocess.run([\n            \"iptables\", \"-t\", \"nat\", \"-A\", \"PROXY_OUTPUT\",\n            \"-j\", \"\
          PROXY_REDIRECT\"\n        ], check=True)\n\n        # Apply to OUTPUT chain\n        subprocess.run([\n        \
          \    \"iptables\", \"-t\", \"nat\", \"-A\", \"OUTPUT\",\n            \"-p\", \"tcp\", \"-j\", \"PROXY_OUTPUT\"\n\
          \        ], check=True)\n\n        # Setup inbound interception\n        subprocess.run([\n            \"iptables\"\
          , \"-t\", \"nat\", \"-A\", \"PREROUTING\",\n            \"-p\", \"tcp\", \"-j\", \"REDIRECT\", \"--to-port\", str(self.config.inbound_port)\n\
          \        ], check=True)\n\n    def get_original_dest(self, client_socket) -> tuple[str, int]:\n        \"\"\"Get\
          \ original destination from redirected socket.\"\"\"\n        # Get original destination using SO_ORIGINAL_DST\n\
          \        dst = client_socket.getsockopt(socket.SOL_IP, SO_ORIGINAL_DST, 16)\n\n        # Parse sockaddr_in structure\n\
          \        port = struct.unpack('>H', dst[2:4])[0]\n        ip = socket.inet_ntoa(dst[4:8])\n\n        return ip,\
          \ port\n\n    def cleanup(self):\n        \"\"\"Remove iptables rules.\"\"\"\n        subprocess.run([\n       \
          \     \"iptables\", \"-t\", \"nat\", \"-D\", \"OUTPUT\", \"-p\", \"tcp\", \"-j\", \"PROXY_OUTPUT\"\n        ], check=False)\n\
          \        subprocess.run([\n            \"iptables\", \"-t\", \"nat\", \"-F\", \"PROXY_OUTPUT\"\n        ], check=False)\n\
          \        subprocess.run([\n            \"iptables\", \"-t\", \"nat\", \"-X\", \"PROXY_OUTPUT\"\n        ], check=False)\n\
          \        subprocess.run([\n            \"iptables\", \"-t\", \"nat\", \"-F\", \"PROXY_REDIRECT\"\n        ], check=False)\n\
          \        subprocess.run([\n            \"iptables\", \"-t\", \"nat\", \"-X\", \"PROXY_REDIRECT\"\n        ], check=False)\n\
          ```"
      pitfalls:
      - Redirect loop if proxy traffic not excluded
      - SO_ORIGINAL_DST not available on all systems
      - IPv6 requires separate ip6tables rules
    - name: Service Discovery Integration
      description: Integrate with service discovery (Consul, Kubernetes) to resolve service names to endpoints.
      hints:
        level1: Watch service registry for endpoint changes.
        level2: Cache endpoints locally, handle stale entries.
        level3: "```python\nimport asyncio\nimport aiohttp\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\
          from abc import ABC, abstractmethod\n\n@dataclass\nclass Endpoint:\n    address: str\n    port: int\n    weight:\
          \ int = 100\n    healthy: bool = True\n    metadata: dict = field(default_factory=dict)\n\n@dataclass\nclass Service:\n\
          \    name: str\n    endpoints: list[Endpoint] = field(default_factory=list)\n    version: str = \"\"\n\nclass ServiceDiscovery(ABC):\n\
          \    @abstractmethod\n    async def get_service(self, name: str) -> Optional[Service]:\n        pass\n\n    @abstractmethod\n\
          \    async def watch(self, name: str, callback):\n        pass\n\nclass KubernetesDiscovery(ServiceDiscovery):\n\
          \    def __init__(self, namespace: str = \"default\"):\n        self.namespace = namespace\n        self.base_url\
          \ = \"https://kubernetes.default.svc\"\n        self._cache: dict[str, Service] = {}\n        self._watchers: dict[str,\
          \ asyncio.Task] = {}\n\n    async def get_service(self, name: str) -> Optional[Service]:\n        if name in self._cache:\n\
          \            return self._cache[name]\n\n        async with aiohttp.ClientSession() as session:\n            url\
          \ = f\"{self.base_url}/api/v1/namespaces/{self.namespace}/endpoints/{name}\"\n            headers = {\"Authorization\"\
          : f\"Bearer {self._get_token()}\"}\n\n            async with session.get(url, headers=headers, ssl=False) as resp:\n\
          \                if resp.status != 200:\n                    return None\n\n                data = await resp.json()\n\
          \                service = self._parse_endpoints(name, data)\n                self._cache[name] = service\n    \
          \            return service\n\n    async def watch(self, name: str, callback):\n        \"\"\"Watch for endpoint\
          \ changes.\"\"\"\n        async with aiohttp.ClientSession() as session:\n            url = f\"{self.base_url}/api/v1/namespaces/{self.namespace}/endpoints\"\
          \n            params = {\"watch\": \"true\", \"fieldSelector\": f\"metadata.name={name}\"}\n            headers\
          \ = {\"Authorization\": f\"Bearer {self._get_token()}\"}\n\n            async with session.get(url, params=params,\
          \ headers=headers, ssl=False) as resp:\n                async for line in resp.content:\n                    if\
          \ line:\n                        import json\n                        event = json.loads(line)\n               \
          \         service = self._parse_endpoints(name, event['object'])\n                        self._cache[name] = service\n\
          \                        await callback(service)\n\n    def _parse_endpoints(self, name: str, data: dict) -> Service:\n\
          \        endpoints = []\n        for subset in data.get('subsets', []):\n            addresses = subset.get('addresses',\
          \ [])\n            ports = subset.get('ports', [])\n\n            for addr in addresses:\n                for port\
          \ in ports:\n                    endpoints.append(Endpoint(\n                        address=addr['ip'],\n     \
          \                   port=port['port'],\n                        metadata={\n                            'node':\
          \ addr.get('nodeName'),\n                            'pod': addr.get('targetRef', {}).get('name')\n            \
          \            }\n                    ))\n\n        return Service(name=name, endpoints=endpoints)\n\n    def _get_token(self)\
          \ -> str:\n        with open('/var/run/secrets/kubernetes.io/serviceaccount/token') as f:\n            return f.read()\n\
          \nclass ServiceRegistry:\n    def __init__(self, discovery: ServiceDiscovery):\n        self.discovery = discovery\n\
          \        self.services: dict[str, Service] = {}\n        self._watch_tasks: dict[str, asyncio.Task] = {}\n\n   \
          \ async def resolve(self, service_name: str) -> list[Endpoint]:\n        \"\"\"Resolve service name to healthy endpoints.\"\
          \"\"\n        if service_name not in self.services:\n            service = await self.discovery.get_service(service_name)\n\
          \            if service:\n                self.services[service_name] = service\n                self._start_watch(service_name)\n\
          \n        service = self.services.get(service_name)\n        if not service:\n            return []\n\n        return\
          \ [ep for ep in service.endpoints if ep.healthy]\n\n    def _start_watch(self, service_name: str):\n        if service_name\
          \ not in self._watch_tasks:\n            self._watch_tasks[service_name] = asyncio.create_task(\n              \
          \  self.discovery.watch(service_name, self._on_update)\n            )\n\n    async def _on_update(self, service:\
          \ Service):\n        self.services[service.name] = service\n```"
      pitfalls:
      - Watch connection drops require reconnection logic
      - Stale cache serves dead endpoints
      - DNS caching conflicts with dynamic discovery
    - name: mTLS and Certificate Management
      description: Implement mutual TLS between services with automatic certificate rotation.
      hints:
        level1: Each service needs its own certificate signed by mesh CA.
        level2: Rotate certificates before expiry, handle during active connections.
        level3: "```python\nimport ssl\nimport asyncio\nfrom dataclasses import dataclass\nfrom datetime import datetime,\
          \ timedelta\nfrom pathlib import Path\nimport subprocess\n\n@dataclass\nclass Certificate:\n    cert_path: str\n\
          \    key_path: str\n    ca_path: str\n    expiry: datetime\n    service_name: str\n\nclass CertificateManager:\n\
          \    def __init__(self, service_name: str, cert_dir: str = \"/etc/certs\"):\n        self.service_name = service_name\n\
          \        self.cert_dir = Path(cert_dir)\n        self.current_cert: Certificate = None\n        self._rotation_task\
          \ = None\n        self.rotation_threshold = timedelta(hours=1)\n\n    async def initialize(self):\n        \"\"\"\
          Load or request initial certificate.\"\"\"\n        cert_path = self.cert_dir / \"cert.pem\"\n        key_path =\
          \ self.cert_dir / \"key.pem\"\n        ca_path = self.cert_dir / \"ca.pem\"\n\n        if not cert_path.exists():\n\
          \            await self._request_certificate()\n\n        self.current_cert = Certificate(\n            cert_path=str(cert_path),\n\
          \            key_path=str(key_path),\n            ca_path=str(ca_path),\n            expiry=self._get_cert_expiry(cert_path),\n\
          \            service_name=self.service_name\n        )\n\n        self._rotation_task = asyncio.create_task(self._rotation_loop())\n\
          \n    def get_ssl_context(self, server: bool = False) -> ssl.SSLContext:\n        \"\"\"Get SSL context for mTLS\
          \ connections.\"\"\"\n        if server:\n            ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        else:\n\
          \            ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n\n        # Load certificate and key\n        ctx.load_cert_chain(\n\
          \            self.current_cert.cert_path,\n            self.current_cert.key_path\n        )\n\n        # Load CA\
          \ for peer verification\n        ctx.load_verify_locations(self.current_cert.ca_path)\n\n        # Require client\
          \ certificate (mTLS)\n        ctx.verify_mode = ssl.CERT_REQUIRED\n        ctx.check_hostname = not server\n\n \
          \       return ctx\n\n    async def _rotation_loop(self):\n        \"\"\"Check and rotate certificates before expiry.\"\
          \"\"\n        while True:\n            time_to_expiry = self.current_cert.expiry - datetime.utcnow()\n\n       \
          \     if time_to_expiry < self.rotation_threshold:\n                await self._rotate_certificate()\n\n       \
          \     # Check every minute\n            await asyncio.sleep(60)\n\n    async def _rotate_certificate(self):\n  \
          \      \"\"\"Request new certificate and hot-swap.\"\"\"\n        # Request new certificate\n        new_cert_path\
          \ = self.cert_dir / \"cert-new.pem\"\n        new_key_path = self.cert_dir / \"key-new.pem\"\n\n        await self._request_certificate(\n\
          \            cert_path=new_cert_path,\n            key_path=new_key_path\n        )\n\n        # Atomic swap\n \
          \       import shutil\n        shutil.move(new_cert_path, self.cert_dir / \"cert.pem\")\n        shutil.move(new_key_path,\
          \ self.cert_dir / \"key.pem\")\n\n        # Update current cert\n        self.current_cert.expiry = self._get_cert_expiry(self.cert_dir\
          \ / \"cert.pem\")\n\n    async def _request_certificate(self, cert_path: Path = None,\n                        \
          \           key_path: Path = None):\n        \"\"\"Request certificate from mesh CA (e.g., SPIFFE/SPIRE).\"\"\"\n\
          \        cert_path = cert_path or self.cert_dir / \"cert.pem\"\n        key_path = key_path or self.cert_dir / \"\
          key.pem\"\n\n        # Generate CSR\n        subprocess.run([\n            \"openssl\", \"req\", \"-new\", \"-newkey\"\
          , \"rsa:2048\",\n            \"-nodes\", \"-keyout\", str(key_path),\n            \"-out\", \"/tmp/csr.pem\",\n\
          \            \"-subj\", f\"/CN={self.service_name}\"\n        ], check=True)\n\n        # In real implementation,\
          \ send CSR to CA API\n        # For now, self-sign (development only)\n        subprocess.run([\n            \"\
          openssl\", \"x509\", \"-req\",\n            \"-in\", \"/tmp/csr.pem\",\n            \"-CA\", str(self.cert_dir /\
          \ \"ca.pem\"),\n            \"-CAkey\", str(self.cert_dir / \"ca-key.pem\"),\n            \"-CAcreateserial\",\n\
          \            \"-out\", str(cert_path),\n            \"-days\", \"1\"\n        ], check=True)\n\n    def _get_cert_expiry(self,\
          \ cert_path: Path) -> datetime:\n        result = subprocess.run([\n            \"openssl\", \"x509\", \"-enddate\"\
          , \"-noout\", \"-in\", str(cert_path)\n        ], capture_output=True, text=True)\n\n        # Parse: notAfter=Jan\
          \  1 00:00:00 2024 GMT\n        date_str = result.stdout.strip().split('=')[1]\n        return datetime.strptime(date_str,\
          \ \"%b %d %H:%M:%S %Y %Z\")\n\ndef verify_peer_identity(ssl_socket, expected_service: str) -> bool:\n    \"\"\"\
          Verify peer certificate matches expected service identity.\"\"\"\n    cert = ssl_socket.getpeercert()\n\n    # Check\
          \ Common Name\n    subject = dict(x[0] for x in cert['subject'])\n    cn = subject.get('commonName', '')\n\n   \
          \ # Check SAN (Subject Alternative Names)\n    san = cert.get('subjectAltName', [])\n    dns_names = [name for type_,\
          \ name in san if type_ == 'DNS']\n    uri_names = [name for type_, name in san if type_ == 'URI']\n\n    # Verify\
          \ identity (SPIFFE format: spiffe://trust-domain/service-name)\n    expected_spiffe = f\"spiffe://mesh.local/{expected_service}\"\
          \n\n    return (cn == expected_service or\n            expected_service in dns_names or\n            expected_spiffe\
          \ in uri_names)\n```"
      pitfalls:
      - Certificate rotation during active requests causes failures
      - Clock skew makes valid certificates appear expired
      - Missing SAN in certificate breaks modern TLS verification
    - name: Load Balancing Algorithms
      description: Implement advanced load balancing including round-robin, least connections, weighted, and consistent hashing.
      hints:
        level1: Round-robin rotates through endpoints sequentially.
        level2: Least connections requires tracking active connection count.
        level3: "```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nimport hashlib\n\
          import bisect\nimport random\nimport threading\n\n@dataclass\nclass EndpointStats:\n    address: str\n    active_connections:\
          \ int = 0\n    total_requests: int = 0\n    failures: int = 0\n    avg_latency_ms: float = 0\n    weight: int =\
          \ 100\n\nclass LoadBalancer(ABC):\n    @abstractmethod\n    def select(self, endpoints: list[EndpointStats], key:\
          \ str = None) -> EndpointStats:\n        pass\n\nclass RoundRobinBalancer(LoadBalancer):\n    def __init__(self):\n\
          \        self._index = 0\n        self._lock = threading.Lock()\n\n    def select(self, endpoints: list[EndpointStats],\
          \ key: str = None) -> EndpointStats:\n        if not endpoints:\n            return None\n\n        with self._lock:\n\
          \            endpoint = endpoints[self._index % len(endpoints)]\n            self._index += 1\n            return\
          \ endpoint\n\nclass WeightedRoundRobinBalancer(LoadBalancer):\n    def __init__(self):\n        self._current_weight\
          \ = 0\n        self._index = 0\n        self._lock = threading.Lock()\n\n    def select(self, endpoints: list[EndpointStats],\
          \ key: str = None) -> EndpointStats:\n        if not endpoints:\n            return None\n\n        with self._lock:\n\
          \            max_weight = max(ep.weight for ep in endpoints)\n            gcd_weight = self._gcd_of_weights([ep.weight\
          \ for ep in endpoints])\n\n            while True:\n                self._index = (self._index + 1) % len(endpoints)\n\
          \                if self._index == 0:\n                    self._current_weight -= gcd_weight\n                \
          \    if self._current_weight <= 0:\n                        self._current_weight = max_weight\n\n              \
          \  if endpoints[self._index].weight >= self._current_weight:\n                    return endpoints[self._index]\n\
          \n    def _gcd_of_weights(self, weights: list[int]) -> int:\n        from math import gcd\n        from functools\
          \ import reduce\n        return reduce(gcd, weights)\n\nclass LeastConnectionsBalancer(LoadBalancer):\n    def select(self,\
          \ endpoints: list[EndpointStats], key: str = None) -> EndpointStats:\n        if not endpoints:\n            return\
          \ None\n\n        # Select endpoint with fewest active connections\n        # Weighted by: connections / weight\n\
          \        return min(\n            endpoints,\n            key=lambda ep: ep.active_connections / max(ep.weight,\
          \ 1)\n        )\n\nclass ConsistentHashBalancer(LoadBalancer):\n    def __init__(self, replicas: int = 150):\n \
          \       self.replicas = replicas\n        self._ring: list[tuple[int, str]] = []\n        self._endpoints: dict[str,\
          \ EndpointStats] = {}\n\n    def update_endpoints(self, endpoints: list[EndpointStats]):\n        \"\"\"Rebuild\
          \ hash ring when endpoints change.\"\"\"\n        self._ring = []\n        self._endpoints = {ep.address: ep for\
          \ ep in endpoints}\n\n        for ep in endpoints:\n            for i in range(self.replicas):\n               \
          \ key = f\"{ep.address}:{i}\"\n                hash_val = self._hash(key)\n                bisect.insort(self._ring,\
          \ (hash_val, ep.address))\n\n    def select(self, endpoints: list[EndpointStats], key: str = None) -> EndpointStats:\n\
          \        if not self._ring:\n            self.update_endpoints(endpoints)\n\n        if not key:\n            key\
          \ = str(random.random())\n\n        hash_val = self._hash(key)\n\n        # Binary search for first hash >= key\
          \ hash\n        idx = bisect.bisect_left(self._ring, (hash_val, \"\"))\n        if idx >= len(self._ring):\n   \
          \         idx = 0\n\n        address = self._ring[idx][1]\n        return self._endpoints.get(address)\n\n    def\
          \ _hash(self, key: str) -> int:\n        return int(hashlib.md5(key.encode()).hexdigest(), 16)\n\nclass P2CBalancer(LoadBalancer):\n\
          \    \"\"\"Power of Two Choices - picks best of 2 random endpoints.\"\"\"\n\n    def select(self, endpoints: list[EndpointStats],\
          \ key: str = None) -> EndpointStats:\n        if not endpoints:\n            return None\n        if len(endpoints)\
          \ == 1:\n            return endpoints[0]\n\n        # Pick two random endpoints\n        a, b = random.sample(endpoints,\
          \ 2)\n\n        # Return one with fewer connections (weighted)\n        score_a = a.active_connections / max(a.weight,\
          \ 1)\n        score_b = b.active_connections / max(b.weight, 1)\n\n        return a if score_a <= score_b else b\n\
          \nclass AdaptiveBalancer(LoadBalancer):\n    \"\"\"Balancer that considers latency and error rates.\"\"\"\n\n  \
          \  def select(self, endpoints: list[EndpointStats], key: str = None) -> EndpointStats:\n        if not endpoints:\n\
          \            return None\n\n        # Score based on multiple factors\n        def score(ep: EndpointStats) -> float:\n\
          \            # Lower is better\n            latency_factor = ep.avg_latency_ms / 100  # Normalize\n            error_rate\
          \ = ep.failures / max(ep.total_requests, 1)\n            connection_factor = ep.active_connections / max(ep.weight,\
          \ 1)\n\n            return (\n                latency_factor * 0.3 +\n                error_rate * 0.5 +\n     \
          \           connection_factor * 0.2\n            )\n\n        return min(endpoints, key=score)\n```"
      pitfalls:
      - Consistent hashing needs ring rebuild on endpoint change
      - Least connections can thundering herd to recovered endpoint
      - Weight=0 causes division by zero
  infrastructure-as-code:
    name: Infrastructure as Code Engine
    description: Build an IaC engine that parses declarative configs, manages state, computes diffs, and applies changes to
      infrastructure.
    why_important: Understanding IaC internals (like Terraform) helps debug state issues, write better modules, and build
      custom providers.
    difficulty: advanced
    tags:
    - devops
    - infrastructure
    - automation
    estimated_hours: 50
    prerequisites:
    - ci-cd-pipeline
    learning_outcomes:
    - Design declarative configuration language
    - Implement state management and locking
    - Build dependency graph for resource ordering
    - Handle provider abstraction for multi-cloud
    milestones:
    - name: Configuration Parser
      description: Parse HCL-like configuration files with resources, variables, outputs, and module references.
      hints:
        level1: Define grammar for resource blocks with attributes.
        level2: Handle variable interpolation and references.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom typing import Any, Optional\nimport re\n\n@dataclass\n\
          class Variable:\n    name: str\n    type: str = \"string\"\n    default: Any = None\n    description: str = \"\"\
          \n\n@dataclass\nclass Resource:\n    type: str\n    name: str\n    attributes: dict = field(default_factory=dict)\n\
          \    depends_on: list[str] = field(default_factory=list)\n    count: int = 1\n    provider: str = None\n\n@dataclass\n\
          class Output:\n    name: str\n    value: Any\n    description: str = \"\"\n\n@dataclass\nclass Module:\n    name:\
          \ str\n    source: str\n    variables: dict = field(default_factory=dict)\n\n@dataclass\nclass Configuration:\n\
          \    variables: dict[str, Variable] = field(default_factory=dict)\n    resources: dict[str, Resource] = field(default_factory=dict)\n\
          \    outputs: dict[str, Output] = field(default_factory=dict)\n    modules: dict[str, Module] = field(default_factory=dict)\n\
          \    providers: dict[str, dict] = field(default_factory=dict)\n\nclass ConfigParser:\n    def __init__(self):\n\
          \        self.config = Configuration()\n        self._var_pattern = re.compile(r'\\$\\{([^}]+)\\}')\n\n    def parse(self,\
          \ content: str) -> Configuration:\n        \"\"\"Parse HCL-like configuration.\"\"\"\n        import hcl2\n    \
          \    import json\n\n        # Parse HCL to dict\n        data = hcl2.loads(content)\n\n        # Parse variables\n\
          \        for var_block in data.get('variable', []):\n            for name, attrs in var_block.items():\n       \
          \         self.config.variables[name] = Variable(\n                    name=name,\n                    type=attrs.get('type',\
          \ 'string'),\n                    default=attrs.get('default'),\n                    description=attrs.get('description',\
          \ '')\n                )\n\n        # Parse resources\n        for res_block in data.get('resource', []):\n    \
          \        for res_type, resources in res_block.items():\n                for res_name, attrs in resources.items():\n\
          \                    resource_id = f\"{res_type}.{res_name}\"\n                    self.config.resources[resource_id]\
          \ = Resource(\n                        type=res_type,\n                        name=res_name,\n                \
          \        attributes=attrs,\n                        depends_on=attrs.pop('depends_on', []),\n                  \
          \      count=attrs.pop('count', 1),\n                        provider=attrs.pop('provider', None)\n            \
          \        )\n\n        # Parse outputs\n        for out_block in data.get('output', []):\n            for name, attrs\
          \ in out_block.items():\n                self.config.outputs[name] = Output(\n                    name=name,\n \
          \                   value=attrs.get('value'),\n                    description=attrs.get('description', '')\n  \
          \              )\n\n        # Parse modules\n        for mod_block in data.get('module', []):\n            for name,\
          \ attrs in mod_block.items():\n                self.config.modules[name] = Module(\n                    name=name,\n\
          \                    source=attrs.pop('source'),\n                    variables=attrs\n                )\n\n   \
          \     return self.config\n\n    def resolve_references(self, value: Any, context: dict) -> Any:\n        \"\"\"\
          Resolve variable and resource references.\"\"\"\n        if isinstance(value, str):\n            return self._resolve_string(value,\
          \ context)\n        elif isinstance(value, list):\n            return [self.resolve_references(v, context) for v\
          \ in value]\n        elif isinstance(value, dict):\n            return {k: self.resolve_references(v, context) for\
          \ k, v in value.items()}\n        return value\n\n    def _resolve_string(self, value: str, context: dict) -> Any:\n\
          \        \"\"\"Resolve interpolations in string value.\"\"\"\n        def replacer(match):\n            ref = match.group(1)\n\
          \            return str(self._resolve_reference(ref, context))\n\n        # Check if entire value is a single reference\n\
          \        if value.startswith('${') and value.endswith('}'):\n            ref = value[2:-1]\n            return self._resolve_reference(ref,\
          \ context)\n\n        return self._var_pattern.sub(replacer, value)\n\n    def _resolve_reference(self, ref: str,\
          \ context: dict) -> Any:\n        \"\"\"Resolve a single reference like var.name or aws_instance.web.id\"\"\"\n\
          \        parts = ref.split('.')\n\n        if parts[0] == 'var':\n            return context.get('variables', {}).get(parts[1])\n\
          \        elif parts[0] == 'local':\n            return context.get('locals', {}).get(parts[1])\n        elif parts[0]\
          \ == 'module':\n            return context.get('modules', {}).get(parts[1], {}).get(parts[2])\n        else:\n \
          \           # Resource reference\n            resource_id = f\"{parts[0]}.{parts[1]}\"\n            resource = context.get('resources',\
          \ {}).get(resource_id, {})\n            if len(parts) > 2:\n                return resource.get(parts[2])\n    \
          \        return resource\n\n        return None\n```"
      pitfalls:
      - Circular references cause infinite resolution loop
      - Interpolation in count creates chicken-egg problem
      - Module source paths need normalization
    - name: State Management
      description: Implement state file tracking of deployed resources with locking for concurrent access.
      hints:
        level1: Store resource IDs and attributes after creation.
        level2: Use file or remote locking to prevent concurrent modifications.
        level3: "```python\nimport json\nimport hashlib\nimport fcntl\nfrom dataclasses import dataclass, field, asdict\n\
          from datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional\nimport asyncio\n\n@dataclass\n\
          class ResourceState:\n    type: str\n    name: str\n    id: str\n    attributes: dict\n    dependencies: list[str]\
          \ = field(default_factory=list)\n    provider: str = \"\"\n\n@dataclass\nclass State:\n    version: int = 1\n  \
          \  serial: int = 0\n    lineage: str = \"\"\n    resources: dict[str, ResourceState] = field(default_factory=dict)\n\
          \    outputs: dict[str, Any] = field(default_factory=dict)\n\nclass StateManager:\n    def __init__(self, state_path:\
          \ str = \"terraform.tfstate\"):\n        self.state_path = Path(state_path)\n        self.lock_path = Path(f\"{state_path}.lock\"\
          )\n        self.state: State = None\n        self._lock_fd = None\n\n    def load(self) -> State:\n        \"\"\"\
          Load state from file.\"\"\"\n        if self.state_path.exists():\n            with open(self.state_path) as f:\n\
          \                data = json.load(f)\n                self.state = State(\n                    version=data.get('version',\
          \ 1),\n                    serial=data.get('serial', 0),\n                    lineage=data.get('lineage', ''),\n\
          \                    resources={\n                        k: ResourceState(**v)\n                        for k,\
          \ v in data.get('resources', {}).items()\n                    },\n                    outputs=data.get('outputs',\
          \ {})\n                )\n        else:\n            import uuid\n            self.state = State(lineage=str(uuid.uuid4()))\n\
          \n        return self.state\n\n    def save(self):\n        \"\"\"Save state to file.\"\"\"\n        self.state.serial\
          \ += 1\n\n        data = {\n            'version': self.state.version,\n            'serial': self.state.serial,\n\
          \            'lineage': self.state.lineage,\n            'resources': {\n                k: asdict(v) for k, v in\
          \ self.state.resources.items()\n            },\n            'outputs': self.state.outputs\n        }\n\n       \
          \ # Write to temp file first\n        temp_path = self.state_path.with_suffix('.tmp')\n        with open(temp_path,\
          \ 'w') as f:\n            json.dump(data, f, indent=2)\n\n        # Atomic rename\n        temp_path.rename(self.state_path)\n\
          \n    def lock(self, timeout: float = 60) -> bool:\n        \"\"\"Acquire state lock.\"\"\"\n        self._lock_fd\
          \ = open(self.lock_path, 'w')\n\n        start = datetime.now()\n        while True:\n            try:\n       \
          \         fcntl.flock(self._lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\n                # Write lock info\n       \
          \         lock_info = {\n                    'locked_at': datetime.utcnow().isoformat(),\n                    'pid':\
          \ os.getpid(),\n                    'hostname': socket.gethostname()\n                }\n                self._lock_fd.write(json.dumps(lock_info))\n\
          \                self._lock_fd.flush()\n                return True\n            except BlockingIOError:\n     \
          \           if (datetime.now() - start).total_seconds() > timeout:\n                    return False\n         \
          \       import time\n                time.sleep(1)\n\n    def unlock(self):\n        \"\"\"Release state lock.\"\
          \"\"\n        if self._lock_fd:\n            fcntl.flock(self._lock_fd, fcntl.LOCK_UN)\n            self._lock_fd.close()\n\
          \            self._lock_fd = None\n            self.lock_path.unlink(missing_ok=True)\n\n    def get_resource(self,\
          \ resource_id: str) -> Optional[ResourceState]:\n        return self.state.resources.get(resource_id)\n\n    def\
          \ set_resource(self, resource_id: str, resource: ResourceState):\n        self.state.resources[resource_id] = resource\n\
          \n    def remove_resource(self, resource_id: str):\n        self.state.resources.pop(resource_id, None)\n\n    def\
          \ compute_checksum(self) -> str:\n        \"\"\"Compute state checksum for change detection.\"\"\"\n        data\
          \ = json.dumps(asdict(self.state), sort_keys=True)\n        return hashlib.sha256(data.encode()).hexdigest()\n\n\
          class RemoteStateBackend:\n    \"\"\"S3 backend for remote state storage.\"\"\"\n\n    def __init__(self, bucket:\
          \ str, key: str, region: str):\n        self.bucket = bucket\n        self.key = key\n        self.region = region\n\
          \        self.lock_table = \"terraform-locks\"  # DynamoDB table\n\n    async def load(self) -> State:\n       \
          \ import aioboto3\n        session = aioboto3.Session()\n\n        async with session.client('s3', region_name=self.region)\
          \ as s3:\n            try:\n                response = await s3.get_object(Bucket=self.bucket, Key=self.key)\n \
          \               data = json.loads(await response['Body'].read())\n                return State(**data)\n       \
          \     except s3.exceptions.NoSuchKey:\n                return State(lineage=str(uuid.uuid4()))\n\n    async def\
          \ save(self, state: State):\n        import aioboto3\n        session = aioboto3.Session()\n\n        async with\
          \ session.client('s3', region_name=self.region) as s3:\n            await s3.put_object(\n                Bucket=self.bucket,\n\
          \                Key=self.key,\n                Body=json.dumps(asdict(state), indent=2),\n                ContentType='application/json'\n\
          \            )\n\n    async def lock(self, lock_id: str) -> bool:\n        import aioboto3\n        session = aioboto3.Session()\n\
          \n        async with session.client('dynamodb', region_name=self.region) as ddb:\n            try:\n           \
          \     await ddb.put_item(\n                    TableName=self.lock_table,\n                    Item={\n        \
          \                'LockID': {'S': f\"{self.bucket}/{self.key}\"},\n                        'Info': {'S': json.dumps({'id':\
          \ lock_id})}\n                    },\n                    ConditionExpression='attribute_not_exists(LockID)'\n \
          \               )\n                return True\n            except ddb.exceptions.ConditionalCheckFailedException:\n\
          \                return False\n```"
      pitfalls:
      - State corruption on partial write (use atomic rename)
      - Stale lock from crashed process blocks everyone
      - Remote state race condition between read and write
    - name: Dependency Graph & Planning
      description: Build resource dependency graph and generate execution plan with create, update, delete operations.
      hints:
        level1: Build DAG from explicit depends_on and implicit references.
        level2: Compare desired state vs current state to determine actions.
        level3: "```python\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\
          import graphlib\n\nclass Action(str, Enum):\n    CREATE = \"create\"\n    UPDATE = \"update\"\n    DELETE = \"delete\"\
          \n    REPLACE = \"replace\"  # delete + create\n    NO_OP = \"no-op\"\n\n@dataclass\nclass ResourceChange:\n   \
          \ resource_id: str\n    action: Action\n    before: dict = None\n    after: dict = None\n    reason: str = \"\"\n\
          \n@dataclass\nclass Plan:\n    changes: list[ResourceChange] = field(default_factory=list)\n    outputs: dict =\
          \ field(default_factory=dict)\n\nclass DependencyGraph:\n    def __init__(self):\n        self.graph: dict[str,\
          \ set[str]] = {}\n\n    def add_resource(self, resource_id: str, depends_on: list[str] = None):\n        self.graph[resource_id]\
          \ = set(depends_on or [])\n\n    def add_dependency(self, resource_id: str, depends_on: str):\n        self.graph.setdefault(resource_id,\
          \ set()).add(depends_on)\n\n    def get_order(self) -> list[str]:\n        \"\"\"Return resources in dependency\
          \ order.\"\"\"\n        ts = graphlib.TopologicalSorter(self.graph)\n        return list(ts.static_order())\n\n\
          \    def get_reverse_order(self) -> list[str]:\n        \"\"\"Return resources in reverse order (for deletion).\"\
          \"\"\n        return list(reversed(self.get_order()))\n\n    def get_dependents(self, resource_id: str) -> set[str]:\n\
          \        \"\"\"Get all resources that depend on this one.\"\"\"\n        dependents = set()\n        for res_id,\
          \ deps in self.graph.items():\n            if resource_id in deps:\n                dependents.add(res_id)\n   \
          \     return dependents\n\nclass Planner:\n    def __init__(self, config: Configuration, state: State):\n      \
          \  self.config = config\n        self.state = state\n        self.graph = DependencyGraph()\n\n    def plan(self)\
          \ -> Plan:\n        \"\"\"Generate execution plan.\"\"\"\n        plan = Plan()\n\n        # Build dependency graph\n\
          \        self._build_graph()\n\n        # Get desired resource IDs\n        desired_ids = set(self.config.resources.keys())\n\
          \        current_ids = set(self.state.resources.keys())\n\n        # Resources to create\n        to_create = desired_ids\
          \ - current_ids\n        # Resources to delete\n        to_delete = current_ids - desired_ids\n        # Resources\
          \ to potentially update\n        to_check = desired_ids & current_ids\n\n        # Process deletions (reverse dependency\
          \ order)\n        for resource_id in self.graph.get_reverse_order():\n            if resource_id in to_delete:\n\
          \                plan.changes.append(ResourceChange(\n                    resource_id=resource_id,\n           \
          \         action=Action.DELETE,\n                    before=self.state.resources[resource_id].attributes\n     \
          \           ))\n\n        # Process creates and updates (dependency order)\n        for resource_id in self.graph.get_order():\n\
          \            if resource_id in to_create:\n                plan.changes.append(ResourceChange(\n               \
          \     resource_id=resource_id,\n                    action=Action.CREATE,\n                    after=self.config.resources[resource_id].attributes\n\
          \                ))\n            elif resource_id in to_check:\n                change = self._diff_resource(resource_id)\n\
          \                if change:\n                    plan.changes.append(change)\n\n        return plan\n\n    def _build_graph(self):\n\
          \        \"\"\"Build dependency graph from configuration.\"\"\"\n        for resource_id, resource in self.config.resources.items():\n\
          \            # Explicit dependencies\n            self.graph.add_resource(resource_id, resource.depends_on)\n\n\
          \            # Implicit dependencies from references\n            refs = self._find_references(resource.attributes)\n\
          \            for ref in refs:\n                if ref in self.config.resources:\n                    self.graph.add_dependency(resource_id,\
          \ ref)\n\n    def _find_references(self, value: Any, refs: set = None) -> set[str]:\n        \"\"\"Find resource\
          \ references in attributes.\"\"\"\n        refs = refs or set()\n\n        if isinstance(value, str):\n        \
          \    import re\n            pattern = r'\\$\\{([a-z_]+\\.[a-z_][a-z0-9_]*)'\n            for match in re.finditer(pattern,\
          \ value):\n                refs.add(match.group(1))\n        elif isinstance(value, list):\n            for v in\
          \ value:\n                self._find_references(v, refs)\n        elif isinstance(value, dict):\n            for\
          \ v in value.values():\n                self._find_references(v, refs)\n\n        return refs\n\n    def _diff_resource(self,\
          \ resource_id: str) -> Optional[ResourceChange]:\n        \"\"\"Compare desired vs current state for a resource.\"\
          \"\"\n        desired = self.config.resources[resource_id]\n        current = self.state.resources[resource_id]\n\
          \n        # Check if attributes changed\n        if self._attributes_changed(desired.attributes, current.attributes):\n\
          \            # Check if change requires replacement\n            if self._requires_replacement(desired.type, desired.attributes,\
          \ current.attributes):\n                return ResourceChange(\n                    resource_id=resource_id,\n \
          \                   action=Action.REPLACE,\n                    before=current.attributes,\n                   \
          \ after=desired.attributes,\n                    reason=\"Force replacement attribute changed\"\n              \
          \  )\n            else:\n                return ResourceChange(\n                    resource_id=resource_id,\n\
          \                    action=Action.UPDATE,\n                    before=current.attributes,\n                   \
          \ after=desired.attributes\n                )\n\n        return None\n\n    def _attributes_changed(self, desired:\
          \ dict, current: dict) -> bool:\n        \"\"\"Check if attributes have changed.\"\"\"\n        # Simplified comparison\
          \ - real implementation needs deep diff\n        return desired != current\n\n    def _requires_replacement(self,\
          \ res_type: str, desired: dict, current: dict) -> bool:\n        \"\"\"Check if change requires resource replacement.\"\
          \"\"\n        # Provider defines which attributes force replacement\n        force_new_attrs = {\n            'aws_instance':\
          \ ['ami', 'instance_type'],\n            'aws_vpc': ['cidr_block'],\n        }\n\n        attrs = force_new_attrs.get(res_type,\
          \ [])\n        for attr in attrs:\n            if desired.get(attr) != current.get(attr):\n                return\
          \ True\n        return False\n```"
      pitfalls:
      - Cycle in dependencies causes topological sort failure
      - Implicit dependency detection misses some patterns
      - Replace action must delete before create if unique constraint
    - name: Provider Abstraction
      description: Build provider interface for abstracting cloud APIs with resource CRUD operations.
      hints:
        level1: Define standard interface for create, read, update, delete.
        level2: Handle resource schema validation and type coercion.
        level3: "```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import\
          \ Any, Optional\nfrom enum import Enum\n\nclass AttributeType(Enum):\n    STRING = \"string\"\n    NUMBER = \"number\"\
          \n    BOOL = \"bool\"\n    LIST = \"list\"\n    MAP = \"map\"\n\n@dataclass\nclass AttributeSchema:\n    name: str\n\
          \    type: AttributeType\n    required: bool = False\n    computed: bool = False  # Set by provider, not user\n\
          \    force_new: bool = False  # Change requires replacement\n    default: Any = None\n    description: str = \"\"\
          \n\n@dataclass\nclass ResourceSchema:\n    type_name: str\n    attributes: dict[str, AttributeSchema] = field(default_factory=dict)\n\
          \    create_timeout: int = 300\n    update_timeout: int = 300\n    delete_timeout: int = 300\n\n@dataclass\nclass\
          \ ResourceData:\n    id: str = \"\"\n    attributes: dict = field(default_factory=dict)\n\nclass Provider(ABC):\n\
          \    @abstractmethod\n    def get_schema(self) -> dict[str, ResourceSchema]:\n        \"\"\"Return schemas for all\
          \ resource types.\"\"\"\n        pass\n\n    @abstractmethod\n    async def configure(self, config: dict):\n   \
          \     \"\"\"Configure provider with credentials and settings.\"\"\"\n        pass\n\n    @abstractmethod\n    async\
          \ def create(self, resource_type: str, config: dict) -> ResourceData:\n        \"\"\"Create a new resource.\"\"\"\
          \n        pass\n\n    @abstractmethod\n    async def read(self, resource_type: str, id: str) -> Optional[ResourceData]:\n\
          \        \"\"\"Read current state of resource.\"\"\"\n        pass\n\n    @abstractmethod\n    async def update(self,\
          \ resource_type: str, id: str,\n                    old_config: dict, new_config: dict) -> ResourceData:\n     \
          \   \"\"\"Update existing resource.\"\"\"\n        pass\n\n    @abstractmethod\n    async def delete(self, resource_type:\
          \ str, id: str):\n        \"\"\"Delete resource.\"\"\"\n        pass\n\nclass AWSProvider(Provider):\n    def __init__(self):\n\
          \        self.session = None\n        self._schemas = self._define_schemas()\n\n    def _define_schemas(self) ->\
          \ dict[str, ResourceSchema]:\n        return {\n            'aws_instance': ResourceSchema(\n                type_name='aws_instance',\n\
          \                attributes={\n                    'ami': AttributeSchema('ami', AttributeType.STRING, required=True,\
          \ force_new=True),\n                    'instance_type': AttributeSchema('instance_type', AttributeType.STRING,\
          \ required=True),\n                    'tags': AttributeSchema('tags', AttributeType.MAP),\n                   \
          \ 'id': AttributeSchema('id', AttributeType.STRING, computed=True),\n                    'public_ip': AttributeSchema('public_ip',\
          \ AttributeType.STRING, computed=True),\n                    'private_ip': AttributeSchema('private_ip', AttributeType.STRING,\
          \ computed=True),\n                }\n            ),\n            'aws_vpc': ResourceSchema(\n                type_name='aws_vpc',\n\
          \                attributes={\n                    'cidr_block': AttributeSchema('cidr_block', AttributeType.STRING,\
          \ required=True, force_new=True),\n                    'tags': AttributeSchema('tags', AttributeType.MAP),\n   \
          \                 'id': AttributeSchema('id', AttributeType.STRING, computed=True),\n                }\n       \
          \     ),\n        }\n\n    def get_schema(self) -> dict[str, ResourceSchema]:\n        return self._schemas\n\n\
          \    async def configure(self, config: dict):\n        import aioboto3\n        self.session = aioboto3.Session(\n\
          \            aws_access_key_id=config.get('access_key'),\n            aws_secret_access_key=config.get('secret_key'),\n\
          \            region_name=config.get('region', 'us-east-1')\n        )\n\n    async def create(self, resource_type:\
          \ str, config: dict) -> ResourceData:\n        if resource_type == 'aws_instance':\n            return await self._create_instance(config)\n\
          \        elif resource_type == 'aws_vpc':\n            return await self._create_vpc(config)\n        raise ValueError(f\"\
          Unknown resource type: {resource_type}\")\n\n    async def _create_instance(self, config: dict) -> ResourceData:\n\
          \        async with self.session.client('ec2') as ec2:\n            response = await ec2.run_instances(\n      \
          \          ImageId=config['ami'],\n                InstanceType=config['instance_type'],\n                MinCount=1,\n\
          \                MaxCount=1,\n                TagSpecifications=[{\n                    'ResourceType': 'instance',\n\
          \                    'Tags': [{'Key': k, 'Value': v} for k, v in config.get('tags', {}).items()]\n             \
          \   }] if config.get('tags') else []\n            )\n\n            instance = response['Instances'][0]\n\n     \
          \       # Wait for running state\n            waiter = ec2.get_waiter('instance_running')\n            await waiter.wait(InstanceIds=[instance['InstanceId']])\n\
          \n            # Get updated info\n            desc = await ec2.describe_instances(InstanceIds=[instance['InstanceId']])\n\
          \            instance = desc['Reservations'][0]['Instances'][0]\n\n            return ResourceData(\n          \
          \      id=instance['InstanceId'],\n                attributes={\n                    'ami': instance['ImageId'],\n\
          \                    'instance_type': instance['InstanceType'],\n                    'public_ip': instance.get('PublicIpAddress',\
          \ ''),\n                    'private_ip': instance.get('PrivateIpAddress', ''),\n                    'tags': {t['Key']:\
          \ t['Value'] for t in instance.get('Tags', [])}\n                }\n            )\n\n    async def read(self, resource_type:\
          \ str, id: str) -> Optional[ResourceData]:\n        if resource_type == 'aws_instance':\n            async with\
          \ self.session.client('ec2') as ec2:\n                try:\n                    response = await ec2.describe_instances(InstanceIds=[id])\n\
          \                    if not response['Reservations']:\n                        return None\n                   \
          \ instance = response['Reservations'][0]['Instances'][0]\n                    return ResourceData(\n           \
          \             id=id,\n                        attributes={\n                            'ami': instance['ImageId'],\n\
          \                            'instance_type': instance['InstanceType'],\n                            'public_ip':\
          \ instance.get('PublicIpAddress', ''),\n                            'private_ip': instance.get('PrivateIpAddress',\
          \ ''),\n                        }\n                    )\n                except Exception:\n                  \
          \  return None\n        return None\n\n    async def delete(self, resource_type: str, id: str):\n        if resource_type\
          \ == 'aws_instance':\n            async with self.session.client('ec2') as ec2:\n                await ec2.terminate_instances(InstanceIds=[id])\n\
          \                waiter = ec2.get_waiter('instance_terminated')\n                await waiter.wait(InstanceIds=[id])\n\
          ```"
      pitfalls:
      - API rate limits require retry with backoff
      - Eventual consistency means read after create may fail
      - Resource stuck in pending state needs timeout handling
  metrics-collector:
    name: Metrics Collection System
    description: Build a Prometheus-like metrics collection system with scraping, storage, and PromQL-style querying.
    why_important: Understanding metrics systems helps you design better instrumentation, optimize queries, and debug performance
      issues in production.
    difficulty: advanced
    tags:
    - observability
    - distributed-systems
    - databases
    estimated_hours: 50
    prerequisites:
    - time-series-db
    learning_outcomes:
    - Design pull-based metrics collection
    - Implement time-series storage efficiently
    - Build query language for aggregations
    - Handle high-cardinality metrics
    milestones:
    - name: Metrics Data Model
      description: Implement the metrics data model with counters, gauges, histograms, and summaries with labels.
      hints:
        level1: Each metric has a name, type, labels (key-value pairs), and value.
        level2: Histograms store bucketed counts, summaries track quantiles over time.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Optional\n\
          import time\nimport threading\nimport math\n\nclass MetricType(str, Enum):\n    COUNTER = \"counter\"\n    GAUGE\
          \ = \"gauge\"\n    HISTOGRAM = \"histogram\"\n    SUMMARY = \"summary\"\n\n@dataclass(frozen=True)\nclass Labels:\n\
          \    \"\"\"Immutable label set for metric identification.\"\"\"\n    _labels: tuple[tuple[str, str], ...]\n\n  \
          \  def __init__(self, **kwargs):\n        object.__setattr__(self, '_labels', tuple(sorted(kwargs.items())))\n\n\
          \    def __hash__(self):\n        return hash(self._labels)\n\n    def __eq__(self, other):\n        return self._labels\
          \ == other._labels\n\n    def to_dict(self) -> dict:\n        return dict(self._labels)\n\n@dataclass\nclass Sample:\n\
          \    timestamp: float\n    value: float\n\nclass Counter:\n    def __init__(self, name: str, help_text: str = \"\
          \"):\n        self.name = name\n        self.help = help_text\n        self.type = MetricType.COUNTER\n        self._values:\
          \ dict[Labels, float] = {}\n        self._lock = threading.Lock()\n\n    def inc(self, labels: Labels = None, value:\
          \ float = 1):\n        labels = labels or Labels()\n        with self._lock:\n            self._values[labels] =\
          \ self._values.get(labels, 0) + value\n\n    def get(self, labels: Labels = None) -> float:\n        labels = labels\
          \ or Labels()\n        return self._values.get(labels, 0)\n\n    def collect(self) -> list[tuple[Labels, float]]:\n\
          \        with self._lock:\n            return [(l, v) for l, v in self._values.items()]\n\nclass Gauge:\n    def\
          \ __init__(self, name: str, help_text: str = \"\"):\n        self.name = name\n        self.help = help_text\n \
          \       self.type = MetricType.GAUGE\n        self._values: dict[Labels, float] = {}\n        self._lock = threading.Lock()\n\
          \n    def set(self, value: float, labels: Labels = None):\n        labels = labels or Labels()\n        with self._lock:\n\
          \            self._values[labels] = value\n\n    def inc(self, labels: Labels = None, value: float = 1):\n     \
          \   labels = labels or Labels()\n        with self._lock:\n            self._values[labels] = self._values.get(labels,\
          \ 0) + value\n\n    def dec(self, labels: Labels = None, value: float = 1):\n        self.inc(labels, -value)\n\n\
          \    def collect(self) -> list[tuple[Labels, float]]:\n        with self._lock:\n            return [(l, v) for\
          \ l, v in self._values.items()]\n\nclass Histogram:\n    DEFAULT_BUCKETS = (0.005, 0.01, 0.025, 0.05, 0.1, 0.25,\
          \ 0.5, 1, 2.5, 5, 10, float('inf'))\n\n    def __init__(self, name: str, help_text: str = \"\", buckets: tuple =\
          \ None):\n        self.name = name\n        self.help = help_text\n        self.type = MetricType.HISTOGRAM\n  \
          \      self.buckets = buckets or self.DEFAULT_BUCKETS\n        self._counts: dict[Labels, list[int]] = {}  # bucket\
          \ counts\n        self._sums: dict[Labels, float] = {}\n        self._totals: dict[Labels, int] = {}\n        self._lock\
          \ = threading.Lock()\n\n    def observe(self, value: float, labels: Labels = None):\n        labels = labels or\
          \ Labels()\n        with self._lock:\n            if labels not in self._counts:\n                self._counts[labels]\
          \ = [0] * len(self.buckets)\n                self._sums[labels] = 0\n                self._totals[labels] = 0\n\n\
          \            # Increment appropriate bucket(s)\n            for i, bound in enumerate(self.buckets):\n         \
          \       if value <= bound:\n                    self._counts[labels][i] += 1\n\n            self._sums[labels] +=\
          \ value\n            self._totals[labels] += 1\n\n    def collect(self) -> list[tuple[Labels, dict]]:\n        with\
          \ self._lock:\n            results = []\n            for labels in self._counts:\n                results.append((labels,\
          \ {\n                    'buckets': list(zip(self.buckets, self._counts[labels])),\n                    'sum': self._sums[labels],\n\
          \                    'count': self._totals[labels]\n                }))\n            return results\n\nclass MetricRegistry:\n\
          \    def __init__(self):\n        self._metrics: dict[str, Counter | Gauge | Histogram] = {}\n        self._lock\
          \ = threading.Lock()\n\n    def counter(self, name: str, help_text: str = \"\") -> Counter:\n        with self._lock:\n\
          \            if name not in self._metrics:\n                self._metrics[name] = Counter(name, help_text)\n   \
          \         return self._metrics[name]\n\n    def gauge(self, name: str, help_text: str = \"\") -> Gauge:\n      \
          \  with self._lock:\n            if name not in self._metrics:\n                self._metrics[name] = Gauge(name,\
          \ help_text)\n            return self._metrics[name]\n\n    def histogram(self, name: str, help_text: str = \"\"\
          , buckets: tuple = None) -> Histogram:\n        with self._lock:\n            if name not in self._metrics:\n  \
          \              self._metrics[name] = Histogram(name, help_text, buckets)\n            return self._metrics[name]\n\
          \n    def collect_all(self) -> dict:\n        with self._lock:\n            return {name: metric.collect() for name,\
          \ metric in self._metrics.items()}\n```"
      pitfalls:
      - High cardinality labels cause memory explosion
      - Counter resets on restart need special handling
      - Histogram bucket boundaries can't change after creation
    - name: Scrape Engine
      description: Build a scrape engine that pulls metrics from configured targets with service discovery support.
      hints:
        level1: Poll targets at configured intervals, parse response format.
        level2: Handle target failures, track scrape duration and status.
        level3: "```python\nimport asyncio\nimport aiohttp\nimport time\nfrom dataclasses import dataclass, field\nfrom typing\
          \ import Optional\nfrom enum import Enum\n\nclass ScrapeHealth(str, Enum):\n    UP = \"up\"\n    DOWN = \"down\"\
          \n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass ScrapeTarget:\n    job_name: str\n    address: str\n    metrics_path:\
          \ str = \"/metrics\"\n    scrape_interval: float = 15.0\n    scrape_timeout: float = 10.0\n    labels: dict = field(default_factory=dict)\n\
          \n@dataclass\nclass ScrapeResult:\n    target: ScrapeTarget\n    timestamp: float\n    samples: list[tuple[str,\
          \ Labels, float]]\n    health: ScrapeHealth\n    scrape_duration: float\n    error: Optional[str] = None\n\nclass\
          \ MetricParser:\n    def parse(self, text: str) -> list[tuple[str, Labels, float]]:\n        \"\"\"Parse Prometheus\
          \ exposition format.\"\"\"\n        samples = []\n        for line in text.strip().split('\\n'):\n            if\
          \ not line or line.startswith('#'):\n                continue\n\n            # Parse metric line: name{label=\"\
          value\"} value timestamp?\n            if '{' in line:\n                name_part, rest = line.split('{', 1)\n \
          \               labels_part, value_part = rest.rsplit('}', 1)\n                labels = self._parse_labels(labels_part)\n\
          \            else:\n                parts = line.split()\n                name_part = parts[0]\n               \
          \ value_part = ' '.join(parts[1:])\n                labels = Labels()\n\n            value = float(value_part.split()[0])\n\
          \            samples.append((name_part.strip(), labels, value))\n\n        return samples\n\n    def _parse_labels(self,\
          \ labels_str: str) -> Labels:\n        import re\n        label_dict = {}\n        pattern = r'(\\w+)=\"([^\"]*)\"\
          '\n        for match in re.finditer(pattern, labels_str):\n            label_dict[match.group(1)] = match.group(2)\n\
          \        return Labels(**label_dict)\n\nclass ScrapeEngine:\n    def __init__(self, storage):\n        self.storage\
          \ = storage\n        self.targets: list[ScrapeTarget] = []\n        self.parser = MetricParser()\n        self._tasks:\
          \ dict[str, asyncio.Task] = {}\n        self._running = False\n\n        # Internal metrics\n        self.scrape_duration\
          \ = Histogram(\"scrape_duration_seconds\", \"Scrape duration\")\n        self.scrape_samples = Gauge(\"scrape_samples_scraped\"\
          , \"Samples scraped\")\n        self.up = Gauge(\"up\", \"Target health\")\n\n    def add_target(self, target: ScrapeTarget):\n\
          \        self.targets.append(target)\n        if self._running:\n            self._start_scrape_loop(target)\n\n\
          \    async def start(self):\n        self._running = True\n        for target in self.targets:\n            self._start_scrape_loop(target)\n\
          \n    def _start_scrape_loop(self, target: ScrapeTarget):\n        key = f\"{target.job_name}:{target.address}\"\
          \n        self._tasks[key] = asyncio.create_task(self._scrape_loop(target))\n\n    async def stop(self):\n     \
          \   self._running = False\n        for task in self._tasks.values():\n            task.cancel()\n        await asyncio.gather(*self._tasks.values(),\
          \ return_exceptions=True)\n\n    async def _scrape_loop(self, target: ScrapeTarget):\n        while self._running:\n\
          \            result = await self._scrape(target)\n            await self._process_result(result)\n            await\
          \ asyncio.sleep(target.scrape_interval)\n\n    async def _scrape(self, target: ScrapeTarget) -> ScrapeResult:\n\
          \        url = f\"http://{target.address}{target.metrics_path}\"\n        start_time = time.time()\n\n        try:\n\
          \            async with aiohttp.ClientSession() as session:\n                async with session.get(url, timeout=aiohttp.ClientTimeout(total=target.scrape_timeout))\
          \ as resp:\n                    if resp.status != 200:\n                        raise Exception(f\"HTTP {resp.status}\"\
          )\n\n                    text = await resp.text()\n                    samples = self.parser.parse(text)\n     \
          \               duration = time.time() - start_time\n\n                    return ScrapeResult(\n              \
          \          target=target,\n                        timestamp=start_time,\n                        samples=samples,\n\
          \                        health=ScrapeHealth.UP,\n                        scrape_duration=duration\n           \
          \         )\n\n        except Exception as e:\n            return ScrapeResult(\n                target=target,\n\
          \                timestamp=start_time,\n                samples=[],\n                health=ScrapeHealth.DOWN,\n\
          \                scrape_duration=time.time() - start_time,\n                error=str(e)\n            )\n\n    async\
          \ def _process_result(self, result: ScrapeResult):\n        target = result.target\n        target_labels = Labels(job=target.job_name,\
          \ instance=target.address, **target.labels)\n\n        # Record internal metrics\n        self.scrape_duration.observe(result.scrape_duration,\
          \ target_labels)\n        self.scrape_samples.set(len(result.samples), target_labels)\n        self.up.set(1 if\
          \ result.health == ScrapeHealth.UP else 0, target_labels)\n\n        # Store samples\n        for name, labels,\
          \ value in result.samples:\n            # Merge target labels with metric labels\n            all_labels = Labels(**{**target_labels.to_dict(),\
          \ **labels.to_dict()})\n            await self.storage.store(name, all_labels, result.timestamp, value)\n```"
      pitfalls:
      - Too aggressive scraping overwhelms targets
      - Network timeouts block scrape loop
      - Label collision between target and metric labels
    - name: Time Series Storage
      description: Implement efficient time-series storage with compression, retention, and downsampling.
      hints:
        level1: Store samples in time-ordered chunks per series.
        level2: Use delta encoding and variable-length integers for compression.
        level3: "```python\nimport struct\nfrom dataclasses import dataclass, field\nfrom typing import Iterator, Optional\n\
          import mmap\nimport os\nimport threading\nfrom bisect import bisect_left, bisect_right\n\n@dataclass\nclass Chunk:\n\
          \    \"\"\"Compressed chunk of time-series samples.\"\"\"\n    start_time: float\n    end_time: float = 0\n    samples:\
          \ int = 0\n    data: bytearray = field(default_factory=bytearray)\n    _prev_ts: float = 0\n    _prev_val: float\
          \ = 0\n\n    MAX_SAMPLES = 120  # ~2 hours at 1 min interval\n\n    def append(self, timestamp: float, value: float):\n\
          \        if self.samples == 0:\n            # First sample - store full values\n            self.data.extend(struct.pack('<d',\
          \ timestamp))\n            self.data.extend(struct.pack('<d', value))\n            self.start_time = timestamp\n\
          \            self._prev_ts = timestamp\n            self._prev_val = value\n        else:\n            # Delta encode\
          \ timestamp\n            ts_delta = int((timestamp - self._prev_ts) * 1000)  # ms precision\n            self._encode_varint(ts_delta)\n\
          \n            # XOR encode value\n            self._encode_xor_float(value)\n\n            self._prev_ts = timestamp\n\
          \            self._prev_val = value\n\n        self.end_time = timestamp\n        self.samples += 1\n\n    def is_full(self)\
          \ -> bool:\n        return self.samples >= self.MAX_SAMPLES\n\n    def _encode_varint(self, value: int):\n     \
          \   while value > 127:\n            self.data.append((value & 0x7F) | 0x80)\n            value >>= 7\n        self.data.append(value)\n\
          \n    def _encode_xor_float(self, value: float):\n        # Simple XOR encoding (real implementation uses Gorilla\
          \ compression)\n        prev_bits = struct.unpack('<Q', struct.pack('<d', self._prev_val))[0]\n        curr_bits\
          \ = struct.unpack('<Q', struct.pack('<d', value))[0]\n        xor = prev_bits ^ curr_bits\n\n        if xor == 0:\n\
          \            self.data.append(0)  # Same value\n        else:\n            self.data.append(1)\n            self.data.extend(struct.pack('<Q',\
          \ xor))\n\n    def iterate(self) -> Iterator[tuple[float, float]]:\n        \"\"\"Iterate over samples in chunk.\"\
          \"\"\n        if not self.data:\n            return\n\n        offset = 0\n        # First sample\n        ts =\
          \ struct.unpack_from('<d', self.data, offset)[0]\n        offset += 8\n        val = struct.unpack_from('<d', self.data,\
          \ offset)[0]\n        offset += 8\n        yield ts, val\n\n        prev_ts = ts\n        prev_val = val\n\n   \
          \     for _ in range(self.samples - 1):\n            # Decode timestamp delta\n            ts_delta, bytes_read\
          \ = self._decode_varint(offset)\n            offset += bytes_read\n            ts = prev_ts + ts_delta / 1000.0\n\
          \n            # Decode value\n            marker = self.data[offset]\n            offset += 1\n            if marker\
          \ == 0:\n                val = prev_val\n            else:\n                xor = struct.unpack_from('<Q', self.data,\
          \ offset)[0]\n                offset += 8\n                prev_bits = struct.unpack('<Q', struct.pack('<d', prev_val))[0]\n\
          \                curr_bits = prev_bits ^ xor\n                val = struct.unpack('<d', struct.pack('<Q', curr_bits))[0]\n\
          \n            yield ts, val\n            prev_ts = ts\n            prev_val = val\n\n    def _decode_varint(self,\
          \ offset: int) -> tuple[int, int]:\n        result = 0\n        shift = 0\n        bytes_read = 0\n        while\
          \ True:\n            byte = self.data[offset + bytes_read]\n            bytes_read += 1\n            result |= (byte\
          \ & 0x7F) << shift\n            if not (byte & 0x80):\n                break\n            shift += 7\n        return\
          \ result, bytes_read\n\n@dataclass(frozen=True)\nclass SeriesKey:\n    name: str\n    labels: Labels\n\n    def\
          \ __hash__(self):\n        return hash((self.name, self.labels))\n\nclass TimeSeriesStorage:\n    def __init__(self,\
          \ data_dir: str, retention_days: int = 15):\n        self.data_dir = data_dir\n        self.retention_days = retention_days\n\
          \        self._series: dict[SeriesKey, list[Chunk]] = {}\n        self._lock = threading.RLock()\n        os.makedirs(data_dir,\
          \ exist_ok=True)\n\n    async def store(self, name: str, labels: Labels, timestamp: float, value: float):\n    \
          \    key = SeriesKey(name, labels)\n\n        with self._lock:\n            if key not in self._series:\n      \
          \          self._series[key] = [Chunk(start_time=timestamp)]\n\n            chunks = self._series[key]\n       \
          \     current_chunk = chunks[-1]\n\n            if current_chunk.is_full():\n                # Start new chunk\n\
          \                chunks.append(Chunk(start_time=timestamp))\n                current_chunk = chunks[-1]\n\n    \
          \        current_chunk.append(timestamp, value)\n\n    def query(self, name: str, labels: Labels,\n            \
          \  start_time: float, end_time: float) -> Iterator[tuple[float, float]]:\n        \"\"\"Query samples in time range.\"\
          \"\"\n        key = SeriesKey(name, labels)\n\n        with self._lock:\n            chunks = self._series.get(key,\
          \ [])\n\n            for chunk in chunks:\n                # Skip chunks outside time range\n                if\
          \ chunk.end_time < start_time or chunk.start_time > end_time:\n                    continue\n\n                for\
          \ ts, val in chunk.iterate():\n                    if start_time <= ts <= end_time:\n                        yield\
          \ ts, val\n\n    def query_by_name(self, name: str, start_time: float,\n                      end_time: float) ->\
          \ dict[Labels, list[tuple[float, float]]]:\n        \"\"\"Query all series with given name.\"\"\"\n        results\
          \ = {}\n\n        with self._lock:\n            for key, chunks in self._series.items():\n                if key.name\
          \ != name:\n                    continue\n\n                samples = list(self.query(name, key.labels, start_time,\
          \ end_time))\n                if samples:\n                    results[key.labels] = samples\n\n        return results\n\
          \n    def compact(self):\n        \"\"\"Compact old chunks and enforce retention.\"\"\"\n        cutoff = time.time()\
          \ - (self.retention_days * 86400)\n\n        with self._lock:\n            for key in list(self._series.keys()):\n\
          \                chunks = self._series[key]\n                # Remove chunks entirely before cutoff\n          \
          \      self._series[key] = [c for c in chunks if c.end_time >= cutoff]\n\n                # Remove empty series\n\
          \                if not self._series[key]:\n                    del self._series[key]\n```"
      pitfalls:
      - Chunk boundaries at exact times cause off-by-one
      - Concurrent writes corrupt chunk data
      - Compression ratio degrades with irregular timestamps
    - name: Query Engine
      description: Build a PromQL-like query engine with instant queries, range queries, and aggregation functions.
      hints:
        level1: Parse expressions into AST, evaluate against storage.
        level2: Implement rate(), sum(), avg() and label matchers.
        level3: "```python\nfrom dataclasses import dataclass\nfrom typing import Union, Callable\nfrom enum import Enum\n\
          import re\n\nclass MatchType(Enum):\n    EQUAL = \"=\"\n    NOT_EQUAL = \"!=\"\n    REGEX = \"=~\"\n    NOT_REGEX\
          \ = \"!~\"\n\n@dataclass\nclass LabelMatcher:\n    name: str\n    value: str\n    type: MatchType\n\n    def matches(self,\
          \ labels: Labels) -> bool:\n        actual = labels.to_dict().get(self.name, \"\")\n        if self.type == MatchType.EQUAL:\n\
          \            return actual == self.value\n        elif self.type == MatchType.NOT_EQUAL:\n            return actual\
          \ != self.value\n        elif self.type == MatchType.REGEX:\n            return bool(re.match(self.value, actual))\n\
          \        elif self.type == MatchType.NOT_REGEX:\n            return not bool(re.match(self.value, actual))\n\n@dataclass\n\
          class VectorSelector:\n    name: str\n    matchers: list[LabelMatcher]\n    range_seconds: float = 0  # 0 for instant,\
          \ >0 for range\n\n@dataclass\nclass AggregateExpr:\n    op: str  # sum, avg, max, min, count\n    expr: 'Expr'\n\
          \    by: list[str] = None  # Group by labels\n    without: list[str] = None  # Group by all except\n\n@dataclass\n\
          class FunctionExpr:\n    name: str  # rate, irate, increase, etc.\n    args: list['Expr']\n\nExpr = Union[VectorSelector,\
          \ AggregateExpr, FunctionExpr]\n\nclass QueryEngine:\n    def __init__(self, storage: TimeSeriesStorage):\n    \
          \    self.storage = storage\n        self.functions = {\n            'rate': self._rate,\n            'irate': self._irate,\n\
          \            'increase': self._increase,\n            'sum_over_time': self._sum_over_time,\n            'avg_over_time':\
          \ self._avg_over_time,\n        }\n        self.aggregations = {\n            'sum': lambda vals: sum(vals),\n \
          \           'avg': lambda vals: sum(vals) / len(vals) if vals else 0,\n            'max': lambda vals: max(vals)\
          \ if vals else 0,\n            'min': lambda vals: min(vals) if vals else 0,\n            'count': lambda vals:\
          \ len(vals),\n        }\n\n    def instant_query(self, query: str, timestamp: float) -> dict[Labels, float]:\n \
          \       \"\"\"Execute instant query at specific time.\"\"\"\n        expr = self._parse(query)\n        return self._eval(expr,\
          \ timestamp, timestamp)\n\n    def range_query(self, query: str, start: float, end: float,\n                   step:\
          \ float) -> dict[Labels, list[tuple[float, float]]]:\n        \"\"\"Execute range query with step.\"\"\"\n     \
          \   expr = self._parse(query)\n        results = {}\n\n        for ts in self._time_range(start, end, step):\n \
          \           instant = self._eval(expr, ts - step, ts)\n            for labels, value in instant.items():\n     \
          \           results.setdefault(labels, []).append((ts, value))\n\n        return results\n\n    def _parse(self,\
          \ query: str) -> Expr:\n        \"\"\"Parse query string into expression tree.\"\"\"\n        # Simplified parser\
          \ - real implementation uses proper grammar\n        query = query.strip()\n\n        # Check for aggregation: sum(metric{...})\n\
          \        agg_match = re.match(r'(sum|avg|max|min|count)\\s*\\((.+)\\)\\s*(by|without)?\\s*\\(([^)]+)\\)?$', query)\n\
          \        if agg_match:\n            op = agg_match.group(1)\n            inner = agg_match.group(2)\n          \
          \  modifier = agg_match.group(3)\n            group_labels = [l.strip() for l in agg_match.group(4).split(',')]\
          \ if agg_match.group(4) else []\n\n            return AggregateExpr(\n                op=op,\n                expr=self._parse(inner),\n\
          \                by=group_labels if modifier == 'by' else None,\n                without=group_labels if modifier\
          \ == 'without' else None\n            )\n\n        # Check for function: rate(metric{...}[5m])\n        func_match\
          \ = re.match(r'(\\w+)\\((.+)\\)$', query)\n        if func_match and func_match.group(1) in self.functions:\n  \
          \          return FunctionExpr(\n                name=func_match.group(1),\n                args=[self._parse(func_match.group(2))]\n\
          \            )\n\n        # Vector selector: metric{label=\"value\"}[5m]\n        return self._parse_vector_selector(query)\n\
          \n    def _parse_vector_selector(self, query: str) -> VectorSelector:\n        # Parse: metric_name{label=\"value\"\
          ,label2=~\"regex\"}[5m]\n        range_match = re.search(r'\\[(\\d+)([smhd])\\]$', query)\n        range_seconds\
          \ = 0\n        if range_match:\n            query = query[:range_match.start()]\n            value = int(range_match.group(1))\n\
          \            unit = range_match.group(2)\n            range_seconds = value * {'s': 1, 'm': 60, 'h': 3600, 'd':\
          \ 86400}[unit]\n\n        if '{' in query:\n            name, labels_str = query.split('{', 1)\n            labels_str\
          \ = labels_str.rstrip('}')\n            matchers = self._parse_matchers(labels_str)\n        else:\n           \
          \ name = query\n            matchers = []\n\n        return VectorSelector(name=name.strip(), matchers=matchers,\
          \ range_seconds=range_seconds)\n\n    def _parse_matchers(self, labels_str: str) -> list[LabelMatcher]:\n      \
          \  matchers = []\n        pattern = r'(\\w+)\\s*(=~|!=|!~|=)\\s*\"([^\"]*)\"'\n        for match in re.finditer(pattern,\
          \ labels_str):\n            matchers.append(LabelMatcher(\n                name=match.group(1),\n              \
          \  value=match.group(3),\n                type=MatchType(match.group(2))\n            ))\n        return matchers\n\
          \n    def _eval(self, expr: Expr, start: float, end: float) -> dict[Labels, float]:\n        if isinstance(expr,\
          \ VectorSelector):\n            return self._eval_selector(expr, start, end)\n        elif isinstance(expr, AggregateExpr):\n\
          \            return self._eval_aggregate(expr, start, end)\n        elif isinstance(expr, FunctionExpr):\n     \
          \       return self._eval_function(expr, start, end)\n\n    def _eval_selector(self, sel: VectorSelector, start:\
          \ float, end: float) -> dict[Labels, float]:\n        # Get all series matching name\n        query_start = start\
          \ - sel.range_seconds if sel.range_seconds else start\n        all_series = self.storage.query_by_name(sel.name,\
          \ query_start, end)\n\n        results = {}\n        for labels, samples in all_series.items():\n            # Apply\
          \ label matchers\n            if all(m.matches(labels) for m in sel.matchers):\n                if sel.range_seconds:\n\
          \                    # Return samples in range (for rate, etc.)\n                    results[labels] = samples\n\
          \                else:\n                    # Return latest sample\n                    if samples:\n          \
          \              results[labels] = samples[-1][1]\n\n        return results\n\n    def _rate(self, samples: list[tuple[float,\
          \ float]]) -> float:\n        \"\"\"Calculate per-second rate of increase.\"\"\"\n        if len(samples) < 2:\n\
          \            return 0\n\n        first_ts, first_val = samples[0]\n        last_ts, last_val = samples[-1]\n\n \
          \       duration = last_ts - first_ts\n        if duration <= 0:\n            return 0\n\n        # Handle counter\
          \ resets\n        increase = last_val - first_val\n        if increase < 0:\n            increase = last_val  #\
          \ Counter reset\n\n        return increase / duration\n```"
      pitfalls:
      - Rate calculation wrong across counter resets
      - Label matching with regex can be slow
      - Query on high-cardinality labels causes OOM
  log-aggregator:
    name: Log Aggregation System
    description: Build a log aggregation system like Loki with log ingestion, indexing, and LogQL-style querying.
    why_important: Log systems are critical for debugging. Understanding log indexing helps optimize queries and reduce storage
      costs.
    difficulty: intermediate
    tags:
    - observability
    - databases
    - search
    estimated_hours: 40
    prerequisites:
    - shell
    learning_outcomes:
    - Design efficient log ingestion pipeline
    - Build inverted index for label-based queries
    - Implement log compression and chunking
    - Handle high-volume log streams
    milestones:
    - name: Log Ingestion
      description: Build log ingestion endpoint that accepts structured logs with labels and timestamps.
      hints:
        level1: Accept logs as JSON with labels, timestamp, and line.
        level2: Batch logs for efficient writes, validate and enrich data.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\n\
          import asyncio\nimport json\nimport time\nimport hashlib\n\n@dataclass\nclass LogEntry:\n    timestamp: datetime\n\
          \    labels: dict[str, str]\n    line: str\n    stream_id: str = \"\"\n\n    def __post_init__(self):\n        if\
          \ not self.stream_id:\n            # Generate stream ID from labels\n            label_str = json.dumps(self.labels,\
          \ sort_keys=True)\n            self.stream_id = hashlib.md5(label_str.encode()).hexdigest()[:16]\n\n@dataclass\n\
          class LogBatch:\n    stream_id: str\n    labels: dict[str, str]\n    entries: list[LogEntry] = field(default_factory=list)\n\
          \nclass LogIngester:\n    def __init__(self, storage, batch_size: int = 1000, flush_interval: float = 1.0):\n  \
          \      self.storage = storage\n        self.batch_size = batch_size\n        self.flush_interval = flush_interval\n\
          \        self._batches: dict[str, LogBatch] = {}\n        self._lock = asyncio.Lock()\n        self._flush_task\
          \ = None\n\n    async def start(self):\n        self._flush_task = asyncio.create_task(self._flush_loop())\n\n \
          \   async def stop(self):\n        if self._flush_task:\n            self._flush_task.cancel()\n        await self._flush_all()\n\
          \n    async def ingest(self, entries: list[LogEntry]):\n        \"\"\"Ingest log entries, batching by stream.\"\"\
          \"\n        async with self._lock:\n            for entry in entries:\n                stream_id = entry.stream_id\n\
          \n                if stream_id not in self._batches:\n                    self._batches[stream_id] = LogBatch(\n\
          \                        stream_id=stream_id,\n                        labels=entry.labels\n                   \
          \ )\n\n                batch = self._batches[stream_id]\n                batch.entries.append(entry)\n\n       \
          \         if len(batch.entries) >= self.batch_size:\n                    await self._flush_batch(stream_id)\n\n\
          \    async def _flush_loop(self):\n        while True:\n            await asyncio.sleep(self.flush_interval)\n \
          \           await self._flush_all()\n\n    async def _flush_all(self):\n        async with self._lock:\n       \
          \     for stream_id in list(self._batches.keys()):\n                await self._flush_batch(stream_id)\n\n    async\
          \ def _flush_batch(self, stream_id: str):\n        batch = self._batches.pop(stream_id, None)\n        if batch\
          \ and batch.entries:\n            # Sort by timestamp\n            batch.entries.sort(key=lambda e: e.timestamp)\n\
          \            await self.storage.write_batch(batch)\n\nclass LogPushHandler:\n    \"\"\"HTTP handler for Loki push\
          \ protocol.\"\"\"\n\n    def __init__(self, ingester: LogIngester):\n        self.ingester = ingester\n\n    async\
          \ def handle_push(self, request_body: bytes) -> dict:\n        \"\"\"Handle Loki push format.\"\"\"\n        data\
          \ = json.loads(request_body)\n        entries = []\n\n        for stream in data.get('streams', []):\n         \
          \   labels = self._parse_labels(stream.get('stream', {}))\n\n            for value in stream.get('values', []):\n\
          \                # Loki format: [timestamp_ns, line]\n                ts_ns = int(value[0])\n                line\
          \ = value[1]\n\n                entries.append(LogEntry(\n                    timestamp=datetime.fromtimestamp(ts_ns\
          \ / 1e9),\n                    labels=labels,\n                    line=line\n                ))\n\n        await\
          \ self.ingester.ingest(entries)\n        return {'status': 'success', 'entries': len(entries)}\n\n    def _parse_labels(self,\
          \ labels: dict) -> dict[str, str]:\n        # Ensure all values are strings\n        return {k: str(v) for k, v\
          \ in labels.items()}\n```"
      pitfalls:
      - Out-of-order timestamps complicate querying
      - Memory pressure from unbounded batching
      - Label value with special characters breaks parsing
    - name: Log Index
      description: Build inverted index for fast label-based log queries with bloom filters for negative lookups.
      hints:
        level1: Index maps label values to stream IDs containing them.
        level2: Use bloom filter per chunk to skip chunks without matches.
        level3: "```python\nfrom dataclasses import dataclass, field\nimport mmh3  # MurmurHash3 for bloom filter\n\nclass\
          \ BloomFilter:\n    def __init__(self, size: int = 10000, num_hashes: int = 7):\n        self.size = size\n    \
          \    self.num_hashes = num_hashes\n        self.bits = bytearray((size + 7) // 8)\n\n    def add(self, item: str):\n\
          \        for i in range(self.num_hashes):\n            idx = mmh3.hash(item, i) % self.size\n            self.bits[idx\
          \ // 8] |= (1 << (idx % 8))\n\n    def might_contain(self, item: str) -> bool:\n        for i in range(self.num_hashes):\n\
          \            idx = mmh3.hash(item, i) % self.size\n            if not (self.bits[idx // 8] & (1 << (idx % 8))):\n\
          \                return False\n        return True\n\n@dataclass\nclass ChunkMeta:\n    chunk_id: str\n    stream_id:\
          \ str\n    start_time: float\n    end_time: float\n    entries: int\n    bloom: BloomFilter\n\n@dataclass\nclass\
          \ StreamMeta:\n    stream_id: str\n    labels: dict[str, str]\n    chunks: list[str] = field(default_factory=list)\
          \  # chunk IDs\n\nclass LogIndex:\n    def __init__(self):\n        # Label index: label_name -> label_value ->\
          \ set of stream_ids\n        self._label_index: dict[str, dict[str, set[str]]] = {}\n        # Stream metadata\n\
          \        self._streams: dict[str, StreamMeta] = {}\n        # Chunk metadata\n        self._chunks: dict[str, ChunkMeta]\
          \ = {}\n\n    def index_stream(self, stream_id: str, labels: dict[str, str]):\n        self._streams[stream_id]\
          \ = StreamMeta(stream_id=stream_id, labels=labels)\n\n        for name, value in labels.items():\n            if\
          \ name not in self._label_index:\n                self._label_index[name] = {}\n            if value not in self._label_index[name]:\n\
          \                self._label_index[name][value] = set()\n            self._label_index[name][value].add(stream_id)\n\
          \n    def index_chunk(self, chunk: ChunkMeta):\n        self._chunks[chunk.chunk_id] = chunk\n        if stream\
          \ := self._streams.get(chunk.stream_id):\n            stream.chunks.append(chunk.chunk_id)\n\n    def find_streams(self,\
          \ matchers: list[LabelMatcher]) -> set[str]:\n        \"\"\"Find stream IDs matching all label matchers.\"\"\"\n\
          \        if not matchers:\n            return set(self._streams.keys())\n\n        result = None\n        for matcher\
          \ in matchers:\n            matching = self._match_label(matcher)\n            if result is None:\n            \
          \    result = matching\n            else:\n                result &= matching\n\n        return result or set()\n\
          \n    def _match_label(self, matcher: LabelMatcher) -> set[str]:\n        label_values = self._label_index.get(matcher.name,\
          \ {})\n\n        if matcher.type == MatchType.EQUAL:\n            return label_values.get(matcher.value, set()).copy()\n\
          \        elif matcher.type == MatchType.NOT_EQUAL:\n            result = set()\n            for value, streams in\
          \ label_values.items():\n                if value != matcher.value:\n                    result |= streams\n   \
          \         return result\n        elif matcher.type == MatchType.REGEX:\n            result = set()\n           \
          \ pattern = re.compile(matcher.value)\n            for value, streams in label_values.items():\n               \
          \ if pattern.match(value):\n                    result |= streams\n            return result\n\n        return set()\n\
          \n    def find_chunks(self, stream_ids: set[str], start_time: float,\n                   end_time: float, text_filter:\
          \ str = None) -> list[str]:\n        \"\"\"Find relevant chunk IDs for query.\"\"\"\n        chunk_ids = []\n\n\
          \        for stream_id in stream_ids:\n            stream = self._streams.get(stream_id)\n            if not stream:\n\
          \                continue\n\n            for chunk_id in stream.chunks:\n                chunk = self._chunks.get(chunk_id)\n\
          \                if not chunk:\n                    continue\n\n                # Time range filter\n          \
          \      if chunk.end_time < start_time or chunk.start_time > end_time:\n                    continue\n\n        \
          \        # Bloom filter for text search\n                if text_filter and not chunk.bloom.might_contain(text_filter):\n\
          \                    continue\n\n                chunk_ids.append(chunk_id)\n\n        return chunk_ids\n\n    def\
          \ get_label_values(self, label_name: str) -> list[str]:\n        \"\"\"Get all values for a label (for autocomplete).\"\
          \"\"\n        return list(self._label_index.get(label_name, {}).keys())\n\n    def get_label_names(self) -> list[str]:\n\
          \        \"\"\"Get all label names.\"\"\"\n        return list(self._label_index.keys())\n```"
      pitfalls:
      - Bloom filter false positives require verification
      - High cardinality labels bloat index
      - Index rebuild on corruption is expensive
    - name: Log Query Engine
      description: Build LogQL-style query engine with label filtering, text search, and log processing functions.
      hints:
        level1: Parse query into stream selector and pipeline stages.
        level2: Stream results to handle large result sets efficiently.
        level3: "```python\nfrom dataclasses import dataclass\nfrom typing import Iterator, Callable\nimport re\n\n@dataclass\n\
          class LogQueryResult:\n    timestamp: datetime\n    labels: dict[str, str]\n    line: str\n\nclass LogQueryEngine:\n\
          \    def __init__(self, index: LogIndex, storage):\n        self.index = index\n        self.storage = storage\n\
          \n    def query(self, query: str, start_time: float,\n              end_time: float, limit: int = 1000) -> Iterator[LogQueryResult]:\n\
          \        \"\"\"Execute LogQL query.\"\"\"\n        parsed = self._parse_query(query)\n        stream_ids = self.index.find_streams(parsed['matchers'])\n\
          \n        if not stream_ids:\n            return\n\n        # Find relevant chunks\n        chunk_ids = self.index.find_chunks(\n\
          \            stream_ids, start_time, end_time,\n            text_filter=parsed.get('line_filter')\n        )\n\n\
          \        count = 0\n        for chunk_id in chunk_ids:\n            for entry in self.storage.read_chunk(chunk_id):\n\
          \                # Apply time filter\n                ts = entry.timestamp.timestamp()\n                if ts <\
          \ start_time or ts > end_time:\n                    continue\n\n                # Apply pipeline\n             \
          \   result = self._apply_pipeline(entry, parsed['pipeline'])\n                if result:\n                    yield\
          \ result\n                    count += 1\n                    if count >= limit:\n                        return\n\
          \n    def _parse_query(self, query: str) -> dict:\n        \"\"\"Parse LogQL query.\"\"\"\n        # {label=\"value\"\
          } |= \"text\" | json | line_format \"{{.field}}\"\n        result = {\n            'matchers': [],\n           \
          \ 'pipeline': [],\n            'line_filter': None\n        }\n\n        # Extract stream selector\n        selector_match\
          \ = re.match(r'\\{([^}]*)\\}', query)\n        if selector_match:\n            result['matchers'] = self._parse_matchers(selector_match.group(1))\n\
          \            query = query[selector_match.end():]\n\n        # Parse pipeline stages\n        stages = query.split('|')\n\
          \        for stage in stages[1:]:  # Skip empty first element\n            stage = stage.strip()\n            if\
          \ stage.startswith('='):\n                # Line filter: |= \"text\" or |~ \"regex\"\n                op = stage[:2]\
          \ if stage[1] in '=~' else stage[0]\n                pattern = stage[len(op):].strip().strip('\"')\n           \
          \     result['pipeline'].append(('filter', op, pattern))\n                if op == '=':\n                    result['line_filter']\
          \ = pattern\n            elif stage == 'json':\n                result['pipeline'].append(('json',))\n         \
          \   elif stage == 'logfmt':\n                result['pipeline'].append(('logfmt',))\n            elif stage.startswith('line_format'):\n\
          \                template = re.search(r'\"([^\"]*)\"', stage).group(1)\n                result['pipeline'].append(('line_format',\
          \ template))\n            elif stage.startswith('label_format'):\n                # label_format new_label=value\n\
          \                assignments = self._parse_label_assignments(stage)\n                result['pipeline'].append(('label_format',\
          \ assignments))\n\n        return result\n\n    def _apply_pipeline(self, entry: LogEntry,\n                   \
          \    pipeline: list) -> LogQueryResult:\n        \"\"\"Apply pipeline stages to log entry.\"\"\"\n        line =\
          \ entry.line\n        labels = entry.labels.copy()\n\n        for stage in pipeline:\n            if stage[0] ==\
          \ 'filter':\n                op, pattern = stage[1], stage[2]\n                if op == '=':\n                 \
          \   if pattern not in line:\n                        return None\n                elif op == '!=':\n           \
          \         if pattern in line:\n                        return None\n                elif op == '=~':\n         \
          \           if not re.search(pattern, line):\n                        return None\n                elif op == '!~':\n\
          \                    if re.search(pattern, line):\n                        return None\n\n            elif stage[0]\
          \ == 'json':\n                try:\n                    parsed = json.loads(line)\n                    labels.update({k:\
          \ str(v) for k, v in parsed.items()})\n                except json.JSONDecodeError:\n                    pass  #\
          \ Keep original line\n\n            elif stage[0] == 'logfmt':\n                # Parse key=value pairs\n      \
          \          for match in re.finditer(r'(\\w+)=(\"([^\"]*)\"|\\S+)', line):\n                    key = match.group(1)\n\
          \                    value = match.group(3) or match.group(2)\n                    labels[key] = value\n\n     \
          \       elif stage[0] == 'line_format':\n                template = stage[1]\n                # Simple template\
          \ substitution\n                for key, value in labels.items():\n                    template = template.replace(f'{{{{{key}}}}}',\
          \ str(value))\n                line = template\n\n        return LogQueryResult(\n            timestamp=entry.timestamp,\n\
          \            labels=labels,\n            line=line\n        )\n\n    def aggregate(self, query: str, start_time:\
          \ float, end_time: float,\n                 step: float) -> dict[str, list[tuple[float, float]]]:\n        \"\"\"\
          Execute metric query over logs (count_over_time, rate, etc.).\"\"\"\n        # Parse aggregation: count_over_time({job=\"\
          app\"}[5m])\n        match = re.match(r'(\\w+)\\((.+)\\[(\\d+)([smh])\\]\\)', query)\n        if not match:\n  \
          \          raise ValueError(\"Invalid aggregation query\")\n\n        func = match.group(1)\n        inner_query\
          \ = match.group(2)\n        interval = int(match.group(3)) * {'s': 1, 'm': 60, 'h': 3600}[match.group(4)]\n\n  \
          \      results = {}\n        for ts in self._time_range(start_time, end_time, step):\n            window_start =\
          \ ts - interval\n            count = 0\n            for _ in self.query(inner_query, window_start, ts, limit=10000):\n\
          \                count += 1\n\n            key = 'total'  # Could be grouped by labels\n            results.setdefault(key,\
          \ []).append((ts, count))\n\n        return results\n```"
      pitfalls:
      - Unbounded query scans entire log history
      - JSON parsing failures silently drop entries
      - Pipeline order matters for filter efficiency
  alerting-system:
    name: Alerting System
    description: Build an alerting system with rule evaluation, alert grouping, silencing, and notification routing.
    why_important: Alerting is critical for on-call. Understanding alert fatigue, grouping, and routing helps design better
      alert systems.
    difficulty: intermediate
    tags:
    - observability
    - distributed-systems
    estimated_hours: 35
    prerequisites:
    - metrics-collector
    learning_outcomes:
    - Design alert rule evaluation engine
    - Implement alert state machine (pending, firing, resolved)
    - Build notification routing and grouping
    - Handle alert silencing and inhibition
    milestones:
    - name: Alert Rule Evaluation
      description: Build rule evaluation engine that periodically queries metrics and triggers alerts based on thresholds.
      hints:
        level1: Define rules with PromQL expression and threshold.
        level2: 'Track alert state transitions: inactive -> pending -> firing.'
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom enum\
          \ import Enum\nfrom typing import Optional\nimport asyncio\n\nclass AlertState(str, Enum):\n    INACTIVE = \"inactive\"\
          \n    PENDING = \"pending\"\n    FIRING = \"firing\"\n\n@dataclass\nclass AlertRule:\n    name: str\n    expr: str\
          \  # PromQL expression\n    for_duration: timedelta = timedelta(0)  # Duration before firing\n    labels: dict[str,\
          \ str] = field(default_factory=dict)\n    annotations: dict[str, str] = field(default_factory=dict)\n    evaluation_interval:\
          \ timedelta = timedelta(seconds=60)\n\n@dataclass\nclass Alert:\n    rule: AlertRule\n    labels: dict[str, str]\n\
          \    state: AlertState = AlertState.INACTIVE\n    active_at: Optional[datetime] = None\n    fired_at: Optional[datetime]\
          \ = None\n    resolved_at: Optional[datetime] = None\n    value: float = 0\n    annotations: dict[str, str] = field(default_factory=dict)\n\
          \n    @property\n    def fingerprint(self) -> str:\n        import hashlib\n        import json\n        label_str\
          \ = json.dumps(self.labels, sort_keys=True)\n        return hashlib.md5(f\"{self.rule.name}:{label_str}\".encode()).hexdigest()\n\
          \nclass AlertEvaluator:\n    def __init__(self, query_engine, notifier):\n        self.query_engine = query_engine\n\
          \        self.notifier = notifier\n        self.rules: list[AlertRule] = []\n        self.active_alerts: dict[str,\
          \ Alert] = {}\n        self._running = False\n\n    def add_rule(self, rule: AlertRule):\n        self.rules.append(rule)\n\
          \n    async def start(self):\n        self._running = True\n        while self._running:\n            await self._evaluate_all()\n\
          \            await asyncio.sleep(60)  # Evaluation interval\n\n    async def stop(self):\n        self._running\
          \ = False\n\n    async def _evaluate_all(self):\n        for rule in self.rules:\n            await self._evaluate_rule(rule)\n\
          \n    async def _evaluate_rule(self, rule: AlertRule):\n        now = datetime.utcnow()\n\n        # Query metrics\n\
          \        results = self.query_engine.instant_query(rule.expr, now.timestamp())\n\n        # Track which alerts are\
          \ still active\n        active_fingerprints = set()\n\n        for labels, value in results.items():\n         \
          \   # Merge rule labels with metric labels\n            all_labels = {**labels.to_dict(), **rule.labels}\n     \
          \       alert = Alert(rule=rule, labels=all_labels, value=value)\n            fingerprint = alert.fingerprint\n\
          \            active_fingerprints.add(fingerprint)\n\n            # Check for existing alert\n            existing\
          \ = self.active_alerts.get(fingerprint)\n\n            if existing:\n                # Update existing alert\n \
          \               self._update_alert(existing, value, now)\n            else:\n                # New alert\n     \
          \           alert.state = AlertState.PENDING\n                alert.active_at = now\n                alert.annotations\
          \ = self._render_annotations(rule.annotations, all_labels, value)\n                self.active_alerts[fingerprint]\
          \ = alert\n\n        # Resolve alerts no longer active\n        for fingerprint in list(self.active_alerts.keys()):\n\
          \            if fingerprint not in active_fingerprints:\n                alert = self.active_alerts[fingerprint]\n\
          \                if alert.state != AlertState.INACTIVE:\n                    await self._resolve_alert(alert, now)\n\
          \n    def _update_alert(self, alert: Alert, value: float, now: datetime):\n        alert.value = value\n\n     \
          \   if alert.state == AlertState.PENDING:\n            # Check if pending duration has passed\n            pending_duration\
          \ = now - alert.active_at\n            if pending_duration >= alert.rule.for_duration:\n                alert.state\
          \ = AlertState.FIRING\n                alert.fired_at = now\n                asyncio.create_task(self.notifier.notify(alert))\n\
          \n        elif alert.state == AlertState.FIRING:\n            # Still firing, update value\n            pass\n\n\
          \    async def _resolve_alert(self, alert: Alert, now: datetime):\n        if alert.state == AlertState.FIRING:\n\
          \            alert.resolved_at = now\n            alert.state = AlertState.INACTIVE\n            await self.notifier.notify_resolved(alert)\n\
          \        del self.active_alerts[alert.fingerprint]\n\n    def _render_annotations(self, templates: dict, labels:\
          \ dict, value: float) -> dict:\n        result = {}\n        for key, template in templates.items():\n         \
          \   text = template\n            for label_name, label_value in labels.items():\n                text = text.replace(f'{{{{\
          \ $labels.{label_name} }}}}', str(label_value))\n            text = text.replace('{{ $value }}', str(value))\n \
          \           result[key] = text\n        return result\n```"
      pitfalls:
      - Flapping alerts from noisy metrics need hysteresis
      - for_duration resets if metric briefly returns normal
      - Template rendering errors crash evaluation loop
    - name: Alert Grouping
      description: Group related alerts together to reduce notification noise with configurable grouping keys.
      hints:
        level1: Group alerts by common labels (e.g., alertname, cluster).
        level2: Batch notifications within group_wait window.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom typing\
          \ import Optional\nimport asyncio\n\n@dataclass\nclass AlertGroup:\n    key: str  # Hash of group labels\n    labels:\
          \ dict[str, str]  # Group labels\n    alerts: dict[str, Alert] = field(default_factory=dict)\n    first_alert_at:\
          \ Optional[datetime] = None\n    last_notification_at: Optional[datetime] = None\n\nclass AlertGrouper:\n    def\
          \ __init__(self, group_by: list[str],\n                 group_wait: timedelta = timedelta(seconds=30),\n       \
          \          group_interval: timedelta = timedelta(minutes=5),\n                 repeat_interval: timedelta = timedelta(hours=4)):\n\
          \        self.group_by = group_by\n        self.group_wait = group_wait\n        self.group_interval = group_interval\n\
          \        self.repeat_interval = repeat_interval\n        self._groups: dict[str, AlertGroup] = {}\n        self._pending_notifications:\
          \ dict[str, asyncio.Task] = {}\n\n    def add_alert(self, alert: Alert) -> AlertGroup:\n        \"\"\"Add alert\
          \ to appropriate group.\"\"\"\n        group_labels = {k: alert.labels.get(k, '') for k in self.group_by}\n    \
          \    group_key = self._compute_key(group_labels)\n\n        if group_key not in self._groups:\n            self._groups[group_key]\
          \ = AlertGroup(\n                key=group_key,\n                labels=group_labels,\n                first_alert_at=datetime.utcnow()\n\
          \            )\n\n        group = self._groups[group_key]\n        group.alerts[alert.fingerprint] = alert\n\n \
          \       # Schedule notification if not already pending\n        if group_key not in self._pending_notifications:\n\
          \            self._schedule_notification(group)\n\n        return group\n\n    def remove_alert(self, alert: Alert)\
          \ -> Optional[AlertGroup]:\n        \"\"\"Remove resolved alert from group.\"\"\"\n        group_labels = {k: alert.labels.get(k,\
          \ '') for k in self.group_by}\n        group_key = self._compute_key(group_labels)\n\n        group = self._groups.get(group_key)\n\
          \        if group:\n            group.alerts.pop(alert.fingerprint, None)\n            if not group.alerts:\n  \
          \              # Group empty, remove it\n                del self._groups[group_key]\n                return None\n\
          \        return group\n\n    def _compute_key(self, labels: dict[str, str]) -> str:\n        import hashlib\n  \
          \      import json\n        return hashlib.md5(json.dumps(labels, sort_keys=True).encode()).hexdigest()\n\n    def\
          \ _schedule_notification(self, group: AlertGroup):\n        \"\"\"Schedule notification after group_wait.\"\"\"\n\
          \        async def notify_after_wait():\n            await asyncio.sleep(self.group_wait.total_seconds())\n    \
          \        await self._send_notification(group)\n\n        self._pending_notifications[group.key] = asyncio.create_task(notify_after_wait())\n\
          \n    async def _send_notification(self, group: AlertGroup):\n        now = datetime.utcnow()\n\n        # Check\
          \ if we should notify\n        if group.last_notification_at:\n            since_last = now - group.last_notification_at\n\
          \n            # Check repeat interval for firing alerts\n            if since_last < self.repeat_interval:\n   \
          \             # Check group interval for new alerts\n                if since_last < self.group_interval:\n    \
          \                # Too soon, reschedule\n                    self._schedule_notification(group)\n              \
          \      return\n\n        group.last_notification_at = now\n        self._pending_notifications.pop(group.key, None)\n\
          \n        # Yield notification to be sent\n        return group\n\n    def get_groups(self) -> list[AlertGroup]:\n\
          \        return list(self._groups.values())\n\nclass NotificationBatcher:\n    \"\"\"Batches notifications to reduce\
          \ alert fatigue.\"\"\"\n\n    def __init__(self, grouper: AlertGrouper, sender):\n        self.grouper = grouper\n\
          \        self.sender = sender\n\n    async def on_alert(self, alert: Alert):\n        group = self.grouper.add_alert(alert)\n\
          \        # Notification will be sent after group_wait\n\n    async def on_resolved(self, alert: Alert):\n      \
          \  group = self.grouper.remove_alert(alert)\n        if group:\n            # Send resolution notification\n   \
          \         await self.sender.send_grouped(group, resolved=[alert])\n\n    async def process_groups(self):\n     \
          \   \"\"\"Periodically check and send pending notifications.\"\"\"\n        for group in self.grouper.get_groups():\n\
          \            notification = await self.grouper._send_notification(group)\n            if notification:\n       \
          \         await self.sender.send_grouped(group)\n```"
      pitfalls:
      - Group key change orphans alert in old group
      - Long group_wait delays critical alerts
      - Memory leak from empty groups not cleaned up
    - name: Silencing & Inhibition
      description: Implement alert silencing for maintenance windows and inhibition to suppress alerts when related alerts
        fire.
      hints:
        level1: Silence matches alerts by label matchers within time window.
        level2: 'Inhibition: if source alert fires, suppress target alerts with matching labels.'
        level3: "```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Optional\n\
          import re\n\n@dataclass\nclass Silence:\n    id: str\n    matchers: list[LabelMatcher]\n    starts_at: datetime\n\
          \    ends_at: datetime\n    created_by: str\n    comment: str\n\n    def is_active(self, now: datetime = None) ->\
          \ bool:\n        now = now or datetime.utcnow()\n        return self.starts_at <= now <= self.ends_at\n\n    def\
          \ matches(self, labels: dict[str, str]) -> bool:\n        return all(m.matches_dict(labels) for m in self.matchers)\n\
          \n@dataclass\nclass InhibitRule:\n    source_matchers: list[LabelMatcher]  # Source alert must match\n    target_matchers:\
          \ list[LabelMatcher]  # Target alert must match\n    equal: list[str]  # Labels that must be equal between source\
          \ and target\n\nclass SilenceManager:\n    def __init__(self):\n        self.silences: dict[str, Silence] = {}\n\
          \n    def create(self, matchers: list[LabelMatcher], starts_at: datetime,\n               ends_at: datetime, created_by:\
          \ str, comment: str) -> Silence:\n        import uuid\n        silence = Silence(\n            id=str(uuid.uuid4()),\n\
          \            matchers=matchers,\n            starts_at=starts_at,\n            ends_at=ends_at,\n            created_by=created_by,\n\
          \            comment=comment\n        )\n        self.silences[silence.id] = silence\n        return silence\n\n\
          \    def delete(self, silence_id: str):\n        self.silences.pop(silence_id, None)\n\n    def is_silenced(self,\
          \ labels: dict[str, str], now: datetime = None) -> Optional[Silence]:\n        for silence in self.silences.values():\n\
          \            if silence.is_active(now) and silence.matches(labels):\n                return silence\n        return\
          \ None\n\n    def cleanup_expired(self):\n        now = datetime.utcnow()\n        self.silences = {\n         \
          \   id: s for id, s in self.silences.items()\n            if s.ends_at > now\n        }\n\nclass InhibitionProcessor:\n\
          \    def __init__(self, rules: list[InhibitRule]):\n        self.rules = rules\n\n    def is_inhibited(self, target:\
          \ Alert, active_alerts: list[Alert]) -> bool:\n        \"\"\"Check if target alert is inhibited by any active alert.\"\
          \"\"\n        for rule in self.rules:\n            # Check if target matches target_matchers\n            if not\
          \ self._matches_all(target.labels, rule.target_matchers):\n                continue\n\n            # Find source\
          \ alerts that could inhibit\n            for source in active_alerts:\n                if source.fingerprint ==\
          \ target.fingerprint:\n                    continue\n\n                if source.state != AlertState.FIRING:\n \
          \                   continue\n\n                # Check if source matches source_matchers\n                if not\
          \ self._matches_all(source.labels, rule.source_matchers):\n                    continue\n\n                # Check\
          \ equal labels\n                if self._labels_equal(source.labels, target.labels, rule.equal):\n             \
          \       return True\n\n        return False\n\n    def _matches_all(self, labels: dict, matchers: list[LabelMatcher])\
          \ -> bool:\n        return all(m.matches_dict(labels) for m in matchers)\n\n    def _labels_equal(self, source:\
          \ dict, target: dict, keys: list[str]) -> bool:\n        return all(source.get(k) == target.get(k) for k in keys)\n\
          \nclass AlertProcessor:\n    \"\"\"Main processor combining silencing, inhibition, and grouping.\"\"\"\n\n    def\
          \ __init__(self, silence_manager: SilenceManager,\n                 inhibitor: InhibitionProcessor,\n          \
          \       grouper: AlertGrouper,\n                 notifier):\n        self.silences = silence_manager\n        self.inhibitor\
          \ = inhibitor\n        self.grouper = grouper\n        self.notifier = notifier\n        self.active_alerts: list[Alert]\
          \ = []\n\n    async def process(self, alert: Alert):\n        # Check if silenced\n        silence = self.silences.is_silenced(alert.labels)\n\
          \        if silence:\n            alert.silenced_by = silence.id\n            return\n\n        # Check if inhibited\n\
          \        if self.inhibitor.is_inhibited(alert, self.active_alerts):\n            alert.inhibited = True\n      \
          \      return\n\n        # Track active alert\n        self.active_alerts = [a for a in self.active_alerts\n   \
          \                          if a.fingerprint != alert.fingerprint]\n        if alert.state == AlertState.FIRING:\n\
          \            self.active_alerts.append(alert)\n\n        # Add to group for notification\n        self.grouper.add_alert(alert)\n\
          ```"
      pitfalls:
      - Inhibition loop when alerts inhibit each other
      - Silence with wrong matchers misses alerts
      - Race between silence creation and firing alert
    - name: Notification Routing
      description: Route alerts to different receivers (Slack, PagerDuty, email) based on matching rules.
      hints:
        level1: Define routes with matchers that direct to receivers.
        level2: Support nested routes with continue/stop semantics.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom abc import ABC, abstractmethod\nfrom typing import\
          \ Optional\nimport aiohttp\n\n@dataclass\nclass Route:\n    matchers: list[LabelMatcher] = field(default_factory=list)\n\
          \    receiver: str = \"\"\n    continue_matching: bool = False\n    children: list['Route'] = field(default_factory=list)\n\
          \    group_by: list[str] = field(default_factory=list)\n\nclass Receiver(ABC):\n    @abstractmethod\n    async def\
          \ send(self, group: AlertGroup):\n        pass\n\nclass SlackReceiver(Receiver):\n    def __init__(self, webhook_url:\
          \ str, channel: str):\n        self.webhook_url = webhook_url\n        self.channel = channel\n\n    async def send(self,\
          \ group: AlertGroup):\n        alerts = list(group.alerts.values())\n        firing = [a for a in alerts if a.state\
          \ == AlertState.FIRING]\n\n        blocks = [{\n            \"type\": \"header\",\n            \"text\": {\n   \
          \             \"type\": \"plain_text\",\n                \"text\": f\"\U0001F525 {len(firing)} alert(s) firing\"\
          \n            }\n        }]\n\n        for alert in firing[:10]:  # Limit to 10\n            blocks.append({\n \
          \               \"type\": \"section\",\n                \"text\": {\n                    \"type\": \"mrkdwn\",\n\
          \                    \"text\": f\"*{alert.labels.get('alertname', 'Unknown')}*\\n\"\n                          \
          \ f\"{alert.annotations.get('summary', '')}\\n\"\n                           f\"Value: {alert.value}\"\n       \
          \         }\n            })\n\n        payload = {\n            \"channel\": self.channel,\n            \"blocks\"\
          : blocks\n        }\n\n        async with aiohttp.ClientSession() as session:\n            await session.post(self.webhook_url,\
          \ json=payload)\n\nclass PagerDutyReceiver(Receiver):\n    def __init__(self, service_key: str):\n        self.service_key\
          \ = service_key\n        self.api_url = \"https://events.pagerduty.com/v2/enqueue\"\n\n    async def send(self,\
          \ group: AlertGroup):\n        alerts = list(group.alerts.values())\n\n        for alert in alerts:\n          \
          \  if alert.state == AlertState.FIRING:\n                event_action = \"trigger\"\n            else:\n       \
          \         event_action = \"resolve\"\n\n            payload = {\n                \"routing_key\": self.service_key,\n\
          \                \"event_action\": event_action,\n                \"dedup_key\": alert.fingerprint,\n          \
          \      \"payload\": {\n                    \"summary\": alert.annotations.get('summary', alert.labels.get('alertname')),\n\
          \                    \"source\": alert.labels.get('instance', 'unknown'),\n                    \"severity\": alert.labels.get('severity',\
          \ 'warning'),\n                    \"custom_details\": alert.labels\n                }\n            }\n\n      \
          \      async with aiohttp.ClientSession() as session:\n                await session.post(self.api_url, json=payload)\n\
          \nclass NotificationRouter:\n    def __init__(self, root_route: Route, receivers: dict[str, Receiver]):\n      \
          \  self.root = root_route\n        self.receivers = receivers\n\n    async def route(self, group: AlertGroup):\n\
          \        \"\"\"Route alert group to matching receivers.\"\"\"\n        matched_receivers = self._find_receivers(group,\
          \ self.root)\n\n        for receiver_name in matched_receivers:\n            receiver = self.receivers.get(receiver_name)\n\
          \            if receiver:\n                try:\n                    await receiver.send(group)\n              \
          \  except Exception as e:\n                    print(f\"Failed to send to {receiver_name}: {e}\")\n\n    def _find_receivers(self,\
          \ group: AlertGroup, route: Route) -> list[str]:\n        \"\"\"Find all matching receivers for alert group.\"\"\
          \"\n        # Check if this route matches\n        if not self._matches_route(group, route):\n            return\
          \ []\n\n        receivers = []\n\n        # Add this route's receiver if specified\n        if route.receiver:\n\
          \            receivers.append(route.receiver)\n\n        # Check children\n        for child in route.children:\n\
          \            child_receivers = self._find_receivers(group, child)\n            receivers.extend(child_receivers)\n\
          \n            # Stop if matched and not continue\n            if child_receivers and not child.continue_matching:\n\
          \                break\n\n        return receivers\n\n    def _matches_route(self, group: AlertGroup, route: Route)\
          \ -> bool:\n        if not route.matchers:\n            return True  # Default route matches all\n\n        # Use\
          \ first alert's labels for matching\n        if not group.alerts:\n            return False\n\n        first_alert\
          \ = list(group.alerts.values())[0]\n        return all(m.matches_dict(first_alert.labels) for m in route.matchers)\n\
          ```"
      pitfalls:
      - Missing default route causes unrouted alerts
      - continue=true on all routes sends duplicate notifications
      - Rate limiting by receiver prevents important alerts
  apm-system:
    name: APM Tracing System
    description: Build an Application Performance Monitoring system with distributed tracing, service maps, and performance
      analysis.
    why_important: APM helps identify performance bottlenecks across microservices. Understanding trace data helps design
      better instrumentation.
    difficulty: advanced
    tags:
    - observability
    - distributed-systems
    - performance
    estimated_hours: 45
    prerequisites:
    - distributed-tracing
    learning_outcomes:
    - Design span collection and storage
    - Build service dependency maps from traces
    - Implement latency percentile analysis
    - Handle high-volume trace sampling
    milestones:
    - name: Trace Collection
      description: Build trace collector that ingests spans from multiple services with proper parent-child linking.
      hints:
        level1: Collect spans with trace ID, span ID, parent ID, timestamps.
        level2: Handle out-of-order span arrival, reconstruct trace tree.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\n\
          from collections import defaultdict\nimport asyncio\n\n@dataclass\nclass Span:\n    trace_id: str\n    span_id:\
          \ str\n    parent_id: Optional[str]\n    operation_name: str\n    service_name: str\n    start_time: datetime\n\
          \    duration_us: int  # microseconds\n    status: str = \"OK\"  # OK, ERROR\n    tags: dict = field(default_factory=dict)\n\
          \    logs: list = field(default_factory=list)\n\n@dataclass\nclass Trace:\n    trace_id: str\n    root_span: Optional[Span]\
          \ = None\n    spans: dict[str, Span] = field(default_factory=dict)  # span_id -> Span\n    services: set = field(default_factory=set)\n\
          \    start_time: Optional[datetime] = None\n    duration_us: int = 0\n\n    def add_span(self, span: Span):\n  \
          \      self.spans[span.span_id] = span\n        self.services.add(span.service_name)\n\n        if span.parent_id\
          \ is None:\n            self.root_span = span\n            self.start_time = span.start_time\n            self.duration_us\
          \ = span.duration_us\n\n    def get_children(self, span_id: str) -> list[Span]:\n        return [s for s in self.spans.values()\
          \ if s.parent_id == span_id]\n\n    def get_depth(self) -> int:\n        if not self.root_span:\n            return\
          \ 0\n\n        def depth(span_id: str) -> int:\n            children = self.get_children(span_id)\n            if\
          \ not children:\n                return 1\n            return 1 + max(depth(c.span_id) for c in children)\n\n  \
          \      return depth(self.root_span.span_id)\n\nclass TraceCollector:\n    def __init__(self, storage, assembler_timeout:\
          \ float = 30.0):\n        self.storage = storage\n        self.assembler_timeout = assembler_timeout\n        self._pending_traces:\
          \ dict[str, Trace] = {}\n        self._trace_timers: dict[str, asyncio.Task] = {}\n        self._lock = asyncio.Lock()\n\
          \n    async def collect(self, spans: list[Span]):\n        \"\"\"Collect batch of spans.\"\"\"\n        async with\
          \ self._lock:\n            for span in spans:\n                await self._add_span(span)\n\n    async def _add_span(self,\
          \ span: Span):\n        trace_id = span.trace_id\n\n        if trace_id not in self._pending_traces:\n         \
          \   self._pending_traces[trace_id] = Trace(trace_id=trace_id)\n            # Start assembly timeout\n          \
          \  self._trace_timers[trace_id] = asyncio.create_task(\n                self._assembly_timeout(trace_id)\n     \
          \       )\n\n        trace = self._pending_traces[trace_id]\n        trace.add_span(span)\n\n        # Check if\
          \ trace is complete (has root and reasonable time passed)\n        if trace.root_span and self._is_likely_complete(trace):\n\
          \            await self._finalize_trace(trace_id)\n\n    def _is_likely_complete(self, trace: Trace) -> bool:\n\
          \        # Heuristic: trace is complete if we have root and all spans\n        # have their parents (except root)\n\
          \        for span in trace.spans.values():\n            if span.parent_id and span.parent_id not in trace.spans:\n\
          \                return False  # Missing parent\n        return True\n\n    async def _assembly_timeout(self, trace_id:\
          \ str):\n        await asyncio.sleep(self.assembler_timeout)\n        async with self._lock:\n            if trace_id\
          \ in self._pending_traces:\n                await self._finalize_trace(trace_id)\n\n    async def _finalize_trace(self,\
          \ trace_id: str):\n        trace = self._pending_traces.pop(trace_id, None)\n        timer = self._trace_timers.pop(trace_id,\
          \ None)\n        if timer:\n            timer.cancel()\n\n        if trace:\n            # Calculate trace-level\
          \ metrics\n            self._calculate_metrics(trace)\n            await self.storage.store_trace(trace)\n\n   \
          \ def _calculate_metrics(self, trace: Trace):\n        if not trace.spans:\n            return\n\n        # Find\
          \ actual start/end times\n        start_times = [s.start_time for s in trace.spans.values()]\n        end_times\
          \ = [\n            s.start_time.timestamp() * 1e6 + s.duration_us\n            for s in trace.spans.values()\n \
          \       ]\n\n        trace.start_time = min(start_times)\n        trace.duration_us = int(max(end_times) - min(s.timestamp()\
          \ * 1e6 for s in start_times))\n\nclass OTLPReceiver:\n    \"\"\"OpenTelemetry Protocol receiver.\"\"\"\n\n    def\
          \ __init__(self, collector: TraceCollector):\n        self.collector = collector\n\n    async def handle_traces(self,\
          \ request_body: bytes) -> dict:\n        # Parse OTLP protobuf (simplified - real impl uses protobuf)\n        import\
          \ json\n        data = json.loads(request_body)\n\n        spans = []\n        for resource_span in data.get('resourceSpans',\
          \ []):\n            service_name = self._get_service_name(resource_span.get('resource', {}))\n\n            for\
          \ scope_span in resource_span.get('scopeSpans', []):\n                for span_data in scope_span.get('spans', []):\n\
          \                    spans.append(Span(\n                        trace_id=span_data['traceId'],\n              \
          \          span_id=span_data['spanId'],\n                        parent_id=span_data.get('parentSpanId'),\n    \
          \                    operation_name=span_data['name'],\n                        service_name=service_name,\n   \
          \                     start_time=datetime.fromtimestamp(span_data['startTimeUnixNano'] / 1e9),\n               \
          \         duration_us=int((span_data['endTimeUnixNano'] - span_data['startTimeUnixNano']) / 1000),\n           \
          \             status=span_data.get('status', {}).get('code', 'OK'),\n                        tags=self._parse_attributes(span_data.get('attributes',\
          \ []))\n                    ))\n\n        await self.collector.collect(spans)\n        return {'accepted': len(spans)}\n\
          \n    def _get_service_name(self, resource: dict) -> str:\n        for attr in resource.get('attributes', []):\n\
          \            if attr['key'] == 'service.name':\n                return attr['value'].get('stringValue', 'unknown')\n\
          \        return 'unknown'\n\n    def _parse_attributes(self, attributes: list) -> dict:\n        result = {}\n \
          \       for attr in attributes:\n            key = attr['key']\n            value = attr['value']\n            if\
          \ 'stringValue' in value:\n                result[key] = value['stringValue']\n            elif 'intValue' in value:\n\
          \                result[key] = value['intValue']\n            elif 'boolValue' in value:\n                result[key]\
          \ = value['boolValue']\n        return result\n```"
      pitfalls:
      - Late-arriving spans miss assembly window
      - Memory grows unbounded with incomplete traces
      - Clock skew makes span ordering incorrect
    - name: Service Map
      description: Build service dependency map from trace data showing call relationships and error rates.
      hints:
        level1: Track caller-callee relationships from parent-child spans.
        level2: 'Aggregate metrics per edge: latency percentiles, error rate, throughput.'
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict\nimport statistics\n\
          \n@dataclass\nclass ServiceEdge:\n    source: str\n    target: str\n    request_count: int = 0\n    error_count:\
          \ int = 0\n    latencies: list[int] = field(default_factory=list)  # microseconds\n\n    @property\n    def error_rate(self)\
          \ -> float:\n        return self.error_count / self.request_count if self.request_count else 0\n\n    @property\n\
          \    def p50_latency(self) -> float:\n        if not self.latencies:\n            return 0\n        return statistics.median(self.latencies)\n\
          \n    @property\n    def p99_latency(self) -> float:\n        if not self.latencies:\n            return 0\n   \
          \     sorted_lat = sorted(self.latencies)\n        idx = int(len(sorted_lat) * 0.99)\n        return sorted_lat[min(idx,\
          \ len(sorted_lat) - 1)]\n\n@dataclass\nclass ServiceNode:\n    name: str\n    request_count: int = 0\n    error_count:\
          \ int = 0\n    latencies: list[int] = field(default_factory=list)\n\nclass ServiceMapBuilder:\n    def __init__(self,\
          \ window_seconds: int = 300):\n        self.window_seconds = window_seconds\n        self.nodes: dict[str, ServiceNode]\
          \ = {}\n        self.edges: dict[str, ServiceEdge] = {}  # \"source->target\" -> edge\n        self._lock = asyncio.Lock()\n\
          \n    async def process_trace(self, trace: Trace):\n        \"\"\"Extract service relationships from trace.\"\"\"\
          \n        async with self._lock:\n            for span in trace.spans.values():\n                # Update service\
          \ node\n                service = span.service_name\n                if service not in self.nodes:\n           \
          \         self.nodes[service] = ServiceNode(name=service)\n\n                node = self.nodes[service]\n      \
          \          node.request_count += 1\n                if span.status == \"ERROR\":\n                    node.error_count\
          \ += 1\n                node.latencies.append(span.duration_us)\n\n                # Update edge if span has parent\n\
          \                if span.parent_id and span.parent_id in trace.spans:\n                    parent = trace.spans[span.parent_id]\n\
          \                    if parent.service_name != span.service_name:\n                        edge_key = f\"{parent.service_name}->{span.service_name}\"\
          \n                        if edge_key not in self.edges:\n                            self.edges[edge_key] = ServiceEdge(\n\
          \                                source=parent.service_name,\n                                target=span.service_name\n\
          \                            )\n\n                        edge = self.edges[edge_key]\n                        edge.request_count\
          \ += 1\n                        if span.status == \"ERROR\":\n                            edge.error_count += 1\n\
          \                        edge.latencies.append(span.duration_us)\n\n    def get_map(self) -> dict:\n        \"\"\
          \"Return service map as graph data.\"\"\"\n        return {\n            'nodes': [\n                {\n       \
          \             'id': name,\n                    'requests': node.request_count,\n                    'error_rate':\
          \ node.error_count / node.request_count if node.request_count else 0,\n                    'p50_latency_ms': statistics.median(node.latencies)\
          \ / 1000 if node.latencies else 0,\n                    'p99_latency_ms': self._percentile(node.latencies, 0.99)\
          \ / 1000 if node.latencies else 0\n                }\n                for name, node in self.nodes.items()\n   \
          \         ],\n            'edges': [\n                {\n                    'source': edge.source,\n          \
          \          'target': edge.target,\n                    'requests': edge.request_count,\n                    'error_rate':\
          \ edge.error_rate,\n                    'p50_latency_ms': edge.p50_latency / 1000,\n                    'p99_latency_ms':\
          \ edge.p99_latency / 1000\n                }\n                for edge in self.edges.values()\n            ]\n \
          \       }\n\n    def _percentile(self, values: list, p: float) -> float:\n        if not values:\n            return\
          \ 0\n        sorted_vals = sorted(values)\n        idx = int(len(sorted_vals) * p)\n        return sorted_vals[min(idx,\
          \ len(sorted_vals) - 1)]\n\n    def get_dependencies(self, service: str) -> dict:\n        \"\"\"Get upstream and\
          \ downstream dependencies for a service.\"\"\"\n        upstream = []\n        downstream = []\n\n        for edge\
          \ in self.edges.values():\n            if edge.target == service:\n                upstream.append({\n         \
          \           'service': edge.source,\n                    'requests': edge.request_count,\n                    'error_rate':\
          \ edge.error_rate\n                })\n            elif edge.source == service:\n                downstream.append({\n\
          \                    'service': edge.target,\n                    'requests': edge.request_count,\n            \
          \        'error_rate': edge.error_rate\n                })\n\n        return {\n            'service': service,\n\
          \            'upstream': upstream,\n            'downstream': downstream\n        }\n```"
      pitfalls:
      - Same-service spans inflate node metrics
      - Async calls appear as separate traces
      - High cardinality operations bloat map
    - name: Trace Sampling
      description: Implement adaptive sampling to reduce storage costs while preserving interesting traces.
      hints:
        level1: Sample N% of traces randomly, but always keep errors.
        level2: 'Tail-based sampling: decide after trace completes based on characteristics.'
        level3: "```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nimport random\nimport\
          \ hashlib\n\nclass Sampler(ABC):\n    @abstractmethod\n    def should_sample(self, trace: Trace) -> bool:\n    \
          \    pass\n\nclass ProbabilisticSampler(Sampler):\n    def __init__(self, sample_rate: float = 0.1):\n        self.sample_rate\
          \ = sample_rate\n\n    def should_sample(self, trace: Trace) -> bool:\n        # Use trace ID for consistent sampling\n\
          \        hash_val = int(hashlib.md5(trace.trace_id.encode()).hexdigest(), 16)\n        return (hash_val % 10000)\
          \ < (self.sample_rate * 10000)\n\nclass RateLimitingSampler(Sampler):\n    def __init__(self, traces_per_second:\
          \ float = 100):\n        self.traces_per_second = traces_per_second\n        self._tokens = traces_per_second\n\
          \        self._last_refill = time.time()\n\n    def should_sample(self, trace: Trace) -> bool:\n        self._refill()\n\
          \        if self._tokens >= 1:\n            self._tokens -= 1\n            return True\n        return False\n\n\
          \    def _refill(self):\n        now = time.time()\n        elapsed = now - self._last_refill\n        self._tokens\
          \ = min(\n            self.traces_per_second,\n            self._tokens + elapsed * self.traces_per_second\n   \
          \     )\n        self._last_refill = now\n\n@dataclass\nclass SamplingPolicy:\n    name: str\n    matcher: callable\
          \  # (Trace) -> bool\n    sampler: Sampler\n    priority: int = 0\n\nclass TailBasedSampler:\n    \"\"\"Decides\
          \ sampling after trace is complete.\"\"\"\n\n    def __init__(self, default_rate: float = 0.01):\n        self.default_sampler\
          \ = ProbabilisticSampler(default_rate)\n        self.policies: list[SamplingPolicy] = []\n        self._setup_default_policies()\n\
          \n    def _setup_default_policies(self):\n        # Always sample errors\n        self.policies.append(SamplingPolicy(\n\
          \            name=\"errors\",\n            matcher=lambda t: any(s.status == \"ERROR\" for s in t.spans.values()),\n\
          \            sampler=ProbabilisticSampler(1.0),  # 100%\n            priority=100\n        ))\n\n        # Sample\
          \ slow traces\n        self.policies.append(SamplingPolicy(\n            name=\"slow\",\n            matcher=lambda\
          \ t: t.duration_us > 5_000_000,  # > 5s\n            sampler=ProbabilisticSampler(0.5),  # 50%\n            priority=90\n\
          \        ))\n\n        # Sample traces with many spans (complex flows)\n        self.policies.append(SamplingPolicy(\n\
          \            name=\"complex\",\n            matcher=lambda t: len(t.spans) > 50,\n            sampler=ProbabilisticSampler(0.3),\n\
          \            priority=80\n        ))\n\n        # Higher rate for specific services\n        self.policies.append(SamplingPolicy(\n\
          \            name=\"payment_service\",\n            matcher=lambda t: \"payment-service\" in t.services,\n     \
          \       sampler=ProbabilisticSampler(0.2),\n            priority=70\n        ))\n\n    def add_policy(self, policy:\
          \ SamplingPolicy):\n        self.policies.append(policy)\n        self.policies.sort(key=lambda p: p.priority, reverse=True)\n\
          \n    def should_sample(self, trace: Trace) -> tuple[bool, str]:\n        \"\"\"Returns (should_sample, reason).\"\
          \"\"\n        for policy in self.policies:\n            if policy.matcher(trace):\n                if policy.sampler.should_sample(trace):\n\
          \                    return True, policy.name\n                # Policy matched but sampler said no\n          \
          \      # Continue to check lower priority policies\n\n        # Fall back to default\n        if self.default_sampler.should_sample(trace):\n\
          \            return True, \"default\"\n\n        return False, \"dropped\"\n\nclass AdaptiveSampler:\n    \"\"\"\
          Adjusts sampling rate based on throughput.\"\"\"\n\n    def __init__(self, target_traces_per_minute: int = 1000):\n\
          \        self.target = target_traces_per_minute\n        self.current_rate = 0.1\n        self._count = 0\n    \
          \    self._last_adjust = time.time()\n        self._adjust_interval = 60  # seconds\n\n    def should_sample(self,\
          \ trace: Trace) -> bool:\n        self._maybe_adjust()\n\n        if random.random() < self.current_rate:\n    \
          \        self._count += 1\n            return True\n        return False\n\n    def _maybe_adjust(self):\n     \
          \   now = time.time()\n        if now - self._last_adjust < self._adjust_interval:\n            return\n\n     \
          \   # Calculate actual rate\n        actual_per_minute = self._count\n\n        # Adjust rate\n        if actual_per_minute\
          \ > self.target * 1.1:\n            # Too many, decrease rate\n            self.current_rate *= 0.9\n        elif\
          \ actual_per_minute < self.target * 0.9:\n            # Too few, increase rate\n            self.current_rate *=\
          \ 1.1\n\n        self.current_rate = max(0.001, min(1.0, self.current_rate))\n        self._count = 0\n        self._last_adjust\
          \ = now\n```"
      pitfalls:
      - Head-based sampling loses interesting traces
      - Rate limiting causes bursty drops
      - Adaptive sampling oscillates under variable load
  feature-flags:
    name: Feature Flag System
    description: Build a feature flag system with gradual rollouts, A/B testing support, targeting rules, and real-time updates.
    why_important: Feature flags enable safe deployments, A/B testing, and gradual rollouts. Understanding flag systems helps
      design better release strategies.
    difficulty: intermediate
    tags:
    - backend
    - architecture
    - devops
    estimated_hours: 35
    prerequisites:
    - rest-api-design
    learning_outcomes:
    - Design flag evaluation with targeting rules
    - Implement percentage-based rollouts
    - Build real-time flag updates without restart
    - Handle flag dependencies and conflicts
    milestones:
    - name: Flag Evaluation Engine
      description: Build the core flag evaluation engine with support for boolean, string, number, and JSON flags.
      hints:
        level1: Store flag configurations with default and variation values.
        level2: Evaluate targeting rules against user context to determine variation.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom typing import Any, Optional, Union\nfrom enum import\
          \ Enum\nimport hashlib\n\nclass FlagType(str, Enum):\n    BOOLEAN = \"boolean\"\n    STRING = \"string\"\n    NUMBER\
          \ = \"number\"\n    JSON = \"json\"\n\n@dataclass\nclass Variation:\n    value: Any\n    name: str = \"\"\n    description:\
          \ str = \"\"\n\n@dataclass\nclass TargetingRule:\n    id: str\n    conditions: list['Condition']\n    variation_index:\
          \ int\n    priority: int = 0\n\n@dataclass\nclass Condition:\n    attribute: str\n    operator: str  # eq, neq,\
          \ contains, startswith, gt, lt, in, regex\n    values: list[Any]\n\n@dataclass\nclass FeatureFlag:\n    key: str\n\
          \    type: FlagType\n    variations: list[Variation]\n    default_variation: int  # Index of default variation\n\
          \    targeting_rules: list[TargetingRule] = field(default_factory=list)\n    percentage_rollout: Optional['PercentageRollout']\
          \ = None\n    enabled: bool = True\n    prerequisites: list[str] = field(default_factory=list)\n\n@dataclass\nclass\
          \ PercentageRollout:\n    bucket_by: str = \"user_id\"  # Attribute to hash for bucketing\n    variations: list[tuple[int,\
          \ int]]  # [(variation_index, percentage), ...]\n\n@dataclass\nclass EvaluationContext:\n    user_id: str = \"\"\
          \n    attributes: dict = field(default_factory=dict)\n\n    def get(self, key: str) -> Any:\n        if key == \"\
          user_id\":\n            return self.user_id\n        return self.attributes.get(key)\n\n@dataclass\nclass EvaluationResult:\n\
          \    value: Any\n    variation_index: int\n    reason: str\n    flag_key: str\n\nclass FlagEvaluator:\n    def __init__(self,\
          \ flags: dict[str, FeatureFlag]):\n        self.flags = flags\n\n    def evaluate(self, flag_key: str, context:\
          \ EvaluationContext,\n                default: Any = None) -> EvaluationResult:\n        flag = self.flags.get(flag_key)\n\
          \n        if not flag:\n            return EvaluationResult(\n                value=default,\n                variation_index=-1,\n\
          \                reason=\"FLAG_NOT_FOUND\",\n                flag_key=flag_key\n            )\n\n        if not\
          \ flag.enabled:\n            return self._result(flag, flag.default_variation, \"FLAG_DISABLED\")\n\n        # Check\
          \ prerequisites\n        for prereq_key in flag.prerequisites:\n            prereq_result = self.evaluate(prereq_key,\
          \ context)\n            if not prereq_result.value:\n                return self._result(flag, flag.default_variation,\
          \ \"PREREQUISITE_FAILED\")\n\n        # Evaluate targeting rules (highest priority first)\n        sorted_rules\
          \ = sorted(flag.targeting_rules, key=lambda r: r.priority, reverse=True)\n        for rule in sorted_rules:\n  \
          \          if self._evaluate_rule(rule, context):\n                return self._result(flag, rule.variation_index,\
          \ f\"RULE:{rule.id}\")\n\n        # Check percentage rollout\n        if flag.percentage_rollout:\n            variation_idx\
          \ = self._evaluate_rollout(flag, context)\n            if variation_idx is not None:\n                return self._result(flag,\
          \ variation_idx, \"ROLLOUT\")\n\n        return self._result(flag, flag.default_variation, \"DEFAULT\")\n\n    def\
          \ _evaluate_rule(self, rule: TargetingRule, context: EvaluationContext) -> bool:\n        return all(self._evaluate_condition(c,\
          \ context) for c in rule.conditions)\n\n    def _evaluate_condition(self, condition: Condition, context: EvaluationContext)\
          \ -> bool:\n        attr_value = context.get(condition.attribute)\n\n        if condition.operator == \"eq\":\n\
          \            return attr_value == condition.values[0]\n        elif condition.operator == \"neq\":\n           \
          \ return attr_value != condition.values[0]\n        elif condition.operator == \"in\":\n            return attr_value\
          \ in condition.values\n        elif condition.operator == \"contains\":\n            return condition.values[0]\
          \ in str(attr_value)\n        elif condition.operator == \"startswith\":\n            return str(attr_value).startswith(condition.values[0])\n\
          \        elif condition.operator == \"gt\":\n            return float(attr_value) > float(condition.values[0])\n\
          \        elif condition.operator == \"lt\":\n            return float(attr_value) < float(condition.values[0])\n\
          \        elif condition.operator == \"regex\":\n            import re\n            return bool(re.match(condition.values[0],\
          \ str(attr_value)))\n\n        return False\n\n    def _evaluate_rollout(self, flag: FeatureFlag, context: EvaluationContext)\
          \ -> Optional[int]:\n        rollout = flag.percentage_rollout\n        bucket_value = context.get(rollout.bucket_by)\n\
          \        if bucket_value is None:\n            return None\n\n        # Hash to get bucket (0-100)\n        hash_input\
          \ = f\"{flag.key}:{bucket_value}\"\n        bucket = int(hashlib.md5(hash_input.encode()).hexdigest(), 16) % 100\n\
          \n        cumulative = 0\n        for variation_idx, percentage in rollout.variations:\n            cumulative +=\
          \ percentage\n            if bucket < cumulative:\n                return variation_idx\n\n        return None\n\
          \n    def _result(self, flag: FeatureFlag, variation_idx: int, reason: str) -> EvaluationResult:\n        return\
          \ EvaluationResult(\n            value=flag.variations[variation_idx].value,\n            variation_index=variation_idx,\n\
          \            reason=reason,\n            flag_key=flag.key\n        )\n```"
      pitfalls:
      - Inconsistent hashing causes users to flip-flop variations
      - Circular prerequisites cause infinite loop
      - Rule priority ties cause non-deterministic evaluation
    - name: Real-time Flag Updates
      description: Implement real-time flag updates using SSE/WebSocket with local caching and fallback.
      hints:
        level1: Poll or stream flag changes from server.
        level2: Cache flags locally, use stale cache if server unavailable.
        level3: "```python\nimport asyncio\nimport aiohttp\nimport json\nfrom dataclasses import dataclass\nfrom datetime\
          \ import datetime\nfrom typing import Callable\n\n@dataclass\nclass FlagCache:\n    flags: dict[str, FeatureFlag]\n\
          \    version: int\n    updated_at: datetime\n\nclass FlagClient:\n    def __init__(self, base_url: str, sdk_key:\
          \ str):\n        self.base_url = base_url\n        self.sdk_key = sdk_key\n        self.cache: FlagCache = None\n\
          \        self.evaluator: FlagEvaluator = None\n        self._listeners: list[Callable] = []\n        self._sse_task\
          \ = None\n        self._running = False\n\n    async def initialize(self):\n        \"\"\"Fetch initial flags and\
          \ start streaming updates.\"\"\"\n        await self._fetch_all_flags()\n        self._running = True\n        self._sse_task\
          \ = asyncio.create_task(self._stream_updates())\n\n    async def close(self):\n        self._running = False\n \
          \       if self._sse_task:\n            self._sse_task.cancel()\n\n    def evaluate(self, flag_key: str, context:\
          \ EvaluationContext,\n                default: Any = None) -> EvaluationResult:\n        if not self.evaluator:\n\
          \            return EvaluationResult(\n                value=default,\n                variation_index=-1,\n   \
          \             reason=\"NOT_INITIALIZED\",\n                flag_key=flag_key\n            )\n        return self.evaluator.evaluate(flag_key,\
          \ context, default)\n\n    def on_change(self, callback: Callable[[str, FeatureFlag], None]):\n        \"\"\"Register\
          \ listener for flag changes.\"\"\"\n        self._listeners.append(callback)\n\n    async def _fetch_all_flags(self):\n\
          \        headers = {\"Authorization\": f\"Bearer {self.sdk_key}\"}\n\n        async with aiohttp.ClientSession()\
          \ as session:\n            async with session.get(f\"{self.base_url}/flags\", headers=headers) as resp:\n      \
          \          if resp.status == 200:\n                    data = await resp.json()\n                    self._update_cache(data)\n\
          \n    async def _stream_updates(self):\n        \"\"\"Stream flag updates using Server-Sent Events.\"\"\"\n    \
          \    headers = {\n            \"Authorization\": f\"Bearer {self.sdk_key}\",\n            \"Accept\": \"text/event-stream\"\
          \n        }\n\n        while self._running:\n            try:\n                async with aiohttp.ClientSession()\
          \ as session:\n                    async with session.get(f\"{self.base_url}/stream\",\n                       \
          \                   headers=headers,\n                                          timeout=None) as resp:\n       \
          \                 async for line in resp.content:\n                            if not self._running:\n         \
          \                       break\n\n                            line = line.decode().strip()\n                    \
          \        if line.startswith(\"data:\"):\n                                event_data = json.loads(line[5:])\n   \
          \                             await self._handle_event(event_data)\n\n            except aiohttp.ClientError:\n\
          \                # Reconnect after delay\n                await asyncio.sleep(5)\n            except asyncio.CancelledError:\n\
          \                break\n\n    async def _handle_event(self, event: dict):\n        event_type = event.get(\"type\"\
          )\n\n        if event_type == \"flag_updated\":\n            flag_data = event.get(\"flag\")\n            flag =\
          \ self._parse_flag(flag_data)\n            self.cache.flags[flag.key] = flag\n            self.cache.version = event.get(\"\
          version\", self.cache.version + 1)\n            self._rebuild_evaluator()\n            self._notify_listeners(flag.key,\
          \ flag)\n\n        elif event_type == \"flag_deleted\":\n            flag_key = event.get(\"key\")\n           \
          \ self.cache.flags.pop(flag_key, None)\n            self._rebuild_evaluator()\n            self._notify_listeners(flag_key,\
          \ None)\n\n        elif event_type == \"full_sync\":\n            self._update_cache(event.get(\"flags\"))\n\n \
          \   def _update_cache(self, data: dict):\n        flags = {}\n        for flag_data in data.get(\"flags\", []):\n\
          \            flag = self._parse_flag(flag_data)\n            flags[flag.key] = flag\n\n        self.cache = FlagCache(\n\
          \            flags=flags,\n            version=data.get(\"version\", 0),\n            updated_at=datetime.utcnow()\n\
          \        )\n        self._rebuild_evaluator()\n\n    def _rebuild_evaluator(self):\n        self.evaluator = FlagEvaluator(self.cache.flags)\n\
          \n    def _notify_listeners(self, flag_key: str, flag: FeatureFlag):\n        for listener in self._listeners:\n\
          \            try:\n                listener(flag_key, flag)\n            except Exception:\n                pass\n\
          \n    def _parse_flag(self, data: dict) -> FeatureFlag:\n        # Convert dict to FeatureFlag object\n        variations\
          \ = [Variation(**v) for v in data.get(\"variations\", [])]\n        rules = [\n            TargetingRule(\n    \
          \            id=r[\"id\"],\n                conditions=[Condition(**c) for c in r.get(\"conditions\", [])],\n  \
          \              variation_index=r[\"variation_index\"],\n                priority=r.get(\"priority\", 0)\n      \
          \      )\n            for r in data.get(\"targeting_rules\", [])\n        ]\n\n        rollout = None\n        if\
          \ \"percentage_rollout\" in data:\n            rollout = PercentageRollout(**data[\"percentage_rollout\"])\n\n \
          \       return FeatureFlag(\n            key=data[\"key\"],\n            type=FlagType(data[\"type\"]),\n      \
          \      variations=variations,\n            default_variation=data[\"default_variation\"],\n            targeting_rules=rules,\n\
          \            percentage_rollout=rollout,\n            enabled=data.get(\"enabled\", True),\n            prerequisites=data.get(\"\
          prerequisites\", [])\n        )\n```"
      pitfalls:
      - SSE reconnection without backoff causes thundering herd
      - Stale cache served indefinitely when stream down
      - Large flag payloads slow down evaluation
    - name: Flag Analytics & Experiments
      description: Track flag evaluations for analytics and support A/B testing with statistical significance.
      hints:
        level1: Log each evaluation with context, variation, and timestamp.
        level2: Calculate conversion rates per variation, check statistical significance.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\n\
          import math\nfrom scipy import stats\n\n@dataclass\nclass EvaluationEvent:\n    flag_key: str\n    variation_index:\
          \ int\n    user_id: str\n    timestamp: datetime\n    context_hash: str  # For deduplication\n\n@dataclass\nclass\
          \ ConversionEvent:\n    experiment_key: str\n    user_id: str\n    metric_name: str\n    metric_value: float\n \
          \   timestamp: datetime\n\n@dataclass\nclass ExperimentResults:\n    control_conversions: int\n    control_total:\
          \ int\n    treatment_conversions: int\n    treatment_total: int\n\n    @property\n    def control_rate(self) ->\
          \ float:\n        return self.control_conversions / self.control_total if self.control_total else 0\n\n    @property\n\
          \    def treatment_rate(self) -> float:\n        return self.treatment_conversions / self.treatment_total if self.treatment_total\
          \ else 0\n\n    @property\n    def relative_lift(self) -> float:\n        if self.control_rate == 0:\n         \
          \   return 0\n        return (self.treatment_rate - self.control_rate) / self.control_rate\n\n    def is_significant(self,\
          \ confidence: float = 0.95) -> bool:\n        \"\"\"Check statistical significance using chi-squared test.\"\"\"\
          \n        observed = [\n            [self.control_conversions, self.control_total - self.control_conversions],\n\
          \            [self.treatment_conversions, self.treatment_total - self.treatment_conversions]\n        ]\n      \
          \  chi2, p_value, _, _ = stats.chi2_contingency(observed)\n        return p_value < (1 - confidence)\n\n    def\
          \ confidence_interval(self, confidence: float = 0.95) -> tuple[float, float]:\n        \"\"\"Calculate confidence\
          \ interval for lift.\"\"\"\n        # Use normal approximation\n        z = stats.norm.ppf(1 - (1 - confidence)\
          \ / 2)\n\n        se_control = math.sqrt(self.control_rate * (1 - self.control_rate) / self.control_total) if self.control_total\
          \ else 0\n        se_treatment = math.sqrt(self.treatment_rate * (1 - self.treatment_rate) / self.treatment_total)\
          \ if self.treatment_total else 0\n\n        se_diff = math.sqrt(se_control**2 + se_treatment**2)\n        diff =\
          \ self.treatment_rate - self.control_rate\n\n        return (diff - z * se_diff, diff + z * se_diff)\n\nclass ExperimentAnalyzer:\n\
          \    def __init__(self, storage):\n        self.storage = storage\n\n    async def get_results(self, experiment_key:\
          \ str, metric_name: str,\n                         start_time: datetime, end_time: datetime) -> ExperimentResults:\n\
          \        # Get users in each variation\n        control_users = await self.storage.get_users_in_variation(\n   \
          \         experiment_key, 0, start_time, end_time\n        )\n        treatment_users = await self.storage.get_users_in_variation(\n\
          \            experiment_key, 1, start_time, end_time\n        )\n\n        # Get conversions\n        control_conversions\
          \ = await self.storage.count_conversions(\n            experiment_key, metric_name, control_users, start_time, end_time\n\
          \        )\n        treatment_conversions = await self.storage.count_conversions(\n            experiment_key, metric_name,\
          \ treatment_users, start_time, end_time\n        )\n\n        return ExperimentResults(\n            control_conversions=control_conversions,\n\
          \            control_total=len(control_users),\n            treatment_conversions=treatment_conversions,\n     \
          \       treatment_total=len(treatment_users)\n        )\n\n    def calculate_sample_size(self, baseline_rate: float,\
          \ min_detectable_effect: float,\n                              power: float = 0.8, significance: float = 0.05) ->\
          \ int:\n        \"\"\"Calculate required sample size per variation.\"\"\"\n        alpha = significance\n      \
          \  beta = 1 - power\n\n        # Standard formula for two-proportion z-test\n        p1 = baseline_rate\n      \
          \  p2 = baseline_rate * (1 + min_detectable_effect)\n        p_pooled = (p1 + p2) / 2\n\n        z_alpha = stats.norm.ppf(1\
          \ - alpha / 2)\n        z_beta = stats.norm.ppf(power)\n\n        n = (\n            (z_alpha * math.sqrt(2 * p_pooled\
          \ * (1 - p_pooled)) +\n             z_beta * math.sqrt(p1 * (1 - p1) + p2 * (1 - p2)))\n            / (p2 - p1)\n\
          \        ) ** 2\n\n        return int(math.ceil(n))\n\nclass FlagAnalytics:\n    def __init__(self, storage):\n\
          \        self.storage = storage\n        self._buffer: list[EvaluationEvent] = []\n        self._buffer_size = 100\n\
          \        self._lock = asyncio.Lock()\n\n    async def track_evaluation(self, result: EvaluationResult, context:\
          \ EvaluationContext):\n        event = EvaluationEvent(\n            flag_key=result.flag_key,\n            variation_index=result.variation_index,\n\
          \            user_id=context.user_id,\n            timestamp=datetime.utcnow(),\n            context_hash=self._hash_context(context)\n\
          \        )\n\n        async with self._lock:\n            self._buffer.append(event)\n            if len(self._buffer)\
          \ >= self._buffer_size:\n                await self._flush()\n\n    async def track_conversion(self, experiment_key:\
          \ str, user_id: str,\n                              metric_name: str, value: float = 1.0):\n        event = ConversionEvent(\n\
          \            experiment_key=experiment_key,\n            user_id=user_id,\n            metric_name=metric_name,\n\
          \            metric_value=value,\n            timestamp=datetime.utcnow()\n        )\n        await self.storage.store_conversion(event)\n\
          \n    async def _flush(self):\n        events = self._buffer\n        self._buffer = []\n        await self.storage.store_evaluations(events)\n\
          \n    def _hash_context(self, context: EvaluationContext) -> str:\n        import hashlib\n        import json\n\
          \        data = json.dumps({\"user_id\": context.user_id, **context.attributes}, sort_keys=True)\n        return\
          \ hashlib.md5(data.encode()).hexdigest()\n```"
      pitfalls:
      - Peeking at results early inflates false positive rate
      - Sample ratio mismatch invalidates experiment
      - Novelty effect skews early results
  job-scheduler:
    name: Distributed Job Scheduler
    description: Build a distributed job scheduler with cron expressions, retries, priorities, and cluster coordination.
    why_important: Background job processing is essential for async workloads. Understanding schedulers helps design reliable
      batch processing systems.
    difficulty: advanced
    tags:
    - distributed-systems
    - backend
    - reliability
    estimated_hours: 45
    prerequisites:
    - redis-clone
    learning_outcomes:
    - Parse and evaluate cron expressions
    - Implement distributed locking for job claims
    - Design retry strategies with backoff
    - Handle worker failures and job recovery
    milestones:
    - name: Cron Expression Parser
      description: Parse cron expressions and calculate next execution times.
      hints:
        level1: 'Parse five fields: minute, hour, day, month, weekday.'
        level2: 'Handle special characters: *, /, -, , and named values.'
        level3: "```python\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import\
          \ Optional, Set\nimport re\n\n@dataclass\nclass CronField:\n    values: Set[int]\n    min_val: int\n    max_val:\
          \ int\n\n    @classmethod\n    def parse(cls, expr: str, min_val: int, max_val: int, names: dict = None) -> 'CronField':\n\
          \        values = set()\n\n        # Handle names (JAN, SUN, etc.)\n        if names:\n            for name, val\
          \ in names.items():\n                expr = expr.upper().replace(name, str(val))\n\n        for part in expr.split(','):\n\
          \            if part == '*':\n                values.update(range(min_val, max_val + 1))\n            elif '/' in\
          \ part:\n                # Step: */5 or 1-10/2\n                range_part, step = part.split('/')\n           \
          \     step = int(step)\n                if range_part == '*':\n                    start, end = min_val, max_val\n\
          \                elif '-' in range_part:\n                    start, end = map(int, range_part.split('-'))\n   \
          \             else:\n                    start = int(range_part)\n                    end = max_val\n          \
          \      values.update(range(start, end + 1, step))\n            elif '-' in part:\n                # Range: 1-5\n\
          \                start, end = map(int, part.split('-'))\n                values.update(range(start, end + 1))\n\
          \            else:\n                values.add(int(part))\n\n        return cls(values=values, min_val=min_val,\
          \ max_val=max_val)\n\nclass CronExpression:\n    MONTH_NAMES = {\n        'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4,\
          \ 'MAY': 5, 'JUN': 6,\n        'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12\n    }\n    DOW_NAMES\
          \ = {\n        'SUN': 0, 'MON': 1, 'TUE': 2, 'WED': 3, 'THU': 4, 'FRI': 5, 'SAT': 6\n    }\n\n    def __init__(self,\
          \ expression: str):\n        self.expression = expression\n        parts = expression.split()\n        if len(parts)\
          \ != 5:\n            raise ValueError(f\"Invalid cron expression: {expression}\")\n\n        self.minute = CronField.parse(parts[0],\
          \ 0, 59)\n        self.hour = CronField.parse(parts[1], 0, 23)\n        self.day = CronField.parse(parts[2], 1,\
          \ 31)\n        self.month = CronField.parse(parts[3], 1, 12, self.MONTH_NAMES)\n        self.dow = CronField.parse(parts[4],\
          \ 0, 6, self.DOW_NAMES)\n\n    def next_run(self, after: datetime = None) -> datetime:\n        \"\"\"Calculate\
          \ next execution time after given datetime.\"\"\"\n        after = after or datetime.now()\n        # Start from\
          \ next minute\n        current = after.replace(second=0, microsecond=0) + timedelta(minutes=1)\n\n        # Limit\
          \ search to prevent infinite loops\n        for _ in range(366 * 24 * 60):  # Max 1 year of minutes\n          \
          \  if self._matches(current):\n                return current\n            current += timedelta(minutes=1)\n\n \
          \       raise ValueError(\"No valid execution time found within 1 year\")\n\n    def _matches(self, dt: datetime)\
          \ -> bool:\n        if dt.minute not in self.minute.values:\n            return False\n        if dt.hour not in\
          \ self.hour.values:\n            return False\n        if dt.month not in self.month.values:\n            return\
          \ False\n\n        # Day and DOW have OR relationship\n        day_match = dt.day in self.day.values\n        dow_match\
          \ = dt.weekday() in self.dow.values\n\n        # If both are restricted (not *), either can match\n        day_restricted\
          \ = self.day.values != set(range(1, 32))\n        dow_restricted = self.dow.values != set(range(0, 7))\n\n     \
          \   if day_restricted and dow_restricted:\n            return day_match or dow_match\n        elif day_restricted:\n\
          \            return day_match\n        elif dow_restricted:\n            return dow_match\n        else:\n     \
          \       return True\n\n    def matches(self, dt: datetime) -> bool:\n        \"\"\"Check if datetime matches this\
          \ cron expression.\"\"\"\n        return self._matches(dt)\n\n    def next_n_runs(self, n: int, after: datetime\
          \ = None) -> list[datetime]:\n        \"\"\"Get next n execution times.\"\"\"\n        runs = []\n        current\
          \ = after or datetime.now()\n        for _ in range(n):\n            next_time = self.next_run(current)\n      \
          \      runs.append(next_time)\n            current = next_time\n        return runs\n\n# Common presets\nCRON_PRESETS\
          \ = {\n    '@yearly': '0 0 1 1 *',\n    '@monthly': '0 0 1 * *',\n    '@weekly': '0 0 * * 0',\n    '@daily': '0\
          \ 0 * * *',\n    '@hourly': '0 * * * *',\n}\n```"
      pitfalls:
      - Daylight saving time causes missed or duplicate runs
      - Day-of-month 31 skips months with fewer days
      - Infinite loop if no valid date exists
    - name: Job Queue with Priorities
      description: Implement a job queue with priorities, delayed execution, and deduplication.
      hints:
        level1: Use sorted set for priority queue with score = priority + timestamp.
        level2: Support delayed jobs by storing with future timestamp.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom enum\
          \ import Enum\nfrom typing import Optional, Any\nimport uuid\nimport json\nimport hashlib\n\nclass JobStatus(str,\
          \ Enum):\n    PENDING = \"pending\"\n    SCHEDULED = \"scheduled\"\n    RUNNING = \"running\"\n    COMPLETED = \"\
          completed\"\n    FAILED = \"failed\"\n    DEAD = \"dead\"  # Max retries exceeded\n\n@dataclass\nclass Job:\n  \
          \  id: str\n    queue: str\n    type: str\n    payload: dict\n    priority: int = 0  # Higher = more important\n\
          \    status: JobStatus = JobStatus.PENDING\n    created_at: datetime = field(default_factory=datetime.utcnow)\n\
          \    scheduled_at: Optional[datetime] = None\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime]\
          \ = None\n    attempts: int = 0\n    max_retries: int = 3\n    error: Optional[str] = None\n    result: Any = None\n\
          \    unique_key: Optional[str] = None  # For deduplication\n\n    @classmethod\n    def create(cls, queue: str,\
          \ job_type: str, payload: dict,\n               priority: int = 0, delay: timedelta = None,\n               unique_key:\
          \ str = None, max_retries: int = 3) -> 'Job':\n        job = cls(\n            id=str(uuid.uuid4()),\n         \
          \   queue=queue,\n            type=job_type,\n            payload=payload,\n            priority=priority,\n   \
          \         max_retries=max_retries,\n            unique_key=unique_key\n        )\n        if delay:\n          \
          \  job.scheduled_at = datetime.utcnow() + delay\n            job.status = JobStatus.SCHEDULED\n        return job\n\
          \n    def score(self) -> float:\n        \"\"\"Calculate priority score for sorted set.\"\"\"\n        # Lower score\
          \ = higher priority (processed first)\n        # Base: scheduled time (or created time)\n        base_time = (self.scheduled_at\
          \ or self.created_at).timestamp()\n        # Subtract priority to make higher priority jobs come first\n       \
          \ return base_time - (self.priority * 1000)\n\nclass JobQueue:\n    def __init__(self, redis):\n        self.redis\
          \ = redis\n\n    async def enqueue(self, job: Job) -> bool:\n        \"\"\"Add job to queue. Returns False if duplicate.\"\
          \"\"\n        # Check for duplicate\n        if job.unique_key:\n            exists = await self.redis.exists(f\"\
          job:unique:{job.unique_key}\")\n            if exists:\n                return False\n            # Set unique key\
          \ with TTL\n            await self.redis.setex(\n                f\"job:unique:{job.unique_key}\",\n           \
          \     86400,  # 24 hours\n                job.id\n            )\n\n        # Store job data\n        await self.redis.hset(f\"\
          job:{job.id}\", mapping={\n            \"data\": json.dumps(self._serialize_job(job))\n        })\n\n        # Add\
          \ to queue\n        if job.status == JobStatus.SCHEDULED:\n            await self.redis.zadd(f\"queue:{job.queue}:scheduled\"\
          , {job.id: job.score()})\n        else:\n            await self.redis.zadd(f\"queue:{job.queue}:pending\", {job.id:\
          \ job.score()})\n\n        return True\n\n    async def dequeue(self, queue: str, worker_id: str,\n            \
          \         visibility_timeout: int = 300) -> Optional[Job]:\n        \"\"\"Fetch next job from queue.\"\"\"\n   \
          \     # First, move any scheduled jobs that are due\n        await self._promote_scheduled(queue)\n\n        # Atomically\
          \ pop job and add to processing set\n        job_id = await self._atomic_claim(queue, worker_id, visibility_timeout)\n\
          \        if not job_id:\n            return None\n\n        job = await self._get_job(job_id)\n        if job:\n\
          \            job.status = JobStatus.RUNNING\n            job.started_at = datetime.utcnow()\n            job.attempts\
          \ += 1\n            await self._update_job(job)\n\n        return job\n\n    async def complete(self, job: Job,\
          \ result: Any = None):\n        \"\"\"Mark job as completed.\"\"\"\n        job.status = JobStatus.COMPLETED\n \
          \       job.completed_at = datetime.utcnow()\n        job.result = result\n\n        await self._update_job(job)\n\
          \        await self.redis.zrem(f\"queue:{job.queue}:processing\", job.id)\n\n        # Clean up unique key\n   \
          \     if job.unique_key:\n            await self.redis.delete(f\"job:unique:{job.unique_key}\")\n\n    async def\
          \ fail(self, job: Job, error: str):\n        \"\"\"Mark job as failed, possibly retry.\"\"\"\n        job.error\
          \ = error\n\n        if job.attempts < job.max_retries:\n            # Retry with exponential backoff\n        \
          \    delay = timedelta(seconds=2 ** job.attempts * 60)\n            job.scheduled_at = datetime.utcnow() + delay\n\
          \            job.status = JobStatus.SCHEDULED\n\n            await self._update_job(job)\n            await self.redis.zrem(f\"\
          queue:{job.queue}:processing\", job.id)\n            await self.redis.zadd(f\"queue:{job.queue}:scheduled\", {job.id:\
          \ job.score()})\n        else:\n            job.status = JobStatus.DEAD\n            job.completed_at = datetime.utcnow()\n\
          \            await self._update_job(job)\n            await self.redis.zrem(f\"queue:{job.queue}:processing\", job.id)\n\
          \            await self.redis.zadd(f\"queue:{job.queue}:dead\", {job.id: job.score()})\n\n    async def _promote_scheduled(self,\
          \ queue: str):\n        \"\"\"Move scheduled jobs that are due to pending.\"\"\"\n        now = datetime.utcnow().timestamp()\n\
          \        due_jobs = await self.redis.zrangebyscore(\n            f\"queue:{queue}:scheduled\", 0, now, limit=100\n\
          \        )\n\n        for job_id in due_jobs:\n            job = await self._get_job(job_id)\n            if job:\n\
          \                job.status = JobStatus.PENDING\n                await self._update_job(job)\n                await\
          \ self.redis.zrem(f\"queue:{queue}:scheduled\", job_id)\n                await self.redis.zadd(f\"queue:{queue}:pending\"\
          , {job_id: job.score()})\n\n    async def _atomic_claim(self, queue: str, worker_id: str,\n                    \
          \       timeout: int) -> Optional[str]:\n        # Use Lua script for atomic claim\n        script = \"\"\"\n  \
          \      local job_id = redis.call('ZRANGE', KEYS[1], 0, 0)[1]\n        if job_id then\n            redis.call('ZREM',\
          \ KEYS[1], job_id)\n            redis.call('ZADD', KEYS[2], ARGV[1], job_id)\n            redis.call('HSET', 'job:'\
          \ .. job_id, 'worker', ARGV[2])\n            return job_id\n        end\n        return nil\n        \"\"\"\n  \
          \      deadline = datetime.utcnow().timestamp() + timeout\n        return await self.redis.eval(\n            script,\n\
          \            2,\n            f\"queue:{queue}:pending\",\n            f\"queue:{queue}:processing\",\n         \
          \   deadline,\n            worker_id\n        )\n```"
      pitfalls:
      - Race condition between claim and processing
      - Visibility timeout too short causes duplicate processing
      - Deduplication key collision across different job types
    - name: Worker Coordination
      description: Coordinate multiple workers with leader election, heartbeats, and job recovery.
      hints:
        level1: Workers send heartbeats, leader monitors for failures.
        level2: Recover jobs from dead workers by checking processing timeout.
        level3: "```python\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\n\
          from typing import Callable, Optional\nimport uuid\n\n@dataclass\nclass Worker:\n    id: str\n    queues: list[str]\n\
          \    last_heartbeat: datetime = field(default_factory=datetime.utcnow)\n    current_job: Optional[str] = None\n\
          \    processed: int = 0\n    failed: int = 0\n\nclass WorkerCoordinator:\n    def __init__(self, redis, job_queue:\
          \ JobQueue):\n        self.redis = redis\n        self.job_queue = job_queue\n        self.heartbeat_interval =\
          \ 30\n        self.worker_timeout = 90\n        self._workers: dict[str, Worker] = {}\n        self._running = False\n\
          \        self._is_leader = False\n\n    async def start_worker(self, queues: list[str], handlers: dict[str, Callable]):\n\
          \        \"\"\"Start a worker process.\"\"\"\n        worker = Worker(\n            id=str(uuid.uuid4()),\n    \
          \        queues=queues\n        )\n        self._workers[worker.id] = worker\n\n        # Start background tasks\n\
          \        self._running = True\n        heartbeat_task = asyncio.create_task(self._heartbeat_loop(worker))\n    \
          \    leader_task = asyncio.create_task(self._leader_election())\n        process_task = asyncio.create_task(self._process_loop(worker,\
          \ handlers))\n\n        await asyncio.gather(heartbeat_task, leader_task, process_task)\n\n    async def _heartbeat_loop(self,\
          \ worker: Worker):\n        while self._running:\n            await self.redis.hset(\n                f\"worker:{worker.id}\"\
          ,\n                mapping={\n                    \"last_heartbeat\": datetime.utcnow().isoformat(),\n         \
          \           \"current_job\": worker.current_job or \"\",\n                    \"processed\": worker.processed,\n\
          \                    \"failed\": worker.failed\n                }\n            )\n            await self.redis.expire(f\"\
          worker:{worker.id}\", self.worker_timeout * 2)\n            await asyncio.sleep(self.heartbeat_interval)\n\n   \
          \ async def _leader_election(self):\n        \"\"\"Try to become leader for cleanup tasks.\"\"\"\n        leader_key\
          \ = \"scheduler:leader\"\n        while self._running:\n            # Try to acquire leadership\n            acquired\
          \ = await self.redis.set(\n                leader_key,\n                self._workers[list(self._workers.keys())[0]].id,\n\
          \                nx=True,\n                ex=self.heartbeat_interval * 2\n            )\n\n            if acquired:\n\
          \                self._is_leader = True\n                await self._leader_tasks()\n            else:\n       \
          \         self._is_leader = False\n\n            await asyncio.sleep(self.heartbeat_interval)\n\n    async def _leader_tasks(self):\n\
          \        \"\"\"Tasks only leader performs.\"\"\"\n        # Check for dead workers\n        await self._recover_dead_worker_jobs()\n\
          \        # Clean up old completed jobs\n        await self._cleanup_old_jobs()\n\n    async def _recover_dead_worker_jobs(self):\n\
          \        \"\"\"Recover jobs from workers that haven't sent heartbeat.\"\"\"\n        # Get all workers\n       \
          \ worker_keys = await self.redis.keys(\"worker:*\")\n\n        for key in worker_keys:\n            worker_data\
          \ = await self.redis.hgetall(key)\n            if not worker_data:\n                continue\n\n            last_heartbeat\
          \ = datetime.fromisoformat(worker_data.get(\"last_heartbeat\", \"\"))\n            if datetime.utcnow() - last_heartbeat\
          \ > timedelta(seconds=self.worker_timeout):\n                # Worker is dead, recover its job\n               \
          \ current_job = worker_data.get(\"current_job\")\n                if current_job:\n                    await self._requeue_job(current_job)\n\
          \n                await self.redis.delete(key)\n\n    async def _requeue_job(self, job_id: str):\n        \"\"\"\
          Put job back in pending queue.\"\"\"\n        job = await self.job_queue._get_job(job_id)\n        if job and job.status\
          \ == JobStatus.RUNNING:\n            job.status = JobStatus.PENDING\n            job.started_at = None\n       \
          \     await self.job_queue._update_job(job)\n\n            # Move from processing to pending\n            for queue\
          \ in job.queue:\n                await self.redis.zrem(f\"queue:{queue}:processing\", job_id)\n                await\
          \ self.redis.zadd(f\"queue:{queue}:pending\", {job_id: job.score()})\n\n    async def _process_loop(self, worker:\
          \ Worker, handlers: dict[str, Callable]):\n        \"\"\"Main processing loop.\"\"\"\n        while self._running:\n\
          \            for queue in worker.queues:\n                job = await self.job_queue.dequeue(queue, worker.id)\n\
          \n                if job:\n                    worker.current_job = job.id\n                    try:\n         \
          \               handler = handlers.get(job.type)\n                        if handler:\n                        \
          \    result = await handler(job.payload)\n                            await self.job_queue.complete(job, result)\n\
          \                            worker.processed += 1\n                        else:\n                            await\
          \ self.job_queue.fail(job, f\"No handler for job type: {job.type}\")\n                            worker.failed\
          \ += 1\n                    except Exception as e:\n                        await self.job_queue.fail(job, str(e))\n\
          \                        worker.failed += 1\n                    finally:\n                        worker.current_job\
          \ = None\n\n            await asyncio.sleep(1)  # Polling interval\n\n    async def _cleanup_old_jobs(self, days:\
          \ int = 7):\n        \"\"\"Remove old completed/dead jobs.\"\"\"\n        cutoff = (datetime.utcnow() - timedelta(days=days)).timestamp()\n\
          \        # Clean up each queue's completed/dead sets\n        queues = await self.redis.smembers(\"queues\")\n \
          \       for queue in queues:\n            await self.redis.zremrangebyscore(f\"queue:{queue}:completed\", 0, cutoff)\n\
          \            await self.redis.zremrangebyscore(f\"queue:{queue}:dead\", 0, cutoff)\n```"
      pitfalls:
      - Leader election split-brain with network partition
      - Job recovered while still processing causes duplicate
      - Worker heartbeat failure during long job
  audit-logging:
    name: Audit Logging System
    description: Build an immutable audit log system for compliance with tamper detection, retention policies, and efficient
      querying.
    why_important: Audit logs are required for compliance (SOC2, HIPAA, GDPR). Understanding audit systems helps design secure,
      compliant applications.
    difficulty: intermediate
    tags:
    - security
    - compliance
    - backend
    estimated_hours: 30
    prerequisites:
    - log-aggregator
    learning_outcomes:
    - Design immutable append-only log storage
    - Implement hash chain for tamper detection
    - Build efficient audit log querying
    - Handle retention and archival policies
    milestones:
    - name: Audit Event Model
      description: Design audit event schema with actor, action, resource, and context information.
      hints:
        level1: Capture who did what to which resource and when.
        level2: 'Include request context: IP, user agent, session ID.'
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\n\
          from typing import Optional, Any\nimport uuid\nimport json\n\nclass AuditAction(str, Enum):\n    CREATE = \"create\"\
          \n    READ = \"read\"\n    UPDATE = \"update\"\n    DELETE = \"delete\"\n    LOGIN = \"login\"\n    LOGOUT = \"\
          logout\"\n    PERMISSION_CHANGE = \"permission_change\"\n    EXPORT = \"export\"\n    IMPORT = \"import\"\n\nclass\
          \ AuditOutcome(str, Enum):\n    SUCCESS = \"success\"\n    FAILURE = \"failure\"\n    DENIED = \"denied\"\n\n@dataclass\n\
          class Actor:\n    id: str\n    type: str  # user, service, system\n    name: str\n    email: Optional[str] = None\n\
          \    roles: list[str] = field(default_factory=list)\n\n@dataclass\nclass Resource:\n    id: str\n    type: str \
          \ # document, user, setting, etc.\n    name: Optional[str] = None\n    attributes: dict = field(default_factory=dict)\n\
          \n@dataclass\nclass RequestContext:\n    ip_address: str\n    user_agent: str\n    session_id: Optional[str] = None\n\
          \    request_id: str = \"\"\n    correlation_id: Optional[str] = None\n    geo_location: Optional[dict] = None\n\
          \n@dataclass\nclass AuditEvent:\n    id: str\n    timestamp: datetime\n    actor: Actor\n    action: AuditAction\n\
          \    resource: Resource\n    outcome: AuditOutcome\n    context: RequestContext\n    changes: Optional[dict] = None\
          \  # Before/after for updates\n    metadata: dict = field(default_factory=dict)\n    sequence: int = 0  # For ordering\n\
          \    hash: str = \"\"  # Chain hash\n\n    @classmethod\n    def create(cls, actor: Actor, action: AuditAction,\
          \ resource: Resource,\n               outcome: AuditOutcome, context: RequestContext,\n               changes: dict\
          \ = None, metadata: dict = None) -> 'AuditEvent':\n        return cls(\n            id=str(uuid.uuid4()),\n    \
          \        timestamp=datetime.utcnow(),\n            actor=actor,\n            action=action,\n            resource=resource,\n\
          \            outcome=outcome,\n            context=context,\n            changes=changes,\n            metadata=metadata\
          \ or {}\n        )\n\n    def to_dict(self) -> dict:\n        return {\n            \"id\": self.id,\n         \
          \   \"timestamp\": self.timestamp.isoformat(),\n            \"actor\": {\n                \"id\": self.actor.id,\n\
          \                \"type\": self.actor.type,\n                \"name\": self.actor.name,\n                \"email\"\
          : self.actor.email,\n                \"roles\": self.actor.roles\n            },\n            \"action\": self.action.value,\n\
          \            \"resource\": {\n                \"id\": self.resource.id,\n                \"type\": self.resource.type,\n\
          \                \"name\": self.resource.name,\n                \"attributes\": self.resource.attributes\n     \
          \       },\n            \"outcome\": self.outcome.value,\n            \"context\": {\n                \"ip_address\"\
          : self.context.ip_address,\n                \"user_agent\": self.context.user_agent,\n                \"session_id\"\
          : self.context.session_id,\n                \"request_id\": self.context.request_id\n            },\n          \
          \  \"changes\": self.changes,\n            \"metadata\": self.metadata,\n            \"sequence\": self.sequence,\n\
          \            \"hash\": self.hash\n        }\n\n    def compute_hash(self, previous_hash: str = \"\") -> str:\n \
          \       import hashlib\n        data = json.dumps({\n            \"id\": self.id,\n            \"timestamp\": self.timestamp.isoformat(),\n\
          \            \"actor_id\": self.actor.id,\n            \"action\": self.action.value,\n            \"resource_id\"\
          : self.resource.id,\n            \"outcome\": self.outcome.value,\n            \"previous_hash\": previous_hash\n\
          \        }, sort_keys=True)\n        return hashlib.sha256(data.encode()).hexdigest()\n\nclass AuditEventBuilder:\n\
          \    \"\"\"Fluent builder for audit events.\"\"\"\n\n    def __init__(self):\n        self._actor = None\n     \
          \   self._action = None\n        self._resource = None\n        self._outcome = AuditOutcome.SUCCESS\n        self._context\
          \ = None\n        self._changes = None\n        self._metadata = {}\n\n    def actor(self, id: str, type: str, name:\
          \ str, **kwargs) -> 'AuditEventBuilder':\n        self._actor = Actor(id=id, type=type, name=name, **kwargs)\n \
          \       return self\n\n    def action(self, action: AuditAction) -> 'AuditEventBuilder':\n        self._action =\
          \ action\n        return self\n\n    def resource(self, id: str, type: str, name: str = None, **kwargs) -> 'AuditEventBuilder':\n\
          \        self._resource = Resource(id=id, type=type, name=name, **kwargs)\n        return self\n\n    def outcome(self,\
          \ outcome: AuditOutcome) -> 'AuditEventBuilder':\n        self._outcome = outcome\n        return self\n\n    def\
          \ context(self, ip: str, user_agent: str, **kwargs) -> 'AuditEventBuilder':\n        self._context = RequestContext(ip_address=ip,\
          \ user_agent=user_agent, **kwargs)\n        return self\n\n    def changes(self, before: dict, after: dict) -> 'AuditEventBuilder':\n\
          \        self._changes = {\"before\": before, \"after\": after}\n        return self\n\n    def meta(self, **kwargs)\
          \ -> 'AuditEventBuilder':\n        self._metadata.update(kwargs)\n        return self\n\n    def build(self) ->\
          \ AuditEvent:\n        return AuditEvent.create(\n            actor=self._actor,\n            action=self._action,\n\
          \            resource=self._resource,\n            outcome=self._outcome,\n            context=self._context,\n\
          \            changes=self._changes,\n            metadata=self._metadata\n        )\n```"
      pitfalls:
      - PII in audit logs violates GDPR right to erasure
      - Missing actor context makes logs useless for investigation
      - Async logging loses context in distributed systems
    - name: Immutable Storage with Hash Chain
      description: Implement append-only storage with hash chain linking for tamper detection.
      hints:
        level1: Each event hash includes hash of previous event.
        level2: Store chain anchors periodically for verification.
        level3: "```python\nimport asyncio\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing\
          \ import Optional\nimport hashlib\n\n@dataclass\nclass ChainAnchor:\n    sequence: int\n    hash: str\n    timestamp:\
          \ datetime\n    event_count: int\n\nclass ImmutableAuditStore:\n    def __init__(self, storage, anchor_interval:\
          \ int = 1000):\n        self.storage = storage\n        self.anchor_interval = anchor_interval\n        self._last_hash\
          \ = \"\"\n        self._sequence = 0\n        self._lock = asyncio.Lock()\n\n    async def initialize(self):\n \
          \       \"\"\"Load last hash and sequence from storage.\"\"\"\n        last_event = await self.storage.get_last_event()\n\
          \        if last_event:\n            self._last_hash = last_event.hash\n            self._sequence = last_event.sequence\n\
          \n    async def append(self, event: AuditEvent) -> AuditEvent:\n        async with self._lock:\n            self._sequence\
          \ += 1\n            event.sequence = self._sequence\n            event.hash = event.compute_hash(self._last_hash)\n\
          \n            # Append to storage (immutable)\n            await self.storage.append(event)\n\n            self._last_hash\
          \ = event.hash\n\n            # Create anchor periodically\n            if self._sequence % self.anchor_interval\
          \ == 0:\n                await self._create_anchor()\n\n            return event\n\n    async def _create_anchor(self):\n\
          \        anchor = ChainAnchor(\n            sequence=self._sequence,\n            hash=self._last_hash,\n      \
          \      timestamp=datetime.utcnow(),\n            event_count=self._sequence\n        )\n        await self.storage.store_anchor(anchor)\n\
          \n    async def verify_chain(self, start_seq: int = 1, end_seq: int = None) -> bool:\n        \"\"\"Verify hash\
          \ chain integrity.\"\"\"\n        end_seq = end_seq or self._sequence\n\n        events = await self.storage.get_range(start_seq,\
          \ end_seq)\n\n        previous_hash = \"\"\n        if start_seq > 1:\n            prev_event = await self.storage.get_event(start_seq\
          \ - 1)\n            previous_hash = prev_event.hash\n\n        for event in events:\n            expected_hash =\
          \ event.compute_hash(previous_hash)\n            if event.hash != expected_hash:\n                return False\n\
          \            previous_hash = event.hash\n\n        return True\n\n    async def verify_from_anchor(self, anchor:\
          \ ChainAnchor) -> bool:\n        \"\"\"Verify chain from anchor point.\"\"\"\n        # Get events since anchor\n\
          \        events = await self.storage.get_range(anchor.sequence, self._sequence)\n\n        if not events:\n    \
          \        return anchor.hash == self._last_hash\n\n        # First event should chain from anchor\n        first_event\
          \ = events[0]\n        if first_event.sequence == anchor.sequence:\n            if first_event.hash != anchor.hash:\n\
          \                return False\n            events = events[1:]\n\n        return await self.verify_chain(anchor.sequence\
          \ + 1, self._sequence)\n\n    async def get_proof(self, event_id: str) -> dict:\n        \"\"\"Generate proof of\
          \ inclusion for an event.\"\"\"\n        event = await self.storage.get_event_by_id(event_id)\n        if not event:\n\
          \            return None\n\n        # Find nearest anchor before event\n        anchor = await self.storage.get_nearest_anchor(event.sequence)\n\
          \n        # Get chain from anchor to event\n        chain = await self.storage.get_range(\n            anchor.sequence\
          \ if anchor else 1,\n            event.sequence\n        )\n\n        return {\n            \"event\": event.to_dict(),\n\
          \            \"anchor\": anchor,\n            \"chain_hashes\": [e.hash for e in chain],\n            \"verified\"\
          : await self.verify_chain(\n                anchor.sequence if anchor else 1,\n                event.sequence\n\
          \            )\n        }\n\nclass PostgresAuditStorage:\n    \"\"\"PostgreSQL storage with append-only table.\"\
          \"\"\n\n    def __init__(self, pool):\n        self.pool = pool\n\n    async def append(self, event: AuditEvent):\n\
          \        async with self.pool.acquire() as conn:\n            await conn.execute(\"\"\"\n                INSERT\
          \ INTO audit_log (\n                    id, sequence, timestamp, actor_id, actor_type, actor_name,\n           \
          \         action, resource_id, resource_type, outcome,\n                    context, changes, metadata, hash\n \
          \               ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)\n            \"\"\",\n  \
          \              event.id, event.sequence, event.timestamp,\n                event.actor.id, event.actor.type, event.actor.name,\n\
          \                event.action.value, event.resource.id, event.resource.type,\n                event.outcome.value,\
          \ json.dumps(event.context.__dict__),\n                json.dumps(event.changes), json.dumps(event.metadata), event.hash\n\
          \            )\n\n    async def get_range(self, start: int, end: int) -> list[AuditEvent]:\n        async with self.pool.acquire()\
          \ as conn:\n            rows = await conn.fetch(\"\"\"\n                SELECT * FROM audit_log\n              \
          \  WHERE sequence >= $1 AND sequence <= $2\n                ORDER BY sequence\n            \"\"\", start, end)\n\
          \            return [self._row_to_event(row) for row in rows]\n```"
      pitfalls:
      - Hash chain breaks if events inserted out of order
      - Chain verification expensive on large datasets
      - Backup/restore must preserve hash chain integrity
    - name: Audit Query & Export
      description: Build efficient querying for audit logs with filtering, search, and compliance export formats.
      hints:
        level1: Index on actor, resource, action, and timestamp.
        level2: Support date range, actor, and resource filtering.
        level3: "```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Optional\n\
          from enum import Enum\n\nclass ExportFormat(str, Enum):\n    JSON = \"json\"\n    CSV = \"csv\"\n    SIEM = \"siem\"\
          \  # CEF/LEEF format\n\n@dataclass\nclass AuditQuery:\n    start_time: Optional[datetime] = None\n    end_time:\
          \ Optional[datetime] = None\n    actor_ids: list[str] = None\n    actor_types: list[str] = None\n    actions: list[AuditAction]\
          \ = None\n    resource_ids: list[str] = None\n    resource_types: list[str] = None\n    outcomes: list[AuditOutcome]\
          \ = None\n    ip_addresses: list[str] = None\n    search_text: Optional[str] = None\n    limit: int = 100\n    offset:\
          \ int = 0\n\nclass AuditQueryEngine:\n    def __init__(self, storage):\n        self.storage = storage\n\n    async\
          \ def query(self, q: AuditQuery) -> list[AuditEvent]:\n        \"\"\"Execute audit query.\"\"\"\n        conditions\
          \ = []\n        params = []\n        param_idx = 1\n\n        if q.start_time:\n            conditions.append(f\"\
          timestamp >= ${param_idx}\")\n            params.append(q.start_time)\n            param_idx += 1\n\n        if\
          \ q.end_time:\n            conditions.append(f\"timestamp <= ${param_idx}\")\n            params.append(q.end_time)\n\
          \            param_idx += 1\n\n        if q.actor_ids:\n            conditions.append(f\"actor_id = ANY(${param_idx})\"\
          )\n            params.append(q.actor_ids)\n            param_idx += 1\n\n        if q.actions:\n            conditions.append(f\"\
          action = ANY(${param_idx})\")\n            params.append([a.value for a in q.actions])\n            param_idx +=\
          \ 1\n\n        if q.resource_types:\n            conditions.append(f\"resource_type = ANY(${param_idx})\")\n   \
          \         params.append(q.resource_types)\n            param_idx += 1\n\n        if q.outcomes:\n            conditions.append(f\"\
          outcome = ANY(${param_idx})\")\n            params.append([o.value for o in q.outcomes])\n            param_idx\
          \ += 1\n\n        if q.search_text:\n            conditions.append(f\"search_vector @@ plainto_tsquery(${param_idx})\"\
          )\n            params.append(q.search_text)\n            param_idx += 1\n\n        where_clause = \" AND \".join(conditions)\
          \ if conditions else \"TRUE\"\n\n        query = f\"\"\"\n            SELECT * FROM audit_log\n            WHERE\
          \ {where_clause}\n            ORDER BY timestamp DESC\n            LIMIT ${param_idx} OFFSET ${param_idx + 1}\n\
          \        \"\"\"\n        params.extend([q.limit, q.offset])\n\n        rows = await self.storage.fetch(query, *params)\n\
          \        return [self._row_to_event(row) for row in rows]\n\n    async def export(self, q: AuditQuery, format: ExportFormat)\
          \ -> bytes:\n        \"\"\"Export audit logs in specified format.\"\"\"\n        events = await self.query(q)\n\n\
          \        if format == ExportFormat.JSON:\n            return self._export_json(events)\n        elif format == ExportFormat.CSV:\n\
          \            return self._export_csv(events)\n        elif format == ExportFormat.SIEM:\n            return self._export_cef(events)\n\
          \n    def _export_json(self, events: list[AuditEvent]) -> bytes:\n        import json\n        return json.dumps([e.to_dict()\
          \ for e in events], indent=2).encode()\n\n    def _export_csv(self, events: list[AuditEvent]) -> bytes:\n      \
          \  import csv\n        import io\n\n        output = io.StringIO()\n        writer = csv.writer(output)\n\n    \
          \    # Header\n        writer.writerow([\n            'timestamp', 'actor_id', 'actor_name', 'action',\n       \
          \     'resource_type', 'resource_id', 'outcome', 'ip_address'\n        ])\n\n        for event in events:\n    \
          \        writer.writerow([\n                event.timestamp.isoformat(),\n                event.actor.id,\n    \
          \            event.actor.name,\n                event.action.value,\n                event.resource.type,\n    \
          \            event.resource.id,\n                event.outcome.value,\n                event.context.ip_address\n\
          \            ])\n\n        return output.getvalue().encode()\n\n    def _export_cef(self, events: list[AuditEvent])\
          \ -> bytes:\n        \"\"\"Export in Common Event Format for SIEM integration.\"\"\"\n        lines = []\n     \
          \   for event in events:\n            # CEF:Version|Device Vendor|Device Product|Device Version|Signature ID|Name|Severity|Extension\n\
          \            severity = 3 if event.outcome == AuditOutcome.SUCCESS else 7\n            extension = (\n         \
          \       f\"src={event.context.ip_address} \"\n                f\"suser={event.actor.name} \"\n                f\"\
          duser={event.resource.id} \"\n                f\"act={event.action.value} \"\n                f\"outcome={event.outcome.value}\"\
          \n            )\n            cef_line = (\n                f\"CEF:0|MyApp|AuditLog|1.0|{event.action.value}|\"\n\
          \                f\"{event.action.value} {event.resource.type}|{severity}|{extension}\"\n            )\n       \
          \     lines.append(cef_line)\n\n        return \"\\n\".join(lines).encode()\n\n    async def get_activity_summary(self,\
          \ actor_id: str,\n                                   start: datetime, end: datetime) -> dict:\n        \"\"\"Get\
          \ activity summary for an actor.\"\"\"\n        rows = await self.storage.fetch(\"\"\"\n            SELECT action,\
          \ outcome, COUNT(*) as count\n            FROM audit_log\n            WHERE actor_id = $1 AND timestamp BETWEEN\
          \ $2 AND $3\n            GROUP BY action, outcome\n        \"\"\", actor_id, start, end)\n\n        return {\n \
          \           \"actor_id\": actor_id,\n            \"period\": {\"start\": start, \"end\": end},\n            \"activity\"\
          : [\n                {\"action\": r[\"action\"], \"outcome\": r[\"outcome\"], \"count\": r[\"count\"]}\n       \
          \         for r in rows\n            ]\n        }\n```"
      pitfalls:
      - Full text search without index causes table scan
      - Large exports exhaust memory (use streaming)
      - Time zone handling inconsistent in queries
  rate-limiter-distributed:
    name: Distributed Rate Limiter
    description: Build a distributed rate limiter supporting multiple algorithms with Redis backend for cluster-wide limiting.
    why_important: Rate limiting protects services from abuse and ensures fair resource usage. Understanding algorithms helps
      choose the right approach.
    difficulty: intermediate
    tags:
    - backend
    - distributed-systems
    - reliability
    estimated_hours: 25
    prerequisites:
    - redis-clone
    learning_outcomes:
    - Implement token bucket and sliding window algorithms
    - Design distributed rate limiting with Redis
    - Handle burst traffic gracefully
    - Build multi-tier rate limiting
    milestones:
    - name: Rate Limiting Algorithms
      description: Implement token bucket, sliding window log, and sliding window counter algorithms.
      hints:
        level1: 'Token bucket: refill tokens at fixed rate, consume on request.'
        level2: 'Sliding window: track requests in time window, count or log-based.'
        level3: "```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\
          import time\nimport math\n\n@dataclass\nclass RateLimitResult:\n    allowed: bool\n    remaining: int\n    reset_at:\
          \ float  # Unix timestamp\n    retry_after: float = 0  # Seconds\n\nclass RateLimiter(ABC):\n    @abstractmethod\n\
          \    async def is_allowed(self, key: str) -> RateLimitResult:\n        pass\n\nclass TokenBucketLimiter(RateLimiter):\n\
          \    def __init__(self, redis, capacity: int, refill_rate: float):\n        self.redis = redis\n        self.capacity\
          \ = capacity\n        self.refill_rate = refill_rate  # tokens per second\n\n    async def is_allowed(self, key:\
          \ str) -> RateLimitResult:\n        now = time.time()\n        bucket_key = f\"ratelimit:bucket:{key}\"\n\n    \
          \    # Lua script for atomic token bucket\n        script = \"\"\"\n        local key = KEYS[1]\n        local capacity\
          \ = tonumber(ARGV[1])\n        local refill_rate = tonumber(ARGV[2])\n        local now = tonumber(ARGV[3])\n\n\
          \        local bucket = redis.call('HMGET', key, 'tokens', 'last_refill')\n        local tokens = tonumber(bucket[1])\
          \ or capacity\n        local last_refill = tonumber(bucket[2]) or now\n\n        -- Refill tokens\n        local\
          \ elapsed = now - last_refill\n        local refill = elapsed * refill_rate\n        tokens = math.min(capacity,\
          \ tokens + refill)\n\n        -- Try to consume\n        local allowed = 0\n        if tokens >= 1 then\n      \
          \      tokens = tokens - 1\n            allowed = 1\n        end\n\n        -- Save state\n        redis.call('HMSET',\
          \ key, 'tokens', tokens, 'last_refill', now)\n        redis.call('EXPIRE', key, math.ceil(capacity / refill_rate)\
          \ + 1)\n\n        return {allowed, tokens, capacity}\n        \"\"\"\n\n        result = await self.redis.eval(\n\
          \            script, 1, bucket_key,\n            self.capacity, self.refill_rate, now\n        )\n\n        allowed,\
          \ remaining, capacity = result\n        reset_at = now + (capacity - remaining) / self.refill_rate\n\n        return\
          \ RateLimitResult(\n            allowed=bool(allowed),\n            remaining=int(remaining),\n            reset_at=reset_at,\n\
          \            retry_after=0 if allowed else 1.0 / self.refill_rate\n        )\n\nclass SlidingWindowLogLimiter(RateLimiter):\n\
          \    \"\"\"Precise but memory-intensive sliding window.\"\"\"\n\n    def __init__(self, redis, limit: int, window_seconds:\
          \ int):\n        self.redis = redis\n        self.limit = limit\n        self.window = window_seconds\n\n    async\
          \ def is_allowed(self, key: str) -> RateLimitResult:\n        now = time.time()\n        window_start = now - self.window\n\
          \        log_key = f\"ratelimit:log:{key}\"\n\n        # Lua script for atomic sliding window\n        script =\
          \ \"\"\"\n        local key = KEYS[1]\n        local limit = tonumber(ARGV[1])\n        local window_start = tonumber(ARGV[2])\n\
          \        local now = tonumber(ARGV[3])\n        local window = tonumber(ARGV[4])\n\n        -- Remove old entries\n\
          \        redis.call('ZREMRANGEBYSCORE', key, 0, window_start)\n\n        -- Count current entries\n        local\
          \ count = redis.call('ZCARD', key)\n\n        local allowed = 0\n        if count < limit then\n            -- Add\
          \ new entry\n            redis.call('ZADD', key, now, now .. ':' .. math.random())\n            allowed = 1\n  \
          \          count = count + 1\n        end\n\n        redis.call('EXPIRE', key, window + 1)\n\n        -- Get oldest\
          \ entry for reset time\n        local oldest = redis.call('ZRANGE', key, 0, 0, 'WITHSCORES')\n        local reset_at\
          \ = oldest[2] and (tonumber(oldest[2]) + window) or (now + window)\n\n        return {allowed, limit - count, reset_at}\n\
          \        \"\"\"\n\n        result = await self.redis.eval(\n            script, 1, log_key,\n            self.limit,\
          \ window_start, now, self.window\n        )\n\n        allowed, remaining, reset_at = result\n\n        return RateLimitResult(\n\
          \            allowed=bool(allowed),\n            remaining=max(0, int(remaining)),\n            reset_at=reset_at,\n\
          \            retry_after=0 if allowed else reset_at - now\n        )\n\nclass SlidingWindowCounterLimiter(RateLimiter):\n\
          \    \"\"\"Memory-efficient approximate sliding window.\"\"\"\n\n    def __init__(self, redis, limit: int, window_seconds:\
          \ int):\n        self.redis = redis\n        self.limit = limit\n        self.window = window_seconds\n\n    async\
          \ def is_allowed(self, key: str) -> RateLimitResult:\n        now = time.time()\n        current_window = int(now\
          \ // self.window)\n        previous_window = current_window - 1\n\n        current_key = f\"ratelimit:counter:{key}:{current_window}\"\
          \n        previous_key = f\"ratelimit:counter:{key}:{previous_window}\"\n\n        # Lua script for atomic counter\n\
          \        script = \"\"\"\n        local current_key = KEYS[1]\n        local previous_key = KEYS[2]\n        local\
          \ limit = tonumber(ARGV[1])\n        local window = tonumber(ARGV[2])\n        local now = tonumber(ARGV[3])\n \
          \       local current_window = tonumber(ARGV[4])\n\n        local previous_count = tonumber(redis.call('GET', previous_key))\
          \ or 0\n        local current_count = tonumber(redis.call('GET', current_key)) or 0\n\n        -- Calculate weighted\
          \ count (sliding window approximation)\n        local window_start = current_window * window\n        local elapsed_ratio\
          \ = (now - window_start) / window\n        local weighted_count = previous_count * (1 - elapsed_ratio) + current_count\n\
          \n        local allowed = 0\n        if weighted_count < limit then\n            redis.call('INCR', current_key)\n\
          \            redis.call('EXPIRE', current_key, window * 2)\n            allowed = 1\n            weighted_count\
          \ = weighted_count + 1\n        end\n\n        return {allowed, math.floor(limit - weighted_count), window_start\
          \ + window}\n        \"\"\"\n\n        result = await self.redis.eval(\n            script, 2, current_key, previous_key,\n\
          \            self.limit, self.window, now, current_window\n        )\n\n        allowed, remaining, reset_at = result\n\
          \n        return RateLimitResult(\n            allowed=bool(allowed),\n            remaining=max(0, int(remaining)),\n\
          \            reset_at=reset_at,\n            retry_after=0 if allowed else reset_at - now\n        )\n```"
      pitfalls:
      - Token bucket allows burst above sustained rate
      - Sliding window log uses O(n) memory per key
      - Counter approximation can allow 2x limit at window boundary
    - name: Multi-tier Rate Limiting
      description: Implement hierarchical rate limiting with per-user, per-API, and global limits.
      hints:
        level1: 'Check limits in order: user -> API -> global.'
        level2: Use different algorithms for different tiers.
        level3: "```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom enum import Enum\n\nclass\
          \ LimitTier(str, Enum):\n    USER = \"user\"\n    API_KEY = \"api_key\"\n    ENDPOINT = \"endpoint\"\n    GLOBAL\
          \ = \"global\"\n\n@dataclass\nclass RateLimitConfig:\n    tier: LimitTier\n    limit: int\n    window_seconds: int\n\
          \    burst_limit: Optional[int] = None  # For token bucket\n\n@dataclass\nclass MultiTierResult:\n    allowed: bool\n\
          \    tier_results: dict[LimitTier, RateLimitResult]\n    limiting_tier: Optional[LimitTier] = None\n\nclass MultiTierRateLimiter:\n\
          \    def __init__(self, redis):\n        self.redis = redis\n        self.limiters: dict[LimitTier, tuple[RateLimitConfig,\
          \ RateLimiter]] = {}\n\n    def configure_tier(self, config: RateLimitConfig):\n        if config.burst_limit:\n\
          \            limiter = TokenBucketLimiter(\n                self.redis,\n                capacity=config.burst_limit,\n\
          \                refill_rate=config.limit / config.window_seconds\n            )\n        else:\n            limiter\
          \ = SlidingWindowCounterLimiter(\n                self.redis,\n                limit=config.limit,\n           \
          \     window_seconds=config.window_seconds\n            )\n        self.limiters[config.tier] = (config, limiter)\n\
          \n    async def is_allowed(self, context: dict) -> MultiTierResult:\n        \"\"\"Check all configured rate limit\
          \ tiers.\"\"\"\n        tier_results = {}\n        limiting_tier = None\n\n        # Check tiers in priority order\n\
          \        tier_order = [LimitTier.USER, LimitTier.API_KEY, LimitTier.ENDPOINT, LimitTier.GLOBAL]\n\n        for tier\
          \ in tier_order:\n            if tier not in self.limiters:\n                continue\n\n            config, limiter\
          \ = self.limiters[tier]\n            key = self._build_key(tier, context)\n\n            result = await limiter.is_allowed(key)\n\
          \            tier_results[tier] = result\n\n            if not result.allowed:\n                limiting_tier =\
          \ tier\n                break\n\n        return MultiTierResult(\n            allowed=limiting_tier is None,\n \
          \           tier_results=tier_results,\n            limiting_tier=limiting_tier\n        )\n\n    def _build_key(self,\
          \ tier: LimitTier, context: dict) -> str:\n        if tier == LimitTier.USER:\n            return f\"user:{context.get('user_id',\
          \ 'anonymous')}\"\n        elif tier == LimitTier.API_KEY:\n            return f\"apikey:{context.get('api_key',\
          \ 'none')}\"\n        elif tier == LimitTier.ENDPOINT:\n            return f\"endpoint:{context.get('endpoint',\
          \ 'unknown')}\"\n        elif tier == LimitTier.GLOBAL:\n            return \"global\"\n        return \"unknown\"\
          \n\nclass AdaptiveRateLimiter:\n    \"\"\"Rate limiter that adjusts based on system load.\"\"\"\n\n    def __init__(self,\
          \ redis, base_limit: int, window: int):\n        self.redis = redis\n        self.base_limit = base_limit\n    \
          \    self.window = window\n        self.limiter = SlidingWindowCounterLimiter(redis, base_limit, window)\n     \
          \   self._load_factor = 1.0\n\n    async def update_load_factor(self, cpu_usage: float, error_rate: float):\n  \
          \      \"\"\"Adjust limits based on system health.\"\"\"\n        # Reduce limit when system is stressed\n     \
          \   if cpu_usage > 0.8 or error_rate > 0.05:\n            self._load_factor = max(0.5, self._load_factor - 0.1)\n\
          \        elif cpu_usage < 0.5 and error_rate < 0.01:\n            self._load_factor = min(1.5, self._load_factor\
          \ + 0.1)\n\n        # Update limiter with adjusted limit\n        adjusted_limit = int(self.base_limit * self._load_factor)\n\
          \        self.limiter = SlidingWindowCounterLimiter(\n            self.redis, adjusted_limit, self.window\n    \
          \    )\n\n    async def is_allowed(self, key: str) -> RateLimitResult:\n        return await self.limiter.is_allowed(key)\n\
          \n# Middleware for HTTP frameworks\nclass RateLimitMiddleware:\n    def __init__(self, limiter: MultiTierRateLimiter):\n\
          \        self.limiter = limiter\n\n    async def __call__(self, request, call_next):\n        context = {\n    \
          \        \"user_id\": request.user.id if request.user else None,\n            \"api_key\": request.headers.get(\"\
          X-API-Key\"),\n            \"endpoint\": f\"{request.method}:{request.path}\",\n            \"ip\": request.client.host\n\
          \        }\n\n        result = await self.limiter.is_allowed(context)\n\n        # Add rate limit headers\n    \
          \    headers = {}\n        for tier, tier_result in result.tier_results.items():\n            headers[f\"X-RateLimit-{tier.value}-Remaining\"\
          ] = str(tier_result.remaining)\n            headers[f\"X-RateLimit-{tier.value}-Reset\"] = str(int(tier_result.reset_at))\n\
          \n        if not result.allowed:\n            return Response(\n                status_code=429,\n             \
          \   headers={\n                    **headers,\n                    \"Retry-After\": str(int(result.tier_results[result.limiting_tier].retry_after))\n\
          \                },\n                content={\"error\": \"Rate limit exceeded\", \"tier\": result.limiting_tier.value}\n\
          \            )\n\n        response = await call_next(request)\n        for key, value in headers.items():\n    \
          \        response.headers[key] = value\n        return response\n```"
      pitfalls:
      - Checking all tiers even after one fails wastes resources
      - Different tier windows cause confusing UX
      - Adaptive limiting oscillates under variable load
  saga-orchestrator:
    name: Saga Orchestrator
    description: Build a saga orchestrator for managing distributed transactions with compensation and recovery.
    why_important: Sagas solve the distributed transaction problem in microservices. Understanding saga patterns helps design
      reliable distributed workflows.
    difficulty: advanced
    tags:
    - distributed-systems
    - microservices
    - reliability
    estimated_hours: 40
    prerequisites:
    - job-scheduler
    - event-sourcing
    learning_outcomes:
    - Design saga step definitions with compensations
    - Implement orchestration vs choreography patterns
    - Handle partial failures and rollback
    - Build saga state machine with persistence
    milestones:
    - name: Saga Definition
      description: Define saga steps with forward actions and compensation handlers.
      hints:
        level1: Each step has an action and a compensating action.
        level2: Steps can be sequential or parallel, with dependencies.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Any, Optional\nfrom enum\
          \ import Enum\nimport uuid\n\nclass StepStatus(str, Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\
          \n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    COMPENSATING = \"compensating\"\n    COMPENSATED =\
          \ \"compensated\"\n\n@dataclass\nclass SagaStep:\n    name: str\n    action: Callable[[dict], Any]  # Forward action\n\
          \    compensation: Callable[[dict, Any], None]  # Rollback action\n    timeout: int = 300  # seconds\n    retries:\
          \ int = 3\n    depends_on: list[str] = field(default_factory=list)\n\n@dataclass\nclass StepExecution:\n    step_name:\
          \ str\n    status: StepStatus = StepStatus.PENDING\n    result: Any = None\n    error: Optional[str] = None\n  \
          \  started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    attempts: int = 0\n\n\
          @dataclass\nclass SagaDefinition:\n    name: str\n    steps: list[SagaStep] = field(default_factory=list)\n\n  \
          \  def add_step(self, name: str, action: Callable, compensation: Callable,\n                 depends_on: list[str]\
          \ = None, **kwargs) -> 'SagaDefinition':\n        self.steps.append(SagaStep(\n            name=name,\n        \
          \    action=action,\n            compensation=compensation,\n            depends_on=depends_on or [],\n        \
          \    **kwargs\n        ))\n        return self\n\n    def get_step(self, name: str) -> Optional[SagaStep]:\n   \
          \     for step in self.steps:\n            if step.name == name:\n                return step\n        return None\n\
          \n    def get_execution_order(self) -> list[list[str]]:\n        \"\"\"Return steps grouped by parallel execution\
          \ levels.\"\"\"\n        remaining = {s.name: set(s.depends_on) for s in self.steps}\n        levels = []\n\n  \
          \      while remaining:\n            # Find steps with no pending dependencies\n            ready = [name for name,\
          \ deps in remaining.items() if not deps]\n            if not ready:\n                raise ValueError(\"Circular\
          \ dependency detected\")\n\n            levels.append(ready)\n\n            # Remove completed steps from dependencies\n\
          \            for name in ready:\n                del remaining[name]\n            for deps in remaining.values():\n\
          \                deps -= set(ready)\n\n        return levels\n\n# Example saga definition\ndef create_order_saga()\
          \ -> SagaDefinition:\n    saga = SagaDefinition(name=\"create_order\")\n\n    saga.add_step(\n        name=\"reserve_inventory\"\
          ,\n        action=lambda ctx: inventory_service.reserve(ctx[\"items\"]),\n        compensation=lambda ctx, result:\
          \ inventory_service.release(result[\"reservation_id\"])\n    )\n\n    saga.add_step(\n        name=\"charge_payment\"\
          ,\n        action=lambda ctx: payment_service.charge(ctx[\"payment_info\"], ctx[\"total\"]),\n        compensation=lambda\
          \ ctx, result: payment_service.refund(result[\"transaction_id\"]),\n        depends_on=[\"reserve_inventory\"]\n\
          \    )\n\n    saga.add_step(\n        name=\"create_shipment\",\n        action=lambda ctx: shipping_service.create(ctx[\"\
          address\"], ctx[\"items\"]),\n        compensation=lambda ctx, result: shipping_service.cancel(result[\"shipment_id\"\
          ]),\n        depends_on=[\"charge_payment\"]\n    )\n\n    saga.add_step(\n        name=\"send_confirmation\",\n\
          \        action=lambda ctx: notification_service.send_order_confirmation(ctx[\"email\"], ctx[\"order_id\"]),\n \
          \       compensation=lambda ctx, result: None,  # No compensation needed\n        depends_on=[\"create_shipment\"\
          ]\n    )\n\n    return saga\n```"
      pitfalls:
      - Compensation that fails leaves inconsistent state
      - Parallel steps with shared state cause race conditions
      - Timeout too short fails legitimate slow operations
    - name: Saga Orchestrator Engine
      description: Build the orchestrator that executes sagas, tracks state, and handles failures.
      hints:
        level1: Execute steps in dependency order, track results.
        level2: On failure, compensate completed steps in reverse order.
        level3: "```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional,\
          \ Any\nimport asyncio\n\nclass SagaStatus(str, Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n  \
          \  COMPLETED = \"completed\"\n    COMPENSATING = \"compensating\"\n    COMPENSATED = \"compensated\"\n    FAILED\
          \ = \"failed\"  # Compensation also failed\n\n@dataclass\nclass SagaExecution:\n    id: str\n    saga_name: str\n\
          \    status: SagaStatus = SagaStatus.PENDING\n    context: dict = field(default_factory=dict)\n    step_executions:\
          \ dict[str, StepExecution] = field(default_factory=dict)\n    started_at: Optional[datetime] = None\n    completed_at:\
          \ Optional[datetime] = None\n    error: Optional[str] = None\n\nclass SagaOrchestrator:\n    def __init__(self,\
          \ storage, event_bus=None):\n        self.storage = storage\n        self.event_bus = event_bus\n        self.sagas:\
          \ dict[str, SagaDefinition] = {}\n\n    def register(self, saga: SagaDefinition):\n        self.sagas[saga.name]\
          \ = saga\n\n    async def execute(self, saga_name: str, context: dict) -> SagaExecution:\n        saga = self.sagas.get(saga_name)\n\
          \        if not saga:\n            raise ValueError(f\"Unknown saga: {saga_name}\")\n\n        execution = SagaExecution(\n\
          \            id=str(uuid.uuid4()),\n            saga_name=saga_name,\n            context=context,\n           \
          \ started_at=datetime.utcnow()\n        )\n\n        # Initialize step executions\n        for step in saga.steps:\n\
          \            execution.step_executions[step.name] = StepExecution(step_name=step.name)\n\n        await self.storage.save(execution)\n\
          \n        try:\n            await self._run_saga(saga, execution)\n            execution.status = SagaStatus.COMPLETED\n\
          \        except Exception as e:\n            execution.error = str(e)\n            await self._compensate(saga,\
          \ execution)\n\n        execution.completed_at = datetime.utcnow()\n        await self.storage.save(execution)\n\
          \n        return execution\n\n    async def _run_saga(self, saga: SagaDefinition, execution: SagaExecution):\n \
          \       execution.status = SagaStatus.RUNNING\n        await self.storage.save(execution)\n\n        levels = saga.get_execution_order()\n\
          \n        for level in levels:\n            # Execute parallel steps\n            tasks = [\n                self._execute_step(saga.get_step(name),\
          \ execution)\n                for name in level\n            ]\n            results = await asyncio.gather(*tasks,\
          \ return_exceptions=True)\n\n            # Check for failures\n            for name, result in zip(level, results):\n\
          \                if isinstance(result, Exception):\n                    raise result\n\n    async def _execute_step(self,\
          \ step: SagaStep, execution: SagaExecution):\n        step_exec = execution.step_executions[step.name]\n       \
          \ step_exec.status = StepStatus.RUNNING\n        step_exec.started_at = datetime.utcnow()\n        step_exec.attempts\
          \ += 1\n\n        await self.storage.save(execution)\n\n        try:\n            # Execute with timeout\n     \
          \       result = await asyncio.wait_for(\n                self._call_action(step.action, execution.context),\n \
          \               timeout=step.timeout\n            )\n\n            step_exec.result = result\n            step_exec.status\
          \ = StepStatus.COMPLETED\n            step_exec.completed_at = datetime.utcnow()\n\n            # Update context\
          \ with result\n            execution.context[f\"{step.name}_result\"] = result\n\n        except Exception as e:\n\
          \            step_exec.error = str(e)\n            step_exec.status = StepStatus.FAILED\n\n            # Retry if\
          \ attempts remaining\n            if step_exec.attempts < step.retries:\n                await asyncio.sleep(2 **\
          \ step_exec.attempts)  # Exponential backoff\n                return await self._execute_step(step, execution)\n\
          \n            raise\n\n        await self.storage.save(execution)\n\n    async def _compensate(self, saga: SagaDefinition,\
          \ execution: SagaExecution):\n        execution.status = SagaStatus.COMPENSATING\n        await self.storage.save(execution)\n\
          \n        # Get completed steps in reverse order\n        levels = saga.get_execution_order()\n        completed_steps\
          \ = []\n\n        for level in levels:\n            for name in level:\n                step_exec = execution.step_executions[name]\n\
          \                if step_exec.status == StepStatus.COMPLETED:\n                    completed_steps.append(name)\n\
          \n        # Compensate in reverse order\n        for step_name in reversed(completed_steps):\n            step =\
          \ saga.get_step(step_name)\n            step_exec = execution.step_executions[step_name]\n\n            try:\n \
          \               step_exec.status = StepStatus.COMPENSATING\n                await self.storage.save(execution)\n\
          \n                await self._call_action(\n                    step.compensation,\n                    execution.context,\n\
          \                    step_exec.result\n                )\n\n                step_exec.status = StepStatus.COMPENSATED\n\
          \            except Exception as e:\n                step_exec.error = f\"Compensation failed: {e}\"\n         \
          \       execution.status = SagaStatus.FAILED\n                await self.storage.save(execution)\n             \
          \   raise\n\n        execution.status = SagaStatus.COMPENSATED\n        await self.storage.save(execution)\n\n \
          \   async def _call_action(self, action: Callable, *args):\n        if asyncio.iscoroutinefunction(action):\n  \
          \          return await action(*args)\n        return action(*args)\n\n    async def resume(self, execution_id:\
          \ str) -> SagaExecution:\n        \"\"\"Resume a failed or interrupted saga.\"\"\"\n        execution = await self.storage.load(execution_id)\n\
          \        saga = self.sagas.get(execution.saga_name)\n\n        if execution.status == SagaStatus.RUNNING:\n    \
          \        # Find incomplete steps and continue\n            await self._run_saga(saga, execution)\n        elif execution.status\
          \ == SagaStatus.COMPENSATING:\n            # Continue compensation\n            await self._compensate(saga, execution)\n\
          \n        return execution\n```"
      pitfalls:
      - Resuming saga loses in-memory step results
      - Parallel compensation can conflict
      - Network partition causes duplicate saga execution
    - name: Saga State Persistence
      description: Persist saga state for recovery and implement idempotent step execution.
      hints:
        level1: Store saga state after each step completion.
        level2: Use idempotency keys to prevent duplicate execution.
        level3: "```python\nfrom dataclasses import asdict\nimport json\n\nclass SagaStorage:\n    def __init__(self, db):\n\
          \        self.db = db\n\n    async def save(self, execution: SagaExecution):\n        await self.db.execute(\"\"\
          \"\n            INSERT INTO saga_executions (\n                id, saga_name, status, context, step_executions,\n\
          \                started_at, completed_at, error\n            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\n      \
          \      ON CONFLICT (id) DO UPDATE SET\n                status = $3, context = $4, step_executions = $5,\n      \
          \          completed_at = $7, error = $8\n        \"\"\",\n            execution.id,\n            execution.saga_name,\n\
          \            execution.status.value,\n            json.dumps(execution.context),\n            json.dumps({k: asdict(v)\
          \ for k, v in execution.step_executions.items()}),\n            execution.started_at,\n            execution.completed_at,\n\
          \            execution.error\n        )\n\n    async def load(self, execution_id: str) -> SagaExecution:\n     \
          \   row = await self.db.fetchrow(\n            \"SELECT * FROM saga_executions WHERE id = $1\",\n            execution_id\n\
          \        )\n        if not row:\n            raise ValueError(f\"Saga execution not found: {execution_id}\")\n\n\
          \        step_data = json.loads(row[\"step_executions\"])\n        return SagaExecution(\n            id=row[\"\
          id\"],\n            saga_name=row[\"saga_name\"],\n            status=SagaStatus(row[\"status\"]),\n           \
          \ context=json.loads(row[\"context\"]),\n            step_executions={\n                k: StepExecution(**v) for\
          \ k, v in step_data.items()\n            },\n            started_at=row[\"started_at\"],\n            completed_at=row[\"\
          completed_at\"],\n            error=row[\"error\"]\n        )\n\n    async def list_pending(self, saga_name: str\
          \ = None) -> list[SagaExecution]:\n        query = \"\"\"\n            SELECT * FROM saga_executions\n         \
          \   WHERE status IN ('pending', 'running', 'compensating')\n        \"\"\"\n        if saga_name:\n            query\
          \ += \" AND saga_name = $1\"\n            rows = await self.db.fetch(query, saga_name)\n        else:\n        \
          \    rows = await self.db.fetch(query)\n\n        return [await self.load(row[\"id\"]) for row in rows]\n\nclass\
          \ IdempotentStepExecutor:\n    \"\"\"Ensures steps are executed exactly once.\"\"\"\n\n    def __init__(self, storage,\
          \ lock_manager):\n        self.storage = storage\n        self.lock_manager = lock_manager\n\n    async def execute(self,\
          \ saga_id: str, step_name: str,\n                     action: Callable, *args) -> Any:\n        idempotency_key\
          \ = f\"saga:{saga_id}:step:{step_name}\"\n\n        # Check if already executed\n        existing = await self.storage.get_step_result(idempotency_key)\n\
          \        if existing:\n            return existing\n\n        # Acquire lock\n        async with self.lock_manager.lock(idempotency_key,\
          \ timeout=300):\n            # Double-check after acquiring lock\n            existing = await self.storage.get_step_result(idempotency_key)\n\
          \            if existing:\n                return existing\n\n            # Execute\n            result = await\
          \ action(*args)\n\n            # Store result\n            await self.storage.store_step_result(idempotency_key,\
          \ result)\n\n            return result\n\nclass SagaRecoveryService:\n    \"\"\"Background service that recovers\
          \ interrupted sagas.\"\"\"\n\n    def __init__(self, orchestrator: SagaOrchestrator, storage: SagaStorage):\n  \
          \      self.orchestrator = orchestrator\n        self.storage = storage\n        self.check_interval = 60  # seconds\n\
          \n    async def start(self):\n        while True:\n            await self._recover_pending()\n            await\
          \ asyncio.sleep(self.check_interval)\n\n    async def _recover_pending(self):\n        pending = await self.storage.list_pending()\n\
          \n        for execution in pending:\n            # Check if saga is actually stuck (no progress for a while)\n \
          \           last_update = self._get_last_update(execution)\n            if datetime.utcnow() - last_update > timedelta(minutes=5):\n\
          \                try:\n                    await self.orchestrator.resume(execution.id)\n                except\
          \ Exception as e:\n                    print(f\"Failed to recover saga {execution.id}: {e}\")\n\n    def _get_last_update(self,\
          \ execution: SagaExecution) -> datetime:\n        times = [execution.started_at]\n        for step_exec in execution.step_executions.values():\n\
          \            if step_exec.started_at:\n                times.append(step_exec.started_at)\n            if step_exec.completed_at:\n\
          \                times.append(step_exec.completed_at)\n        return max(t for t in times if t)\n```"
      pitfalls:
      - JSON serialization loses datetime precision
      - Lock timeout shorter than step timeout causes issues
      - Recovery loop processes same saga repeatedly
  oauth2-provider:
    name: OAuth2/OIDC Provider
    description: Build a complete OAuth2 and OpenID Connect identity provider supporting authorization code flow, PKCE, refresh
      tokens, and JWT access tokens.
    why_expert: Identity is the foundation of security. Understanding OAuth2/OIDC internals helps debug auth issues, design
      secure systems, and integrate with any identity provider.
    difficulty: expert
    tags:
    - security
    - authentication
    - identity
    - oauth2
    - jwt
    estimated_hours: 50
    prerequisites:
    - build-http-server
    milestones:
    - name: Client Registration & Authorization Endpoint
      description: Implement client registration and the authorization endpoint with PKCE support
      skills:
      - OAuth2 flows
      - PKCE
      - Cryptographic challenges
      hints:
        level1: Start with client_id/client_secret storage and the /authorize endpoint
        level2: 'PKCE: client generates code_verifier, sends SHA256(code_verifier) as code_challenge'
        level3: "\n```python\nimport hashlib\nimport base64\nimport secrets\nfrom dataclasses import dataclass\nfrom typing\
          \ import Optional\nimport time\n\n@dataclass\nclass OAuthClient:\n    client_id: str\n    client_secret_hash: str\n\
          \    redirect_uris: list[str]\n    grant_types: list[str]  # authorization_code, refresh_token, client_credentials\n\
          \    scopes: list[str]\n\nclass AuthorizationServer:\n    def __init__(self):\n        self.clients: dict[str, OAuthClient]\
          \ = {}\n        self.authorization_codes: dict[str, dict] = {}  # code -> {client_id, user_id, scopes, code_challenge,\
          \ expires_at}\n        self.refresh_tokens: dict[str, dict] = {}\n\n    def register_client(self, redirect_uris:\
          \ list[str], grant_types: list[str]) -> tuple[str, str]:\n        client_id = secrets.token_urlsafe(16)\n      \
          \  client_secret = secrets.token_urlsafe(32)\n        client_secret_hash = hashlib.sha256(client_secret.encode()).hexdigest()\n\
          \n        self.clients[client_id] = OAuthClient(\n            client_id=client_id,\n            client_secret_hash=client_secret_hash,\n\
          \            redirect_uris=redirect_uris,\n            grant_types=grant_types,\n            scopes=[\"openid\"\
          , \"profile\", \"email\"]\n        )\n        return client_id, client_secret\n\n    def authorize(self, client_id:\
          \ str, redirect_uri: str, response_type: str,\n                  scope: str, state: str, code_challenge: Optional[str]\
          \ = None,\n                  code_challenge_method: Optional[str] = None, user_id: str = None) -> str:\n       \
          \ client = self.clients.get(client_id)\n        if not client:\n            raise ValueError(\"Invalid client_id\"\
          )\n        if redirect_uri not in client.redirect_uris:\n            raise ValueError(\"Invalid redirect_uri\")\n\
          \        if response_type != \"code\":\n            raise ValueError(\"Only authorization_code flow supported\"\
          )\n\n        # Generate authorization code\n        code = secrets.token_urlsafe(32)\n        self.authorization_codes[code]\
          \ = {\n            \"client_id\": client_id,\n            \"user_id\": user_id,\n            \"redirect_uri\": redirect_uri,\n\
          \            \"scopes\": scope.split(),\n            \"code_challenge\": code_challenge,\n            \"code_challenge_method\"\
          : code_challenge_method,\n            \"expires_at\": time.time() + 600  # 10 minutes\n        }\n\n        return\
          \ f\"{redirect_uri}?code={code}&state={state}\"\n\n    def verify_pkce(self, code_verifier: str, code_challenge:\
          \ str, method: str) -> bool:\n        if method == \"S256\":\n            computed = base64.urlsafe_b64encode(\n\
          \                hashlib.sha256(code_verifier.encode()).digest()\n            ).rstrip(b'=').decode()\n        \
          \    return secrets.compare_digest(computed, code_challenge)\n        elif method == \"plain\":\n            return\
          \ secrets.compare_digest(code_verifier, code_challenge)\n        return False\n```\n"
      pitfalls:
      - Authorization codes must be single-use and short-lived (10 min max)
      - Always validate redirect_uri exactly - no partial matches
      - PKCE is mandatory for public clients (mobile, SPA)
      - State parameter prevents CSRF - must be unpredictable
    - name: Token Endpoint & JWT Generation
      description: Implement the token endpoint with JWT access tokens and refresh token rotation
      skills:
      - JWT signing
      - Token rotation
      - Secure token storage
      hints:
        level1: Token endpoint exchanges authorization code for access_token + refresh_token
        level2: Use RS256 (asymmetric) for JWTs so resource servers can verify without shared secret
        level3: "\n```python\nimport jwt\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric\
          \ import rsa\nfrom datetime import datetime, timedelta\n\nclass TokenService:\n    def __init__(self):\n       \
          \ # Generate RSA key pair for JWT signing\n        self.private_key = rsa.generate_private_key(\n            public_exponent=65537,\n\
          \            key_size=2048\n        )\n        self.public_key = self.private_key.public_key()\n        self.issuer\
          \ = \"https://auth.example.com\"\n\n    def get_jwks(self) -> dict:\n        '''Return JWKS for token verification'''\n\
          \        from cryptography.hazmat.primitives.serialization import Encoding, PublicFormat\n        import json\n\n\
          \        public_bytes = self.public_key.public_bytes(\n            Encoding.PEM, PublicFormat.SubjectPublicKeyInfo\n\
          \        )\n        # Convert to JWK format\n        numbers = self.public_key.public_numbers()\n\n        def int_to_base64(n:\
          \ int) -> str:\n            length = (n.bit_length() + 7) // 8\n            return base64.urlsafe_b64encode(n.to_bytes(length,\
          \ 'big')).rstrip(b'=').decode()\n\n        return {\n            \"keys\": [{\n                \"kty\": \"RSA\"\
          ,\n                \"use\": \"sig\",\n                \"alg\": \"RS256\",\n                \"kid\": \"key-1\",\n\
          \                \"n\": int_to_base64(numbers.n),\n                \"e\": int_to_base64(numbers.e)\n           \
          \ }]\n        }\n\n    def create_access_token(self, user_id: str, client_id: str, scopes: list[str]) -> str:\n\
          \        now = datetime.utcnow()\n        payload = {\n            \"iss\": self.issuer,\n            \"sub\": user_id,\n\
          \            \"aud\": client_id,\n            \"exp\": now + timedelta(hours=1),\n            \"iat\": now,\n  \
          \          \"scope\": \" \".join(scopes),\n            \"jti\": secrets.token_urlsafe(16)  # Unique token ID\n \
          \       }\n\n        private_pem = self.private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n\
          \            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n\
          \        )\n\n        return jwt.encode(payload, private_pem, algorithm=\"RS256\", headers={\"kid\": \"key-1\"})\n\
          \n    def create_id_token(self, user_id: str, client_id: str, nonce: str, user_info: dict) -> str:\n        '''OpenID\
          \ Connect ID Token'''\n        now = datetime.utcnow()\n        payload = {\n            \"iss\": self.issuer,\n\
          \            \"sub\": user_id,\n            \"aud\": client_id,\n            \"exp\": now + timedelta(hours=1),\n\
          \            \"iat\": now,\n            \"auth_time\": int(now.timestamp()),\n            \"nonce\": nonce,\n  \
          \          **user_info  # name, email, etc.\n        }\n\n        private_pem = self.private_key.private_bytes(\n\
          \            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n     \
          \       encryption_algorithm=serialization.NoEncryption()\n        )\n\n        return jwt.encode(payload, private_pem,\
          \ algorithm=\"RS256\", headers={\"kid\": \"key-1\"})\n\n    def create_refresh_token(self) -> str:\n        return\
          \ secrets.token_urlsafe(32)\n```\n"
      pitfalls:
      - Never include sensitive data in JWT payload - it's base64, not encrypted
      - 'Refresh token rotation: issue new refresh token on each use, invalidate old one'
      - Access tokens should be short-lived (15 min - 1 hour)
      - Always use constant-time comparison for token validation
    - name: Token Introspection & Revocation
      description: Implement RFC 7662 token introspection and RFC 7009 token revocation
      skills:
      - Token lifecycle
      - Revocation strategies
      - Cache invalidation
      hints:
        level1: Introspection lets resource servers validate opaque tokens
        level2: Revocation must handle both access tokens and refresh tokens
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport time\n\n@dataclass\n\
          class TokenMetadata:\n    active: bool\n    client_id: str\n    username: str\n    scope: str\n    sub: str\n  \
          \  aud: str\n    iss: str\n    exp: int\n    iat: int\n    token_type: str = \"Bearer\"\n\nclass TokenManager:\n\
          \    def __init__(self, token_service: TokenService):\n        self.token_service = token_service\n        self.revoked_tokens:\
          \ set[str] = set()  # Set of revoked JTIs\n        self.refresh_tokens: dict[str, dict] = {}  # token -> metadata\n\
          \        self.token_families: dict[str, str] = {}  # refresh_token -> family_id\n\n    def introspect(self, token:\
          \ str, token_type_hint: Optional[str] = None) -> dict:\n        '''RFC 7662 - Token Introspection'''\n        #\
          \ Check if it's a JWT access token\n        try:\n            # Verify JWT signature\n            public_pem = self.token_service.public_key.public_bytes(\n\
          \                encoding=serialization.Encoding.PEM,\n                format=serialization.PublicFormat.SubjectPublicKeyInfo\n\
          \            )\n            payload = jwt.decode(token, public_pem, algorithms=[\"RS256\"],\n                  \
          \             options={\"verify_aud\": False})\n\n            # Check if revoked\n            if payload.get(\"\
          jti\") in self.revoked_tokens:\n                return {\"active\": False}\n\n            # Check expiration\n \
          \           if payload[\"exp\"] < time.time():\n                return {\"active\": False}\n\n            return\
          \ {\n                \"active\": True,\n                \"client_id\": payload.get(\"aud\"),\n                \"\
          username\": payload.get(\"sub\"),\n                \"scope\": payload.get(\"scope\"),\n                \"sub\":\
          \ payload.get(\"sub\"),\n                \"aud\": payload.get(\"aud\"),\n                \"iss\": payload.get(\"\
          iss\"),\n                \"exp\": payload.get(\"exp\"),\n                \"iat\": payload.get(\"iat\"),\n      \
          \          \"token_type\": \"Bearer\"\n            }\n        except jwt.InvalidTokenError:\n            pass\n\n\
          \        # Check refresh tokens\n        if token in self.refresh_tokens:\n            meta = self.refresh_tokens[token]\n\
          \            if meta[\"expires_at\"] > time.time():\n                return {\"active\": True, **meta}\n\n     \
          \   return {\"active\": False}\n\n    def revoke(self, token: str, token_type_hint: Optional[str] = None) -> bool:\n\
          \        '''RFC 7009 - Token Revocation'''\n        # Try to decode as JWT to get JTI\n        try:\n          \
          \  # Decode without verification to get JTI\n            payload = jwt.decode(token, options={\"verify_signature\"\
          : False})\n            if \"jti\" in payload:\n                self.revoked_tokens.add(payload[\"jti\"])\n     \
          \           return True\n        except:\n            pass\n\n        # Check if it's a refresh token\n        if\
          \ token in self.refresh_tokens:\n            # Revoke entire token family (prevents refresh token reuse attacks)\n\
          \            family_id = self.token_families.get(token)\n            if family_id:\n                tokens_to_revoke\
          \ = [t for t, fid in self.token_families.items() if fid == family_id]\n                for t in tokens_to_revoke:\n\
          \                    del self.refresh_tokens[t]\n                    del self.token_families[t]\n            else:\n\
          \                del self.refresh_tokens[token]\n            return True\n\n        return False\n```\n"
      pitfalls:
      - Introspection endpoint must be protected (client authentication required)
      - Revoked JWTs need tracking until expiry (use jti claim)
      - 'Refresh token families: if old token reused, revoke entire family (theft detection)'
      - Consider using Redis for revocation list with TTL matching token expiry
    - name: UserInfo Endpoint & Consent Management
      description: Implement OIDC UserInfo endpoint and user consent flows for scope approval
      skills:
      - OIDC compliance
      - Consent UX
      - Scope management
      hints:
        level1: UserInfo returns claims based on granted scopes (profile, email, address, phone)
        level2: Store user consents per client - don't re-ask for already-granted scopes
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom enum import Enum\n\nclass\
          \ Scope(Enum):\n    OPENID = \"openid\"\n    PROFILE = \"profile\"  # name, family_name, given_name, picture, etc.\n\
          \    EMAIL = \"email\"      # email, email_verified\n    ADDRESS = \"address\"  # address claim\n    PHONE = \"\
          phone\"      # phone_number, phone_number_verified\n\n@dataclass\nclass UserConsent:\n    user_id: str\n    client_id:\
          \ str\n    granted_scopes: set[str]\n    granted_at: float\n\nclass UserInfoService:\n    # Standard OIDC claims\
          \ per scope\n    SCOPE_CLAIMS = {\n        \"profile\": [\"name\", \"family_name\", \"given_name\", \"middle_name\"\
          , \"nickname\",\n                   \"preferred_username\", \"profile\", \"picture\", \"website\", \"gender\",\n\
          \                   \"birthdate\", \"zoneinfo\", \"locale\", \"updated_at\"],\n        \"email\": [\"email\", \"\
          email_verified\"],\n        \"address\": [\"address\"],\n        \"phone\": [\"phone_number\", \"phone_number_verified\"\
          ]\n    }\n\n    def __init__(self):\n        self.user_consents: dict[tuple[str, str], UserConsent] = {}  # (user_id,\
          \ client_id) -> consent\n        self.user_profiles: dict[str, dict] = {}  # user_id -> profile data\n\n    def\
          \ get_userinfo(self, access_token: str, token_manager: TokenManager) -> dict:\n        '''OIDC UserInfo Endpoint'''\n\
          \        introspection = token_manager.introspect(access_token)\n        if not introspection.get(\"active\"):\n\
          \            raise ValueError(\"Invalid or expired token\")\n\n        user_id = introspection[\"sub\"]\n      \
          \  scopes = introspection.get(\"scope\", \"\").split()\n\n        if \"openid\" not in scopes:\n            raise\
          \ ValueError(\"openid scope required\")\n\n        profile = self.user_profiles.get(user_id, {})\n        claims\
          \ = {\"sub\": user_id}\n\n        for scope in scopes:\n            if scope in self.SCOPE_CLAIMS:\n           \
          \     for claim in self.SCOPE_CLAIMS[scope]:\n                    if claim in profile:\n                       \
          \ claims[claim] = profile[claim]\n\n        return claims\n\n    def check_consent(self, user_id: str, client_id:\
          \ str, requested_scopes: set[str]) -> set[str]:\n        '''Check which scopes need user consent'''\n        key\
          \ = (user_id, client_id)\n        if key not in self.user_consents:\n            return requested_scopes  # All\
          \ scopes need consent\n\n        existing = self.user_consents[key].granted_scopes\n        return requested_scopes\
          \ - existing  # Return scopes needing consent\n\n    def record_consent(self, user_id: str, client_id: str, scopes:\
          \ set[str]):\n        '''Record user's consent for scopes'''\n        key = (user_id, client_id)\n        if key\
          \ in self.user_consents:\n            self.user_consents[key].granted_scopes.update(scopes)\n        else:\n   \
          \         self.user_consents[key] = UserConsent(\n                user_id=user_id,\n                client_id=client_id,\n\
          \                granted_scopes=scopes,\n                granted_at=time.time()\n            )\n\n    def revoke_consent(self,\
          \ user_id: str, client_id: str):\n        '''Allow user to revoke consent for a client'''\n        key = (user_id,\
          \ client_id)\n        if key in self.user_consents:\n            del self.user_consents[key]\n            # Also\
          \ revoke all tokens for this user/client pair\n            return True\n        return False\n```\n"
      pitfalls:
      - UserInfo must use access token, not ID token
      - Only return claims for scopes actually granted, not requested
      - Consent screen must clearly show what data will be shared
      - Allow users to revoke consent and see all authorized applications
  rbac-system:
    name: RBAC/ABAC Authorization System
    description: Build a flexible authorization system supporting Role-Based Access Control (RBAC) and Attribute-Based Access
      Control (ABAC) with policy evaluation.
    why_expert: Authorization logic is often scattered and inconsistent. A centralized policy engine prevents security bugs
      and simplifies auditing.
    difficulty: expert
    tags:
    - security
    - authorization
    - rbac
    - abac
    - policy
    estimated_hours: 40
    prerequisites: []
    milestones:
    - name: Role & Permission Model
      description: Implement hierarchical roles with permission inheritance and efficient lookup
      skills:
      - Role hierarchies
      - Permission modeling
      - Graph traversal
      hints:
        level1: Roles can inherit from other roles - use DAG (no cycles)
        level2: Denormalize effective permissions for O(1) lookup at runtime
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          import time\n\nclass Action(Enum):\n    CREATE = \"create\"\n    READ = \"read\"\n    UPDATE = \"update\"\n    DELETE\
          \ = \"delete\"\n    EXECUTE = \"execute\"\n    ADMIN = \"*\"  # All actions\n\n@dataclass\nclass Permission:\n \
          \   resource: str      # e.g., \"documents\", \"users\", \"reports\"\n    action: Action\n    conditions: Optional[dict]\
          \ = None  # For ABAC conditions\n\n    def __hash__(self):\n        return hash((self.resource, self.action))\n\n\
          \    def matches(self, resource: str, action: Action) -> bool:\n        if self.action == Action.ADMIN:\n      \
          \      return self.resource == resource or self.resource == \"*\"\n        return self.resource == resource and\
          \ self.action == action\n\n@dataclass\nclass Role:\n    name: str\n    permissions: set[Permission] = field(default_factory=set)\n\
          \    parent_roles: set[str] = field(default_factory=set)  # Inheritance\n\nclass RBACEngine:\n    def __init__(self):\n\
          \        self.roles: dict[str, Role] = {}\n        self.user_roles: dict[str, set[str]] = {}  # user_id -> role\
          \ names\n        self._effective_permissions_cache: dict[str, set[Permission]] = {}\n\n    def create_role(self,\
          \ name: str, permissions: list[Permission],\n                    parent_roles: list[str] = None) -> Role:\n    \
          \    # Validate no cycles\n        if parent_roles:\n            for parent in parent_roles:\n                if\
          \ self._would_create_cycle(name, parent):\n                    raise ValueError(f\"Role inheritance would create\
          \ cycle: {name} -> {parent}\")\n\n        role = Role(\n            name=name,\n            permissions=set(permissions),\n\
          \            parent_roles=set(parent_roles) if parent_roles else set()\n        )\n        self.roles[name] = role\n\
          \        self._invalidate_cache()\n        return role\n\n    def _would_create_cycle(self, new_role: str, parent:\
          \ str, visited: set = None) -> bool:\n        if visited is None:\n            visited = set()\n        if parent\
          \ == new_role:\n            return True\n        if parent in visited:\n            return False\n        visited.add(parent)\n\
          \n        parent_role = self.roles.get(parent)\n        if not parent_role:\n            return False\n        for\
          \ grandparent in parent_role.parent_roles:\n            if self._would_create_cycle(new_role, grandparent, visited):\n\
          \                return True\n        return False\n\n    def get_effective_permissions(self, role_name: str) ->\
          \ set[Permission]:\n        if role_name in self._effective_permissions_cache:\n            return self._effective_permissions_cache[role_name]\n\
          \n        role = self.roles.get(role_name)\n        if not role:\n            return set()\n\n        # Start with\
          \ direct permissions\n        effective = set(role.permissions)\n\n        # Add inherited permissions (BFS)\n \
          \       visited = {role_name}\n        queue = list(role.parent_roles)\n\n        while queue:\n            parent_name\
          \ = queue.pop(0)\n            if parent_name in visited:\n                continue\n            visited.add(parent_name)\n\
          \n            parent = self.roles.get(parent_name)\n            if parent:\n                effective.update(parent.permissions)\n\
          \                queue.extend(parent.parent_roles)\n\n        self._effective_permissions_cache[role_name] = effective\n\
          \        return effective\n\n    def assign_role(self, user_id: str, role_name: str):\n        if role_name not\
          \ in self.roles:\n            raise ValueError(f\"Role not found: {role_name}\")\n        if user_id not in self.user_roles:\n\
          \            self.user_roles[user_id] = set()\n        self.user_roles[user_id].add(role_name)\n\n    def check_permission(self,\
          \ user_id: str, resource: str, action: Action) -> bool:\n        user_role_names = self.user_roles.get(user_id,\
          \ set())\n\n        for role_name in user_role_names:\n            permissions = self.get_effective_permissions(role_name)\n\
          \            for perm in permissions:\n                if perm.matches(resource, action):\n                    return\
          \ True\n        return False\n\n    def _invalidate_cache(self):\n        self._effective_permissions_cache.clear()\n\
          ```\n"
      pitfalls:
      - Role inheritance cycles cause infinite loops - validate DAG property
      - Cache invalidation needed when roles change - use versioning
      - Wildcard permissions (*) need careful handling to avoid over-granting
      - 'Role explosion: too many fine-grained roles become unmanageable'
    - name: ABAC Policy Engine
      description: Extend to attribute-based policies with conditions on user/resource/environment attributes
      skills:
      - Policy languages
      - Condition evaluation
      - Context propagation
      hints:
        level1: ABAC evaluates conditions like 'user.department == resource.owner_department'
        level2: 'Policies combine: allow if ANY policy allows, deny if ANY policy denies (deny wins)'
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Any, Callable\nfrom enum import Enum\n\
          import operator\n\nclass Effect(Enum):\n    ALLOW = \"allow\"\n    DENY = \"deny\"\n\nclass Operator(Enum):\n  \
          \  EQUALS = \"eq\"\n    NOT_EQUALS = \"neq\"\n    GREATER_THAN = \"gt\"\n    LESS_THAN = \"lt\"\n    IN = \"in\"\
          \n    NOT_IN = \"not_in\"\n    CONTAINS = \"contains\"\n    MATCHES = \"matches\"  # Regex\n\n@dataclass\nclass\
          \ Condition:\n    attribute: str      # e.g., \"user.department\", \"resource.classification\"\n    operator: Operator\n\
          \    value: Any\n\n    def evaluate(self, context: dict) -> bool:\n        # Navigate dot notation: \"user.department\"\
          \ -> context[\"user\"][\"department\"]\n        attr_value = self._get_nested(context, self.attribute)\n       \
          \ if attr_value is None:\n            return False\n\n        ops = {\n            Operator.EQUALS: lambda a, b:\
          \ a == b,\n            Operator.NOT_EQUALS: lambda a, b: a != b,\n            Operator.GREATER_THAN: lambda a, b:\
          \ a > b,\n            Operator.LESS_THAN: lambda a, b: a < b,\n            Operator.IN: lambda a, b: a in b,\n \
          \           Operator.NOT_IN: lambda a, b: a not in b,\n            Operator.CONTAINS: lambda a, b: b in a,\n   \
          \     }\n\n        return ops[self.operator](attr_value, self.value)\n\n    def _get_nested(self, obj: dict, path:\
          \ str) -> Any:\n        parts = path.split(\".\")\n        current = obj\n        for part in parts:\n         \
          \   if isinstance(current, dict):\n                current = current.get(part)\n            else:\n            \
          \    return None\n            if current is None:\n                return None\n        return current\n\n@dataclass\n\
          class Policy:\n    id: str\n    effect: Effect\n    resources: list[str]    # Patterns like \"documents/*\", \"\
          users/{self}\"\n    actions: list[str]\n    conditions: list[Condition] = field(default_factory=list)\n    priority:\
          \ int = 0       # Higher priority evaluated first\n\n    def matches(self, resource: str, action: str, context:\
          \ dict) -> tuple[bool, Effect]:\n        # Check resource pattern\n        if not self._matches_resource(resource,\
          \ context):\n            return False, None\n\n        # Check action\n        if action not in self.actions and\
          \ \"*\" not in self.actions:\n            return False, None\n\n        # Evaluate all conditions (AND logic)\n\
          \        for condition in self.conditions:\n            if not condition.evaluate(context):\n                return\
          \ False, None\n\n        return True, self.effect\n\n    def _matches_resource(self, resource: str, context: dict)\
          \ -> bool:\n        for pattern in self.resources:\n            # Handle {self} placeholder\n            if \"{self}\"\
          \ in pattern:\n                user_id = context.get(\"user\", {}).get(\"id\", \"\")\n                pattern =\
          \ pattern.replace(\"{self}\", user_id)\n\n            # Simple glob matching\n            if pattern == \"*\" or\
          \ pattern == resource:\n                return True\n            if pattern.endswith(\"/*\"):\n                prefix\
          \ = pattern[:-2]\n                if resource.startswith(prefix):\n                    return True\n        return\
          \ False\n\nclass ABACEngine:\n    def __init__(self):\n        self.policies: list[Policy] = []\n\n    def add_policy(self,\
          \ policy: Policy):\n        self.policies.append(policy)\n        # Keep sorted by priority (descending)\n     \
          \   self.policies.sort(key=lambda p: p.priority, reverse=True)\n\n    def evaluate(self, resource: str, action:\
          \ str, context: dict) -> bool:\n        '''\n        Evaluate all policies. Logic:\n        1. If any DENY policy\
          \ matches -> deny\n        2. If any ALLOW policy matches -> allow\n        3. Default deny\n        '''\n     \
          \   allow_matched = False\n\n        for policy in self.policies:\n            matches, effect = policy.matches(resource,\
          \ action, context)\n            if matches:\n                if effect == Effect.DENY:\n                    return\
          \ False  # Deny wins immediately\n                elif effect == Effect.ALLOW:\n                    allow_matched\
          \ = True\n\n        return allow_matched  # Default deny if no allow matched\n\n# Example usage:\n# policy = Policy(\n\
          #     id=\"owner-full-access\",\n#     effect=Effect.ALLOW,\n#     resources=[\"documents/*\"],\n#     actions=[\"\
          *\"],\n#     conditions=[\n#         Condition(\"user.id\", Operator.EQUALS, \"resource.owner_id\")\n#     ]\n#\
          \ )\n```\n"
      pitfalls:
      - 'Deny-by-default: no matching policy = deny'
      - Explicit deny always wins over allow (principle of least privilege)
      - Context must include all attributes needed for evaluation
      - Policy ordering matters - define clear priority rules
    - name: Resource-Based & Multi-tenancy
      description: Implement resource-level permissions and tenant isolation for SaaS applications
      skills:
      - Multi-tenancy
      - Resource ownership
      - Tenant isolation
      hints:
        level1: Every resource belongs to a tenant; users can only access resources in their tenant
        level2: Cross-tenant access requires explicit sharing with capability-based tokens
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          import secrets\n\nclass ShareLevel(Enum):\n    VIEWER = \"viewer\"    # Read-only\n    EDITOR = \"editor\"    #\
          \ Read + Write\n    ADMIN = \"admin\"      # Read + Write + Share + Delete\n\n@dataclass\nclass Resource:\n    id:\
          \ str\n    tenant_id: str\n    owner_id: str\n    resource_type: str\n    shares: dict[str, ShareLevel] = field(default_factory=dict)\
          \  # user_id -> level\n\n@dataclass\nclass ShareLink:\n    token: str\n    resource_id: str\n    level: ShareLevel\n\
          \    expires_at: Optional[float] = None\n    max_uses: Optional[int] = None\n    uses: int = 0\n\nclass MultiTenantAuthz:\n\
          \    def __init__(self, rbac_engine: RBACEngine, abac_engine: ABACEngine):\n        self.rbac = rbac_engine\n  \
          \      self.abac = abac_engine\n        self.resources: dict[str, Resource] = {}\n        self.share_links: dict[str,\
          \ ShareLink] = {}\n        self.user_tenants: dict[str, str] = {}  # user_id -> tenant_id\n\n    def check_access(self,\
          \ user_id: str, resource_id: str, action: str) -> bool:\n        resource = self.resources.get(resource_id)\n  \
          \      if not resource:\n            return False\n\n        user_tenant = self.user_tenants.get(user_id)\n\n  \
          \      # 1. Check tenant isolation\n        if user_tenant != resource.tenant_id:\n            # Cross-tenant -\
          \ only allowed via explicit share\n            return self._check_share(user_id, resource, action)\n\n        #\
          \ 2. Check if owner (owners have full access)\n        if resource.owner_id == user_id:\n            return True\n\
          \n        # 3. Check direct share\n        if self._check_share(user_id, resource, action):\n            return\
          \ True\n\n        # 4. Check RBAC (tenant-scoped roles)\n        context = {\n            \"user\": {\"id\": user_id,\
          \ \"tenant_id\": user_tenant},\n            \"resource\": {\n                \"id\": resource_id,\n            \
          \    \"tenant_id\": resource.tenant_id,\n                \"owner_id\": resource.owner_id,\n                \"type\"\
          : resource.resource_type\n            }\n        }\n\n        if self.abac.evaluate(f\"{resource.resource_type}/{resource_id}\"\
          , action, context):\n            return True\n\n        return False\n\n    def _check_share(self, user_id: str,\
          \ resource: Resource, action: str) -> bool:\n        share_level = resource.shares.get(user_id)\n        if not\
          \ share_level:\n            return False\n\n        action_requirements = {\n            \"read\": [ShareLevel.VIEWER,\
          \ ShareLevel.EDITOR, ShareLevel.ADMIN],\n            \"write\": [ShareLevel.EDITOR, ShareLevel.ADMIN],\n       \
          \     \"delete\": [ShareLevel.ADMIN],\n            \"share\": [ShareLevel.ADMIN]\n        }\n\n        required\
          \ = action_requirements.get(action, [ShareLevel.ADMIN])\n        return share_level in required\n\n    def create_share_link(self,\
          \ user_id: str, resource_id: str,\n                          level: ShareLevel, expires_in: int = None,\n      \
          \                    max_uses: int = None) -> str:\n        # Verify user can share\n        if not self.check_access(user_id,\
          \ resource_id, \"share\"):\n            raise PermissionError(\"Cannot share this resource\")\n\n        token =\
          \ secrets.token_urlsafe(32)\n        expires_at = time.time() + expires_in if expires_in else None\n\n        self.share_links[token]\
          \ = ShareLink(\n            token=token,\n            resource_id=resource_id,\n            level=level,\n     \
          \       expires_at=expires_at,\n            max_uses=max_uses\n        )\n\n        return token\n\n    def redeem_share_link(self,\
          \ token: str, user_id: str) -> bool:\n        link = self.share_links.get(token)\n        if not link:\n       \
          \     return False\n\n        # Check expiry\n        if link.expires_at and time.time() > link.expires_at:\n  \
          \          del self.share_links[token]\n            return False\n\n        # Check uses\n        if link.max_uses\
          \ and link.uses >= link.max_uses:\n            return False\n\n        # Grant access\n        resource = self.resources.get(link.resource_id)\n\
          \        if resource:\n            resource.shares[user_id] = link.level\n            link.uses += 1\n         \
          \   return True\n\n        return False\n```\n"
      pitfalls:
      - Tenant isolation must be enforced at database query level too
      - Admin roles should be tenant-scoped, not global
      - Share links need expiry and use limits to prevent abuse
      - Revoking share should be immediate - no caching of permissions
    - name: Audit Logging & Policy Testing
      description: Implement comprehensive audit logging and policy simulation for testing
      skills:
      - Security auditing
      - Policy testing
      - Compliance
      hints:
        level1: Log every authorization decision with full context for forensics
        level2: Policy simulation lets admins test 'what if' scenarios before deploying
        level3: "\n```python\nfrom dataclasses import dataclass, asdict\nfrom typing import Optional\nfrom datetime import\
          \ datetime\nimport json\n\n@dataclass\nclass AuthzDecision:\n    timestamp: datetime\n    user_id: str\n    resource:\
          \ str\n    action: str\n    decision: bool\n    policies_evaluated: list[str]\n    matching_policy: Optional[str]\n\
          \    context: dict\n    latency_ms: float\n\nclass AuthzAuditLog:\n    def __init__(self):\n        self.decisions:\
          \ list[AuthzDecision] = []\n\n    def log_decision(self, decision: AuthzDecision):\n        self.decisions.append(decision)\n\
          \        # In production: send to SIEM, write to immutable log\n        print(json.dumps({\n            \"type\"\
          : \"authz_decision\",\n            \"timestamp\": decision.timestamp.isoformat(),\n            \"user\": decision.user_id,\n\
          \            \"resource\": decision.resource,\n            \"action\": decision.action,\n            \"allowed\"\
          : decision.decision,\n            \"policy\": decision.matching_policy\n        }))\n\n    def query(self, user_id:\
          \ str = None, resource: str = None,\n              action: str = None, decision: bool = None,\n              start_time:\
          \ datetime = None, end_time: datetime = None) -> list[AuthzDecision]:\n        results = self.decisions\n\n    \
          \    if user_id:\n            results = [d for d in results if d.user_id == user_id]\n        if resource:\n   \
          \         results = [d for d in results if resource in d.resource]\n        if action:\n            results = [d\
          \ for d in results if d.action == action]\n        if decision is not None:\n            results = [d for d in results\
          \ if d.decision == decision]\n        if start_time:\n            results = [d for d in results if d.timestamp >=\
          \ start_time]\n        if end_time:\n            results = [d for d in results if d.timestamp <= end_time]\n\n \
          \       return results\n\nclass PolicySimulator:\n    def __init__(self, abac_engine: ABACEngine):\n        self.abac\
          \ = abac_engine\n\n    def simulate(self, policies: list[Policy], test_cases: list[dict]) -> list[dict]:\n     \
          \   '''\n        Run policies against test cases without affecting production.\n\n        test_cases format:\n \
          \       [\n            {\n                \"description\": \"Owner can read own document\",\n                \"\
          resource\": \"documents/123\",\n                \"action\": \"read\",\n                \"context\": {\"user\": {\"\
          id\": \"user1\"}, \"resource\": {\"owner_id\": \"user1\"}},\n                \"expected\": True\n            }\n\
          \        ]\n        '''\n        # Create isolated engine for simulation\n        sim_engine = ABACEngine()\n  \
          \      for policy in policies:\n            sim_engine.add_policy(policy)\n\n        results = []\n        for case\
          \ in test_cases:\n            actual = sim_engine.evaluate(\n                case[\"resource\"],\n             \
          \   case[\"action\"],\n                case[\"context\"]\n            )\n\n            results.append({\n      \
          \          \"description\": case[\"description\"],\n                \"passed\": actual == case[\"expected\"],\n\
          \                \"expected\": case[\"expected\"],\n                \"actual\": actual,\n                \"resource\"\
          : case[\"resource\"],\n                \"action\": case[\"action\"]\n            })\n\n        return results\n\n\
          \    def find_violations(self, policies: list[Policy],\n                        security_invariants: list[dict])\
          \ -> list[dict]:\n        '''\n        Check policies against security invariants.\n\n        invariants format:\n\
          \        [\n            {\n                \"description\": \"No user can delete audit logs\",\n               \
          \ \"resource_pattern\": \"audit-logs/*\",\n                \"action\": \"delete\",\n                \"should_be\"\
          : \"denied\",\n                \"for_any_context\": True\n            }\n        ]\n        '''\n        violations\
          \ = []\n\n        for invariant in security_invariants:\n            # Generate test contexts\n            test_contexts\
          \ = self._generate_test_contexts(invariant)\n\n            for context in test_contexts:\n                result\
          \ = self.simulate(policies, [{\n                    \"description\": invariant[\"description\"],\n             \
          \       \"resource\": invariant[\"resource_pattern\"].replace(\"*\", \"test\"),\n                    \"action\"\
          : invariant[\"action\"],\n                    \"context\": context,\n                    \"expected\": invariant[\"\
          should_be\"] == \"denied\"\n                }])\n\n                if not result[0][\"passed\"]:\n             \
          \       violations.append({\n                        \"invariant\": invariant[\"description\"],\n              \
          \          \"context\": context,\n                        \"expected\": invariant[\"should_be\"],\n            \
          \            \"actual\": \"allowed\" if result[0][\"actual\"] else \"denied\"\n                    })\n\n      \
          \  return violations\n\n    def _generate_test_contexts(self, invariant: dict) -> list[dict]:\n        # Generate\
          \ various contexts to test the invariant\n        return [\n            {\"user\": {\"id\": \"admin\", \"role\"\
          : \"admin\"}},\n            {\"user\": {\"id\": \"user\", \"role\": \"user\"}},\n            {\"user\": {\"id\"\
          : \"owner\", \"role\": \"owner\"}, \"resource\": {\"owner_id\": \"owner\"}},\n        ]\n```\n"
      pitfalls:
      - Audit logs must be immutable - use append-only storage
      - Include enough context to understand decision without leaking secrets
      - Simulation environment must match production exactly
      - Test negative cases (should be denied) not just positive
  secret-management:
    name: Secret Management System
    description: Build a Vault-like secret management system with encryption, access control, dynamic secrets, and audit logging.
    why_expert: Hardcoded secrets cause breaches. Understanding secret management helps design secure systems and properly
      integrate with existing solutions.
    difficulty: expert
    tags:
    - security
    - secrets
    - encryption
    - vault
    - infrastructure
    estimated_hours: 45
    prerequisites:
    - build-http-server
    milestones:
    - name: Encrypted Secret Storage
      description: Implement envelope encryption with master key and data encryption keys
      skills:
      - Envelope encryption
      - Key management
      - Secure storage
      hints:
        level1: Master Key encrypts Data Encryption Keys (DEKs). DEKs encrypt actual secrets.
        level2: Never store master key with encrypted data. Use HSM or external KMS in production.
        level3: "\n```python\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import\
          \ PBKDF2HMAC\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.backends import default_backend\n\
          import base64\nimport os\nimport json\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport time\n\
          \n@dataclass\nclass EncryptedSecret:\n    encrypted_dek: bytes      # DEK encrypted with master key\n    encrypted_value:\
          \ bytes    # Value encrypted with DEK\n    created_at: float\n    version: int\n    metadata: dict\n\nclass SecretStore:\n\
          \    def __init__(self, master_key: bytes = None):\n        # In production: master key from HSM, Kubernetes secret,\
          \ or cloud KMS\n        if master_key:\n            self.master_key = master_key\n        else:\n            # Derive\
          \ from password for demo (use proper KMS in prod!)\n            self.master_key = self._derive_key(b\"demo-password\"\
          , b\"salt\")\n\n        self.master_fernet = Fernet(base64.urlsafe_b64encode(self.master_key))\n        self.secrets:\
          \ dict[str, list[EncryptedSecret]] = {}  # path -> versions\n\n    def _derive_key(self, password: bytes, salt:\
          \ bytes) -> bytes:\n        kdf = PBKDF2HMAC(\n            algorithm=hashes.SHA256(),\n            length=32,\n\
          \            salt=salt,\n            iterations=480000,\n            backend=default_backend()\n        )\n    \
          \    return kdf.derive(password)\n\n    def _generate_dek(self) -> tuple[bytes, Fernet]:\n        '''Generate new\
          \ Data Encryption Key'''\n        dek = Fernet.generate_key()\n        return dek, Fernet(dek)\n\n    def put(self,\
          \ path: str, value: str, metadata: dict = None) -> int:\n        '''Store a secret with envelope encryption'''\n\
          \        # Generate new DEK for this secret\n        dek, dek_fernet = self._generate_dek()\n\n        # Encrypt\
          \ value with DEK\n        encrypted_value = dek_fernet.encrypt(value.encode())\n\n        # Encrypt DEK with master\
          \ key\n        encrypted_dek = self.master_fernet.encrypt(dek)\n\n        # Determine version\n        existing\
          \ = self.secrets.get(path, [])\n        version = len(existing) + 1\n\n        secret = EncryptedSecret(\n     \
          \       encrypted_dek=encrypted_dek,\n            encrypted_value=encrypted_value,\n            created_at=time.time(),\n\
          \            version=version,\n            metadata=metadata or {}\n        )\n\n        if path not in self.secrets:\n\
          \            self.secrets[path] = []\n        self.secrets[path].append(secret)\n\n        return version\n\n  \
          \  def get(self, path: str, version: int = None) -> Optional[str]:\n        '''Retrieve and decrypt a secret'''\n\
          \        versions = self.secrets.get(path)\n        if not versions:\n            return None\n\n        # Get requested\
          \ version or latest\n        if version:\n            if version < 1 or version > len(versions):\n             \
          \   return None\n            secret = versions[version - 1]\n        else:\n            secret = versions[-1]\n\n\
          \        # Decrypt DEK with master key\n        dek = self.master_fernet.decrypt(secret.encrypted_dek)\n       \
          \ dek_fernet = Fernet(dek)\n\n        # Decrypt value with DEK\n        value = dek_fernet.decrypt(secret.encrypted_value)\n\
          \        return value.decode()\n\n    def rotate_master_key(self, new_master_key: bytes):\n        '''Re-encrypt\
          \ all DEKs with new master key'''\n        new_master_fernet = Fernet(base64.urlsafe_b64encode(new_master_key))\n\
          \n        for path, versions in self.secrets.items():\n            for secret in versions:\n                # Decrypt\
          \ DEK with old key\n                dek = self.master_fernet.decrypt(secret.encrypted_dek)\n                # Re-encrypt\
          \ with new key\n                secret.encrypted_dek = new_master_fernet.encrypt(dek)\n\n        self.master_key\
          \ = new_master_key\n        self.master_fernet = new_master_fernet\n```\n"
      pitfalls:
      - Master key in memory can be extracted - consider memory encryption
      - Secret versioning needed for rotation without breaking consumers
      - Backup encrypted data AND keys separately (but both needed for recovery)
      - 'Key derivation: use high iteration count (480k+) for PBKDF2'
    - name: Access Policies & Authentication
      description: Implement path-based ACLs and multiple authentication methods (token, AppRole, mTLS)
      skills:
      - Path-based ACLs
      - Authentication methods
      - Token management
      hints:
        level1: Policies define what paths a token can access with what capabilities
        level2: 'AppRole: role_id (public) + secret_id (private) = token for apps'
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          import secrets\nimport hashlib\nimport time\nimport fnmatch\n\nclass Capability(Enum):\n    CREATE = \"create\"\n\
          \    READ = \"read\"\n    UPDATE = \"update\"\n    DELETE = \"delete\"\n    LIST = \"list\"\n    SUDO = \"sudo\"\
          \     # Override policies\n    DENY = \"deny\"     # Explicit deny\n\n@dataclass\nclass Policy:\n    name: str\n\
          \    path_rules: dict[str, set[Capability]]  # path pattern -> capabilities\n\n    def check(self, path: str, capability:\
          \ Capability) -> bool:\n        for pattern, caps in self.path_rules.items():\n            if fnmatch.fnmatch(path,\
          \ pattern):\n                if Capability.DENY in caps:\n                    return False\n                if capability\
          \ in caps or Capability.SUDO in caps:\n                    return True\n        return False\n\n@dataclass\nclass\
          \ Token:\n    id: str\n    policies: list[str]\n    created_at: float\n    expires_at: Optional[float]\n    renewable:\
          \ bool\n    metadata: dict = field(default_factory=dict)\n\n@dataclass\nclass AppRole:\n    role_id: str\n    secret_id_hash:\
          \ str\n    policies: list[str]\n    token_ttl: int = 3600\n    secret_id_ttl: Optional[int] = None\n    secret_id_num_uses:\
          \ Optional[int] = None\n    bind_secret_id: bool = True\n\nclass AuthManager:\n    def __init__(self):\n       \
          \ self.policies: dict[str, Policy] = {}\n        self.tokens: dict[str, Token] = {}\n        self.app_roles: dict[str,\
          \ AppRole] = {}\n        self.secret_id_uses: dict[str, int] = {}  # secret_id_hash -> uses\n\n    def create_policy(self,\
          \ name: str, rules: dict[str, list[str]]) -> Policy:\n        path_rules = {}\n        for path, caps in rules.items():\n\
          \            path_rules[path] = {Capability(c) for c in caps}\n\n        policy = Policy(name=name, path_rules=path_rules)\n\
          \        self.policies[name] = policy\n        return policy\n\n    def create_token(self, policies: list[str],\
          \ ttl: int = 3600,\n                     renewable: bool = True, metadata: dict = None) -> str:\n        token_id\
          \ = \"hvs.\" + secrets.token_urlsafe(32)\n\n        token = Token(\n            id=token_id,\n            policies=policies,\n\
          \            created_at=time.time(),\n            expires_at=time.time() + ttl if ttl else None,\n            renewable=renewable,\n\
          \            metadata=metadata or {}\n        )\n\n        self.tokens[token_id] = token\n        return token_id\n\
          \n    def validate_token(self, token_id: str) -> Optional[Token]:\n        token = self.tokens.get(token_id)\n \
          \       if not token:\n            return None\n        if token.expires_at and time.time() > token.expires_at:\n\
          \            del self.tokens[token_id]\n            return None\n        return token\n\n    def check_permission(self,\
          \ token_id: str, path: str, capability: Capability) -> bool:\n        token = self.validate_token(token_id)\n  \
          \      if not token:\n            return False\n\n        for policy_name in token.policies:\n            policy\
          \ = self.policies.get(policy_name)\n            if policy and policy.check(path, capability):\n                return\
          \ True\n        return False\n\n    # AppRole authentication\n    def create_app_role(self, name: str, policies:\
          \ list[str],\n                        token_ttl: int = 3600) -> str:\n        role_id = secrets.token_urlsafe(16)\n\
          \n        self.app_roles[name] = AppRole(\n            role_id=role_id,\n            secret_id_hash=\"\",  # No\
          \ secret yet\n            policies=policies,\n            token_ttl=token_ttl\n        )\n\n        return role_id\n\
          \n    def generate_secret_id(self, role_name: str) -> str:\n        role = self.app_roles.get(role_name)\n     \
          \   if not role:\n            raise ValueError(\"Role not found\")\n\n        secret_id = secrets.token_urlsafe(32)\n\
          \        role.secret_id_hash = hashlib.sha256(secret_id.encode()).hexdigest()\n\n        return secret_id\n\n  \
          \  def login_approle(self, role_id: str, secret_id: str) -> Optional[str]:\n        '''Authenticate with AppRole,\
          \ return token'''\n        # Find role by role_id\n        role = None\n        for r in self.app_roles.values():\n\
          \            if r.role_id == role_id:\n                role = r\n                break\n\n        if not role:\n\
          \            return None\n\n        # Verify secret_id\n        secret_hash = hashlib.sha256(secret_id.encode()).hexdigest()\n\
          \        if not secrets.compare_digest(secret_hash, role.secret_id_hash):\n            return None\n\n        #\
          \ Check secret_id uses\n        if role.secret_id_num_uses:\n            uses = self.secret_id_uses.get(secret_hash,\
          \ 0)\n            if uses >= role.secret_id_num_uses:\n                return None\n            self.secret_id_uses[secret_hash]\
          \ = uses + 1\n\n        # Issue token\n        return self.create_token(\n            policies=role.policies,\n\
          \            ttl=role.token_ttl,\n            metadata={\"auth_method\": \"approle\"}\n        )\n```\n"
      pitfalls:
      - role_id can be embedded in code; secret_id must be delivered securely
      - Token lookup table grows unbounded - need periodic cleanup
      - 'Glob patterns in paths: ensure * doesn''t match too broadly'
      - Constant-time comparison for secret validation prevents timing attacks
    - name: Dynamic Secrets
      description: Generate short-lived credentials on-demand for databases, cloud providers, etc.
      skills:
      - Dynamic credentials
      - Lease management
      - Secret rotation
      hints:
        level1: Dynamic secrets are generated per-request with automatic expiration
        level2: 'Lease: tracks TTL and allows renewal or revocation'
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Callable, Any, Optional\nfrom abc import\
          \ ABC, abstractmethod\nimport secrets\nimport string\nimport time\nimport threading\n\n@dataclass\nclass Lease:\n\
          \    lease_id: str\n    secret_type: str\n    ttl: int\n    renewable: bool\n    created_at: float\n    expires_at:\
          \ float\n    data: dict  # Actual credentials\n    revoke_callback: Callable[[], None]\n\nclass SecretEngine(ABC):\n\
          \    @abstractmethod\n    def generate(self, role: str) -> tuple[dict, Callable[[], None]]:\n        '''Generate\
          \ credentials and return (creds, revoke_fn)'''\n        pass\n\nclass PostgresSecretEngine(SecretEngine):\n    def\
          \ __init__(self, conn_string: str):\n        self.conn_string = conn_string\n\n    def generate(self, role: str)\
          \ -> tuple[dict, Callable[[], None]]:\n        # Generate random username/password\n        username = f\"v-{role}-{secrets.token_hex(4)}\"\
          \n        password = ''.join(secrets.choice(string.ascii_letters + string.digits)\n                          for\
          \ _ in range(32))\n\n        # In production: actually create the user in Postgres\n        # CREATE USER {username}\
          \ WITH PASSWORD '{password}';\n        # GRANT {role} TO {username};\n\n        def revoke():\n            # DROP\
          \ USER {username};\n            print(f\"Revoking Postgres user: {username}\")\n\n        return {\n           \
          \ \"username\": username,\n            \"password\": password,\n            \"connection_string\": f\"postgresql://{username}:{password}@...\"\
          \n        }, revoke\n\nclass AWSSecretEngine(SecretEngine):\n    def __init__(self, access_key: str, secret_key:\
          \ str):\n        self.access_key = access_key\n        self.secret_key = secret_key\n\n    def generate(self, role:\
          \ str) -> tuple[dict, Callable[[], None]]:\n        # In production: use AWS STS AssumeRole or IAM CreateAccessKey\n\
          \        temp_access_key = f\"AKIA{secrets.token_hex(8).upper()}\"\n        temp_secret_key = secrets.token_urlsafe(32)\n\
          \n        def revoke():\n            # IAM DeleteAccessKey\n            print(f\"Revoking AWS key: {temp_access_key}\"\
          )\n\n        return {\n            \"access_key\": temp_access_key,\n            \"secret_key\": temp_secret_key,\n\
          \            \"session_token\": None  # For STS\n        }, revoke\n\nclass LeaseManager:\n    def __init__(self):\n\
          \        self.leases: dict[str, Lease] = {}\n        self.engines: dict[str, SecretEngine] = {}\n        self._start_reaper()\n\
          \n    def register_engine(self, name: str, engine: SecretEngine):\n        self.engines[name] = engine\n\n    def\
          \ generate(self, engine_name: str, role: str, ttl: int = 3600) -> Lease:\n        engine = self.engines.get(engine_name)\n\
          \        if not engine:\n            raise ValueError(f\"Unknown engine: {engine_name}\")\n\n        creds, revoke_fn\
          \ = engine.generate(role)\n\n        lease_id = f\"{engine_name}/{role}/{secrets.token_hex(8)}\"\n        now =\
          \ time.time()\n\n        lease = Lease(\n            lease_id=lease_id,\n            secret_type=engine_name,\n\
          \            ttl=ttl,\n            renewable=True,\n            created_at=now,\n            expires_at=now + ttl,\n\
          \            data=creds,\n            revoke_callback=revoke_fn\n        )\n\n        self.leases[lease_id] = lease\n\
          \        return lease\n\n    def renew(self, lease_id: str, increment: int = None) -> Optional[Lease]:\n       \
          \ lease = self.leases.get(lease_id)\n        if not lease or not lease.renewable:\n            return None\n\n \
          \       # Extend TTL\n        extension = increment or lease.ttl\n        lease.expires_at = time.time() + extension\n\
          \        return lease\n\n    def revoke(self, lease_id: str) -> bool:\n        lease = self.leases.get(lease_id)\n\
          \        if not lease:\n            return False\n\n        # Call revoke callback (delete user, invalidate key,\
          \ etc.)\n        try:\n            lease.revoke_callback()\n        except Exception as e:\n            print(f\"\
          Error revoking {lease_id}: {e}\")\n\n        del self.leases[lease_id]\n        return True\n\n    def _start_reaper(self):\n\
          \        '''Background thread to revoke expired leases'''\n        def reap():\n            while True:\n      \
          \          time.sleep(60)\n                now = time.time()\n                expired = [lid for lid, l in self.leases.items()\n\
          \                          if l.expires_at < now]\n                for lease_id in expired:\n                  \
          \  self.revoke(lease_id)\n\n        thread = threading.Thread(target=reap, daemon=True)\n        thread.start()\n\
          ```\n"
      pitfalls:
      - Lease reaper must handle engine failures gracefully
      - Max TTL should be enforced even for renewals
      - Revocation can fail - need retry logic and alerting
      - 'Connection pooling: don''t create new DB user for every request'
    - name: Unsealing & High Availability
      description: Implement Shamir's secret sharing for master key unsealing and HA replication
      skills:
      - Shamir's secret sharing
      - Consensus
      - Replication
      hints:
        level1: Split master key into N shares; require K shares to reconstruct (K-of-N)
        level2: 'Sealed state: encrypted data exists but can''t be decrypted without unsealing'
        level3: "\n```python\nfrom typing import List, Tuple\nimport secrets\nfrom functools import reduce\n\nclass ShamirSecretSharing:\n\
          \    '''\n    Shamir's Secret Sharing Scheme\n    Split secret into n shares, requiring k shares to reconstruct\n\
          \    '''\n    PRIME = 2**127 - 1  # Mersenne prime for finite field\n\n    @classmethod\n    def split(cls, secret:\
          \ int, k: int, n: int) -> List[Tuple[int, int]]:\n        '''Split secret into n shares with threshold k'''\n  \
          \      if k > n:\n            raise ValueError(\"Threshold cannot exceed total shares\")\n\n        # Generate random\
          \ polynomial coefficients\n        # f(x) = secret + a1*x + a2*x^2 + ... + a(k-1)*x^(k-1)\n        coefficients\
          \ = [secret] + [secrets.randbelow(cls.PRIME) for _ in range(k - 1)]\n\n        # Generate shares: (x, f(x)) for\
          \ x = 1, 2, ..., n\n        shares = []\n        for x in range(1, n + 1):\n            y = cls._evaluate_polynomial(coefficients,\
          \ x)\n            shares.append((x, y))\n\n        return shares\n\n    @classmethod\n    def reconstruct(cls, shares:\
          \ List[Tuple[int, int]]) -> int:\n        '''Reconstruct secret from k shares using Lagrange interpolation'''\n\
          \        k = len(shares)\n        secret = 0\n\n        for i, (xi, yi) in enumerate(shares):\n            # Compute\
          \ Lagrange basis polynomial at x=0\n            numerator = 1\n            denominator = 1\n\n            for j,\
          \ (xj, _) in enumerate(shares):\n                if i != j:\n                    numerator = (numerator * (-xj))\
          \ % cls.PRIME\n                    denominator = (denominator * (xi - xj)) % cls.PRIME\n\n            # Modular\
          \ multiplicative inverse\n            lagrange = (yi * numerator * pow(denominator, -1, cls.PRIME)) % cls.PRIME\n\
          \            secret = (secret + lagrange) % cls.PRIME\n\n        return secret\n\n    @classmethod\n    def _evaluate_polynomial(cls,\
          \ coefficients: List[int], x: int) -> int:\n        result = 0\n        for i, coef in enumerate(coefficients):\n\
          \            result = (result + coef * pow(x, i, cls.PRIME)) % cls.PRIME\n        return result\n\nclass SealableVault:\n\
          \    def __init__(self, threshold: int = 3, shares: int = 5):\n        self.threshold = threshold\n        self.total_shares\
          \ = shares\n        self.sealed = True\n        self.master_key: bytes = None\n        self.unseal_shares: List[Tuple[int,\
          \ int]] = []\n        self.secret_store: SecretStore = None\n\n    def initialize(self) -> List[bytes]:\n      \
          \  '''Initialize vault, return key shares (distribute to operators)'''\n        # Generate master key\n        master_key_int\
          \ = int.from_bytes(secrets.token_bytes(16), 'big')\n\n        # Split using Shamir\n        shares = ShamirSecretSharing.split(\n\
          \            master_key_int, self.threshold, self.total_shares\n        )\n\n        # Encode shares for distribution\n\
          \        encoded_shares = []\n        for x, y in shares:\n            # Encode as \"x:y\" in hex\n            share_bytes\
          \ = f\"{x}:{y:032x}\".encode()\n            encoded_shares.append(share_bytes)\n\n        # Store encrypted version\
          \ of master key (for verification later)\n        self._master_key_hash = hashlib.sha256(\n            master_key_int.to_bytes(16,\
          \ 'big')\n        ).hexdigest()\n\n        return encoded_shares\n\n    def unseal(self, share: bytes) -> bool:\n\
          \        '''Provide an unseal key. Returns True when vault is unsealed.'''\n        if not self.sealed:\n      \
          \      return True\n\n        # Decode share\n        parts = share.decode().split(':')\n        x = int(parts[0])\n\
          \        y = int(parts[1], 16)\n\n        # Add to collected shares (avoid duplicates)\n        if not any(s[0]\
          \ == x for s in self.unseal_shares):\n            self.unseal_shares.append((x, y))\n\n        # Try to reconstruct\
          \ if we have enough shares\n        if len(self.unseal_shares) >= self.threshold:\n            master_key_int =\
          \ ShamirSecretSharing.reconstruct(\n                self.unseal_shares[:self.threshold]\n            )\n       \
          \     master_key = master_key_int.to_bytes(16, 'big')\n\n            # Verify\n            if hashlib.sha256(master_key).hexdigest()\
          \ == self._master_key_hash:\n                self.master_key = master_key\n                self.secret_store = SecretStore(master_key)\n\
          \                self.sealed = False\n                self.unseal_shares = []  # Clear shares from memory\n    \
          \            return True\n            else:\n                # Invalid reconstruction - wrong shares\n         \
          \       self.unseal_shares = []\n                raise ValueError(\"Invalid unseal keys\")\n\n        return False\n\
          \n    def seal(self):\n        '''Seal the vault - clear master key from memory'''\n        self.master_key = None\n\
          \        self.secret_store = None\n        self.sealed = True\n```\n"
      pitfalls:
      - Never store all shares together - defeats the purpose
      - Clear shares from memory after reconstruction
      - Sealed state must reject all secret operations
      - 'HA: only one node should be active writer to prevent split-brain'
  session-management:
    name: Distributed Session Management
    description: Build a production-grade session management system with distributed storage, secure cookies, and session
      fixation prevention.
    why_expert: Session management bugs (fixation, hijacking) are common vulnerabilities. Understanding internals helps prevent
      security issues.
    difficulty: advanced
    tags:
    - security
    - sessions
    - distributed
    - cookies
    - authentication
    estimated_hours: 30
    prerequisites:
    - build-redis
    milestones:
    - name: Secure Session Creation & Storage
      description: Implement cryptographically secure session IDs with distributed storage backend
      skills:
      - Session security
      - Distributed storage
      - Cookie handling
      hints:
        level1: Session ID must be cryptographically random (128+ bits of entropy)
        level2: Store session data server-side, only session ID in cookie
        level3: "\n```python\nimport secrets\nimport hashlib\nimport time\nimport json\nfrom dataclasses import dataclass,\
          \ field\nfrom typing import Optional, Any\nfrom abc import ABC, abstractmethod\n\n@dataclass\nclass Session:\n \
          \   id: str\n    user_id: Optional[str]\n    data: dict\n    created_at: float\n    last_accessed: float\n    expires_at:\
          \ float\n    ip_address: str\n    user_agent: str\n\nclass SessionStore(ABC):\n    @abstractmethod\n    def save(self,\
          \ session: Session) -> None: pass\n\n    @abstractmethod\n    def load(self, session_id: str) -> Optional[Session]:\
          \ pass\n\n    @abstractmethod\n    def delete(self, session_id: str) -> None: pass\n\n    @abstractmethod\n    def\
          \ delete_user_sessions(self, user_id: str) -> int: pass\n\nclass RedisSessionStore(SessionStore):\n    def __init__(self,\
          \ redis_client, prefix: str = \"session:\"):\n        self.redis = redis_client\n        self.prefix = prefix\n\n\
          \    def save(self, session: Session) -> None:\n        key = f\"{self.prefix}{session.id}\"\n        ttl = int(session.expires_at\
          \ - time.time())\n\n        data = {\n            \"id\": session.id,\n            \"user_id\": session.user_id,\n\
          \            \"data\": session.data,\n            \"created_at\": session.created_at,\n            \"last_accessed\"\
          : session.last_accessed,\n            \"expires_at\": session.expires_at,\n            \"ip_address\": session.ip_address,\n\
          \            \"user_agent\": session.user_agent\n        }\n\n        self.redis.setex(key, ttl, json.dumps(data))\n\
          \n        # Index by user for \"logout all devices\"\n        if session.user_id:\n            user_key = f\"{self.prefix}user:{session.user_id}\"\
          \n            self.redis.sadd(user_key, session.id)\n            self.redis.expire(user_key, ttl)\n\n    def load(self,\
          \ session_id: str) -> Optional[Session]:\n        key = f\"{self.prefix}{session_id}\"\n        data = self.redis.get(key)\n\
          \        if not data:\n            return None\n\n        d = json.loads(data)\n        return Session(**d)\n\n\
          \    def delete(self, session_id: str) -> None:\n        key = f\"{self.prefix}{session_id}\"\n        self.redis.delete(key)\n\
          \n    def delete_user_sessions(self, user_id: str) -> int:\n        user_key = f\"{self.prefix}user:{user_id}\"\n\
          \        session_ids = self.redis.smembers(user_key)\n\n        for sid in session_ids:\n            self.delete(sid.decode()\
          \ if isinstance(sid, bytes) else sid)\n\n        self.redis.delete(user_key)\n        return len(session_ids)\n\n\
          class SessionManager:\n    def __init__(self, store: SessionStore,\n                 session_ttl: int = 86400, \
          \ # 24 hours\n                 idle_timeout: int = 1800):  # 30 minutes\n        self.store = store\n        self.session_ttl\
          \ = session_ttl\n        self.idle_timeout = idle_timeout\n\n    def create_session(self, ip_address: str, user_agent:\
          \ str,\n                       user_id: str = None) -> Session:\n        # Generate cryptographically secure session\
          \ ID\n        session_id = secrets.token_urlsafe(32)  # 256 bits\n\n        now = time.time()\n        session =\
          \ Session(\n            id=session_id,\n            user_id=user_id,\n            data={},\n            created_at=now,\n\
          \            last_accessed=now,\n            expires_at=now + self.session_ttl,\n            ip_address=ip_address,\n\
          \            user_agent=user_agent\n        )\n\n        self.store.save(session)\n        return session\n\n  \
          \  def get_session(self, session_id: str, ip_address: str = None) -> Optional[Session]:\n        session = self.store.load(session_id)\n\
          \        if not session:\n            return None\n\n        now = time.time()\n\n        # Check absolute expiry\n\
          \        if now > session.expires_at:\n            self.store.delete(session_id)\n            return None\n\n  \
          \      # Check idle timeout\n        if now - session.last_accessed > self.idle_timeout:\n            self.store.delete(session_id)\n\
          \            return None\n\n        # Optional: IP binding for high-security applications\n        # if ip_address\
          \ and session.ip_address != ip_address:\n        #     return None  # Session hijacking attempt?\n\n        # Update\
          \ last accessed\n        session.last_accessed = now\n        self.store.save(session)\n\n        return session\n\
          \n    def regenerate_id(self, old_session_id: str) -> Optional[Session]:\n        '''Regenerate session ID (call\
          \ after login to prevent fixation)'''\n        session = self.store.load(old_session_id)\n        if not session:\n\
          \            return None\n\n        # Delete old session\n        self.store.delete(old_session_id)\n\n        #\
          \ Create new session with same data but new ID\n        session.id = secrets.token_urlsafe(32)\n        session.last_accessed\
          \ = time.time()\n\n        self.store.save(session)\n        return session\n```\n"
      pitfalls:
      - 'Session fixation: ALWAYS regenerate session ID after login'
      - Session ID in URL is insecure (referer leaks, logs) - use cookies only
      - Idle timeout and absolute timeout are different - need both
      - 'Race condition: concurrent requests can cause session data loss'
    - name: Cookie Security & Transport
      description: Implement secure cookie handling with proper flags and encryption
      skills:
      - Cookie security
      - CSRF prevention
      - Secure transport
      hints:
        level1: Set Secure, HttpOnly, SameSite flags on session cookies
        level2: Sign cookies to detect tampering; encrypt if storing data in cookie
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport hmac\nimport hashlib\n\
          import base64\nimport time\nimport json\nfrom cryptography.fernet import Fernet\n\n@dataclass\nclass CookieOptions:\n\
          \    secure: bool = True          # HTTPS only\n    http_only: bool = True       # No JavaScript access\n    same_site:\
          \ str = \"Lax\"       # CSRF protection: Strict, Lax, None\n    domain: Optional[str] = None\n    path: str = \"\
          /\"\n    max_age: Optional[int] = None  # Seconds until expiry\n\n    def to_header_string(self, name: str, value:\
          \ str) -> str:\n        parts = [f\"{name}={value}\"]\n\n        if self.max_age is not None:\n            parts.append(f\"\
          Max-Age={self.max_age}\")\n        if self.domain:\n            parts.append(f\"Domain={self.domain}\")\n      \
          \  parts.append(f\"Path={self.path}\")\n        if self.secure:\n            parts.append(\"Secure\")\n        if\
          \ self.http_only:\n            parts.append(\"HttpOnly\")\n        parts.append(f\"SameSite={self.same_site}\")\n\
          \n        return \"; \".join(parts)\n\nclass SecureCookieManager:\n    def __init__(self, secret_key: bytes, encryption_key:\
          \ bytes = None):\n        self.secret_key = secret_key\n        self.fernet = Fernet(encryption_key) if encryption_key\
          \ else None\n\n    def sign(self, value: str) -> str:\n        '''Sign a value for integrity verification'''\n \
          \       timestamp = str(int(time.time()))\n        message = f\"{timestamp}|{value}\"\n\n        signature = hmac.new(\n\
          \            self.secret_key,\n            message.encode(),\n            hashlib.sha256\n        ).hexdigest()\n\
          \n        return f\"{message}|{signature}\"\n\n    def verify_signature(self, signed_value: str, max_age: int =\
          \ None) -> Optional[str]:\n        '''Verify signature and optionally check age'''\n        try:\n            parts\
          \ = signed_value.rsplit(\"|\", 2)\n            if len(parts) != 3:\n                return None\n\n            timestamp_str,\
          \ value, signature = parts\n            timestamp = int(timestamp_str)\n\n            # Check age\n            if\
          \ max_age and (time.time() - timestamp) > max_age:\n                return None\n\n            # Verify signature\n\
          \            message = f\"{timestamp_str}|{value}\"\n            expected = hmac.new(\n                self.secret_key,\n\
          \                message.encode(),\n                hashlib.sha256\n            ).hexdigest()\n\n            if\
          \ hmac.compare_digest(signature, expected):\n                return value\n            return None\n\n        except\
          \ (ValueError, TypeError):\n            return None\n\n    def encrypt(self, data: dict) -> str:\n        '''Encrypt\
          \ data for cookie storage'''\n        if not self.fernet:\n            raise ValueError(\"Encryption key not configured\"\
          )\n\n        json_data = json.dumps(data)\n        encrypted = self.fernet.encrypt(json_data.encode())\n       \
          \ return base64.urlsafe_b64encode(encrypted).decode()\n\n    def decrypt(self, encrypted_value: str) -> Optional[dict]:\n\
          \        '''Decrypt cookie data'''\n        if not self.fernet:\n            return None\n\n        try:\n     \
          \       encrypted = base64.urlsafe_b64decode(encrypted_value.encode())\n            decrypted = self.fernet.decrypt(encrypted)\n\
          \            return json.loads(decrypted.decode())\n        except Exception:\n            return None\n\n    def\
          \ create_session_cookie(self, session_id: str,\n                              options: CookieOptions = None) ->\
          \ str:\n        '''Create a secure session cookie'''\n        if options is None:\n            options = CookieOptions()\n\
          \n        # Sign the session ID\n        signed_id = self.sign(session_id)\n\n        return options.to_header_string(\"\
          session\", signed_id)\n\n    def parse_session_cookie(self, cookie_value: str,\n                             max_age:\
          \ int = 86400) -> Optional[str]:\n        '''Parse and verify session cookie'''\n        return self.verify_signature(cookie_value,\
          \ max_age)\n\n# CSRF Token management\nclass CSRFProtection:\n    def __init__(self, secret_key: bytes):\n     \
          \   self.secret_key = secret_key\n\n    def generate_token(self, session_id: str) -> str:\n        '''Generate CSRF\
          \ token bound to session'''\n        random_part = secrets.token_urlsafe(16)\n        message = f\"{session_id}|{random_part}\"\
          \n\n        signature = hmac.new(\n            self.secret_key,\n            message.encode(),\n            hashlib.sha256\n\
          \        ).hexdigest()[:16]\n\n        return f\"{random_part}.{signature}\"\n\n    def validate_token(self, token:\
          \ str, session_id: str) -> bool:\n        '''Validate CSRF token'''\n        try:\n            random_part, signature\
          \ = token.split(\".\")\n            message = f\"{session_id}|{random_part}\"\n\n            expected = hmac.new(\n\
          \                self.secret_key,\n                message.encode(),\n                hashlib.sha256\n         \
          \   ).hexdigest()[:16]\n\n            return hmac.compare_digest(signature, expected)\n        except ValueError:\n\
          \            return False\n```\n"
      pitfalls:
      - SameSite=None requires Secure flag (HTTPS)
      - Cookie size limit is ~4KB - don't store too much data
      - CSRF tokens must be tied to session - not global
      - Double-submit cookie pattern needs both cookie AND header/form
    - name: Multi-Device & Concurrent Sessions
      description: Handle multiple sessions per user, device tracking, and forced logout
      skills:
      - Device fingerprinting
      - Session listing
      - Forced logout
      hints:
        level1: Track all active sessions per user for 'logout all devices'
        level2: Device fingerprint (IP, User-Agent, etc.) helps detect session theft
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport hashlib\nimport time\n\
          from user_agents import parse as parse_ua  # pip install user-agents\n\n@dataclass\nclass DeviceInfo:\n    device_id:\
          \ str\n    device_type: str      # mobile, tablet, desktop\n    os: str\n    browser: str\n    ip_address: str\n\
          \    location: Optional[str]  # From IP geolocation\n    first_seen: float\n    last_seen: float\n    is_current:\
          \ bool = False\n\n@dataclass\nclass UserSession:\n    session_id: str\n    device: DeviceInfo\n    created_at: float\n\
          \    last_active: float\n    is_current: bool\n\nclass MultiDeviceSessionManager:\n    def __init__(self, session_manager:\
          \ SessionManager, store: SessionStore):\n        self.sessions = session_manager\n        self.store = store\n \
          \       self.max_sessions_per_user = 10\n\n    def create_session_with_device(self, user_id: str, ip_address: str,\n\
          \                                   user_agent: str) -> Session:\n        # Parse user agent\n        ua = parse_ua(user_agent)\n\
          \n        # Create device fingerprint\n        device_id = self._fingerprint_device(ip_address, user_agent)\n\n\
          \        # Check session limit\n        existing = self.get_user_sessions(user_id)\n        if len(existing) >=\
          \ self.max_sessions_per_user:\n            # Remove oldest session\n            oldest = min(existing, key=lambda\
          \ s: s.last_active)\n            self.store.delete(oldest.session_id)\n\n        # Create session\n        session\
          \ = self.sessions.create_session(\n            ip_address=ip_address,\n            user_agent=user_agent,\n    \
          \        user_id=user_id\n        )\n\n        # Store device info in session\n        session.data[\"device\"]\
          \ = {\n            \"device_id\": device_id,\n            \"device_type\": self._get_device_type(ua),\n        \
          \    \"os\": f\"{ua.os.family} {ua.os.version_string}\",\n            \"browser\": f\"{ua.browser.family} {ua.browser.version_string}\"\
          ,\n            \"ip_address\": ip_address\n        }\n\n        self.store.save(session)\n        return session\n\
          \n    def _fingerprint_device(self, ip: str, user_agent: str) -> str:\n        '''Create stable device fingerprint'''\n\
          \        # In production: add more signals (screen size, timezone, etc.)\n        data = f\"{ip}|{user_agent}\"\n\
          \        return hashlib.sha256(data.encode()).hexdigest()[:16]\n\n    def _get_device_type(self, ua) -> str:\n \
          \       if ua.is_mobile:\n            return \"mobile\"\n        elif ua.is_tablet:\n            return \"tablet\"\
          \n        return \"desktop\"\n\n    def get_user_sessions(self, user_id: str) -> list[UserSession]:\n        '''List\
          \ all active sessions for a user'''\n        # Get all session IDs for user from Redis set\n        user_key = f\"\
          session:user:{user_id}\"\n        session_ids = self.store.redis.smembers(user_key)\n\n        sessions = []\n \
          \       for sid in session_ids:\n            sid_str = sid.decode() if isinstance(sid, bytes) else sid\n       \
          \     session = self.store.load(sid_str)\n            if session:\n                device_data = session.data.get(\"\
          device\", {})\n                sessions.append(UserSession(\n                    session_id=session.id,\n      \
          \              device=DeviceInfo(\n                        device_id=device_data.get(\"device_id\", \"unknown\"\
          ),\n                        device_type=device_data.get(\"device_type\", \"unknown\"),\n                       \
          \ os=device_data.get(\"os\", \"unknown\"),\n                        browser=device_data.get(\"browser\", \"unknown\"\
          ),\n                        ip_address=device_data.get(\"ip_address\", \"unknown\"),\n                        location=None,\
          \  # Add geolocation lookup\n                        first_seen=session.created_at,\n                        last_seen=session.last_accessed,\n\
          \                        is_current=False\n                    ),\n                    created_at=session.created_at,\n\
          \                    last_active=session.last_accessed,\n                    is_current=False\n                ))\n\
          \n        return sorted(sessions, key=lambda s: s.last_active, reverse=True)\n\n    def logout_session(self, user_id:\
          \ str, session_id: str,\n                       current_session_id: str) -> bool:\n        '''Logout a specific\
          \ session (from settings page)'''\n        session = self.store.load(session_id)\n        if not session or session.user_id\
          \ != user_id:\n            return False\n\n        # Don't allow logging out current session via this method\n \
          \       if session_id == current_session_id:\n            return False\n\n        self.store.delete(session_id)\n\
          \        return True\n\n    def logout_all_except_current(self, user_id: str,\n                                \
          \  current_session_id: str) -> int:\n        '''Logout all sessions except current'''\n        sessions = self.get_user_sessions(user_id)\n\
          \        count = 0\n\n        for session in sessions:\n            if session.session_id != current_session_id:\n\
          \                self.store.delete(session.session_id)\n                count += 1\n\n        return count\n\n \
          \   def force_logout_user(self, user_id: str) -> int:\n        '''Admin: force logout all sessions for a user'''\n\
          \        return self.store.delete_user_sessions(user_id)\n\n    def detect_anomaly(self, session: Session, ip_address:\
          \ str,\n                       user_agent: str) -> list[str]:\n        '''Detect suspicious session activity'''\n\
          \        warnings = []\n        device_data = session.data.get(\"device\", {})\n\n        # IP changed significantly\n\
          \        if device_data.get(\"ip_address\") != ip_address:\n            # Could check if same /16 subnet or geolocation\n\
          \            warnings.append(\"ip_changed\")\n\n        # User agent changed (browser update is normal, complete\
          \ change is suspicious)\n        if device_data.get(\"browser\") != user_agent:\n            new_fp = self._fingerprint_device(ip_address,\
          \ user_agent)\n            if new_fp != device_data.get(\"device_id\"):\n                warnings.append(\"device_changed\"\
          )\n\n        return warnings\n```\n"
      pitfalls:
      - Device fingerprinting has false positives (VPN, browser updates)
      - Session limit without cleanup leads to locked-out users
      - Time-based anomaly detection needs to consider time zones
      - Don't show full session ID in UI - use hash or partial
  payment-gateway:
    name: Payment Gateway Integration
    description: Build a payment processing system handling credit cards, idempotency, PCI compliance considerations, refunds,
      and webhook reconciliation.
    why_expert: Payment bugs cost real money and trust. Understanding payment flow internals helps prevent double-charges,
      handle edge cases, and integrate any payment provider.
    difficulty: expert
    tags:
    - payments
    - fintech
    - security
    - api-integration
    - idempotency
    estimated_hours: 45
    prerequisites:
    - build-http-server
    milestones:
    - name: Payment Intent & Idempotency
      description: Implement payment intents with idempotency keys to prevent duplicate charges
      skills:
      - Idempotency patterns
      - State machines
      - Atomic operations
      hints:
        level1: Create a payment intent before charging - it represents customer's intention to pay
        level2: 'Idempotency key: same key = same response. Store request hash with response.'
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          import secrets\nimport hashlib\nimport time\nimport json\n\nclass PaymentStatus(Enum):\n    CREATED = \"created\"\
          \n    REQUIRES_PAYMENT_METHOD = \"requires_payment_method\"\n    REQUIRES_CONFIRMATION = \"requires_confirmation\"\
          \n    PROCESSING = \"processing\"\n    SUCCEEDED = \"succeeded\"\n    FAILED = \"failed\"\n    CANCELED = \"canceled\"\
          \n    REFUNDED = \"refunded\"\n    PARTIALLY_REFUNDED = \"partially_refunded\"\n\n@dataclass\nclass PaymentIntent:\n\
          \    id: str\n    amount: int              # In smallest currency unit (cents)\n    currency: str\n    status: PaymentStatus\n\
          \    customer_id: Optional[str]\n    payment_method_id: Optional[str]\n    description: str\n    metadata: dict\n\
          \    created_at: float\n    updated_at: float\n    idempotency_key: Optional[str]\n    client_secret: str      \
          \ # For frontend confirmation\n    error_message: Optional[str] = None\n    charge_id: Optional[str] = None\n\n\
          @dataclass\nclass IdempotencyRecord:\n    key: str\n    request_hash: str\n    response: dict\n    status_code:\
          \ int\n    created_at: float\n    expires_at: float\n\nclass IdempotencyManager:\n    def __init__(self, ttl: int\
          \ = 86400):  # 24 hour TTL\n        self.records: dict[str, IdempotencyRecord] = {}\n        self.ttl = ttl\n\n\
          \    def _hash_request(self, method: str, path: str, body: dict) -> str:\n        data = f\"{method}|{path}|{json.dumps(body,\
          \ sort_keys=True)}\"\n        return hashlib.sha256(data.encode()).hexdigest()\n\n    def check(self, idempotency_key:\
          \ str, method: str,\n              path: str, body: dict) -> Optional[tuple[dict, int]]:\n        '''Check if we've\
          \ seen this request before'''\n        record = self.records.get(idempotency_key)\n        if not record:\n    \
          \        return None\n\n        # Check expiry\n        if time.time() > record.expires_at:\n            del self.records[idempotency_key]\n\
          \            return None\n\n        # Verify request matches (prevent key reuse with different request)\n      \
          \  request_hash = self._hash_request(method, path, body)\n        if request_hash != record.request_hash:\n    \
          \        raise ValueError(\n                \"Idempotency key already used with different request parameters\"\n\
          \            )\n\n        return record.response, record.status_code\n\n    def store(self, idempotency_key: str,\
          \ method: str, path: str,\n              body: dict, response: dict, status_code: int):\n        '''Store response\
          \ for idempotency key'''\n        self.records[idempotency_key] = IdempotencyRecord(\n            key=idempotency_key,\n\
          \            request_hash=self._hash_request(method, path, body),\n            response=response,\n            status_code=status_code,\n\
          \            created_at=time.time(),\n            expires_at=time.time() + self.ttl\n        )\n\nclass PaymentService:\n\
          \    def __init__(self):\n        self.intents: dict[str, PaymentIntent] = {}\n        self.idempotency = IdempotencyManager()\n\
          \n    def create_payment_intent(self, amount: int, currency: str,\n                              customer_id: str\
          \ = None,\n                              description: str = \"\",\n                              metadata: dict\
          \ = None,\n                              idempotency_key: str = None) -> PaymentIntent:\n        # Check idempotency\n\
          \        if idempotency_key:\n            existing = self.idempotency.check(\n                idempotency_key, \"\
          POST\", \"/payment_intents\",\n                {\"amount\": amount, \"currency\": currency}\n            )\n   \
          \         if existing:\n                return self.intents[existing[0][\"id\"]]\n\n        # Create new payment\
          \ intent\n        intent_id = f\"pi_{secrets.token_urlsafe(16)}\"\n        client_secret = f\"{intent_id}_secret_{secrets.token_urlsafe(16)}\"\
          \n\n        now = time.time()\n        intent = PaymentIntent(\n            id=intent_id,\n            amount=amount,\n\
          \            currency=currency.lower(),\n            status=PaymentStatus.REQUIRES_PAYMENT_METHOD,\n           \
          \ customer_id=customer_id,\n            payment_method_id=None,\n            description=description,\n        \
          \    metadata=metadata or {},\n            created_at=now,\n            updated_at=now,\n            idempotency_key=idempotency_key,\n\
          \            client_secret=client_secret\n        )\n\n        self.intents[intent_id] = intent\n\n        # Store\
          \ for idempotency\n        if idempotency_key:\n            self.idempotency.store(\n                idempotency_key,\
          \ \"POST\", \"/payment_intents\",\n                {\"amount\": amount, \"currency\": currency},\n             \
          \   {\"id\": intent_id, \"client_secret\": client_secret},\n                200\n            )\n\n        return\
          \ intent\n\n    def attach_payment_method(self, intent_id: str,\n                              payment_method_id:\
          \ str) -> PaymentIntent:\n        intent = self.intents.get(intent_id)\n        if not intent:\n            raise\
          \ ValueError(\"Payment intent not found\")\n\n        if intent.status not in [PaymentStatus.REQUIRES_PAYMENT_METHOD,\n\
          \                                 PaymentStatus.REQUIRES_CONFIRMATION]:\n            raise ValueError(f\"Cannot\
          \ modify intent in status: {intent.status}\")\n\n        intent.payment_method_id = payment_method_id\n        intent.status\
          \ = PaymentStatus.REQUIRES_CONFIRMATION\n        intent.updated_at = time.time()\n\n        return intent\n```\n"
      pitfalls:
      - Idempotency key reused with different params should error, not return old response
      - Client secret must be kept secret - only share with authenticated user
      - Amount in cents prevents floating point errors ($10.00 = 1000 cents)
      - Status transitions must be validated - can't go backwards
    - name: Payment Processing & 3DS
      description: Implement payment confirmation with 3D Secure authentication flow
      skills:
      - 3DS flow
      - Payment processing
      - Error handling
      hints:
        level1: 3DS (3D Secure) is required for Strong Customer Authentication (SCA) in EU
        level2: Payment can be synchronous (immediate) or async (3DS redirect needed)
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom enum import Enum\n\nclass\
          \ ThreeDSStatus(Enum):\n    NOT_REQUIRED = \"not_required\"\n    REQUIRED = \"required\"\n    CHALLENGE = \"challenge\"\
          \      # User must complete challenge\n    SUCCEEDED = \"succeeded\"\n    FAILED = \"failed\"\n\n@dataclass\nclass\
          \ ThreeDSResult:\n    status: ThreeDSStatus\n    redirect_url: Optional[str] = None  # For challenge flow\n    version:\
          \ str = \"2.0\"\n\n@dataclass\nclass PaymentMethodDetails:\n    id: str\n    card_brand: str           # visa, mastercard,\
          \ amex\n    last4: str\n    exp_month: int\n    exp_year: int\n    fingerprint: str          # Unique card identifier\n\
          \    three_ds_supported: bool\n\nclass PaymentProcessor:\n    def __init__(self, payment_service: PaymentService):\n\
          \        self.service = payment_service\n        self.charges: dict[str, dict] = {}\n\n    def confirm_payment(self,\
          \ intent_id: str,\n                        return_url: str = None) -> dict:\n        '''\n        Confirm payment\
          \ intent. May require 3DS.\n        Returns: {status, next_action?, charge_id?}\n        '''\n        intent = self.service.intents.get(intent_id)\n\
          \        if not intent:\n            raise ValueError(\"Payment intent not found\")\n\n        if intent.status\
          \ != PaymentStatus.REQUIRES_CONFIRMATION:\n            raise ValueError(f\"Cannot confirm intent in status: {intent.status}\"\
          )\n\n        # Check if 3DS is required (based on card, amount, region)\n        three_ds = self._check_3ds_requirement(intent)\n\
          \n        if three_ds.status == ThreeDSStatus.CHALLENGE:\n            # Need user interaction\n            intent.status\
          \ = PaymentStatus.PROCESSING\n            return {\n                \"status\": \"requires_action\",\n         \
          \       \"next_action\": {\n                    \"type\": \"redirect_to_url\",\n                    \"redirect_to_url\"\
          : {\n                        \"url\": three_ds.redirect_url,\n                        \"return_url\": return_url\n\
          \                    }\n                }\n            }\n\n        if three_ds.status == ThreeDSStatus.FAILED:\n\
          \            intent.status = PaymentStatus.FAILED\n            intent.error_message = \"3D Secure authentication\
          \ failed\"\n            return {\"status\": \"failed\", \"error\": intent.error_message}\n\n        # Process payment\n\
          \        return self._process_charge(intent)\n\n    def handle_3ds_callback(self, intent_id: str,\n            \
          \                three_ds_result: str) -> dict:\n        '''Handle return from 3DS challenge'''\n        intent\
          \ = self.service.intents.get(intent_id)\n        if not intent:\n            raise ValueError(\"Payment intent not\
          \ found\")\n\n        # Verify 3DS result (in production: verify with card network)\n        if three_ds_result\
          \ == \"authenticated\":\n            return self._process_charge(intent)\n        else:\n            intent.status\
          \ = PaymentStatus.FAILED\n            intent.error_message = \"3D Secure authentication failed\"\n            return\
          \ {\"status\": \"failed\", \"error\": intent.error_message}\n\n    def _check_3ds_requirement(self, intent: PaymentIntent)\
          \ -> ThreeDSResult:\n        '''Determine if 3DS is required'''\n        # In production: check card issuer requirements,\
          \ SCA rules, etc.\n\n        # SCA required for EU cards over certain threshold\n        if intent.amount > 3000:\
          \  # â‚¬30.00\n            return ThreeDSResult(\n                status=ThreeDSStatus.CHALLENGE,\n              \
          \  redirect_url=f\"https://3ds.example.com/challenge/{intent.id}\"\n            )\n\n        return ThreeDSResult(status=ThreeDSStatus.NOT_REQUIRED)\n\
          \n    def _process_charge(self, intent: PaymentIntent) -> dict:\n        '''Actually charge the card'''\n      \
          \  intent.status = PaymentStatus.PROCESSING\n\n        try:\n            # In production: call payment processor\
          \ API (Stripe, Adyen, etc.)\n            charge_id = f\"ch_{secrets.token_urlsafe(16)}\"\n\n            # Simulate\
          \ processing\n            success = True  # In reality: check processor response\n\n            if success:\n  \
          \              intent.status = PaymentStatus.SUCCEEDED\n                intent.charge_id = charge_id\n         \
          \       intent.updated_at = time.time()\n\n                self.charges[charge_id] = {\n                    \"id\"\
          : charge_id,\n                    \"amount\": intent.amount,\n                    \"currency\": intent.currency,\n\
          \                    \"payment_intent\": intent.id,\n                    \"status\": \"succeeded\",\n          \
          \          \"created_at\": time.time()\n                }\n\n                return {\n                    \"status\"\
          : \"succeeded\",\n                    \"charge_id\": charge_id\n                }\n            else:\n         \
          \       intent.status = PaymentStatus.FAILED\n                intent.error_message = \"Card declined\"\n       \
          \         return {\"status\": \"failed\", \"error\": \"card_declined\"}\n\n        except Exception as e:\n    \
          \        intent.status = PaymentStatus.FAILED\n            intent.error_message = str(e)\n            return {\"\
          status\": \"failed\", \"error\": str(e)}\n\n    def capture_payment(self, intent_id: str, amount: int = None) ->\
          \ dict:\n        '''\n        Capture a previously authorized payment.\n        Used for auth-then-capture flows\
          \ (hotels, rentals).\n        '''\n        intent = self.service.intents.get(intent_id)\n        if not intent or\
          \ intent.status != PaymentStatus.SUCCEEDED:\n            raise ValueError(\"Invalid intent for capture\")\n\n  \
          \      capture_amount = amount or intent.amount\n        if capture_amount > intent.amount:\n            raise ValueError(\"\
          Capture amount exceeds authorized amount\")\n\n        # In production: call processor capture API\n        return\
          \ {\n            \"status\": \"captured\",\n            \"amount\": capture_amount,\n            \"charge_id\":\
          \ intent.charge_id\n        }\n```\n"
      pitfalls:
      - 3DS redirects must include return_url for user to come back
      - Payment can succeed but webhook fail - always reconcile
      - 'Auth-capture: authorization expires (usually 7 days) - capture before expiry'
      - Currency mismatch between intent and capture causes errors
    - name: Refunds & Disputes
      description: Implement refund processing, partial refunds, and dispute handling
      skills:
      - Refund workflows
      - Dispute handling
      - Financial reconciliation
      hints:
        level1: Refunds can be full or partial; total refunds can't exceed original charge
        level2: Disputes (chargebacks) require evidence submission within deadline
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom enum import Enum\nimport\
          \ time\n\nclass RefundStatus(Enum):\n    PENDING = \"pending\"\n    SUCCEEDED = \"succeeded\"\n    FAILED = \"failed\"\
          \n    CANCELED = \"canceled\"\n\nclass DisputeStatus(Enum):\n    WARNING_NEEDS_RESPONSE = \"warning_needs_response\"\
          \n    WARNING_UNDER_REVIEW = \"warning_under_review\"\n    WARNING_CLOSED = \"warning_closed\"\n    NEEDS_RESPONSE\
          \ = \"needs_response\"\n    UNDER_REVIEW = \"under_review\"\n    CHARGE_REFUNDED = \"charge_refunded\"\n    WON\
          \ = \"won\"\n    LOST = \"lost\"\n\nclass DisputeReason(Enum):\n    DUPLICATE = \"duplicate\"\n    FRAUDULENT =\
          \ \"fraudulent\"\n    SUBSCRIPTION_CANCELED = \"subscription_canceled\"\n    PRODUCT_NOT_RECEIVED = \"product_not_received\"\
          \n    PRODUCT_UNACCEPTABLE = \"product_unacceptable\"\n    UNRECOGNIZED = \"unrecognized\"\n    CREDIT_NOT_PROCESSED\
          \ = \"credit_not_processed\"\n    GENERAL = \"general\"\n\n@dataclass\nclass Refund:\n    id: str\n    charge_id:\
          \ str\n    payment_intent_id: str\n    amount: int\n    currency: str\n    status: RefundStatus\n    reason: Optional[str]\n\
          \    created_at: float\n    metadata: dict\n\n@dataclass\nclass Dispute:\n    id: str\n    charge_id: str\n    amount:\
          \ int\n    currency: str\n    status: DisputeStatus\n    reason: DisputeReason\n    evidence_due_by: float\n   \
          \ evidence: Optional[dict]\n    created_at: float\n\nclass RefundService:\n    def __init__(self, payment_service:\
          \ PaymentService,\n                 processor: PaymentProcessor):\n        self.payments = payment_service\n   \
          \     self.processor = processor\n        self.refunds: dict[str, Refund] = {}\n        self.charge_refunds: dict[str,\
          \ list[str]] = {}  # charge_id -> refund_ids\n\n    def create_refund(self, charge_id: str = None,\n           \
          \           payment_intent_id: str = None,\n                      amount: int = None,\n                      reason:\
          \ str = None) -> Refund:\n        # Find the charge\n        if payment_intent_id:\n            intent = self.payments.intents.get(payment_intent_id)\n\
          \            if not intent or not intent.charge_id:\n                raise ValueError(\"Payment intent has no charge\"\
          )\n            charge_id = intent.charge_id\n\n        charge = self.processor.charges.get(charge_id)\n        if\
          \ not charge:\n            raise ValueError(\"Charge not found\")\n\n        # Calculate refundable amount\n   \
          \     existing_refunds = self.charge_refunds.get(charge_id, [])\n        total_refunded = sum(\n            self.refunds[rid].amount\
          \ for rid in existing_refunds\n            if self.refunds[rid].status == RefundStatus.SUCCEEDED\n        )\n\n\
          \        refundable = charge[\"amount\"] - total_refunded\n        refund_amount = amount or refundable\n\n    \
          \    if refund_amount > refundable:\n            raise ValueError(\n                f\"Refund amount ({refund_amount})\
          \ exceeds \"\n                f\"refundable amount ({refundable})\"\n            )\n\n        if refund_amount <=\
          \ 0:\n            raise ValueError(\"Charge already fully refunded\")\n\n        # Create refund\n        refund_id\
          \ = f\"re_{secrets.token_urlsafe(16)}\"\n        refund = Refund(\n            id=refund_id,\n            charge_id=charge_id,\n\
          \            payment_intent_id=charge.get(\"payment_intent\"),\n            amount=refund_amount,\n            currency=charge[\"\
          currency\"],\n            status=RefundStatus.PENDING,\n            reason=reason,\n            created_at=time.time(),\n\
          \            metadata={}\n        )\n\n        self.refunds[refund_id] = refund\n\n        # Track refunds per charge\n\
          \        if charge_id not in self.charge_refunds:\n            self.charge_refunds[charge_id] = []\n        self.charge_refunds[charge_id].append(refund_id)\n\
          \n        # Process refund (in production: async with webhook confirmation)\n        self._process_refund(refund)\n\
          \n        # Update payment intent status\n        intent = self.payments.intents.get(charge.get(\"payment_intent\"\
          ))\n        if intent:\n            if total_refunded + refund_amount >= charge[\"amount\"]:\n                intent.status\
          \ = PaymentStatus.REFUNDED\n            else:\n                intent.status = PaymentStatus.PARTIALLY_REFUNDED\n\
          \n        return refund\n\n    def _process_refund(self, refund: Refund):\n        # In production: call processor\
          \ refund API\n        try:\n            # Simulate processing\n            refund.status = RefundStatus.SUCCEEDED\n\
          \        except Exception as e:\n            refund.status = RefundStatus.FAILED\n\nclass DisputeService:\n    def\
          \ __init__(self, processor: PaymentProcessor):\n        self.processor = processor\n        self.disputes: dict[str,\
          \ Dispute] = {}\n\n    def handle_dispute_webhook(self, event_type: str, data: dict):\n        '''Handle dispute\
          \ webhook from payment processor'''\n        if event_type == \"charge.dispute.created\":\n            dispute =\
          \ Dispute(\n                id=data[\"id\"],\n                charge_id=data[\"charge\"],\n                amount=data[\"\
          amount\"],\n                currency=data[\"currency\"],\n                status=DisputeStatus.NEEDS_RESPONSE,\n\
          \                reason=DisputeReason(data[\"reason\"]),\n                evidence_due_by=data[\"evidence_due_by\"\
          ],\n                evidence=None,\n                created_at=time.time()\n            )\n            self.disputes[dispute.id]\
          \ = dispute\n\n            # Alert team immediately\n            self._alert_dispute(dispute)\n\n        elif event_type\
          \ == \"charge.dispute.closed\":\n            dispute = self.disputes.get(data[\"id\"])\n            if dispute:\n\
          \                dispute.status = DisputeStatus(data[\"status\"])\n\n    def submit_evidence(self, dispute_id: str,\
          \ evidence: dict) -> Dispute:\n        '''\n        Submit evidence for a dispute.\n        Evidence includes: receipts,\
          \ shipping proof, correspondence, etc.\n        '''\n        dispute = self.disputes.get(dispute_id)\n        if\
          \ not dispute:\n            raise ValueError(\"Dispute not found\")\n\n        if time.time() > dispute.evidence_due_by:\n\
          \            raise ValueError(\"Evidence submission deadline passed\")\n\n        if dispute.status not in [DisputeStatus.NEEDS_RESPONSE,\n\
          \                                   DisputeStatus.WARNING_NEEDS_RESPONSE]:\n            raise ValueError(\"Dispute\
          \ not accepting evidence\")\n\n        # Validate required evidence based on reason\n        required_fields = self._get_required_evidence(dispute.reason)\n\
          \        missing = [f for f in required_fields if f not in evidence]\n        if missing:\n            raise ValueError(f\"\
          Missing required evidence: {missing}\")\n\n        dispute.evidence = evidence\n        dispute.status = DisputeStatus.UNDER_REVIEW\n\
          \n        # In production: submit to processor API\n        return dispute\n\n    def _get_required_evidence(self,\
          \ reason: DisputeReason) -> list[str]:\n        evidence_requirements = {\n            DisputeReason.PRODUCT_NOT_RECEIVED:\
          \ [\n                \"shipping_carrier\", \"shipping_tracking_number\",\n                \"shipping_date\"\n  \
          \          ],\n            DisputeReason.FRAUDULENT: [\n                \"customer_email_address\", \"customer_ip_address\"\
          ,\n                \"billing_address\"\n            ],\n            DisputeReason.DUPLICATE: [\n               \
          \ \"duplicate_charge_explanation\",\n                \"original_transaction\"\n            ],\n        }\n     \
          \   return evidence_requirements.get(reason, [])\n\n    def _alert_dispute(self, dispute: Dispute):\n        print(f\"\
          ALERT: New dispute {dispute.id} for ${dispute.amount/100:.2f}\")\n        print(f\"Reason: {dispute.reason.value}\"\
          )\n        print(f\"Evidence due: {dispute.evidence_due_by}\")\n```\n"
      pitfalls:
      - Refund can take 5-10 business days to appear on statement
      - Dispute evidence deadline is strict - automate alerts
      - Dispute fee charged even if you win - prevention is key
      - 'Partial refunds: track total refunded vs original amount carefully'
    - name: Webhook Reconciliation
      description: Implement reliable webhook processing with signature verification and reconciliation
      skills:
      - Webhook security
      - Event processing
      - Reconciliation
      hints:
        level1: Always verify webhook signature before processing
        level2: Webhooks can arrive out of order or duplicated - handle idempotently
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Callable\nimport hmac\nimport hashlib\n\
          import time\nimport json\n\n@dataclass\nclass WebhookEvent:\n    id: str\n    type: str\n    data: dict\n    created_at:\
          \ float\n    processed_at: float = None\n    attempts: int = 0\n\nclass PaymentWebhookHandler:\n    def __init__(self,\
          \ webhook_secret: str,\n                 payment_service: PaymentService,\n                 refund_service: RefundService,\n\
          \                 dispute_service: DisputeService):\n        self.secret = webhook_secret\n        self.payments\
          \ = payment_service\n        self.refunds = refund_service\n        self.disputes = dispute_service\n        self.processed_events:\
          \ set[str] = set()\n        self.handlers: dict[str, Callable] = {\n            \"payment_intent.succeeded\": self._handle_payment_succeeded,\n\
          \            \"payment_intent.payment_failed\": self._handle_payment_failed,\n            \"charge.refunded\": self._handle_refund,\n\
          \            \"charge.dispute.created\": self._handle_dispute_created,\n            \"charge.dispute.closed\": self._handle_dispute_closed,\n\
          \        }\n\n    def verify_signature(self, payload: bytes, signature: str,\n                         timestamp:\
          \ str) -> bool:\n        '''Verify Stripe-style webhook signature'''\n        # Signature format: t=timestamp,v1=signature\n\
          \        expected = hmac.new(\n            self.secret.encode(),\n            f\"{timestamp}.{payload.decode()}\"\
          .encode(),\n            hashlib.sha256\n        ).hexdigest()\n\n        # Constant-time comparison\n        return\
          \ hmac.compare_digest(f\"v1={expected}\", signature.split(\",\")[1])\n\n    def handle_webhook(self, payload: bytes,\
          \ signature: str) -> dict:\n        '''Process incoming webhook'''\n        # Parse signature header\n        parts\
          \ = dict(p.split(\"=\") for p in signature.split(\",\"))\n        timestamp = parts.get(\"t\", \"\")\n\n       \
          \ # Verify signature\n        if not self.verify_signature(payload, signature, timestamp):\n            raise ValueError(\"\
          Invalid webhook signature\")\n\n        # Check timestamp (prevent replay attacks)\n        if abs(time.time() -\
          \ int(timestamp)) > 300:  # 5 minute tolerance\n            raise ValueError(\"Webhook timestamp too old\")\n\n\
          \        # Parse event\n        event_data = json.loads(payload)\n        event_id = event_data[\"id\"]\n      \
          \  event_type = event_data[\"type\"]\n\n        # Idempotency check\n        if event_id in self.processed_events:\n\
          \            return {\"status\": \"already_processed\"}\n\n        event = WebhookEvent(\n            id=event_id,\n\
          \            type=event_type,\n            data=event_data[\"data\"][\"object\"],\n            created_at=event_data[\"\
          created\"]\n        )\n\n        # Route to handler\n        handler = self.handlers.get(event_type)\n        if\
          \ handler:\n            try:\n                handler(event)\n                event.processed_at = time.time()\n\
          \                self.processed_events.add(event_id)\n                return {\"status\": \"processed\"}\n     \
          \       except Exception as e:\n                event.attempts += 1\n                # Log error, will be retried\
          \ by processor\n                return {\"status\": \"error\", \"error\": str(e)}\n        else:\n            #\
          \ Unknown event type - acknowledge to prevent retries\n            return {\"status\": \"ignored\", \"reason\":\
          \ \"unknown_event_type\"}\n\n    def _handle_payment_succeeded(self, event: WebhookEvent):\n        '''Reconcile\
          \ successful payment'''\n        intent_id = event.data[\"id\"]\n        intent = self.payments.intents.get(intent_id)\n\
          \n        if intent:\n            # Update our records to match processor state\n            intent.status = PaymentStatus.SUCCEEDED\n\
          \            intent.charge_id = event.data.get(\"latest_charge\")\n            intent.updated_at = time.time()\n\
          \n            # Trigger fulfillment (send email, provision service, etc.)\n            self._trigger_fulfillment(intent)\n\
          \        else:\n            # Payment succeeded but we don't have the intent\n            # This can happen if webhook\
          \ arrives before API response\n            # Log for investigation\n            print(f\"WARNING: Unknown payment\
          \ intent succeeded: {intent_id}\")\n\n    def _handle_payment_failed(self, event: WebhookEvent):\n        '''Handle\
          \ failed payment'''\n        intent_id = event.data[\"id\"]\n        intent = self.payments.intents.get(intent_id)\n\
          \n        if intent:\n            intent.status = PaymentStatus.FAILED\n            intent.error_message = event.data.get(\"\
          last_payment_error\", {}).get(\"message\")\n            intent.updated_at = time.time()\n\n    def _handle_refund(self,\
          \ event: WebhookEvent):\n        '''Reconcile refund'''\n        charge_id = event.data[\"id\"]\n        # Check\
          \ if refund already recorded\n        existing = self.refunds.charge_refunds.get(charge_id, [])\n\n        for refund_data\
          \ in event.data.get(\"refunds\", {}).get(\"data\", []):\n            if refund_data[\"id\"] not in [self.refunds.refunds[r].id\
          \ for r in existing]:\n                # Refund happened outside our system (admin dashboard, etc.)\n          \
          \      # Record it\n                print(f\"Recording external refund: {refund_data['id']}\")\n\n    def _handle_dispute_created(self,\
          \ event: WebhookEvent):\n        self.disputes.handle_dispute_webhook(\"charge.dispute.created\", event.data)\n\n\
          \    def _handle_dispute_closed(self, event: WebhookEvent):\n        self.disputes.handle_dispute_webhook(\"charge.dispute.closed\"\
          , event.data)\n\n    def _trigger_fulfillment(self, intent: PaymentIntent):\n        '''Trigger order fulfillment\
          \ after successful payment'''\n        # In production: queue job to send confirmation email,\n        # update\
          \ order status, provision service, etc.\n        print(f\"Fulfilling order for payment: {intent.id}\")\n\n    def\
          \ reconcile(self):\n        '''\n        Periodic reconciliation to catch missed webhooks.\n        Compare our\
          \ records with processor's records.\n        '''\n        # In production: fetch recent payments from processor\
          \ API\n        # Compare status with local records\n        # Update any mismatches\n        pass\n```\n"
      pitfalls:
      - Webhook signature verification is critical - never skip
      - Events can arrive out of order - check timestamps and states
      - Processor may retry webhooks - always be idempotent
      - Run periodic reconciliation - webhooks can fail silently
  subscription-billing:
    name: Subscription & Billing System
    description: Build a complete subscription management system with plans, billing cycles, proration, trials, and usage-based
      billing.
    why_expert: Subscription logic is complex with edge cases (upgrades, downgrades, trials, cancellations). Building one
      teaches financial calculations and state management.
    difficulty: expert
    tags:
    - billing
    - subscriptions
    - saas
    - fintech
    - recurring-payments
    estimated_hours: 50
    prerequisites:
    - payment-gateway
    milestones:
    - name: Plans & Pricing
      description: Implement flexible pricing plans with tiers, features, and currencies
      skills:
      - Pricing models
      - Feature flags
      - Multi-currency
      hints:
        level1: Plans define pricing; subscriptions track who's on what plan
        level2: Support multiple billing intervals (monthly, yearly) with different prices
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          from decimal import Decimal\nimport time\n\nclass BillingInterval(Enum):\n    DAILY = \"daily\"\n    WEEKLY = \"\
          weekly\"\n    MONTHLY = \"monthly\"\n    YEARLY = \"yearly\"\n\nclass PricingModel(Enum):\n    FLAT_RATE = \"flat_rate\"\
          \           # Fixed price\n    PER_SEAT = \"per_seat\"             # Price Ã— number of users\n    TIERED = \"tiered\"\
          \                 # Different rates at volume tiers\n    VOLUME = \"volume\"                 # Single rate based\
          \ on total volume\n    USAGE_BASED = \"usage_based\"       # Pay for what you use\n\n@dataclass\nclass PriceTier:\n\
          \    up_to: Optional[int]  # None = unlimited\n    unit_amount: int      # Price per unit in cents\n    flat_amount:\
          \ int = 0  # Optional flat fee for this tier\n\n@dataclass\nclass Price:\n    id: str\n    plan_id: str\n    currency:\
          \ str\n    unit_amount: int                          # Base price in cents\n    billing_interval: BillingInterval\n\
          \    interval_count: int = 1                   # Every N intervals\n    pricing_model: PricingModel = PricingModel.FLAT_RATE\n\
          \    tiers: list[PriceTier] = field(default_factory=list)\n    trial_period_days: int = 0\n    metadata: dict =\
          \ field(default_factory=dict)\n\n@dataclass\nclass Feature:\n    id: str\n    name: str\n    description: str\n\
          \    value_type: str = \"boolean\"  # boolean, number, unlimited\n    default_value: any = False\n\n@dataclass\n\
          class Plan:\n    id: str\n    name: str\n    description: str\n    prices: dict[str, Price]   # currency -> Price\n\
          \    features: dict[str, any]   # feature_id -> value\n    active: bool = True\n    metadata: dict = field(default_factory=dict)\n\
          \nclass PricingEngine:\n    def __init__(self):\n        self.plans: dict[str, Plan] = {}\n        self.features:\
          \ dict[str, Feature] = {}\n\n    def create_plan(self, plan_id: str, name: str, description: str,\n            \
          \        base_price: int, currency: str = \"usd\",\n                    interval: BillingInterval = BillingInterval.MONTHLY,\n\
          \                    features: dict = None,\n                    trial_days: int = 0) -> Plan:\n        price =\
          \ Price(\n            id=f\"price_{plan_id}_{currency}\",\n            plan_id=plan_id,\n            currency=currency,\n\
          \            unit_amount=base_price,\n            billing_interval=interval,\n            trial_period_days=trial_days\n\
          \        )\n\n        plan = Plan(\n            id=plan_id,\n            name=name,\n            description=description,\n\
          \            prices={currency: price},\n            features=features or {}\n        )\n\n        self.plans[plan_id]\
          \ = plan\n        return plan\n\n    def add_tiered_pricing(self, plan_id: str, currency: str,\n               \
          \            tiers: list[dict], model: PricingModel):\n        '''Add tiered or volume pricing to a plan'''\n  \
          \      plan = self.plans.get(plan_id)\n        if not plan:\n            raise ValueError(\"Plan not found\")\n\n\
          \        price = plan.prices.get(currency)\n        if not price:\n            raise ValueError(\"Price not found\
          \ for currency\")\n\n        price.pricing_model = model\n        price.tiers = [\n            PriceTier(\n    \
          \            up_to=t.get(\"up_to\"),\n                unit_amount=t[\"unit_amount\"],\n                flat_amount=t.get(\"\
          flat_amount\", 0)\n            )\n            for t in tiers\n        ]\n\n    def calculate_price(self, plan_id:\
          \ str, currency: str,\n                        quantity: int = 1) -> dict:\n        '''Calculate price for given\
          \ quantity'''\n        plan = self.plans.get(plan_id)\n        if not plan:\n            raise ValueError(\"Plan\
          \ not found\")\n\n        price = plan.prices.get(currency)\n        if not price:\n            raise ValueError(\"\
          Price not found for currency\")\n\n        if price.pricing_model == PricingModel.FLAT_RATE:\n            return\
          \ {\n                \"subtotal\": price.unit_amount,\n                \"quantity\": 1,\n                \"unit_amount\"\
          : price.unit_amount\n            }\n\n        elif price.pricing_model == PricingModel.PER_SEAT:\n            return\
          \ {\n                \"subtotal\": price.unit_amount * quantity,\n                \"quantity\": quantity,\n    \
          \            \"unit_amount\": price.unit_amount\n            }\n\n        elif price.pricing_model == PricingModel.TIERED:\n\
          \            # Each tier applies to units within that tier\n            total = 0\n            remaining = quantity\n\
          \            breakdown = []\n\n            for tier in sorted(price.tiers, key=lambda t: t.up_to or float('inf')):\n\
          \                if remaining <= 0:\n                    break\n\n                tier_max = tier.up_to or float('inf')\n\
          \                prev_max = breakdown[-1][\"up_to\"] if breakdown else 0\n                tier_quantity = min(remaining,\
          \ tier_max - prev_max)\n\n                tier_amount = tier.flat_amount + (tier.unit_amount * tier_quantity)\n\
          \                total += tier_amount\n                remaining -= tier_quantity\n\n                breakdown.append({\n\
          \                    \"up_to\": tier.up_to,\n                    \"quantity\": tier_quantity,\n                \
          \    \"unit_amount\": tier.unit_amount,\n                    \"amount\": tier_amount\n                })\n\n   \
          \         return {\n                \"subtotal\": total,\n                \"quantity\": quantity,\n            \
          \    \"breakdown\": breakdown\n            }\n\n        elif price.pricing_model == PricingModel.VOLUME:\n     \
          \       # Single rate based on total quantity (find applicable tier)\n            applicable_tier = None\n     \
          \       for tier in sorted(price.tiers, key=lambda t: t.up_to or float('inf')):\n                if tier.up_to is\
          \ None or quantity <= tier.up_to:\n                    applicable_tier = tier\n                    break\n\n   \
          \         if not applicable_tier:\n                applicable_tier = price.tiers[-1]\n\n            total = applicable_tier.flat_amount\
          \ + (applicable_tier.unit_amount * quantity)\n            return {\n                \"subtotal\": total,\n     \
          \           \"quantity\": quantity,\n                \"unit_amount\": applicable_tier.unit_amount,\n           \
          \     \"tier_up_to\": applicable_tier.up_to\n            }\n\n        raise ValueError(f\"Unknown pricing model:\
          \ {price.pricing_model}\")\n\n    def compare_plans(self, plan_id_1: str, plan_id_2: str) -> dict:\n        '''Compare\
          \ features between two plans'''\n        plan1 = self.plans.get(plan_id_1)\n        plan2 = self.plans.get(plan_id_2)\n\
          \n        if not plan1 or not plan2:\n            raise ValueError(\"Plan not found\")\n\n        comparison = {}\n\
          \        all_features = set(plan1.features.keys()) | set(plan2.features.keys())\n\n        for feature_id in all_features:\n\
          \            comparison[feature_id] = {\n                plan_id_1: plan1.features.get(feature_id),\n          \
          \      plan_id_2: plan2.features.get(feature_id)\n            }\n\n        return comparison\n```\n"
      pitfalls:
      - 'Tiered vs Volume pricing: tiered charges each tier separately; volume uses single rate'
      - 'Currency precision: always use smallest unit (cents) to avoid float errors'
      - 'Plan changes: archive old plans rather than deleting (existing subscribers)'
      - 'Feature access: check plan features, not subscription status alone'
    - name: Subscription Lifecycle
      description: Implement subscription creation, activation, renewal, and cancellation
      skills:
      - State machines
      - Billing cycles
      - Grace periods
      hints:
        level1: 'Subscription has lifecycle: trialing -> active -> past_due -> canceled'
        level2: Track current_period_start/end for billing; anchor date for consistent billing
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          from datetime import datetime, timedelta\nimport calendar\n\nclass SubscriptionStatus(Enum):\n    TRIALING = \"\
          trialing\"\n    ACTIVE = \"active\"\n    PAST_DUE = \"past_due\"      # Payment failed, retrying\n    UNPAID = \"\
          unpaid\"          # Payment failed, no more retries\n    CANCELED = \"canceled\"\n    INCOMPLETE = \"incomplete\"\
          \   # Initial payment failed\n    INCOMPLETE_EXPIRED = \"incomplete_expired\"\n\nclass CancellationReason(Enum):\n\
          \    CUSTOMER_REQUEST = \"customer_request\"\n    PAYMENT_FAILURE = \"payment_failure\"\n    FRAUD = \"fraud\"\n\
          \n@dataclass\nclass Subscription:\n    id: str\n    customer_id: str\n    plan_id: str\n    price_id: str\n    status:\
          \ SubscriptionStatus\n    quantity: int\n    current_period_start: float\n    current_period_end: float\n    trial_start:\
          \ Optional[float]\n    trial_end: Optional[float]\n    canceled_at: Optional[float]\n    cancel_at_period_end: bool\n\
          \    ended_at: Optional[float]\n    billing_cycle_anchor: float\n    created_at: float\n    metadata: dict = field(default_factory=dict)\n\
          \nclass SubscriptionManager:\n    def __init__(self, pricing: PricingEngine,\n                 payment_service):\
          \  # PaymentService from payment project\n        self.pricing = pricing\n        self.payments = payment_service\n\
          \        self.subscriptions: dict[str, Subscription] = {}\n\n    def create_subscription(self, customer_id: str,\
          \ plan_id: str,\n                            currency: str = \"usd\",\n                            quantity: int\
          \ = 1,\n                            trial_from_plan: bool = True,\n                            billing_cycle_anchor:\
          \ int = None) -> Subscription:\n        plan = self.pricing.plans.get(plan_id)\n        if not plan:\n         \
          \   raise ValueError(\"Plan not found\")\n\n        price = plan.prices.get(currency)\n        if not price:\n \
          \           raise ValueError(\"Price not found\")\n\n        now = time.time()\n        sub_id = f\"sub_{secrets.token_urlsafe(16)}\"\
          \n\n        # Determine trial period\n        trial_end = None\n        if trial_from_plan and price.trial_period_days\
          \ > 0:\n            trial_end = now + (price.trial_period_days * 86400)\n\n        # Set billing anchor (day of\
          \ month for monthly, etc.)\n        anchor = billing_cycle_anchor or now\n\n        # Calculate initial period\n\
          \        if trial_end:\n            period_start = now\n            period_end = trial_end\n            status =\
          \ SubscriptionStatus.TRIALING\n        else:\n            period_start = now\n            period_end = self._calculate_period_end(\n\
          \                now, price.billing_interval, price.interval_count\n            )\n            status = SubscriptionStatus.INCOMPLETE\n\
          \n        subscription = Subscription(\n            id=sub_id,\n            customer_id=customer_id,\n         \
          \   plan_id=plan_id,\n            price_id=price.id,\n            status=status,\n            quantity=quantity,\n\
          \            current_period_start=period_start,\n            current_period_end=period_end,\n            trial_start=now\
          \ if trial_end else None,\n            trial_end=trial_end,\n            canceled_at=None,\n            cancel_at_period_end=False,\n\
          \            ended_at=None,\n            billing_cycle_anchor=anchor,\n            created_at=now\n        )\n\n\
          \        self.subscriptions[sub_id] = subscription\n\n        # If no trial, create initial payment\n        if\
          \ not trial_end:\n            self._create_initial_invoice(subscription)\n\n        return subscription\n\n    def\
          \ _calculate_period_end(self, start: float,\n                              interval: BillingInterval,\n        \
          \                      interval_count: int) -> float:\n        dt = datetime.fromtimestamp(start)\n\n        if\
          \ interval == BillingInterval.DAILY:\n            end = dt + timedelta(days=interval_count)\n        elif interval\
          \ == BillingInterval.WEEKLY:\n            end = dt + timedelta(weeks=interval_count)\n        elif interval == BillingInterval.MONTHLY:\n\
          \            # Add months, preserving day of month (or last day if overflow)\n            month = dt.month + interval_count\n\
          \            year = dt.year + (month - 1) // 12\n            month = (month - 1) % 12 + 1\n            day = min(dt.day,\
          \ calendar.monthrange(year, month)[1])\n            end = dt.replace(year=year, month=month, day=day)\n        elif\
          \ interval == BillingInterval.YEARLY:\n            try:\n                end = dt.replace(year=dt.year + interval_count)\n\
          \            except ValueError:\n                # Feb 29 -> Feb 28\n                end = dt.replace(year=dt.year\
          \ + interval_count, day=28)\n\n        return end.timestamp()\n\n    def cancel_subscription(self, subscription_id:\
          \ str,\n                            at_period_end: bool = True,\n                            reason: CancellationReason\
          \ = CancellationReason.CUSTOMER_REQUEST) -> Subscription:\n        sub = self.subscriptions.get(subscription_id)\n\
          \        if not sub:\n            raise ValueError(\"Subscription not found\")\n\n        if sub.status in [SubscriptionStatus.CANCELED,\n\
          \                          SubscriptionStatus.INCOMPLETE_EXPIRED]:\n            raise ValueError(\"Subscription\
          \ already canceled\")\n\n        sub.canceled_at = time.time()\n\n        if at_period_end:\n            # Cancel\
          \ at end of billing period (most common)\n            sub.cancel_at_period_end = True\n        else:\n         \
          \   # Immediate cancellation\n            sub.status = SubscriptionStatus.CANCELED\n            sub.ended_at = time.time()\n\
          \n        return sub\n\n    def reactivate_subscription(self, subscription_id: str) -> Subscription:\n        '''Reactivate\
          \ a subscription scheduled for cancellation'''\n        sub = self.subscriptions.get(subscription_id)\n        if\
          \ not sub:\n            raise ValueError(\"Subscription not found\")\n\n        if not sub.cancel_at_period_end:\n\
          \            raise ValueError(\"Subscription not scheduled for cancellation\")\n\n        if sub.status == SubscriptionStatus.CANCELED:\n\
          \            raise ValueError(\"Cannot reactivate fully canceled subscription\")\n\n        sub.cancel_at_period_end\
          \ = False\n        sub.canceled_at = None\n\n        return sub\n\n    def renew_subscription(self, subscription_id:\
          \ str) -> Subscription:\n        '''Process subscription renewal (called by billing job)'''\n        sub = self.subscriptions.get(subscription_id)\n\
          \        if not sub:\n            raise ValueError(\"Subscription not found\")\n\n        # Check if should be canceled\n\
          \        if sub.cancel_at_period_end:\n            sub.status = SubscriptionStatus.CANCELED\n            sub.ended_at\
          \ = time.time()\n            return sub\n\n        # Create invoice for new period\n        plan = self.pricing.plans[sub.plan_id]\n\
          \        price = plan.prices[sub.price_id.split(\"_\")[1]]  # Extract currency\n\n        # Update period\n    \
          \    sub.current_period_start = sub.current_period_end\n        sub.current_period_end = self._calculate_period_end(\n\
          \            sub.current_period_start,\n            price.billing_interval,\n            price.interval_count\n\
          \        )\n\n        # Create payment\n        self._create_renewal_invoice(sub)\n\n        return sub\n\n    def\
          \ _create_initial_invoice(self, sub: Subscription):\n        plan = self.pricing.plans[sub.plan_id]\n        price_info\
          \ = self.pricing.calculate_price(\n            sub.plan_id, \"usd\", sub.quantity\n        )\n\n        # In production:\
          \ create invoice, attempt payment\n        # If payment succeeds: sub.status = ACTIVE\n        # If payment fails:\
          \ sub.status = INCOMPLETE\n\n    def _create_renewal_invoice(self, sub: Subscription):\n        # Similar to initial,\
          \ but with retry logic for failures\n        pass\n```\n"
      pitfalls:
      - 'Billing anchor: ensures consistent billing date each month'
      - 'Past due vs unpaid: past_due = still retrying; unpaid = gave up'
      - 'Cancel at period end: most user-friendly, they paid for the period'
      - 'Month overflow: Jan 31 + 1 month = Feb 28, not Mar 3'
    - name: Proration & Plan Changes
      description: Implement upgrade/downgrade with prorated charges and credits
      skills:
      - Proration calculation
      - Credits
      - Mid-cycle changes
      hints:
        level1: 'Proration: charge/credit proportionally for unused time'
        level2: Upgrade immediately charges difference; downgrade creates credit for next invoice
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom decimal import Decimal, ROUND_HALF_UP\n\
          \nclass ProrationBehavior(Enum):\n    CREATE_PRORATIONS = \"create_prorations\"   # Immediate adjustment\n    NONE\
          \ = \"none\"                              # No proration\n    ALWAYS_INVOICE = \"always_invoice\"          # Invoice\
          \ immediately\n\n@dataclass\nclass ProrationItem:\n    description: str\n    amount: int           # Positive =\
          \ charge, negative = credit\n    quantity: int\n    period_start: float\n    period_end: float\n\nclass ProrationCalculator:\n\
          \    def calculate_proration(self, old_price: int, new_price: int,\n                            remaining_days:\
          \ int, total_days: int,\n                            old_quantity: int = 1,\n                            new_quantity:\
          \ int = 1) -> list[ProrationItem]:\n        '''\n        Calculate proration items for plan change.\n\n        Returns\
          \ list of line items:\n        - Credit for unused time on old plan\n        - Charge for remaining time on new\
          \ plan\n        '''\n        items = []\n\n        # Credit for unused time on old plan\n        daily_old = Decimal(old_price\
          \ * old_quantity) / total_days\n        credit = int((daily_old * remaining_days).quantize(\n            Decimal('1'),\
          \ rounding=ROUND_HALF_UP\n        ))\n\n        if credit > 0:\n            items.append(ProrationItem(\n      \
          \          description=\"Unused time on previous plan\",\n                amount=-credit,  # Negative = credit\n\
          \                quantity=old_quantity,\n                period_start=0,  # Will be filled in\n                period_end=0\n\
          \            ))\n\n        # Charge for remaining time on new plan\n        daily_new = Decimal(new_price * new_quantity)\
          \ / total_days\n        charge = int((daily_new * remaining_days).quantize(\n            Decimal('1'), rounding=ROUND_HALF_UP\n\
          \        ))\n\n        if charge > 0:\n            items.append(ProrationItem(\n                description=\"Remaining\
          \ time on new plan\",\n                amount=charge,  # Positive = charge\n                quantity=new_quantity,\n\
          \                period_start=0,\n                period_end=0\n            ))\n\n        return items\n\nclass\
          \ PlanChangeManager:\n    def __init__(self, subscription_manager: SubscriptionManager,\n                 pricing:\
          \ PricingEngine):\n        self.subscriptions = subscription_manager\n        self.pricing = pricing\n        self.proration_calc\
          \ = ProrationCalculator()\n        self.customer_credits: dict[str, int] = {}  # customer_id -> credit balance\n\
          \n    def preview_change(self, subscription_id: str, new_plan_id: str,\n                       new_quantity: int\
          \ = None) -> dict:\n        '''Preview what charges/credits would result from plan change'''\n        sub = self.subscriptions.subscriptions.get(subscription_id)\n\
          \        if not sub:\n            raise ValueError(\"Subscription not found\")\n\n        old_plan = self.pricing.plans[sub.plan_id]\n\
          \        new_plan = self.pricing.plans.get(new_plan_id)\n        if not new_plan:\n            raise ValueError(\"\
          New plan not found\")\n\n        currency = \"usd\"  # Simplified\n        old_price_info = self.pricing.calculate_price(\n\
          \            sub.plan_id, currency, sub.quantity\n        )\n        new_quantity = new_quantity or sub.quantity\n\
          \        new_price_info = self.pricing.calculate_price(\n            new_plan_id, currency, new_quantity\n     \
          \   )\n\n        # Calculate remaining time in period\n        now = time.time()\n        total_seconds = sub.current_period_end\
          \ - sub.current_period_start\n        remaining_seconds = sub.current_period_end - now\n        total_days = int(total_seconds\
          \ / 86400)\n        remaining_days = int(remaining_seconds / 86400)\n\n        items = self.proration_calc.calculate_proration(\n\
          \            old_price_info[\"subtotal\"],\n            new_price_info[\"subtotal\"],\n            remaining_days,\n\
          \            total_days,\n            sub.quantity,\n            new_quantity\n        )\n\n        total_due =\
          \ sum(item.amount for item in items)\n\n        return {\n            \"items\": [\n                {\n        \
          \            \"description\": item.description,\n                    \"amount\": item.amount\n                }\n\
          \                for item in items\n            ],\n            \"total_due\": total_due,\n            \"is_upgrade\"\
          : new_price_info[\"subtotal\"] > old_price_info[\"subtotal\"],\n            \"immediate_charge\": total_due if total_due\
          \ > 0 else 0,\n            \"credit_applied\": abs(total_due) if total_due < 0 else 0\n        }\n\n    def change_plan(self,\
          \ subscription_id: str, new_plan_id: str,\n                    new_quantity: int = None,\n                    proration_behavior:\
          \ ProrationBehavior = ProrationBehavior.CREATE_PRORATIONS) -> Subscription:\n        '''Execute plan change with\
          \ proration'''\n        sub = self.subscriptions.subscriptions.get(subscription_id)\n        if not sub:\n     \
          \       raise ValueError(\"Subscription not found\")\n\n        preview = self.preview_change(subscription_id, new_plan_id,\
          \ new_quantity)\n\n        if proration_behavior == ProrationBehavior.CREATE_PRORATIONS:\n            if preview[\"\
          total_due\"] > 0:\n                # Upgrade: charge immediately\n                # In production: create payment\
          \ intent and process\n                print(f\"Charging ${preview['total_due']/100:.2f} for upgrade\")\n       \
          \     elif preview[\"total_due\"] < 0:\n                # Downgrade: apply credit\n                credit = abs(preview[\"\
          total_due\"])\n                self.customer_credits[sub.customer_id] = (\n                    self.customer_credits.get(sub.customer_id,\
          \ 0) + credit\n                )\n                print(f\"Applied ${credit/100:.2f} credit for downgrade\")\n\n\
          \        # Update subscription\n        new_plan = self.pricing.plans[new_plan_id]\n        new_price = new_plan.prices.get(\"\
          usd\")\n\n        sub.plan_id = new_plan_id\n        sub.price_id = new_price.id\n        sub.quantity = new_quantity\
          \ or sub.quantity\n\n        return sub\n\n    def change_quantity(self, subscription_id: str,\n               \
          \         new_quantity: int) -> Subscription:\n        '''Change subscription quantity (seats)'''\n        sub =\
          \ self.subscriptions.subscriptions.get(subscription_id)\n        if not sub:\n            raise ValueError(\"Subscription\
          \ not found\")\n\n        return self.change_plan(subscription_id, sub.plan_id, new_quantity)\n\n    def apply_credit_to_invoice(self,\
          \ customer_id: str,\n                                invoice_amount: int) -> tuple[int, int]:\n        '''Apply\
          \ customer credit balance to invoice'''\n        credit = self.customer_credits.get(customer_id, 0)\n        if\
          \ credit <= 0:\n            return invoice_amount, 0\n\n        credit_applied = min(credit, invoice_amount)\n \
          \       remaining_due = invoice_amount - credit_applied\n        self.customer_credits[customer_id] = credit - credit_applied\n\
          \n        return remaining_due, credit_applied\n```\n"
      pitfalls:
      - 'Proration direction: upgrade charges extra; downgrade credits'
      - Round proration carefully - small errors accumulate over many customers
      - Credits must be applied automatically on next invoice
      - Quantity changes are proration too - not just plan changes
    - name: Usage-Based Billing
      description: Implement metered billing with usage tracking, aggregation, and reporting
      skills:
      - Usage metering
      - Aggregation
      - Rate limiting
      hints:
        level1: Track usage events in real-time; aggregate for billing at period end
        level2: 'Usage can be: sum, max, last, unique count over billing period'
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          from collections import defaultdict\nimport time\n\nclass AggregationType(Enum):\n    SUM = \"sum\"            \
          \  # Total usage (API calls, storage GB)\n    MAX = \"max\"              # Peak usage (concurrent users)\n    LAST\
          \ = \"last\"            # Final value (current storage)\n    UNIQUE_COUNT = \"unique\"  # Unique values (active\
          \ users)\n\n@dataclass\nclass Meter:\n    id: str\n    name: str\n    event_name: str                    # e.g.,\
          \ \"api_call\", \"storage_byte\"\n    aggregation: AggregationType\n    unit_label: str                    # e.g.,\
          \ \"API calls\", \"GB\"\n    filter_expression: Optional[str]   # Filter which events count\n\n@dataclass\nclass\
          \ UsageRecord:\n    meter_id: str\n    subscription_id: str\n    timestamp: float\n    quantity: int\n    properties:\
          \ dict                   # Additional metadata\n\n@dataclass\nclass UsageSummary:\n    meter_id: str\n    subscription_id:\
          \ str\n    period_start: float\n    period_end: float\n    total_quantity: int\n    billable_quantity: int\n   \
          \ unit_amount: int\n    total_amount: int\n\nclass UsageTracker:\n    def __init__(self):\n        self.meters:\
          \ dict[str, Meter] = {}\n        self.usage_records: list[UsageRecord] = []\n        # Indexed for fast lookup\n\
          \        self.usage_by_sub: dict[str, dict[str, list[UsageRecord]]] = defaultdict(\n            lambda: defaultdict(list)\n\
          \        )  # subscription_id -> meter_id -> records\n\n    def create_meter(self, meter_id: str, name: str, event_name:\
          \ str,\n                     aggregation: AggregationType,\n                     unit_label: str = \"units\") ->\
          \ Meter:\n        meter = Meter(\n            id=meter_id,\n            name=name,\n            event_name=event_name,\n\
          \            aggregation=aggregation,\n            unit_label=unit_label,\n            filter_expression=None\n\
          \        )\n        self.meters[meter_id] = meter\n        return meter\n\n    def record_usage(self, subscription_id:\
          \ str, meter_id: str,\n                     quantity: int = 1,\n                     timestamp: float = None,\n\
          \                     properties: dict = None,\n                     idempotency_key: str = None):\n        '''Record\
          \ a usage event'''\n        meter = self.meters.get(meter_id)\n        if not meter:\n            raise ValueError(\"\
          Meter not found\")\n\n        record = UsageRecord(\n            meter_id=meter_id,\n            subscription_id=subscription_id,\n\
          \            timestamp=timestamp or time.time(),\n            quantity=quantity,\n            properties=properties\
          \ or {}\n        )\n\n        # In production: check idempotency key to prevent duplicates\n        self.usage_records.append(record)\n\
          \        self.usage_by_sub[subscription_id][meter_id].append(record)\n\n    def get_usage_summary(self, subscription_id:\
          \ str, meter_id: str,\n                          period_start: float,\n                          period_end: float)\
          \ -> UsageSummary:\n        '''Aggregate usage for a billing period'''\n        meter = self.meters.get(meter_id)\n\
          \        if not meter:\n            raise ValueError(\"Meter not found\")\n\n        records = [\n            r\
          \ for r in self.usage_by_sub[subscription_id][meter_id]\n            if period_start <= r.timestamp < period_end\n\
          \        ]\n\n        if not records:\n            total = 0\n        elif meter.aggregation == AggregationType.SUM:\n\
          \            total = sum(r.quantity for r in records)\n        elif meter.aggregation == AggregationType.MAX:\n\
          \            total = max(r.quantity for r in records)\n        elif meter.aggregation == AggregationType.LAST:\n\
          \            latest = max(records, key=lambda r: r.timestamp)\n            total = latest.quantity\n        elif\
          \ meter.aggregation == AggregationType.UNIQUE_COUNT:\n            # Count unique values of a property (e.g., user_id)\n\
          \            unique_values = set()\n            for r in records:\n                unique_values.add(r.properties.get(\"\
          unique_key\", r.quantity))\n            total = len(unique_values)\n\n        return UsageSummary(\n           \
          \ meter_id=meter_id,\n            subscription_id=subscription_id,\n            period_start=period_start,\n   \
          \         period_end=period_end,\n            total_quantity=total,\n            billable_quantity=total,  # May\
          \ differ with included amounts\n            unit_amount=0,  # Set by billing\n            total_amount=0\n     \
          \   )\n\nclass UsageBilling:\n    def __init__(self, usage_tracker: UsageTracker, pricing: PricingEngine):\n   \
          \     self.usage = usage_tracker\n        self.pricing = pricing\n\n    def calculate_usage_charges(self, subscription_id:\
          \ str,\n                                period_start: float,\n                                period_end: float)\
          \ -> list[dict]:\n        '''Calculate usage charges for all meters'''\n        charges = []\n\n        for meter_id\
          \ in self.usage.meters:\n            summary = self.usage.get_usage_summary(\n                subscription_id, meter_id,\
          \ period_start, period_end\n            )\n\n            if summary.total_quantity == 0:\n                continue\n\
          \n            # In production: get meter-specific pricing from plan\n            # This is simplified\n        \
          \    unit_price = 10  # $0.10 per unit\n\n            charge = {\n                \"meter_id\": meter_id,\n    \
          \            \"meter_name\": self.usage.meters[meter_id].name,\n                \"quantity\": summary.total_quantity,\n\
          \                \"unit_amount\": unit_price,\n                \"total_amount\": summary.total_quantity * unit_price,\n\
          \                \"unit_label\": self.usage.meters[meter_id].unit_label\n            }\n            charges.append(charge)\n\
          \n        return charges\n\n    def get_current_usage(self, subscription_id: str) -> dict:\n        '''Get real-time\
          \ usage for customer dashboard'''\n        # In production: might need current period from subscription\n      \
          \  now = time.time()\n        period_start = now - (30 * 86400)  # Last 30 days approximation\n\n        usage =\
          \ {}\n        for meter_id, meter in self.usage.meters.items():\n            summary = self.usage.get_usage_summary(\n\
          \                subscription_id, meter_id, period_start, now\n            )\n            usage[meter_id] = {\n\
          \                \"name\": meter.name,\n                \"quantity\": summary.total_quantity,\n                \"\
          unit_label\": meter.unit_label\n            }\n\n        return usage\n\n# Example usage tracking middleware\nclass\
          \ UsageMiddleware:\n    def __init__(self, tracker: UsageTracker):\n        self.tracker = tracker\n\n    def track_api_call(self,\
          \ subscription_id: str, endpoint: str):\n        '''Track an API call'''\n        self.tracker.record_usage(\n \
          \           subscription_id=subscription_id,\n            meter_id=\"api_calls\",\n            quantity=1,\n   \
          \         properties={\"endpoint\": endpoint}\n        )\n\n    def track_storage(self, subscription_id: str, bytes_used:\
          \ int):\n        '''Track storage usage (last value aggregation)'''\n        self.tracker.record_usage(\n      \
          \      subscription_id=subscription_id,\n            meter_id=\"storage\",\n            quantity=bytes_used,  #\
          \ Total storage, not delta\n        )\n\n    def track_active_user(self, subscription_id: str, user_id: str):\n\
          \        '''Track unique active user'''\n        self.tracker.record_usage(\n            subscription_id=subscription_id,\n\
          \            meter_id=\"active_users\",\n            quantity=1,\n            properties={\"unique_key\": user_id}\n\
          \        )\n```\n"
      pitfalls:
      - 'Idempotency: same event reported twice shouldn''t be double-counted'
      - 'Clock skew: use server timestamps, not client timestamps'
      - 'Aggregation type matters: sum vs max give very different results'
      - 'Usage limits: alert before hitting limit, not after'
  notification-service:
    name: Multi-Channel Notification Service
    description: Build a unified notification system supporting email, SMS, push notifications, and in-app messages with templates,
      preferences, and delivery tracking.
    why_expert: Every app needs notifications. Understanding delivery patterns, rate limiting, and preference management helps
      build user-friendly communication systems.
    difficulty: expert
    tags:
    - notifications
    - messaging
    - email
    - push
    - sms
    estimated_hours: 45
    prerequisites:
    - build-message-queue
    milestones:
    - name: Channel Abstraction & Routing
      description: Implement pluggable channels (email, SMS, push) with intelligent routing
      skills:
      - Channel abstraction
      - Provider fallback
      - Routing logic
      hints:
        level1: Each channel (email, SMS, push) is a plugin implementing common interface
        level2: Router decides which channel(s) based on notification type and user preferences
        level3: "\n```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import\
          \ Optional\nfrom enum import Enum\nimport time\n\nclass Channel(Enum):\n    EMAIL = \"email\"\n    SMS = \"sms\"\
          \n    PUSH = \"push\"\n    IN_APP = \"in_app\"\n    WEBHOOK = \"webhook\"\n\nclass Priority(Enum):\n    LOW = \"\
          low\"\n    NORMAL = \"normal\"\n    HIGH = \"high\"\n    URGENT = \"urgent\"\n\n@dataclass\nclass Notification:\n\
          \    id: str\n    user_id: str\n    type: str                     # e.g., \"order_shipped\", \"password_reset\"\n\
          \    title: str\n    body: str\n    data: dict = field(default_factory=dict)\n    channels: list[Channel] = field(default_factory=list)\n\
          \    priority: Priority = Priority.NORMAL\n    created_at: float = field(default_factory=time.time)\n    scheduled_for:\
          \ Optional[float] = None\n    expires_at: Optional[float] = None\n\n@dataclass\nclass DeliveryResult:\n    channel:\
          \ Channel\n    success: bool\n    provider: str\n    message_id: Optional[str] = None\n    error: Optional[str]\
          \ = None\n    delivered_at: Optional[float] = None\n\nclass NotificationChannel(ABC):\n    @abstractmethod\n   \
          \ def send(self, notification: Notification, recipient: dict) -> DeliveryResult:\n        pass\n\n    @abstractmethod\n\
          \    def supports_batch(self) -> bool:\n        pass\n\nclass EmailChannel(NotificationChannel):\n    def __init__(self,\
          \ providers: list):\n        self.providers = providers  # Ordered by preference\n        self.current_provider_idx\
          \ = 0\n\n    def send(self, notification: Notification, recipient: dict) -> DeliveryResult:\n        email = recipient.get(\"\
          email\")\n        if not email:\n            return DeliveryResult(\n                channel=Channel.EMAIL,\n  \
          \              success=False,\n                provider=\"none\",\n                error=\"No email address\"\n\
          \            )\n\n        # Try providers with fallback\n        for i, provider in enumerate(self.providers[self.current_provider_idx:]):\n\
          \            try:\n                result = provider.send_email(\n                    to=email,\n              \
          \      subject=notification.title,\n                    body=notification.body,\n                    html=notification.data.get(\"\
          html_body\")\n                )\n                return DeliveryResult(\n                    channel=Channel.EMAIL,\n\
          \                    success=True,\n                    provider=provider.name,\n                    message_id=result.message_id,\n\
          \                    delivered_at=time.time()\n                )\n            except Exception as e:\n         \
          \       if i < len(self.providers) - 1:\n                    continue  # Try next provider\n                return\
          \ DeliveryResult(\n                    channel=Channel.EMAIL,\n                    success=False,\n            \
          \        provider=provider.name,\n                    error=str(e)\n                )\n\n    def supports_batch(self)\
          \ -> bool:\n        return True\n\nclass SMSChannel(NotificationChannel):\n    def __init__(self, provider):\n \
          \       self.provider = provider\n\n    def send(self, notification: Notification, recipient: dict) -> DeliveryResult:\n\
          \        phone = recipient.get(\"phone\")\n        if not phone:\n            return DeliveryResult(\n         \
          \       channel=Channel.SMS,\n                success=False,\n                provider=\"none\",\n             \
          \   error=\"No phone number\"\n            )\n\n        try:\n            # SMS is expensive - keep messages short\n\
          \            message = notification.body[:160]  # SMS character limit\n\n            result = self.provider.send_sms(to=phone,\
          \ body=message)\n            return DeliveryResult(\n                channel=Channel.SMS,\n                success=True,\n\
          \                provider=self.provider.name,\n                message_id=result.sid\n            )\n        except\
          \ Exception as e:\n            return DeliveryResult(\n                channel=Channel.SMS,\n                success=False,\n\
          \                provider=self.provider.name,\n                error=str(e)\n            )\n\n    def supports_batch(self)\
          \ -> bool:\n        return False\n\nclass PushChannel(NotificationChannel):\n    def __init__(self, fcm_client,\
          \ apns_client):\n        self.fcm = fcm_client      # Firebase for Android\n        self.apns = apns_client    #\
          \ Apple Push for iOS\n\n    def send(self, notification: Notification, recipient: dict) -> DeliveryResult:\n   \
          \     device_tokens = recipient.get(\"device_tokens\", [])\n        if not device_tokens:\n            return DeliveryResult(\n\
          \                channel=Channel.PUSH,\n                success=False,\n                provider=\"none\",\n   \
          \             error=\"No device tokens\"\n            )\n\n        errors = []\n        for token in device_tokens:\n\
          \            try:\n                platform = token.get(\"platform\", \"android\")\n                push_token =\
          \ token.get(\"token\")\n\n                if platform == \"ios\":\n                    self.apns.send(\n       \
          \                 token=push_token,\n                        title=notification.title,\n                       \
          \ body=notification.body,\n                        data=notification.data\n                    )\n             \
          \   else:\n                    self.fcm.send(\n                        token=push_token,\n                     \
          \   title=notification.title,\n                        body=notification.body,\n                        data=notification.data\n\
          \                    )\n            except Exception as e:\n                errors.append(str(e))\n\n        if\
          \ len(errors) == len(device_tokens):\n            return DeliveryResult(\n                channel=Channel.PUSH,\n\
          \                success=False,\n                provider=\"fcm/apns\",\n                error=\"; \".join(errors)\n\
          \            )\n\n        return DeliveryResult(\n            channel=Channel.PUSH,\n            success=True,\n\
          \            provider=\"fcm/apns\",\n            delivered_at=time.time()\n        )\n\n    def supports_batch(self)\
          \ -> bool:\n        return True\n\nclass NotificationRouter:\n    def __init__(self):\n        self.channels: dict[Channel,\
          \ NotificationChannel] = {}\n        self.type_channel_map: dict[str, list[Channel]] = {}\n        self.priority_channel_map:\
          \ dict[Priority, list[Channel]] = {\n            Priority.URGENT: [Channel.SMS, Channel.PUSH, Channel.EMAIL],\n\
          \            Priority.HIGH: [Channel.PUSH, Channel.EMAIL],\n            Priority.NORMAL: [Channel.EMAIL, Channel.IN_APP],\n\
          \            Priority.LOW: [Channel.IN_APP]\n        }\n\n    def register_channel(self, channel: Channel, implementation:\
          \ NotificationChannel):\n        self.channels[channel] = implementation\n\n    def configure_notification_type(self,\
          \ notification_type: str, channels: list[Channel]):\n        self.type_channel_map[notification_type] = channels\n\
          \n    def route(self, notification: Notification, user_preferences: dict) -> list[Channel]:\n        '''Determine\
          \ which channels to use for a notification'''\n\n        # 1. Check if channels explicitly specified\n        if\
          \ notification.channels:\n            return notification.channels\n\n        # 2. Check notification type configuration\n\
          \        if notification.type in self.type_channel_map:\n            channels = self.type_channel_map[notification.type]\n\
          \        else:\n            # 3. Fall back to priority-based routing\n            channels = self.priority_channel_map.get(\n\
          \                notification.priority,\n                [Channel.EMAIL]\n            )\n\n        # 4. Filter by\
          \ user preferences\n        filtered = []\n        for channel in channels:\n            pref_key = f\"{channel.value}_enabled\"\
          \n            if user_preferences.get(pref_key, True):  # Default enabled\n                filtered.append(channel)\n\
          \n        # 5. Check quiet hours (don't SMS/Push during sleep)\n        if self._is_quiet_hours(user_preferences):\n\
          \            filtered = [c for c in filtered if c not in [Channel.SMS, Channel.PUSH]]\n\n        return filtered\
          \ or [Channel.IN_APP]  # Always fallback to in-app\n\n    def _is_quiet_hours(self, preferences: dict) -> bool:\n\
          \        quiet_start = preferences.get(\"quiet_hours_start\")  # e.g., 22 (10 PM)\n        quiet_end = preferences.get(\"\
          quiet_hours_end\")      # e.g., 8 (8 AM)\n\n        if not quiet_start or not quiet_end:\n            return False\n\
          \n        from datetime import datetime\n        hour = datetime.now().hour\n\n        if quiet_start < quiet_end:\n\
          \            return quiet_start <= hour < quiet_end\n        else:  # Crosses midnight\n            return hour\
          \ >= quiet_start or hour < quiet_end\n```\n"
      pitfalls:
      - Provider fallback must track failures to avoid cascading errors
      - SMS costs money - only use for truly urgent notifications
      - Push token can become invalid - handle token refresh errors
      - Quiet hours must consider user's timezone, not server's
    - name: Template System
      description: Implement notification templates with localization and personalization
      skills:
      - Template engines
      - i18n
      - Content personalization
      hints:
        level1: Templates separate content from delivery logic - easier to update
        level2: Support different content per channel (email HTML vs SMS text)
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom jinja2 import Environment,\
          \ BaseLoader, TemplateNotFound\nimport json\n\n@dataclass\nclass NotificationTemplate:\n    id: str\n    name: str\n\
          \    type: str                    # Notification type this template is for\n    channel_content: dict        # channel\
          \ -> {subject, body, html_body}\n    variables: list[str]         # Required variables\n    default_locale: str\
          \ = \"en\"\n    localized_content: dict = field(default_factory=dict)  # locale -> channel_content\n\nclass TemplateStore:\n\
          \    def __init__(self):\n        self.templates: dict[str, NotificationTemplate] = {}\n\n    def create_template(self,\
          \ template_id: str, name: str, notification_type: str,\n                        channel_content: dict, variables:\
          \ list[str]) -> NotificationTemplate:\n        template = NotificationTemplate(\n            id=template_id,\n \
          \           name=name,\n            type=notification_type,\n            channel_content=channel_content,\n    \
          \        variables=variables\n        )\n        self.templates[template_id] = template\n        return template\n\
          \n    def add_localization(self, template_id: str, locale: str, channel_content: dict):\n        template = self.templates.get(template_id)\n\
          \        if not template:\n            raise ValueError(\"Template not found\")\n        template.localized_content[locale]\
          \ = channel_content\n\n    def get_template(self, template_id: str, locale: str = None) -> dict:\n        template\
          \ = self.templates.get(template_id)\n        if not template:\n            raise ValueError(\"Template not found\"\
          )\n\n        if locale and locale in template.localized_content:\n            return template.localized_content[locale]\n\
          \        return template.channel_content\n\nclass TemplateRenderer:\n    def __init__(self, template_store: TemplateStore):\n\
          \        self.store = template_store\n        self.jinja_env = Environment(loader=BaseLoader())\n\n        # Add\
          \ custom filters\n        self.jinja_env.filters['currency'] = self._currency_filter\n        self.jinja_env.filters['date']\
          \ = self._date_filter\n        self.jinja_env.filters['truncate_sms'] = lambda s: s[:160] if s else ''\n\n    def\
          \ render(self, template_id: str, channel: Channel,\n               variables: dict, locale: str = \"en\") -> dict:\n\
          \        '''Render template for a specific channel'''\n        template_content = self.store.get_template(template_id,\
          \ locale)\n        channel_content = template_content.get(channel.value, {})\n\n        if not channel_content:\n\
          \            # Fallback to email content\n            channel_content = template_content.get(\"email\", {})\n\n\
          \        rendered = {}\n        for key in ['subject', 'body', 'html_body']:\n            if key in channel_content:\n\
          \                template = self.jinja_env.from_string(channel_content[key])\n                rendered[key] = template.render(**variables)\n\
          \n        return rendered\n\n    def validate_variables(self, template_id: str, variables: dict) -> list[str]:\n\
          \        '''Check if all required variables are provided'''\n        template = self.store.templates.get(template_id)\n\
          \        if not template:\n            raise ValueError(\"Template not found\")\n\n        missing = [v for v in\
          \ template.variables if v not in variables]\n        return missing\n\n    def _currency_filter(self, amount: int,\
          \ currency: str = \"USD\") -> str:\n        '''Format amount (in cents) as currency'''\n        symbols = {\"USD\"\
          : \"$\", \"EUR\": \"â‚¬\", \"GBP\": \"Â£\"}\n        symbol = symbols.get(currency, currency)\n        return f\"{symbol}{amount/100:.2f}\"\
          \n\n    def _date_filter(self, timestamp: float, format: str = \"%B %d, %Y\") -> str:\n        '''Format timestamp\
          \ as date'''\n        from datetime import datetime\n        return datetime.fromtimestamp(timestamp).strftime(format)\n\
          \n# Example template setup\ndef setup_templates(store: TemplateStore):\n    # Order shipped notification\n    store.create_template(\n\
          \        template_id=\"order_shipped\",\n        name=\"Order Shipped\",\n        notification_type=\"order_shipped\"\
          ,\n        channel_content={\n            \"email\": {\n                \"subject\": \"Your order #{{ order_id }}\
          \ has shipped!\",\n                \"body\": \"Hi {{ user_name }},\\n\\nGreat news! Your order has shipped.\\n\\\
          nTracking: {{ tracking_number }}\\nCarrier: {{ carrier }}\\n\\nEstimated delivery: {{ delivery_date | date }}\"\
          ,\n                \"html_body\": '''\n                    <h1>Your order is on its way!</h1>\n                \
          \    <p>Hi {{ user_name }},</p>\n                    <p>Great news! Your order <strong>#{{ order_id }}</strong>\
          \ has shipped.</p>\n                    <div class=\"tracking-info\">\n                        <p><strong>Tracking\
          \ Number:</strong> {{ tracking_number }}</p>\n                        <p><strong>Carrier:</strong> {{ carrier }}</p>\n\
          \                        <p><strong>Estimated Delivery:</strong> {{ delivery_date | date }}</p>\n              \
          \      </div>\n                    <a href=\"{{ tracking_url }}\" class=\"button\">Track Package</a>\n         \
          \       '''\n            },\n            \"sms\": {\n                \"body\": \"Order #{{ order_id }} shipped!\
          \ Track: {{ tracking_url | truncate_sms }}\"\n            },\n            \"push\": {\n                \"subject\"\
          : \"Order Shipped\",\n                \"body\": \"Your order #{{ order_id }} is on the way!\"\n            }\n \
          \       },\n        variables=[\"user_name\", \"order_id\", \"tracking_number\", \"carrier\",\n                \
          \   \"delivery_date\", \"tracking_url\"]\n    )\n\n    # Add Vietnamese localization\n    store.add_localization(\"\
          order_shipped\", \"vi\", {\n        \"email\": {\n            \"subject\": \"ÄÆ¡n hÃ ng #{{ order_id }} Ä‘Ã£ Ä‘Æ°á»£c gá»­i!\"\
          ,\n            \"body\": \"Xin chÃ o {{ user_name }},\\n\\nÄÆ¡n hÃ ng cá»§a báº¡n Ä‘Ã£ Ä‘Æ°á»£c gá»­i.\\n\\nMÃ£ váº­n Ä‘Æ¡n: {{ tracking_number\
          \ }}\"\n        },\n        \"sms\": {\n            \"body\": \"ÄÆ¡n hÃ ng #{{ order_id }} Ä‘Ã£ gá»­i! Theo dÃµi: {{ tracking_url\
          \ | truncate_sms }}\"\n        }\n    })\n```\n"
      pitfalls:
      - SMS templates must fit in 160 chars (or pay for multiple segments)
      - HTML email needs inline styles - CSS classes often stripped
      - Template variables need escaping for XSS prevention in HTML
      - Locale detection should fall back gracefully (vi -> vi-VN -> en)
    - name: User Preferences & Unsubscribe
      description: Implement user notification preferences with granular controls and one-click unsubscribe
      skills:
      - Preference management
      - GDPR compliance
      - Unsubscribe handling
      hints:
        level1: 'Users need control: what notifications, which channels, when'
        level2: 'CAN-SPAM/GDPR: must include unsubscribe link in marketing emails'
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          import secrets\nimport time\nimport hashlib\nimport hmac\n\nclass NotificationCategory(Enum):\n    TRANSACTIONAL\
          \ = \"transactional\"  # Order confirmations, receipts\n    SECURITY = \"security\"             # Password reset,\
          \ login alerts\n    MARKETING = \"marketing\"           # Promotions, newsletters\n    PRODUCT = \"product\"   \
          \            # Feature updates, tips\n    SOCIAL = \"social\"                 # Comments, mentions\n\n@dataclass\n\
          class UserPreferences:\n    user_id: str\n    email_enabled: bool = True\n    sms_enabled: bool = False\n    push_enabled:\
          \ bool = True\n    in_app_enabled: bool = True\n\n    # Category-specific settings\n    category_settings: dict[str,\
          \ dict] = field(default_factory=dict)\n    # e.g., {\"marketing\": {\"email\": False, \"push\": True}}\n\n    #\
          \ Quiet hours\n    quiet_hours_enabled: bool = False\n    quiet_hours_start: int = 22  # 10 PM\n    quiet_hours_end:\
          \ int = 8     # 8 AM\n    timezone: str = \"UTC\"\n\n    # Frequency limits\n    max_emails_per_day: Optional[int]\
          \ = None\n    max_sms_per_week: Optional[int] = None\n\n    # Unsubscribe tracking\n    unsubscribed_types: set[str]\
          \ = field(default_factory=set)\n    global_unsubscribe: bool = False\n\n    updated_at: float = field(default_factory=time.time)\n\
          \nclass PreferenceManager:\n    def __init__(self, secret_key: bytes):\n        self.preferences: dict[str, UserPreferences]\
          \ = {}\n        self.secret_key = secret_key\n        # Track sends for frequency limiting\n        self.send_counts:\
          \ dict[str, dict] = {}  # user_id -> {channel: count}\n\n    def get_preferences(self, user_id: str) -> UserPreferences:\n\
          \        if user_id not in self.preferences:\n            self.preferences[user_id] = UserPreferences(user_id=user_id)\n\
          \        return self.preferences[user_id]\n\n    def update_preferences(self, user_id: str, updates: dict) -> UserPreferences:\n\
          \        prefs = self.get_preferences(user_id)\n\n        for key, value in updates.items():\n            if hasattr(prefs,\
          \ key):\n                setattr(prefs, key, value)\n\n        prefs.updated_at = time.time()\n        return prefs\n\
          \n    def set_category_preference(self, user_id: str, category: NotificationCategory,\n                        \
          \         channel: Channel, enabled: bool):\n        prefs = self.get_preferences(user_id)\n\n        if category.value\
          \ not in prefs.category_settings:\n            prefs.category_settings[category.value] = {}\n\n        prefs.category_settings[category.value][channel.value]\
          \ = enabled\n        prefs.updated_at = time.time()\n\n    def can_send(self, user_id: str, notification_type: str,\n\
          \                 channel: Channel, category: NotificationCategory) -> tuple[bool, str]:\n        '''Check if notification\
          \ can be sent to user'''\n        prefs = self.get_preferences(user_id)\n\n        # Global unsubscribe\n      \
          \  if prefs.global_unsubscribe and category != NotificationCategory.TRANSACTIONAL:\n            return False, \"\
          global_unsubscribe\"\n\n        # Type-specific unsubscribe\n        if notification_type in prefs.unsubscribed_types:\n\
          \            return False, f\"unsubscribed_from_{notification_type}\"\n\n        # Channel disabled globally\n \
          \       channel_enabled = getattr(prefs, f\"{channel.value}_enabled\", True)\n        if not channel_enabled:\n\
          \            return False, f\"{channel.value}_disabled\"\n\n        # Category-specific setting\n        cat_settings\
          \ = prefs.category_settings.get(category.value, {})\n        if channel.value in cat_settings and not cat_settings[channel.value]:\n\
          \            return False, f\"category_{category.value}_disabled_for_{channel.value}\"\n\n        # Frequency limits\n\
          \        if not self._check_frequency_limit(user_id, channel, prefs):\n            return False, \"frequency_limit_exceeded\"\
          \n\n        # Transactional and security always allowed (after unsubscribe checks)\n        if category in [NotificationCategory.TRANSACTIONAL,\
          \ NotificationCategory.SECURITY]:\n            return True, \"allowed_transactional\"\n\n        return True, \"\
          allowed\"\n\n    def _check_frequency_limit(self, user_id: str, channel: Channel,\n                            \
          \    prefs: UserPreferences) -> bool:\n        counts = self.send_counts.get(user_id, {})\n\n        if channel\
          \ == Channel.EMAIL and prefs.max_emails_per_day:\n            today_count = counts.get(\"email_today\", 0)\n   \
          \         return today_count < prefs.max_emails_per_day\n\n        if channel == Channel.SMS and prefs.max_sms_per_week:\n\
          \            week_count = counts.get(\"sms_week\", 0)\n            return week_count < prefs.max_sms_per_week\n\n\
          \        return True\n\n    def record_send(self, user_id: str, channel: Channel):\n        if user_id not in self.send_counts:\n\
          \            self.send_counts[user_id] = {}\n\n        if channel == Channel.EMAIL:\n            self.send_counts[user_id][\"\
          email_today\"] = (\n                self.send_counts[user_id].get(\"email_today\", 0) + 1\n            )\n     \
          \   elif channel == Channel.SMS:\n            self.send_counts[user_id][\"sms_week\"] = (\n                self.send_counts[user_id].get(\"\
          sms_week\", 0) + 1\n            )\n\n    # Unsubscribe link generation\n    def generate_unsubscribe_token(self,\
          \ user_id: str,\n                                    notification_type: str = None) -> str:\n        '''Generate\
          \ signed unsubscribe token'''\n        timestamp = str(int(time.time()))\n        data = f\"{user_id}|{notification_type\
          \ or 'all'}|{timestamp}\"\n\n        signature = hmac.new(\n            self.secret_key,\n            data.encode(),\n\
          \            hashlib.sha256\n        ).hexdigest()[:16]\n\n        token = f\"{data}|{signature}\"\n        # Base64\
          \ encode for URL safety\n        import base64\n        return base64.urlsafe_b64encode(token.encode()).decode()\n\
          \n    def process_unsubscribe(self, token: str) -> dict:\n        '''Process unsubscribe from token'''\n       \
          \ import base64\n        try:\n            decoded = base64.urlsafe_b64decode(token.encode()).decode()\n       \
          \     parts = decoded.split(\"|\")\n            if len(parts) != 4:\n                return {\"success\": False,\
          \ \"error\": \"invalid_token\"}\n\n            user_id, notification_type, timestamp, signature = parts\n\n    \
          \        # Verify signature\n            data = f\"{user_id}|{notification_type}|{timestamp}\"\n            expected\
          \ = hmac.new(\n                self.secret_key,\n                data.encode(),\n                hashlib.sha256\n\
          \            ).hexdigest()[:16]\n\n            if not hmac.compare_digest(signature, expected):\n              \
          \  return {\"success\": False, \"error\": \"invalid_signature\"}\n\n            # Check token age (30 days max)\n\
          \            if time.time() - int(timestamp) > 30 * 86400:\n                return {\"success\": False, \"error\"\
          : \"token_expired\"}\n\n            # Process unsubscribe\n            prefs = self.get_preferences(user_id)\n \
          \           if notification_type == \"all\":\n                prefs.global_unsubscribe = True\n            else:\n\
          \                prefs.unsubscribed_types.add(notification_type)\n\n            return {\n                \"success\"\
          : True,\n                \"user_id\": user_id,\n                \"unsubscribed_from\": notification_type\n     \
          \       }\n\n        except Exception as e:\n            return {\"success\": False, \"error\": str(e)}\n\n    def\
          \ generate_unsubscribe_url(self, user_id: str, notification_type: str,\n                                  base_url:\
          \ str) -> str:\n        token = self.generate_unsubscribe_token(user_id, notification_type)\n        return f\"\
          {base_url}/unsubscribe?token={token}\"\n```\n"
      pitfalls:
      - Transactional emails (receipts, password reset) can't be unsubscribed per CAN-SPAM
      - One-click unsubscribe required for marketing - RFC 8058
      - Unsubscribe token must be signed to prevent enumeration attacks
      - 'GDPR: must honor unsubscribe within 10 days (do it immediately)'
    - name: Delivery Tracking & Analytics
      description: Implement delivery status tracking, open/click tracking, and analytics
      skills:
      - Event tracking
      - Pixel tracking
      - Analytics
      hints:
        level1: 'Track: sent, delivered, bounced, opened, clicked'
        level2: Email opens tracked via tracking pixel; clicks via redirect
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          from collections import defaultdict\nimport time\nimport secrets\nimport hashlib\n\nclass DeliveryStatus(Enum):\n\
          \    PENDING = \"pending\"\n    SENT = \"sent\"\n    DELIVERED = \"delivered\"\n    OPENED = \"opened\"\n    CLICKED\
          \ = \"clicked\"\n    BOUNCED = \"bounced\"\n    COMPLAINED = \"complained\"  # Spam report\n    FAILED = \"failed\"\
          \n    UNSUBSCRIBED = \"unsubscribed\"\n\n@dataclass\nclass DeliveryEvent:\n    notification_id: str\n    channel:\
          \ Channel\n    status: DeliveryStatus\n    timestamp: float\n    metadata: dict = field(default_factory=dict)\n\n\
          @dataclass\nclass NotificationDelivery:\n    notification_id: str\n    user_id: str\n    channel: Channel\n    status:\
          \ DeliveryStatus\n    provider_message_id: Optional[str]\n    events: list[DeliveryEvent] = field(default_factory=list)\n\
          \    created_at: float = field(default_factory=time.time)\n\nclass DeliveryTracker:\n    def __init__(self, tracking_domain:\
          \ str, secret_key: bytes):\n        self.deliveries: dict[str, NotificationDelivery] = {}\n        self.tracking_domain\
          \ = tracking_domain\n        self.secret_key = secret_key\n        # Tracking ID -> notification mapping\n     \
          \   self.tracking_ids: dict[str, str] = {}\n\n    def create_delivery(self, notification_id: str, user_id: str,\n\
          \                        channel: Channel) -> NotificationDelivery:\n        delivery = NotificationDelivery(\n\
          \            notification_id=notification_id,\n            user_id=user_id,\n            channel=channel,\n    \
          \        status=DeliveryStatus.PENDING,\n            provider_message_id=None\n        )\n        self.deliveries[notification_id]\
          \ = delivery\n        return delivery\n\n    def update_status(self, notification_id: str, status: DeliveryStatus,\n\
          \                      metadata: dict = None):\n        delivery = self.deliveries.get(notification_id)\n      \
          \  if not delivery:\n            return\n\n        event = DeliveryEvent(\n            notification_id=notification_id,\n\
          \            channel=delivery.channel,\n            status=status,\n            timestamp=time.time(),\n       \
          \     metadata=metadata or {}\n        )\n\n        delivery.events.append(event)\n        delivery.status = status\n\
          \n    def handle_provider_webhook(self, provider: str, event_type: str,\n                                 data:\
          \ dict):\n        '''Handle delivery webhooks from email/SMS providers'''\n        # Map provider event types to\
          \ our statuses\n        event_mapping = {\n            \"sendgrid\": {\n                \"delivered\": DeliveryStatus.DELIVERED,\n\
          \                \"open\": DeliveryStatus.OPENED,\n                \"click\": DeliveryStatus.CLICKED,\n        \
          \        \"bounce\": DeliveryStatus.BOUNCED,\n                \"spamreport\": DeliveryStatus.COMPLAINED\n      \
          \      },\n            \"twilio\": {\n                \"delivered\": DeliveryStatus.DELIVERED,\n               \
          \ \"failed\": DeliveryStatus.FAILED,\n                \"undelivered\": DeliveryStatus.BOUNCED\n            }\n \
          \       }\n\n        mapping = event_mapping.get(provider, {})\n        status = mapping.get(event_type)\n\n   \
          \     if status:\n            message_id = data.get(\"message_id\") or data.get(\"MessageSid\")\n            # Find\
          \ notification by provider message ID\n            for nid, delivery in self.deliveries.items():\n             \
          \   if delivery.provider_message_id == message_id:\n                    self.update_status(nid, status, {\"provider_event\"\
          : event_type})\n                    break\n\n    # Email tracking pixel and link tracking\n    def generate_tracking_pixel_url(self,\
          \ notification_id: str) -> str:\n        '''Generate unique tracking pixel URL for email opens'''\n        tracking_id\
          \ = secrets.token_urlsafe(16)\n        self.tracking_ids[tracking_id] = notification_id\n\n        # Sign to prevent\
          \ forgery\n        signature = hmac.new(\n            self.secret_key,\n            tracking_id.encode(),\n    \
          \        hashlib.sha256\n        ).hexdigest()[:8]\n\n        return f\"https://{self.tracking_domain}/track/open/{tracking_id}/{signature}.gif\"\
          \n\n    def generate_tracking_link(self, notification_id: str,\n                                original_url: str,\
          \ link_id: str = None) -> str:\n        '''Generate tracking redirect URL for link clicks'''\n        tracking_id\
          \ = secrets.token_urlsafe(16)\n        link_id = link_id or hashlib.md5(original_url.encode()).hexdigest()[:8]\n\
          \n        # Store mapping\n        self.tracking_ids[tracking_id] = {\n            \"notification_id\": notification_id,\n\
          \            \"original_url\": original_url,\n            \"link_id\": link_id\n        }\n\n        return f\"\
          https://{self.tracking_domain}/track/click/{tracking_id}\"\n\n    def handle_tracking_pixel(self, tracking_id: str,\
          \ signature: str,\n                               ip: str, user_agent: str) -> bytes:\n        '''Handle tracking\
          \ pixel request - return 1x1 transparent GIF'''\n        # Verify signature\n        expected = hmac.new(\n    \
          \        self.secret_key,\n            tracking_id.encode(),\n            hashlib.sha256\n        ).hexdigest()[:8]\n\
          \n        if hmac.compare_digest(signature, expected):\n            notification_id = self.tracking_ids.get(tracking_id)\n\
          \            if notification_id:\n                self.update_status(notification_id, DeliveryStatus.OPENED, {\n\
          \                    \"ip\": ip,\n                    \"user_agent\": user_agent\n                })\n\n       \
          \ # Return 1x1 transparent GIF\n        return bytes([\n            0x47, 0x49, 0x46, 0x38, 0x39, 0x61, 0x01, 0x00,\n\
          \            0x01, 0x00, 0x80, 0x00, 0x00, 0xff, 0xff, 0xff,\n            0x00, 0x00, 0x00, 0x21, 0xf9, 0x04, 0x01,\
          \ 0x00,\n            0x00, 0x00, 0x00, 0x2c, 0x00, 0x00, 0x00, 0x00,\n            0x01, 0x00, 0x01, 0x00, 0x00,\
          \ 0x02, 0x02, 0x44,\n            0x01, 0x00, 0x3b\n        ])\n\n    def handle_tracking_click(self, tracking_id:\
          \ str,\n                               ip: str, user_agent: str) -> Optional[str]:\n        '''Handle click tracking\
          \ - return redirect URL'''\n        data = self.tracking_ids.get(tracking_id)\n        if not data or not isinstance(data,\
          \ dict):\n            return None\n\n        self.update_status(data[\"notification_id\"], DeliveryStatus.CLICKED,\
          \ {\n            \"ip\": ip,\n            \"user_agent\": user_agent,\n            \"link_id\": data[\"link_id\"\
          ]\n        })\n\n        return data[\"original_url\"]\n\nclass NotificationAnalytics:\n    def __init__(self, tracker:\
          \ DeliveryTracker):\n        self.tracker = tracker\n\n    def get_delivery_stats(self, start_time: float, end_time:\
          \ float) -> dict:\n        '''Get delivery statistics for time period'''\n        stats = defaultdict(lambda: defaultdict(int))\n\
          \n        for delivery in self.tracker.deliveries.values():\n            if start_time <= delivery.created_at <=\
          \ end_time:\n                channel = delivery.channel.value\n                stats[channel][\"total\"] += 1\n\
          \                stats[channel][delivery.status.value] += 1\n\n        # Calculate rates\n        result = {}\n\
          \        for channel, counts in stats.items():\n            total = counts[\"total\"]\n            result[channel]\
          \ = {\n                \"total\": total,\n                \"delivered\": counts.get(\"delivered\", 0),\n       \
          \         \"delivery_rate\": counts.get(\"delivered\", 0) / total if total else 0,\n                \"opened\":\
          \ counts.get(\"opened\", 0),\n                \"open_rate\": counts.get(\"opened\", 0) / counts.get(\"delivered\"\
          , 1),\n                \"clicked\": counts.get(\"clicked\", 0),\n                \"click_rate\": counts.get(\"clicked\"\
          , 0) / counts.get(\"opened\", 1),\n                \"bounced\": counts.get(\"bounced\", 0),\n                \"\
          bounce_rate\": counts.get(\"bounced\", 0) / total if total else 0,\n                \"complained\": counts.get(\"\
          complained\", 0)\n            }\n\n        return result\n\n    def get_notification_type_performance(self, notification_type:\
          \ str) -> dict:\n        '''Analyze performance of specific notification type'''\n        # Would join with notification\
          \ data to get type\n        pass\n```\n"
      pitfalls:
      - Email opens tracked via pixel are unreliable (image blocking)
      - Apple Mail Privacy Protection prefetches pixels - inflates open rates
      - Don't track transactional emails for privacy (password resets)
      - Spam complaints must trigger immediate unsubscribe
  webhook-delivery:
    name: Webhook Delivery System
    description: Build a reliable webhook delivery system with signature verification, retry logic, circuit breakers, and
      delivery guarantees.
    why_expert: Webhooks are the backbone of modern integrations. Building one teaches async delivery patterns, failure handling,
      and at-least-once guarantees.
    difficulty: advanced
    tags:
    - webhooks
    - async
    - reliability
    - integration
    - events
    estimated_hours: 35
    prerequisites:
    - build-message-queue
    milestones:
    - name: Webhook Registration & Security
      description: Implement webhook endpoint registration with signature verification
      skills:
      - HMAC signatures
      - Endpoint validation
      - Secret management
      hints:
        level1: Each webhook endpoint gets a unique signing secret
        level2: Verify endpoint ownership via challenge before activating
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum\n\
          import secrets\nimport hmac\nimport hashlib\nimport time\nimport httpx\n\nclass WebhookStatus(Enum):\n    PENDING_VERIFICATION\
          \ = \"pending_verification\"\n    ACTIVE = \"active\"\n    DISABLED = \"disabled\"\n    FAILED = \"failed\"  # Too\
          \ many delivery failures\n\n@dataclass\nclass WebhookEndpoint:\n    id: str\n    url: str\n    secret: str\n   \
          \ events: list[str]               # Event types to subscribe to\n    status: WebhookStatus\n    created_at: float\n\
          \    verified_at: Optional[float] = None\n    description: str = \"\"\n    metadata: dict = field(default_factory=dict)\n\
          \n    # Health tracking\n    consecutive_failures: int = 0\n    last_failure_at: Optional[float] = None\n    total_deliveries:\
          \ int = 0\n    successful_deliveries: int = 0\n\nclass WebhookRegistry:\n    def __init__(self, verify_timeout:\
          \ int = 30):\n        self.endpoints: dict[str, WebhookEndpoint] = {}\n        self.verify_timeout = verify_timeout\n\
          \n    def register_endpoint(self, url: str, events: list[str],\n                          description: str = \"\"\
          ) -> WebhookEndpoint:\n        '''Register new webhook endpoint'''\n        # Validate URL\n        if not url.startswith(\"\
          https://\"):\n            raise ValueError(\"Webhook URLs must use HTTPS\")\n\n        endpoint_id = secrets.token_urlsafe(16)\n\
          \        signing_secret = f\"whsec_{secrets.token_urlsafe(32)}\"\n\n        endpoint = WebhookEndpoint(\n      \
          \      id=endpoint_id,\n            url=url,\n            secret=signing_secret,\n            events=events,\n \
          \           status=WebhookStatus.PENDING_VERIFICATION,\n            created_at=time.time(),\n            description=description\n\
          \        )\n\n        self.endpoints[endpoint_id] = endpoint\n\n        # Initiate verification\n        self._send_verification_challenge(endpoint)\n\
          \n        return endpoint\n\n    def _send_verification_challenge(self, endpoint: WebhookEndpoint):\n        '''Send\
          \ challenge to verify endpoint ownership'''\n        challenge = secrets.token_urlsafe(32)\n\n        # Store challenge\
          \ for later verification\n        endpoint.metadata[\"verification_challenge\"] = challenge\n        endpoint.metadata[\"\
          verification_expires\"] = time.time() + 3600  # 1 hour\n\n        payload = {\n            \"type\": \"webhook.verification\"\
          ,\n            \"challenge\": challenge\n        }\n\n        try:\n            # Endpoint must respond with challenge\
          \ value\n            response = httpx.post(\n                endpoint.url,\n                json=payload,\n    \
          \            headers=self._build_headers(endpoint, payload),\n                timeout=self.verify_timeout\n    \
          \        )\n\n            if response.status_code == 200:\n                response_data = response.json()\n   \
          \             if response_data.get(\"challenge\") == challenge:\n                    endpoint.status = WebhookStatus.ACTIVE\n\
          \                    endpoint.verified_at = time.time()\n                    del endpoint.metadata[\"verification_challenge\"\
          ]\n        except Exception as e:\n            endpoint.metadata[\"verification_error\"] = str(e)\n\n    def _build_headers(self,\
          \ endpoint: WebhookEndpoint, payload: dict) -> dict:\n        '''Build headers with signature'''\n        timestamp\
          \ = str(int(time.time()))\n        payload_str = json.dumps(payload, separators=(',', ':'), sort_keys=True)\n\n\
          \        # Create signature\n        signature_payload = f\"{timestamp}.{payload_str}\"\n        signature = hmac.new(\n\
          \            endpoint.secret.encode(),\n            signature_payload.encode(),\n            hashlib.sha256\n  \
          \      ).hexdigest()\n\n        return {\n            \"Content-Type\": \"application/json\",\n            \"X-Webhook-ID\"\
          : endpoint.id,\n            \"X-Webhook-Timestamp\": timestamp,\n            \"X-Webhook-Signature\": f\"v1={signature}\"\
          ,\n            \"User-Agent\": \"WebhookDelivery/1.0\"\n        }\n\n    def update_events(self, endpoint_id: str,\
          \ events: list[str]) -> WebhookEndpoint:\n        endpoint = self.endpoints.get(endpoint_id)\n        if not endpoint:\n\
          \            raise ValueError(\"Endpoint not found\")\n\n        endpoint.events = events\n        return endpoint\n\
          \n    def disable_endpoint(self, endpoint_id: str):\n        endpoint = self.endpoints.get(endpoint_id)\n      \
          \  if endpoint:\n            endpoint.status = WebhookStatus.DISABLED\n\n    def get_endpoints_for_event(self, event_type:\
          \ str) -> list[WebhookEndpoint]:\n        '''Get all active endpoints subscribed to event type'''\n        return\
          \ [\n            e for e in self.endpoints.values()\n            if e.status == WebhookStatus.ACTIVE\n         \
          \   and (event_type in e.events or \"*\" in e.events)\n        ]\n\n# Signature verification helper for receivers\n\
          class WebhookSignatureVerifier:\n    def __init__(self, secret: str, tolerance: int = 300):\n        self.secret\
          \ = secret\n        self.tolerance = tolerance  # Timestamp tolerance in seconds\n\n    def verify(self, payload:\
          \ bytes, timestamp: str, signature: str) -> bool:\n        '''Verify webhook signature'''\n        # Check timestamp\
          \ to prevent replay attacks\n        try:\n            ts = int(timestamp)\n            if abs(time.time() - ts)\
          \ > self.tolerance:\n                return False\n        except ValueError:\n            return False\n\n    \
          \    # Verify signature\n        expected_payload = f\"{timestamp}.{payload.decode()}\"\n        expected_sig =\
          \ hmac.new(\n            self.secret.encode(),\n            expected_payload.encode(),\n            hashlib.sha256\n\
          \        ).hexdigest()\n\n        # Extract signature value\n        if signature.startswith(\"v1=\"):\n       \
          \     signature = signature[3:]\n\n        return hmac.compare_digest(expected_sig, signature)\n```\n"
      pitfalls:
      - HTTPS only - never deliver webhooks over HTTP
      - Timestamp tolerance prevents replay but allows clock skew
      - 'Signing secret rotation: support multiple active secrets temporarily'
      - 'URL validation: block private IPs to prevent SSRF'
    - name: Delivery Queue & Retry Logic
      description: Implement reliable delivery with exponential backoff and dead letter queue
      skills:
      - Retry strategies
      - Exponential backoff
      - Dead letter queues
      hints:
        level1: Use message queue for delivery; retry with exponential backoff on failure
        level2: After N retries, move to dead letter queue for manual review
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Callable\nfrom enum import\
          \ Enum\nimport time\nimport random\nimport httpx\nimport asyncio\n\nclass DeliveryStatus(Enum):\n    PENDING = \"\
          pending\"\n    IN_FLIGHT = \"in_flight\"\n    DELIVERED = \"delivered\"\n    FAILED = \"failed\"\n    DEAD_LETTER\
          \ = \"dead_letter\"\n\n@dataclass\nclass WebhookDelivery:\n    id: str\n    endpoint_id: str\n    event_type: str\n\
          \    payload: dict\n    status: DeliveryStatus\n    created_at: float\n    scheduled_for: float         # When to\
          \ attempt delivery\n    attempts: int = 0\n    max_attempts: int = 5\n    last_attempt_at: Optional[float] = None\n\
          \    last_error: Optional[str] = None\n    last_response_code: Optional[int] = None\n    delivered_at: Optional[float]\
          \ = None\n\nclass RetryPolicy:\n    def __init__(self, base_delay: int = 60,\n                 max_delay: int =\
          \ 3600,\n                 jitter: float = 0.1):\n        self.base_delay = base_delay   # 1 minute\n        self.max_delay\
          \ = max_delay     # 1 hour\n        self.jitter = jitter           # 10% random jitter\n\n    def get_next_delay(self,\
          \ attempt: int) -> int:\n        '''Calculate delay with exponential backoff and jitter'''\n        # Exponential:\
          \ 1m, 2m, 4m, 8m, 16m...\n        delay = min(self.base_delay * (2 ** attempt), self.max_delay)\n\n        # Add\
          \ jitter to prevent thundering herd\n        jitter_range = delay * self.jitter\n        delay += random.uniform(-jitter_range,\
          \ jitter_range)\n\n        return int(delay)\n\nclass WebhookDeliveryQueue:\n    def __init__(self, registry: WebhookRegistry,\n\
          \                 retry_policy: RetryPolicy = None):\n        self.registry = registry\n        self.retry_policy\
          \ = retry_policy or RetryPolicy()\n        self.pending: list[WebhookDelivery] = []\n        self.dead_letter: list[WebhookDelivery]\
          \ = []\n        self.delivered: list[WebhookDelivery] = []\n\n    def enqueue(self, endpoint_id: str, event_type:\
          \ str,\n                payload: dict) -> WebhookDelivery:\n        '''Add webhook delivery to queue'''\n      \
          \  delivery = WebhookDelivery(\n            id=secrets.token_urlsafe(16),\n            endpoint_id=endpoint_id,\n\
          \            event_type=event_type,\n            payload=payload,\n            status=DeliveryStatus.PENDING,\n\
          \            created_at=time.time(),\n            scheduled_for=time.time()  # Immediate\n        )\n\n        self.pending.append(delivery)\n\
          \        return delivery\n\n    def enqueue_for_event(self, event_type: str, payload: dict) -> list[WebhookDelivery]:\n\
          \        '''Fan out event to all subscribed endpoints'''\n        endpoints = self.registry.get_endpoints_for_event(event_type)\n\
          \        deliveries = []\n\n        for endpoint in endpoints:\n            delivery = self.enqueue(endpoint.id,\
          \ event_type, payload)\n            deliveries.append(delivery)\n\n        return deliveries\n\n    async def process_queue(self):\n\
          \        '''Process pending deliveries'''\n        now = time.time()\n        ready = [d for d in self.pending if\
          \ d.scheduled_for <= now]\n\n        for delivery in ready:\n            self.pending.remove(delivery)\n       \
          \     await self._attempt_delivery(delivery)\n\n    async def _attempt_delivery(self, delivery: WebhookDelivery):\n\
          \        '''Attempt single delivery'''\n        endpoint = self.registry.endpoints.get(delivery.endpoint_id)\n \
          \       if not endpoint or endpoint.status != WebhookStatus.ACTIVE:\n            delivery.status = DeliveryStatus.DEAD_LETTER\n\
          \            delivery.last_error = \"Endpoint not active\"\n            self.dead_letter.append(delivery)\n    \
          \        return\n\n        delivery.status = DeliveryStatus.IN_FLIGHT\n        delivery.attempts += 1\n        delivery.last_attempt_at\
          \ = time.time()\n\n        try:\n            # Build signed request\n            headers = self.registry._build_headers(endpoint,\
          \ delivery.payload)\n\n            async with httpx.AsyncClient() as client:\n                response = await client.post(\n\
          \                    endpoint.url,\n                    json=delivery.payload,\n                    headers=headers,\n\
          \                    timeout=30.0\n                )\n\n            delivery.last_response_code = response.status_code\n\
          \n            # 2xx = success\n            if 200 <= response.status_code < 300:\n                delivery.status\
          \ = DeliveryStatus.DELIVERED\n                delivery.delivered_at = time.time()\n                self.delivered.append(delivery)\n\
          \n                # Update endpoint health\n                endpoint.total_deliveries += 1\n                endpoint.successful_deliveries\
          \ += 1\n                endpoint.consecutive_failures = 0\n                return\n\n            # 4xx (except 429)\
          \ = don't retry, bad request\n            if 400 <= response.status_code < 500 and response.status_code != 429:\n\
          \                delivery.status = DeliveryStatus.DEAD_LETTER\n                delivery.last_error = f\"HTTP {response.status_code}\"\
          \n                self.dead_letter.append(delivery)\n                return\n\n            # 5xx or 429 = retry\n\
          \            delivery.last_error = f\"HTTP {response.status_code}\"\n\n        except httpx.TimeoutException:\n\
          \            delivery.last_error = \"Timeout\"\n        except httpx.RequestError as e:\n            delivery.last_error\
          \ = str(e)\n\n        # Failed - check retry\n        endpoint.total_deliveries += 1\n        endpoint.consecutive_failures\
          \ += 1\n        endpoint.last_failure_at = time.time()\n\n        if delivery.attempts >= delivery.max_attempts:\n\
          \            delivery.status = DeliveryStatus.DEAD_LETTER\n            self.dead_letter.append(delivery)\n\n   \
          \         # Disable endpoint if too many failures\n            if endpoint.consecutive_failures >= 10:\n       \
          \         endpoint.status = WebhookStatus.FAILED\n        else:\n            # Schedule retry\n            delay\
          \ = self.retry_policy.get_next_delay(delivery.attempts)\n            delivery.status = DeliveryStatus.PENDING\n\
          \            delivery.scheduled_for = time.time() + delay\n            self.pending.append(delivery)\n\n    def\
          \ retry_dead_letter(self, delivery_id: str) -> Optional[WebhookDelivery]:\n        '''Manually retry a dead letter\
          \ delivery'''\n        for delivery in self.dead_letter:\n            if delivery.id == delivery_id:\n         \
          \       self.dead_letter.remove(delivery)\n                delivery.status = DeliveryStatus.PENDING\n          \
          \      delivery.scheduled_for = time.time()\n                delivery.attempts = 0  # Reset attempt count\n    \
          \            self.pending.append(delivery)\n                return delivery\n        return None\n\n    def get_delivery_status(self,\
          \ delivery_id: str) -> Optional[WebhookDelivery]:\n        for queue in [self.pending, self.delivered, self.dead_letter]:\n\
          \            for delivery in queue:\n                if delivery.id == delivery_id:\n                    return\
          \ delivery\n        return None\n```\n"
      pitfalls:
      - Jitter prevents thundering herd when many webhooks fail simultaneously
      - 4xx errors (except 429) shouldn't retry - request is malformed
      - Dead letter queue needs alerting and manual review process
      - 'Circuit breaker: disable endpoint after consecutive failures'
    - name: Circuit Breaker & Rate Limiting
      description: Implement circuit breaker to protect failing endpoints and rate limiting
      skills:
      - Circuit breaker pattern
      - Rate limiting
      - Health checks
      hints:
        level1: Circuit breaker prevents hammering failing endpoints
        level2: 'States: closed (normal), open (skip), half-open (test)'
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom enum import Enum\nimport\
          \ time\nimport threading\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"      # Normal operation\n    OPEN\
          \ = \"open\"          # Failing, skip requests\n    HALF_OPEN = \"half_open\"  # Testing recovery\n\n@dataclass\n\
          class CircuitBreaker:\n    endpoint_id: str\n    state: CircuitState = CircuitState.CLOSED\n    failure_count: int\
          \ = 0\n    success_count: int = 0\n    last_failure_time: Optional[float] = None\n    last_state_change: float =\
          \ 0\n\n    # Thresholds\n    failure_threshold: int = 5     # Failures to open\n    success_threshold: int = 3 \
          \    # Successes to close\n    timeout: int = 60              # Seconds before half-open\n\nclass CircuitBreakerManager:\n\
          \    def __init__(self):\n        self.breakers: dict[str, CircuitBreaker] = {}\n        self.lock = threading.Lock()\n\
          \n    def get_breaker(self, endpoint_id: str) -> CircuitBreaker:\n        with self.lock:\n            if endpoint_id\
          \ not in self.breakers:\n                self.breakers[endpoint_id] = CircuitBreaker(\n                    endpoint_id=endpoint_id,\n\
          \                    last_state_change=time.time()\n                )\n            return self.breakers[endpoint_id]\n\
          \n    def can_execute(self, endpoint_id: str) -> bool:\n        '''Check if request should proceed'''\n        breaker\
          \ = self.get_breaker(endpoint_id)\n\n        if breaker.state == CircuitState.CLOSED:\n            return True\n\
          \n        if breaker.state == CircuitState.OPEN:\n            # Check if timeout has passed\n            if time.time()\
          \ - breaker.last_state_change >= breaker.timeout:\n                self._transition(breaker, CircuitState.HALF_OPEN)\n\
          \                return True  # Allow test request\n            return False\n\n        if breaker.state == CircuitState.HALF_OPEN:\n\
          \            return True  # Allow test requests\n\n        return False\n\n    def record_success(self, endpoint_id:\
          \ str):\n        '''Record successful delivery'''\n        breaker = self.get_breaker(endpoint_id)\n\n        with\
          \ self.lock:\n            if breaker.state == CircuitState.HALF_OPEN:\n                breaker.success_count +=\
          \ 1\n                if breaker.success_count >= breaker.success_threshold:\n                    self._transition(breaker,\
          \ CircuitState.CLOSED)\n            elif breaker.state == CircuitState.CLOSED:\n                breaker.failure_count\
          \ = 0  # Reset on success\n\n    def record_failure(self, endpoint_id: str):\n        '''Record failed delivery'''\n\
          \        breaker = self.get_breaker(endpoint_id)\n\n        with self.lock:\n            breaker.failure_count +=\
          \ 1\n            breaker.last_failure_time = time.time()\n\n            if breaker.state == CircuitState.HALF_OPEN:\n\
          \                # Any failure in half-open goes back to open\n                self._transition(breaker, CircuitState.OPEN)\n\
          \            elif breaker.state == CircuitState.CLOSED:\n                if breaker.failure_count >= breaker.failure_threshold:\n\
          \                    self._transition(breaker, CircuitState.OPEN)\n\n    def _transition(self, breaker: CircuitBreaker,\
          \ new_state: CircuitState):\n        breaker.state = new_state\n        breaker.last_state_change = time.time()\n\
          \n        if new_state == CircuitState.CLOSED:\n            breaker.failure_count = 0\n            breaker.success_count\
          \ = 0\n        elif new_state == CircuitState.HALF_OPEN:\n            breaker.success_count = 0\n\nclass RateLimiter:\n\
          \    '''Per-endpoint rate limiting using sliding window'''\n\n    def __init__(self, requests_per_second: int =\
          \ 10,\n                 requests_per_minute: int = 100):\n        self.rps = requests_per_second\n        self.rpm\
          \ = requests_per_minute\n        self.second_windows: dict[str, list[float]] = {}\n        self.minute_windows:\
          \ dict[str, list[float]] = {}\n        self.lock = threading.Lock()\n\n    def can_send(self, endpoint_id: str)\
          \ -> tuple[bool, Optional[float]]:\n        '''Check if can send, returns (allowed, retry_after)'''\n        now\
          \ = time.time()\n\n        with self.lock:\n            # Check per-second limit\n            second_window = self.second_windows.get(endpoint_id,\
          \ [])\n            second_window = [t for t in second_window if now - t < 1]\n            self.second_windows[endpoint_id]\
          \ = second_window\n\n            if len(second_window) >= self.rps:\n                retry_after = 1 - (now - second_window[0])\n\
          \                return False, retry_after\n\n            # Check per-minute limit\n            minute_window =\
          \ self.minute_windows.get(endpoint_id, [])\n            minute_window = [t for t in minute_window if now - t < 60]\n\
          \            self.minute_windows[endpoint_id] = minute_window\n\n            if len(minute_window) >= self.rpm:\n\
          \                retry_after = 60 - (now - minute_window[0])\n                return False, retry_after\n\n    \
          \        return True, None\n\n    def record_send(self, endpoint_id: str):\n        '''Record a send'''\n      \
          \  now = time.time()\n        with self.lock:\n            if endpoint_id not in self.second_windows:\n        \
          \        self.second_windows[endpoint_id] = []\n            if endpoint_id not in self.minute_windows:\n       \
          \         self.minute_windows[endpoint_id] = []\n\n            self.second_windows[endpoint_id].append(now)\n  \
          \          self.minute_windows[endpoint_id].append(now)\n\nclass WebhookDeliveryWithProtection:\n    '''Delivery\
          \ queue with circuit breaker and rate limiting'''\n\n    def __init__(self, queue: WebhookDeliveryQueue):\n    \
          \    self.queue = queue\n        self.circuit_breaker = CircuitBreakerManager()\n        self.rate_limiter = RateLimiter()\n\
          \n    async def attempt_delivery(self, delivery: WebhookDelivery) -> bool:\n        endpoint_id = delivery.endpoint_id\n\
          \n        # Check circuit breaker\n        if not self.circuit_breaker.can_execute(endpoint_id):\n            #\
          \ Reschedule for later\n            delivery.scheduled_for = time.time() + 60\n            return False\n\n    \
          \    # Check rate limit\n        allowed, retry_after = self.rate_limiter.can_send(endpoint_id)\n        if not\
          \ allowed:\n            delivery.scheduled_for = time.time() + (retry_after or 1)\n            return False\n\n\
          \        # Attempt delivery\n        self.rate_limiter.record_send(endpoint_id)\n        success = await self.queue._attempt_delivery(delivery)\n\
          \n        if success:\n            self.circuit_breaker.record_success(endpoint_id)\n        else:\n           \
          \ self.circuit_breaker.record_failure(endpoint_id)\n\n        return success\n```\n"
      pitfalls:
      - 'Half-open: only allow limited test requests, not full traffic'
      - Circuit breaker timeout should increase on repeated failures
      - Rate limiting should respect Retry-After headers from receiver
      - Alert when circuit opens - endpoint owner needs to know
    - name: Event Log & Replay
      description: Implement event logging for debugging and replay capability
      skills:
      - Event sourcing
      - Log retention
      - Event replay
      hints:
        level1: Log all webhook events for debugging and audit
        level2: Allow replaying failed webhooks from a specific time
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Iterator\nimport time\n\
          import json\n\n@dataclass\nclass WebhookEventLog:\n    id: str\n    event_type: str\n    payload: dict\n    timestamp:\
          \ float\n    deliveries: list[dict] = field(default_factory=list)\n    # Each delivery: {endpoint_id, delivery_id,\
          \ status, attempts, delivered_at}\n\nclass EventLogStore:\n    def __init__(self, retention_days: int = 30):\n \
          \       self.events: list[WebhookEventLog] = []\n        self.retention_days = retention_days\n        self.index_by_type:\
          \ dict[str, list[int]] = {}  # event_type -> indices\n\n    def log_event(self, event_type: str, payload: dict)\
          \ -> WebhookEventLog:\n        '''Log a new event'''\n        event = WebhookEventLog(\n            id=secrets.token_urlsafe(16),\n\
          \            event_type=event_type,\n            payload=payload,\n            timestamp=time.time()\n        )\n\
          \n        idx = len(self.events)\n        self.events.append(event)\n\n        # Index by type\n        if event_type\
          \ not in self.index_by_type:\n            self.index_by_type[event_type] = []\n        self.index_by_type[event_type].append(idx)\n\
          \n        return event\n\n    def record_delivery(self, event_id: str, endpoint_id: str,\n                     \
          \   delivery_id: str, status: str,\n                        attempts: int = 0, delivered_at: float = None):\n  \
          \      '''Record delivery attempt for an event'''\n        for event in self.events:\n            if event.id ==\
          \ event_id:\n                event.deliveries.append({\n                    \"endpoint_id\": endpoint_id,\n    \
          \                \"delivery_id\": delivery_id,\n                    \"status\": status,\n                    \"\
          attempts\": attempts,\n                    \"delivered_at\": delivered_at\n                })\n                return\n\
          \n    def query_events(self, event_type: str = None,\n                     start_time: float = None,\n         \
          \            end_time: float = None,\n                     limit: int = 100,\n                     offset: int =\
          \ 0) -> list[WebhookEventLog]:\n        '''Query events with filters'''\n        results = self.events\n\n     \
          \   if event_type:\n            indices = self.index_by_type.get(event_type, [])\n            results = [self.events[i]\
          \ for i in indices]\n\n        if start_time:\n            results = [e for e in results if e.timestamp >= start_time]\n\
          \        if end_time:\n            results = [e for e in results if e.timestamp <= end_time]\n\n        # Sort by\
          \ timestamp descending (newest first)\n        results = sorted(results, key=lambda e: e.timestamp, reverse=True)\n\
          \n        return results[offset:offset + limit]\n\n    def get_failed_deliveries(self, endpoint_id: str,\n     \
          \                         start_time: float = None) -> list[WebhookEventLog]:\n        '''Get events with failed\
          \ deliveries to specific endpoint'''\n        results = []\n\n        for event in self.events:\n            if\
          \ start_time and event.timestamp < start_time:\n                continue\n\n            for delivery in event.deliveries:\n\
          \                if (delivery[\"endpoint_id\"] == endpoint_id and\n                    delivery[\"status\"] in [\"\
          failed\", \"dead_letter\"]):\n                    results.append(event)\n                    break\n\n        return\
          \ results\n\n    def cleanup_old_events(self):\n        '''Remove events older than retention period'''\n      \
          \  cutoff = time.time() - (self.retention_days * 86400)\n        self.events = [e for e in self.events if e.timestamp\
          \ >= cutoff]\n        # Rebuild indices\n        self._rebuild_indices()\n\n    def _rebuild_indices(self):\n  \
          \      self.index_by_type = {}\n        for i, event in enumerate(self.events):\n            if event.event_type\
          \ not in self.index_by_type:\n                self.index_by_type[event.event_type] = []\n            self.index_by_type[event.event_type].append(i)\n\
          \nclass WebhookReplayService:\n    def __init__(self, event_store: EventLogStore,\n                 delivery_queue:\
          \ WebhookDeliveryQueue):\n        self.store = event_store\n        self.queue = delivery_queue\n\n    def replay_event(self,\
          \ event_id: str,\n                     endpoint_ids: list[str] = None) -> list[WebhookDelivery]:\n        '''Replay\
          \ a specific event'''\n        event = None\n        for e in self.store.events:\n            if e.id == event_id:\n\
          \                event = e\n                break\n\n        if not event:\n            raise ValueError(\"Event\
          \ not found\")\n\n        deliveries = []\n\n        if endpoint_ids:\n            # Replay to specific endpoints\n\
          \            for endpoint_id in endpoint_ids:\n                delivery = self.queue.enqueue(\n                \
          \    endpoint_id, event.event_type, event.payload\n                )\n                deliveries.append(delivery)\n\
          \        else:\n            # Replay to all original endpoints that failed\n            for d in event.deliveries:\n\
          \                if d[\"status\"] in [\"failed\", \"dead_letter\"]:\n                    delivery = self.queue.enqueue(\n\
          \                        d[\"endpoint_id\"], event.event_type, event.payload\n                    )\n          \
          \          deliveries.append(delivery)\n\n        return deliveries\n\n    def replay_range(self, endpoint_id: str,\n\
          \                     start_time: float, end_time: float = None) -> list[WebhookDelivery]:\n        '''Replay all\
          \ events in a time range to an endpoint'''\n        end_time = end_time or time.time()\n        events = self.store.query_events(\n\
          \            start_time=start_time,\n            end_time=end_time,\n            limit=10000  # Safety limit\n \
          \       )\n\n        deliveries = []\n        for event in events:\n            # Check if endpoint is subscribed\
          \ to this event type\n            endpoint = self.queue.registry.endpoints.get(endpoint_id)\n            if endpoint\
          \ and (event.event_type in endpoint.events or \"*\" in endpoint.events):\n                delivery = self.queue.enqueue(\n\
          \                    endpoint_id, event.event_type, event.payload\n                )\n                deliveries.append(delivery)\n\
          \n        return deliveries\n\n    def export_events(self, start_time: float, end_time: float,\n               \
          \       event_types: list[str] = None) -> Iterator[dict]:\n        '''Export events as JSON for external processing'''\n\
          \        events = self.store.query_events(\n            start_time=start_time,\n            end_time=end_time,\n\
          \            limit=100000\n        )\n\n        for event in events:\n            if event_types and event.event_type\
          \ not in event_types:\n                continue\n\n            yield {\n                \"id\": event.id,\n    \
          \            \"type\": event.event_type,\n                \"timestamp\": event.timestamp,\n                \"payload\"\
          : event.payload\n            }\n```\n"
      pitfalls:
      - Event log can grow huge - implement retention and archival
      - Replay should use new delivery IDs to track separately
      - Bulk replay can overwhelm endpoints - respect rate limits
      - Payload might be stale on replay - warn users
  search-engine:
    name: Full-Text Search Engine
    description: Build a search engine with inverted indexes, TF-IDF ranking, fuzzy matching, and query parsing like Elasticsearch/Meilisearch.
    why_expert: Search is everywhere. Understanding inverted indexes, ranking algorithms, and query optimization helps debug
      search issues and build better search UX.
    difficulty: expert
    tags:
    - search
    - indexing
    - information-retrieval
    - ranking
    - nlp
    estimated_hours: 55
    prerequisites: []
    milestones:
    - name: Inverted Index
      description: Implement an inverted index with tokenization and normalization
      skills:
      - Inverted indexes
      - Tokenization
      - Text normalization
      hints:
        level1: 'Inverted index: term -> [doc_id, positions] for fast lookup'
        level2: Tokenization splits text; normalization lowercases, removes accents, stems
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom collections import\
          \ defaultdict\nimport re\nimport unicodedata\n\n@dataclass\nclass Posting:\n    doc_id: str\n    positions: list[int]\
          \      # Word positions in document\n    term_frequency: int       # Count of term in doc\n\n@dataclass\nclass Document:\n\
          \    id: str\n    content: str\n    fields: dict = field(default_factory=dict)  # title, body, tags, etc.\n    metadata:\
          \ dict = field(default_factory=dict)\n\nclass Tokenizer:\n    def __init__(self, min_length: int = 2, stopwords:\
          \ set = None):\n        self.min_length = min_length\n        self.stopwords = stopwords or {\n            'the',\
          \ 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',\n            'to', 'for', 'of', 'is', 'it', 'this', 'that', 'with'\n\
          \        }\n\n    def tokenize(self, text: str) -> list[tuple[str, int]]:\n        '''Tokenize text, return (token,\
          \ position) pairs'''\n        # Normalize unicode\n        text = unicodedata.normalize('NFKD', text)\n        text\
          \ = ''.join(c for c in text if not unicodedata.combining(c))\n\n        # Lowercase\n        text = text.lower()\n\
          \n        # Split on non-alphanumeric\n        tokens = []\n        position = 0\n\n        for match in re.finditer(r'[a-z0-9]+',\
          \ text):\n            word = match.group()\n            if len(word) >= self.min_length and word not in self.stopwords:\n\
          \                tokens.append((word, position))\n            position += 1\n\n        return tokens\n\n    def\
          \ stem(self, word: str) -> str:\n        '''Simple Porter-like stemming'''\n        # Very simplified - real implementation\
          \ uses Porter/Snowball\n        suffixes = ['ing', 'ed', 'ly', 'es', 's']\n        for suffix in suffixes:\n   \
          \         if word.endswith(suffix) and len(word) > len(suffix) + 2:\n                return word[:-len(suffix)]\n\
          \        return word\n\nclass InvertedIndex:\n    def __init__(self, tokenizer: Tokenizer = None):\n        self.tokenizer\
          \ = tokenizer or Tokenizer()\n        self.index: dict[str, list[Posting]] = defaultdict(list)\n        self.documents:\
          \ dict[str, Document] = {}\n        self.doc_lengths: dict[str, int] = {}  # For BM25\n\n    def add_document(self,\
          \ doc: Document):\n        '''Index a document'''\n        self.documents[doc.id] = doc\n\n        # Combine all\
          \ fields for indexing\n        full_text = doc.content\n        for field_name, field_value in doc.fields.items():\n\
          \            if isinstance(field_value, str):\n                full_text += \" \" + field_value\n\n        tokens\
          \ = self.tokenizer.tokenize(full_text)\n        self.doc_lengths[doc.id] = len(tokens)\n\n        # Build term ->\
          \ positions mapping for this doc\n        term_positions: dict[str, list[int]] = defaultdict(list)\n        for\
          \ token, position in tokens:\n            stemmed = self.tokenizer.stem(token)\n            term_positions[stemmed].append(position)\n\
          \n        # Add to inverted index\n        for term, positions in term_positions.items():\n            posting =\
          \ Posting(\n                doc_id=doc.id,\n                positions=positions,\n                term_frequency=len(positions)\n\
          \            )\n\n            # Insert in sorted order by doc_id for efficient merging\n            postings = self.index[term]\n\
          \            # Binary search insert\n            lo, hi = 0, len(postings)\n            while lo < hi:\n       \
          \         mid = (lo + hi) // 2\n                if postings[mid].doc_id < doc.id:\n                    lo = mid\
          \ + 1\n                else:\n                    hi = mid\n            postings.insert(lo, posting)\n\n    def\
          \ remove_document(self, doc_id: str):\n        '''Remove document from index'''\n        if doc_id not in self.documents:\n\
          \            return\n\n        del self.documents[doc_id]\n        del self.doc_lengths[doc_id]\n\n        # Remove\
          \ from all posting lists\n        for term in list(self.index.keys()):\n            self.index[term] = [p for p\
          \ in self.index[term] if p.doc_id != doc_id]\n            if not self.index[term]:\n                del self.index[term]\n\
          \n    def get_postings(self, term: str) -> list[Posting]:\n        '''Get postings for a term'''\n        stemmed\
          \ = self.tokenizer.stem(term.lower())\n        return self.index.get(stemmed, [])\n\n    def search(self, query:\
          \ str) -> list[tuple[str, list[Posting]]]:\n        '''Basic AND search - returns docs containing all terms'''\n\
          \        tokens = self.tokenizer.tokenize(query)\n        if not tokens:\n            return []\n\n        # Get\
          \ postings for each term\n        term_postings = []\n        for token, _ in tokens:\n            stemmed = self.tokenizer.stem(token)\n\
          \            postings = self.index.get(stemmed, [])\n            if not postings:\n                return []  #\
          \ AND semantics: no matches\n            term_postings.append((stemmed, postings))\n\n        # Intersect posting\
          \ lists (find docs containing all terms)\n        # Start with smallest list for efficiency\n        term_postings.sort(key=lambda\
          \ x: len(x[1]))\n\n        result_docs = {p.doc_id for p in term_postings[0][1]}\n        for _, postings in term_postings[1:]:\n\
          \            result_docs &= {p.doc_id for p in postings}\n\n        # Collect matching postings\n        results\
          \ = []\n        for doc_id in result_docs:\n            doc_postings = []\n            for term, postings in term_postings:\n\
          \                for p in postings:\n                    if p.doc_id == doc_id:\n                        doc_postings.append(p)\n\
          \                        break\n            results.append((doc_id, doc_postings))\n\n        return results\n```\n"
      pitfalls:
      - Stemming can over-stem (running -> run -> r) - use proven algorithms
      - Stopwords list should be configurable per language
      - Unicode normalization crucial for international text
      - Index updates are expensive - use batch updates when possible
    - name: TF-IDF & BM25 Ranking
      description: Implement relevance ranking with TF-IDF and BM25 algorithms
      skills:
      - TF-IDF
      - BM25
      - Relevance scoring
      hints:
        level1: 'TF-IDF: terms appearing often in doc but rarely overall are important'
        level2: BM25 improves on TF-IDF with document length normalization
        level3: "\n```python\nimport math\nfrom dataclasses import dataclass\n\n@dataclass\nclass SearchResult:\n    doc_id:\
          \ str\n    score: float\n    highlights: dict = None  # term -> [positions]\n\nclass Scorer:\n    def __init__(self,\
          \ index: InvertedIndex):\n        self.index = index\n\n    def tf(self, term_freq: int) -> float:\n        '''Term\
          \ frequency component'''\n        return 1 + math.log(term_freq) if term_freq > 0 else 0\n\n    def idf(self, term:\
          \ str) -> float:\n        '''Inverse document frequency'''\n        N = len(self.index.documents)\n        df =\
          \ len(self.index.get_postings(term))\n        if df == 0:\n            return 0\n        return math.log(N / df)\n\
          \n    def tf_idf_score(self, doc_id: str, terms: list[str]) -> float:\n        '''Calculate TF-IDF score for document'''\n\
          \        score = 0.0\n        for term in terms:\n            postings = self.index.get_postings(term)\n       \
          \     for posting in postings:\n                if posting.doc_id == doc_id:\n                    score += self.tf(posting.term_frequency)\
          \ * self.idf(term)\n                    break\n        return score\n\nclass BM25Scorer(Scorer):\n    def __init__(self,\
          \ index: InvertedIndex, k1: float = 1.2, b: float = 0.75):\n        super().__init__(index)\n        self.k1 = k1\
          \  # Term frequency saturation\n        self.b = b    # Length normalization\n\n        # Precompute average document\
          \ length\n        total_length = sum(index.doc_lengths.values())\n        self.avg_doc_length = total_length / len(index.documents)\
          \ if index.documents else 0\n\n    def bm25_idf(self, term: str) -> float:\n        '''BM25 IDF variant'''\n   \
          \     N = len(self.index.documents)\n        df = len(self.index.get_postings(term))\n        if df == 0:\n    \
          \        return 0\n        # Robertson-Sparck Jones IDF\n        return math.log((N - df + 0.5) / (df + 0.5) + 1)\n\
          \n    def score(self, doc_id: str, terms: list[str]) -> float:\n        '''Calculate BM25 score'''\n        doc_length\
          \ = self.index.doc_lengths.get(doc_id, 0)\n        if doc_length == 0:\n            return 0\n\n        score =\
          \ 0.0\n        for term in terms:\n            idf = self.bm25_idf(term)\n\n            # Find term frequency in\
          \ this doc\n            tf = 0\n            for posting in self.index.get_postings(term):\n                if posting.doc_id\
          \ == doc_id:\n                    tf = posting.term_frequency\n                    break\n\n            if tf ==\
          \ 0:\n                continue\n\n            # BM25 formula\n            numerator = tf * (self.k1 + 1)\n     \
          \       denominator = tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)\n            score\
          \ += idf * (numerator / denominator)\n\n        return score\n\n    def search(self, query: str, limit: int = 10)\
          \ -> list[SearchResult]:\n        '''Search with BM25 ranking'''\n        tokens = self.index.tokenizer.tokenize(query)\n\
          \        terms = [self.index.tokenizer.stem(t[0]) for t in tokens]\n\n        # Get candidate documents (OR semantics\
          \ for ranking)\n        candidate_docs = set()\n        for term in terms:\n            for posting in self.index.get_postings(term):\n\
          \                candidate_docs.add(posting.doc_id)\n\n        # Score all candidates\n        results = []\n  \
          \      for doc_id in candidate_docs:\n            score = self.score(doc_id, terms)\n            if score > 0:\n\
          \                # Build highlights\n                highlights = {}\n                for term in terms:\n     \
          \               for posting in self.index.get_postings(term):\n                        if posting.doc_id == doc_id:\n\
          \                            highlights[term] = posting.positions[:5]  # First 5 positions\n                   \
          \         break\n\n                results.append(SearchResult(\n                    doc_id=doc_id,\n          \
          \          score=score,\n                    highlights=highlights\n                ))\n\n        # Sort by score\
          \ descending\n        results.sort(key=lambda r: r.score, reverse=True)\n        return results[:limit]\n\nclass\
          \ FieldWeightedScorer(BM25Scorer):\n    '''BM25 with field boosting (title matches worth more than body)'''\n\n\
          \    def __init__(self, index: InvertedIndex, field_weights: dict = None):\n        super().__init__(index)\n  \
          \      self.field_weights = field_weights or {\n            'title': 3.0,\n            'body': 1.0,\n          \
          \  'tags': 2.0\n        }\n\n    def score(self, doc_id: str, terms: list[str]) -> float:\n        '''Score with\
          \ field weighting'''\n        base_score = super().score(doc_id, terms)\n\n        # Boost based on field matches\n\
          \        doc = self.index.documents.get(doc_id)\n        if not doc:\n            return base_score\n\n        boost\
          \ = 1.0\n        for field_name, weight in self.field_weights.items():\n            field_value = doc.fields.get(field_name,\
          \ '')\n            if isinstance(field_value, str):\n                field_tokens = set(\n                    self.index.tokenizer.stem(t[0])\n\
          \                    for t in self.index.tokenizer.tokenize(field_value)\n                )\n                # Check\
          \ how many query terms match this field\n                matches = sum(1 for term in terms if term in field_tokens)\n\
          \                if matches > 0:\n                    boost += (weight - 1.0) * (matches / len(terms))\n\n     \
          \   return base_score * boost\n```\n"
      pitfalls:
      - BM25 k1 and b parameters need tuning for your data
      - Very short documents can get artificially high scores
      - Precompute IDF values for performance
      - Field boosting can be gamed - normalize carefully
    - name: Fuzzy Matching & Autocomplete
      description: Implement typo tolerance with Levenshtein distance and prefix-based autocomplete
      skills:
      - Edit distance
      - Prefix trees
      - Fuzzy search
      hints:
        level1: Levenshtein distance counts edits needed to transform one string to another
        level2: For autocomplete, use trie (prefix tree) for O(prefix_length) lookup
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nclass TrieNode:\n \
          \   def __init__(self):\n        self.children: dict[str, 'TrieNode'] = {}\n        self.is_end: bool = False\n\
          \        self.frequency: int = 0  # For ranking suggestions\n        self.doc_ids: set[str] = set()\n\nclass Trie:\n\
          \    def __init__(self):\n        self.root = TrieNode()\n\n    def insert(self, word: str, doc_id: str = None,\
          \ frequency: int = 1):\n        node = self.root\n        for char in word.lower():\n            if char not in\
          \ node.children:\n                node.children[char] = TrieNode()\n            node = node.children[char]\n   \
          \     node.is_end = True\n        node.frequency += frequency\n        if doc_id:\n            node.doc_ids.add(doc_id)\n\
          \n    def search_prefix(self, prefix: str, limit: int = 10) -> list[tuple[str, int, set]]:\n        '''Find all\
          \ words with given prefix'''\n        node = self.root\n        for char in prefix.lower():\n            if char\
          \ not in node.children:\n                return []\n            node = node.children[char]\n\n        # DFS to find\
          \ all words\n        results = []\n        self._collect_words(node, prefix, results, limit * 3)\n\n        # Sort\
          \ by frequency and return top results\n        results.sort(key=lambda x: x[1], reverse=True)\n        return results[:limit]\n\
          \n    def _collect_words(self, node: TrieNode, prefix: str,\n                       results: list, limit: int):\n\
          \        if len(results) >= limit:\n            return\n\n        if node.is_end:\n            results.append((prefix,\
          \ node.frequency, node.doc_ids))\n\n        for char, child in node.children.items():\n            self._collect_words(child,\
          \ prefix + char, results, limit)\n\nclass FuzzyMatcher:\n    def __init__(self, max_distance: int = 2):\n      \
          \  self.max_distance = max_distance\n\n    def levenshtein_distance(self, s1: str, s2: str) -> int:\n        '''Calculate\
          \ Levenshtein edit distance'''\n        if len(s1) < len(s2):\n            s1, s2 = s2, s1\n\n        if len(s2)\
          \ == 0:\n            return len(s1)\n\n        previous_row = list(range(len(s2) + 1))\n        for i, c1 in enumerate(s1):\n\
          \            current_row = [i + 1]\n            for j, c2 in enumerate(s2):\n                insertions = previous_row[j\
          \ + 1] + 1\n                deletions = current_row[j] + 1\n                substitutions = previous_row[j] + (c1\
          \ != c2)\n                current_row.append(min(insertions, deletions, substitutions))\n            previous_row\
          \ = current_row\n\n        return previous_row[-1]\n\n    def damerau_levenshtein(self, s1: str, s2: str) -> int:\n\
          \        '''Damerau-Levenshtein: includes transpositions'''\n        len1, len2 = len(s1), len(s2)\n\n        #\
          \ Create distance matrix\n        d = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n\n        for i in range(len1\
          \ + 1):\n            d[i][0] = i\n        for j in range(len2 + 1):\n            d[0][j] = j\n\n        for i in\
          \ range(1, len1 + 1):\n            for j in range(1, len2 + 1):\n                cost = 0 if s1[i-1] == s2[j-1]\
          \ else 1\n\n                d[i][j] = min(\n                    d[i-1][j] + 1,      # Deletion\n               \
          \     d[i][j-1] + 1,      # Insertion\n                    d[i-1][j-1] + cost  # Substitution\n                )\n\
          \n                # Transposition\n                if i > 1 and j > 1 and s1[i-1] == s2[j-2] and s1[i-2] == s2[j-1]:\n\
          \                    d[i][j] = min(d[i][j], d[i-2][j-2] + cost)\n\n        return d[len1][len2]\n\n    def find_fuzzy_matches(self,\
          \ query: str, vocabulary: list[str],\n                           max_distance: int = None) -> list[tuple[str, int]]:\n\
          \        '''Find words within edit distance'''\n        max_dist = max_distance or self.max_distance\n        matches\
          \ = []\n\n        for word in vocabulary:\n            distance = self.damerau_levenshtein(query.lower(), word.lower())\n\
          \            if distance <= max_dist:\n                matches.append((word, distance))\n\n        # Sort by distance\
          \ then alphabetically\n        matches.sort(key=lambda x: (x[1], x[0]))\n        return matches\n\nclass FuzzySearcher:\n\
          \    def __init__(self, index: InvertedIndex, max_typos: int = 2):\n        self.index = index\n        self.trie\
          \ = Trie()\n        self.fuzzy = FuzzyMatcher(max_typos)\n        self.vocabulary = set()\n\n        # Build trie\
          \ from index vocabulary\n        for term in index.index.keys():\n            self.trie.insert(term)\n         \
          \   self.vocabulary.add(term)\n\n    def autocomplete(self, prefix: str, limit: int = 10) -> list[dict]:\n     \
          \   '''Autocomplete suggestions'''\n        suggestions = self.trie.search_prefix(prefix, limit)\n        return\
          \ [\n            {'term': term, 'frequency': freq, 'doc_count': len(docs)}\n            for term, freq, docs in\
          \ suggestions\n        ]\n\n    def expand_query(self, query: str) -> list[str]:\n        '''Expand query with fuzzy\
          \ matches'''\n        tokens = self.index.tokenizer.tokenize(query)\n        expanded_terms = []\n\n        for\
          \ token, _ in tokens:\n            stemmed = self.index.tokenizer.stem(token)\n\n            # Exact match first\n\
          \            if stemmed in self.vocabulary:\n                expanded_terms.append(stemmed)\n                continue\n\
          \n            # Fuzzy match\n            matches = self.fuzzy.find_fuzzy_matches(\n                stemmed, list(self.vocabulary),\
          \ max_distance=2\n            )\n\n            if matches:\n                # Add top fuzzy match\n            \
          \    expanded_terms.append(matches[0][0])\n            else:\n                expanded_terms.append(stemmed)\n\n\
          \        return expanded_terms\n\n    def search(self, query: str, limit: int = 10) -> list[SearchResult]:\n   \
          \     '''Search with typo tolerance'''\n        expanded_terms = self.expand_query(query)\n\n        # Use BM25\
          \ scorer with expanded terms\n        scorer = BM25Scorer(self.index)\n\n        # Get candidates\n        candidate_docs\
          \ = set()\n        for term in expanded_terms:\n            for posting in self.index.get_postings(term):\n    \
          \            candidate_docs.add(posting.doc_id)\n\n        results = []\n        for doc_id in candidate_docs:\n\
          \            score = scorer.score(doc_id, expanded_terms)\n            if score > 0:\n                results.append(SearchResult(doc_id=doc_id,\
          \ score=score))\n\n        results.sort(key=lambda r: r.score, reverse=True)\n        return results[:limit]\n```\n"
      pitfalls:
      - Edit distance is O(n*m) - prefilter candidates first
      - Max 2 typos is usually enough; more causes too many false matches
      - Transpositions are common typos - Damerau-Levenshtein handles them
      - Short words need fewer allowed edits (1 typo in 'cat' is too much)
    - name: Query Parser & Filters
      description: Implement query parsing with boolean operators, phrases, and field filters
      skills:
      - Query parsing
      - Boolean logic
      - Filter expressions
      hints:
        level1: 'Parse: ''title:python AND (error OR exception) -deprecated'''
        level2: Build AST from query, then evaluate against documents
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom enum import Enum\nimport re\n\
          \nclass QueryOperator(Enum):\n    AND = \"AND\"\n    OR = \"OR\"\n    NOT = \"NOT\"\n\n@dataclass\nclass TermQuery:\n\
          \    term: str\n    field: str = None        # None = search all fields\n    is_phrase: bool = False  # \"exact\
          \ phrase\"\n    is_prefix: bool = False  # python*\n\n@dataclass\nclass BooleanQuery:\n    operator: QueryOperator\n\
          \    operands: list  # List of Query objects\n\n@dataclass\nclass FilterQuery:\n    field: str\n    operator: str\
          \  # =, !=, >, <, >=, <=, in\n    value: any\n\nQuery = Union[TermQuery, BooleanQuery, FilterQuery]\n\nclass QueryParser:\n\
          \    def __init__(self):\n        self.default_operator = QueryOperator.AND\n\n    def parse(self, query_string:\
          \ str) -> Query:\n        '''Parse query string into AST'''\n        tokens = self._tokenize(query_string)\n   \
          \     return self._parse_or(tokens, 0)[0]\n\n    def _tokenize(self, query: str) -> list[str]:\n        '''Tokenize\
          \ query into terms and operators'''\n        tokens = []\n        i = 0\n\n        while i < len(query):\n     \
          \       # Skip whitespace\n            if query[i].isspace():\n                i += 1\n                continue\n\
          \n            # Quoted phrase\n            if query[i] == '\"':\n                j = query.find('\"', i + 1)\n \
          \               if j == -1:\n                    j = len(query)\n                tokens.append(('PHRASE', query[i+1:j]))\n\
          \                i = j + 1\n                continue\n\n            # Parentheses\n            if query[i] in '()':\n\
          \                tokens.append((query[i], query[i]))\n                i += 1\n                continue\n\n     \
          \       # Operators and terms\n            word_match = re.match(r'[\\w:*\\-\\.]+', query[i:])\n            if word_match:\n\
          \                word = word_match.group()\n                if word.upper() in ('AND', 'OR', 'NOT'):\n         \
          \           tokens.append((word.upper(), word.upper()))\n                elif ':' in word:\n                   \
          \ field, value = word.split(':', 1)\n                    tokens.append(('FIELD', (field, value)))\n            \
          \    elif word.startswith('-'):\n                    tokens.append(('NOT_TERM', word[1:]))\n                elif\
          \ word.endswith('*'):\n                    tokens.append(('PREFIX', word[:-1]))\n                else:\n       \
          \             tokens.append(('TERM', word))\n                i += len(word)\n                continue\n\n      \
          \      i += 1\n\n        return tokens\n\n    def _parse_or(self, tokens: list, pos: int) -> tuple[Query, int]:\n\
          \        '''Parse OR expressions'''\n        left, pos = self._parse_and(tokens, pos)\n\n        while pos < len(tokens)\
          \ and tokens[pos][0] == 'OR':\n            pos += 1\n            right, pos = self._parse_and(tokens, pos)\n   \
          \         left = BooleanQuery(QueryOperator.OR, [left, right])\n\n        return left, pos\n\n    def _parse_and(self,\
          \ tokens: list, pos: int) -> tuple[Query, int]:\n        '''Parse AND expressions'''\n        left, pos = self._parse_not(tokens,\
          \ pos)\n\n        while pos < len(tokens):\n            if tokens[pos][0] == 'AND':\n                pos += 1\n\
          \                right, pos = self._parse_not(tokens, pos)\n                left = BooleanQuery(QueryOperator.AND,\
          \ [left, right])\n            elif tokens[pos][0] in ('TERM', 'PHRASE', 'FIELD', 'PREFIX', '('):\n             \
          \   # Implicit AND\n                right, pos = self._parse_not(tokens, pos)\n                left = BooleanQuery(QueryOperator.AND,\
          \ [left, right])\n            else:\n                break\n\n        return left, pos\n\n    def _parse_not(self,\
          \ tokens: list, pos: int) -> tuple[Query, int]:\n        '''Parse NOT expressions'''\n        if pos < len(tokens):\n\
          \            if tokens[pos][0] == 'NOT':\n                pos += 1\n                operand, pos = self._parse_primary(tokens,\
          \ pos)\n                return BooleanQuery(QueryOperator.NOT, [operand]), pos\n            elif tokens[pos][0]\
          \ == 'NOT_TERM':\n                term = tokens[pos][1]\n                return BooleanQuery(QueryOperator.NOT,\
          \ [TermQuery(term)]), pos + 1\n\n        return self._parse_primary(tokens, pos)\n\n    def _parse_primary(self,\
          \ tokens: list, pos: int) -> tuple[Query, int]:\n        '''Parse primary expressions'''\n        if pos >= len(tokens):\n\
          \            return TermQuery(''), pos\n\n        token_type, token_value = tokens[pos]\n\n        if token_type\
          \ == '(':\n            result, pos = self._parse_or(tokens, pos + 1)\n            if pos < len(tokens) and tokens[pos][0]\
          \ == ')':\n                pos += 1\n            return result, pos\n\n        if token_type == 'PHRASE':\n    \
          \        return TermQuery(token_value, is_phrase=True), pos + 1\n\n        if token_type == 'FIELD':\n         \
          \   field, value = token_value\n            return TermQuery(value, field=field), pos + 1\n\n        if token_type\
          \ == 'PREFIX':\n            return TermQuery(token_value, is_prefix=True), pos + 1\n\n        if token_type == 'TERM':\n\
          \            return TermQuery(token_value), pos + 1\n\n        return TermQuery(''), pos + 1\n\nclass QueryExecutor:\n\
          \    def __init__(self, index: InvertedIndex, scorer: BM25Scorer):\n        self.index = index\n        self.scorer\
          \ = scorer\n\n    def execute(self, query: Query) -> set[str]:\n        '''Execute query, return matching doc IDs'''\n\
          \        if isinstance(query, TermQuery):\n            return self._execute_term(query)\n        elif isinstance(query,\
          \ BooleanQuery):\n            return self._execute_boolean(query)\n        return set()\n\n    def _execute_term(self,\
          \ query: TermQuery) -> set[str]:\n        if query.is_phrase:\n            return self._phrase_search(query.term,\
          \ query.field)\n        elif query.is_prefix:\n            return self._prefix_search(query.term, query.field)\n\
          \        else:\n            postings = self.index.get_postings(query.term)\n            return {p.doc_id for p in\
          \ postings}\n\n    def _execute_boolean(self, query: BooleanQuery) -> set[str]:\n        if query.operator == QueryOperator.AND:\n\
          \            result = None\n            for operand in query.operands:\n                matches = self.execute(operand)\n\
          \                if result is None:\n                    result = matches\n                else:\n             \
          \       result &= matches\n            return result or set()\n\n        elif query.operator == QueryOperator.OR:\n\
          \            result = set()\n            for operand in query.operands:\n                result |= self.execute(operand)\n\
          \            return result\n\n        elif query.operator == QueryOperator.NOT:\n            all_docs = set(self.index.documents.keys())\n\
          \            excluded = self.execute(query.operands[0])\n            return all_docs - excluded\n\n        return\
          \ set()\n\n    def _phrase_search(self, phrase: str, field: str = None) -> set[str]:\n        '''Search for exact\
          \ phrase'''\n        tokens = self.index.tokenizer.tokenize(phrase)\n        if not tokens:\n            return\
          \ set()\n\n        terms = [self.index.tokenizer.stem(t[0]) for t in tokens]\n\n        # Get docs containing all\
          \ terms\n        candidate_docs = None\n        term_postings = {}\n        for term in terms:\n            postings\
          \ = self.index.get_postings(term)\n            doc_ids = {p.doc_id for p in postings}\n            if candidate_docs\
          \ is None:\n                candidate_docs = doc_ids\n            else:\n                candidate_docs &= doc_ids\n\
          \            term_postings[term] = {p.doc_id: p.positions for p in postings}\n\n        if not candidate_docs:\n\
          \            return set()\n\n        # Check phrase positions\n        results = set()\n        for doc_id in candidate_docs:\n\
          \            positions_list = [term_postings[t].get(doc_id, []) for t in terms]\n            if self._check_phrase_positions(positions_list):\n\
          \                results.add(doc_id)\n\n        return results\n\n    def _check_phrase_positions(self, positions_list:\
          \ list[list[int]]) -> bool:\n        '''Check if positions form consecutive phrase'''\n        if not positions_list\
          \ or not positions_list[0]:\n            return False\n\n        # For each starting position of first term\n  \
          \      for start_pos in positions_list[0]:\n            match = True\n            for i, positions in enumerate(positions_list[1:],\
          \ 1):\n                if start_pos + i not in positions:\n                    match = False\n                 \
          \   break\n            if match:\n                return True\n        return False\n\n    def _prefix_search(self,\
          \ prefix: str, field: str = None) -> set[str]:\n        '''Search for terms starting with prefix'''\n        results\
          \ = set()\n        prefix = prefix.lower()\n\n        for term in self.index.index.keys():\n            if term.startswith(prefix):\n\
          \                for posting in self.index.index[term]:\n                    results.add(posting.doc_id)\n\n   \
          \     return results\n```\n"
      pitfalls:
      - Phrase search requires position tracking - expensive
      - NOT without other terms returns entire index
      - Deeply nested queries can stack overflow - limit depth
      - Wildcard at start is very expensive - requires full scan
  etl-pipeline:
    name: Data Pipeline / ETL System
    description: Build an ETL pipeline system with job scheduling, transformation DAGs, data validation, and monitoring.
    why_expert: Data pipelines are core infrastructure. Understanding ETL patterns helps build reliable data systems and debug
      data issues.
    difficulty: expert
    tags:
    - etl
    - data-engineering
    - pipelines
    - scheduling
    - batch
    estimated_hours: 45
    prerequisites:
    - job-scheduler
    milestones:
    - name: Pipeline DAG Definition
      description: Implement pipeline definition with dependencies as directed acyclic graph
      skills:
      - DAG modeling
      - Task dependencies
      - Configuration
      hints:
        level1: Tasks form a DAG - each task can depend on others
        level2: Topological sort determines execution order; detect cycles
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional, Any\nfrom enum\
          \ import Enum\nfrom collections import defaultdict\nimport time\n\nclass TaskStatus(Enum):\n    PENDING = \"pending\"\
          \n    RUNNING = \"running\"\n    SUCCESS = \"success\"\n    FAILED = \"failed\"\n    SKIPPED = \"skipped\"\n   \
          \ UPSTREAM_FAILED = \"upstream_failed\"\n\n@dataclass\nclass Task:\n    id: str\n    name: str\n    callable: Callable\n\
          \    dependencies: list[str] = field(default_factory=list)\n    retries: int = 3\n    retry_delay: int = 60\n  \
          \  timeout: int = 3600\n    params: dict = field(default_factory=dict)\n\n@dataclass\nclass TaskRun:\n    task_id:\
          \ str\n    status: TaskStatus\n    started_at: Optional[float] = None\n    finished_at: Optional[float] = None\n\
          \    attempt: int = 1\n    error: Optional[str] = None\n    output: Any = None\n\n@dataclass\nclass Pipeline:\n\
          \    id: str\n    name: str\n    tasks: dict[str, Task] = field(default_factory=dict)\n    schedule: Optional[str]\
          \ = None  # Cron expression\n    description: str = \"\"\n\nclass PipelineBuilder:\n    def __init__(self, pipeline_id:\
          \ str, name: str):\n        self.pipeline = Pipeline(id=pipeline_id, name=name)\n\n    def add_task(self, task_id:\
          \ str, name: str, callable: Callable,\n                 dependencies: list[str] = None, **kwargs) -> 'PipelineBuilder':\n\
          \        task = Task(\n            id=task_id,\n            name=name,\n            callable=callable,\n       \
          \     dependencies=dependencies or [],\n            **kwargs\n        )\n        self.pipeline.tasks[task_id] =\
          \ task\n        return self\n\n    def validate(self) -> list[str]:\n        '''Validate pipeline, return errors'''\n\
          \        errors = []\n\n        # Check for missing dependencies\n        for task_id, task in self.pipeline.tasks.items():\n\
          \            for dep in task.dependencies:\n                if dep not in self.pipeline.tasks:\n               \
          \     errors.append(f\"Task '{task_id}' depends on unknown task '{dep}'\")\n\n        # Check for cycles\n     \
          \   if self._has_cycle():\n            errors.append(\"Pipeline contains a cycle\")\n\n        return errors\n\n\
          \    def _has_cycle(self) -> bool:\n        '''Detect cycles using DFS'''\n        WHITE, GRAY, BLACK = 0, 1, 2\n\
          \        color = {t: WHITE for t in self.pipeline.tasks}\n\n        def dfs(task_id: str) -> bool:\n           \
          \ color[task_id] = GRAY\n            for dep in self.pipeline.tasks[task_id].dependencies:\n                if color[dep]\
          \ == GRAY:\n                    return True  # Back edge = cycle\n                if color[dep] == WHITE and dfs(dep):\n\
          \                    return True\n            color[task_id] = BLACK\n            return False\n\n        for task_id\
          \ in self.pipeline.tasks:\n            if color[task_id] == WHITE:\n                if dfs(task_id):\n         \
          \           return True\n        return False\n\n    def build(self) -> Pipeline:\n        errors = self.validate()\n\
          \        if errors:\n            raise ValueError(f\"Invalid pipeline: {errors}\")\n        return self.pipeline\n\
          \nclass DAGExecutor:\n    def __init__(self):\n        self.runs: dict[str, dict[str, TaskRun]] = {}  # pipeline_run_id\
          \ -> task_id -> run\n\n    def topological_sort(self, pipeline: Pipeline) -> list[str]:\n        '''Get tasks in\
          \ execution order'''\n        in_degree = defaultdict(int)\n        for task_id, task in pipeline.tasks.items():\n\
          \            for dep in task.dependencies:\n                in_degree[task_id] += 1\n\n        # Start with tasks\
          \ that have no dependencies\n        queue = [t for t in pipeline.tasks if in_degree[t] == 0]\n        result =\
          \ []\n\n        while queue:\n            task_id = queue.pop(0)\n            result.append(task_id)\n\n       \
          \     # Reduce in-degree of dependent tasks\n            for other_id, other_task in pipeline.tasks.items():\n \
          \               if task_id in other_task.dependencies:\n                    in_degree[other_id] -= 1\n         \
          \           if in_degree[other_id] == 0:\n                        queue.append(other_id)\n\n        return result\n\
          \n    def get_ready_tasks(self, pipeline: Pipeline,\n                        task_runs: dict[str, TaskRun]) -> list[str]:\n\
          \        '''Get tasks ready to execute (all deps satisfied)'''\n        ready = []\n\n        for task_id, task\
          \ in pipeline.tasks.items():\n            run = task_runs.get(task_id)\n\n            # Already running or finished\n\
          \            if run and run.status != TaskStatus.PENDING:\n                continue\n\n            # Check all dependencies\
          \ succeeded\n            deps_satisfied = True\n            for dep in task.dependencies:\n                dep_run\
          \ = task_runs.get(dep)\n                if not dep_run or dep_run.status != TaskStatus.SUCCESS:\n              \
          \      deps_satisfied = False\n                    break\n\n            if deps_satisfied:\n                ready.append(task_id)\n\
          \n        return ready\n\n    def should_skip_task(self, pipeline: Pipeline, task_id: str,\n                   \
          \      task_runs: dict[str, TaskRun]) -> bool:\n        '''Check if task should be skipped due to upstream failure'''\n\
          \        task = pipeline.tasks[task_id]\n\n        for dep in task.dependencies:\n            dep_run = task_runs.get(dep)\n\
          \            if dep_run and dep_run.status in [TaskStatus.FAILED, TaskStatus.UPSTREAM_FAILED]:\n               \
          \ return True\n        return False\n```\n"
      pitfalls:
      - Cycle detection must run before execution starts
      - Topological sort doesn't handle failures - need separate tracking
      - Parallel execution needs thread-safe state management
      - Long chains of dependencies slow down pipeline
    - name: Data Extraction & Loading
      description: Implement extractors and loaders for common data sources
      skills:
      - Data connectors
      - Batch processing
      - Incremental loads
      hints:
        level1: Extractors read from sources; loaders write to destinations
        level2: Support full and incremental loads using watermarks
        level3: "\n```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Iterator,\
          \ Optional, Any\nimport json\n\n@dataclass\nclass DataBatch:\n    records: list[dict]\n    schema: Optional[dict]\
          \ = None\n    metadata: dict = field(default_factory=dict)\n\nclass Extractor(ABC):\n    @abstractmethod\n    def\
          \ extract(self, **kwargs) -> Iterator[DataBatch]:\n        '''Extract data in batches'''\n        pass\n\n    @abstractmethod\n\
          \    def get_watermark(self) -> Any:\n        '''Get current watermark for incremental loads'''\n        pass\n\n\
          class Loader(ABC):\n    @abstractmethod\n    def load(self, batch: DataBatch) -> int:\n        '''Load batch, return\
          \ records loaded'''\n        pass\n\n    @abstractmethod\n    def begin_transaction(self):\n        pass\n\n   \
          \ @abstractmethod\n    def commit(self):\n        pass\n\n    @abstractmethod\n    def rollback(self):\n       \
          \ pass\n\nclass DatabaseExtractor(Extractor):\n    def __init__(self, connection, table: str, batch_size: int =\
          \ 1000,\n                 watermark_column: str = None):\n        self.conn = connection\n        self.table = table\n\
          \        self.batch_size = batch_size\n        self.watermark_column = watermark_column\n        self._watermark\
          \ = None\n\n    def extract(self, since: Any = None, **kwargs) -> Iterator[DataBatch]:\n        query = f\"SELECT\
          \ * FROM {self.table}\"\n        params = []\n\n        if since and self.watermark_column:\n            query +=\
          \ f\" WHERE {self.watermark_column} > ?\"\n            params.append(since)\n\n        if self.watermark_column:\n\
          \            query += f\" ORDER BY {self.watermark_column}\"\n\n        cursor = self.conn.execute(query, params)\n\
          \        columns = [desc[0] for desc in cursor.description]\n\n        batch = []\n        for row in cursor:\n\
          \            record = dict(zip(columns, row))\n            batch.append(record)\n\n            if self.watermark_column:\n\
          \                self._watermark = record[self.watermark_column]\n\n            if len(batch) >= self.batch_size:\n\
          \                yield DataBatch(records=batch, schema={\"columns\": columns})\n                batch = []\n\n \
          \       if batch:\n            yield DataBatch(records=batch, schema={\"columns\": columns})\n\n    def get_watermark(self)\
          \ -> Any:\n        return self._watermark\n\nclass APIExtractor(Extractor):\n    def __init__(self, base_url: str,\
          \ endpoint: str,\n                 auth: dict = None, batch_size: int = 100):\n        self.base_url = base_url\n\
          \        self.endpoint = endpoint\n        self.auth = auth\n        self.batch_size = batch_size\n        self._watermark\
          \ = None\n\n    def extract(self, since: Any = None, **kwargs) -> Iterator[DataBatch]:\n        import httpx\n\n\
          \        page = 1\n        while True:\n            params = {\"page\": page, \"per_page\": self.batch_size}\n \
          \           if since:\n                params[\"since\"] = since\n\n            response = httpx.get(\n        \
          \        f\"{self.base_url}/{self.endpoint}\",\n                params=params,\n                headers=self._build_headers()\n\
          \            )\n            response.raise_for_status()\n\n            data = response.json()\n            records\
          \ = data.get(\"data\", data)  # Handle nested or flat response\n\n            if not records:\n                break\n\
          \n            # Track watermark\n            if records and \"updated_at\" in records[-1]:\n                self._watermark\
          \ = records[-1][\"updated_at\"]\n\n            yield DataBatch(records=records)\n            page += 1\n\n     \
          \       # Check if last page\n            if len(records) < self.batch_size:\n                break\n\n    def _build_headers(self)\
          \ -> dict:\n        headers = {\"Accept\": \"application/json\"}\n        if self.auth:\n            if \"token\"\
          \ in self.auth:\n                headers[\"Authorization\"] = f\"Bearer {self.auth['token']}\"\n        return headers\n\
          \n    def get_watermark(self) -> Any:\n        return self._watermark\n\nclass FileExtractor(Extractor):\n    def\
          \ __init__(self, path: str, format: str = \"json\", batch_size: int = 1000):\n        self.path = path\n       \
          \ self.format = format\n        self.batch_size = batch_size\n\n    def extract(self, **kwargs) -> Iterator[DataBatch]:\n\
          \        import csv\n\n        if self.format == \"json\":\n            with open(self.path) as f:\n           \
          \     data = json.load(f)\n                for i in range(0, len(data), self.batch_size):\n                    yield\
          \ DataBatch(records=data[i:i + self.batch_size])\n\n        elif self.format == \"csv\":\n            with open(self.path)\
          \ as f:\n                reader = csv.DictReader(f)\n                batch = []\n                for row in reader:\n\
          \                    batch.append(row)\n                    if len(batch) >= self.batch_size:\n                \
          \        yield DataBatch(records=batch)\n                        batch = []\n                if batch:\n       \
          \             yield DataBatch(records=batch)\n\n        elif self.format == \"jsonl\":\n            with open(self.path)\
          \ as f:\n                batch = []\n                for line in f:\n                    batch.append(json.loads(line))\n\
          \                    if len(batch) >= self.batch_size:\n                        yield DataBatch(records=batch)\n\
          \                        batch = []\n                if batch:\n                    yield DataBatch(records=batch)\n\
          \n    def get_watermark(self) -> Any:\n        import os\n        return os.path.getmtime(self.path)\n\nclass DatabaseLoader(Loader):\n\
          \    def __init__(self, connection, table: str, mode: str = \"append\"):\n        self.conn = connection\n     \
          \   self.table = table\n        self.mode = mode  # append, replace, upsert\n\n    def begin_transaction(self):\n\
          \        pass  # Connection handles this\n\n    def load(self, batch: DataBatch) -> int:\n        if not batch.records:\n\
          \            return 0\n\n        columns = list(batch.records[0].keys())\n        placeholders = \", \".join([\"\
          ?\" for _ in columns])\n        column_names = \", \".join(columns)\n\n        if self.mode == \"upsert\":\n   \
          \         # SQLite upsert syntax\n            query = f'''\n                INSERT INTO {self.table} ({column_names})\n\
          \                VALUES ({placeholders})\n                ON CONFLICT DO UPDATE SET\n                {\", \".join(f\"\
          {c} = excluded.{c}\" for c in columns)}\n            '''\n        else:\n            query = f\"INSERT INTO {self.table}\
          \ ({column_names}) VALUES ({placeholders})\"\n\n        for record in batch.records:\n            values = [record.get(c)\
          \ for c in columns]\n            self.conn.execute(query, values)\n\n        return len(batch.records)\n\n    def\
          \ commit(self):\n        self.conn.commit()\n\n    def rollback(self):\n        self.conn.rollback()\n```\n"
      pitfalls:
      - API pagination can change during extraction - use stable cursors
      - Watermark must be saved atomically with loaded data
      - Upsert needs proper conflict detection keys
      - Large batches can exhaust memory - stream when possible
    - name: Data Transformations
      description: Implement transformation operations with schema validation
      skills:
      - Data mapping
      - Schema validation
      - Type conversion
      hints:
        level1: 'Transformations: map, filter, aggregate, join'
        level2: Validate schema before and after transformation
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Callable, Any, Optional\nfrom abc import\
          \ ABC, abstractmethod\nimport json\nimport re\n\nclass Transform(ABC):\n    @abstractmethod\n    def apply(self,\
          \ batch: DataBatch) -> DataBatch:\n        pass\n\nclass MapTransform(Transform):\n    def __init__(self, mapper:\
          \ Callable[[dict], dict]):\n        self.mapper = mapper\n\n    def apply(self, batch: DataBatch) -> DataBatch:\n\
          \        transformed = [self.mapper(record) for record in batch.records]\n        return DataBatch(records=transformed,\
          \ metadata=batch.metadata)\n\nclass FilterTransform(Transform):\n    def __init__(self, predicate: Callable[[dict],\
          \ bool]):\n        self.predicate = predicate\n\n    def apply(self, batch: DataBatch) -> DataBatch:\n        filtered\
          \ = [r for r in batch.records if self.predicate(r)]\n        return DataBatch(records=filtered, metadata=batch.metadata)\n\
          \nclass RenameColumnsTransform(Transform):\n    def __init__(self, mapping: dict[str, str]):\n        self.mapping\
          \ = mapping\n\n    def apply(self, batch: DataBatch) -> DataBatch:\n        transformed = []\n        for record\
          \ in batch.records:\n            new_record = {}\n            for key, value in record.items():\n              \
          \  new_key = self.mapping.get(key, key)\n                new_record[new_key] = value\n            transformed.append(new_record)\n\
          \        return DataBatch(records=transformed, metadata=batch.metadata)\n\nclass TypeCastTransform(Transform):\n\
          \    def __init__(self, type_mapping: dict[str, type]):\n        self.type_mapping = type_mapping\n\n    def apply(self,\
          \ batch: DataBatch) -> DataBatch:\n        transformed = []\n        for record in batch.records:\n            new_record\
          \ = dict(record)\n            for field, target_type in self.type_mapping.items():\n                if field in\
          \ new_record:\n                    new_record[field] = self._cast(new_record[field], target_type)\n            transformed.append(new_record)\n\
          \        return DataBatch(records=transformed, metadata=batch.metadata)\n\n    def _cast(self, value: Any, target_type:\
          \ type) -> Any:\n        if value is None:\n            return None\n        if target_type == int:\n          \
          \  return int(float(value))\n        elif target_type == float:\n            return float(value)\n        elif target_type\
          \ == bool:\n            return value in (True, 'true', 'True', '1', 1)\n        elif target_type == str:\n     \
          \       return str(value)\n        return value\n\nclass DeriveColumnTransform(Transform):\n    def __init__(self,\
          \ column: str, expression: Callable[[dict], Any]):\n        self.column = column\n        self.expression = expression\n\
          \n    def apply(self, batch: DataBatch) -> DataBatch:\n        transformed = []\n        for record in batch.records:\n\
          \            new_record = dict(record)\n            new_record[self.column] = self.expression(record)\n        \
          \    transformed.append(new_record)\n        return DataBatch(records=transformed, metadata=batch.metadata)\n\n\
          # Schema validation\n@dataclass\nclass FieldSchema:\n    name: str\n    type: str  # string, int, float, bool, datetime,\
          \ json\n    required: bool = True\n    nullable: bool = False\n    pattern: Optional[str] = None\n    min_value:\
          \ Optional[float] = None\n    max_value: Optional[float] = None\n    enum_values: Optional[list] = None\n\n@dataclass\n\
          class DataSchema:\n    fields: list[FieldSchema]\n    strict: bool = False  # Reject unknown fields\n\nclass SchemaValidator:\n\
          \    def __init__(self, schema: DataSchema):\n        self.schema = schema\n        self.field_map = {f.name: f\
          \ for f in schema.fields}\n\n    def validate(self, batch: DataBatch) -> list[dict]:\n        '''Validate batch,\
          \ return list of errors'''\n        errors = []\n\n        for i, record in enumerate(batch.records):\n        \
          \    record_errors = self._validate_record(record, i)\n            errors.extend(record_errors)\n\n        return\
          \ errors\n\n    def _validate_record(self, record: dict, row_num: int) -> list[dict]:\n        errors = []\n\n \
          \       # Check required fields\n        for field in self.schema.fields:\n            if field.required and field.name\
          \ not in record:\n                errors.append({\n                    \"row\": row_num,\n                    \"\
          field\": field.name,\n                    \"error\": \"required_field_missing\"\n                })\n\n        #\
          \ Check each field\n        for field_name, value in record.items():\n            if field_name not in self.field_map:\n\
          \                if self.schema.strict:\n                    errors.append({\n                        \"row\": row_num,\n\
          \                        \"field\": field_name,\n                        \"error\": \"unknown_field\"\n        \
          \            })\n                continue\n\n            field = self.field_map[field_name]\n            field_errors\
          \ = self._validate_field(value, field, row_num)\n            errors.extend(field_errors)\n\n        return errors\n\
          \n    def _validate_field(self, value: Any, field: FieldSchema,\n                        row_num: int) -> list[dict]:\n\
          \        errors = []\n\n        # Null check\n        if value is None:\n            if not field.nullable:\n  \
          \              errors.append({\n                    \"row\": row_num,\n                    \"field\": field.name,\n\
          \                    \"error\": \"null_not_allowed\"\n                })\n            return errors\n\n        #\
          \ Type check\n        type_valid = self._check_type(value, field.type)\n        if not type_valid:\n           \
          \ errors.append({\n                \"row\": row_num,\n                \"field\": field.name,\n                \"\
          error\": f\"invalid_type_expected_{field.type}\",\n                \"value\": str(value)[:100]\n            })\n\
          \            return errors\n\n        # Pattern check\n        if field.pattern and field.type == \"string\":\n\
          \            if not re.match(field.pattern, str(value)):\n                errors.append({\n                    \"\
          row\": row_num,\n                    \"field\": field.name,\n                    \"error\": \"pattern_mismatch\"\
          \n                })\n\n        # Range check\n        if field.min_value is not None and value < field.min_value:\n\
          \            errors.append({\n                \"row\": row_num,\n                \"field\": field.name,\n      \
          \          \"error\": f\"below_minimum_{field.min_value}\"\n            })\n        if field.max_value is not None\
          \ and value > field.max_value:\n            errors.append({\n                \"row\": row_num,\n               \
          \ \"field\": field.name,\n                \"error\": f\"above_maximum_{field.max_value}\"\n            })\n\n  \
          \      # Enum check\n        if field.enum_values and value not in field.enum_values:\n            errors.append({\n\
          \                \"row\": row_num,\n                \"field\": field.name,\n                \"error\": \"invalid_enum_value\"\
          \n            })\n\n        return errors\n\n    def _check_type(self, value: Any, expected: str) -> bool:\n   \
          \     if expected == \"string\":\n            return isinstance(value, str)\n        elif expected == \"int\":\n\
          \            return isinstance(value, int) and not isinstance(value, bool)\n        elif expected == \"float\":\n\
          \            return isinstance(value, (int, float)) and not isinstance(value, bool)\n        elif expected == \"\
          bool\":\n            return isinstance(value, bool)\n        elif expected == \"json\":\n            return isinstance(value,\
          \ (dict, list))\n        return True\n```\n"
      pitfalls:
      - Type coercion can lose precision (float -> int)
      - Schema validation on every record is slow - sample or skip
      - Derived columns can fail - handle exceptions per record
      - Null handling differs between databases
    - name: Pipeline Orchestration & Monitoring
      description: Implement pipeline execution with monitoring, alerting, and lineage tracking
      skills:
      - Orchestration
      - Monitoring
      - Data lineage
      hints:
        level1: 'Track execution metrics: duration, records, errors'
        level2: 'Data lineage: track where data came from and where it went'
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Any\nfrom enum import\
          \ Enum\nimport time\nimport traceback\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass PipelineRunStatus(Enum):\n\
          \    PENDING = \"pending\"\n    RUNNING = \"running\"\n    SUCCESS = \"success\"\n    FAILED = \"failed\"\n    CANCELLED\
          \ = \"cancelled\"\n\n@dataclass\nclass PipelineRun:\n    id: str\n    pipeline_id: str\n    status: PipelineRunStatus\n\
          \    started_at: Optional[float] = None\n    finished_at: Optional[float] = None\n    task_runs: dict[str, TaskRun]\
          \ = field(default_factory=dict)\n    error: Optional[str] = None\n    metrics: dict = field(default_factory=dict)\n\
          \n@dataclass\nclass LineageRecord:\n    run_id: str\n    task_id: str\n    source: str          # Table, API, file\
          \ path\n    destination: str\n    records_read: int\n    records_written: int\n    timestamp: float\n\nclass PipelineOrchestrator:\n\
          \    def __init__(self, max_parallel: int = 4):\n        self.executor = ThreadPoolExecutor(max_workers=max_parallel)\n\
          \        self.dag_executor = DAGExecutor()\n        self.runs: dict[str, PipelineRun] = {}\n        self.lineage:\
          \ list[LineageRecord] = []\n\n    def run_pipeline(self, pipeline: Pipeline,\n                     params: dict\
          \ = None) -> PipelineRun:\n        '''Execute pipeline'''\n        run_id = f\"{pipeline.id}_{int(time.time())}\"\
          \n        run = PipelineRun(\n            id=run_id,\n            pipeline_id=pipeline.id,\n            status=PipelineRunStatus.RUNNING,\n\
          \            started_at=time.time()\n        )\n        self.runs[run_id] = run\n\n        # Initialize task runs\n\
          \        for task_id in pipeline.tasks:\n            run.task_runs[task_id] = TaskRun(\n                task_id=task_id,\n\
          \                status=TaskStatus.PENDING\n            )\n\n        try:\n            self._execute_pipeline(pipeline,\
          \ run, params or {})\n            run.status = PipelineRunStatus.SUCCESS\n        except Exception as e:\n     \
          \       run.status = PipelineRunStatus.FAILED\n            run.error = str(e)\n        finally:\n            run.finished_at\
          \ = time.time()\n            self._collect_metrics(run)\n\n        return run\n\n    def _execute_pipeline(self,\
          \ pipeline: Pipeline, run: PipelineRun,\n                          params: dict):\n        '''Execute tasks in dependency\
          \ order'''\n        completed = set()\n\n        while len(completed) < len(pipeline.tasks):\n            # Find\
          \ tasks ready to run\n            ready = self.dag_executor.get_ready_tasks(pipeline, run.task_runs)\n\n       \
          \     if not ready:\n                # Check for failures\n                failed = [t for t, r in run.task_runs.items()\n\
          \                         if r.status == TaskStatus.FAILED]\n                if failed:\n                    # Mark\
          \ downstream as upstream_failed\n                    for task_id, task_run in run.task_runs.items():\n         \
          \               if task_run.status == TaskStatus.PENDING:\n                            if self.dag_executor.should_skip_task(\n\
          \                                pipeline, task_id, run.task_runs\n                            ):\n            \
          \                    task_run.status = TaskStatus.UPSTREAM_FAILED\n                                completed.add(task_id)\n\
          \                    if len(completed) >= len(pipeline.tasks):\n                        break\n                \
          \    raise Exception(f\"Pipeline failed: tasks {failed}\")\n                continue\n\n            # Execute ready\
          \ tasks (could parallelize here)\n            for task_id in ready:\n                self._execute_task(pipeline.tasks[task_id],\
          \ run, params)\n                completed.add(task_id)\n\n    def _execute_task(self, task: Task, run: PipelineRun,\
          \ params: dict):\n        '''Execute single task with retries'''\n        task_run = run.task_runs[task.id]\n  \
          \      task_run.started_at = time.time()\n        task_run.status = TaskStatus.RUNNING\n\n        # Merge pipeline\
          \ params with task params\n        task_params = {**task.params, **params}\n\n        for attempt in range(1, task.retries\
          \ + 1):\n            task_run.attempt = attempt\n            try:\n                result = task.callable(**task_params)\n\
          \                task_run.status = TaskStatus.SUCCESS\n                task_run.output = result\n              \
          \  task_run.finished_at = time.time()\n\n                # Record lineage if result contains it\n              \
          \  if isinstance(result, dict) and 'lineage' in result:\n                    self._record_lineage(run.id, task.id,\
          \ result['lineage'])\n\n                return\n\n            except Exception as e:\n                task_run.error\
          \ = f\"Attempt {attempt}: {str(e)}\"\n                if attempt < task.retries:\n                    time.sleep(task.retry_delay)\n\
          \                else:\n                    task_run.status = TaskStatus.FAILED\n                    task_run.finished_at\
          \ = time.time()\n                    raise\n\n    def _record_lineage(self, run_id: str, task_id: str, lineage:\
          \ dict):\n        record = LineageRecord(\n            run_id=run_id,\n            task_id=task_id,\n          \
          \  source=lineage.get('source', 'unknown'),\n            destination=lineage.get('destination', 'unknown'),\n  \
          \          records_read=lineage.get('records_read', 0),\n            records_written=lineage.get('records_written',\
          \ 0),\n            timestamp=time.time()\n        )\n        self.lineage.append(record)\n\n    def _collect_metrics(self,\
          \ run: PipelineRun):\n        '''Collect run metrics'''\n        run.metrics = {\n            'duration_seconds':\
          \ (run.finished_at or time.time()) - run.started_at,\n            'tasks_total': len(run.task_runs),\n         \
          \   'tasks_succeeded': sum(1 for r in run.task_runs.values()\n                                   if r.status ==\
          \ TaskStatus.SUCCESS),\n            'tasks_failed': sum(1 for r in run.task_runs.values()\n                    \
          \            if r.status == TaskStatus.FAILED),\n            'total_records': sum(\n                r.output.get('records_processed',\
          \ 0)\n                for r in run.task_runs.values()\n                if r.output and isinstance(r.output, dict)\n\
          \            )\n        }\n\n    def get_lineage(self, destination: str) -> list[LineageRecord]:\n        '''Get\
          \ lineage for a destination'''\n        return [l for l in self.lineage if l.destination == destination]\n\nclass\
          \ PipelineMonitor:\n    def __init__(self, orchestrator: PipelineOrchestrator):\n        self.orchestrator = orchestrator\n\
          \        self.alerts: list[dict] = []\n\n    def check_health(self) -> dict:\n        '''Check overall pipeline\
          \ health'''\n        recent_runs = [\n            r for r in self.orchestrator.runs.values()\n            if r.finished_at\
          \ and time.time() - r.finished_at < 86400\n        ]\n\n        success_rate = (\n            sum(1 for r in recent_runs\
          \ if r.status == PipelineRunStatus.SUCCESS) /\n            len(recent_runs) if recent_runs else 1.0\n        )\n\
          \n        return {\n            'total_runs_24h': len(recent_runs),\n            'success_rate': success_rate,\n\
          \            'healthy': success_rate > 0.9\n        }\n\n    def alert_on_failure(self, run: PipelineRun):\n   \
          \     '''Create alert for failed run'''\n        if run.status == PipelineRunStatus.FAILED:\n            self.alerts.append({\n\
          \                'type': 'pipeline_failure',\n                'pipeline_id': run.pipeline_id,\n                'run_id':\
          \ run.id,\n                'error': run.error,\n                'timestamp': time.time()\n            })\n```\n"
      pitfalls:
      - Retry delay should use exponential backoff
      - Parallel task execution needs careful state management
      - Lineage tracking adds overhead - make it optional
      - Cancelled pipelines need cleanup of partial data
  cdc-system:
    name: Change Data Capture (CDC) System
    description: Build a CDC system that captures database changes in real-time using transaction logs and streams them to
      consumers.
    why_expert: CDC is essential for real-time data sync. Understanding WAL parsing, log-based replication, and event streaming
      enables building reactive data systems.
    difficulty: expert
    tags:
    - cdc
    - replication
    - streaming
    - database
    - events
    estimated_hours: 50
    prerequisites:
    - build-sqlite
    milestones:
    - name: Log Parsing & Change Events
      description: Parse database transaction logs to extract change events
      skills:
      - WAL parsing
      - Binary protocols
      - Event modeling
      hints:
        level1: 'WAL/binlog contains: operation type, table, old values, new values'
        level2: 'Events: INSERT, UPDATE (with before/after), DELETE'
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Any\nfrom enum import\
          \ Enum\nimport json\nimport time\n\nclass OperationType(Enum):\n    INSERT = \"insert\"\n    UPDATE = \"update\"\
          \n    DELETE = \"delete\"\n    TRUNCATE = \"truncate\"\n    DDL = \"ddl\"\n\n@dataclass\nclass ChangeEvent:\n  \
          \  id: str\n    timestamp: float\n    transaction_id: str\n    operation: OperationType\n    database: str\n   \
          \ schema: str\n    table: str\n    primary_key: dict\n    before: Optional[dict] = None   # For UPDATE/DELETE\n\
          \    after: Optional[dict] = None    # For INSERT/UPDATE\n    metadata: dict = field(default_factory=dict)\n\n \
          \   def to_json(self) -> str:\n        return json.dumps({\n            'id': self.id,\n            'timestamp':\
          \ self.timestamp,\n            'transaction_id': self.transaction_id,\n            'operation': self.operation.value,\n\
          \            'database': self.database,\n            'schema': self.schema,\n            'table': self.table,\n\
          \            'primary_key': self.primary_key,\n            'before': self.before,\n            'after': self.after\n\
          \        })\n\n@dataclass\nclass Position:\n    '''Track position in transaction log'''\n    log_file: str\n   \
          \ log_position: int\n    timestamp: float\n\nclass LogParser:\n    '''Base class for log parsers'''\n\n    def __init__(self,\
          \ position: Position = None):\n        self.position = position\n        self.event_count = 0\n\n    def parse(self)\
          \ -> Iterator[ChangeEvent]:\n        raise NotImplementedError\n\n    def get_position(self) -> Position:\n    \
          \    return self.position\n\n# Simulated PostgreSQL logical replication\nclass PostgresLogicalDecoder:\n    '''Parse\
          \ PostgreSQL logical replication stream'''\n\n    def __init__(self, connection, slot_name: str, publication: str):\n\
          \        self.conn = connection\n        self.slot_name = slot_name\n        self.publication = publication\n  \
          \      self.position = None\n\n    def create_slot(self):\n        '''Create replication slot'''\n        self.conn.execute(f'''\n\
          \            SELECT pg_create_logical_replication_slot(\n                '{self.slot_name}', 'pgoutput'\n      \
          \      )\n        ''')\n\n    def read_changes(self, start_lsn: str = None) -> Iterator[ChangeEvent]:\n        '''Read\
          \ changes from replication slot'''\n        # In production: use psycopg2 replication protocol\n        # This is\
          \ a simulation using polling\n\n        cursor = self.conn.execute(f'''\n            SELECT lsn, data FROM pg_logical_slot_get_changes(\n\
          \                '{self.slot_name}', NULL, NULL,\n                'publication_names', '{self.publication}'\n  \
          \          )\n        ''')\n\n        for lsn, data in cursor:\n            event = self._parse_message(lsn, data)\n\
          \            if event:\n                yield event\n\n    def _parse_message(self, lsn: str, data: bytes) -> Optional[ChangeEvent]:\n\
          \        '''Parse logical replication message'''\n        # pgoutput format: message_type + payload\n        if\
          \ not data:\n            return None\n\n        msg_type = data[0:1]\n\n        if msg_type == b'I':  # Insert\n\
          \            return self._parse_insert(lsn, data)\n        elif msg_type == b'U':  # Update\n            return\
          \ self._parse_update(lsn, data)\n        elif msg_type == b'D':  # Delete\n            return self._parse_delete(lsn,\
          \ data)\n\n        return None\n\n    def _parse_insert(self, lsn: str, data: bytes) -> ChangeEvent:\n        #\
          \ Simplified - real implementation parses binary format\n        table_info = self._extract_table_info(data)\n \
          \       new_tuple = self._extract_tuple(data)\n\n        return ChangeEvent(\n            id=f\"{lsn}_{time.time()}\"\
          ,\n            timestamp=time.time(),\n            transaction_id=lsn,\n            operation=OperationType.INSERT,\n\
          \            database=table_info['database'],\n            schema=table_info['schema'],\n            table=table_info['table'],\n\
          \            primary_key=self._extract_pk(new_tuple),\n            after=new_tuple\n        )\n\n# MySQL binlog\
          \ simulation\nclass MySQLBinlogReader:\n    '''Parse MySQL binary log'''\n\n    def __init__(self, connection, server_id:\
          \ int):\n        self.conn = connection\n        self.server_id = server_id\n        self.position = None\n\n  \
          \  def read_events(self, binlog_file: str = None,\n                    position: int = 0) -> Iterator[ChangeEvent]:\n\
          \        '''Read events from binlog'''\n        # In production: use mysql-replication package\n        # This simulates\
          \ by querying\n\n        cursor = self.conn.execute('''\n            SHOW BINLOG EVENTS\n        ''')\n\n      \
          \  for row in cursor:\n            event = self._parse_binlog_event(row)\n            if event:\n              \
          \  yield event\n\n    def _parse_binlog_event(self, row) -> Optional[ChangeEvent]:\n        event_type = row['Event_type']\n\
          \n        if event_type == 'Write_rows':\n            return self._create_event(OperationType.INSERT, row)\n   \
          \     elif event_type == 'Update_rows':\n            return self._create_event(OperationType.UPDATE, row)\n    \
          \    elif event_type == 'Delete_rows':\n            return self._create_event(OperationType.DELETE, row)\n\n   \
          \     return None\n\n# Generic trigger-based CDC (works with any database)\nclass TriggerBasedCDC:\n    '''CDC using\
          \ database triggers - fallback for when log access is unavailable'''\n\n    def __init__(self, connection, tables:\
          \ list[str]):\n        self.conn = connection\n        self.tables = tables\n        self.change_table = '_cdc_changes'\n\
          \n    def setup(self):\n        '''Create change capture infrastructure'''\n        # Create change log table\n\
          \        self.conn.execute(f'''\n            CREATE TABLE IF NOT EXISTS {self.change_table} (\n                id\
          \ INTEGER PRIMARY KEY AUTOINCREMENT,\n                table_name TEXT,\n                operation TEXT,\n      \
          \          primary_key TEXT,\n                old_data TEXT,\n                new_data TEXT,\n                timestamp\
          \ REAL,\n                processed INTEGER DEFAULT 0\n            )\n        ''')\n\n        # Create triggers for\
          \ each table\n        for table in self.tables:\n            self._create_triggers(table)\n\n    def _create_triggers(self,\
          \ table: str):\n        # Insert trigger\n        self.conn.execute(f'''\n            CREATE TRIGGER IF NOT EXISTS\
          \ {table}_insert_cdc\n            AFTER INSERT ON {table}\n            BEGIN\n                INSERT INTO {self.change_table}\n\
          \                (table_name, operation, primary_key, new_data, timestamp)\n                VALUES ('{table}', 'INSERT',\n\
          \                        NEW.id, json_object('id', NEW.id), unixepoch());\n            END\n        ''')\n\n   \
          \     # Update trigger\n        self.conn.execute(f'''\n            CREATE TRIGGER IF NOT EXISTS {table}_update_cdc\n\
          \            AFTER UPDATE ON {table}\n            BEGIN\n                INSERT INTO {self.change_table}\n     \
          \           (table_name, operation, primary_key, old_data, new_data, timestamp)\n                VALUES ('{table}',\
          \ 'UPDATE',\n                        NEW.id,\n                        json_object('id', OLD.id),\n             \
          \           json_object('id', NEW.id),\n                        unixepoch());\n            END\n        ''')\n\n\
          \        # Delete trigger\n        self.conn.execute(f'''\n            CREATE TRIGGER IF NOT EXISTS {table}_delete_cdc\n\
          \            AFTER DELETE ON {table}\n            BEGIN\n                INSERT INTO {self.change_table}\n     \
          \           (table_name, operation, primary_key, old_data, timestamp)\n                VALUES ('{table}', 'DELETE',\n\
          \                        OLD.id, json_object('id', OLD.id), unixepoch());\n            END\n        ''')\n\n   \
          \ def read_changes(self, batch_size: int = 100) -> list[ChangeEvent]:\n        '''Read unprocessed changes'''\n\
          \        cursor = self.conn.execute(f'''\n            SELECT * FROM {self.change_table}\n            WHERE processed\
          \ = 0\n            ORDER BY id\n            LIMIT ?\n        ''', (batch_size,))\n\n        events = []\n      \
          \  for row in cursor:\n            event = ChangeEvent(\n                id=str(row['id']),\n                timestamp=row['timestamp'],\n\
          \                transaction_id=str(row['id']),\n                operation=OperationType(row['operation'].lower()),\n\
          \                database='main',\n                schema='public',\n                table=row['table_name'],\n\
          \                primary_key=json.loads(row['primary_key']) if row['primary_key'] else {},\n                before=json.loads(row['old_data'])\
          \ if row['old_data'] else None,\n                after=json.loads(row['new_data']) if row['new_data'] else None\n\
          \            )\n            events.append(event)\n\n        return events\n\n    def mark_processed(self, event_ids:\
          \ list[str]):\n        '''Mark events as processed'''\n        placeholders = ','.join(['?' for _ in event_ids])\n\
          \        self.conn.execute(f'''\n            UPDATE {self.change_table}\n            SET processed = 1\n       \
          \     WHERE id IN ({placeholders})\n        ''', event_ids)\n        self.conn.commit()\n```\n"
      pitfalls:
      - WAL parsing is database-specific - need adapter per DB
      - Trigger-based CDC adds write overhead to main tables
      - Binary log formats change between versions
      - Large transactions can create huge change events
    - name: Event Streaming & Delivery
      description: Stream change events to consumers with ordering and delivery guarantees
      skills:
      - Event streaming
      - Ordering guarantees
      - Consumer groups
      hints:
        level1: Events for same row must be delivered in order
        level2: Partition by table/pk for parallel processing with ordering
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional\nfrom collections\
          \ import defaultdict\nimport threading\nimport queue\nimport time\n\n@dataclass\nclass EventPartition:\n    key:\
          \ str  # table:pk combination\n    events: queue.Queue = field(default_factory=queue.Queue)\n    last_event_time:\
          \ float = 0\n    consumer_offset: int = 0\n\n@dataclass\nclass Consumer:\n    id: str\n    group_id: str\n    assigned_partitions:\
          \ set[str] = field(default_factory=set)\n    last_heartbeat: float = 0\n\nclass CDCEventStream:\n    '''Stream CDC\
          \ events with ordering guarantees'''\n\n    def __init__(self, num_partitions: int = 16):\n        self.num_partitions\
          \ = num_partitions\n        self.partitions: dict[int, list[ChangeEvent]] = defaultdict(list)\n        self.partition_locks:\
          \ dict[int, threading.Lock] = {\n            i: threading.Lock() for i in range(num_partitions)\n        }\n\n \
          \       # Consumer group management\n        self.consumer_groups: dict[str, dict[str, Consumer]] = defaultdict(dict)\n\
          \        self.consumer_offsets: dict[str, dict[int, int]] = defaultdict(\n            lambda: defaultdict(int)\n\
          \        )  # group_id -> partition -> offset\n\n    def _get_partition(self, event: ChangeEvent) -> int:\n    \
          \    '''Determine partition for event (consistent hashing by table:pk)'''\n        partition_key = f\"{event.table}:{json.dumps(event.primary_key,\
          \ sort_keys=True)}\"\n        return hash(partition_key) % self.num_partitions\n\n    def publish(self, event: ChangeEvent):\n\
          \        '''Publish event to stream'''\n        partition_id = self._get_partition(event)\n\n        with self.partition_locks[partition_id]:\n\
          \            self.partitions[partition_id].append(event)\n\n    def publish_batch(self, events: list[ChangeEvent]):\n\
          \        '''Publish batch of events'''\n        # Group by partition\n        by_partition = defaultdict(list)\n\
          \        for event in events:\n            partition_id = self._get_partition(event)\n            by_partition[partition_id].append(event)\n\
          \n        # Publish to each partition\n        for partition_id, partition_events in by_partition.items():\n   \
          \         with self.partition_locks[partition_id]:\n                self.partitions[partition_id].extend(partition_events)\n\
          \n    def subscribe(self, group_id: str, consumer_id: str) -> Consumer:\n        '''Register consumer in group'''\n\
          \        consumer = Consumer(\n            id=consumer_id,\n            group_id=group_id,\n            last_heartbeat=time.time()\n\
          \        )\n\n        self.consumer_groups[group_id][consumer_id] = consumer\n        self._rebalance(group_id)\n\
          \n        return consumer\n\n    def _rebalance(self, group_id: str):\n        '''Rebalance partitions among consumers\
          \ in group'''\n        consumers = list(self.consumer_groups[group_id].values())\n        if not consumers:\n  \
          \          return\n\n        # Clear current assignments\n        for consumer in consumers:\n            consumer.assigned_partitions.clear()\n\
          \n        # Assign partitions round-robin\n        for i in range(self.num_partitions):\n            consumer =\
          \ consumers[i % len(consumers)]\n            consumer.assigned_partitions.add(i)\n\n    def poll(self, consumer:\
          \ Consumer, max_events: int = 100,\n             timeout: float = 1.0) -> list[ChangeEvent]:\n        '''Poll for\
          \ events assigned to consumer'''\n        consumer.last_heartbeat = time.time()\n        events = []\n\n       \
          \ for partition_id in consumer.assigned_partitions:\n            offset = self.consumer_offsets[consumer.group_id][partition_id]\n\
          \n            with self.partition_locks[partition_id]:\n                partition_events = self.partitions[partition_id]\n\
          \                new_events = partition_events[offset:offset + max_events]\n                events.extend(new_events)\n\
          \n        return events\n\n    def commit_offset(self, consumer: Consumer, partition_id: int, offset: int):\n  \
          \      '''Commit consumer offset'''\n        self.consumer_offsets[consumer.group_id][partition_id] = offset\n\n\
          \    def get_lag(self, group_id: str) -> dict[int, int]:\n        '''Get consumer group lag per partition'''\n \
          \       lag = {}\n        for partition_id in range(self.num_partitions):\n            with self.partition_locks[partition_id]:\n\
          \                current_offset = len(self.partitions[partition_id])\n            committed_offset = self.consumer_offsets[group_id][partition_id]\n\
          \            lag[partition_id] = current_offset - committed_offset\n        return lag\n\nclass CDCEventProcessor:\n\
          \    '''Process CDC events with handlers'''\n\n    def __init__(self, stream: CDCEventStream, group_id: str):\n\
          \        self.stream = stream\n        self.group_id = group_id\n        self.handlers: dict[str, list[Callable]]\
          \ = defaultdict(list)\n        self.running = False\n\n    def register_handler(self, table: str, handler: Callable[[ChangeEvent],\
          \ None]):\n        '''Register handler for table changes'''\n        self.handlers[table].append(handler)\n\n  \
          \  def register_global_handler(self, handler: Callable[[ChangeEvent], None]):\n        '''Register handler for all\
          \ changes'''\n        self.handlers['*'].append(handler)\n\n    def start(self, consumer_id: str):\n        '''Start\
          \ processing events'''\n        consumer = self.stream.subscribe(self.group_id, consumer_id)\n        self.running\
          \ = True\n\n        while self.running:\n            events = self.stream.poll(consumer)\n\n            for event\
          \ in events:\n                self._process_event(event)\n\n            # Commit offsets after processing\n    \
          \        for partition_id in consumer.assigned_partitions:\n                current_offset = len(self.stream.partitions[partition_id])\n\
          \                self.stream.commit_offset(consumer, partition_id, current_offset)\n\n            if not events:\n\
          \                time.sleep(0.1)  # Back off when no events\n\n    def _process_event(self, event: ChangeEvent):\n\
          \        '''Process single event'''\n        # Table-specific handlers\n        for handler in self.handlers.get(event.table,\
          \ []):\n            try:\n                handler(event)\n            except Exception as e:\n                print(f\"\
          Handler error for {event.table}: {e}\")\n\n        # Global handlers\n        for handler in self.handlers.get('*',\
          \ []):\n            try:\n                handler(event)\n            except Exception as e:\n                print(f\"\
          Global handler error: {e}\")\n\n    def stop(self):\n        self.running = False\n```\n"
      pitfalls:
      - Consumer rebalancing causes brief processing pause
      - Partition by pk, not just table, for better parallelism
      - Offset commit after processing prevents duplicates
      - Consumer heartbeat failure needs automatic reassignment
    - name: Schema Evolution & Compatibility
      description: Handle schema changes without breaking consumers
      skills:
      - Schema evolution
      - Compatibility checks
      - Migration
      hints:
        level1: Track schema versions; include schema in events or registry
        level2: 'Compatibility: backward (new reader, old data), forward (old reader, new data)'
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom enum import Enum\nimport\
          \ json\n\nclass CompatibilityType(Enum):\n    BACKWARD = \"backward\"      # New reader can read old data\n    FORWARD\
          \ = \"forward\"        # Old reader can read new data\n    FULL = \"full\"              # Both directions\n    NONE\
          \ = \"none\"              # No compatibility check\n\n@dataclass\nclass SchemaVersion:\n    id: int\n    table:\
          \ str\n    columns: list[dict]  # [{name, type, nullable, default}]\n    primary_key: list[str]\n    created_at:\
          \ float\n\nclass SchemaRegistry:\n    def __init__(self):\n        self.schemas: dict[str, list[SchemaVersion]]\
          \ = defaultdict(list)\n        self.compatibility_mode: dict[str, CompatibilityType] = {}\n\n    def register_schema(self,\
          \ table: str, columns: list[dict],\n                        primary_key: list[str]) -> SchemaVersion:\n        '''Register\
          \ new schema version'''\n        versions = self.schemas[table]\n        version_id = len(versions) + 1\n\n    \
          \    schema = SchemaVersion(\n            id=version_id,\n            table=table,\n            columns=columns,\n\
          \            primary_key=primary_key,\n            created_at=time.time()\n        )\n\n        # Check compatibility\
          \ with previous version\n        if versions:\n            mode = self.compatibility_mode.get(table, CompatibilityType.BACKWARD)\n\
          \            if not self._check_compatibility(versions[-1], schema, mode):\n                raise ValueError(f\"\
          Schema change not compatible with {mode.value}\")\n\n        versions.append(schema)\n        return schema\n\n\
          \    def get_schema(self, table: str, version: int = None) -> Optional[SchemaVersion]:\n        '''Get schema version'''\n\
          \        versions = self.schemas.get(table, [])\n        if not versions:\n            return None\n        if version:\n\
          \            return versions[version - 1] if version <= len(versions) else None\n        return versions[-1]\n\n\
          \    def _check_compatibility(self, old_schema: SchemaVersion,\n                              new_schema: SchemaVersion,\n\
          \                              mode: CompatibilityType) -> bool:\n        '''Check if schema change is compatible'''\n\
          \        if mode == CompatibilityType.NONE:\n            return True\n\n        old_cols = {c['name']: c for c in\
          \ old_schema.columns}\n        new_cols = {c['name']: c for c in new_schema.columns}\n\n        # Backward: new\
          \ reader can read old data\n        # - New columns must be nullable or have default\n        # - Column types must\
          \ be compatible (can widen, not narrow)\n        if mode in [CompatibilityType.BACKWARD, CompatibilityType.FULL]:\n\
          \            for name, col in new_cols.items():\n                if name not in old_cols:\n                    #\
          \ New column\n                    if not col.get('nullable') and col.get('default') is None:\n                 \
          \       return False\n\n            for name, old_col in old_cols.items():\n                if name in new_cols:\n\
          \                    # Type change must be compatible\n                    if not self._types_compatible(old_col['type'],\n\
          \                                                   new_cols[name]['type']):\n                        return False\n\
          \n        # Forward: old reader can read new data\n        # - Cannot remove required columns\n        # - Cannot\
          \ add required columns\n        if mode in [CompatibilityType.FORWARD, CompatibilityType.FULL]:\n            for\
          \ name, old_col in old_cols.items():\n                if name not in new_cols:\n                    return False\
          \  # Column removed\n\n        return True\n\n    def _types_compatible(self, old_type: str, new_type: str) -> bool:\n\
          \        '''Check if type change is compatible'''\n        # Widening is OK: int -> bigint, varchar(10) -> varchar(100)\n\
          \        compatible_widenings = {\n            ('int', 'bigint'): True,\n            ('float', 'double'): True,\n\
          \            ('varchar', 'text'): True,\n        }\n\n        if old_type == new_type:\n            return True\n\
          \n        return compatible_widenings.get((old_type, new_type), False)\n\nclass SchemaEvolutionHandler:\n    '''Handle\
          \ schema changes in CDC stream'''\n\n    def __init__(self, registry: SchemaRegistry):\n        self.registry =\
          \ registry\n\n    def detect_schema_change(self, event: ChangeEvent,\n                              expected_schema:\
          \ SchemaVersion) -> Optional[list[dict]]:\n        '''Detect if event has different schema'''\n        event_columns\
          \ = set()\n\n        if event.after:\n            event_columns.update(event.after.keys())\n        if event.before:\n\
          \            event_columns.update(event.before.keys())\n\n        expected_columns = {c['name'] for c in expected_schema.columns}\n\
          \n        # Check for differences\n        new_columns = event_columns - expected_columns\n        removed_columns\
          \ = expected_columns - event_columns\n\n        if new_columns or removed_columns:\n            return {\n     \
          \           'new_columns': list(new_columns),\n                'removed_columns': list(removed_columns)\n      \
          \      }\n\n        return None\n\n    def transform_event(self, event: ChangeEvent,\n                         source_schema:\
          \ SchemaVersion,\n                         target_schema: SchemaVersion) -> ChangeEvent:\n        '''Transform event\
          \ from source schema to target schema'''\n        source_cols = {c['name']: c for c in source_schema.columns}\n\
          \        target_cols = {c['name']: c for c in target_schema.columns}\n\n        def transform_record(record: dict)\
          \ -> dict:\n            if not record:\n                return record\n\n            transformed = {}\n\n      \
          \      for col_name, col_def in target_cols.items():\n                if col_name in record:\n                 \
          \   # Column exists - may need type conversion\n                    transformed[col_name] = self._convert_type(\n\
          \                        record[col_name],\n                        source_cols.get(col_name, {}).get('type'),\n\
          \                        col_def['type']\n                    )\n                elif col_def.get('default') is\
          \ not None:\n                    # Use default value\n                    transformed[col_name] = col_def['default']\n\
          \                elif col_def.get('nullable'):\n                    # Nullable - use None\n                    transformed[col_name]\
          \ = None\n                # else: required column missing - error\n\n            return transformed\n\n        return\
          \ ChangeEvent(\n            id=event.id,\n            timestamp=event.timestamp,\n            transaction_id=event.transaction_id,\n\
          \            operation=event.operation,\n            database=event.database,\n            schema=event.schema,\n\
          \            table=event.table,\n            primary_key=event.primary_key,\n            before=transform_record(event.before),\n\
          \            after=transform_record(event.after),\n            metadata={**event.metadata, 'schema_version': target_schema.id}\n\
          \        )\n\n    def _convert_type(self, value: Any, source_type: str,\n                       target_type: str)\
          \ -> Any:\n        '''Convert value between types'''\n        if value is None:\n            return None\n\n   \
          \     if source_type == target_type:\n            return value\n\n        # Type conversions\n        if target_type\
          \ in ('bigint', 'int'):\n            return int(value)\n        elif target_type in ('float', 'double'):\n     \
          \       return float(value)\n        elif target_type in ('varchar', 'text', 'string'):\n            return str(value)\n\
          \        elif target_type == 'boolean':\n            return bool(value)\n\n        return value\n```\n"
      pitfalls:
      - DDL events need special handling - may require resync
      - Schema registry is single point of failure - replicate it
      - Backward compatibility is usually most important
      - Type narrowing (bigint->int) can cause data loss
  file-upload-service:
    name: Resumable File Upload Service
    description: Build a file upload service supporting chunked uploads, resumable transfers, virus scanning, and storage
      backends (S3, local).
    why_expert: File uploads are tricky - large files, network failures, security. Understanding chunked protocols and storage
      abstractions enables reliable upload handling.
    difficulty: advanced
    tags:
    - files
    - uploads
    - storage
    - s3
    - resumable
    estimated_hours: 35
    prerequisites:
    - build-http-server
    milestones:
    - name: Chunked Upload Protocol
      description: Implement tus.io-style resumable upload protocol
      skills:
      - Chunked uploads
      - Resume logic
      - HTTP headers
      hints:
        level1: Client sends chunks; server tracks offset; resume from last chunk
        level2: Use Upload-Offset header to track progress
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional, BinaryIO\nfrom enum import\
          \ Enum\nimport os\nimport secrets\nimport time\nimport hashlib\n\nclass UploadStatus(Enum):\n    CREATED = \"created\"\
          \n    IN_PROGRESS = \"in_progress\"\n    COMPLETE = \"complete\"\n    EXPIRED = \"expired\"\n\n@dataclass\nclass\
          \ Upload:\n    id: str\n    size: int                    # Total file size\n    offset: int                  # Current\
          \ offset\n    status: UploadStatus\n    filename: Optional[str]\n    content_type: Optional[str]\n    metadata:\
          \ dict\n    created_at: float\n    expires_at: float\n    checksum: Optional[str] = None\n\nclass ChunkedUploadService:\n\
          \    def __init__(self, storage_path: str, max_size: int = 5 * 1024**3,\n                 chunk_size: int = 5 *\
          \ 1024**2, expiry_hours: int = 24):\n        self.storage_path = storage_path\n        self.max_size = max_size\n\
          \        self.chunk_size = chunk_size\n        self.expiry_hours = expiry_hours\n        self.uploads: dict[str,\
          \ Upload] = {}\n\n        os.makedirs(storage_path, exist_ok=True)\n        os.makedirs(os.path.join(storage_path,\
          \ 'chunks'), exist_ok=True)\n        os.makedirs(os.path.join(storage_path, 'complete'), exist_ok=True)\n\n    def\
          \ create_upload(self, size: int, filename: str = None,\n                      content_type: str = None, metadata:\
          \ dict = None) -> Upload:\n        '''Create new upload session (POST /uploads)'''\n        if size > self.max_size:\n\
          \            raise ValueError(f\"File size exceeds maximum ({self.max_size})\")\n\n        upload_id = secrets.token_urlsafe(16)\n\
          \        now = time.time()\n\n        upload = Upload(\n            id=upload_id,\n            size=size,\n    \
          \        offset=0,\n            status=UploadStatus.CREATED,\n            filename=filename,\n            content_type=content_type,\n\
          \            metadata=metadata or {},\n            created_at=now,\n            expires_at=now + (self.expiry_hours\
          \ * 3600)\n        )\n\n        self.uploads[upload_id] = upload\n\n        # Create chunk file\n        chunk_path\
          \ = self._get_chunk_path(upload_id)\n        with open(chunk_path, 'wb') as f:\n            f.truncate(size)  #\
          \ Pre-allocate\n\n        return upload\n\n    def upload_chunk(self, upload_id: str, data: bytes,\n           \
          \          offset: int, checksum: str = None) -> Upload:\n        '''Upload a chunk (PATCH /uploads/{id})'''\n \
          \       upload = self.uploads.get(upload_id)\n        if not upload:\n            raise ValueError(\"Upload not\
          \ found\")\n\n        if upload.status == UploadStatus.EXPIRED:\n            raise ValueError(\"Upload expired\"\
          )\n\n        if upload.status == UploadStatus.COMPLETE:\n            raise ValueError(\"Upload already complete\"\
          )\n\n        if offset != upload.offset:\n            raise ValueError(f\"Offset mismatch. Expected {upload.offset},\
          \ got {offset}\")\n\n        # Verify checksum if provided\n        if checksum:\n            computed = hashlib.sha256(data).hexdigest()\n\
          \            if computed != checksum:\n                raise ValueError(\"Checksum mismatch\")\n\n        # Write\
          \ chunk\n        chunk_path = self._get_chunk_path(upload_id)\n        with open(chunk_path, 'r+b') as f:\n    \
          \        f.seek(offset)\n            f.write(data)\n\n        upload.offset += len(data)\n        upload.status\
          \ = UploadStatus.IN_PROGRESS\n\n        # Check if complete\n        if upload.offset >= upload.size:\n        \
          \    self._finalize_upload(upload)\n\n        return upload\n\n    def get_upload(self, upload_id: str) -> Optional[Upload]:\n\
          \        '''Get upload status (HEAD /uploads/{id})'''\n        upload = self.uploads.get(upload_id)\n        if\
          \ upload and time.time() > upload.expires_at:\n            upload.status = UploadStatus.EXPIRED\n        return\
          \ upload\n\n    def _finalize_upload(self, upload: Upload):\n        '''Move completed upload to final location'''\n\
          \        chunk_path = self._get_chunk_path(upload.id)\n        final_path = self._get_final_path(upload.id, upload.filename)\n\
          \n        os.rename(chunk_path, final_path)\n        upload.status = UploadStatus.COMPLETE\n\n        # Calculate\
          \ final checksum\n        with open(final_path, 'rb') as f:\n            upload.checksum = hashlib.sha256(f.read()).hexdigest()\n\
          \n    def _get_chunk_path(self, upload_id: str) -> str:\n        return os.path.join(self.storage_path, 'chunks',\
          \ upload_id)\n\n    def _get_final_path(self, upload_id: str, filename: str = None) -> str:\n        name = filename\
          \ or upload_id\n        return os.path.join(self.storage_path, 'complete', f\"{upload_id}_{name}\")\n\n    def cleanup_expired(self):\n\
          \        '''Remove expired uploads'''\n        now = time.time()\n        expired = [uid for uid, u in self.uploads.items()\n\
          \                   if now > u.expires_at and u.status != UploadStatus.COMPLETE]\n\n        for upload_id in expired:\n\
          \            chunk_path = self._get_chunk_path(upload_id)\n            if os.path.exists(chunk_path):\n        \
          \        os.remove(chunk_path)\n            del self.uploads[upload_id]\n```\n"
      pitfalls:
      - Pre-allocate file to avoid fragmentation
      - 'Offset mismatch: client must track and resume from server''s offset'
      - Chunk checksum prevents corruption from network errors
      - Expiry cleanup must not delete in-progress uploads
    - name: Storage Abstraction
      description: Implement storage backends (local, S3, GCS) with common interface
      skills:
      - Storage abstraction
      - S3 API
      - Multipart uploads
      hints:
        level1: 'Abstract storage: put, get, delete, list operations'
        level2: S3 multipart upload for large files
        level3: "\n```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import BinaryIO,\
          \ Iterator, Optional\nimport os\nimport shutil\n\n@dataclass\nclass StorageObject:\n    key: str\n    size: int\n\
          \    content_type: Optional[str]\n    last_modified: float\n    metadata: dict\n    etag: Optional[str] = None\n\
          \nclass StorageBackend(ABC):\n    @abstractmethod\n    def put(self, key: str, data: BinaryIO, size: int,\n    \
          \        content_type: str = None, metadata: dict = None) -> StorageObject:\n        pass\n\n    @abstractmethod\n\
          \    def get(self, key: str) -> tuple[BinaryIO, StorageObject]:\n        pass\n\n    @abstractmethod\n    def delete(self,\
          \ key: str) -> bool:\n        pass\n\n    @abstractmethod\n    def list(self, prefix: str = \"\", limit: int = 1000)\
          \ -> Iterator[StorageObject]:\n        pass\n\n    @abstractmethod\n    def exists(self, key: str) -> bool:\n  \
          \      pass\n\nclass LocalStorageBackend(StorageBackend):\n    def __init__(self, base_path: str):\n        self.base_path\
          \ = base_path\n        os.makedirs(base_path, exist_ok=True)\n\n    def _get_path(self, key: str) -> str:\n    \
          \    # Sanitize key to prevent path traversal\n        safe_key = key.replace('..', '').lstrip('/')\n        return\
          \ os.path.join(self.base_path, safe_key)\n\n    def put(self, key: str, data: BinaryIO, size: int,\n           \
          \ content_type: str = None, metadata: dict = None) -> StorageObject:\n        path = self._get_path(key)\n     \
          \   os.makedirs(os.path.dirname(path), exist_ok=True)\n\n        with open(path, 'wb') as f:\n            shutil.copyfileobj(data,\
          \ f)\n\n        # Store metadata in sidecar file\n        meta_path = path + '.meta'\n        import json\n    \
          \    with open(meta_path, 'w') as f:\n            json.dump({\n                'content_type': content_type,\n \
          \               'metadata': metadata or {}\n            }, f)\n\n        stat = os.stat(path)\n        return StorageObject(\n\
          \            key=key,\n            size=stat.st_size,\n            content_type=content_type,\n            last_modified=stat.st_mtime,\n\
          \            metadata=metadata or {}\n        )\n\n    def get(self, key: str) -> tuple[BinaryIO, StorageObject]:\n\
          \        path = self._get_path(key)\n        if not os.path.exists(path):\n            raise FileNotFoundError(key)\n\
          \n        stat = os.stat(path)\n\n        # Load metadata\n        meta_path = path + '.meta'\n        content_type\
          \ = None\n        metadata = {}\n        if os.path.exists(meta_path):\n            import json\n            with\
          \ open(meta_path) as f:\n                meta = json.load(f)\n                content_type = meta.get('content_type')\n\
          \                metadata = meta.get('metadata', {})\n\n        obj = StorageObject(\n            key=key,\n   \
          \         size=stat.st_size,\n            content_type=content_type,\n            last_modified=stat.st_mtime,\n\
          \            metadata=metadata\n        )\n\n        return open(path, 'rb'), obj\n\n    def delete(self, key: str)\
          \ -> bool:\n        path = self._get_path(key)\n        meta_path = path + '.meta'\n\n        if os.path.exists(path):\n\
          \            os.remove(path)\n            if os.path.exists(meta_path):\n                os.remove(meta_path)\n\
          \            return True\n        return False\n\n    def list(self, prefix: str = \"\", limit: int = 1000) -> Iterator[StorageObject]:\n\
          \        base = self._get_path(prefix)\n        count = 0\n\n        for root, dirs, files in os.walk(self.base_path):\n\
          \            for name in files:\n                if name.endswith('.meta'):\n                    continue\n\n  \
          \              path = os.path.join(root, name)\n                key = os.path.relpath(path, self.base_path)\n\n\
          \                if prefix and not key.startswith(prefix):\n                    continue\n\n                stat\
          \ = os.stat(path)\n                yield StorageObject(\n                    key=key,\n                    size=stat.st_size,\n\
          \                    content_type=None,\n                    last_modified=stat.st_mtime,\n                    metadata={}\n\
          \                )\n\n                count += 1\n                if count >= limit:\n                    return\n\
          \n    def exists(self, key: str) -> bool:\n        return os.path.exists(self._get_path(key))\n\nclass S3StorageBackend(StorageBackend):\n\
          \    '''S3-compatible storage backend'''\n\n    def __init__(self, bucket: str, region: str = 'us-east-1',\n   \
          \              endpoint_url: str = None):\n        import boto3\n        self.bucket = bucket\n        self.client\
          \ = boto3.client(\n            's3',\n            region_name=region,\n            endpoint_url=endpoint_url  #\
          \ For MinIO, LocalStack\n        )\n\n    def put(self, key: str, data: BinaryIO, size: int,\n            content_type:\
          \ str = None, metadata: dict = None) -> StorageObject:\n\n        extra_args = {}\n        if content_type:\n  \
          \          extra_args['ContentType'] = content_type\n        if metadata:\n            extra_args['Metadata'] =\
          \ {k: str(v) for k, v in metadata.items()}\n\n        # Use multipart for large files\n        if size > 100 * 1024\
          \ * 1024:  # 100MB\n            return self._multipart_upload(key, data, size, extra_args)\n\n        self.client.upload_fileobj(data,\
          \ self.bucket, key, ExtraArgs=extra_args)\n\n        # Get object info\n        response = self.client.head_object(Bucket=self.bucket,\
          \ Key=key)\n\n        return StorageObject(\n            key=key,\n            size=response['ContentLength'],\n\
          \            content_type=response.get('ContentType'),\n            last_modified=response['LastModified'].timestamp(),\n\
          \            metadata=response.get('Metadata', {}),\n            etag=response['ETag'].strip('\"')\n        )\n\n\
          \    def _multipart_upload(self, key: str, data: BinaryIO, size: int,\n                          extra_args: dict)\
          \ -> StorageObject:\n        '''Multipart upload for large files'''\n        part_size = 100 * 1024 * 1024  # 100MB\
          \ parts\n\n        # Initiate multipart upload\n        response = self.client.create_multipart_upload(\n      \
          \      Bucket=self.bucket,\n            Key=key,\n            **extra_args\n        )\n        upload_id = response['UploadId']\n\
          \n        parts = []\n        part_number = 1\n\n        try:\n            while True:\n                chunk =\
          \ data.read(part_size)\n                if not chunk:\n                    break\n\n                response = self.client.upload_part(\n\
          \                    Bucket=self.bucket,\n                    Key=key,\n                    UploadId=upload_id,\n\
          \                    PartNumber=part_number,\n                    Body=chunk\n                )\n\n            \
          \    parts.append({\n                    'PartNumber': part_number,\n                    'ETag': response['ETag']\n\
          \                })\n                part_number += 1\n\n            # Complete upload\n            self.client.complete_multipart_upload(\n\
          \                Bucket=self.bucket,\n                Key=key,\n                UploadId=upload_id,\n          \
          \      MultipartUpload={'Parts': parts}\n            )\n\n        except Exception:\n            # Abort on failure\n\
          \            self.client.abort_multipart_upload(\n                Bucket=self.bucket,\n                Key=key,\n\
          \                UploadId=upload_id\n            )\n            raise\n\n        return self.get(key)[1]\n\n   \
          \ def get(self, key: str) -> tuple[BinaryIO, StorageObject]:\n        response = self.client.get_object(Bucket=self.bucket,\
          \ Key=key)\n\n        obj = StorageObject(\n            key=key,\n            size=response['ContentLength'],\n\
          \            content_type=response.get('ContentType'),\n            last_modified=response['LastModified'].timestamp(),\n\
          \            metadata=response.get('Metadata', {}),\n            etag=response['ETag'].strip('\"')\n        )\n\n\
          \        return response['Body'], obj\n\n    def delete(self, key: str) -> bool:\n        try:\n            self.client.delete_object(Bucket=self.bucket,\
          \ Key=key)\n            return True\n        except:\n            return False\n\n    def list(self, prefix: str\
          \ = \"\", limit: int = 1000) -> Iterator[StorageObject]:\n        paginator = self.client.get_paginator('list_objects_v2')\n\
          \        count = 0\n\n        for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):\n            for\
          \ obj in page.get('Contents', []):\n                yield StorageObject(\n                    key=obj['Key'],\n\
          \                    size=obj['Size'],\n                    content_type=None,\n                    last_modified=obj['LastModified'].timestamp(),\n\
          \                    metadata={},\n                    etag=obj['ETag'].strip('\"')\n                )\n       \
          \         count += 1\n                if count >= limit:\n                    return\n\n    def exists(self, key:\
          \ str) -> bool:\n        try:\n            self.client.head_object(Bucket=self.bucket, Key=key)\n            return\
          \ True\n        except:\n            return False\n```\n"
      pitfalls:
      - 'S3 multipart: parts must be at least 5MB (except last)'
      - 'Local storage: sanitize keys to prevent path traversal'
      - S3 eventually consistent for overwrite - use versioning
      - Streaming large files - don't load entire file in memory
    - name: Virus Scanning & Validation
      description: Implement file validation with type checking and virus scanning
      skills:
      - File validation
      - MIME detection
      - Antivirus integration
      hints:
        level1: Check magic bytes, not just extension
        level2: ClamAV for virus scanning via clamd socket
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional, BinaryIO\nfrom enum import Enum\n\
          import mimetypes\n\nclass ScanResult(Enum):\n    CLEAN = \"clean\"\n    INFECTED = \"infected\"\n    ERROR = \"\
          error\"\n    SKIPPED = \"skipped\"\n\n@dataclass\nclass ValidationResult:\n    valid: bool\n    mime_type: Optional[str]\n\
          \    detected_extension: Optional[str]\n    scan_result: ScanResult\n    scan_details: Optional[str] = None\n  \
          \  errors: list[str] = None\n\nclass FileValidator:\n    # Magic bytes for common file types\n    MAGIC_BYTES =\
          \ {\n        b'\\x89PNG\\r\\n\\x1a\\n': ('image/png', '.png'),\n        b'\\xff\\xd8\\xff': ('image/jpeg', '.jpg'),\n\
          \        b'GIF87a': ('image/gif', '.gif'),\n        b'GIF89a': ('image/gif', '.gif'),\n        b'%PDF': ('application/pdf',\
          \ '.pdf'),\n        b'PK\\x03\\x04': ('application/zip', '.zip'),\n        b'\\x1f\\x8b': ('application/gzip', '.gz'),\n\
          \    }\n\n    def __init__(self, allowed_types: list[str] = None,\n                 max_size: int = 100 * 1024 *\
          \ 1024,\n                 scan_viruses: bool = True):\n        self.allowed_types = set(allowed_types) if allowed_types\
          \ else None\n        self.max_size = max_size\n        self.scan_viruses = scan_viruses\n        self.scanner =\
          \ ClamAVScanner() if scan_viruses else None\n\n    def validate(self, data: BinaryIO, filename: str = None,\n  \
          \               claimed_type: str = None) -> ValidationResult:\n        errors = []\n\n        # Read header for\
          \ magic byte detection\n        header = data.read(16)\n        data.seek(0)\n\n        # Detect MIME type from\
          \ magic bytes\n        detected_type, detected_ext = self._detect_type(header)\n\n        # Check size\n       \
          \ data.seek(0, 2)  # Seek to end\n        size = data.tell()\n        data.seek(0)\n\n        if size > self.max_size:\n\
          \            errors.append(f\"File too large: {size} > {self.max_size}\")\n\n        # Check allowed types\n   \
          \     if self.allowed_types and detected_type:\n            if detected_type not in self.allowed_types:\n      \
          \          errors.append(f\"File type not allowed: {detected_type}\")\n\n        # Check extension matches content\n\
          \        if filename and detected_ext:\n            actual_ext = '.' + filename.rsplit('.', 1)[-1].lower() if '.'\
          \ in filename else ''\n            if actual_ext and actual_ext != detected_ext:\n                errors.append(f\"\
          Extension mismatch: {actual_ext} vs detected {detected_ext}\")\n\n        # Virus scan\n        scan_result = ScanResult.SKIPPED\n\
          \        scan_details = None\n\n        if self.scan_viruses and self.scanner:\n            scan_result, scan_details\
          \ = self.scanner.scan(data)\n            data.seek(0)\n\n            if scan_result == ScanResult.INFECTED:\n  \
          \              errors.append(f\"Virus detected: {scan_details}\")\n\n        return ValidationResult(\n        \
          \    valid=len(errors) == 0,\n            mime_type=detected_type,\n            detected_extension=detected_ext,\n\
          \            scan_result=scan_result,\n            scan_details=scan_details,\n            errors=errors if errors\
          \ else None\n        )\n\n    def _detect_type(self, header: bytes) -> tuple[Optional[str], Optional[str]]:\n  \
          \      for magic, (mime_type, ext) in self.MAGIC_BYTES.items():\n            if header.startswith(magic):\n    \
          \            return mime_type, ext\n        return None, None\n\nclass ClamAVScanner:\n    '''Virus scanner using\
          \ ClamAV daemon'''\n\n    def __init__(self, socket_path: str = '/var/run/clamav/clamd.ctl',\n                 host:\
          \ str = None, port: int = 3310):\n        self.socket_path = socket_path\n        self.host = host\n        self.port\
          \ = port\n\n    def scan(self, data: BinaryIO) -> tuple[ScanResult, Optional[str]]:\n        import socket\n\n \
          \       try:\n            if self.host:\n                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\
          \                sock.connect((self.host, self.port))\n            else:\n                sock = socket.socket(socket.AF_UNIX,\
          \ socket.SOCK_STREAM)\n                sock.connect(self.socket_path)\n\n            # Send INSTREAM command\n \
          \           sock.send(b'nINSTREAM\\n')\n\n            # Send file in chunks\n            while True:\n         \
          \       chunk = data.read(8192)\n                if not chunk:\n                    break\n                # Send\
          \ chunk size (4 bytes big endian) + data\n                sock.send(len(chunk).to_bytes(4, 'big') + chunk)\n\n \
          \           # Send zero-length chunk to end\n            sock.send(b'\\x00\\x00\\x00\\x00')\n\n            # Read\
          \ response\n            response = sock.recv(4096).decode().strip()\n            sock.close()\n\n            if\
          \ 'OK' in response:\n                return ScanResult.CLEAN, None\n            elif 'FOUND' in response:\n    \
          \            virus_name = response.split('FOUND')[0].strip()\n                return ScanResult.INFECTED, virus_name\n\
          \            else:\n                return ScanResult.ERROR, response\n\n        except Exception as e:\n      \
          \      return ScanResult.ERROR, str(e)\n```\n"
      pitfalls:
      - Never trust file extension alone - always check magic bytes
      - Virus scan before storing, not after
      - ClamAV can timeout on large files - set appropriate limits
      - Quarantine infected files, don't delete immediately (for forensics)
  media-processing:
    name: Media Processing Pipeline
    description: Build a media processing service for image resizing, video transcoding, and thumbnail generation.
    why_expert: Media processing is CPU-intensive and complex. Understanding codecs, formats, and async processing enables
      building efficient media services.
    difficulty: advanced
    tags:
    - media
    - images
    - video
    - transcoding
    - thumbnails
    estimated_hours: 40
    prerequisites:
    - build-message-queue
    milestones:
    - name: Image Processing
      description: Implement image resizing, format conversion, and optimization
      skills:
      - Image formats
      - Resizing algorithms
      - Optimization
      hints:
        level1: Generate multiple sizes on upload for responsive images
        level2: Preserve aspect ratio; use Lanczos for quality downscaling
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional, BinaryIO\nfrom enum import Enum\n\
          from PIL import Image\nimport io\n\nclass ImageFormat(Enum):\n    JPEG = \"jpeg\"\n    PNG = \"png\"\n    WEBP =\
          \ \"webp\"\n    AVIF = \"avif\"\n\n@dataclass\nclass ImageVariant:\n    name: str\n    width: int\n    height: Optional[int]\
          \  # None = maintain aspect ratio\n    format: ImageFormat\n    quality: int = 85\n\n@dataclass\nclass ProcessedImage:\n\
          \    variant: str\n    width: int\n    height: int\n    format: ImageFormat\n    size: int\n    data: bytes\n\n\
          class ImageProcessor:\n    DEFAULT_VARIANTS = [\n        ImageVariant(\"thumbnail\", 150, 150, ImageFormat.WEBP,\
          \ 80),\n        ImageVariant(\"small\", 320, None, ImageFormat.WEBP, 85),\n        ImageVariant(\"medium\", 640,\
          \ None, ImageFormat.WEBP, 85),\n        ImageVariant(\"large\", 1280, None, ImageFormat.WEBP, 90),\n        ImageVariant(\"\
          original\", 0, None, ImageFormat.WEBP, 95),  # 0 = keep size\n    ]\n\n    def __init__(self, variants: list[ImageVariant]\
          \ = None):\n        self.variants = variants or self.DEFAULT_VARIANTS\n\n    def process(self, image_data: BinaryIO)\
          \ -> list[ProcessedImage]:\n        '''Process image into all variants'''\n        original = Image.open(image_data)\n\
          \n        # Convert to RGB if necessary (for JPEG output)\n        if original.mode in ('RGBA', 'P'):\n        \
          \    background = Image.new('RGB', original.size, (255, 255, 255))\n            if original.mode == 'P':\n     \
          \           original = original.convert('RGBA')\n            background.paste(original, mask=original.split()[3])\n\
          \            original = background\n        elif original.mode != 'RGB':\n            original = original.convert('RGB')\n\
          \n        results = []\n        for variant in self.variants:\n            processed = self._process_variant(original,\
          \ variant)\n            results.append(processed)\n\n        return results\n\n    def _process_variant(self, image:\
          \ Image.Image,\n                         variant: ImageVariant) -> ProcessedImage:\n        # Calculate target size\n\
          \        if variant.width == 0:\n            # Keep original size\n            target_size = image.size\n      \
          \  elif variant.height:\n            # Fixed dimensions (crop to fit)\n            target_size = (variant.width,\
          \ variant.height)\n            image = self._crop_to_aspect(image, target_size)\n        else:\n            # Width\
          \ only - maintain aspect ratio\n            ratio = variant.width / image.width\n            target_size = (variant.width,\
          \ int(image.height * ratio))\n\n        # Resize if needed\n        if target_size != image.size:\n            image\
          \ = image.resize(target_size, Image.Resampling.LANCZOS)\n\n        # Convert to output format\n        output =\
          \ io.BytesIO()\n        format_str = variant.format.value.upper()\n\n        if variant.format == ImageFormat.WEBP:\n\
          \            image.save(output, 'WEBP', quality=variant.quality, method=6)\n        elif variant.format == ImageFormat.JPEG:\n\
          \            image.save(output, 'JPEG', quality=variant.quality, optimize=True)\n        elif variant.format ==\
          \ ImageFormat.PNG:\n            image.save(output, 'PNG', optimize=True)\n        elif variant.format == ImageFormat.AVIF:\n\
          \            # Requires pillow-avif-plugin\n            image.save(output, 'AVIF', quality=variant.quality)\n\n\
          \        data = output.getvalue()\n\n        return ProcessedImage(\n            variant=variant.name,\n       \
          \     width=image.width,\n            height=image.height,\n            format=variant.format,\n            size=len(data),\n\
          \            data=data\n        )\n\n    def _crop_to_aspect(self, image: Image.Image,\n                       \
          \ target_size: tuple[int, int]) -> Image.Image:\n        '''Crop image to target aspect ratio (center crop)'''\n\
          \        target_ratio = target_size[0] / target_size[1]\n        image_ratio = image.width / image.height\n\n  \
          \      if image_ratio > target_ratio:\n            # Image is wider - crop sides\n            new_width = int(image.height\
          \ * target_ratio)\n            left = (image.width - new_width) // 2\n            image = image.crop((left, 0, left\
          \ + new_width, image.height))\n        elif image_ratio < target_ratio:\n            # Image is taller - crop top/bottom\n\
          \            new_height = int(image.width / target_ratio)\n            top = (image.height - new_height) // 2\n\
          \            image = image.crop((0, top, image.width, top + new_height))\n\n        return image\n\n    def extract_metadata(self,\
          \ image_data: BinaryIO) -> dict:\n        '''Extract EXIF and other metadata'''\n        image = Image.open(image_data)\n\
          \        metadata = {\n            'width': image.width,\n            'height': image.height,\n            'format':\
          \ image.format,\n            'mode': image.mode\n        }\n\n        # Extract EXIF\n        if hasattr(image,\
          \ '_getexif') and image._getexif():\n            exif = image._getexif()\n            # Map common EXIF tags\n \
          \           exif_tags = {\n                271: 'camera_make',\n                272: 'camera_model',\n         \
          \       306: 'datetime',\n                274: 'orientation'\n            }\n            for tag_id, name in exif_tags.items():\n\
          \                if tag_id in exif:\n                    metadata[name] = exif[tag_id]\n\n        return metadata\n\
          ```\n"
      pitfalls:
      - 'EXIF orientation: rotate image according to tag before processing'
      - WebP/AVIF save significant bandwidth but check browser support
      - Lanczos is best for downscaling; use different for upscaling
      - Strip EXIF from output for privacy (location data)
    - name: Video Transcoding
      description: Implement video transcoding with FFmpeg for web playback
      skills:
      - FFmpeg
      - Video codecs
      - HLS streaming
      hints:
        level1: Transcode to H.264/AAC for broad compatibility
        level2: Generate HLS segments for adaptive streaming
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport subprocess\nimport os\n\
          import json\n\n@dataclass\nclass VideoProfile:\n    name: str\n    width: int\n    height: int\n    video_bitrate:\
          \ str\n    audio_bitrate: str\n    preset: str = \"medium\"\n\n@dataclass\nclass TranscodeResult:\n    profile:\
          \ str\n    path: str\n    width: int\n    height: int\n    duration: float\n    size: int\n\nclass VideoTranscoder:\n\
          \    PROFILES = [\n        VideoProfile(\"360p\", 640, 360, \"800k\", \"96k\"),\n        VideoProfile(\"480p\",\
          \ 854, 480, \"1400k\", \"128k\"),\n        VideoProfile(\"720p\", 1280, 720, \"2800k\", \"128k\"),\n        VideoProfile(\"\
          1080p\", 1920, 1080, \"5000k\", \"192k\"),\n    ]\n\n    def __init__(self, output_dir: str, ffmpeg_path: str =\
          \ \"ffmpeg\"):\n        self.output_dir = output_dir\n        self.ffmpeg = ffmpeg_path\n        os.makedirs(output_dir,\
          \ exist_ok=True)\n\n    def get_video_info(self, input_path: str) -> dict:\n        '''Get video metadata using\
          \ ffprobe'''\n        cmd = [\n            'ffprobe', '-v', 'quiet',\n            '-print_format', 'json',\n   \
          \         '-show_format', '-show_streams',\n            input_path\n        ]\n\n        result = subprocess.run(cmd,\
          \ capture_output=True, text=True)\n        data = json.loads(result.stdout)\n\n        video_stream = next(\n  \
          \          (s for s in data['streams'] if s['codec_type'] == 'video'),\n            None\n        )\n\n        return\
          \ {\n            'duration': float(data['format'].get('duration', 0)),\n            'width': video_stream['width']\
          \ if video_stream else 0,\n            'height': video_stream['height'] if video_stream else 0,\n            'codec':\
          \ video_stream['codec_name'] if video_stream else None,\n            'bitrate': int(data['format'].get('bit_rate',\
          \ 0))\n        }\n\n    def transcode_mp4(self, input_path: str, job_id: str,\n                      profile: VideoProfile)\
          \ -> TranscodeResult:\n        '''Transcode to MP4 with H.264'''\n        output_path = os.path.join(\n        \
          \    self.output_dir, f\"{job_id}_{profile.name}.mp4\"\n        )\n\n        cmd = [\n            self.ffmpeg, '-y',\n\
          \            '-i', input_path,\n            '-c:v', 'libx264',\n            '-preset', profile.preset,\n       \
          \     '-b:v', profile.video_bitrate,\n            '-maxrate', profile.video_bitrate,\n            '-bufsize', str(int(profile.video_bitrate.rstrip('k'))\
          \ * 2) + 'k',\n            '-vf', f'scale={profile.width}:{profile.height}:force_original_aspect_ratio=decrease,pad={profile.width}:{profile.height}:(ow-iw)/2:(oh-ih)/2',\n\
          \            '-c:a', 'aac',\n            '-b:a', profile.audio_bitrate,\n            '-movflags', '+faststart',\
          \  # Enable streaming\n            output_path\n        ]\n\n        subprocess.run(cmd, check=True, capture_output=True)\n\
          \n        info = self.get_video_info(output_path)\n        return TranscodeResult(\n            profile=profile.name,\n\
          \            path=output_path,\n            width=info['width'],\n            height=info['height'],\n         \
          \   duration=info['duration'],\n            size=os.path.getsize(output_path)\n        )\n\n    def generate_hls(self,\
          \ input_path: str, job_id: str) -> dict:\n        '''Generate HLS playlist with multiple qualities'''\n        hls_dir\
          \ = os.path.join(self.output_dir, job_id, 'hls')\n        os.makedirs(hls_dir, exist_ok=True)\n\n        info =\
          \ self.get_video_info(input_path)\n        source_height = info['height']\n\n        # Select appropriate profiles\n\
          \        profiles = [p for p in self.PROFILES if p.height <= source_height]\n        if not profiles:\n        \
          \    profiles = [self.PROFILES[0]]\n\n        variants = []\n        for profile in profiles:\n            variant_dir\
          \ = os.path.join(hls_dir, profile.name)\n            os.makedirs(variant_dir, exist_ok=True)\n\n            output_playlist\
          \ = os.path.join(variant_dir, 'playlist.m3u8')\n\n            cmd = [\n                self.ffmpeg, '-y',\n    \
          \            '-i', input_path,\n                '-c:v', 'libx264',\n                '-preset', 'fast',\n       \
          \         '-b:v', profile.video_bitrate,\n                '-vf', f'scale={profile.width}:-2',\n                '-c:a',\
          \ 'aac',\n                '-b:a', profile.audio_bitrate,\n                '-hls_time', '6',\n                '-hls_playlist_type',\
          \ 'vod',\n                '-hls_segment_filename', os.path.join(variant_dir, 'segment_%03d.ts'),\n             \
          \   output_playlist\n            ]\n\n            subprocess.run(cmd, check=True, capture_output=True)\n\n     \
          \       variants.append({\n                'name': profile.name,\n                'bandwidth': int(profile.video_bitrate.rstrip('k'))\
          \ * 1000,\n                'resolution': f'{profile.width}x{profile.height}',\n                'playlist': f'{profile.name}/playlist.m3u8'\n\
          \            })\n\n        # Generate master playlist\n        master_path = os.path.join(hls_dir, 'master.m3u8')\n\
          \        self._write_master_playlist(master_path, variants)\n\n        return {\n            'master_playlist':\
          \ master_path,\n            'variants': variants,\n            'duration': info['duration']\n        }\n\n    def\
          \ _write_master_playlist(self, path: str, variants: list[dict]):\n        with open(path, 'w') as f:\n         \
          \   f.write('#EXTM3U\\n')\n            for v in sorted(variants, key=lambda x: x['bandwidth']):\n              \
          \  f.write(f'#EXT-X-STREAM-INF:BANDWIDTH={v[\"bandwidth\"]},RESOLUTION={v[\"resolution\"]}\\n')\n              \
          \  f.write(f'{v[\"playlist\"]}\\n')\n\n    def generate_thumbnail(self, input_path: str, job_id: str,\n        \
          \                   time_offset: float = 1.0) -> str:\n        '''Generate thumbnail at specified time'''\n    \
          \    output_path = os.path.join(self.output_dir, f\"{job_id}_thumb.jpg\")\n\n        cmd = [\n            self.ffmpeg,\
          \ '-y',\n            '-i', input_path,\n            '-ss', str(time_offset),\n            '-vframes', '1',\n   \
          \         '-vf', 'scale=640:-1',\n            '-q:v', '3',\n            output_path\n        ]\n\n        subprocess.run(cmd,\
          \ check=True, capture_output=True)\n        return output_path\n```\n"
      pitfalls:
      - H.264 baseline profile for maximum compatibility
      - -movflags +faststart enables progressive download
      - HLS segment size affects startup time vs seeking
      - Video processing is CPU-intensive - use job queue
    - name: Processing Queue & Progress
      description: Implement async processing queue with progress tracking
      skills:
      - Job queues
      - Progress tracking
      - Webhooks
      hints:
        level1: Queue processing jobs; track progress with callbacks
        level2: Report progress percentage; notify on completion
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional\nfrom enum import\
          \ Enum\nimport threading\nimport queue\nimport time\n\nclass JobStatus(Enum):\n    QUEUED = \"queued\"\n    PROCESSING\
          \ = \"processing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n@dataclass\nclass ProcessingJob:\n\
          \    id: str\n    type: str  # image, video\n    input_path: str\n    status: JobStatus\n    progress: float = 0.0\n\
          \    created_at: float = field(default_factory=time.time)\n    started_at: Optional[float] = None\n    completed_at:\
          \ Optional[float] = None\n    result: Optional[dict] = None\n    error: Optional[str] = None\n    webhook_url: Optional[str]\
          \ = None\n\nclass MediaProcessingQueue:\n    def __init__(self, image_processor: ImageProcessor,\n             \
          \    video_transcoder: VideoTranscoder,\n                 num_workers: int = 4):\n        self.image_processor =\
          \ image_processor\n        self.video_transcoder = video_transcoder\n        self.queue = queue.PriorityQueue()\n\
          \        self.jobs: dict[str, ProcessingJob] = {}\n        self.callbacks: dict[str, list[Callable]] = {}\n    \
          \    self.workers = []\n\n        for i in range(num_workers):\n            worker = threading.Thread(target=self._worker,\
          \ daemon=True)\n            worker.start()\n            self.workers.append(worker)\n\n    def submit(self, job_id:\
          \ str, job_type: str, input_path: str,\n               priority: int = 5, webhook_url: str = None) -> ProcessingJob:\n\
          \        job = ProcessingJob(\n            id=job_id,\n            type=job_type,\n            input_path=input_path,\n\
          \            status=JobStatus.QUEUED,\n            webhook_url=webhook_url\n        )\n\n        self.jobs[job_id]\
          \ = job\n        self.queue.put((priority, time.time(), job_id))\n\n        return job\n\n    def get_status(self,\
          \ job_id: str) -> Optional[ProcessingJob]:\n        return self.jobs.get(job_id)\n\n    def on_progress(self, job_id:\
          \ str, callback: Callable[[float], None]):\n        if job_id not in self.callbacks:\n            self.callbacks[job_id]\
          \ = []\n        self.callbacks[job_id].append(callback)\n\n    def _worker(self):\n        while True:\n       \
          \     try:\n                priority, timestamp, job_id = self.queue.get()\n                job = self.jobs.get(job_id)\n\
          \n                if not job:\n                    continue\n\n                self._process_job(job)\n        \
          \        self.queue.task_done()\n\n            except Exception as e:\n                print(f\"Worker error: {e}\"\
          )\n\n    def _process_job(self, job: ProcessingJob):\n        job.status = JobStatus.PROCESSING\n        job.started_at\
          \ = time.time()\n\n        try:\n            if job.type == \"image\":\n                result = self._process_image(job)\n\
          \            elif job.type == \"video\":\n                result = self._process_video(job)\n            else:\n\
          \                raise ValueError(f\"Unknown job type: {job.type}\")\n\n            job.status = JobStatus.COMPLETED\n\
          \            job.result = result\n            job.progress = 100.0\n\n        except Exception as e:\n         \
          \   job.status = JobStatus.FAILED\n            job.error = str(e)\n\n        finally:\n            job.completed_at\
          \ = time.time()\n            self._notify_completion(job)\n\n    def _process_image(self, job: ProcessingJob) ->\
          \ dict:\n        with open(job.input_path, 'rb') as f:\n            results = self.image_processor.process(f)\n\n\
          \        self._update_progress(job, 100)\n\n        return {\n            'variants': [\n                {\n   \
          \                 'name': r.variant,\n                    'width': r.width,\n                    'height': r.height,\n\
          \                    'format': r.format.value,\n                    'size': r.size\n                }\n        \
          \        for r in results\n            ]\n        }\n\n    def _process_video(self, job: ProcessingJob) -> dict:\n\
          \        # Generate HLS\n        self._update_progress(job, 10)\n\n        hls_result = self.video_transcoder.generate_hls(\n\
          \            job.input_path, job.id\n        )\n        self._update_progress(job, 80)\n\n        # Generate thumbnail\n\
          \        thumbnail = self.video_transcoder.generate_thumbnail(\n            job.input_path, job.id\n        )\n\
          \        self._update_progress(job, 100)\n\n        return {\n            'hls': hls_result,\n            'thumbnail':\
          \ thumbnail\n        }\n\n    def _update_progress(self, job: ProcessingJob, progress: float):\n        job.progress\
          \ = progress\n\n        for callback in self.callbacks.get(job.id, []):\n            try:\n                callback(progress)\n\
          \            except:\n                pass\n\n    def _notify_completion(self, job: ProcessingJob):\n        if\
          \ job.webhook_url:\n            import httpx\n            try:\n                httpx.post(job.webhook_url, json={\n\
          \                    'job_id': job.id,\n                    'status': job.status.value,\n                    'result':\
          \ job.result,\n                    'error': job.error\n                })\n            except:\n               \
          \ pass\n```\n"
      pitfalls:
      - Video progress is hard to estimate - use stages not percentage
      - Webhook delivery can fail - implement retry
      - Clean up temp files after processing
      - 'Memory limits: process one large video at a time'
  cdn-implementation:
    name: Content Delivery Network (CDN)
    description: Build a CDN with edge caching, cache invalidation, and origin shielding.
    why_expert: CDNs are critical for performance. Understanding caching strategies, invalidation, and edge logic helps optimize
      content delivery.
    difficulty: expert
    tags:
    - cdn
    - caching
    - edge
    - performance
    - distributed
    estimated_hours: 45
    prerequisites:
    - build-http-server
    - build-redis
    milestones:
    - name: Edge Cache Implementation
      description: Implement edge caching with TTL and cache control headers
      skills:
      - HTTP caching
      - Cache-Control
      - Vary headers
      hints:
        level1: Cache responses based on Cache-Control headers from origin
        level2: Vary header determines cache key variations (Accept-Encoding, etc)
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport hashlib\nimport\
          \ time\n\n@dataclass\nclass CacheEntry:\n    key: str\n    body: bytes\n    headers: dict\n    status_code: int\n\
          \    created_at: float\n    expires_at: float\n    etag: Optional[str]\n    last_modified: Optional[str]\n    vary_headers:\
          \ list[str]\n\nclass CacheControl:\n    def __init__(self, header: str):\n        self.max_age: Optional[int] =\
          \ None\n        self.s_maxage: Optional[int] = None\n        self.no_cache = False\n        self.no_store = False\n\
          \        self.private = False\n        self.public = False\n        self.must_revalidate = False\n        self.stale_while_revalidate:\
          \ Optional[int] = None\n        self.stale_if_error: Optional[int] = None\n\n        self._parse(header)\n\n   \
          \ def _parse(self, header: str):\n        if not header:\n            return\n\n        for directive in header.split(','):\n\
          \            directive = directive.strip().lower()\n\n            if directive == 'no-cache':\n                self.no_cache\
          \ = True\n            elif directive == 'no-store':\n                self.no_store = True\n            elif directive\
          \ == 'private':\n                self.private = True\n            elif directive == 'public':\n                self.public\
          \ = True\n            elif directive == 'must-revalidate':\n                self.must_revalidate = True\n      \
          \      elif directive.startswith('max-age='):\n                self.max_age = int(directive.split('=')[1])\n   \
          \         elif directive.startswith('s-maxage='):\n                self.s_maxage = int(directive.split('=')[1])\n\
          \            elif directive.startswith('stale-while-revalidate='):\n                self.stale_while_revalidate\
          \ = int(directive.split('=')[1])\n            elif directive.startswith('stale-if-error='):\n                self.stale_if_error\
          \ = int(directive.split('=')[1])\n\n    def is_cacheable(self) -> bool:\n        if self.no_store:\n           \
          \ return False\n        if self.private:\n            return False  # CDN can't cache private\n        return True\n\
          \n    def get_ttl(self) -> int:\n        # s-maxage takes precedence for shared caches (CDN)\n        if self.s_maxage\
          \ is not None:\n            return self.s_maxage\n        if self.max_age is not None:\n            return self.max_age\n\
          \        return 0\n\nclass EdgeCache:\n    def __init__(self, max_size_bytes: int = 1024 * 1024 * 1024):  # 1GB\n\
          \        self.cache: dict[str, CacheEntry] = {}\n        self.max_size = max_size_bytes\n        self.current_size\
          \ = 0\n\n    def generate_cache_key(self, url: str, vary_headers: dict) -> str:\n        '''Generate cache key from\
          \ URL and Vary headers'''\n        key_parts = [url]\n\n        for header_name in sorted(vary_headers.keys()):\n\
          \            key_parts.append(f\"{header_name}:{vary_headers[header_name]}\")\n\n        key_string = '|'.join(key_parts)\n\
          \        return hashlib.sha256(key_string.encode()).hexdigest()\n\n    def get(self, key: str) -> Optional[CacheEntry]:\n\
          \        entry = self.cache.get(key)\n        if not entry:\n            return None\n\n        now = time.time()\n\
          \n        # Check if expired\n        if now > entry.expires_at:\n            # Check stale-while-revalidate\n \
          \           # (would need to trigger background revalidation)\n            return None\n\n        return entry\n\
          \n    def put(self, key: str, response: 'Response') -> Optional[CacheEntry]:\n        '''Cache response if cacheable'''\n\
          \        cache_control = CacheControl(response.headers.get('Cache-Control', ''))\n\n        if not cache_control.is_cacheable():\n\
          \            return None\n\n        ttl = cache_control.get_ttl()\n        if ttl <= 0:\n            return None\n\
          \n        # Parse Vary header\n        vary_headers = []\n        vary = response.headers.get('Vary', '')\n    \
          \    if vary and vary != '*':\n            vary_headers = [h.strip() for h in vary.split(',')]\n\n        now =\
          \ time.time()\n        entry = CacheEntry(\n            key=key,\n            body=response.body,\n            headers=dict(response.headers),\n\
          \            status_code=response.status_code,\n            created_at=now,\n            expires_at=now + ttl,\n\
          \            etag=response.headers.get('ETag'),\n            last_modified=response.headers.get('Last-Modified'),\n\
          \            vary_headers=vary_headers\n        )\n\n        # Evict if needed\n        entry_size = len(entry.body)\n\
          \        while self.current_size + entry_size > self.max_size:\n            self._evict_one()\n\n        self.cache[key]\
          \ = entry\n        self.current_size += entry_size\n\n        return entry\n\n    def _evict_one(self):\n      \
          \  '''Evict oldest entry (simple LRU would be better)'''\n        if not self.cache:\n            return\n\n   \
          \     oldest_key = min(self.cache.keys(),\n                         key=lambda k: self.cache[k].created_at)\n  \
          \      entry = self.cache.pop(oldest_key)\n        self.current_size -= len(entry.body)\n\n    def invalidate(self,\
          \ pattern: str):\n        '''Invalidate cache entries matching pattern'''\n        import fnmatch\n        keys_to_remove\
          \ = [\n            k for k in self.cache.keys()\n            if fnmatch.fnmatch(k, pattern)\n        ]\n       \
          \ for key in keys_to_remove:\n            entry = self.cache.pop(key)\n            self.current_size -= len(entry.body)\n\
          \n    def conditional_get(self, entry: CacheEntry, request_headers: dict) -> bool:\n        '''Check if conditional\
          \ GET can return 304'''\n        # If-None-Match (ETag)\n        if_none_match = request_headers.get('If-None-Match')\n\
          \        if if_none_match and entry.etag:\n            if if_none_match == entry.etag or if_none_match == '*':\n\
          \                return True\n\n        # If-Modified-Since\n        if_modified = request_headers.get('If-Modified-Since')\n\
          \        if if_modified and entry.last_modified:\n            # Parse and compare dates\n            return if_modified\
          \ == entry.last_modified\n\n        return False\n```\n"
      pitfalls:
      - 'Vary: * means never cache - handle this case'
      - 's-maxage vs max-age: CDN should use s-maxage'
      - 'ETag weak vs strong: weak allows semantic equivalence'
      - Cache key must include all Vary dimensions
    - name: Cache Invalidation
      description: Implement purge, ban, and tag-based invalidation
      skills:
      - Invalidation strategies
      - Purge propagation
      - Surrogate keys
      hints:
        level1: 'Purge: remove specific URL; Ban: remove by pattern'
        level2: 'Surrogate keys: tag content for group invalidation'
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport fnmatch\nimport time\n\
          import threading\n\n@dataclass\nclass SurrogateKey:\n    key: str\n    cache_keys: set[str]\n\nclass CacheInvalidator:\n\
          \    def __init__(self, cache: EdgeCache):\n        self.cache = cache\n        self.surrogate_keys: dict[str, set[str]]\
          \ = {}  # surrogate -> cache_keys\n        self.bans: list[tuple[str, float]] = []  # (pattern, timestamp)\n   \
          \     self.ban_lock = threading.Lock()\n\n    def purge(self, url: str) -> bool:\n        '''Purge specific URL\
          \ from cache'''\n        # Generate all possible cache keys for this URL\n        # (simplified - would need to\
          \ check all Vary combinations)\n        key = self.cache.generate_cache_key(url, {})\n\n        if key in self.cache.cache:\n\
          \            entry = self.cache.cache.pop(key)\n            self.cache.current_size -= len(entry.body)\n       \
          \     return True\n        return False\n\n    def purge_surrogate(self, surrogate_key: str) -> int:\n        '''Purge\
          \ all objects tagged with surrogate key'''\n        cache_keys = self.surrogate_keys.get(surrogate_key, set())\n\
          \        count = 0\n\n        for cache_key in list(cache_keys):\n            if cache_key in self.cache.cache:\n\
          \                entry = self.cache.cache.pop(cache_key)\n                self.cache.current_size -= len(entry.body)\n\
          \                count += 1\n\n        # Clear surrogate mapping\n        if surrogate_key in self.surrogate_keys:\n\
          \            del self.surrogate_keys[surrogate_key]\n\n        return count\n\n    def ban(self, pattern: str, ttl:\
          \ int = 3600):\n        '''Ban pattern - matching requests bypass cache'''\n        with self.ban_lock:\n      \
          \      expires = time.time() + ttl\n            self.bans.append((pattern, expires))\n\n            # Also immediately\
          \ remove matching entries\n            self.cache.invalidate(pattern)\n\n    def is_banned(self, url: str) -> bool:\n\
          \        '''Check if URL matches any active ban'''\n        now = time.time()\n\n        with self.ban_lock:\n \
          \           # Clean expired bans\n            self.bans = [(p, e) for p, e in self.bans if e > now]\n\n        \
          \    for pattern, expires in self.bans:\n                if fnmatch.fnmatch(url, pattern):\n                   \
          \ return True\n\n        return False\n\n    def register_surrogate(self, cache_key: str, surrogate_keys: list[str]):\n\
          \        '''Register cache entry with surrogate keys'''\n        for sk in surrogate_keys:\n            if sk not\
          \ in self.surrogate_keys:\n                self.surrogate_keys[sk] = set()\n            self.surrogate_keys[sk].add(cache_key)\n\
          \n    def parse_surrogate_header(self, header: str) -> list[str]:\n        '''Parse Surrogate-Key header'''\n  \
          \      if not header:\n            return []\n        return [k.strip() for k in header.split()]\n\nclass CDNNode:\n\
          \    def __init__(self, node_id: str, cache: EdgeCache,\n                 invalidator: CacheInvalidator):\n    \
          \    self.node_id = node_id\n        self.cache = cache\n        self.invalidator = invalidator\n        self.peers:\
          \ list['CDNNode'] = []\n\n    def propagate_purge(self, url: str):\n        '''Propagate purge to all peers'''\n\
          \        self.invalidator.purge(url)\n\n        for peer in self.peers:\n            # In production: async HTTP\
          \ call to peer\n            peer.invalidator.purge(url)\n\n    def propagate_ban(self, pattern: str):\n        '''Propagate\
          \ ban to all peers'''\n        self.invalidator.ban(pattern)\n\n        for peer in self.peers:\n            peer.invalidator.ban(pattern)\n\
          ```\n"
      pitfalls:
      - Surrogate keys enable efficient invalidation of related content
      - 'Propagation delay: clients may get stale content briefly'
      - Soft purge (stale-while-revalidate) better than hard purge
      - Bans can grow unbounded - implement TTL and cleanup
    - name: Origin Shield & Request Collapsing
      description: Implement origin shielding and request collapsing to reduce origin load
      skills:
      - Origin protection
      - Request deduplication
      - Thundering herd
      hints:
        level1: 'Origin shield: single edge fetches from origin, others fetch from it'
        level2: 'Request collapsing: one request to origin, all waiters get result'
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional, Callable\nimport threading\n\
          import time\nfrom concurrent.futures import Future\n\n@dataclass\nclass PendingRequest:\n    key: str\n    future:\
          \ Future\n    created_at: float\n\nclass RequestCollapser:\n    '''Collapse multiple requests for same resource\
          \ into one'''\n\n    def __init__(self, timeout: float = 30.0):\n        self.pending: dict[str, PendingRequest]\
          \ = {}\n        self.lock = threading.Lock()\n        self.timeout = timeout\n\n    def get_or_fetch(self, key:\
          \ str,\n                     fetch_fn: Callable[[], 'Response']) -> 'Response':\n        '''Get from pending request\
          \ or create new one'''\n\n        with self.lock:\n            # Check if request already pending\n            if\
          \ key in self.pending:\n                pending = self.pending[key]\n                # Wait for result\n       \
          \         return pending.future.result(timeout=self.timeout)\n\n            # Create new pending request\n     \
          \       future = Future()\n            self.pending[key] = PendingRequest(\n                key=key,\n         \
          \       future=future,\n                created_at=time.time()\n            )\n\n        # Fetch outside lock\n\
          \        try:\n            response = fetch_fn()\n            future.set_result(response)\n            return response\n\
          \n        except Exception as e:\n            future.set_exception(e)\n            raise\n\n        finally:\n \
          \           with self.lock:\n                if key in self.pending:\n                    del self.pending[key]\n\
          \nclass OriginShield:\n    '''Shield origin from direct edge requests'''\n\n    def __init__(self, shield_cache:\
          \ EdgeCache, origin_url: str):\n        self.cache = shield_cache\n        self.origin_url = origin_url\n      \
          \  self.collapser = RequestCollapser()\n        self.request_count = 0\n        self.origin_requests = 0\n\n   \
          \ def fetch(self, path: str, headers: dict) -> 'Response':\n        '''Fetch from shield cache or origin'''\n  \
          \      self.request_count += 1\n\n        cache_key = self.cache.generate_cache_key(path, {})\n\n        # Check\
          \ shield cache\n        entry = self.cache.get(cache_key)\n        if entry:\n            return Response(\n   \
          \             status_code=entry.status_code,\n                headers=entry.headers,\n                body=entry.body\n\
          \            )\n\n        # Fetch from origin with request collapsing\n        def fetch_origin():\n           \
          \ self.origin_requests += 1\n            import httpx\n            response = httpx.get(\n                f\"{self.origin_url}{path}\"\
          ,\n                headers=headers,\n                timeout=30.0\n            )\n            return Response(\n\
          \                status_code=response.status_code,\n                headers=dict(response.headers),\n          \
          \      body=response.content\n            )\n\n        response = self.collapser.get_or_fetch(cache_key, fetch_origin)\n\
          \n        # Cache response\n        self.cache.put(cache_key, response)\n\n        return response\n\n    def get_stats(self)\
          \ -> dict:\n        return {\n            'total_requests': self.request_count,\n            'origin_requests':\
          \ self.origin_requests,\n            'shield_hit_rate': 1 - (self.origin_requests / max(1, self.request_count))\n\
          \        }\n\nclass CDNEdge:\n    '''Edge node with shield support'''\n\n    def __init__(self, edge_id: str, local_cache:\
          \ EdgeCache,\n                 shield: OriginShield, invalidator: CacheInvalidator):\n        self.edge_id = edge_id\n\
          \        self.cache = local_cache\n        self.shield = shield\n        self.invalidator = invalidator\n      \
          \  self.collapser = RequestCollapser()\n\n    def handle_request(self, request: 'Request') -> 'Response':\n    \
          \    '''Handle incoming request'''\n        url = request.url\n\n        # Check bans\n        if self.invalidator.is_banned(url):\n\
          \            return self._fetch_from_shield(request)\n\n        # Generate cache key\n        cache_key = self.cache.generate_cache_key(url,\
          \ {})\n\n        # Check local cache\n        entry = self.cache.get(cache_key)\n        if entry:\n           \
          \ # Check conditional request\n            if self.cache.conditional_get(entry, request.headers):\n            \
          \    return Response(\n                    status_code=304,\n                    headers={'ETag': entry.etag},\n\
          \                    body=b''\n                )\n\n            return Response(\n                status_code=entry.status_code,\n\
          \                headers=entry.headers,\n                body=entry.body\n            )\n\n        # Fetch from\
          \ shield (with collapsing)\n        def fetch():\n            return self.shield.fetch(url, request.headers)\n\n\
          \        response = self.collapser.get_or_fetch(cache_key, fetch)\n\n        # Cache locally\n        entry = self.cache.put(cache_key,\
          \ response)\n        if entry:\n            # Register surrogate keys\n            surrogate_header = response.headers.get('Surrogate-Key')\n\
          \            if surrogate_header:\n                keys = self.invalidator.parse_surrogate_header(surrogate_header)\n\
          \                self.invalidator.register_surrogate(cache_key, keys)\n\n        return response\n\n    def _fetch_from_shield(self,\
          \ request: 'Request') -> 'Response':\n        return self.shield.fetch(request.url, request.headers)\n```\n"
      pitfalls:
      - Request collapsing timeout must be shorter than client timeout
      - Shield adds latency but dramatically reduces origin load
      - 'Negative caching: cache 404s briefly to prevent origin storms'
      - 'Health checks: bypass shield when origin is unhealthy'
  load-testing-framework:
    name: Distributed Load Testing Framework
    description: Build a load testing tool like k6/Locust with distributed workers, realistic user simulation, and real-time
      metrics.
    why_expert: Performance testing prevents outages. Understanding load generation, metrics collection, and bottleneck analysis
      is crucial for production systems.
    difficulty: expert
    tags:
    - testing
    - performance
    - load-testing
    - distributed
    - metrics
    estimated_hours: 45
    prerequisites:
    - build-http-server
    milestones:
    - name: Virtual User Simulation
      description: Implement virtual users with realistic think times and behavior
      skills:
      - User simulation
      - HTTP client
      - Think times
      hints:
        level1: Virtual user executes scenario repeatedly with configurable think times
        level2: 'Simulate realistic patterns: login, browse, action, logout'
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional\nfrom enum import\
          \ Enum\nimport asyncio\nimport aiohttp\nimport random\nimport time\n\n@dataclass\nclass RequestMetrics:\n    method:\
          \ str\n    url: str\n    status_code: int\n    response_time_ms: float\n    response_size: int\n    error: Optional[str]\
          \ = None\n    timestamp: float = field(default_factory=time.time)\n\nclass LoadProfile(Enum):\n    CONSTANT = \"\
          constant\"      # Fixed number of VUs\n    RAMP_UP = \"ramp_up\"        # Gradually increase\n    STEP = \"step\"\
          \              # Step increases\n    SPIKE = \"spike\"            # Sudden spike\n\n@dataclass\nclass StageConfig:\n\
          \    duration: int      # Seconds\n    target_vus: int    # Target virtual users\n    think_time: tuple[float, float]\
          \ = (1.0, 3.0)  # Min, max think time\n\nclass VirtualUser:\n    def __init__(self, user_id: int, scenario: 'Scenario',\n\
          \                 metrics_callback: Callable[[RequestMetrics], None]):\n        self.user_id = user_id\n       \
          \ self.scenario = scenario\n        self.metrics_callback = metrics_callback\n        self.session: Optional[aiohttp.ClientSession]\
          \ = None\n        self.cookies = {}\n        self.running = False\n\n    async def start(self):\n        self.running\
          \ = True\n        self.session = aiohttp.ClientSession()\n\n        try:\n            while self.running:\n    \
          \            await self._run_iteration()\n        finally:\n            await self.session.close()\n\n    async\
          \ def stop(self):\n        self.running = False\n\n    async def _run_iteration(self):\n        '''Run one complete\
          \ scenario iteration'''\n        for step in self.scenario.steps:\n            if not self.running:\n          \
          \      break\n\n            # Execute step\n            metrics = await self._execute_step(step)\n            self.metrics_callback(metrics)\n\
          \n            # Think time\n            if step.think_time:\n                await asyncio.sleep(random.uniform(*step.think_time))\n\
          \n    async def _execute_step(self, step: 'ScenarioStep') -> RequestMetrics:\n        start_time = time.time()\n\
          \n        try:\n            async with self.session.request(\n                method=step.method,\n            \
          \    url=step.url,\n                headers=step.headers,\n                json=step.body if step.body else None,\n\
          \                timeout=aiohttp.ClientTimeout(total=30)\n            ) as response:\n                body = await\
          \ response.read()\n\n                return RequestMetrics(\n                    method=step.method,\n         \
          \           url=step.url,\n                    status_code=response.status,\n                    response_time_ms=(time.time()\
          \ - start_time) * 1000,\n                    response_size=len(body)\n                )\n\n        except Exception\
          \ as e:\n            return RequestMetrics(\n                method=step.method,\n                url=step.url,\n\
          \                status_code=0,\n                response_time_ms=(time.time() - start_time) * 1000,\n         \
          \       response_size=0,\n                error=str(e)\n            )\n\n@dataclass\nclass ScenarioStep:\n    name:\
          \ str\n    method: str\n    url: str\n    headers: dict = field(default_factory=dict)\n    body: Optional[dict]\
          \ = None\n    think_time: Optional[tuple[float, float]] = None\n    check: Optional[Callable] = None  # Response\
          \ validation\n\n@dataclass\nclass Scenario:\n    name: str\n    steps: list[ScenarioStep]\n    weight: int = 1 \
          \ # For weighted random selection\n\nclass LoadRunner:\n    def __init__(self, base_url: str):\n        self.base_url\
          \ = base_url\n        self.scenarios: list[Scenario] = []\n        self.stages: list[StageConfig] = []\n       \
          \ self.virtual_users: list[VirtualUser] = []\n        self.metrics: list[RequestMetrics] = []\n        self.running\
          \ = False\n\n    def add_scenario(self, scenario: Scenario):\n        self.scenarios.append(scenario)\n\n    def\
          \ add_stage(self, duration: int, target_vus: int, **kwargs):\n        self.stages.append(StageConfig(\n        \
          \    duration=duration,\n            target_vus=target_vus,\n            **kwargs\n        ))\n\n    def collect_metric(self,\
          \ metric: RequestMetrics):\n        self.metrics.append(metric)\n\n    async def run(self):\n        self.running\
          \ = True\n        start_time = time.time()\n\n        for stage in self.stages:\n            stage_start = time.time()\n\
          \n            # Adjust VU count\n            await self._scale_to(stage.target_vus)\n\n            # Wait for stage\
          \ duration\n            while time.time() - stage_start < stage.duration:\n                if not self.running:\n\
          \                    break\n                await asyncio.sleep(0.1)\n\n        # Cleanup\n        await self._scale_to(0)\n\
          \        self.running = False\n\n    async def _scale_to(self, target: int):\n        current = len(self.virtual_users)\n\
          \n        if target > current:\n            # Add VUs\n            for i in range(current, target):\n          \
          \      scenario = random.choice(self.scenarios)  # Could weight\n                vu = VirtualUser(i, scenario, self.collect_metric)\n\
          \                self.virtual_users.append(vu)\n                asyncio.create_task(vu.start())\n\n        elif\
          \ target < current:\n            # Remove VUs\n            for vu in self.virtual_users[target:]:\n            \
          \    await vu.stop()\n            self.virtual_users = self.virtual_users[:target]\n```\n"
      pitfalls:
      - Think time prevents unrealistic load - real users pause between actions
      - Connection pooling affects results - configure properly
      - 'Coordinated omission: measure time from request creation, not send'
      - 'Virtual user state: some tests need session/cookie persistence'
    - name: Distributed Workers
      description: Implement distributed load generation with coordinator and workers
      skills:
      - Distributed coordination
      - Worker management
      - Aggregation
      hints:
        level1: Coordinator divides work; workers generate load and report metrics
        level2: Workers heartbeat to coordinator; auto-rebalance on failure
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport asyncio\nimport\
          \ json\nimport time\nfrom enum import Enum\n\nclass WorkerStatus(Enum):\n    IDLE = \"idle\"\n    RUNNING = \"running\"\
          \n    STOPPING = \"stopping\"\n    ERROR = \"error\"\n\n@dataclass\nclass Worker:\n    id: str\n    host: str\n\
          \    port: int\n    status: WorkerStatus\n    current_vus: int = 0\n    last_heartbeat: float = 0\n    metrics_count:\
          \ int = 0\n\n@dataclass\nclass TestConfig:\n    scenarios: list[dict]\n    stages: list[dict]\n    base_url: str\n\
          \nclass Coordinator:\n    def __init__(self, port: int = 5000):\n        self.port = port\n        self.workers:\
          \ dict[str, Worker] = {}\n        self.test_config: Optional[TestConfig] = None\n        self.running = False\n\
          \        self.aggregated_metrics: list[RequestMetrics] = []\n\n    async def register_worker(self, worker_id: str,\
          \ host: str, port: int) -> Worker:\n        worker = Worker(\n            id=worker_id,\n            host=host,\n\
          \            port=port,\n            status=WorkerStatus.IDLE,\n            last_heartbeat=time.time()\n       \
          \ )\n        self.workers[worker_id] = worker\n        return worker\n\n    async def start_test(self, config: TestConfig):\n\
          \        self.test_config = config\n        self.running = True\n\n        # Divide stages among workers\n     \
          \   worker_count = len(self.workers)\n        if worker_count == 0:\n            raise ValueError(\"No workers available\"\
          )\n\n        for stage in config.stages:\n            # Distribute VUs evenly\n            vus_per_worker = stage['target_vus']\
          \ // worker_count\n            remainder = stage['target_vus'] % worker_count\n\n            for i, worker in enumerate(self.workers.values()):\n\
          \                worker_vus = vus_per_worker + (1 if i < remainder else 0)\n\n                await self._send_to_worker(worker,\
          \ {\n                    'command': 'run_stage',\n                    'duration': stage['duration'],\n         \
          \           'target_vus': worker_vus,\n                    'scenarios': config.scenarios,\n                    'base_url':\
          \ config.base_url\n                })\n\n            # Wait for stage\n            await asyncio.sleep(stage['duration'])\n\
          \n        await self.stop_test()\n\n    async def stop_test(self):\n        self.running = False\n        for worker\
          \ in self.workers.values():\n            await self._send_to_worker(worker, {'command': 'stop'})\n\n    async def\
          \ _send_to_worker(self, worker: Worker, message: dict):\n        import aiohttp\n        async with aiohttp.ClientSession()\
          \ as session:\n            await session.post(\n                f\"http://{worker.host}:{worker.port}/command\"\
          ,\n                json=message\n            )\n\n    async def handle_heartbeat(self, worker_id: str, metrics:\
          \ list[dict]):\n        if worker_id in self.workers:\n            self.workers[worker_id].last_heartbeat = time.time()\n\
          \            self.workers[worker_id].metrics_count += len(metrics)\n\n            # Aggregate metrics\n        \
          \    for m in metrics:\n                self.aggregated_metrics.append(RequestMetrics(**m))\n\n    async def check_worker_health(self):\n\
          \        now = time.time()\n        for worker in list(self.workers.values()):\n            if now - worker.last_heartbeat\
          \ > 30:\n                worker.status = WorkerStatus.ERROR\n                # Redistribute load\n             \
          \   await self._rebalance()\n\n    async def _rebalance(self):\n        healthy = [w for w in self.workers.values()\n\
          \                   if w.status != WorkerStatus.ERROR]\n        if not healthy or not self.running:\n          \
          \  return\n\n        # Recalculate VU distribution\n        # ... (similar to start_test)\n\nclass WorkerNode:\n\
          \    def __init__(self, worker_id: str, coordinator_url: str, port: int = 5001):\n        self.worker_id = worker_id\n\
          \        self.coordinator_url = coordinator_url\n        self.port = port\n        self.load_runner: Optional[LoadRunner]\
          \ = None\n        self.pending_metrics: list[RequestMetrics] = []\n\n    async def run(self):\n        # Start web\
          \ server for commands\n        from aiohttp import web\n\n        app = web.Application()\n        app.router.add_post('/command',\
          \ self.handle_command)\n\n        # Start heartbeat task\n        asyncio.create_task(self._heartbeat_loop())\n\n\
          \        runner = web.AppRunner(app)\n        await runner.setup()\n        site = web.TCPSite(runner, '0.0.0.0',\
          \ self.port)\n        await site.start()\n\n    async def handle_command(self, request):\n        data = await request.json()\n\
          \        command = data.get('command')\n\n        if command == 'run_stage':\n            await self._run_stage(data)\n\
          \        elif command == 'stop':\n            if self.load_runner:\n                self.load_runner.running = False\n\
          \n        return web.Response(text='ok')\n\n    async def _run_stage(self, config: dict):\n        self.load_runner\
          \ = LoadRunner(config['base_url'])\n\n        # Add scenarios\n        for s in config['scenarios']:\n         \
          \   steps = [ScenarioStep(**step) for step in s['steps']]\n            self.load_runner.add_scenario(Scenario(\n\
          \                name=s['name'],\n                steps=steps\n            ))\n\n        # Collect metrics\n   \
          \     self.load_runner.metrics_callback = lambda m: self.pending_metrics.append(m)\n\n        # Add stage\n    \
          \    self.load_runner.add_stage(\n            duration=config['duration'],\n            target_vus=config['target_vus']\n\
          \        )\n\n        await self.load_runner.run()\n\n    async def _heartbeat_loop(self):\n        import aiohttp\n\
          \n        while True:\n            await asyncio.sleep(5)\n\n            # Send pending metrics\n            metrics_to_send\
          \ = self.pending_metrics[:100]\n            self.pending_metrics = self.pending_metrics[100:]\n\n            try:\n\
          \                async with aiohttp.ClientSession() as session:\n                    await session.post(\n     \
          \                   f\"{self.coordinator_url}/heartbeat\",\n                        json={\n                   \
          \         'worker_id': self.worker_id,\n                            'metrics': [m.__dict__ for m in metrics_to_send]\n\
          \                        }\n                    )\n            except:\n                pass  # Coordinator unavailable\n\
          ```\n"
      pitfalls:
      - Network latency between workers adds to reported response time
      - 'Time synchronization: use relative times or sync clocks'
      - 'Worker failure mid-test: decide whether to continue or abort'
      - Metric aggregation can create memory pressure
    - name: Real-time Metrics & Reporting
      description: Implement live metrics dashboard with percentiles and analysis
      skills:
      - Percentile calculation
      - Time series
      - Streaming aggregation
      hints:
        level1: Track p50, p95, p99 latency, not just average
        level2: Use t-digest or HDR histogram for accurate percentiles
        level3: "\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport math\nimport time\nfrom\
          \ collections import defaultdict\n\nclass HDRHistogram:\n    '''High Dynamic Range Histogram for accurate percentiles'''\n\
          \n    def __init__(self, lowest: int = 1, highest: int = 3600000,\n                 significant_figures: int = 3):\n\
          \        self.lowest = lowest\n        self.highest = highest\n        self.significant_figures = significant_figures\n\
          \n        # Calculate bucket count\n        self.bucket_count = self._calculate_bucket_count()\n        self.counts\
          \ = [0] * self.bucket_count\n        self.total_count = 0\n\n    def _calculate_bucket_count(self) -> int:\n   \
          \     # Simplified - real implementation is more complex\n        return int(math.log2(self.highest / self.lowest)\
          \ * 1000)\n\n    def _get_bucket(self, value: float) -> int:\n        if value < self.lowest:\n            return\
          \ 0\n        if value > self.highest:\n            return self.bucket_count - 1\n        return int(math.log2(value\
          \ / self.lowest) * 100)\n\n    def record(self, value: float):\n        bucket = self._get_bucket(value)\n     \
          \   self.counts[bucket] += 1\n        self.total_count += 1\n\n    def percentile(self, p: float) -> float:\n  \
          \      if self.total_count == 0:\n            return 0\n\n        target_count = int(self.total_count * p / 100)\n\
          \        count_so_far = 0\n\n        for bucket, count in enumerate(self.counts):\n            count_so_far += count\n\
          \            if count_so_far >= target_count:\n                # Convert bucket back to value\n                return\
          \ self.lowest * (2 ** (bucket / 100))\n\n        return self.highest\n\n@dataclass\nclass MetricsSummary:\n    count:\
          \ int\n    error_count: int\n    error_rate: float\n    min_ms: float\n    max_ms: float\n    mean_ms: float\n \
          \   p50_ms: float\n    p95_ms: float\n    p99_ms: float\n    requests_per_second: float\n\nclass MetricsAggregator:\n\
          \    def __init__(self, window_size: int = 10):\n        self.window_size = window_size  # Seconds\n        self.histograms:\
          \ dict[str, HDRHistogram] = {}  # Per endpoint\n        self.global_histogram = HDRHistogram()\n\n        self.window_metrics:\
          \ list[RequestMetrics] = []\n        self.window_start: float = time.time()\n\n        # Counters\n        self.total_requests\
          \ = 0\n        self.total_errors = 0\n        self.endpoint_counts: dict[str, int] = defaultdict(int)\n\n    def\
          \ record(self, metric: RequestMetrics):\n        self.total_requests += 1\n\n        if metric.error or metric.status_code\
          \ >= 400:\n            self.total_errors += 1\n\n        # Record to histograms\n        self.global_histogram.record(metric.response_time_ms)\n\
          \n        endpoint = f\"{metric.method} {metric.url}\"\n        if endpoint not in self.histograms:\n          \
          \  self.histograms[endpoint] = HDRHistogram()\n        self.histograms[endpoint].record(metric.response_time_ms)\n\
          \        self.endpoint_counts[endpoint] += 1\n\n        # Window metrics\n        self.window_metrics.append(metric)\n\
          \        self._cleanup_window()\n\n    def _cleanup_window(self):\n        now = time.time()\n        cutoff = now\
          \ - self.window_size\n        self.window_metrics = [\n            m for m in self.window_metrics\n            if\
          \ m.timestamp > cutoff\n        ]\n\n    def get_summary(self) -> MetricsSummary:\n        if self.total_requests\
          \ == 0:\n            return MetricsSummary(\n                count=0, error_count=0, error_rate=0,\n           \
          \     min_ms=0, max_ms=0, mean_ms=0,\n                p50_ms=0, p95_ms=0, p99_ms=0,\n                requests_per_second=0\n\
          \            )\n\n        response_times = [m.response_time_ms for m in self.window_metrics]\n\n        return MetricsSummary(\n\
          \            count=self.total_requests,\n            error_count=self.total_errors,\n            error_rate=self.total_errors\
          \ / self.total_requests,\n            min_ms=min(response_times) if response_times else 0,\n            max_ms=max(response_times)\
          \ if response_times else 0,\n            mean_ms=sum(response_times) / len(response_times) if response_times else\
          \ 0,\n            p50_ms=self.global_histogram.percentile(50),\n            p95_ms=self.global_histogram.percentile(95),\n\
          \            p99_ms=self.global_histogram.percentile(99),\n            requests_per_second=len(self.window_metrics)\
          \ / self.window_size\n        )\n\n    def get_endpoint_summary(self, endpoint: str) -> Optional[MetricsSummary]:\n\
          \        if endpoint not in self.histograms:\n            return None\n\n        hist = self.histograms[endpoint]\n\
          \        count = self.endpoint_counts[endpoint]\n\n        return MetricsSummary(\n            count=count,\n  \
          \          error_count=0,  # Would need separate tracking\n            error_rate=0,\n            min_ms=0,\n  \
          \          max_ms=0,\n            mean_ms=0,\n            p50_ms=hist.percentile(50),\n            p95_ms=hist.percentile(95),\n\
          \            p99_ms=hist.percentile(99),\n            requests_per_second=0\n        )\n\nclass ReportGenerator:\n\
          \    def __init__(self, aggregator: MetricsAggregator):\n        self.aggregator = aggregator\n\n    def generate_text_report(self)\
          \ -> str:\n        summary = self.aggregator.get_summary()\n\n        lines = [\n            \"Load Test Results\"\
          ,\n            \"=\" * 50,\n            f\"Total Requests:    {summary.count:,}\",\n            f\"Failed Requests:\
          \   {summary.error_count:,} ({summary.error_rate:.1%})\",\n            f\"Requests/sec:      {summary.requests_per_second:.1f}\"\
          ,\n            \"\",\n            \"Response Times (ms):\",\n            f\"  Min:    {summary.min_ms:.1f}\",\n\
          \            f\"  Mean:   {summary.mean_ms:.1f}\",\n            f\"  P50:    {summary.p50_ms:.1f}\",\n         \
          \   f\"  P95:    {summary.p95_ms:.1f}\",\n            f\"  P99:    {summary.p99_ms:.1f}\",\n            f\"  Max:\
          \    {summary.max_ms:.1f}\",\n        ]\n\n        return \"\\n\".join(lines)\n\n    def generate_json_report(self)\
          \ -> dict:\n        summary = self.aggregator.get_summary()\n        return {\n            'summary': {\n      \
          \          'total_requests': summary.count,\n                'error_count': summary.error_count,\n             \
          \   'error_rate': summary.error_rate,\n                'requests_per_second': summary.requests_per_second\n    \
          \        },\n            'latency': {\n                'min': summary.min_ms,\n                'mean': summary.mean_ms,\n\
          \                'p50': summary.p50_ms,\n                'p95': summary.p95_ms,\n                'p99': summary.p99_ms,\n\
          \                'max': summary.max_ms\n            },\n            'endpoints': {\n                endpoint: {\n\
          \                    'count': self.aggregator.endpoint_counts[endpoint],\n                    'p50': self.aggregator.histograms[endpoint].percentile(50),\n\
          \                    'p99': self.aggregator.histograms[endpoint].percentile(99)\n                }\n           \
          \     for endpoint in self.aggregator.histograms\n            }\n        }\n```\n"
      pitfalls:
      - Average hides outliers - always report percentiles
      - HDR histogram more accurate than naive quantile calculation
      - 'Streaming percentiles: use t-digest for memory efficiency'
      - Report generator should handle empty metrics gracefully
  chaos-engineering:
    name: Chaos Engineering Platform
    description: Build a chaos engineering tool to test system resilience through controlled failure injection.
    why_expert: Systems fail in production. Chaos engineering proactively finds weaknesses. Understanding failure modes helps
      build resilient systems.
    difficulty: expert
    tags:
    - chaos
    - reliability
    - testing
    - resilience
    - fault-injection
    estimated_hours: 50
    prerequisites:
    - build-http-server
    - container-runtime
    milestones:
    - name: Fault Injection Framework
      description: Implement fault injection primitives (latency, errors, resource exhaustion)
      skills:
      - Fault injection
      - Proxy interception
      - Resource limits
      hints:
        level1: Inject faults at network, process, or application level
        level2: Use proxy to inject latency/errors; cgroups for resource limits
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Callable\nfrom abc import\
          \ ABC, abstractmethod\nfrom enum import Enum\nimport random\nimport time\nimport asyncio\n\nclass FaultType(Enum):\n\
          \    LATENCY = \"latency\"\n    ERROR = \"error\"\n    PACKET_LOSS = \"packet_loss\"\n    CPU_STRESS = \"cpu_stress\"\
          \n    MEMORY_STRESS = \"memory_stress\"\n    DISK_STRESS = \"disk_stress\"\n    PROCESS_KILL = \"process_kill\"\n\
          \n@dataclass\nclass FaultConfig:\n    type: FaultType\n    target: str           # Service, pod, container\n   \
          \ duration: int         # Seconds\n    probability: float = 1.0  # 0-1, for partial faults\n    parameters: dict\
          \ = field(default_factory=dict)\n\nclass Fault(ABC):\n    def __init__(self, config: FaultConfig):\n        self.config\
          \ = config\n        self.active = False\n\n    @abstractmethod\n    async def inject(self):\n        pass\n\n  \
          \  @abstractmethod\n    async def rollback(self):\n        pass\n\nclass LatencyFault(Fault):\n    '''Inject network\
          \ latency'''\n\n    async def inject(self):\n        latency_ms = self.config.parameters.get('latency_ms', 500)\n\
          \        jitter_ms = self.config.parameters.get('jitter_ms', 100)\n\n        # Use tc (traffic control) to add latency\n\
          \        import subprocess\n        subprocess.run([\n            'tc', 'qdisc', 'add', 'dev', 'eth0', 'root', 'netem',\n\
          \            'delay', f'{latency_ms}ms', f'{jitter_ms}ms'\n        ], check=True)\n\n        self.active = True\n\
          \n    async def rollback(self):\n        import subprocess\n        subprocess.run([\n            'tc', 'qdisc',\
          \ 'del', 'dev', 'eth0', 'root'\n        ], check=False)\n        self.active = False\n\nclass ErrorFault(Fault):\n\
          \    '''Inject HTTP errors via proxy'''\n\n    def __init__(self, config: FaultConfig, proxy: 'FaultProxy'):\n \
          \       super().__init__(config)\n        self.proxy = proxy\n\n    async def inject(self):\n        status_code\
          \ = self.config.parameters.get('status_code', 500)\n        self.proxy.add_rule(FaultRule(\n            target=self.config.target,\n\
          \            probability=self.config.probability,\n            action='error',\n            status_code=status_code\n\
          \        ))\n        self.active = True\n\n    async def rollback(self):\n        self.proxy.remove_rule(self.config.target)\n\
          \        self.active = False\n\nclass CPUStressFault(Fault):\n    '''Stress CPU to simulate resource contention'''\n\
          \n    async def inject(self):\n        cores = self.config.parameters.get('cores', 1)\n        load_percent = self.config.parameters.get('load',\
          \ 80)\n\n        # Start stress workers\n        self._workers = []\n        for _ in range(cores):\n          \
          \  task = asyncio.create_task(self._stress_worker(load_percent))\n            self._workers.append(task)\n\n   \
          \     self.active = True\n\n    async def rollback(self):\n        for worker in self._workers:\n            worker.cancel()\n\
          \        self._workers = []\n        self.active = False\n\n    async def _stress_worker(self, load_percent: int):\n\
          \        while True:\n            # Busy loop for load_percent of time\n            busy_time = load_percent / 100\n\
          \            start = time.time()\n            while time.time() - start < busy_time:\n                _ = sum(i*i\
          \ for i in range(1000))\n\n            # Sleep for remainder\n            await asyncio.sleep(1 - busy_time)\n\n\
          class ProcessKillFault(Fault):\n    '''Kill process to test recovery'''\n\n    async def inject(self):\n       \
          \ process_name = self.config.parameters.get('process')\n        signal = self.config.parameters.get('signal', 'SIGKILL')\n\
          \n        import subprocess\n        subprocess.run(['pkill', f'-{signal}', process_name])\n        self.active\
          \ = True\n\n    async def rollback(self):\n        # Process kill is permanent - can't rollback\n        # Could\
          \ restart process if we have that capability\n        self.active = False\n\n@dataclass\nclass FaultRule:\n    target:\
          \ str\n    probability: float\n    action: str\n    status_code: int = 500\n    latency_ms: int = 0\n\nclass FaultProxy:\n\
          \    '''HTTP proxy that injects faults'''\n\n    def __init__(self, port: int = 8080):\n        self.port = port\n\
          \        self.rules: dict[str, FaultRule] = {}\n\n    def add_rule(self, rule: FaultRule):\n        self.rules[rule.target]\
          \ = rule\n\n    def remove_rule(self, target: str):\n        if target in self.rules:\n            del self.rules[target]\n\
          \n    async def handle_request(self, request) -> 'Response':\n        # Check if any rule matches\n        for target,\
          \ rule in self.rules.items():\n            if target in request.url:\n                if random.random() < rule.probability:\n\
          \                    return self._apply_fault(request, rule)\n\n        # Forward to upstream\n        return await\
          \ self._forward(request)\n\n    def _apply_fault(self, request, rule: FaultRule) -> 'Response':\n        if rule.action\
          \ == 'error':\n            return Response(\n                status_code=rule.status_code,\n                body=b'Injected\
          \ fault'\n            )\n        elif rule.action == 'latency':\n            time.sleep(rule.latency_ms / 1000)\n\
          \            return self._forward(request)\n\n    async def _forward(self, request) -> 'Response':\n        import\
          \ aiohttp\n        async with aiohttp.ClientSession() as session:\n            async with session.request(\n   \
          \             method=request.method,\n                url=request.url.replace(f':{self.port}', ''),\n          \
          \      headers=request.headers,\n                data=request.body\n            ) as resp:\n                return\
          \ Response(\n                    status_code=resp.status,\n                    body=await resp.read()\n        \
          \        )\n```\n"
      pitfalls:
      - tc commands require root/CAP_NET_ADMIN
      - CPU stress can affect chaos tool itself - isolate
      - Process kill needs restart mechanism or test fails
      - Probability <1.0 creates intermittent failures (realistic)
    - name: Experiment Orchestration
      description: Implement experiment definition, scheduling, and safety controls
      skills:
      - Experiment design
      - Safety controls
      - Rollback
      hints:
        level1: 'Experiment: hypothesis, faults to inject, metrics to observe'
        level2: 'Safety: abort conditions, blast radius limits, automatic rollback'
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional\nfrom enum import\
          \ Enum\nimport asyncio\nimport time\n\nclass ExperimentStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"\
          running\"\n    COMPLETED = \"completed\"\n    ABORTED = \"aborted\"\n    FAILED = \"failed\"\n\n@dataclass\nclass\
          \ SteadyStateHypothesis:\n    name: str\n    probe: Callable[[], bool]  # Returns True if steady state\n    tolerance:\
          \ float = 0.95   # Acceptable success rate\n\n@dataclass\nclass AbortCondition:\n    name: str\n    check: Callable[[],\
          \ bool]  # Returns True if should abort\n    message: str\n\n@dataclass\nclass Experiment:\n    id: str\n    name:\
          \ str\n    description: str\n    hypothesis: SteadyStateHypothesis\n    faults: list[FaultConfig]\n    abort_conditions:\
          \ list[AbortCondition]\n    duration: int  # Seconds\n    blast_radius: float = 0.1  # Max % of instances affected\n\
          \n@dataclass\nclass ExperimentResult:\n    experiment_id: str\n    status: ExperimentStatus\n    started_at: float\n\
          \    ended_at: float\n    steady_state_before: bool\n    steady_state_after: bool\n    abort_reason: Optional[str]\
          \ = None\n    observations: list[dict] = field(default_factory=list)\n\nclass ExperimentRunner:\n    def __init__(self):\n\
          \        self.active_faults: list[Fault] = []\n        self.running = False\n\n    async def run_experiment(self,\
          \ experiment: Experiment) -> ExperimentResult:\n        result = ExperimentResult(\n            experiment_id=experiment.id,\n\
          \            status=ExperimentStatus.RUNNING,\n            started_at=time.time(),\n            ended_at=0,\n  \
          \          steady_state_before=False,\n            steady_state_after=False\n        )\n\n        try:\n       \
          \     # 1. Verify steady state before\n            result.steady_state_before = await self._check_steady_state(\n\
          \                experiment.hypothesis\n            )\n            if not result.steady_state_before:\n        \
          \        result.status = ExperimentStatus.FAILED\n                result.abort_reason = \"System not in steady state\
          \ before experiment\"\n                return result\n\n            # 2. Inject faults\n            self.running\
          \ = True\n            for fault_config in experiment.faults:\n                fault = self._create_fault(fault_config)\n\
          \                await fault.inject()\n                self.active_faults.append(fault)\n\n            # 3. Run\
          \ for duration while monitoring abort conditions\n            abort_reason = await self._monitor_experiment(\n \
          \               experiment.duration,\n                experiment.abort_conditions\n            )\n\n           \
          \ if abort_reason:\n                result.status = ExperimentStatus.ABORTED\n                result.abort_reason\
          \ = abort_reason\n            else:\n                # 4. Verify steady state after\n                result.steady_state_after\
          \ = await self._check_steady_state(\n                    experiment.hypothesis\n                )\n            \
          \    result.status = ExperimentStatus.COMPLETED\n\n        except Exception as e:\n            result.status = ExperimentStatus.FAILED\n\
          \            result.abort_reason = str(e)\n\n        finally:\n            # 5. Rollback all faults\n          \
          \  await self._rollback_all()\n            result.ended_at = time.time()\n            self.running = False\n\n \
          \       return result\n\n    async def _check_steady_state(self, hypothesis: SteadyStateHypothesis) -> bool:\n \
          \       # Run probe multiple times to check stability\n        successes = 0\n        attempts = 10\n\n        for\
          \ _ in range(attempts):\n            try:\n                if hypothesis.probe():\n                    successes\
          \ += 1\n            except:\n                pass\n            await asyncio.sleep(0.5)\n\n        return (successes\
          \ / attempts) >= hypothesis.tolerance\n\n    async def _monitor_experiment(self, duration: int,\n              \
          \                     abort_conditions: list[AbortCondition]) -> Optional[str]:\n        start = time.time()\n\n\
          \        while time.time() - start < duration:\n            # Check abort conditions\n            for condition\
          \ in abort_conditions:\n                try:\n                    if condition.check():\n                      \
          \  return condition.message\n                except:\n                    pass\n\n            await asyncio.sleep(1)\n\
          \n        return None\n\n    async def _rollback_all(self):\n        for fault in self.active_faults:\n        \
          \    try:\n                await fault.rollback()\n            except Exception as e:\n                print(f\"\
          Rollback failed: {e}\")\n\n        self.active_faults = []\n\n    def _create_fault(self, config: FaultConfig) ->\
          \ Fault:\n        fault_classes = {\n            FaultType.LATENCY: LatencyFault,\n            FaultType.CPU_STRESS:\
          \ CPUStressFault,\n            FaultType.PROCESS_KILL: ProcessKillFault,\n        }\n        return fault_classes[config.type](config)\n\
          \nclass SafetyController:\n    '''Global safety controls for chaos experiments'''\n\n    def __init__(self):\n \
          \       self.global_abort = False\n        self.active_experiments: list[str] = []\n        self.max_concurrent\
          \ = 1\n        self.allowed_hours: tuple[int, int] = (9, 17)  # 9 AM - 5 PM\n\n    def can_start_experiment(self,\
          \ experiment: Experiment) -> tuple[bool, str]:\n        if self.global_abort:\n            return False, \"Global\
          \ abort is active\"\n\n        if len(self.active_experiments) >= self.max_concurrent:\n            return False,\
          \ \"Too many concurrent experiments\"\n\n        # Check time window\n        hour = time.localtime().tm_hour\n\
          \        if not (self.allowed_hours[0] <= hour < self.allowed_hours[1]):\n            return False, f\"Outside allowed\
          \ hours {self.allowed_hours}\"\n\n        return True, \"\"\n\n    def emergency_stop(self):\n        '''Stop all\
          \ experiments immediately'''\n        self.global_abort = True\n        # In production: send signal to all experiment\
          \ runners\n```\n"
      pitfalls:
      - Always verify steady state BEFORE injecting faults
      - Automatic rollback is critical - manual cleanup is error-prone
      - Abort conditions should check error rates, not just availability
      - Run during business hours initially - easier to respond to issues
    - name: GameDay Automation
      description: Implement scheduled chaos experiments with runbooks and incident response
      skills:
      - GameDay planning
      - Runbooks
      - Incident response
      hints:
        level1: 'GameDay: scheduled chaos with observers and runbooks'
        level2: 'Automated runbooks: if X happens, do Y to recover'
        level3: "\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional\nfrom enum import\
          \ Enum\nimport asyncio\nimport time\nfrom datetime import datetime, timedelta\n\n@dataclass\nclass RunbookStep:\n\
          \    name: str\n    action: Callable\n    rollback: Optional[Callable] = None\n    timeout: int = 60\n\n@dataclass\n\
          class Runbook:\n    id: str\n    name: str\n    trigger_condition: Callable[[], bool]\n    steps: list[RunbookStep]\n\
          \    auto_execute: bool = False\n\nclass RunbookExecutor:\n    async def execute(self, runbook: Runbook) -> dict:\n\
          \        results = {'steps': [], 'success': True}\n        executed_steps = []\n\n        for step in runbook.steps:\n\
          \            try:\n                await asyncio.wait_for(\n                    asyncio.create_task(step.action()),\n\
          \                    timeout=step.timeout\n                )\n                results['steps'].append({\n      \
          \              'name': step.name,\n                    'status': 'success'\n                })\n               \
          \ executed_steps.append(step)\n\n            except Exception as e:\n                results['steps'].append({\n\
          \                    'name': step.name,\n                    'status': 'failed',\n                    'error': str(e)\n\
          \                })\n                results['success'] = False\n\n                # Rollback executed steps\n \
          \               await self._rollback(executed_steps)\n                break\n\n        return results\n\n    async\
          \ def _rollback(self, steps: list[RunbookStep]):\n        for step in reversed(steps):\n            if step.rollback:\n\
          \                try:\n                    await step.rollback()\n                except:\n                    pass\n\
          \n@dataclass\nclass GameDay:\n    id: str\n    name: str\n    description: str\n    scheduled_start: datetime\n\
          \    experiments: list[Experiment]\n    runbooks: list[Runbook]\n    observers: list[str]  # Email/Slack of observers\n\
          \    duration_hours: int = 2\n\nclass GameDayCoordinator:\n    def __init__(self, experiment_runner: ExperimentRunner,\n\
          \                 runbook_executor: RunbookExecutor,\n                 notifier: 'Notifier'):\n        self.runner\
          \ = experiment_runner\n        self.runbook_exec = runbook_executor\n        self.notifier = notifier\n        self.scheduled_gamedays:\
          \ list[GameDay] = []\n\n    async def schedule_gameday(self, gameday: GameDay):\n        self.scheduled_gamedays.append(gameday)\n\
          \n        # Notify observers\n        await self.notifier.notify(\n            gameday.observers,\n            f\"\
          GameDay '{gameday.name}' scheduled for {gameday.scheduled_start}\"\n        )\n\n    async def run_gameday(self,\
          \ gameday: GameDay):\n        await self.notifier.notify(\n            gameday.observers,\n            f\"GameDay\
          \ '{gameday.name}' starting now!\"\n        )\n\n        results = {\n            'gameday_id': gameday.id,\n  \
          \          'experiments': [],\n            'runbooks_triggered': []\n        }\n\n        for experiment in gameday.experiments:\n\
          \            # Run experiment\n            exp_result = await self.runner.run_experiment(experiment)\n         \
          \   results['experiments'].append({\n                'id': experiment.id,\n                'status': exp_result.status.value,\n\
          \                'steady_state_maintained': exp_result.steady_state_after\n            })\n\n            # Check\
          \ if any runbooks should trigger\n            for runbook in gameday.runbooks:\n                if runbook.trigger_condition():\n\
          \                    if runbook.auto_execute:\n                        rb_result = await self.runbook_exec.execute(runbook)\n\
          \                        results['runbooks_triggered'].append({\n                            'runbook': runbook.id,\n\
          \                            'result': rb_result\n                        })\n                    else:\n      \
          \                  await self.notifier.notify(\n                            gameday.observers,\n               \
          \             f\"Runbook '{runbook.name}' should be executed manually\"\n                        )\n\n         \
          \   # Brief pause between experiments\n            await asyncio.sleep(60)\n\n        await self.notifier.notify(\n\
          \            gameday.observers,\n            f\"GameDay '{gameday.name}' completed. Results: {results}\"\n     \
          \   )\n\n        return results\n\n@dataclass\nclass Observation:\n    timestamp: float\n    observer: str\n   \
          \ type: str  # 'issue', 'note', 'recovery'\n    description: str\n    metrics: dict = field(default_factory=dict)\n\
          \nclass GameDayRecorder:\n    '''Record observations during GameDay'''\n\n    def __init__(self, gameday_id: str):\n\
          \        self.gameday_id = gameday_id\n        self.observations: list[Observation] = []\n        self.timeline:\
          \ list[dict] = []\n\n    def record_observation(self, observer: str, obs_type: str,\n                          \
          \ description: str, metrics: dict = None):\n        obs = Observation(\n            timestamp=time.time(),\n   \
          \         observer=observer,\n            type=obs_type,\n            description=description,\n            metrics=metrics\
          \ or {}\n        )\n        self.observations.append(obs)\n\n    def record_event(self, event_type: str, details:\
          \ dict):\n        self.timeline.append({\n            'timestamp': time.time(),\n            'event': event_type,\n\
          \            'details': details\n        })\n\n    def generate_report(self) -> dict:\n        return {\n      \
          \      'gameday_id': self.gameday_id,\n            'timeline': self.timeline,\n            'observations': [\n \
          \               {\n                    'time': obs.timestamp,\n                    'observer': obs.observer,\n \
          \                   'type': obs.type,\n                    'description': obs.description\n                }\n \
          \               for obs in self.observations\n            ],\n            'issues_found': [\n                obs\
          \ for obs in self.observations\n                if obs.type == 'issue'\n            ],\n            'recommendations':\
          \ self._generate_recommendations()\n        }\n\n    def _generate_recommendations(self) -> list[str]:\n       \
          \ recommendations = []\n\n        # Analyze observations for patterns\n        issues = [o for o in self.observations\
          \ if o.type == 'issue']\n        for issue in issues:\n            if 'timeout' in issue.description.lower():\n\
          \                recommendations.append(\"Consider implementing circuit breakers\")\n            if 'memory' in\
          \ issue.description.lower():\n                recommendations.append(\"Review memory limits and implement backpressure\"\
          )\n\n        return recommendations\n```\n"
      pitfalls:
      - GameDays need preparation - brief observers on experiments
      - Manual runbook steps may be needed for complex recovery
      - Record everything - observations are valuable learning
      - Schedule GameDays when team is available to respond
  time-series-db:
    name: Time-Series Database
    description: Build a specialized database optimized for time-stamped data with compression, downsampling, and efficient
      range queries - essential for metrics, IoT, and financial data
    category: Data Storage
    difficulty: advanced
    estimated_hours: 180
    skills:
    - Time-series data modeling
    - Columnar storage
    - Delta encoding and compression
    - Write-ahead logging
    - Retention policies
    - Continuous queries
    prerequisites:
    - database-engine
    - distributed-cache
    learning_outcomes:
    - Understand time-series data characteristics and access patterns
    - Implement columnar storage with compression algorithms
    - Build efficient time-range query execution
    - Design downsampling and aggregation pipelines
    - Create retention and compaction policies
    milestones:
    - name: Storage Engine
      description: Time-series optimized storage with compression
      skills:
      - Columnar storage
      - Delta encoding
      - Run-length encoding
      deliverables:
      - Time-structured merge tree (TSM) implementation
      - Delta-of-delta timestamp compression
      - Gorilla float compression algorithm
      - Dictionary encoding for tags/labels
      - Block-based storage with index
      - Memory-mapped file access
    - name: Write Path
      description: High-throughput ingestion with buffering
      skills:
      - Write batching
      - WAL
      - Memory management
      deliverables:
      - Write-ahead log for durability
      - In-memory buffer (memtable) for writes
      - Batch point ingestion API
      - Out-of-order write handling
      - Series cardinality tracking
      - Backpressure mechanisms
    - name: Query Engine
      description: Efficient time-range queries and aggregations
      skills:
      - Query planning
      - Aggregations
      - Downsampling
      deliverables:
      - Time-range predicate pushdown
      - Tag-based filtering and indexing
      - Built-in aggregation functions (sum, avg, min, max, count)
      - Windowed aggregations (tumbling, sliding)
      - GROUP BY time buckets
      - Last/first value queries optimization
    - name: Retention & Compaction
      description: Automatic data lifecycle management
      skills:
      - Retention policies
      - Compaction
      - Downsampling
      deliverables:
      - TTL-based retention policies
      - Automatic data expiration and deletion
      - Background compaction process
      - Level-based compaction strategy
      - Continuous downsampling queries
      - Rollup aggregation storage
    - name: Query Language & API
      description: Expressive query interface for time-series
      skills:
      - Query parsing
      - API design
      - PromQL/InfluxQL
      deliverables:
      - SQL-like query language with time extensions
      - Flux-style functional query pipeline
      - HTTP write API (Line Protocol compatible)
      - Query API with multiple output formats
      - Prometheus remote read/write API
      - Grafana data source compatibility
  graph-db:
    name: Graph Database
    description: Build a graph database with native graph storage, traversal algorithms, and query language - fundamental
      for social networks, recommendations, and knowledge graphs
    category: Data Storage
    difficulty: advanced
    estimated_hours: 200
    skills:
    - Graph data modeling
    - Index-free adjacency
    - Graph traversal algorithms
    - Query optimization
    - Pattern matching
    - Graph partitioning
    prerequisites:
    - database-engine
    - btree-implementation
    learning_outcomes:
    - Understand graph storage models and their trade-offs
    - Implement index-free adjacency for O(1) traversals
    - Build graph traversal and pathfinding algorithms
    - Design a graph query language parser and executor
    - Create efficient pattern matching for subgraph queries
    milestones:
    - name: Graph Storage Engine
      description: Native graph storage with index-free adjacency
      skills:
      - Node/edge storage
      - Property storage
      - Adjacency lists
      deliverables:
      - Node store with fixed-size records
      - Relationship store with double-linked lists
      - Property store with dynamic records
      - Label and relationship type indexes
      - Index-free adjacency implementation
      - Transaction log and recovery
    - name: Graph Traversal
      description: Efficient graph exploration algorithms
      skills:
      - BFS/DFS
      - Pathfinding
      - Pattern matching
      deliverables:
      - Breadth-first and depth-first traversal
      - Shortest path (Dijkstra, A*)
      - All paths between nodes
      - Variable-length path patterns
      - Bidirectional search optimization
      - Traversal result streaming
    - name: Query Language (Cypher-like)
      description: Declarative graph query language
      skills:
      - Query parsing
      - AST
      - Pattern matching
      deliverables:
      - MATCH clause for pattern specification
      - WHERE clause for filtering
      - CREATE/MERGE for graph mutations
      - RETURN with aggregations
      - WITH clause for query chaining
      - OPTIONAL MATCH for outer joins
    - name: Query Optimization
      description: Cost-based query planning for graphs
      skills:
      - Query planning
      - Statistics
      - Join ordering
      deliverables:
      - Pattern matching query planner
      - Cardinality estimation for patterns
      - Join order optimization
      - Index selection for label scans
      - Eager vs lazy evaluation
      - Query plan caching
    - name: Graph Algorithms
      description: Built-in graph analytics algorithms
      skills:
      - Centrality
      - Community detection
      - Similarity
      deliverables:
      - PageRank algorithm
      - Betweenness/closeness centrality
      - Community detection (Louvain, Label Propagation)
      - Node similarity (Jaccard, Cosine)
      - Triangle counting and clustering coefficient
      - Streaming/approximate algorithms for scale
    - name: Full-text & Spatial
      description: Extended indexing capabilities
      skills:
      - Full-text search
      - Spatial indexing
      - Composite indexes
      deliverables:
      - Full-text index on node properties
      - Fuzzy matching and relevance scoring
      - Spatial index (R-tree) for location data
      - Distance and bounding box queries
      - Composite property indexes
      - Index-backed ORDER BY
