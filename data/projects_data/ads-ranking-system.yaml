id: ads-ranking-system
name: Ads Ranking System
description: >
  Build a real-time ad ranking system like those used by Google and Meta.
  Implement click-through rate prediction, auction mechanisms, and real-time
  bidding with sub-millisecond latency requirements.

difficulty: expert
estimated_hours: 70-90
domain: world-scale

essence: >
  Real-time ad selection through multi-stage ranking (retrieval -> filtering -> ranking),
  click-through rate prediction with massive-scale feature engineering, second-price
  auction mechanisms, and strict latency SLAs measured in milliseconds.

why_important: >
  Ads ranking powers the $500B+ digital advertising industry. Understanding real-time
  bidding, CTR prediction, and auction mechanics is valuable for ad tech engineers
  at $200K-400K+ at companies like Google, Meta, and TikTok.

learning_outcomes:
  - Implement CTR prediction model with feature engineering
  - Build multi-stage ranking pipeline (recall -> rank)
  - Implement second-price auction with quality score
  - Handle real-time feature serving at scale
  - Build negative feedback loops (ad fatigue, frequency capping)
  - Implement A/B testing framework for ranking experiments
  - Optimize for both revenue and user experience
  - Handle cold-start for new ads and users

skills:
  - CTR Prediction
  - Feature Engineering
  - Auction Mechanics
  - Real-Time Serving
  - Multi-Stage Ranking
  - A/B Testing
  - Revenue Optimization
  - Cold-Start Handling

tags:
  - expert
  - ads-ranking
  - ctr-prediction
  - real-time-bidding
  - auction
  - ad-tech

languages:
  recommended:
    - Python
    - Go
  also_possible:
    - Java
    - C++

resources:
  - name: "Deep Learning in Ads Ranking"
    url: https://arxiv.org/abs/2006.04481
    type: paper
  - name: "Practical Lessons from Predicting Clicks on Ads at Facebook"
    url: https://research.facebook.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/
    type: paper
  - name: "Wide & Deep Learning for Recommender Systems"
    url: https://arxiv.org/abs/1606.07792
    type: paper
  - name: "Google's ad auction mechanism"
    url: https://support.google.com/google-ads/answer/6299
    type: documentation

prerequisites:
  - type: skill
    name: Machine learning fundamentals
  - type: skill
    name: Python and PyTorch proficiency
  - type: skill
    name: Understanding of logistic regression, neural networks
  - type: project
    name: recommendation-engine or equivalent

milestones:
  - id: ads-ranking-m1
    name: CTR Prediction Model
    description: >
      Build a click-through rate prediction model with feature engineering
      for ads ranking.
    acceptance_criteria:
      - Feature engineering: user features, ad features, context features
      - Categorical features embedded, numerical features normalized
      - Model architecture: logistic regression baseline, then deep model
      - Cross-entropy loss for binary classification (click/no-click)
      - AUC and log loss evaluated on held-out test set
      - Model handles class imbalance (clicks are rare, ~1-5%)
      - Feature importance analysis identifies key signals
    pitfalls:
      - Data leakage from using post-click features
      - Class imbalance requires proper handling (weighting, sampling)
      - Feature cardinality explosion with one-hot encoding
      - Not handling cold-start (new users/ads) properly
    concepts:
      - CTR prediction
      - Feature engineering
      - Class imbalance
      - Model evaluation
    skills:
      - Feature engineering
      - CTR model training
      - Imbalance handling
      - Evaluation metrics
    deliverables:
      - CTR prediction model
      - Feature engineering pipeline
      - Evaluation on test set
      - Feature importance analysis
    estimated_hours: "14-18"

  - id: ads-ranking-m2
    name: Multi-Stage Ranking Pipeline
    description: >
      Build a multi-stage ranking pipeline that efficiently narrows down
      from millions of candidate ads to a few top choices.
    acceptance_criteria:
      - Stage 1 (retrieval): fast candidate generation from inverted index
      - Stage 2 (filtering): apply hard constraints (budget, targeting, policy)
      - Stage 3 (ranking): CTR model scores remaining candidates
      - Latency budget: retrieval < 5ms, ranking < 10ms, total < 20ms
      - Pipeline processes 1000+ requests per second
      - Candidates reduced by 1000x through pipeline stages
    pitfalls:
      - Retrieval too aggressive misses good candidates
      - Bottleneck in any stage breaks latency SLA
      - Not parallelizing stages wastes compute
      - Feature fetching becomes latency bottleneck
    concepts:
      - Funnel architecture
      - Latency budgeting
      - Candidate generation
      - Hard vs soft filtering
    skills:
      - Pipeline design
      - Latency optimization
      - Candidate retrieval
      - Parallel processing
    deliverables:
      - Multi-stage ranking pipeline
      - Retrieval system
      - Filtering rules
      - Latency benchmarks
    estimated_hours: "14-18"

  - id: ads-ranking-m3
    name: Auction Mechanism
    description: >
      Implement second-price auction with quality score to determine
      ad winners and pricing.
    acceptance_criteria:
      - Second-price auction: winner pays second-highest bid + $0.01
      - Quality score (QS) multiplies bid: effective bid = bid Ã— QS
      - QS based on CTR prediction, landing page quality, relevance
      - Auction resolves in < 1ms for 100s of bidders
      - Budget deduction after impression/click
      - Tie-breaking rules clearly defined and deterministic
    pitfalls:
      - First-price auction encourages bid shading
      - Quality score manipulation by advertisers
      - Budget exhaustion during auction causes re-auction
      - Not handling ties consistently
    concepts:
      - Second-price auction
      - Quality score
      - Effective bid calculation
      - Budget management
    skills:
      - Auction implementation
      - Quality scoring
      - Budget handling
      - Tie-breaking
    deliverables:
      - Second-price auction
      - Quality score calculator
      - Budget manager
      - Auction simulator
    estimated_hours: "10-14"

  - id: ads-ranking-m4
    name: Real-Time Feature Serving
    description: >
      Build a real-time feature serving system that provides user and
      context features at request time.
    acceptance_criteria:
      - Feature store serves features in < 5ms p99
      - User features: historical clicks, demographics, interests
      - Context features: time, location, device, page content
      - Real-time features updated within seconds of user action
      - Feature caching reduces database load
      - Handles feature misses gracefully with defaults
    pitfalls:
      - Feature store becomes bottleneck
      - Stale features hurt prediction accuracy
      - Cache invalidation complexity
      - Hot keys (popular users) cause load imbalance
    concepts:
      - Feature stores
      - Real-time serving
      - Caching strategies
      - Feature freshness
    skills:
      - Feature serving
      - Cache design
      - Latency optimization
      - Handling misses
    deliverables:
      - Feature serving system
      - Feature cache
      - Real-time updates
      - Latency benchmarks
    estimated_hours: "12-16"

  - id: ads-ranking-m5
    name: Frequency Capping & Negative Feedback
    description: >
      Implement frequency capping and negative feedback loops to
      prevent ad fatigue and improve user experience.
    acceptance_criteria:
      - Frequency capping limits ad views per user per time window
      - Configurable caps: X views per day, Y per week, Z per campaign
      - Negative signals: skips, hides, "not interested" feedback
      - Downweighting based on negative feedback
      - Counter resets after configurable time period
      - Distributed counter for scale (Redis, DynamoDB)
    pitfalls:
      - Over-capping reduces revenue opportunity
      - Under-capping causes user annoyance
      - Distributed counter accuracy vs latency tradeoff
      - Not accounting for cross-device users
    concepts:
      - Frequency capping
      - Negative feedback
      - User experience
      - Distributed counters
    skills:
      - Capping implementation
      - Feedback processing
      - Downweighting
      - Distributed state
    deliverables:
      - Frequency capping system
      - Negative feedback processing
      - Downweighting logic
      - Distributed counters
    estimated_hours: "10-14"

  - id: ads-ranking-m6
    name: A/B Testing & Experimentation
    description: >
      Build an A/B testing framework for ranking experiments with
      proper statistical analysis.
    acceptance_criteria:
      - Experiments assign users to control/treatment buckets
      - Consistent bucket assignment (same user -> same bucket)
      - Multiple concurrent experiments with orthogonal buckets
      - Metrics: CTR, CPC, revenue per mille (RPM)
      - Statistical significance testing (t-test, bootstrap)
      - Experiment dashboard shows results in real-time
    pitfalls:
      - Sample ratio mismatch indicates assignment bug
      - Simpson's paradox when aggregating across segments
      - Multiple comparison problem with many metrics
      - Not running experiments long enough for significance
    concepts:
      - A/B testing
      - Bucket assignment
      - Statistical significance
      - Experiment analysis
    skills:
      - Experiment framework
      - Bucket assignment
      - Statistical testing
      - Result interpretation
    deliverables:
      - A/B testing framework
      - Bucket assignment
      - Statistical analysis
      - Experiment dashboard
    estimated_hours: "10-12"
