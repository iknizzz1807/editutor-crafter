id: background-job-processor
name: Background Job Processor
description: Async task queue with reliable delivery, retries, scheduling, and monitoring. Uses Redis for durable job persistence with at-least-once delivery semantics.
difficulty: intermediate
estimated_hours: 55
essence: Durable message persistence in Redis using reliable dequeue patterns (RPOPLPUSH/BLMOVE or Redis Streams with consumer groups), worker pool concurrency management, and failure recovery through retry scheduling with exponential backoff. Provides at-least-once delivery semantics with idempotency key support for safe retries. Job ordering is preserved per-queue but not globally across concurrent workers.
why_important: Building this teaches you distributed systems fundamentals essential for scalable applications — from handling failure scenarios gracefully to understanding concurrency patterns that power production systems at companies like Stripe, GitHub, and Shopify.
learning_outcomes:
- Design reliable async job processing with at-least-once delivery semantics
- Implement exponential backoff and retry logic with jitter
- Handle job failures gracefully with dead letter queues and error tracking
- Build monitoring and observability for background jobs
- Understand trade-offs between job ordering, throughput, and concurrency
skills:
- Message queues
- Worker processes
- Job scheduling
- Retry strategies
- Concurrency control
- Job persistence
- Idempotency patterns
tags:
- data-structures
- devops
- intermediate
- queues
- retries
- scheduling
- workers
architecture_doc: architecture-docs/background-job-processor/index.md
languages:
  recommended:
  - Python
  - Go
  - Rust
  also_possible: []
resources:
- name: Celery Documentation
  url: https://docs.celeryq.dev/en/stable/getting-started/first-steps-with-celery.html
  type: documentation
- name: 'RQ: Simple Job Queues for Python'
  url: https://python-rq.org/
  type: documentation
- name: Redis Streams
  url: https://redis.io/docs/latest/develop/data-types/streams/
  type: documentation
- name: AWS Retry with Backoff Pattern
  url: https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/retry-backoff.html
  type: article
- name: Exponential Backoff in Distributed Systems
  url: https://betterstack.com/community/guides/monitoring/exponential-backoff/
  type: tutorial
- name: Redis RPOPLPUSH Pattern for Reliable Queues
  url: https://redis.io/docs/latest/commands/lmove/
  type: documentation
prerequisites:
- type: skill
  name: Redis basics
- type: skill
  name: Process management
milestones:
- id: background-job-processor-m1
  name: Job Queue Core with Reliable Dequeue
  description: Build the core job queue with durable enqueue/dequeue operations using Redis. Jobs must not be lost if a worker crashes after dequeue.
  estimated_hours: 12
  concepts:
  - Redis LMOVE (formerly RPOPLPUSH) for atomic move from processing queue to in-flight set
  - Alternative: Redis Streams with XREADGROUP and consumer groups for durable delivery
  - Job serialization with JSON or msgpack for cross-language compatibility
  - Connection pooling to Redis for efficient resource usage
  - At-least-once delivery semantics and why exactly-once is impossible in distributed systems
  skills:
  - Redis lists and streams
  - Job serialization
  - Reliable queue patterns
  - Atomic operations
  acceptance_criteria:
  - Jobs are enqueued with a JSON-serialized payload to a Redis list using LPUSH and dequeued using LMOVE (or BLMOVE) which atomically moves the job to a processing list, ensuring no data loss if the worker crashes after dequeue
  - Multiple named queues are supported with configurable priority weights; workers poll higher-priority queues more frequently using weighted random or strict priority ordering, and the polling ratio is configurable
  - Job serialization and deserialization round-trips correctly preserving all payload fields and types including nested objects, arrays, and numeric precision
  - Payloads larger than 1MB are rejected at enqueue time with a descriptive validation error including the actual size and the limit
  - Each job metadata key in Redis has a configurable TTL (default 7 days) to prevent unbounded memory growth
  - Idempotency keys can be optionally attached to jobs; enqueue rejects duplicate idempotency keys within a configurable window
  - Redis connection failures during enqueue raise a retriable exception with clear error classification (transient vs permanent)
  pitfalls:
  - Using plain RPOP/LPOP instead of LMOVE or Streams — data is lost if the worker crashes between dequeue and job completion
  - Forgetting to set TTL on job metadata keys causing unbounded memory growth over weeks
  - Using blocking operations (BRPOP) without timeout causing inability to check shutdown signals
  - Serializing non-serializable objects like file handles or database connections
  - Not validating idempotency key uniqueness, leading to duplicate job execution
  deliverables:
  - Job class with JSON serialization storing job type, arguments, metadata, idempotency key, and enqueue timestamp
  - Enqueue operation using LPUSH with atomic LMOVE-based dequeue (or Redis Streams XADD/XREADGROUP alternative)
  - Processing list (in-flight set) that tracks jobs currently being executed by workers
  - Multiple named queues with independent configurable priority levels
  - Job ID generation using ULIDs (time-sortable) to uniquely identify each job
  - Job payload validation rejecting malformed or oversized payloads before enqueuing
  - Queue inspection APIs returning queue length, peek at pending jobs, list queue names, and in-flight job count
  - Idempotency key checker that prevents duplicate enqueue within a configurable deduplication window
- id: background-job-processor-m2
  name: Worker Process
  description: Build a worker process that reliably dequeues and executes jobs, with concurrency control, heartbeats, and graceful shutdown.
  estimated_hours: 12
  concepts:
  - BLMOVE for blocking reliable dequeue with timeout
  - Worker heartbeat mechanism to detect crashed processes
  - Graceful shutdown with signal handling and in-flight job completion
  - Process forking vs threading for job isolation — threads share memory, processes isolate failures
  - Stale job recovery by scanning the processing list for jobs whose worker heartbeat has expired
  skills:
  - Process management
  - Signal handling
  - Concurrency (threads/processes)
  - Error handling
  acceptance_criteria:
  - Workers poll configured queues using BLMOVE (or XREADGROUP) with a configurable timeout (default 5s) and dispatch jobs to the correct handler based on job type
  - Configurable concurrency (thread pool or process pool) allows a single worker process to execute N jobs in parallel where N is configurable at startup
  - 'Graceful shutdown on SIGTERM: the worker stops accepting new jobs and waits up to a configurable timeout (default 30s) for in-flight jobs to complete before exiting'
  - Worker heartbeat is updated in Redis every 5 seconds (configurable); a separate reaper process or the worker itself detects stale workers (no heartbeat for 3x interval) and returns their in-flight jobs to the queue
  - Unhandled exceptions in job execution are caught, logged, and do not crash the worker process
  - Per-job execution timeout is enforced; jobs exceeding the timeout are terminated and moved to the retry or dead letter queue
  pitfalls:
  - Not catching exceptions in job handlers causing the entire worker to crash
  - Blocking indefinitely on queue reads without timeout — prevents checking shutdown signals and heartbeat updates
  - Failing to return in-flight jobs to the queue when a worker dies (stale job recovery is essential)
  - Running CPU-bound tasks in threads in Python (GIL) — use process pool for CPU-bound work
  - Not cleaning up the processing list entry after job completion, causing phantom in-flight jobs
  deliverables:
  - Worker main loop polling queues with BLMOVE (or XREADGROUP) with configurable timeout
  - Job handler registry mapping job type strings to handler functions/classes
  - Per-job timeout enforcement that terminates long-running jobs
  - Graceful shutdown handler catching SIGTERM/SIGINT, draining in-flight jobs
  - Worker heartbeat writing liveness to Redis with expiring key
  - Stale job reaper that scans for expired worker heartbeats and re-enqueues their in-flight jobs
  - Concurrent job execution via configurable thread or process pool
  - Job completion callback that removes the job from the processing list and records result
- id: background-job-processor-m3
  name: Retry & Error Handling
  description: Implement automatic retries with exponential backoff, dead letter queue, and full error tracking.
  estimated_hours: 10
  concepts:
  - Exponential backoff with jitter: delay = min(base * 2^attempt + random_jitter, max_delay)
  - Dead letter queue for permanently failed jobs
  - Idempotency considerations for retried jobs (at-least-once means handlers should be idempotent)
  - Retry state tracking with attempt counter and error history
  skills:
  - Retry strategies
  - Dead letter queue management
  - Error tracking and forensics
  acceptance_criteria:
  - 'Failed jobs are retried with exponential backoff: delay = min(base * 2^attempt + uniform_random(0, jitter_max), max_delay) where base, jitter_max, and max_delay are configurable per job type with global defaults'
  - Jobs that exhaust their maximum retry count are moved to the dead letter queue with full error history (every attempt's exception class, message, backtrace, and timestamp)
  - 'Error details for each attempt are stored as an array on the job record: [{attempt: N, error_class, message, backtrace, timestamp}]'
  - Dead letter queue jobs can be manually retried (re-enqueued to original queue) or permanently deleted through a management API
  - Retry scheduling uses a Redis sorted set with the scheduled retry timestamp as score; a poller moves due jobs back to the work queue
  - Maximum retry count is configurable per job type (e.g., EmailJob max 5, PaymentJob max 10) with a global default (e.g., 3)
  pitfalls:
  - Not capping retry delay with a max_delay ceiling causing multi-hour waits
  - Using fixed retry delays that create thundering herd when many jobs fail simultaneously
  - Losing error context from previous attempts when retrying
  - Retrying non-idempotent operations (e.g., charging a credit card) without idempotency keys causes duplicate side effects
  - Not implementing jitter causing synchronized retry storms
  deliverables:
  - Exponential backoff calculator: delay = min(base * 2^attempt + random(0, jitter_max), max_delay)
  - Retry queue as Redis sorted set with scheduled execution timestamps as scores
  - Retry poller that moves due jobs from the sorted set back to the work queue
  - Dead letter queue storing permanently failed jobs with full error history
  - Error serialization capturing exception class, message, and backtrace per attempt
  - Per-job-type retry configuration (max_retries, base_delay, max_delay, jitter)
  - Custom retry strategy hooks allowing per-job-type override of backoff algorithm
  - Management API for dead letter queue (list, retry single, retry all, delete)
- id: background-job-processor-m4
  name: Scheduling & Cron
  description: Schedule jobs for future execution and define recurring jobs with cron-like syntax.
  estimated_hours: 10
  concepts:
  - Redis sorted sets for timestamp-based job scheduling (same mechanism as retry queue)
  - Cron expression parsing and next-execution-time calculation
  - Timezone handling and DST transitions — a 2: 30 AM job may not fire or fire twice during DST change
  - Leader election or distributed lock for the scheduler process to prevent duplicate cron enqueue
  skills:
  - Cron parsing
  - Scheduled job management
  - Timezone-aware time handling
  - Distributed locking
  acceptance_criteria:
  - Delayed jobs are stored in a Redis sorted set with their scheduled execution timestamp as score; a poller enqueues them when current_time >= score
  - Recurring jobs defined with standard 5-field cron expressions (minute, hour, day-of-month, month, day-of-week) are automatically re-enqueued after each execution
  - 'Unique job constraints prevent duplicate enqueue: for recurring jobs, a distributed lock or unique key ensures only one instance is enqueued per schedule period'
  - 'Scheduler crash recovery on restart: scans for overdue scheduled jobs (scheduled_time < now) and enqueues them immediately'
  - Cron expressions are evaluated in a configurable timezone (default UTC); DST transitions are handled correctly (jobs scheduled during a skipped hour are run at the next valid time, jobs during a repeated hour are run only once)
  - Only one scheduler process enqueues cron jobs at a time, enforced via a Redis distributed lock with TTL
  pitfalls:
  - 'Not handling DST transitions: 2:30 AM may not exist (spring forward) or exist twice (fall back)'
  - Running multiple scheduler processes without distributed locking causes duplicate cron enqueues
  - Missing scheduled jobs during downtime without catch-up logic
  - Cron parsing libraries that don't handle edge cases (day 31, Feb 29)
  - Using system local time instead of a consistent timezone reference
  deliverables:
  - Delayed job API accepting a job and a future timestamp, stored in Redis sorted set
  - Recurring job definitions: job class, arguments, cron expression, timezone
  - Cron expression parser supporting 5-field syntax with wildcards, ranges, steps, and lists
  - Scheduler poller process that checks for due jobs and enqueues them
  - Distributed lock for scheduler leader election preventing duplicate cron enqueue
  - Timezone-aware cron evaluation handling DST edge cases
  - Crash recovery logic that enqueues overdue jobs on scheduler startup
- id: background-job-processor-m5
  name: Monitoring & Dashboard
  description: Build real-time monitoring, metrics, and a web dashboard for operational visibility into the job processing system.
  estimated_hours: 11
  concepts:
  - WebSocket or SSE for pushing metric updates to the dashboard
  - Aggregating job metrics without blocking worker processes (use separate Redis keys/HyperLogLog)
  - Rate limiting dashboard API to prevent Redis overload from frequent polling
  - Key metrics: queue depth, processing rate (jobs/sec), error rate, p95 processing time
  skills:
  - Metrics collection
  - Web UI development
  - Real-time data streaming
  acceptance_criteria:
  - Dashboard displays queue depth, jobs processed/sec, error rate, and average processing time, updated at most every 2 seconds via polling or push
  - Web dashboard shows per-queue job counts (pending, active, completed, failed, dead), active workers with their current job and last heartbeat, and recent failures
  - Alerting triggers (logged warning or webhook) when queue backlog exceeds a configurable threshold or error rate exceeds a configurable percentage over a rolling window
  - Job search supports filtering by job ID, status (pending/active/completed/failed/dead), queue name, and enqueue time range with paginated results (max 100 per page)
  - Job history is retained for a configurable duration (default 7 days) with automatic cleanup of older records
  - Dashboard queries use a read-only Redis connection or separate analytics keys to avoid impacting worker performance
  pitfalls:
  - Storing all job history indefinitely causing unbounded memory growth — enforce TTL-based cleanup
  - Not implementing pagination for large job lists causing timeouts
  - Exposing sensitive job payload data (PII, credentials) in the monitoring interface
  - Dashboard polling too frequently overloading Redis — enforce minimum poll interval
  - High-cardinality metrics (per-job-ID) instead of aggregate metrics causing memory explosion
  deliverables:
  - Per-queue metrics: pending, active, completed, failed, dead job counts
  - Worker status tracker: active workers, current job, last heartbeat, uptime
  - Job history log with configurable retention (TTL-based cleanup)
  - Rolling error rate calculator per queue and per job type
  - Web dashboard UI with real-time updates (SSE or WebSocket)
  - Dead letter queue management UI: list, retry, delete
  - Job search API with filters: ID, status, queue, time range, with pagination
  - Alert configuration for queue depth and error rate thresholds
domain: app-dev
