id: benchmark-framework
name: Benchmark Framework
description: >
  Build a statistical benchmarking framework with warmup, multiple
  runs, outlier detection, and significance testing for reliable
  performance measurement.

difficulty: intermediate
estimated_hours: 15-25
domain: performance-engineering

essence: >
  Statistical methodology for reliable performance measurement including
  warmup phases to reach steady state, multiple runs for statistical
  significance, outlier detection and handling, confidence intervals,
  and comparative analysis with statistical tests.

why_important: >
  Benchmarking is deceptively hard. Understanding proper methodology
  prevents false conclusions from noisy measurements. Essential for
  any performance-sensitive development at $120K-200K+.

learning_outcomes:
  - Implement proper warmup methodology
  - Build statistical analysis for benchmark results
  - Detect and handle outliers appropriately
  - Calculate confidence intervals
  - Compare benchmark results with statistical tests
  - Handle JIT and caching effects
  - Report results with proper precision
  - Build regression detection for CI/CD

skills:
  - Statistical Benchmarking
  - Warmup Methodology
  - Outlier Detection
  - Confidence Intervals
  - Significance Testing
  - Regression Detection
  - CI/CD Integration
  - Result Reporting

tags:
  - benchmarking
  - intermediate
  - measurement
  - performance
  - statistics
  - testing

languages:
  recommended:
    - Rust
    - Go
    - Python
  also_possible:
    - JavaScript
    - Java

resources:
  - name: "How to Benchmark"
    url: https://dzone.com/articles/how-to-benchmark
    type: article
  - name: "Statistically Rigorous Java Benchmarking"
    url: https://www.ibm.com/developerworks/java/library/j-jtp02274/
    type: paper
  - name: "Criterion.rs Documentation"
    url: https://bheisler.github.io/criterion.rs/book/
    type: documentation
  - name: "JMH - Java Microbenchmark Harness"
    url: https://openjdk.org/projects/code-tools/jmh/
    type: documentation

prerequisites:
  - type: skill
    name: Basic statistics (mean, stddev, percentiles)
  - type: skill
    name: Programming language proficiency
  - type: skill
    name: Understanding of JIT/compilation effects

milestones:
  - id: benchmark-m1
    name: Core Benchmark Execution
    description: >
      Build the core benchmark execution engine with timing,
      iteration counting, and basic measurement.
    acceptance_criteria:
      - Benchmark function executed multiple iterations
      - High-resolution timing (nanosecond precision)
      - Configurable iteration count and duration
      - Prevents dead code elimination of benchmark work
      - Handles benchmark function returning error
      - Measures wall time and optionally CPU time
    pitfalls:
      - Dead code elimination removes benchmark work
      - Timer resolution affects short benchmark accuracy
      - Compiler optimizations change benchmark behavior
      - First runs slower due to cold caches/JIT
      - System clock changes affect wall time
    concepts:
      - High-resolution timing
      - Iteration control
      - Dead code prevention
      - Measurement types
    skills:
      - Timer APIs
      - Benchmark harness
      - Optimization barriers
      - Error handling
    deliverables:
      - Benchmark executor
      - High-resolution timer
      - Iteration controller
      - Dead code prevention
      - Time measurement
    estimated_hours: "4-5"

  - id: benchmark-m2
    name: Warmup & Steady State Detection
    description: >
      Implement warmup phases and automatic steady state detection
      to ensure reliable measurements.
    acceptance_criteria:
      - Configurable warmup iterations before measurement
      - Automatic warmup: run until variance stabilizes
      - Steady state detection using coefficient of variation
      - Separate warmup stats from measurement stats
      - JIT compilation triggered during warmup
      - Cache warming effect handled
    pitfalls:
      - Too few warmup iterations leaves JIT incomplete
      - Too many warmup iterations wastes time
      - Some benchmarks never reach steady state
      - Warmup affects subsequent measurements (memory)
      - Auto-detection threshold is corpus-dependent
    concepts:
      - JIT warmup
      - Cache warming
      - Steady state
      - Variance analysis
    skills:
      - Warmup implementation
      - Variance calculation
      - Auto-tuning
      - State separation
    deliverables:
      - Warmup phase executor
      - Steady state detector
      - Auto-warmup tuner
      - Warmup stats separator
      - JIT triggering
    estimated_hours: "4-5"

  - id: benchmark-m3
    name: Statistical Analysis & Confidence Intervals
    description: >
      Implement statistical analysis with confidence intervals
      and proper result reporting.
    acceptance_criteria:
      - Mean, median, stddev, min, max calculated
      - Percentiles (p50, p90, p95, p99) computed
      - Confidence interval at configurable level (default 95%)
      - Standard error of the mean reported
      - Throughput calculations (ops/sec, bytes/sec)
      - Results reported with appropriate precision
      - Memory usage tracked (optional)
    pitfalls:
      - Mean sensitive to outliers; median more robust
      - Confidence interval assumes normal distribution
      - Percentile accuracy depends on sample size
      - Precision should not exceed measurement accuracy
      - Memory tracking affects performance
    concepts:
      - Descriptive statistics
      - Confidence intervals
      - Percentiles
      - Throughput metrics
    skills:
      - Statistical calculations
      - Confidence intervals
      - Percentile computation
      - Metric presentation
    deliverables:
      - Statistics calculator
      - Confidence interval
      - Percentile reporter
      - Throughput calculator
      - Result formatter
    estimated_hours: "4-5"

  - id: benchmark-m4
    name: Outlier Detection & Handling
    description: >
      Implement outlier detection and multiple strategies for
      handling anomalous measurements.
    acceptance_criteria:
      - Outlier detection using IQR or z-score method
      - Configurable outlier threshold
      - Multiple handling strategies: keep, remove, winsorize
      - Outlier reporting (how many, which runs)
      - Visual indication of outlier presence
      - Robust statistics (trimmed mean) option
    pitfalls:
      - Removing outliers can hide real issues
      - Too aggressive threshold removes valid data
      - Outliers may indicate benchmark instability
      - Different methods catch different outliers
      - Winsorization changes distribution shape
    concepts:
      - Outlier detection methods
      - Robust statistics
      - Data cleaning
      - Measurement quality
    skills:
      - IQR calculation
      - Z-score detection
      - Winsorization
      - Robust estimation
    deliverables:
      - Outlier detector (IQR, z-score)
      - Configurable threshold
      - Multiple handling strategies
      - Outlier reporter
      - Robust statistics option
    estimated_hours: "3-4"

  - id: benchmark-m5
    name: Comparison & Regression Detection
    description: >
      Build benchmark comparison with statistical significance
      tests and regression detection for CI/CD.
    acceptance_criteria:
      - Compare two benchmark results (before/after)
      - Student's t-test for significance
      - Mann-Whitney U test for non-normal distributions
      - Report: faster/slower with p-value and effect size
      - Regression threshold configurable (e.g., 5% slower)
      - Historical baseline storage
      - CI exit codes for regression detection
    pitfalls:
      - Multiple comparisons increase false positive rate
      - Small effect sizes may be statistically significant
      - Baseline drift over time
      - Non-normal distributions invalidate t-test
      - Effect size matters more than p-value
    concepts:
      - Statistical hypothesis testing
      - Effect size
      - Regression threshold
      - Baseline management
    skills:
      - t-test implementation
      - Non-parametric tests
      - Effect size calculation
      - CI/CD integration
    deliverables:
      - Benchmark comparator
      - Significance tests (t-test, Mann-Whitney)
      - Effect size reporter
      - Regression detector
      - CI integration
    estimated_hours: "4-5"
