id: bigtable-like-storage
name: Bigtable-like Storage
description: >
  Build a petabyte-scale NoSQL storage system like Google Bigtable or Apache HBase.
  Implement LSM-tree storage, distributed sharding, and write-optimized workloads
  for massive-scale data ingestion.

difficulty: expert
estimated_hours: 80-100
domain: world-scale

essence: >
  Write-optimized distributed storage using LSM-trees (memtable + sorted string tables),
  automatic sharding via range partitioning, and multi-level compaction for read
  amplification control - designed for petabyte-scale time series and entity data.

why_important: >
  Bigtable powers Google Search, Maps, and YouTube. Understanding LSM-tree storage,
  compaction strategies, and distributed sharding is essential for infrastructure
  engineers at $200K-400K+ working on large-scale data systems.

learning_outcomes:
  - Implement LSM-tree storage engine with memtable and SSTables
  - Build multi-level compaction (Leveled, Tiered)
  - Implement distributed sharding with automatic splitting
  - Build bloom filters for efficient point lookups
  - Handle write-heavy workloads with minimal read amplification
  - Implement time-to-live (TTL) and versioning
  - Build scan operations with efficient range queries
  - Handle compaction backpressure

skills:
  - LSM-Tree Storage
  - Compaction Strategies
  - Distributed Sharding
  - Bloom Filters
  - Write Optimization
  - Range Partitioning
  - SSTable Format
  - Compaction Backpressure

tags:
  - expert
  - nosql
  - lsm-tree
  - bigtable
  - hbase
  - distributed-storage

languages:
  recommended:
    - Go
    - Rust
  also_possible:
    - Java
    - C++

resources:
  - name: "Bigtable: A Distributed Storage System for Structured Data"
    url: https://research.google/pubs/pub27898/
    type: paper
  - name: "The Log-Structured Merge-Tree (LSM-Tree)"
    url: https://www.cs.umb.edu/~poneil/lsmtree.pdf
    type: paper
  - name: "RocksDB Tuning Guide"
    url: https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide
    type: documentation
  - name: "HBase Architecture"
    url: https://hbase.apache.org/book.html#arch
    type: documentation

prerequisites:
  - type: project
    name: build-redis or equivalent storage engine
  - type: skill
    name: Understanding of B-trees and disk I/O
  - type: skill
    name: Distributed systems fundamentals
  - type: skill
    name: Go or Rust proficiency

milestones:
  - id: bigtable-m1
    name: Memtable & Write Path
    description: >
      Implement the write path with in-memory memtable and write-ahead log.
    acceptance_criteria:
      - Writes go to memtable (sorted in-memory structure)
      - WAL (write-ahead log) ensures durability before ack
      - Memtable flushes to SSTable when size threshold reached
      - SSTable is immutable sorted file on disk
      - Writes achieve > 100K ops/sec on single thread
      - Crash recovery replays WAL to restore memtable state
    pitfalls:
      - Not fsyncing WAL loses data on crash
      - Memtable too large causes long flush pauses
      - Not blocking writes during flush causes consistency issues
      - Memory fragmentation from many allocations
    concepts:
      - Write path
      - Memtable
      - Write-ahead log
      - SSTable format
    skills:
      - Memtable implementation
      - WAL design
      - SSTable writing
      - Crash recovery
    deliverables:
      - Memtable implementation
      - WAL with recovery
      - SSTable writer
      - Write benchmark
    estimated_hours: "14-18"

  - id: bigtable-m2
    name: Read Path & Bloom Filters
    description: >
      Implement the read path with bloom filters for efficient lookups
      across multiple SSTables.
    acceptance_criteria:
      - Reads check memtable first, then SSTables newest to oldest
      - Bloom filter per SSTable avoids unnecessary disk reads
      - Point lookups complete in < 1ms for cached bloom filter
      - Range scans iterate across memtable and relevant SSTables
      - Block cache reduces disk reads for hot data
      - Read/write ratio handled efficiently
    pitfalls:
      - Too many SSTables slow reads - need compaction
      - Bloom filter false negatives are unacceptable
      - Block cache miss causes expensive disk read
      - Not using index blocks requires full scan
    concepts:
      - Read path
      - Bloom filters
      - Block cache
      - SSTable indexing
    skills:
      - Read implementation
      - Bloom filter
      - Block cache
      - Range scan
    deliverables:
      - Read path
      - Bloom filters
      - Block cache
      - Range scan
    estimated_hours: "14-18"

  - id: bigtable-m3
    name: Compaction Strategies
    description: >
      Implement multi-level compaction to merge SSTables and
      reclaim space from deleted data.
    acceptance_criteria:
      - Leveled compaction: L0 -> L1 -> L2 ... with size ratios
      - Tiered compaction: merge same-level SSTables together
      - Compaction runs in background without blocking writes
      - Deleted/expired data reclaimed during compaction
      - Read amplification bounded (max SSTables to check)
      - Compaction throttling prevents disk bandwidth exhaustion
    pitfalls:
      - Write amplification from too much compaction
      - Compaction falling behind causes read degradation
      - Long compaction pauses block other operations
      - Not handling compaction failure gracefully
    concepts:
      - Compaction strategies
      - Write amplification
      - Read amplification
      - Space reclamation
    skills:
      - Leveled compaction
      - Tiered compaction
      - Throttling
      - Amplification tuning
    deliverables:
      - Leveled compaction
      - Tiered compaction option
      - Throttling mechanism
      - Amplification metrics
    estimated_hours: "16-20"

  - id: bigtable-m4
    name: Distributed Sharding
    description: >
      Implement automatic sharding with tablet servers that split
      and merge based on data distribution.
    acceptance_criteria:
      - Table split into tablets (row ranges) across servers
      - Master assigns tablets to tablet servers
      - Tablets split automatically when size threshold exceeded
      - Tablet migration for load balancing
      - Failure of tablet server triggers reassignment
      - Client library locates tablets via metadata table
    pitfalls:
      - Hot tablets cause load imbalance
      - Split decision based on wrong metric (size vs traffic)
      - Tablet migration disrupts ongoing operations
      - Single master becomes bottleneck
    concepts:
      - Range sharding
      - Tablet splitting
      - Load balancing
      - Failure recovery
    skills:
      - Sharding implementation
      - Tablet management
      - Load balancing
      - Client routing
    deliverables:
      - Tablet server
      - Master coordination
      - Auto-splitting
      - Client library
    estimated_hours: "18-22"

  - id: bigtable-m5
    name: Versioning & TTL
    description: >
      Implement cell versioning for time-series data and TTL
      for automatic expiration.
    acceptance_criteria:
      - Each cell can have multiple versions (timestamped)
      - Reads can specify version range or latest N versions
      - TTL automatically expires old versions
      - Garbage collection removes expired cells during compaction
      - Version count limits prevent unbounded growth
      - Timestamp ordering (descending) for recent-first access
    pitfalls:
      - Too many versions increases read cost
      - TTL expiration not immediate - depends on compaction
      - Timestamp ordering issues with clock skew
      - Version limit can lose data unexpectedly
    concepts:
      - Multi-versioning
      - Time-series data
      - TTL expiration
      - Garbage collection
    skills:
      - Version management
      - TTL implementation
      - GC during compaction
      - Time-series queries
    deliverables:
      - Cell versioning
      - TTL support
      - Version queries
      - GC integration
    estimated_hours: "10-14"
