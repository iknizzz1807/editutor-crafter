id: build-gpu-compute
name: GPU Compute Programming
description: >
  Build GPU compute applications using CUDA or OpenCL. Implement parallel
  algorithms, memory management, and optimization techniques for high-performance
  GPU computing.

difficulty: advanced
estimated_hours: 50-70
domain: systems

essence: >
  Massive parallelism through thousands of GPU threads, memory hierarchy optimization
  (global, shared, constant, texture), kernel design for coalesced access,
  occupancy optimization, and stream-based asynchronous execution.

why_important: >
  GPU computing powers AI training, scientific computing, and game development.
  Understanding GPU programming is valuable at $180K-350K+ for ML infra and HPC.

learning_outcomes:
  - Implement CUDA/OpenCL kernels for parallel computation
  - Optimize memory access patterns for coalescing
  - Use shared memory for performance optimization
  - Implement parallel reduction and scan algorithms
  - Handle thread synchronization and atomics
  - Profile and optimize kernel performance
  - Implement stream-based asynchronous execution
  - Handle GPU memory management efficiently

skills:
  - CUDA/OpenCL Programming
  - Parallel Algorithms
  - Memory Optimization
  - Shared Memory
  - Thread Synchronization
  - Performance Profiling
  - Asynchronous Execution
  - GPU Memory Management

tags:
  - advanced
  - gpu-computing
  - cuda
  - opencl
  - parallel-programming
  - hpc

languages:
  recommended:
    - C++
    - CUDA
  also_possible:
    - Python (with CuPy/PyCUDA)
    - Rust (with cudarc)

resources:
  - name: "CUDA C++ Programming Guide"
    url: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
    type: documentation
  - name: "Programming Massively Parallel Processors"
    url: https://www.elsevier.com/books/programming-massively-parallel-processors/hwu/978-0-12-811986-0
    type: book
  - name: "GPU Gems"
    url: https://developer.nvidia.com/gpugems/GPUGems/gpugems_pref01.html
    type: book
  - name: "NVIDIA Nsight"
    url: https://developer.nvidia.com/nsight-visual-studio-edition
    type: tool

prerequisites:
  - type: skill
    name: C/C++ proficiency
  - type: skill
    name: Understanding of parallel programming concepts
  - type: skill
    name: Basic GPU architecture knowledge
  - type: project
    name: Any performance optimization experience

milestones:
  - id: gpu-compute-m1
    name: CUDA Fundamentals & Memory
    description: >
      Learn CUDA fundamentals including kernel launch, thread hierarchy,
      and memory management.
    acceptance_criteria:
      - Kernel function definition with __global__
      - Grid and block dimension configuration
      - Thread indexing (blockIdx, threadIdx)
      - Global memory allocation (cudaMalloc, cudaFree)
      - Host-device memory transfer (cudaMemcpy)
      - Successful vector addition kernel
    pitfalls:
      - Wrong grid/block dimensions
      - Memory transfer direction errors
      - Exceeding thread/block limits
      - Not checking for errors
    concepts:
      - CUDA execution model
      - Thread hierarchy
      - Memory types
      - Kernel launch
    skills:
      - Kernel writing
      - Memory management
      - Thread indexing
      - Error handling
    deliverables:
      - Basic kernel
      - Memory allocator
      - Vector operations
      - Error checking
    estimated_hours: "10-14"

  - id: gpu-compute-m2
    name: Memory Optimization & Coalescing
    description: >
      Optimize memory access patterns for coalesced reads/writes
      and use shared memory for performance.
    acceptance_criteria:
      - Coalesced global memory access pattern
      - Shared memory declaration and usage
      - Bank conflict avoidance
      - Memory bandwidth benchmarking
      - Matrix transpose using shared memory
      - Performance comparison: naive vs optimized
    pitfalls:
      - Non-coalesced access kills performance
      - Shared memory bank conflicts
      - Race conditions in shared memory
      - __syncthreads() placement
    concepts:
      - Memory coalescing
      - Shared memory
      - Bank conflicts
      - Synchronization
    skills:
      - Access pattern optimization
      - Shared memory usage
      - Bank conflict resolution
      - Synchronization
    deliverables:
      - Coalesced access kernel
      - Shared memory kernel
      - Matrix transpose
      - Bandwidth benchmarks
    estimated_hours: "10-14"

  - id: gpu-compute-m3
    name: Parallel Algorithms
    description: >
      Implement fundamental parallel algorithms: reduction,
      scan (prefix sum), and histogram.
    acceptance_criteria:
      - Parallel reduction with shared memory
      - Warp shuffle for reduction (optional)
      - Blelloch scan (prefix sum)
      - Parallel histogram with atomics
      - Performance scales with data size
      - Comparison with CPU implementations
    pitfalls:
      - Divergent warps in reduction
      - Bank conflicts in scan
      - Atomic contention in histogram
      - Not handling arbitrary sizes
    concepts:
      - Parallel reduction
      - Prefix sum
      - Atomics
      - Work efficiency
    skills:
      - Reduction implementation
      - Scan algorithm
      - Atomic operations
      - Scalability
    deliverables:
      - Parallel reduction
      - Prefix sum
      - Parallel histogram
      - Performance comparison
    estimated_hours: "12-16"

  - id: gpu-compute-m4
    name: Streams & Asynchronous Execution
    description: >
      Implement stream-based asynchronous execution for
      overlapping compute and memory transfers.
    acceptance_criteria:
      - Multiple CUDA streams for concurrency
      - Overlap kernel execution with memory transfer
      - Stream synchronization options
      - Event-based timing
      - Pinned memory for faster transfers
      - Concurrent kernel execution
    pitfalls:
      - Default stream serialization
      - Synchronization overhead
      - Pinned memory limits
      - Resource contention
    concepts:
      - CUDA streams
      - Asynchronous execution
      - Events
      - Pinned memory
    skills:
      - Stream management
      - Overlap optimization
      - Event usage
      - Memory optimization
    deliverables:
      - Stream manager
      - Overlap benchmark
      - Event timing
      - Pinned memory usage
    estimated_hours: "8-12"

  - id: gpu-compute-m5
    name: Profiling & Optimization
    description: >
      Profile GPU applications and optimize for occupancy,
      bandwidth, and instruction throughput.
    acceptance_criteria:
      - Nsight Compute or Nsight Systems profiling
      - Occupancy calculation and optimization
      - Memory bandwidth utilization analysis
      - Identify limiting factor (memory vs compute)
      - Optimize based on profiling data
      - Document optimization decisions
    pitfalls:
      - Optimizing wrong bottleneck
      - Occupancy isn't everything
      - Ignoring launch overhead
      - Not considering power/thermal
    concepts:
      - GPU profiling
      - Occupancy
      - Bandwidth analysis
      - Compute vs memory bound
    skills:
      - Profiler usage
      - Occupancy tuning
      - Bottleneck identification
      - Optimization methodology
    deliverables:
      - Profile analysis
      - Occupancy calculator
      - Optimized kernel
      - Optimization report
    estimated_hours: "8-12"
