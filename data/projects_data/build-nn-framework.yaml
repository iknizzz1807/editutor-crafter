id: build-nn-framework
name: Build Your Own Neural Network Framework
description: Build a deep learning framework from scratch with tensor operations, automatic differentiation, neural network layers, loss functions, and optimizers.
difficulty: expert
estimated_hours: 65-100
essence: Computational graph construction with reverse-mode automatic differentiation (autograd), implementing the chain rule through dynamic graph traversal to compute gradients for arbitrary tensor operations, enabling gradient-based optimization of neural network parameters.
why_important: Building a deep learning framework from scratch demystifies how PyTorch and TensorFlow work internally, teaching the mathematical foundations of backpropagation, memory-efficient gradient computation, and the design patterns underlying all modern AI systems.
learning_outcomes:
- Implement N-dimensional tensors with broadcasting and operator overloading
- Design reverse-mode automatic differentiation with dynamic computational graphs
- Build neural network layers with parameter management and initialization
- Implement loss functions with numerically stable gradient computation
- Build gradient descent optimizers (SGD, Adam) with training loops
- Debug numerical stability issues through gradient checking
- Verify correctness by comparing against NumPy and PyTorch reference implementations
skills:
- Automatic Differentiation
- Computational Graphs
- Numerical Computing
- Gradient Optimization
- Memory Management
- Broadcasting Semantics
- Numerical Stability
tags:
- ai-ml
- autograd
- backpropagation
- build-from-scratch
- expert
- framework
- python
- tensors
architecture_doc: architecture-docs/build-nn-framework/index.md
languages:
  recommended:
  - Python
  - Rust
  - C++
  also_possible:
  - Julia
resources:
- type: video
  name: Micrograd by Karpathy
  url: https://www.youtube.com/watch?v=VMj-3S1tku0
- type: article
  name: Autodiff from Scratch
  url: https://sidsite.com/posts/autodiff/
- type: repository
  name: Tinygrad
  url: https://github.com/tinygrad/tinygrad
prerequisites:
- type: skill
  name: Linear algebra (matrix multiplication, transpose, broadcasting)
- type: skill
  name: Calculus (chain rule, partial derivatives)
- type: skill
  name: Python and NumPy
- type: skill
  name: Basic neural network concepts
milestones:
- id: build-nn-framework-m1
  name: Tensor & Operations
  description: Implement the core tensor data structure with N-dimensional storage, element-wise operations, matrix multiplication, and broadcasting.
  acceptance_criteria:
  - Tensor stores N-dimensional array data with shape, dtype, and requires_grad attributes
  - Element-wise add, subtract, multiply, and divide produce correct results verified against NumPy
  - Matrix multiplication yields correct output for 2D inputs and batched (3D+) inputs
  - Broadcasting automatically expands dimensions following NumPy broadcasting rules for all element-wise operations
  - Operator overloading supports +, -, *, /, @ syntax
  - Reshape, transpose, and sum (with axis parameter) operations produce correct shapes
  - Negative indexing and slicing work for at least 1D and 2D tensors
  - All operations verified against NumPy on at least 5 random input pairs each
  pitfalls:
  - 'Broadcasting gradient computation requires summing gradients along broadcast dimensions—this is the #1 autograd bug and must be handled in M2'
  - In-place operations (+=, *=) on tensors that require gradients corrupt the computation graph
  - Stride-based memory layout is complex; start with contiguous-only tensors and add views later
  - Not handling scalar tensors (shape ()) as a special case of 0-dimensional arrays
  concepts:
  - N-dimensional array storage
  - Broadcasting semantics
  - Operator overloading
  - Memory layout (contiguous vs strided)
  skills:
  - Multidimensional array manipulation
  - NumPy broadcasting rules
  - Operator overloading in Python
  - Numerical computing
  deliverables:
  - Tensor class with shape, dtype, data, and requires_grad attributes
  - Element-wise arithmetic operations with operator overloading
  - Matrix multiplication supporting 2D and batched inputs
  - Broadcasting logic expanding dimensions per NumPy rules
  - Reshape, transpose, and sum operations
  - NumPy comparison test suite verifying all operations
  estimated_hours: 12-18
- id: build-nn-framework-m2
  name: Automatic Differentiation
  description: Implement reverse-mode automatic differentiation (backpropagation) by building a dynamic computation graph during the forward pass and traversing it backward to compute gradients.
  acceptance_criteria:
  - Each operation records its inputs and a backward function in the computation graph during forward pass
  - Backward pass traverses the graph in reverse topological order computing gradients via chain rule
  - Gradient accumulation correctly sums contributions when a tensor is used in multiple operations
  - 'Broadcasting gradient: when a [1, 3] tensor is broadcast to [4, 3] during forward, backward sums gradients along the broadcast dimension to produce [1, 3] gradient'
  - 'Gradient checking: for every differentiable operation, analytical gradient matches numerical gradient (finite differences with epsilon=1e-5) within relative tolerance of 1e-4'
  - 'Gradients computed for: add, subtract, multiply, divide, matmul, sum, relu, exp, log, pow'
  - no_grad context manager disables gradient tracking within its scope
  - zero_grad resets all gradients to None/zero for all parameters
  pitfalls:
  - Wrong topological order causes gradients to be computed before upstream contributions arrive
  - Not accumulating gradients when a tensor feeds into multiple operations produces incorrect gradients
  - Broadcasting gradient reduction is the hardest part—sum along broadcast dimensions, not all dimensions
  - Numerical gradient checking with too-small epsilon causes floating point noise; too-large epsilon gives inaccurate finite differences
  - Vanishing/exploding gradients in deep chains indicate numerical stability issues in the backward implementation
  concepts:
  - Computational graph (DAG of operations)
  - Chain rule and Jacobian-vector products
  - Reverse-mode (adjoint) differentiation
  - Gradient accumulation
  - Numerical gradient checking
  skills:
  - Computational graph construction
  - Topological sort for backward traversal
  - Gradient accumulation implementation
  - Numerical differentiation for verification
  deliverables:
  - Forward pass graph builder recording operations and backward functions
  - Backward pass traversal using topological sort
  - Gradient accumulation handling multi-use tensors
  - Broadcasting gradient reduction along broadcast dimensions
  - Backward functions for all basic operations (add, sub, mul, div, matmul, sum, relu, exp, log)
  - Gradient checking utility comparing analytical vs numerical gradients
  - no_grad context manager
  estimated_hours: 15-22
- id: build-nn-framework-m3
  name: Neural Network Layers & Loss Functions
  description: Implement neural network layers with parameter management, activation functions, and loss functions with numerically stable implementations.
  acceptance_criteria:
  - Linear layer computes y = Wx + b with correct output shape; W initialized with Xavier/Kaiming initialization
  - ReLU, Sigmoid, and Tanh activation functions applied element-wise with correct gradients (verified by gradient checking)
  - Module base class with parameters() method recursively collecting all trainable tensors from submodules
  - Sequential container chains layers and forwards input through each in order
  - 'Mean Squared Error loss: MSE = mean((predicted - target)^2) with correct gradient'
  - Cross-Entropy loss implemented as log_softmax + nll_loss for numerical stability (NOT log(softmax(x)) which overflows)
  - 'Softmax computed with max-subtraction trick: softmax(x) = exp(x - max(x)) / sum(exp(x - max(x)))'
  - 'All loss functions verified: gradient checking passes and outputs match PyTorch within 1e-4 tolerance'
  - Dropout layer randomly zeros elements with probability p during training; passes all inputs during eval
  pitfalls:
  - Parameters not tracked by the module system means the optimizer can't find them
  - 'Xavier/Kaiming initialization matters: zero or large random initialization causes vanishing or exploding gradients'
  - Naive cross-entropy (log of softmax) causes numerical overflow; must use log-softmax formulation
  - Softmax without max subtraction overflows for large logits
  - Dropout must be disabled during evaluation; forgetting this causes non-deterministic inference
  concepts:
  - Module/parameter pattern
  - Weight initialization strategies
  - Numerically stable softmax and cross-entropy
  - Dropout regularization
  skills:
  - Object-oriented framework design
  - Parameter initialization (Xavier, Kaiming)
  - Numerically stable loss implementation
  - Module composition patterns
  deliverables:
  - Linear layer with configurable weight initialization (Xavier, Kaiming)
  - Activation functions: ReLU, Sigmoid, Tanh
  - Module base class with recursive parameter collection
  - Sequential container for chaining layers
  - MSE loss function with gradient
  - Cross-Entropy loss using numerically stable log-softmax
  - Dropout layer with train/eval mode switching
  - PyTorch comparison test suite for all layers and losses
  estimated_hours: 14-20
- id: build-nn-framework-m4
  name: Optimizers & Training
  description: Implement gradient descent optimizers and demonstrate training a neural network on a real task.
  acceptance_criteria:
  - 'SGD optimizer updates parameters: p = p - lr * p.grad, with optional momentum'
  - Adam optimizer with bias-corrected first (m) and second (v) moment estimates; epsilon in denominator for numerical stability
  - Learning rate scheduler supports at least step decay (reduce LR every N epochs)
  - 'Training loop: forward pass → loss computation → backward pass → optimizer step → zero_grad, iterated over mini-batches'
  - Mini-batch training shuffles data each epoch and processes fixed-size batches
  - 'Train a 2-layer network on MNIST or XOR: loss decreases monotonically and accuracy exceeds 90% (MNIST) or 100% (XOR)""'
  - Adam converges faster (fewer epochs to target accuracy) than vanilla SGD on the same task
  - Gradient clipping (max norm) implemented as optional utility
  pitfalls:
  - Forgetting bias correction in Adam (dividing by 1 - beta^t) causes large initial updates
  - Not zeroing gradients before each backward pass accumulates gradients across batches
  - Learning rate too high causes loss oscillation; too low causes painfully slow convergence
  - Not shuffling data each epoch can cause the model to learn batch ordering artifacts
  - Momentum accumulator not initialized to zero causes incorrect first update
  concepts:
  - Gradient descent optimization
  - Momentum and adaptive learning rates
  - Bias correction in Adam
  - Mini-batch training
  skills:
  - Optimizer implementation
  - Training loop design
  - Learning rate scheduling
  - Convergence debugging
  deliverables:
  - SGD optimizer with configurable learning rate and momentum
  - Adam optimizer with bias-corrected moment estimates and epsilon
  - Learning rate scheduler (step decay)
  - Training loop abstraction (forward, loss, backward, step, zero_grad)
  - Data loader producing shuffled mini-batches
  - Demo training script achieving target accuracy on MNIST or XOR
  - Convergence comparison plot: SGD vs Adam on the same task
  estimated_hours: 15-20
domain: ai-ml
