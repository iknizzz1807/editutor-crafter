id: build-sqlite
name: Build Your Own SQLite
description: Embedded SQL database with tokenizer, parser, bytecode compiler (VDBE),
  page-based B-tree/B+tree storage, buffer pool, query planner, and ACID transactions
  via rollback journal and WAL.
difficulty: expert
estimated_hours: 105
essence: SQL tokenization and recursive-descent parsing producing ASTs, compiled into
  bytecode instructions executed by a virtual machine (VDBE), operating over a page-based
  storage engine with B-trees for clustered table storage and B+trees for secondary
  indexes, managed through a buffer pool with LRU eviction, and ACID guarantees via
  either rollback journal or write-ahead logging.
why_important: Building a database from scratch teaches the fundamental data structures
  and algorithms underlying all modern databases. These skills—disk I/O management,
  B-tree indexing, bytecode execution, query optimization, and crash recovery— are
  directly applicable to backend engineering, distributed systems, and performance
  optimization roles.
learning_outcomes:
- Build a lexer and recursive-descent parser converting SQL text into an AST
- Compile ASTs into bytecode instructions for a virtual machine (VDBE)
- Implement a buffer pool manager with LRU eviction and dirty page tracking
- Design page-based B-tree storage for tables (clustered) and B+tree for indexes
- Implement row storage with variable-length record encoding
- Build a cost-based query planner with statistics-driven cardinality estimation
- Implement ACID transactions with rollback journal for crash recovery
- Design WAL mode for improved concurrent read/write performance
skills:
- SQL Parsing
- Bytecode Compilation
- Virtual Machine Execution
- Buffer Pool Management
- B-tree/B+tree Indexing
- Query Optimization
- Page-Based Storage
- Write-Ahead Logging
- Transaction Management
- Binary File Formats
tags:
- acid
- btree
- build-from-scratch
- databases
- expert
- persistence
- query-engine
- sql
- virtual-machine
architecture_doc: architecture-docs/build-sqlite/index.md
languages:
  recommended:
  - C
  - Rust
  - Go
  also_possible:
  - Java
resources:
- name: Let's Build a Simple Database
  url: https://cstack.github.io/db_tutorial/
  type: tutorial
- name: SQLite Architecture
  url: https://www.sqlite.org/arch.html
  type: documentation
- name: SQLite File Format
  url: https://www.sqlite.org/fileformat2.html
  type: documentation
- name: SQLite VDBE Documentation
  url: https://www.sqlite.org/vdbe.html
  type: documentation
- name: CMU 15-445 Database Systems
  url: https://15445.courses.cs.cmu.edu
  type: course
- name: CodeCrafters SQLite Challenge
  url: https://app.codecrafters.io/courses/sqlite/overview
  type: tool
prerequisites:
- type: skill
  name: B-tree data structure
- type: skill
  name: SQL basics
- type: skill
  name: File I/O and binary formats
- type: skill
  name: Basic compiler concepts (lexer, parser, AST)
milestones:
- id: build-sqlite-m1
  name: SQL Tokenizer
  description: Build a lexer that converts SQL text into a stream of typed tokens.
  acceptance_criteria:
  - Tokenizer recognizes SQL keywords case-insensitively
  - String literals correctly handle escaped single quotes ('') and preserve internal
    content
  - Numeric literals distinguish between integers (42) and floats (3.14)
  - The tokenizer returns a list or stream of objects containing type, value, line,
    and column
  - Lexical errors provide position information (line/column)
  pitfalls:
  - Not handling escaped quotes in string literals ('it''s') causes premature string
    termination
  - Keywords must be case-insensitive but identifiers may be case-sensitive depending
    on quoting—handle both
  - Unicode identifiers require careful handling; start simple with ASCII and document
    limitations
  - Negative numbers may be ambiguous with subtraction operator; handle at parser
    level, not tokenizer
  concepts:
  - Lexical analysis converts character stream to token stream
  - Finite state machine drives character-by-character token recognition
  - Token types classify each lexeme (keyword, identifier, literal, operator)
  - Error reporting with source location enables useful diagnostics
  skills:
  - String parsing
  - State machine implementation
  - Token classification
  - Error reporting
  deliverables:
  - Lexer producing token stream from SQL input string
  - Keyword recognition for all supported SQL words
  - String and numeric literal parsing with escape handling
  - Operator and punctuation tokenization
  - Error reporting with line and column position
  - Test suite with at least 20 SQL statements
  estimated_hours: 4
- id: build-sqlite-m2
  name: SQL Parser (AST)
  description: Build a recursive-descent parser that converts the token stream into
    an Abstract Syntax Tree (AST).
  acceptance_criteria:
  - 'Expression parser correctly handles operator precedence: NOT > comparison > AND
    > OR'
  - SELECT parser produces AST with column list including * wildcard support
  - SELECT parser produces optional WHERE clause as Expression node
  - CREATE TABLE parser extracts column definitions with names and data types (INTEGER,
    TEXT, REAL, BLOB)
  - Parser provides error position (line and column) for syntax errors
  - NULL keyword is parsed as LiteralExpression not IdentifierExpression
  pitfalls:
  - Left recursion in expression grammar causes infinite recursion in recursive-descent
    parsers—use precedence climbing or Pratt parsing
  - AND binds tighter than OR in SQL (unlike some programming languages)—get this
    wrong and WHERE clauses evaluate incorrectly
  - Not handling parenthesized sub-expressions breaks complex WHERE clauses
  - NULL is a keyword, not an identifier—treat it specially in expression parsing
  concepts:
  - Recursive descent parsing uses mutually recursive functions for grammar rules
  - AST represents the syntactic structure of a SQL statement as a tree
  - Operator precedence determines evaluation order of expressions
  - Pratt parsing or precedence climbing handles binary operators elegantly
  skills:
  - Recursive function design
  - Tree data structures
  - Grammar rule encoding
  - Precedence handling
  deliverables:
  - SELECT statement parser producing AST
  - INSERT statement parser producing AST
  - CREATE TABLE parser producing AST with column definitions and constraints
  - Expression parser with correct operator precedence
  - Error reporting with token position for parse errors
  - Test suite for valid and invalid SQL inputs
  estimated_hours: 7
- id: build-sqlite-m3
  name: Bytecode Compiler (VDBE)
  description: Compile parsed AST into bytecode instructions executed by a virtual
    machine. This is the execution engine of the database.
  acceptance_criteria:
  - Bytecode includes OpenRead, Rewind, Column, ResultRow, Next, Halt
  - VM implements register-based storage for intermediate values
  - Compiler handles backpatching for jump targets in loops
  - WHERE clause translates to conditional jump opcodes
  - EXPLAIN command displays human-readable opcode sequence
  - VM executes fetch-decode-execute loop with < 10ns overhead per instruction
  pitfalls:
  - Directly interpreting AST nodes (tree-walking interpreter) is simpler but significantly
    slower than bytecode—commit to bytecode
  - Register allocation must handle nested expressions without clobbering intermediate
    values
  - Opcodes for cursor management (open, rewind, next, close) must match the storage
    engine's iterator interface
  - Missing a Halt opcode causes the VM to run past the end of the program into garbage
    memory
  concepts:
  - Bytecode compilation translates high-level AST into low-level instruction sequence
  - Virtual machine executes bytecode using a fetch-decode-execute loop
  - Register-based VM uses a register file for operand storage (vs stack-based)
  - Cursor opcodes abstract B-tree traversal for the VM
  skills:
  - Bytecode generation
  - Virtual machine implementation
  - Register allocation
  - Instruction set design
  deliverables:
  - Bytecode instruction set with opcodes for table operations, comparisons, jumps,
    and output
  - Compiler translating SELECT, INSERT, CREATE TABLE ASTs into bytecode
  - Virtual machine executing bytecode with register file and program counter
  - EXPLAIN command displaying bytecode for any SQL statement
  - WHERE clause compilation to conditional jumps
  estimated_hours: 10
- id: build-sqlite-m4
  name: Buffer Pool Manager
  description: Implement a page cache that sits between the B-tree layer and disk,
    managing fixed-size pages with LRU eviction and dirty page tracking.
  acceptance_criteria:
  - Buffer pool must implement LRU eviction
  - Dirty pages must be flushed to disk before eviction
  - Pinned pages (pin_count > 0) must never be evicted
  - Page access (fetch/unpin) must be O(1) in memory
  - Disk I/O must use 4096-byte page boundaries
  pitfalls:
  - Evicting a pinned page causes data corruption or use-after-free—enforce pin counting
  - Not flushing dirty pages before eviction loses committed data
  - Page ID collisions if page numbering is not globally unique across the database
    file
  - Buffer pool deadlocks when B-tree operations pin too many pages simultaneously—set
    pin limits
  concepts:
  - Buffer pool caches disk pages in memory for fast access
  - LRU (Least Recently Used) eviction approximates optimal page replacement
  - Pin counting prevents eviction of actively-used pages
  - Dirty page tracking enables write-back caching
  skills:
  - Page cache implementation
  - LRU eviction algorithm
  - Pin/unpin lifecycle management
  - Disk I/O management
  deliverables:
  - Buffer pool with configurable frame count and page size
  - FetchPage loading pages from disk or returning cached frames
  - LRU eviction selecting least recently used unpinned page
  - Dirty page tracking and write-back on eviction
  - Pin/Unpin API for B-tree layer
  - FlushAll for shutdown and checkpoint
  estimated_hours: 8
- id: build-sqlite-m5
  name: B-tree Page Format & Table Storage
  description: Implement the on-disk page structure for B-trees (tables) and B+trees
    (indexes), with row serialization and node splitting.
  acceptance_criteria:
  - Implement 1-9 byte Varint encoding/decoding supporting full 64-bit range.
  - Design 4096-byte Slotted Page header with page type, cell count, and content start
    offsets.
  - Implement Big-Endian serialization for all on-disk multi-byte integers.
  - Develop a B-tree node split algorithm that promotes the median key to a parent
    node.
  - Implement binary search within the cell pointer array for O(log N) row lookups
    per page.
  - Ensure Table B-tree internal nodes store only separators and leaf nodes store
    full payloads.
  - Maintain a 'content_start' pointer that tracks the bottom-up growth of the cell
    heap.
  - Implement a 'freeblock' linked list within pages to reclaim space from deleted
    cells.
  pitfalls:
  - Conflating B-tree (data in all nodes) with B+tree (data only in leaves)—tables
    use B-tree, indexes use B+tree in SQLite
  - Cell overflow when a row exceeds page capacity requires overflow pages—handle
    or document the size limit
  - Page fragmentation after deletions wastes space—track free space within pages
  - Endianness must be consistent between write and read (SQLite uses big-endian for
    portability)
  - Variable-length integer encoding (varint) must handle the full range of 64-bit
    integers
  concepts:
  - B-tree stores key-value pairs in all nodes (used for rowid-keyed tables)
  - B+tree stores data only in leaf nodes with linked leaf pages (used for indexes)
  - Slotted page format uses cell pointers for variable-length records
  - Node splitting maintains B-tree balance on insert overflow
  - System catalog stores table and index schema metadata
  skills:
  - Binary page format design
  - B-tree/B+tree implementation
  - Variable-length record encoding
  - Node splitting algorithms
  deliverables:
  - Page format with header, cell pointer array, and cell content area
  - Table B-tree with leaf (row storage) and internal (separator + child pointer)
    pages
  - Index B+tree with leaf (key + rowid) and internal (separator + child pointer)
    pages
  - Row serialization with variable-length encoding for column values
  - Node splitting on insert overflow with separator key promotion
  - System catalog table storing schema metadata
  - Full table scan via leaf page traversal
  estimated_hours: 12
- id: build-sqlite-m6
  name: SELECT Execution & DML
  description: Execute SELECT, INSERT, UPDATE, and DELETE via the bytecode VM, with
    row deserialization, projection, and filtering.
  acceptance_criteria:
  - Cursor recognizes EOF across multiple B-tree pages
  - RecordDeserializer correctly skips variable-length strings to reach subsequent
    columns
  - OP_Column returns VAL_NULL for missing columns in a record
  - Insert operation triggers RESERVED lock upgrade in Pager
  - Two-pass delete prevents cursor invalidation errors
  pitfalls:
  - 'Column name case sensitivity: SQL standard is case-insensitive for identifiers;
    be consistent'
  - 'NULL handling in WHERE: NULL = NULL evaluates to NULL (falsy), not TRUE—use IS
    NULL for null checks'
  - B-tree rebalancing after DELETE is complex; initially, mark rows as deleted and
    reclaim space during compaction
  - Updating the primary key (rowid) requires delete + re-insert at the new position
  - Memory management for large result sets—stream results row-by-row, don't buffer
    all in memory
  concepts:
  - Cursor pattern abstracts B-tree traversal for the VM
  - Projection selects a subset of columns from each row
  - Predicate evaluation filters rows during scan
  - Three-valued logic (TRUE, FALSE, NULL) for SQL boolean expressions
  skills:
  - B-tree cursor implementation
  - Row deserialization
  - Expression evaluation
  - DML execution
  deliverables:
  - Table scan operator iterating all rows via B-tree cursor
  - Row deserialization from binary page format to typed column values
  - Column projection selecting specified fields
  - WHERE clause evaluation with three-valued logic
  - INSERT, UPDATE, DELETE execution via bytecode VM
  - NOT NULL and UNIQUE constraint enforcement during writes
  estimated_hours: 10
- id: build-sqlite-m7
  name: Secondary Indexes
  description: Implement secondary indexes using B+trees and integrate index lookups
    into query execution.
  acceptance_criteria:
  - Implement B+tree variant with horizontal leaf linking via next_leaf pointer.
  - Index cells must contain the RowID as a tie-breaker to ensure key uniqueness.
  - Implement synchronous index maintenance hooks for INSERT, UPDATE, and DELETE.
  - UNIQUE constraint enforcement via O(log N) pre-insertion search.
  - 'New VDBE opcodes: IdxGE (Greater-Equal Seek) and IdxRowid (RowID Extraction).'
  - Index comparison logic must handle NULLs and SQL Type Affinity correctly.
  pitfalls:
  - Forgetting to update indexes on INSERT/UPDATE/DELETE causes stale or missing index
    entries
  - Index on a column with many NULL values—NULLs must be handled consistently (SQLite
    allows multiple NULLs in UNIQUE index)
  - Composite indexes (multi-column) require careful key comparison—leftmost prefix
    must match for index to be useful
  - Index maintenance overhead can make writes slower; only create indexes that benefit
    read patterns
  concepts:
  - Secondary index maps indexed column values to primary keys (rowids)
  - B+tree leaf traversal enables efficient range scans
  - Index maintenance ensures indexes stay consistent with table data
  - Covering index can satisfy a query without table lookup if all needed columns
    are in the index
  skills:
  - B+tree index implementation
  - Index maintenance on DML
  - Index scan execution
  - Unique constraint enforcement
  deliverables:
  - CREATE INDEX building B+tree from existing table data
  - Index maintenance on INSERT, UPDATE, DELETE
  - Index equality lookup avoiding full table scan
  - Index range scan using B+tree leaf traversal
  - UNIQUE index constraint enforcement
  estimated_hours: 8
- id: build-sqlite-m8
  name: Query Planner & Statistics
  description: Implement a cost-based query planner that chooses between table scan
    and index scan based on collected statistics.
  acceptance_criteria:
  - Implement ANALYZE command collecting table row count and index cardinality
  - Design cost model weighing Random I/O vs Sequential I/O
  - Implement selectivity estimation for equality and range predicates
  - Integrate planner with Bytecode Compiler to select Index Scan vs Table Scan
  - Support basic join order optimization based on table size
  - Ensure EXPLAIN output reflects the chosen access path
  pitfalls:
  - Without statistics (before ANALYZE), the planner has no data for cost estimation—use
    default assumptions (e.g., assume 1M rows)
  - Stale statistics after many inserts/deletes cause the planner to choose suboptimal
    plans—recommend periodic ANALYZE
  - Cardinality estimation errors compound through joins—a 10x error in one table
    becomes 100x after a two-table join
  - Plan search space explodes exponentially with number of tables in JOIN—limit to
    dynamic programming for ≤10 tables
  concepts:
  - Cost-based optimization compares estimated cost of alternative plans
  - Cardinality estimation predicts the number of rows each operator produces
  - Selectivity is the fraction of rows matching a predicate
  - Statistics collection (ANALYZE) provides data for cost estimation
  - Dynamic programming-based join ordering for multi-table queries
  skills:
  - Statistics collection
  - Cost model design
  - Cardinality estimation
  - Plan enumeration
  deliverables:
  - ANALYZE command collecting table and index statistics
  - Cost model estimating I/O for table scan vs index scan
  - Plan selection choosing cheapest access path per table
  - EXPLAIN command displaying chosen plan with cost estimates
  - Join order optimization for multi-table queries
  estimated_hours: 10
- id: build-sqlite-m9
  name: Transactions (Rollback Journal)
  description: Implement ACID transactions using a rollback journal for crash recovery.
  acceptance_criteria:
  - Journal file is created before any database modification
  - fsync is called on the journal before the database is touched
  - Hot journal is detected and processed on startup
  - The database lock state transitions correctly through SHARED, RESERVED, PENDING,
    EXCLUSIVE
  - The database file is truncated to its original size during rollback if pages were
    appended
  pitfalls:
  - Writing modified pages to database before journal is fsync'd causes unrecoverable
    corruption on crash
  - Partial page writes (torn pages) on crash can corrupt the database—journal must
    contain complete original pages
  - Lock ordering between multiple connections must be consistent to prevent deadlocks
  - Long-running transactions holding locks block all other writers—document the locking
    behavior
  concepts:
  - ACID: Atomicity (rollback journal), Consistency (constraints), Isolation (locking),
      Durability (fsync)
  - Rollback journal records undo information (original page images) before modification
  - Write ordering: journal fsync → database write → journal delete
  - Crash recovery: hot journal detected → restore original pages → delete journal
  skills:
  - Rollback journal implementation
  - Write ordering enforcement
  - Crash recovery logic
  - Lock management
  deliverables:
  - BEGIN/COMMIT/ROLLBACK command implementation
  - Rollback journal recording original page contents before modification
  - Write ordering ensuring journal is durable before database pages are modified
  - Crash recovery detecting hot journal and restoring database to pre-transaction
    state
  - ACID guarantee verification test suite
  estimated_hours: 10
- id: build-sqlite-m10
  name: WAL Mode
  description: Implement Write-Ahead Logging as an alternative to rollback journal,
    enabling concurrent readers during writes.
  acceptance_criteria:
  - Recognizes WAL keywords and PRAGMA journal_mode=WAL
  - Implements WALHeader and WALFrame binary serialization with cumulative checksums
  - Manages a memory-mapped WAL Index for O(1) page searches
  - Implements Snapshot Isolation via Read Marks in shared memory
  - Executes Passive Checkpointing without blocking concurrent readers
  - Detects and recovers from torn WAL writes using checksum validation
  pitfalls:
  - WAL grows unbounded without checkpointing—auto-checkpoint is not optional, it's
    required
  - Readers pinning old WAL frames prevent checkpoint from truncating—long-running
    reads block WAL cleanup
  - WAL and rollback journal are mutually exclusive modes—switching requires careful
    state management
  - Checkpoint must not run while readers are using WAL frames that would be overwritten
  - WAL file corruption must be detected (checksums) to prevent propagating bad data
    to the main database
  concepts:
  - WAL appends redo information (new page images) to a log file
  - Readers use WAL index (wal-index) to find most recent page version
  - Checkpoint merges WAL changes back into the main database file
  - Snapshot isolation: each reader sees the database as of its start time
  - WAL vs rollback journal: WAL enables concurrent readers, rollback journal does
      not
  skills:
  - Write-ahead log implementation
  - Snapshot isolation for readers
  - Checkpoint algorithm
  - Concurrent read/write coordination
  deliverables:
  - WAL file format appending modified pages with checksums
  - WAL reader looking up most recent page version before main database
  - Checkpoint process copying WAL pages into main database
  - Auto-checkpoint triggered by WAL page count threshold
  - Concurrent reader support during active write transactions
  - WAL mode toggle (PRAGMA journal_mode=WAL)
  estimated_hours: 12
- id: build-sqlite-m11
  name: Aggregate Functions & JOIN
  description: Implement aggregate functions (COUNT, SUM, AVG, MIN, MAX), GROUP BY,
    and basic JOIN execution.
  acceptance_criteria:
  - Implement Aggregate Accumulator Logic for SUM, COUNT, MIN, MAX, AVG
  - Implement Nested Loop Join (NLJ) as the primary relational join mechanism
  - Design a Hash-based GROUP BY manager for streaming aggregation
  - Ensure NULL values are handled per SQL standard in aggregates (ignored) and JOINS
    (Three-Valued Logic)
  - Support multi-cursor synchronization within the VDBE fetch-decode-execute loop
  pitfalls:
  - AVG must handle integer division correctly (return REAL, not INTEGER)
  - 'NULL handling in aggregates: COUNT(*) counts NULLs, COUNT(col) does not; SUM/AVG
    ignore NULLs'
  - GROUP BY without aggregate function is valid SQL but confusing—handle correctly
  - Nested loop join is O(n*m)—acceptable for small tables but document the limitation
  concepts:
  - Aggregate functions accumulate values across row groups
  - GROUP BY partitions rows into groups for aggregation
  - Nested loop join iterates all combinations of rows from two tables
  - HAVING filters groups after aggregation (vs WHERE which filters before)
  skills:
  - Aggregate computation
  - Group-by execution
  - Join algorithms
  - NULL handling in aggregates
  deliverables:
  - COUNT, SUM, AVG, MIN, MAX aggregate functions
  - GROUP BY execution with hash-based or sort-based grouping
  - HAVING clause filtering groups after aggregation
  - Nested loop INNER JOIN execution
  - Test suite covering aggregates with NULLs, empty tables, and multi-table joins
  estimated_hours: 14
domain: data-storage
