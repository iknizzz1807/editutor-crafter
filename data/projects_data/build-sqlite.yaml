id: build-sqlite
name: Build Your Own SQLite
description: Embedded SQL database with tokenizer, parser, bytecode compiler (VDBE),
  page-based B-tree/B+tree storage, buffer pool, query planner, and ACID transactions
  via rollback journal and WAL.
difficulty: expert
estimated_hours: 105
essence: SQL tokenization and recursive-descent parsing producing ASTs, compiled into
  bytecode instructions executed by a virtual machine (VDBE), operating over a page-based
  storage engine with B-trees for clustered table storage and B+trees for secondary
  indexes, managed through a buffer pool with LRU eviction, and ACID guarantees via
  either rollback journal or write-ahead logging.
why_important: Building a database from scratch teaches the fundamental data structures
  and algorithms underlying all modern databases. These skills—disk I/O management,
  B-tree indexing, bytecode execution, query optimization, and crash recovery— are
  directly applicable to backend engineering, distributed systems, and performance
  optimization roles.
learning_outcomes:
- Build a lexer and recursive-descent parser converting SQL text into an AST
- Compile ASTs into bytecode instructions for a virtual machine (VDBE)
- Implement a buffer pool manager with LRU eviction and dirty page tracking
- Design page-based B-tree storage for tables (clustered) and B+tree for indexes
- Implement row storage with variable-length record encoding
- Build a cost-based query planner with statistics-driven cardinality estimation
- Implement ACID transactions with rollback journal for crash recovery
- Design WAL mode for improved concurrent read/write performance
skills:
- SQL Parsing
- Bytecode Compilation
- Virtual Machine Execution
- Buffer Pool Management
- B-tree/B+tree Indexing
- Query Optimization
- Page-Based Storage
- Write-Ahead Logging
- Transaction Management
- Binary File Formats
tags:
- acid
- btree
- build-from-scratch
- databases
- expert
- persistence
- query-engine
- sql
- virtual-machine
architecture_doc: architecture-docs/build-sqlite/index.md
languages:
  recommended:
  - C
  - Rust
  - Go
  also_possible:
  - Java
resources:
- name: Let's Build a Simple Database
  url: https://cstack.github.io/db_tutorial/
  type: tutorial
- name: SQLite Architecture
  url: https://www.sqlite.org/arch.html
  type: documentation
- name: SQLite File Format
  url: https://www.sqlite.org/fileformat2.html
  type: documentation
- name: SQLite VDBE Documentation
  url: https://www.sqlite.org/vdbe.html
  type: documentation
- name: CMU 15-445 Database Systems
  url: https://15445.courses.cs.cmu.edu
  type: course
- name: CodeCrafters SQLite Challenge
  url: https://app.codecrafters.io/courses/sqlite/overview
  type: tool
prerequisites:
- type: skill
  name: B-tree data structure
- type: skill
  name: SQL basics
- type: skill
  name: File I/O and binary formats
- type: skill
  name: Basic compiler concepts (lexer, parser, AST)
milestones:
- id: build-sqlite-m1
  name: SQL Tokenizer
  description: Build a lexer that converts SQL text into a stream of typed tokens.
  acceptance_criteria:
  - Tokenizer recognizes SQL keywords (SELECT, INSERT, CREATE, WHERE, JOIN, etc.)
    case-insensitively
  - String literals enclosed in single quotes are parsed including escaped quotes
    ('it''s' → it's)
  - Numeric literals including integers and floating-point values (42, 3.14, -7) are
    recognized as distinct token types
  - Operators (=, <, >, <=, >=, !=, <>) and punctuation (comma, parentheses, semicolon)
    are tokenized as distinct tokens
  - Identifiers (table names, column names) support quoted identifiers with double
    quotes ("column name")
  - Tokenizer reports error position (line and column) for unrecognized characters
  - Token stream correctly tokenizes at least 20 diverse SQL statements in a test
    suite
  - Tokenizer recognizes keywords like SELECT and INSERT case-insensitively
  - String literals correctly handle escaped single quotes ('') and preserve internal
    content
  - Numeric literals distinguish between integers (42) and floats (3.14)
  - Double-quoted identifiers ('"Table Name"') are correctly tokenized as identifiers
  - The tokenizer returns a list or stream of objects containing type, value, line,
    and column
  - A test suite of 20+ SQL statements produces the expected token sequence
  pitfalls:
  - Not handling escaped quotes in string literals ('it''s') causes premature string
    termination
  - Keywords must be case-insensitive but identifiers may be case-sensitive depending
    on quoting—handle both
  - Unicode identifiers require careful handling; start simple with ASCII and document
    limitations
  - Negative numbers may be ambiguous with subtraction operator; handle at parser
    level, not tokenizer
  concepts:
  - Lexical analysis converts character stream to token stream
  - Finite state machine drives character-by-character token recognition
  - Token types classify each lexeme (keyword, identifier, literal, operator)
  - Error reporting with source location enables useful diagnostics
  skills:
  - String parsing
  - State machine implementation
  - Token classification
  - Error reporting
  deliverables:
  - Lexer producing token stream from SQL input string
  - Keyword recognition for all supported SQL words
  - String and numeric literal parsing with escape handling
  - Operator and punctuation tokenization
  - Error reporting with line and column position
  - Test suite with at least 20 SQL statements
  estimated_hours: 4
- id: build-sqlite-m2
  name: SQL Parser (AST)
  description: Build a recursive-descent parser that converts the token stream into
    an Abstract Syntax Tree (AST).
  acceptance_criteria:
  - SELECT parser produces AST with column list (including *), FROM clause, optional
    WHERE, ORDER BY, and LIMIT
  - INSERT parser produces AST with target table, optional column names, and VALUES
    clause
  - CREATE TABLE parser extracts column definitions with names, types (INTEGER, TEXT,
    REAL, BLOB), and constraints (PRIMARY KEY, NOT NULL, UNIQUE)
  - 'Expression parser correctly handles operator precedence: NOT > comparison (=,<,>)
    > AND > OR'
  - Parenthesized expressions override default precedence
  - Parser produces clear error messages with token position for syntax errors
  - Test suite covers at least 15 valid and 10 invalid SQL statements
  - SELECT parser produces AST with column list, FROM, and optional WHERE/LIMIT
  - INSERT parser handles target table and VALUES mapping
  - CREATE TABLE parser extracts column names, types, and constraints (PRIMARY KEY,
    NOT NULL)
  - Expression parser correctly handles NOT > AND > OR precedence
  - Parenthesized expressions correctly override default precedence levels
  - Parser provides error position (line/column) for syntax errors
  - Test suite passes for 15+ valid and 10+ invalid SQL edge cases
  pitfalls:
  - Left recursion in expression grammar causes infinite recursion in recursive-descent
    parsers—use precedence climbing or Pratt parsing
  - AND binds tighter than OR in SQL (unlike some programming languages)—get this
    wrong and WHERE clauses evaluate incorrectly
  - Not handling parenthesized sub-expressions breaks complex WHERE clauses
  - NULL is a keyword, not an identifier—treat it specially in expression parsing
  concepts:
  - Recursive descent parsing uses mutually recursive functions for grammar rules
  - AST represents the syntactic structure of a SQL statement as a tree
  - Operator precedence determines evaluation order of expressions
  - Pratt parsing or precedence climbing handles binary operators elegantly
  skills:
  - Recursive function design
  - Tree data structures
  - Grammar rule encoding
  - Precedence handling
  deliverables:
  - SELECT statement parser producing AST
  - INSERT statement parser producing AST
  - CREATE TABLE parser producing AST with column definitions and constraints
  - Expression parser with correct operator precedence
  - Error reporting with token position for parse errors
  - Test suite for valid and invalid SQL inputs
  estimated_hours: 7
- id: build-sqlite-m3
  name: Bytecode Compiler (VDBE)
  description: Compile parsed AST into bytecode instructions executed by a virtual
    machine. This is the execution engine of the database.
  acceptance_criteria:
  - Compiler translates SELECT AST into a bytecode program with opcodes for OpenTable,
    Rewind, Column, ResultRow, Next, Halt
  - Compiler translates INSERT AST into bytecode with opcodes for OpenTable, MakeRecord,
    Insert, Halt
  - Virtual machine executes bytecode programs step-by-step, processing one opcode
    per cycle
  - VM maintains a register file (array of typed values) for intermediate computation
  - EXPLAIN command outputs the bytecode program for a given SQL statement in human-readable
    format
  - WHERE clause compiles to conditional jump opcodes that skip non-matching rows
  - Bytecode execution of 'SELECT * FROM t' on a 10,000-row table completes in under
    100ms
  - Compiler translates SELECT AST into opcodes including OpenTable, Rewind, Column,
    ResultRow, Next, and Halt
  - Compiler translates INSERT AST into opcodes including OpenTable, MakeRecord, and
    Insert
  - VM executes bytecode in a fetch-decode-execute loop, processing one opcode per
    cycle
  - VM manages a register file of typed values for intermediate calculations
  - WHERE clauses are correctly compiled into conditional jump opcodes (e.g., Gt,
    Le, Ne)
  - The EXPLAIN command displays the human-readable opcode sequence for any valid
    SQL statement
  - The VM executes a full table scan of 10,000 rows in under 100ms
  pitfalls:
  - Directly interpreting AST nodes (tree-walking interpreter) is simpler but significantly
    slower than bytecode—commit to bytecode
  - Register allocation must handle nested expressions without clobbering intermediate
    values
  - Opcodes for cursor management (open, rewind, next, close) must match the storage
    engine's iterator interface
  - Missing a Halt opcode causes the VM to run past the end of the program into garbage
    memory
  concepts:
  - Bytecode compilation translates high-level AST into low-level instruction sequence
  - Virtual machine executes bytecode using a fetch-decode-execute loop
  - Register-based VM uses a register file for operand storage (vs stack-based)
  - Cursor opcodes abstract B-tree traversal for the VM
  skills:
  - Bytecode generation
  - Virtual machine implementation
  - Register allocation
  - Instruction set design
  deliverables:
  - Bytecode instruction set with opcodes for table operations, comparisons, jumps,
    and output
  - Compiler translating SELECT, INSERT, CREATE TABLE ASTs into bytecode
  - Virtual machine executing bytecode with register file and program counter
  - EXPLAIN command displaying bytecode for any SQL statement
  - WHERE clause compilation to conditional jumps
  estimated_hours: 10
- id: build-sqlite-m4
  name: Buffer Pool Manager
  description: Implement a page cache that sits between the B-tree layer and disk,
    managing fixed-size pages with LRU eviction and dirty page tracking.
  acceptance_criteria:
  - Buffer pool manages a configurable number of in-memory page frames (default 1000
    pages)
  - Pages are fixed-size (4096 bytes by default, configurable)
  - FetchPage loads a page from disk into a free frame, or returns the cached frame
    if already resident
  - LRU eviction selects the least recently used unpinned page for replacement when
    no free frames exist
  - Dirty page tracking marks pages modified in memory; eviction writes dirty pages
    to disk before replacement
  - Pin/Unpin mechanism prevents eviction of pages currently in use by B-tree operations
  - FlushAll writes all dirty pages to disk (used before checkpoint or shutdown)
  - Buffer pool hit rate is measurable and logged for performance tuning
  - Buffer pool initializes with a fixed number of 4096-byte frames
  - FetchPage returns the correct page from memory if already loaded (hit)
  - FetchPage loads page from disk if not in memory (miss)
  - LRU algorithm correctly identifies the least recently used page for eviction
  - Pinned pages (count > 0) are never selected for eviction
  - Dirty pages are written back to disk only when evicted or on FlushAll
  - Buffer pool hit rate is tracked and accessible for performance metrics
  pitfalls:
  - Evicting a pinned page causes data corruption or use-after-free—enforce pin counting
  - Not flushing dirty pages before eviction loses committed data
  - Page ID collisions if page numbering is not globally unique across the database
    file
  - Buffer pool deadlocks when B-tree operations pin too many pages simultaneously—set
    pin limits
  concepts:
  - Buffer pool caches disk pages in memory for fast access
  - LRU (Least Recently Used) eviction approximates optimal page replacement
  - Pin counting prevents eviction of actively-used pages
  - Dirty page tracking enables write-back caching
  skills:
  - Page cache implementation
  - LRU eviction algorithm
  - Pin/unpin lifecycle management
  - Disk I/O management
  deliverables:
  - Buffer pool with configurable frame count and page size
  - FetchPage loading pages from disk or returning cached frames
  - LRU eviction selecting least recently used unpinned page
  - Dirty page tracking and write-back on eviction
  - Pin/Unpin API for B-tree layer
  - FlushAll for shutdown and checkpoint
  estimated_hours: 8
- id: build-sqlite-m5
  name: B-tree Page Format & Table Storage
  description: Implement the on-disk page structure for B-trees (tables) and B+trees
    (indexes), with row serialization and node splitting.
  acceptance_criteria:
  - Page header contains page type (leaf/internal, table/index), cell count, free
    space pointer, and right-child pointer (internal only)
  - Table B-tree leaf pages store rows keyed by rowid with variable-length record
    encoding
  - Table B-tree internal pages store rowid separator keys and child page numbers
  - Index B+tree leaf pages store (indexed column value, rowid) pairs with data only
    in leaves
  - Index B+tree internal pages store only separator keys and child pointers (no row
    data)
  - CREATE TABLE creates a B-tree root page and records the schema in a system catalog
    (sqlite_master equivalent)
  - INSERT serializes a row and inserts into the correct B-tree leaf; node splitting
    creates a new page and promotes a separator key
  - Full table scan traverses all leaf pages in rowid order, returning all rows
  - Pages serialize to and deserialize from exactly 4096-byte buffers via the buffer
    pool
  - Page header correctly identifies Leaf vs Internal and Table vs Index types
  - Slotted page format implements bidirectional growth (pointers vs cells)
  - Table B-tree stores full records in leaf nodes keyed by rowid
  - Index B+tree stores key/rowid pairs only in leaf nodes
  - Node split algorithm correctly rebalances the tree and promotes keys to parents
  - Varint implementation handles 1-9 byte encoding for 64-bit integers
  - System catalog (sqlite_master) persists table root page numbers
  - Full table scan successfully iterates through all leaf pages in order
  pitfalls:
  - Conflating B-tree (data in all nodes) with B+tree (data only in leaves)—tables
    use B-tree, indexes use B+tree in SQLite
  - Cell overflow when a row exceeds page capacity requires overflow pages—handle
    or document the size limit
  - Page fragmentation after deletions wastes space—track free space within pages
  - Endianness must be consistent between write and read (SQLite uses big-endian for
    portability)
  - Variable-length integer encoding (varint) must handle the full range of 64-bit
    integers
  concepts:
  - B-tree stores key-value pairs in all nodes (used for rowid-keyed tables)
  - B+tree stores data only in leaf nodes with linked leaf pages (used for indexes)
  - Slotted page format uses cell pointers for variable-length records
  - Node splitting maintains B-tree balance on insert overflow
  - System catalog stores table and index schema metadata
  skills:
  - Binary page format design
  - B-tree/B+tree implementation
  - Variable-length record encoding
  - Node splitting algorithms
  deliverables:
  - Page format with header, cell pointer array, and cell content area
  - Table B-tree with leaf (row storage) and internal (separator + child pointer)
    pages
  - Index B+tree with leaf (key + rowid) and internal (separator + child pointer)
    pages
  - Row serialization with variable-length encoding for column values
  - Node splitting on insert overflow with separator key promotion
  - System catalog table storing schema metadata
  - Full table scan via leaf page traversal
  estimated_hours: 12
- id: build-sqlite-m6
  name: SELECT Execution & DML
  description: Execute SELECT, INSERT, UPDATE, and DELETE via the bytecode VM, with
    row deserialization, projection, and filtering.
  acceptance_criteria:
  - SELECT * FROM table returns all rows in rowid order via B-tree leaf scan
  - SELECT col1, col2 returns only specified columns (projection)
  - WHERE clause filters rows during scan, evaluating boolean expressions on deserialized
    column values
  - INSERT adds a row to the B-tree; subsequent SELECT returns the inserted data
  - UPDATE modifies columns in rows matching WHERE; subsequent SELECT reflects changes
  - DELETE removes rows matching WHERE; subsequent SELECT no longer returns them
  - NOT NULL constraint rejects INSERT or UPDATE setting a NOT NULL column to null
  - Operations on non-existent tables return an error with the table name
  - SELECT * returns all rows by iterating through the B-tree leaf sequence
  - SELECT with column names correctly projects only the requested fields
  - WHERE clause correctly filters rows using Three-Valued Logic (handling NULLs)
  - INSERT adds a new row and updates the B-tree structure correctly
  - UPDATE and DELETE modify/remove rows while maintaining B-tree integrity
  - NOT NULL constraints reject invalid writes with a descriptive error
  - Attempting to query a table not in the System Catalog returns an 'undefined table'
    error
  pitfalls:
  - 'Column name case sensitivity: SQL standard is case-insensitive for identifiers;
    be consistent'
  - 'NULL handling in WHERE: NULL = NULL evaluates to NULL (falsy), not TRUE—use IS
    NULL for null checks'
  - B-tree rebalancing after DELETE is complex; initially, mark rows as deleted and
    reclaim space during compaction
  - Updating the primary key (rowid) requires delete + re-insert at the new position
  - Memory management for large result sets—stream results row-by-row, don't buffer
    all in memory
  concepts:
  - Cursor pattern abstracts B-tree traversal for the VM
  - Projection selects a subset of columns from each row
  - Predicate evaluation filters rows during scan
  - Three-valued logic (TRUE, FALSE, NULL) for SQL boolean expressions
  skills:
  - B-tree cursor implementation
  - Row deserialization
  - Expression evaluation
  - DML execution
  deliverables:
  - Table scan operator iterating all rows via B-tree cursor
  - Row deserialization from binary page format to typed column values
  - Column projection selecting specified fields
  - WHERE clause evaluation with three-valued logic
  - INSERT, UPDATE, DELETE execution via bytecode VM
  - NOT NULL and UNIQUE constraint enforcement during writes
  estimated_hours: 10
- id: build-sqlite-m7
  name: Secondary Indexes
  description: Implement secondary indexes using B+trees and integrate index lookups
    into query execution.
  acceptance_criteria:
  - CREATE INDEX builds a B+tree index mapping (indexed column value → rowid) from
    existing table data
  - Index is automatically maintained on INSERT, UPDATE, and DELETE (index entries
    added/removed/updated)
  - Index lookup retrieves rows matching an equality predicate without full table
    scan, verified by counting pages read
  - Range scan on index returns rows within a value range using B+tree leaf traversal
  - Query execution uses index scan when an indexed column appears in WHERE with equality
    or range predicate
  - UNIQUE index rejects INSERT or UPDATE that would create duplicate values
  - CREATE INDEX builds a B+tree mapping column values to rowids
  - INSERT/UPDATE/DELETE operations maintain all associated indexes synchronously
  - Index lookup (equality) avoids full table scan and visits significantly fewer
    pages
  - Index range scan (BETWEEN or < >) traverses linked leaf pages
  - UNIQUE index correctly rejects duplicate value insertions
  - Bytecode VM can perform a 'Double Lookup' from index cursor to table cursor
  pitfalls:
  - Forgetting to update indexes on INSERT/UPDATE/DELETE causes stale or missing index
    entries
  - Index on a column with many NULL values—NULLs must be handled consistently (SQLite
    allows multiple NULLs in UNIQUE index)
  - Composite indexes (multi-column) require careful key comparison—leftmost prefix
    must match for index to be useful
  - Index maintenance overhead can make writes slower; only create indexes that benefit
    read patterns
  concepts:
  - Secondary index maps indexed column values to primary keys (rowids)
  - B+tree leaf traversal enables efficient range scans
  - Index maintenance ensures indexes stay consistent with table data
  - Covering index can satisfy a query without table lookup if all needed columns
    are in the index
  skills:
  - B+tree index implementation
  - Index maintenance on DML
  - Index scan execution
  - Unique constraint enforcement
  deliverables:
  - CREATE INDEX building B+tree from existing table data
  - Index maintenance on INSERT, UPDATE, DELETE
  - Index equality lookup avoiding full table scan
  - Index range scan using B+tree leaf traversal
  - UNIQUE index constraint enforcement
  estimated_hours: 8
- id: build-sqlite-m8
  name: Query Planner & Statistics
  description: Implement a cost-based query planner that chooses between table scan
    and index scan based on collected statistics.
  acceptance_criteria:
  - 'ANALYZE command collects statistics: row count per table, distinct value count
    per indexed column'
  - Cost model estimates pages read for full table scan (total_pages) and index scan
    (estimated_rows / rows_per_page)
  - Planner selects index scan when estimated selectivity (matching_rows / total_rows)
    is below a threshold (e.g., 20%)
  - Planner falls back to table scan when no suitable index exists or selectivity
    is too low
  - EXPLAIN shows the chosen plan including scan type, index name (if used), and estimated
    row count
  - For multi-table queries (JOIN), planner estimates join cardinality and selects
    join order to minimize intermediate result size
  pitfalls:
  - Without statistics (before ANALYZE), the planner has no data for cost estimation—use
    default assumptions (e.g., assume 1M rows)
  - Stale statistics after many inserts/deletes cause the planner to choose suboptimal
    plans—recommend periodic ANALYZE
  - Cardinality estimation errors compound through joins—a 10x error in one table
    becomes 100x after a two-table join
  - Plan search space explodes exponentially with number of tables in JOIN—limit to
    dynamic programming for ≤10 tables
  concepts:
  - Cost-based optimization compares estimated cost of alternative plans
  - Cardinality estimation predicts the number of rows each operator produces
  - Selectivity is the fraction of rows matching a predicate
  - Statistics collection (ANALYZE) provides data for cost estimation
  - Dynamic programming-based join ordering for multi-table queries
  skills:
  - Statistics collection
  - Cost model design
  - Cardinality estimation
  - Plan enumeration
  deliverables:
  - ANALYZE command collecting table and index statistics
  - Cost model estimating I/O for table scan vs index scan
  - Plan selection choosing cheapest access path per table
  - EXPLAIN command displaying chosen plan with cost estimates
  - Join order optimization for multi-table queries
  estimated_hours: 10
- id: build-sqlite-m9
  name: Transactions (Rollback Journal)
  description: Implement ACID transactions using a rollback journal for crash recovery.
  acceptance_criteria:
  - BEGIN starts a transaction; all subsequent writes are buffered until COMMIT or
    ROLLBACK
  - COMMIT makes all changes permanent by flushing dirty pages and removing the rollback
    journal
  - ROLLBACK undoes all changes by restoring original pages from the rollback journal
  - Rollback journal records original page contents BEFORE modification (for undo
    on crash)
  - Changes are not visible to other connections until COMMIT (basic read isolation)
  - Crash recovery on startup detects an existing rollback journal and automatically
    rolls back the incomplete transaction
  - Journal file is fsync'd before modified pages are written to the database file
    (write ordering guarantee)
  - BEGIN/COMMIT/ROLLBACK commands correctly toggle the engine state
  - A .db-journal file is created and contains original page data before any write
    to the main .db file
  - The journal file is physically flushed to disk (fsync) before the main database
    is modified
  - A manual ROLLBACK restores the state from the journal and clears the journal file
  - Startup logic detects a 'Hot Journal' and automatically restores the database
    to a consistent state
  - Writes are not visible to other database connections until the COMMIT is complete
  pitfalls:
  - Writing modified pages to database before journal is fsync'd causes unrecoverable
    corruption on crash
  - Partial page writes (torn pages) on crash can corrupt the database—journal must
    contain complete original pages
  - Lock ordering between multiple connections must be consistent to prevent deadlocks
  - Long-running transactions holding locks block all other writers—document the locking
    behavior
  concepts:
  - ACID: Atomicity (rollback journal), Consistency (constraints), Isolation (locking),
      Durability (fsync)
  - Rollback journal records undo information (original page images) before modification
  - Write ordering: journal fsync → database write → journal delete
  - Crash recovery: hot journal detected → restore original pages → delete journal
  skills:
  - Rollback journal implementation
  - Write ordering enforcement
  - Crash recovery logic
  - Lock management
  deliverables:
  - BEGIN/COMMIT/ROLLBACK command implementation
  - Rollback journal recording original page contents before modification
  - Write ordering ensuring journal is durable before database pages are modified
  - Crash recovery detecting hot journal and restoring database to pre-transaction
    state
  - ACID guarantee verification test suite
  estimated_hours: 10
- id: build-sqlite-m10
  name: WAL Mode
  description: Implement Write-Ahead Logging as an alternative to rollback journal,
    enabling concurrent readers during writes.
  acceptance_criteria:
  - WAL mode appends modified pages to a separate WAL file instead of modifying the
    main database file
  - Writers append to WAL; readers check WAL for the most recent version of a page
    before reading from the main database
  - Multiple readers can execute queries concurrently while a single writer appends
    to the WAL
  - Checkpoint (PRAGMA wal_checkpoint) copies WAL pages back into the main database
    file
  - WAL checkpoint is required to prevent unbounded WAL growth—auto-checkpoint triggers
    after configurable page count (default 1000)
  - 'Readers see a consistent snapshot: a reader that starts before a commit does
    not see that commit''s changes (snapshot isolation for reads)'
  - WAL file corruption is detected via page checksums
  - Writers append to a separate WAL file instead of modifying the main .db file
  - Readers search the WAL for the most recent page version before falling back to
    the main file
  - Writers and multiple readers can operate simultaneously without blocking
  - Checkpointing copies WAL pages to the main database and truncates the WAL
  - Automatic checkpoint triggers after 1000 pages (configurable)
  - Readers use a consistent snapshot based on the WAL state at their start time
  - Checksums are used to detect and reject corrupted WAL frames
  pitfalls:
  - WAL grows unbounded without checkpointing—auto-checkpoint is not optional, it's
    required
  - Readers pinning old WAL frames prevent checkpoint from truncating—long-running
    reads block WAL cleanup
  - WAL and rollback journal are mutually exclusive modes—switching requires careful
    state management
  - Checkpoint must not run while readers are using WAL frames that would be overwritten
  - WAL file corruption must be detected (checksums) to prevent propagating bad data
    to the main database
  concepts:
  - WAL appends redo information (new page images) to a log file
  - Readers use WAL index (wal-index) to find most recent page version
  - Checkpoint merges WAL changes back into the main database file
  - Snapshot isolation: each reader sees the database as of its start time
  - WAL vs rollback journal: WAL enables concurrent readers, rollback journal does
      not
  skills:
  - Write-ahead log implementation
  - Snapshot isolation for readers
  - Checkpoint algorithm
  - Concurrent read/write coordination
  deliverables:
  - WAL file format appending modified pages with checksums
  - WAL reader looking up most recent page version before main database
  - Checkpoint process copying WAL pages into main database
  - Auto-checkpoint triggered by WAL page count threshold
  - Concurrent reader support during active write transactions
  - WAL mode toggle (PRAGMA journal_mode=WAL)
  estimated_hours: 12
- id: build-sqlite-m11
  name: Aggregate Functions & JOIN
  description: Implement aggregate functions (COUNT, SUM, AVG, MIN, MAX), GROUP BY,
    and basic JOIN execution.
  acceptance_criteria:
  - COUNT(*) returns the number of rows; COUNT(col) returns count of non-NULL values
  - SUM, AVG, MIN, MAX produce correct results over grouped and ungrouped queries
  - GROUP BY groups rows by specified columns before applying aggregate functions
  - HAVING filters groups after aggregation
  - INNER JOIN combines rows from two tables matching a join condition
  - Nested loop join is implemented as the baseline join algorithm
  - JOIN with WHERE clause filters correctly after join
  - COUNT(*) accurately counts rows including NULLs
  - COUNT(col) ignores NULL values in the target column
  - AVG returns a REAL/float even if input column is INTEGER
  - GROUP BY correctly partitions aggregate states into buckets
  - HAVING filters out aggregated groups based on result values
  - INNER JOIN correctly combines rows from two tables using a nested loop
  - JOIN with WHERE correctly filters rows before or during the join process
  pitfalls:
  - AVG must handle integer division correctly (return REAL, not INTEGER)
  - 'NULL handling in aggregates: COUNT(*) counts NULLs, COUNT(col) does not; SUM/AVG
    ignore NULLs'
  - GROUP BY without aggregate function is valid SQL but confusing—handle correctly
  - Nested loop join is O(n*m)—acceptable for small tables but document the limitation
  concepts:
  - Aggregate functions accumulate values across row groups
  - GROUP BY partitions rows into groups for aggregation
  - Nested loop join iterates all combinations of rows from two tables
  - HAVING filters groups after aggregation (vs WHERE which filters before)
  skills:
  - Aggregate computation
  - Group-by execution
  - Join algorithms
  - NULL handling in aggregates
  deliverables:
  - COUNT, SUM, AVG, MIN, MAX aggregate functions
  - GROUP BY execution with hash-based or sort-based grouping
  - HAVING clause filtering groups after aggregation
  - Nested loop INNER JOIN execution
  - Test suite covering aggregates with NULLs, empty tables, and multi-table joins
  estimated_hours: 14
domain: data-storage
