id: build-sqlite
name: Build Your Own SQLite
description: >-
  Embedded SQL database with tokenizer, parser, bytecode compiler (VDBE),
  page-based B-tree/B+tree storage, buffer pool, query planner, and ACID
  transactions via rollback journal and WAL.
difficulty: expert
estimated_hours: 105
essence: >-
  SQL tokenization and recursive-descent parsing producing ASTs, compiled into
  bytecode instructions executed by a virtual machine (VDBE), operating over a
  page-based storage engine with B-trees for clustered table storage and
  B+trees for secondary indexes, managed through a buffer pool with LRU
  eviction, and ACID guarantees via either rollback journal or write-ahead
  logging.
why_important: >-
  Building a database from scratch teaches the fundamental data structures and
  algorithms underlying all modern databases. These skills—disk I/O management,
  B-tree indexing, bytecode execution, query optimization, and crash recovery—
  are directly applicable to backend engineering, distributed systems, and
  performance optimization roles.
learning_outcomes:
  - Build a lexer and recursive-descent parser converting SQL text into an AST
  - Compile ASTs into bytecode instructions for a virtual machine (VDBE)
  - Implement a buffer pool manager with LRU eviction and dirty page tracking
  - Design page-based B-tree storage for tables (clustered) and B+tree for indexes
  - Implement row storage with variable-length record encoding
  - Build a cost-based query planner with statistics-driven cardinality estimation
  - Implement ACID transactions with rollback journal for crash recovery
  - Design WAL mode for improved concurrent read/write performance
skills:
  - SQL Parsing
  - Bytecode Compilation
  - Virtual Machine Execution
  - Buffer Pool Management
  - B-tree/B+tree Indexing
  - Query Optimization
  - Page-Based Storage
  - Write-Ahead Logging
  - Transaction Management
  - Binary File Formats
tags:
  - acid
  - btree
  - build-from-scratch
  - databases
  - expert
  - persistence
  - query-engine
  - sql
  - virtual-machine
architecture_doc: architecture-docs/build-sqlite/index.md
languages:
  recommended:
    - C
    - Rust
    - Go
  also_possible:
    - Java
resources:
  - name: Let's Build a Simple Database
    url: https://cstack.github.io/db_tutorial/
    type: tutorial
  - name: SQLite Architecture
    url: https://www.sqlite.org/arch.html
    type: documentation
  - name: SQLite File Format
    url: https://www.sqlite.org/fileformat2.html
    type: documentation
  - name: SQLite VDBE Documentation
    url: https://www.sqlite.org/vdbe.html
    type: documentation
  - name: CMU 15-445 Database Systems
    url: https://15445.courses.cs.cmu.edu
    type: course
  - name: CodeCrafters SQLite Challenge
    url: https://app.codecrafters.io/courses/sqlite/overview
    type: tool
prerequisites:
  - type: skill
    name: B-tree data structure
  - type: skill
    name: SQL basics
  - type: skill
    name: File I/O and binary formats
  - type: skill
    name: Basic compiler concepts (lexer, parser, AST)
milestones:
  - id: build-sqlite-m1
    name: SQL Tokenizer
    description: >-
      Build a lexer that converts SQL text into a stream of typed tokens.
    acceptance_criteria:
      - "Tokenizer recognizes SQL keywords (SELECT, INSERT, CREATE, WHERE, JOIN, etc.) case-insensitively"
      - "String literals enclosed in single quotes are parsed including escaped quotes ('it''s' → it's)"
      - "Numeric literals including integers and floating-point values (42, 3.14, -7) are recognized as distinct token types"
      - "Operators (=, <, >, <=, >=, !=, <>) and punctuation (comma, parentheses, semicolon) are tokenized as distinct tokens"
      - "Identifiers (table names, column names) support quoted identifiers with double quotes (\"column name\")"
      - "Tokenizer reports error position (line and column) for unrecognized characters"
      - "Token stream correctly tokenizes at least 20 diverse SQL statements in a test suite"
    pitfalls:
      - "Not handling escaped quotes in string literals ('it''s') causes premature string termination"
      - "Keywords must be case-insensitive but identifiers may be case-sensitive depending on quoting—handle both"
      - "Unicode identifiers require careful handling; start simple with ASCII and document limitations"
      - "Negative numbers may be ambiguous with subtraction operator; handle at parser level, not tokenizer"
    concepts:
      - Lexical analysis converts character stream to token stream
      - Finite state machine drives character-by-character token recognition
      - Token types classify each lexeme (keyword, identifier, literal, operator)
      - Error reporting with source location enables useful diagnostics
    skills:
      - String parsing
      - State machine implementation
      - Token classification
      - Error reporting
    deliverables:
      - Lexer producing token stream from SQL input string
      - Keyword recognition for all supported SQL words
      - String and numeric literal parsing with escape handling
      - Operator and punctuation tokenization
      - Error reporting with line and column position
      - Test suite with at least 20 SQL statements
    estimated_hours: 4

  - id: build-sqlite-m2
    name: SQL Parser (AST)
    description: >-
      Build a recursive-descent parser that converts the token stream into
      an Abstract Syntax Tree (AST).
    acceptance_criteria:
      - "SELECT parser produces AST with column list (including *), FROM clause, optional WHERE, ORDER BY, and LIMIT"
      - "INSERT parser produces AST with target table, optional column names, and VALUES clause"
      - "CREATE TABLE parser extracts column definitions with names, types (INTEGER, TEXT, REAL, BLOB), and constraints (PRIMARY KEY, NOT NULL, UNIQUE)"
      - "Expression parser correctly handles operator precedence: NOT > comparison (=,<,>) > AND > OR"
      - "Parenthesized expressions override default precedence"
      - "Parser produces clear error messages with token position for syntax errors"
      - "Test suite covers at least 15 valid and 10 invalid SQL statements"
    pitfalls:
      - "Left recursion in expression grammar causes infinite recursion in recursive-descent parsers—use precedence climbing or Pratt parsing"
      - "AND binds tighter than OR in SQL (unlike some programming languages)—get this wrong and WHERE clauses evaluate incorrectly"
      - "Not handling parenthesized sub-expressions breaks complex WHERE clauses"
      - "NULL is a keyword, not an identifier—treat it specially in expression parsing"
    concepts:
      - Recursive descent parsing uses mutually recursive functions for grammar rules
      - AST represents the syntactic structure of a SQL statement as a tree
      - Operator precedence determines evaluation order of expressions
      - Pratt parsing or precedence climbing handles binary operators elegantly
    skills:
      - Recursive function design
      - Tree data structures
      - Grammar rule encoding
      - Precedence handling
    deliverables:
      - SELECT statement parser producing AST
      - INSERT statement parser producing AST
      - CREATE TABLE parser producing AST with column definitions and constraints
      - Expression parser with correct operator precedence
      - Error reporting with token position for parse errors
      - Test suite for valid and invalid SQL inputs
    estimated_hours: 7

  - id: build-sqlite-m3
    name: Bytecode Compiler (VDBE)
    description: >-
      Compile parsed AST into bytecode instructions executed by a virtual
      machine. This is the execution engine of the database.
    acceptance_criteria:
      - "Compiler translates SELECT AST into a bytecode program with opcodes for OpenTable, Rewind, Column, ResultRow, Next, Halt"
      - "Compiler translates INSERT AST into bytecode with opcodes for OpenTable, MakeRecord, Insert, Halt"
      - "Virtual machine executes bytecode programs step-by-step, processing one opcode per cycle"
      - "VM maintains a register file (array of typed values) for intermediate computation"
      - "EXPLAIN command outputs the bytecode program for a given SQL statement in human-readable format"
      - "WHERE clause compiles to conditional jump opcodes that skip non-matching rows"
      - "Bytecode execution of 'SELECT * FROM t' on a 10,000-row table completes in under 100ms"
    pitfalls:
      - "Directly interpreting AST nodes (tree-walking interpreter) is simpler but significantly slower than bytecode—commit to bytecode"
      - "Register allocation must handle nested expressions without clobbering intermediate values"
      - "Opcodes for cursor management (open, rewind, next, close) must match the storage engine's iterator interface"
      - "Missing a Halt opcode causes the VM to run past the end of the program into garbage memory"
    concepts:
      - Bytecode compilation translates high-level AST into low-level instruction sequence
      - Virtual machine executes bytecode using a fetch-decode-execute loop
      - Register-based VM uses a register file for operand storage (vs stack-based)
      - Cursor opcodes abstract B-tree traversal for the VM
    skills:
      - Bytecode generation
      - Virtual machine implementation
      - Register allocation
      - Instruction set design
    deliverables:
      - Bytecode instruction set with opcodes for table operations, comparisons, jumps, and output
      - Compiler translating SELECT, INSERT, CREATE TABLE ASTs into bytecode
      - Virtual machine executing bytecode with register file and program counter
      - EXPLAIN command displaying bytecode for any SQL statement
      - WHERE clause compilation to conditional jumps
    estimated_hours: 10

  - id: build-sqlite-m4
    name: Buffer Pool Manager
    description: >-
      Implement a page cache that sits between the B-tree layer and disk,
      managing fixed-size pages with LRU eviction and dirty page tracking.
    acceptance_criteria:
      - "Buffer pool manages a configurable number of in-memory page frames (default 1000 pages)"
      - "Pages are fixed-size (4096 bytes by default, configurable)"
      - "FetchPage loads a page from disk into a free frame, or returns the cached frame if already resident"
      - "LRU eviction selects the least recently used unpinned page for replacement when no free frames exist"
      - "Dirty page tracking marks pages modified in memory; eviction writes dirty pages to disk before replacement"
      - "Pin/Unpin mechanism prevents eviction of pages currently in use by B-tree operations"
      - "FlushAll writes all dirty pages to disk (used before checkpoint or shutdown)"
      - "Buffer pool hit rate is measurable and logged for performance tuning"
    pitfalls:
      - "Evicting a pinned page causes data corruption or use-after-free—enforce pin counting"
      - "Not flushing dirty pages before eviction loses committed data"
      - "Page ID collisions if page numbering is not globally unique across the database file"
      - "Buffer pool deadlocks when B-tree operations pin too many pages simultaneously—set pin limits"
    concepts:
      - Buffer pool caches disk pages in memory for fast access
      - LRU (Least Recently Used) eviction approximates optimal page replacement
      - Pin counting prevents eviction of actively-used pages
      - Dirty page tracking enables write-back caching
    skills:
      - Page cache implementation
      - LRU eviction algorithm
      - Pin/unpin lifecycle management
      - Disk I/O management
    deliverables:
      - Buffer pool with configurable frame count and page size
      - FetchPage loading pages from disk or returning cached frames
      - LRU eviction selecting least recently used unpinned page
      - Dirty page tracking and write-back on eviction
      - Pin/Unpin API for B-tree layer
      - FlushAll for shutdown and checkpoint
    estimated_hours: 8

  - id: build-sqlite-m5
    name: B-tree Page Format & Table Storage
    description: >-
      Implement the on-disk page structure for B-trees (tables) and B+trees
      (indexes), with row serialization and node splitting.
    acceptance_criteria:
      - "Page header contains page type (leaf/internal, table/index), cell count, free space pointer, and right-child pointer (internal only)"
      - "Table B-tree leaf pages store rows keyed by rowid with variable-length record encoding"
      - "Table B-tree internal pages store rowid separator keys and child page numbers"
      - "Index B+tree leaf pages store (indexed column value, rowid) pairs with data only in leaves"
      - "Index B+tree internal pages store only separator keys and child pointers (no row data)"
      - "CREATE TABLE creates a B-tree root page and records the schema in a system catalog (sqlite_master equivalent)"
      - "INSERT serializes a row and inserts into the correct B-tree leaf; node splitting creates a new page and promotes a separator key"
      - "Full table scan traverses all leaf pages in rowid order, returning all rows"
      - "Pages serialize to and deserialize from exactly 4096-byte buffers via the buffer pool"
    pitfalls:
      - "Conflating B-tree (data in all nodes) with B+tree (data only in leaves)—tables use B-tree, indexes use B+tree in SQLite"
      - "Cell overflow when a row exceeds page capacity requires overflow pages—handle or document the size limit"
      - "Page fragmentation after deletions wastes space—track free space within pages"
      - "Endianness must be consistent between write and read (SQLite uses big-endian for portability)"
      - "Variable-length integer encoding (varint) must handle the full range of 64-bit integers"
    concepts:
      - B-tree stores key-value pairs in all nodes (used for rowid-keyed tables)
      - B+tree stores data only in leaf nodes with linked leaf pages (used for indexes)
      - Slotted page format uses cell pointers for variable-length records
      - Node splitting maintains B-tree balance on insert overflow
      - System catalog stores table and index schema metadata
    skills:
      - Binary page format design
      - B-tree/B+tree implementation
      - Variable-length record encoding
      - Node splitting algorithms
    deliverables:
      - Page format with header, cell pointer array, and cell content area
      - Table B-tree with leaf (row storage) and internal (separator + child pointer) pages
      - Index B+tree with leaf (key + rowid) and internal (separator + child pointer) pages
      - Row serialization with variable-length encoding for column values
      - Node splitting on insert overflow with separator key promotion
      - System catalog table storing schema metadata
      - Full table scan via leaf page traversal
    estimated_hours: 12

  - id: build-sqlite-m6
    name: SELECT Execution & DML
    description: >-
      Execute SELECT, INSERT, UPDATE, and DELETE via the bytecode VM,
      with row deserialization, projection, and filtering.
    acceptance_criteria:
      - "SELECT * FROM table returns all rows in rowid order via B-tree leaf scan"
      - "SELECT col1, col2 returns only specified columns (projection)"
      - "WHERE clause filters rows during scan, evaluating boolean expressions on deserialized column values"
      - "INSERT adds a row to the B-tree; subsequent SELECT returns the inserted data"
      - "UPDATE modifies columns in rows matching WHERE; subsequent SELECT reflects changes"
      - "DELETE removes rows matching WHERE; subsequent SELECT no longer returns them"
      - "NOT NULL constraint rejects INSERT or UPDATE setting a NOT NULL column to null"
      - "Operations on non-existent tables return an error with the table name"
    pitfalls:
      - "Column name case sensitivity: SQL standard is case-insensitive for identifiers; be consistent"
      - "NULL handling in WHERE: NULL = NULL evaluates to NULL (falsy), not TRUE—use IS NULL for null checks"
      - "B-tree rebalancing after DELETE is complex; initially, mark rows as deleted and reclaim space during compaction"
      - "Updating the primary key (rowid) requires delete + re-insert at the new position"
      - "Memory management for large result sets—stream results row-by-row, don't buffer all in memory"
    concepts:
      - Cursor pattern abstracts B-tree traversal for the VM
      - Projection selects a subset of columns from each row
      - Predicate evaluation filters rows during scan
      - Three-valued logic (TRUE, FALSE, NULL) for SQL boolean expressions
    skills:
      - B-tree cursor implementation
      - Row deserialization
      - Expression evaluation
      - DML execution
    deliverables:
      - Table scan operator iterating all rows via B-tree cursor
      - Row deserialization from binary page format to typed column values
      - Column projection selecting specified fields
      - WHERE clause evaluation with three-valued logic
      - INSERT, UPDATE, DELETE execution via bytecode VM
      - NOT NULL and UNIQUE constraint enforcement during writes
    estimated_hours: 10

  - id: build-sqlite-m7
    name: Secondary Indexes
    description: >-
      Implement secondary indexes using B+trees and integrate index lookups
      into query execution.
    acceptance_criteria:
      - "CREATE INDEX builds a B+tree index mapping (indexed column value → rowid) from existing table data"
      - "Index is automatically maintained on INSERT, UPDATE, and DELETE (index entries added/removed/updated)"
      - "Index lookup retrieves rows matching an equality predicate without full table scan, verified by counting pages read"
      - "Range scan on index returns rows within a value range using B+tree leaf traversal"
      - "Query execution uses index scan when an indexed column appears in WHERE with equality or range predicate"
      - "UNIQUE index rejects INSERT or UPDATE that would create duplicate values"
    pitfalls:
      - "Forgetting to update indexes on INSERT/UPDATE/DELETE causes stale or missing index entries"
      - "Index on a column with many NULL values—NULLs must be handled consistently (SQLite allows multiple NULLs in UNIQUE index)"
      - "Composite indexes (multi-column) require careful key comparison—leftmost prefix must match for index to be useful"
      - "Index maintenance overhead can make writes slower; only create indexes that benefit read patterns"
    concepts:
      - Secondary index maps indexed column values to primary keys (rowids)
      - B+tree leaf traversal enables efficient range scans
      - Index maintenance ensures indexes stay consistent with table data
      - Covering index can satisfy a query without table lookup if all needed columns are in the index
    skills:
      - B+tree index implementation
      - Index maintenance on DML
      - Index scan execution
      - Unique constraint enforcement
    deliverables:
      - CREATE INDEX building B+tree from existing table data
      - Index maintenance on INSERT, UPDATE, DELETE
      - Index equality lookup avoiding full table scan
      - Index range scan using B+tree leaf traversal
      - UNIQUE index constraint enforcement
    estimated_hours: 8

  - id: build-sqlite-m8
    name: Query Planner & Statistics
    description: >-
      Implement a cost-based query planner that chooses between table scan
      and index scan based on collected statistics.
    acceptance_criteria:
      - "ANALYZE command collects statistics: row count per table, distinct value count per indexed column"
      - "Cost model estimates pages read for full table scan (total_pages) and index scan (estimated_rows / rows_per_page)"
      - "Planner selects index scan when estimated selectivity (matching_rows / total_rows) is below a threshold (e.g., 20%)"
      - "Planner falls back to table scan when no suitable index exists or selectivity is too low"
      - "EXPLAIN shows the chosen plan including scan type, index name (if used), and estimated row count"
      - "For multi-table queries (JOIN), planner estimates join cardinality and selects join order to minimize intermediate result size"
    pitfalls:
      - "Without statistics (before ANALYZE), the planner has no data for cost estimation—use default assumptions (e.g., assume 1M rows)"
      - "Stale statistics after many inserts/deletes cause the planner to choose suboptimal plans—recommend periodic ANALYZE"
      - "Cardinality estimation errors compound through joins—a 10x error in one table becomes 100x after a two-table join"
      - "Plan search space explodes exponentially with number of tables in JOIN—limit to dynamic programming for ≤10 tables"
    concepts:
      - Cost-based optimization compares estimated cost of alternative plans
      - Cardinality estimation predicts the number of rows each operator produces
      - Selectivity is the fraction of rows matching a predicate
      - Statistics collection (ANALYZE) provides data for cost estimation
      - Dynamic programming-based join ordering for multi-table queries
    skills:
      - Statistics collection
      - Cost model design
      - Cardinality estimation
      - Plan enumeration
    deliverables:
      - ANALYZE command collecting table and index statistics
      - Cost model estimating I/O for table scan vs index scan
      - Plan selection choosing cheapest access path per table
      - EXPLAIN command displaying chosen plan with cost estimates
      - Join order optimization for multi-table queries
    estimated_hours: 10

  - id: build-sqlite-m9
    name: Transactions (Rollback Journal)
    description: >-
      Implement ACID transactions using a rollback journal for crash recovery.
    acceptance_criteria:
      - "BEGIN starts a transaction; all subsequent writes are buffered until COMMIT or ROLLBACK"
      - "COMMIT makes all changes permanent by flushing dirty pages and removing the rollback journal"
      - "ROLLBACK undoes all changes by restoring original pages from the rollback journal"
      - "Rollback journal records original page contents BEFORE modification (for undo on crash)"
      - "Changes are not visible to other connections until COMMIT (basic read isolation)"
      - "Crash recovery on startup detects an existing rollback journal and automatically rolls back the incomplete transaction"
      - "Journal file is fsync'd before modified pages are written to the database file (write ordering guarantee)"
    pitfalls:
      - "Writing modified pages to database before journal is fsync'd causes unrecoverable corruption on crash"
      - "Partial page writes (torn pages) on crash can corrupt the database—journal must contain complete original pages"
      - "Lock ordering between multiple connections must be consistent to prevent deadlocks"
      - "Long-running transactions holding locks block all other writers—document the locking behavior"
    concepts:
      - ACID: Atomicity (rollback journal), Consistency (constraints), Isolation (locking), Durability (fsync)
      - Rollback journal records undo information (original page images) before modification
      - Write ordering: journal fsync → database write → journal delete
      - Crash recovery: hot journal detected → restore original pages → delete journal
    skills:
      - Rollback journal implementation
      - Write ordering enforcement
      - Crash recovery logic
      - Lock management
    deliverables:
      - BEGIN/COMMIT/ROLLBACK command implementation
      - Rollback journal recording original page contents before modification
      - Write ordering ensuring journal is durable before database pages are modified
      - Crash recovery detecting hot journal and restoring database to pre-transaction state
      - ACID guarantee verification test suite
    estimated_hours: 10

  - id: build-sqlite-m10
    name: WAL Mode
    description: >-
      Implement Write-Ahead Logging as an alternative to rollback journal,
      enabling concurrent readers during writes.
    acceptance_criteria:
      - "WAL mode appends modified pages to a separate WAL file instead of modifying the main database file"
      - "Writers append to WAL; readers check WAL for the most recent version of a page before reading from the main database"
      - "Multiple readers can execute queries concurrently while a single writer appends to the WAL"
      - "Checkpoint (PRAGMA wal_checkpoint) copies WAL pages back into the main database file"
      - "WAL checkpoint is required to prevent unbounded WAL growth—auto-checkpoint triggers after configurable page count (default 1000)"
      - "Readers see a consistent snapshot: a reader that starts before a commit does not see that commit's changes (snapshot isolation for reads)"
      - "WAL file corruption is detected via page checksums"
    pitfalls:
      - "WAL grows unbounded without checkpointing—auto-checkpoint is not optional, it's required"
      - "Readers pinning old WAL frames prevent checkpoint from truncating—long-running reads block WAL cleanup"
      - "WAL and rollback journal are mutually exclusive modes—switching requires careful state management"
      - "Checkpoint must not run while readers are using WAL frames that would be overwritten"
      - "WAL file corruption must be detected (checksums) to prevent propagating bad data to the main database"
    concepts:
      - WAL appends redo information (new page images) to a log file
      - Readers use WAL index (wal-index) to find most recent page version
      - Checkpoint merges WAL changes back into the main database file
      - Snapshot isolation: each reader sees the database as of its start time
      - WAL vs rollback journal: WAL enables concurrent readers, rollback journal does not
    skills:
      - Write-ahead log implementation
      - Snapshot isolation for readers
      - Checkpoint algorithm
      - Concurrent read/write coordination
    deliverables:
      - WAL file format appending modified pages with checksums
      - WAL reader looking up most recent page version before main database
      - Checkpoint process copying WAL pages into main database
      - Auto-checkpoint triggered by WAL page count threshold
      - Concurrent reader support during active write transactions
      - WAL mode toggle (PRAGMA journal_mode=WAL)
    estimated_hours: 12

  - id: build-sqlite-m11
    name: Aggregate Functions & JOIN
    description: >-
      Implement aggregate functions (COUNT, SUM, AVG, MIN, MAX), GROUP BY,
      and basic JOIN execution.
    acceptance_criteria:
      - "COUNT(*) returns the number of rows; COUNT(col) returns count of non-NULL values"
      - "SUM, AVG, MIN, MAX produce correct results over grouped and ungrouped queries"
      - "GROUP BY groups rows by specified columns before applying aggregate functions"
      - "HAVING filters groups after aggregation"
      - "INNER JOIN combines rows from two tables matching a join condition"
      - "Nested loop join is implemented as the baseline join algorithm"
      - "JOIN with WHERE clause filters correctly after join"
    pitfalls:
      - "AVG must handle integer division correctly (return REAL, not INTEGER)"
      - "NULL handling in aggregates: COUNT(*) counts NULLs, COUNT(col) does not; SUM/AVG ignore NULLs"
      - "GROUP BY without aggregate function is valid SQL but confusing—handle correctly"
      - "Nested loop join is O(n*m)—acceptable for small tables but document the limitation"
    concepts:
      - Aggregate functions accumulate values across row groups
      - GROUP BY partitions rows into groups for aggregation
      - Nested loop join iterates all combinations of rows from two tables
      - HAVING filters groups after aggregation (vs WHERE which filters before)
    skills:
      - Aggregate computation
      - Group-by execution
      - Join algorithms
      - NULL handling in aggregates
    deliverables:
      - COUNT, SUM, AVG, MIN, MAX aggregate functions
      - GROUP BY execution with hash-based or sort-based grouping
      - HAVING clause filtering groups after aggregation
      - Nested loop INNER JOIN execution
      - Test suite covering aggregates with NULLs, empty tables, and multi-table joins
    estimated_hours: 14