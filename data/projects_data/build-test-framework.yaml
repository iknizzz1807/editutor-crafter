id: build-test-framework
name: Build Your Own Test Framework
description: A pytest/jest clone with AST-based assertion rewriting, fixture dependency injection, parallel execution, and plugin architecture.
difficulty: expert
estimated_hours: 50-70
essence: 'Test discovery through reflection and module introspection, AST-based assertion rewriting for rich failure diagnostics, fixture dependency injection with scoped lifecycle management, pluggable test execution with hooks, and parallel runner orchestration with process-level isolation.

  '
why_important: 'Building a test framework reveals how metaprogramming, code introspection, and AST manipulation work in real tools millions of developers rely on daily, while teaching advanced software architecture patterns like plugin systems, dependency injection, and reporter abstractions that apply across many domains.

  '
learning_outcomes:
- Implement reflection-based test discovery scanning modules for test functions
- Design AST-based assertion rewriting that captures sub-expression values in failure messages
- Build fixture dependency graphs with automatic resolution, scoped lifecycle, and cleanup ordering
- Create pluggable architecture with hooks for reporters, plugins, and extensions
- Implement parallel test execution with process-level isolation and result aggregation
- Parse and execute CLI arguments for test filtering, verbosity, markers, and configuration
- Generate structured test reports in multiple formats (JSON, JUnit XML, terminal)
- Handle test lifecycle events including global hooks, setup, teardown, and error recovery
skills:
- Abstract Syntax Trees
- Code Introspection / Reflection
- Plugin Architecture
- Dependency Injection
- Process-based Parallelization
- CLI Design
- Reporter Patterns
- Metaprogramming
tags:
- assertions
- build-from-scratch
- expert
- fixtures
- framework
- go
- javascript
- mocking
- python
- runners
- testing
architecture_doc: architecture-docs/build-test-framework/index.md
languages:
  recommended:
  - Python
  - JavaScript
  - Go
  also_possible:
  - Rust
  - Java
resources:
- type: repository
  name: pytest source
  url: https://github.com/pytest-dev/pytest
- type: article
  name: Building a Test Framework
  url: https://www.destroyallsoftware.com/screencasts/catalog/building-a-test-framework
- type: article
  name: pytest assertion rewriting internals
  url: https://docs.pytest.org/en/latest/how-to/assert.html
prerequisites:
- type: skill
  name: Reflection/metaprogramming
- type: skill
  name: Abstract Syntax Trees (basic understanding)
- type: skill
  name: CLI development
- type: skill
  name: Process management (fork/subprocess)
milestones:
- id: build-test-framework-m1
  name: Test Discovery & Basic Execution
  description: 'Discover test functions by scanning file system paths and dynamically importing modules, then execute each discovered test in isolation recording pass/fail/error.

    '
  acceptance_criteria:
  - 'Discovery recursively finds all files matching a configurable glob pattern (default: test_*.py or *.test.js)'
  - Within each module, discovery collects all functions/methods matching a naming convention (e.g., test_ prefix) using language reflection APIs
  - Runner executes each test function and categorizes the outcome as PASSED, FAILED (assertion error), or ERROR (unexpected exception)
  - 'Each test runs in isolation: module-level global state is reset or re-imported so one test''s side effects do not affect subsequent tests'
  - Tests can be marked as skip or expected-failure (xfail) via decorators/markers, and the runner respects these markers in its output
  - A test that exceeds a configurable timeout (default 30s) is terminated and reported as ERROR with a timeout message
  pitfalls:
  - 'Module import side effects: modules with top-level code (e.g., database connections, print statements) execute during discovery, polluting state or causing import failures. Guard discovery with try/except and report import errors clearly.'
  - 'Path handling: relative vs absolute paths, sys.path manipulation, and package __init__.py files cause discovery to silently miss tests. Normalize all paths before import.'
  - 'Test ordering assumptions: tests that pass only when run in a specific order indicate shared state leakage. Randomize execution order to surface this.'
  concepts:
  - Reflection and dynamic module loading
  - File system traversal
  - Test isolation strategies
  skills:
  - File system traversal and glob patterns
  - Dynamic module importing (importlib / require / reflection)
  - Decorator/marker pattern for metadata annotation
  - Timeout enforcement via signals or subprocess
  deliverables:
  - Test discovery engine scanning directories for test modules and collecting test functions
  - Test runner executing discovered tests and collecting PASSED/FAILED/ERROR results
  - Skip and xfail marker support with correct result categorization
  - Per-test timeout enforcement
  estimated_hours: 8-12
- id: build-test-framework-m2
  name: Assertions & Rich Failure Reporting
  description: 'Implement an assertion library with AST-based introspection that captures sub-expression values on failure, plus domain-specific matchers.

    '
  acceptance_criteria:
  - assertEqual(a, b) on failure displays both the expression text (variable names) and their evaluated values using AST rewriting or frame inspection — not just the values
  - For container types (list, dict, set), failure messages include a unified diff showing exactly which elements differ
  - assertAlmostEqual supports configurable absolute and relative tolerances for floating-point comparison
  - assertRaises (or equivalent context manager) fails with a clear message if the expected exception type is not raised, and captures the exception instance for further assertions
  - 'Custom matcher API: users can register predicate functions with custom failure message formatters, and the framework calls them seamlessly within assertion chains'
  - Assertion failure output includes the source file, line number, and the failing source line for immediate developer orientation
  pitfalls:
  - Naive assertion messages that show only 'True != False' are useless. The entire value of assertion rewriting is showing intermediate values like 'where x.name = alice and y.name = bob'.
  - Float comparison using == is a classic bug. Always require tolerance-based comparison for floating-point assertions.
  - AST rewriting must handle multi-line expressions, f-strings, and complex boolean expressions without corrupting the source. Test the rewriter on its own output.
  - 'Exception context loss: catching exceptions without preserving __cause__ or __context__ (Python) destroys the traceback chain.'
  concepts:
  - AST parsing and rewriting
  - Frame inspection for variable capture
  - Diff algorithms (unified diff)
  - Context managers for exception assertion
  skills:
  - AST manipulation (ast module in Python, Babel in JS, go/ast in Go)
  - Deep object comparison and structural diffing
  - Custom assertion DSL design
  - String formatting with ANSI color codes
  deliverables:
  - AST-based assertion rewriter that instruments assert statements to capture sub-expression values
  - Standard assertion functions (assertEqual, assertTrue, assertFalse, assertAlmostEqual, assertRaises)
  - Collection assertion with unified diff output
  - Custom matcher registration API with user-defined predicates and messages
  estimated_hours: 10-14
- id: build-test-framework-m3
  name: Fixtures, Hooks & Dependency Injection
  description: 'Implement a fixture system with scoped lifecycle (function, class, module, session), dependency injection by parameter name, automatic teardown ordering, and global before/after hooks.

    '
  acceptance_criteria:
  - Function-scoped fixtures are created fresh before each test and torn down after, even if the test fails or errors
  - Module-scoped fixtures are created once when the first test in the module requests them and torn down after the last test in that module completes
  - Session-scoped fixtures (beforeAll/afterAll equivalent) are created once per test run and torn down after all tests complete, suitable for expensive resources like database connections or server startup
  - Fixtures are injected into test functions by matching parameter names to registered fixture names — no manual lookup or global state required
  - Fixtures can depend on other fixtures; the framework resolves the dependency DAG and creates them in topological order, tearing down in reverse order
  - Circular fixture dependencies are detected at collection time and reported as a configuration error before any test executes
  - 'Generator/yield-based fixtures support: code before yield is setup, code after yield is teardown, and teardown runs even on test failure'
  - Global hooks (beforeEach, afterEach, beforeAll, afterAll) can be registered via plugin API and execute around the appropriate scope
  pitfalls:
  - 'Teardown not running on exception: if teardown is in a finally block or generator finalization is missing, resources leak on failure.'
  - 'Scope leaks: a function-scoped test accidentally mutating a module-scoped fixture''s internal state corrupts all subsequent tests sharing that fixture. Defensive copying or immutability checks are essential.'
  - Topological sort must handle diamonds (A depends on B and C, both depend on D) without creating D twice.
  - 'Session-scoped fixture teardown at process exit: if the runner crashes, cleanup may never run. Provide atexit hooks as a safety net.'
  concepts:
  - Dependency injection
  - Directed acyclic graph resolution
  - Resource lifecycle management
  - Generator-based setup/teardown
  skills:
  - Topological sort implementation
  - Function signature introspection for parameter names
  - Context managers and generator protocols
  - Scope management and lifetime tracking
  deliverables:
  - Fixture registry with function, module, and session scopes
  - Dependency injection engine resolving fixtures by test function parameter names
  - DAG-based fixture resolution with circular dependency detection
  - Generator/yield fixture support with guaranteed teardown
  - Global beforeAll/afterAll and beforeEach/afterEach hook registration
  estimated_hours: 12-18
- id: build-test-framework-m4
  name: Reporting, CLI & Parameterization
  description: 'Build the CLI interface, multiple output reporters, parameterized test support, and structured report generation for CI integration.

    '
  acceptance_criteria:
  - 'CLI accepts arguments for: file/directory paths, name-based filter patterns (-k), marker-based selection (-m), verbosity level, output format, parallelism count, and fail-fast flag'
  - Terminal reporter displays each test with PASSED/FAILED/SKIPPED/XFAIL status, execution duration, and a summary line with counts and total wall-clock time
  - On failure, the terminal reporter prints the assertion introspection output (from M2) with source location, variable values, and diff
  - JUnit XML output is schema-valid and correctly parsed by Jenkins, GitHub Actions, and GitLab CI (validate against the JUnit XSD)
  - Parameterized test decorator/annotation generates N test cases from a parameter list, each reported as a distinct test with its parameter values in the name
  - Exit code is 0 if all tests pass (or xfail), 1 if any test fails, and 2 for usage/configuration errors
  - 'JSON report output includes structured data for every test: name, outcome, duration, failure message, fixture setup times'
  pitfalls:
  - 'Exit codes: many CI systems only check exit code. Returning 0 on failure silently breaks CI pipelines.'
  - 'XML escaping: test names or failure messages containing <, >, &, or CDATA-breaking sequences corrupt JUnit XML. Use proper XML serialization, not string concatenation.'
  - 'Terminal detection: piping output to a file while using ANSI color codes produces garbage. Detect isatty() and strip colors when not a terminal.'
  - 'Parameterized test IDs: parameters containing special characters (slashes, brackets) break test selection filters. Sanitize parameter representations in test IDs.'
  concepts:
  - CLI argument parsing
  - Reporter/formatter pattern
  - Test parameterization
  - Structured output formats
  skills:
  - Argument parsing libraries (argparse, commander, cobra)
  - XML serialization with proper escaping
  - JSON structured output
  - Terminal capability detection (isatty, TERM)
  - Test ID generation and filtering
  deliverables:
  - CLI interface with filtering, verbosity, format selection, and fail-fast options
  - Terminal reporter with color-coded pass/fail, durations, and failure details
  - JUnit XML reporter validated against the JUnit XSD schema
  - JSON reporter with structured test result data
  - Parameterized test support generating distinct test cases from input data
  estimated_hours: 10-14
- id: build-test-framework-m5
  name: Parallel Execution & Plugin Architecture
  description: 'Implement parallel test execution using process-level isolation, and build a plugin/hook system allowing third-party extensions for discovery, execution, and reporting.

    '
  acceptance_criteria:
  - Parallel mode spawns N worker processes (configurable via CLI --workers flag), each executing a disjoint subset of collected tests
  - Each worker process independently imports test modules, ensuring complete memory isolation — a segfault or OOM in one worker does not crash others
  - Results from all workers are aggregated into a single unified report as if tests ran sequentially
  - Parallel execution achieves at least 2x speedup on a suite of 100+ CPU-bound tests when run with 4 workers vs. 1 worker (demonstrate with benchmark)
  - 'Plugin hook API exposes lifecycle events: collection_start, test_start, test_end, test_skip, collection_end, report_generation with ability to register multiple listeners'
  - A sample plugin (e.g., code coverage reporter or slow-test highlighter) is implemented to validate the hook API
  - 'Fixtures with module or session scope are correctly handled in parallel mode: either each worker gets its own instance, or the framework detects and warns about scope conflicts'
  pitfalls:
  - 'Process forking on macOS/Windows: fork() behavior differs across platforms. Use subprocess spawning (not fork) for portability.'
  - 'Serialization of test results: results must cross process boundaries via IPC (pipes, sockets, or files). Complex objects (exceptions with tracebacks) may not serialize cleanly — serialize a simplified representation.'
  - 'Session-scoped fixtures in parallel mode: if each worker creates its own session fixture, the semantics change. Document this clearly or implement a coordinator process.'
  - 'stdout/stderr interleaving: parallel workers writing to the same terminal produce garbled output. Buffer output per-test and flush atomically.'
  - 'Plugin ordering: multiple plugins hooking the same event need a defined execution order (priority system or registration order).'
  concepts:
  - Process-based parallelism
  - Inter-process communication
  - Plugin/hook architecture
  - Event-driven lifecycle
  skills:
  - Subprocess management and IPC
  - Work distribution and load balancing
  - Hook/event system design
  - Result serialization across process boundaries
  - Platform-aware process spawning
  deliverables:
  - Parallel test runner with configurable worker count and process-level isolation
  - Result aggregation combining per-worker results into unified output
  - Plugin hook API with lifecycle event registration
  - At least one example plugin demonstrating the hook API
  - Benchmark demonstrating parallel speedup
  estimated_hours: 12-16
domain: software-engineering
