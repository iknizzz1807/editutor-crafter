id: cache-optimized-structures
name: Cache-Optimized Data Structures
description: Cache-oblivious algorithms, B+ trees with van Emde Boas layout, memory layout transformations, and blocked matrix operations.
difficulty: advanced
estimated_hours: 45
essence: Minimizing cache misses through memory layout transformations and cache-oblivious algorithmic design that exploits spatial and temporal locality across the CPU memory hierarchy without requiring hardware-specific cache size parameters.
why_important: Understanding cache behavior is critical for writing high-performance systems code - performance differences between cache-friendly and cache-hostile implementations can easily be 10-100x. This project teaches low-level optimization skills essential for systems programming, game engines, databases, and high-performance computing.
learning_outcomes:
- Measure cache performance using hardware counters and profiling tools to identify cache misses and bottlenecks
- Implement AoS to SoA memory layout transformations to improve cache line utilization and SIMD vectorization
- Design open-addressed hash tables with linear probing optimized for cache locality
- Build cache-oblivious B-trees using van Emde Boas recursive layout
- Apply loop tiling and blocking to matrix operations for optimal cache data reuse
- Analyze memory access patterns and refactor for spatial and temporal locality
- Debug false sharing, alignment issues, and prefetching failures
skills:
- Cache profiling with perf/cachegrind
- Memory layout optimization
- Data-oriented design
- Cache-oblivious algorithms
- SIMD and vectorization
- Software prefetching
- Low-level performance tuning
tags:
- advanced
- alignment
- c
- c++
- cache-lines
- caching
- prefetching
- rust
- performance
architecture_doc: architecture-docs/cache-optimized-structures/index.md
languages:
  recommended:
  - C
  - C++
  - Rust
  also_possible:
  - Go
  - Zig
resources:
- name: What Every Programmer Should Know About Memory
  url: https://people.freebsd.org/~lstewart/articles/cpumemory.pdf
  type: paper
- name: Cache-Oblivious Algorithms
  url: https://en.wikipedia.org/wiki/Cache-oblivious_algorithm
  type: article
- name: Agner Fog's Optimization Manuals
  url: https://www.agner.org/optimize/
  type: documentation
prerequisites:
- type: skill
  name: Data structures (arrays, trees, hash tables)
- type: skill
  name: Understanding of CPU cache hierarchy (L1/L2/L3)
- type: skill
  name: Memory alignment and struct layout concepts
- type: skill
  name: Performance profiling basics (perf, cachegrind, or equivalent)
milestones:
- id: cache-optimized-structures-m1
  name: Cache Fundamentals & Benchmarking
  description: Understand cache behavior and build benchmarking tools to measure cache performance across the memory hierarchy.
  acceptance_criteria:
  - Benchmark detects L1, L2, and L3 cache sizes by measuring access latency inflection points at different working set sizes
  - Sequential access benchmark demonstrates measurably faster throughput than random access for arrays exceeding L1 cache size
  - Cache line stride benchmark shows constant access time for strides up to cache line size (typically 64 bytes) then increasing latency for larger strides
  - Hardware performance counters (perf stat or equivalent) report L1, L2, and L3 cache miss counts for each benchmark
  - Results are documented with annotated graphs showing clear latency/throughput transitions at cache boundaries
  - All benchmarks include warmup iterations, multiple runs, and statistical reporting (mean, stddev, min, max)
  pitfalls:
  - Compiler optimizations can eliminate memory accesses entirely - use volatile reads or compiler barriers to prevent dead code elimination
  - Warmup effects cause the first few iterations to be unrepresentative - discard initial iterations and run sufficient repetitions
  - System noise (interrupts, scheduler, other processes) causes variance - pin to a single core and use CLOCK_MONOTONIC for timing
  - Modern CPUs have aggressive hardware prefetchers that can mask sequential access patterns - use pointer-chasing for true random access measurement
  - TLB misses can dominate for very large working sets, confounding cache miss measurements - use huge pages to isolate cache effects
  concepts:
  - CPU cache hierarchy (L1d, L1i, L2, L3) with typical sizes and latencies
  - Cache line as the unit of transfer (typically 64 bytes)
  - Spatial locality (accessing nearby addresses) vs temporal locality (reusing recently accessed addresses)
  - Hardware prefetching and its impact on sequential access performance
  - TLB (Translation Lookaside Buffer) and its interaction with large working sets
  skills:
  - Performance profiling with perf/cachegrind
  - Memory access pattern analysis
  - Cache miss measurement
  - Statistical benchmarking methodology
  deliverables:
  - Cache hierarchy size detection tool using latency-based measurement
  - Sequential vs random access comparison benchmark with annotated results
  - Cache line stride benchmark demonstrating cache line granularity
  - Hardware performance counter collection for L1/L2/L3 miss rates
  - Documented results with graphs showing cache boundary transitions
  estimated_hours: 7
- id: cache-optimized-structures-m2
  name: Array of Structs vs Struct of Arrays
  description: Implement and compare AoS vs SoA memory layouts, demonstrating cache utilization and SIMD vectorization differences.
  acceptance_criteria:
  - AoS particle system stores all fields (x, y, z, vx, vy, vz, mass) per particle in a contiguous struct array
  - SoA particle system stores each field in its own separate contiguous array
  - SoA measurably outperforms AoS when a hot loop updates only position fields (x, y, z) without accessing velocity fields
  - AoS performs comparably or better when all fields of each particle are accessed together in a single loop
  - Compiler output (assembly or LLVM IR) demonstrates that SoA layout enables SIMD auto-vectorization that AoS does not
  - Cache miss profiling (cachegrind or perf) quantifies the difference in L1 cache misses between the two layouts for partial-field access
  pitfalls:
  - Not aligning SoA arrays to cache line boundaries (64 bytes) or SIMD register width (16/32/64 bytes) prevents vectorization
  - SIMD loops must handle the remainder when array length is not a multiple of SIMD width - off-by-one causes out-of-bounds
  - Over-optimizing with SoA when all fields are always accessed together adds complexity for no cache benefit
  - Struct padding in AoS wastes cache line space - know your compiler's padding rules and use __attribute__((packed)) carefully
  - Memory allocation alignment for SoA arrays requires aligned_alloc or equivalent, not malloc
  concepts:
  - Data-oriented design vs object-oriented design tradeoffs
  - Memory layout impact on cache line utilization
  - SIMD vectorization opportunity with contiguous same-type data
  - Struct padding and alignment rules
  - Compiler auto-vectorization and how to verify it
  skills:
  - Memory layout transformation
  - SIMD awareness and verification
  - Structure alignment and padding
  - Data-oriented design patterns
  deliverables:
  - AoS particle system with interleaved field storage
  - SoA particle system with separate per-field arrays
  - Partial-field update benchmark (position only) comparing AoS vs SoA
  - Full-field update benchmark comparing both layouts
  - Compiler vectorization report showing SIMD usage in SoA hot loop
  - Cache miss comparison using profiling tools
  estimated_hours: 8
- id: cache-optimized-structures-m3
  name: Cache-Friendly Hash Table
  description: Implement an open-addressed hash table with linear probing and Robin Hood hashing optimized for cache performance.
  acceptance_criteria:
  - Open addressing with linear probing keeps the probe sequence in contiguous memory for cache-friendly sequential scanning
  - Keys and metadata are stored in a separate array from values so that key-only probing during lookup touches fewer cache lines
  - Robin Hood hashing reduces maximum probe distance by displacing entries with shorter probe distances, improving worst-case lookup
  - Robin Hood hashing supports load factors up to 0.85 while maintaining bounded probe distances
  - Benchmark shows fewer cache misses per lookup compared to a separate-chaining hash table at equivalent load factors
  - Software prefetch hints (__builtin_prefetch or equivalent) are used during probing to hide memory latency for the probable next access
  - Resize doubles the table and rehashes all entries when load factor exceeds the configured threshold
  pitfalls:
  - Standard linear probing degrades rapidly above 70% load factor, but Robin Hood hashing pushes this to 85-90% by bounding probe variance
  - Poor hash function quality (clustering) negates all cache optimization - use a high-quality hash (wyhash, xxhash) with good avalanche
  - Resize must rehash every entry; not handling this correctly causes lost entries or infinite loops in probing
  - Tombstones from deletion in linear probing create probe chain fragmentation - consider backward-shift deletion instead
  - Software prefetch too far ahead wastes cache capacity; too close doesn't hide latency - tune the prefetch distance empirically
  concepts:
  - Open addressing vs chaining and their cache behavior differences
  - Linear probing cache locality advantage (sequential memory access)
  - Robin Hood hashing for bounded probe variance at high load
  - Key-value separation for cache-efficient key scanning
  - Software prefetching to hide memory latency
  skills:
  - Hash table implementation
  - Open addressing with Robin Hood
  - Software prefetching
  - Load factor optimization
  deliverables:
  - Open-addressed hash table with linear probing
  - Robin Hood displacement logic maintaining probe distance invariant
  - Separate key/metadata and value arrays for cache-efficient probing
  - Software prefetch hints during probe sequence
  - Dynamic resize with full rehashing at configurable load factor threshold
  - Cache miss comparison benchmark against chaining-based hash map
  estimated_hours: 10
- id: cache-optimized-structures-m4
  name: Cache-Oblivious B-Tree
  description: Implement a van Emde Boas layout B-tree that achieves optimal cache behavior without knowing cache parameters.
  acceptance_criteria:
  - Van Emde Boas layout recursively divides the tree and places co-accessed subtrees in contiguous memory regions
  - Search achieves O(log_B N) cache misses where B is the number of tree nodes per cache line, without the algorithm knowing B at compile or run time
  - vEB layout measurably outperforms pointer-based binary search tree for tree sizes exceeding L2 cache capacity
  - Index arithmetic correctly maps logical tree positions to physical memory addresses in the vEB layout
  - Performance is measured on at least two different hardware configurations (or simulated via different data sizes) showing consistent improvement
  - Cache miss count is measured via profiling tools confirming the theoretical access pattern improvement
  pitfalls:
  - vEB index calculations are complex and error-prone - off-by-one errors cause incorrect tree traversal or out-of-bounds access
  - Non-power-of-2 tree sizes require careful handling at recursion boundaries; padding to the next power of 2 wastes memory
  - For small trees (fitting in L1/L2 cache), the vEB layout overhead exceeds its benefit - there is a crossover point
  - Insert and delete operations are significantly more complex in vEB layout compared to pointer-based trees; focus on static trees first
  - The constant factor matters - vEB layout's benefit is in reducing cache misses, but the index arithmetic adds CPU instructions per node access
  concepts:
  - Cache-oblivious algorithm design principles
  - van Emde Boas recursive memory layout
  - O(log_B N) cache complexity without knowing B
  - Static vs dynamic tree operations in implicit array layout
  - Crossover point where vEB outperforms standard layout
  skills:
  - Recursive memory layout design
  - Index arithmetic for implicit trees
  - Cache-oblivious algorithm analysis
  - Comparative benchmarking
  deliverables:
  - Van Emde Boas layout construction from sorted data into cache-optimal array order
  - Index mapping function converting logical tree position to physical array index
  - Search implementation traversing vEB layout with minimal cache misses
  - Performance comparison against pointer-based BST and sorted array binary search
  - Cache miss measurement confirming theoretical improvement for large trees
  estimated_hours: 12
- id: cache-optimized-structures-m5
  name: Blocked Matrix Operations
  description: Implement cache-blocked matrix multiplication with auto-tuning of block size for optimal cache utilization.
  acceptance_criteria:
  - Naive O(n^3) matrix multiplication serves as baseline with known poor cache behavior for large matrices
  - Blocked (tiled) multiplication achieves measurable speedup (2x minimum) over naive for matrices exceeding L2 cache size
  - Block size auto-tuner empirically tests multiple block sizes and selects the one with minimum execution time
  - Non-block-aligned matrix dimensions are handled correctly with remainder blocks at matrix edges
  - Blocked matrix transpose outperforms naive transpose for matrices larger than L1 cache
  - Cache miss profiling confirms blocked version has fewer L2/L3 misses than naive for equivalent matrix sizes
  pitfalls:
  - Block size larger than L1/L2 cache defeats the purpose - the whole point is that each block fits in cache
  - Not handling non-block-aligned dimensions (matrix size not divisible by block size) causes incorrect results at boundaries
  - Memory alignment of matrix rows affects cache behavior - row-major vs column-major layout matters for access patterns
  - Auto-tuner must measure actual execution time, not just cache misses - the fastest block size balances cache utilization and loop overhead
  - Compiler optimization flags (-O2, -O3) can enable auto-vectorization that changes the optimal block size; benchmark at the target optimization level
  concepts:
  - Loop tiling (blocking) for cache data reuse
  - Working set size vs cache size relationship
  - Auto-tuning by empirical measurement
  - Row-major vs column-major access patterns
  - Interaction between blocking and SIMD vectorization
  skills:
  - Loop tiling and blocking
  - Matrix operation optimization
  - Empirical auto-tuning
  - Handling non-aligned dimensions
  deliverables:
  - Naive matrix multiplication as cache-hostile baseline
  - Blocked matrix multiplication with configurable tile size
  - Block size auto-tuner selecting optimal size via empirical measurement
  - Non-aligned dimension handling for matrices not divisible by block size
  - Blocked matrix transpose with cache performance comparison
  - Cache miss profiling comparing naive vs blocked implementations
  estimated_hours: 8
domain: specialized
