id: ci-cd-pipeline
name: CI/CD Pipeline Builder
description: >-
  Automated build, test, deploy workflows with DAG execution, artifact management,
  secret handling, and deployment strategies with rollback.
difficulty: intermediate
estimated_hours: "40-50"
essence: >-
  Directed acyclic graph (DAG) execution of isolated build/test/deploy jobs with
  dependency resolution, secret injection and masking, artifact lifecycle tracking
  across stages, workspace isolation, and traffic-shifting deployment patterns
  with automated health-based rollback.
why_important: >-
  CI/CD is the backbone of modern software delivery. Understanding pipeline
  internals—DAG scheduling, secret management, artifact integrity, and deployment
  orchestration—makes you better at debugging, optimizing, and securing delivery
  workflows.
learning_outcomes:
  - Design multi-stage pipeline workflows with DAG-based dependency resolution
  - Implement build and test automation with isolated job execution
  - Handle artifact versioning, integrity verification, and storage
  - Manage secrets securely with injection, masking, and rotation
  - Automate deployment with rollback support and health checking
  - Implement workspace cleanup and isolation between job runs
  - Detect and reject circular dependencies with clear error reporting
skills:
  - YAML Pipeline DSL
  - Process Isolation
  - Artifact Versioning
  - Secret Management
  - Deployment Automation
  - Health Check Monitoring
  - Rollback Strategies
  - Concurrent Execution
  - Container Orchestration
  - DAG Cycle Detection
tags:
  - artifacts
  - automation
  - devops
  - infrastructure
  - intermediate
  - runners
  - secrets
  - stages
  - streaming
  - testing
  - triggers
architecture_doc: architecture-docs/ci-cd-pipeline/index.md
languages:
  recommended:
    - Python
    - Go
    - Shell
  also_possible:
    - JavaScript
resources:
  - name: GitLab CI/CD Pipeline Documentation
    url: https://docs.gitlab.com/ci/pipelines/
    type: documentation
  - name: Docker Container Isolation Best Practices
    url: https://snyk.io/blog/best-practices-for-container-isolation/
    type: article
  - name: Blue-Green and Canary Deployment Strategies
    url: https://www.harness.io/blog/blue-green-canary-deployment-strategies
    type: tutorial
  - name: Azure Pipelines YAML Schema Reference
    url: https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/
    type: documentation
prerequisites:
  - type: skill
    name: Shell scripting
  - type: skill
    name: Docker basics
  - type: skill
    name: Git hooks/webhooks
  - type: skill
    name: YAML syntax
milestones:
  - id: ci-cd-pipeline-m1
    name: Pipeline Definition Parser & DAG Validation
    description: >-
      Parse YAML pipeline definitions with stages, jobs, steps, and dependencies.
      Build and validate the dependency DAG, rejecting circular dependencies.
    acceptance_criteria:
      - "Parser reads a YAML pipeline file and produces structured stage/job/step objects with all required fields validated"
      - "Validation rejects malformed definitions with descriptive error messages including line numbers and field names"
      - "Dependency graph is constructed as a DAG; topological sort produces correct parallel execution ordering"
      - "Circular dependencies are detected using DFS-based cycle detection and rejected with an error message listing the cycle path (e.g., 'job-A -> job-B -> job-C -> job-A')"
      - "Missing dependency references (job depends on non-existent job) are rejected with a clear error before execution"
      - "Conditional execution expressions (if/when) are parsed and evaluated per job context, supporting variable references and boolean operators"
      - "Variable interpolation resolves ${VAR} references in step definitions from pipeline-level and environment variables"
    pitfalls:
      - "Topological sort on a cyclic graph does not infinite loop—it fails to produce a complete ordering. The real pitfall is not detecting the cycle upfront and reporting which jobs form the cycle."
      - "YAML anchors and aliases (*anchor, <<: *merge) need explicit handling or they produce unexpected merged structures"
      - "Variable interpolation before validation can mask errors—validate structure first, then interpolate"
      - "Duplicate job names in different stages silently shadow each other—validate uniqueness across the entire pipeline"
    concepts:
      - YAML parsing and schema validation
      - Directed acyclic graph (DAG) construction
      - Topological sorting for dependency resolution
      - DFS-based cycle detection
      - Variable interpolation
    skills:
      - YAML Processing
      - Graph Algorithms (topological sort, cycle detection)
      - Dependency Management
      - Configuration Validation
    deliverables:
      - "YAML pipeline parser extracting stages, jobs, steps, and dependency declarations"
      - "Schema validator checking required fields, types, and cross-references with descriptive errors"
      - "DAG builder constructing dependency graph with cycle detection that reports cycle path on failure"
      - "Variable substitution engine resolving environment and pipeline variables in step definitions"
      - "Conditional expression evaluator for if/when clauses on jobs"
    estimated_hours: "8-10"

  - id: ci-cd-pipeline-m2
    name: Job Executor with Secret Management
    description: >-
      Execute jobs in isolated Docker containers with logging, timeout handling,
      exit code propagation, workspace cleanup, and secure secret injection.
    acceptance_criteria:
      - "Each job step executes in an isolated Docker container with the specified image; the container is removed after step completion regardless of success or failure"
      - "Step logs are streamed in real-time to pipeline output; secret values appearing in stdout/stderr are masked with '***' before display"
      - "Secrets are injected into containers as environment variables and are never written to disk, logged unmasked, or included in artifact uploads"
      - "Step timeout terminates the container and marks the step as failed after the configured duration; zombie container cleanup is verified"
      - "Failed steps trigger configurable retry with exponential backoff (configurable max retries and initial delay) before marking the job as failed"
      - "Workspace directory is cleaned up (all temporary files, volumes, and intermediate containers removed) between job runs to prevent state leakage"
      - "Pipeline execution can be cancelled; cancellation terminates running containers within 10 seconds using SIGTERM then SIGKILL"
    pitfalls:
      - "Zombie containers when the executor process is killed without cleanup—always use defer/finally to remove containers"
      - "Environment variable injection of secrets: if a step runs 'env' or 'printenv', all secrets are exposed in logs unless masked"
      - "Large stdout output (e.g., verbose build logs) causes memory exhaustion if buffered entirely—stream to disk with a size cap"
      - "Docker-in-Docker (DinD) has security implications: the inner Docker daemon has root access to the host. Prefer Docker socket mounting with read-only where possible."
      - "Workspace not cleaned between retries can cause 'works on retry' non-determinism"
    concepts:
      - Process isolation using containers
      - Secret injection and log masking
      - Resource limits and cgroup management
      - Signal propagation and graceful shutdown
      - Workspace lifecycle management
    skills:
      - Docker API or CLI automation
      - Secret management patterns
      - Process and container lifecycle management
      - Streaming log handling
    deliverables:
      - "Docker container executor running each job step in an isolated container with automatic cleanup"
      - "Secret manager that injects secrets as environment variables and masks their values in all log output"
      - "Log streamer that captures stdout/stderr in real-time with secret masking and configurable size limits"
      - "Timeout and retry handler with exponential backoff and configurable max attempts"
      - "Workspace cleanup routine that removes all temporary files, volumes, and containers between job runs"
      - "Cancellation handler that terminates running containers gracefully on pipeline abort"
    estimated_hours: "10-12"

  - id: ci-cd-pipeline-m3
    name: Artifact Management
    description: >-
      Implement artifact upload, download, integrity verification, and caching
      between pipeline stages with path traversal protection.
    acceptance_criteria:
      - "Artifacts uploaded from one job are downloadable by subsequent dependent jobs using a declared artifact name"
      - "Artifact download verifies SHA-256 checksum to detect corruption during transfer or storage; mismatches fail the job"
      - "Retention policy deletes artifacts older than the configured age; manual pinning prevents deletion of release artifacts"
      - "Large artifacts (>1GB) are handled via streaming upload/download without loading the entire file into memory"
      - "Path traversal attacks are prevented: artifact extraction rejects entries containing '..' or absolute paths, and symlinks pointing outside the extraction directory are rejected"
      - "Cache keys are namespaced per pipeline and include a content hash to prevent unrelated cache collisions"
    pitfalls:
      - "Symlinks in tarballs can escape the extraction directory (zip-slip vulnerability)—validate every entry path before extraction"
      - "Cache key collision: using only branch name as cache key overwrites caches from different pipelines on the same branch"
      - "Streaming upload interrupted mid-transfer leaves partial artifacts—use atomic rename (write to temp, rename on completion)"
      - "Not verifying checksum before marking artifact as available allows corrupted artifacts to propagate to downstream jobs"
    concepts:
      - Content-addressable storage
      - Streaming I/O for large file transfers
      - Path traversal vulnerability mitigation (zip-slip)
      - Cache invalidation strategies
    skills:
      - File System Operations
      - Stream Processing
      - Security Hardening
      - Data Compression and Integrity
    deliverables:
      - "Artifact upload handler storing output files with SHA-256 checksum in versioned storage"
      - "Artifact download handler retrieving stored artifacts with checksum verification"
      - "Path traversal validator rejecting archive entries with '..', absolute paths, or external symlinks"
      - "Retention policy engine with configurable TTL and manual pin support"
      - "Cache manager with namespaced, content-hash-based keys to prevent collisions"
    estimated_hours: "8-10"

  - id: ci-cd-pipeline-m4
    name: Deployment Strategies with Rollback
    description: >-
      Implement blue-green deployment with health checks, automatic rollback,
      and manual approval gates.
    acceptance_criteria:
      - "Blue-green deployment deploys new version to inactive environment, runs health checks, and switches traffic atomically via load balancer configuration update"
      - "Health check validates HTTP 200 response from the /health endpoint within configurable timeout (default 30s) and retry count (default 3)"
      - "Automatic rollback switches traffic back to previous environment within 30 seconds when health checks fail after cutover"
      - "Manual approval gate pauses the pipeline after deployment to inactive environment and before traffic switch; pipeline resumes only after authorized user approval via API or CLI"
      - "Deployment state is persisted so that a coordinator restart does not lose track of which environment is active and what version is deployed"
      - "Connection draining waits for in-flight requests (up to configurable timeout, default 30s) to complete before decommissioning the old environment"
    pitfalls:
      - "Database migration incompatibility between versions: the expand-contract pattern is required but out of scope—document this as a prerequisite assumption"
      - "Session affinity (sticky sessions) can cause users to be stuck on the old environment after traffic switch—document and test with stateless services"
      - "Health check passing on /health but application failing on actual traffic: smoke tests should hit real endpoints, not just health"
      - "Missing persistent state for deployment: if the coordinator restarts mid-deployment, it must know which environment is active"
    concepts:
      - Blue-green deployment pattern
      - Load balancer traffic switching
      - Health check protocols
      - Connection draining
      - Deployment state persistence
    skills:
      - Deployment Orchestration
      - Traffic Management
      - Health Check Design
      - State Persistence
    deliverables:
      - "Blue-green deployment orchestrator managing two environments with traffic switching"
      - "Health check validator with configurable endpoint, timeout, retry count, and expected status"
      - "Automatic rollback handler reverting traffic on health check failure"
      - "Manual approval gate with API endpoint for authorized user approval"
      - "Deployment state store persisting active environment, version, and deployment history"
    estimated_hours: "10-12"