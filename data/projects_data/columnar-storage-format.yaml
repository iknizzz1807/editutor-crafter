id: columnar-storage-format
name: Columnar Storage Format
description: >
  Build a columnar data storage format with SIMD-optimized compression
  for analytical workloads, similar to Apache Parquet and Arrow.

difficulty: advanced
estimated_hours: 60-80
domain: data-storage

essence: >
  Column-oriented storage with encoding schemes (RLE, dictionary, delta),
  SIMD-accelerated compression and decompression, predicate pushdown for
  filtering, and efficient scan patterns for analytical queries.

why_important: >
  Columnar storage is the foundation of modern data warehouses (Snowflake,
  BigQuery, Redshift). Understanding columnar formats is valuable at
  $180K-350K+ for data engineers and database developers.

learning_outcomes:
  - Design columnar file format with metadata
  - Implement RLE, dictionary, and delta encoding
  - Build SIMD-accelerated compression/decompression
  - Implement predicate pushdown for filtering
  - Build efficient column scanners
  - Handle nested data types (structs, lists)
  - Implement zone maps and statistics
  - Benchmark against row-oriented storage

skills:
  - Columnar Storage
  - Data Encoding
  - SIMD Compression
  - Predicate Pushdown
  - File Format Design
  - Nested Data Handling
  - Statistics Collection
  - Query Optimization

tags:
  - advanced
  - analytics
  - columnar
  - compression
  - data-engineering
  - parquet
  - simd
  - storage

languages:
  recommended:
    - Rust
    - C++
  also_possible:
    - Go
    - Java

resources:
  - name: "Apache Parquet Format"
    url: https://parquet.apache.org/docs/file-format/
    type: documentation
  - name: "Apache Arrow Columnar Format"
    url: https://arrow.apache.org/docs/format/Columnar.html
    type: documentation
  - name: "Dremel Paper (Columnar Nested Data)"
    url: https://research.google/pubs/pub36632/
    type: paper
  - name: "C-Store Paper"
    url: https://db.cs.cmu.edu/events/vldb2006/vldb/program/slides/thu/p649-stonebraker.ppt
    type: paper

prerequisites:
  - type: skill
    name: Binary file format design
  - type: skill
    name: SIMD programming basics
  - type: skill
    name: Understanding of compression algorithms
  - type: project
    name: File format or storage project experience

milestones:
  - id: csf-m1
    name: Columnar File Format
    description: >
      Design and implement the basic columnar file format with metadata,
      row groups, and column chunks.
    acceptance_criteria:
      - File header with magic bytes and version
      - Row groups for row-level organization
      - Column chunks within row groups
      - Footer with metadata (schema, statistics, offsets)
      - Supports basic types: int32, int64, float, double, string
      - Read/write API for individual columns
      - File size overhead under 5% vs raw data
    pitfalls:
      - Metadata size grows with column count
      - Row group size affects compression ratio
      - Endianness handling for cross-platform
      - String handling variable-length
      - Footer must be written last; requires seeking
    concepts:
      - Columnar layout
      - Row groups
      - File metadata
      - Schema definition
    skills:
      - File format design
      - Binary I/O
      - Metadata management
      - Schema handling
    deliverables:
      - File format specification
      - Writer implementation
      - Reader implementation
      - Basic test files
    estimated_hours: "12-14"

  - id: csf-m2
    name: Encoding Schemes
    description: >
      Implement multiple encoding schemes optimized for different data
      patterns: RLE, dictionary, delta, and prefix compression.
    acceptance_criteria:
      - RLE encoding for repeated values
      - Dictionary encoding for low-cardinality strings
      - Delta encoding for sorted integers
      - Bit-packing for small integers
      - Automatic encoding selection based on data statistics
      - Decoding performance under 2x raw read
      - Compression ratio reported per column
    pitfalls:
      - Dictionary encoding overhead for high-cardinality
      - Delta encoding negative on unsorted data
      - RLE inefficient for random data
      - Encoding selection heuristics can be wrong
      - Dictionary ID size varies with cardinality
    concepts:
      - Run-length encoding
      - Dictionary encoding
      - Delta encoding
      - Bit-packing
    skills:
      - Compression algorithms
      - Statistics analysis
      - Encoding selection
      - Performance tuning
    deliverables:
      - RLE encoder/decoder
      - Dictionary encoder/decoder
      - Delta encoder/decoder
      - Encoding selector
    estimated_hours: "14-16"

  - id: csf-m3
    name: SIMD Compression
    description: >
      Implement SIMD-accelerated compression and decompression for
      high-throughput column scanning.
    acceptance_criteria:
      - SIMD bit-unpacking for packed integers
      - Vectorized delta decoding
      - SIMD dictionary lookup
      - AVX2 implementation with SSE2 fallback
      - 4x+ speedup over scalar implementation
      - Handles unaligned data safely
      - Benchmark with realistic data sizes
    pitfalls:
      - SIMD alignment requirements
      - Remainder handling for non-vector lengths
      - Different SIMD widths (SSE, AVX, AVX-512)
      - Portability across architectures
      - Compiler auto-vectorization can compete
    concepts:
      - SIMD bit manipulation
      - Vectorized decoding
      - SIMD alignment
      - Portability
    skills:
      - SIMD intrinsics
      - Vectorization
      - Performance optimization
      - Cross-platform code
    deliverables:
      - SIMD bit-unpacking
      - Vectorized delta decode
      - SIMD dictionary lookup
      - Performance comparison
    estimated_hours: "14-16"

  - id: csf-m4
    name: Predicate Pushdown & Statistics
    description: >
      Implement column statistics and predicate pushdown to avoid
      reading unnecessary data during queries.
    acceptance_criteria:
      - Min/max statistics per column chunk
      - Null count tracking
      - Distinct count estimation
      - Predicate pushdown filters row groups
      - Bloom filter option for membership tests
      - Zone maps for range queries
      - 10x+ speedup on selective queries
    pitfalls:
      - Statistics must be accurate; bugs cause wrong results
      - Bloom filter false positives waste work
      - Statistics collection adds write overhead
      - Stale statistics after updates
      - Predicate evaluation cost vs I/O savings
    concepts:
      - Column statistics
      - Predicate pushdown
      - Zone maps
      - Bloom filters
    skills:
      - Statistics collection
      - Query optimization
      - Filtering implementation
      - Bloom filter design
    deliverables:
      - Statistics collector
      - Predicate evaluator
      - Zone map implementation
      - Bloom filter option
    estimated_hours: "12-14"

  - id: csf-m5
    name: Nested Data & Integration
    description: >
      Implement support for nested data types (structs, lists) and
      integrate components into a complete columnar storage system.
    acceptance_criteria:
      - Struct columns with multiple child columns
      - List columns with repetition/definition levels
      - Null handling at all nesting levels
      - Schema evolution support (add columns)
      - Full read/write API for all types
      - Integration with query engine (basic)
      - Benchmark vs row storage shows benefits
    pitfalls:
      - Repetition/definition levels are complex
      - Deeply nested data hurts compression
      - Schema evolution backward compatibility
      - Memory usage for nested reconstruction
      - Projection pushdown with nested data
    concepts:
      - Nested data representation
      - Repetition/definition levels
      - Schema evolution
      - Query integration
    skills:
      - Nested data handling
      - Schema management
      - API design
      - Integration testing
    deliverables:
      - Nested type support
      - Schema evolution
      - Complete API
      - Integration tests
      - Performance report
    estimated_hours: "14-16"
