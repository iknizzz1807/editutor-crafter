id: computer-vision-pipeline
name: Computer Vision Pipeline
description: >
  Build a computer vision pipeline implementing object detection (YOLO-style),
  image classification, and feature extraction from scratch. Understand the
  full stack from convolution operations to non-maximum suppression.

difficulty: advanced
estimated_hours: 60-80
domain: ai-ml

essence: >
  Visual perception through convolutional feature extraction, region proposal
  networks for object localization, anchor-based detection with classification
  heads, and post-processing (NMS) for final predictions.

why_important: >
  Computer vision powers autonomous vehicles, medical imaging, and surveillance.
  Understanding object detection from scratch is valuable for CV engineers at
  $180K-350K+ at companies like Tesla, Waymo, and medical imaging startups.

learning_outcomes:
  - Implement convolution, pooling, and batch normalization from scratch
  - Build CNN backbone for feature extraction (ResNet-style)
  - Implement anchor-based object detection (YOLO/SSD style)
  - Build non-maximum suppression for duplicate removal
  - Implement IoU calculation and mAP evaluation
  - Handle multi-scale detection with feature pyramids
  - Implement data augmentation for training
  - Optimize inference for real-time performance

skills:
  - Convolutional Networks
  - Object Detection
  - Non-Maximum Suppression
  - Feature Pyramids
  - IoU/mAP Evaluation
  - Data Augmentation
  - Anchor Design
  - Real-Time Inference

tags:
  - advanced
  - computer-vision
  - object-detection
  - yolo
  - cnn
  - deep-learning

languages:
  recommended:
    - Python
  also_possible: []

resources:
  - name: "You Only Look Once: Unified, Real-Time Object Detection"
    url: https://arxiv.org/abs/1506.02640
    type: paper
  - name: "SSD: Single Shot MultiBox Detector"
    url: https://arxiv.org/abs/1512.02325
    type: paper
  - name: "Feature Pyramid Networks for Object Detection"
    url: https://arxiv.org/abs/1612.03144
    type: paper
  - name: "COCO Dataset"
    url: https://cocodataset.org/
    type: dataset

prerequisites:
  - type: skill
    name: Deep learning fundamentals
  - type: skill
    name: PyTorch or TensorFlow proficiency
  - type: skill
    name: Linear algebra and convolution basics
  - type: project
    name: neural-network-basic or equivalent

milestones:
  - id: cv-pipeline-m1
    name: CNN Backbone from Scratch
    description: >
      Build a CNN backbone with convolutions, pooling, and residual
      connections for feature extraction.
    acceptance_criteria:
      - Conv2d implements forward pass with proper padding/stride
      - MaxPool and AvgPool for spatial downsampling
      - BatchNorm for training stability
      - Residual block with skip connections (ResNet-style)
      - Feature extractor outputs multi-scale features
      - Pretrained weights can be loaded (optional transfer learning)
    pitfalls:
      - Incorrect padding causes spatial dimension mismatch
      - BatchNorm statistics during inference
      - Vanishing gradients in deep networks
      - Memory usage scales with batch size and resolution
    concepts:
      - Convolution operations
      - Pooling layers
      - Batch normalization
      - Residual connections
    skills:
      - CNN implementation
      - ResNet blocks
      - Feature extraction
      - Transfer learning
    deliverables:
      - Conv2d implementation
      - ResNet-style backbone
      - Multi-scale features
      - BatchNorm handling
    estimated_hours: "14-18"

  - id: cv-pipeline-m2
    name: Anchor-Based Detection Head
    description: >
      Implement detection head with anchor boxes, classification,
      and bounding box regression.
    acceptance_criteria:
      - Anchor boxes generated at multiple scales and aspect ratios
      - Classification head predicts class scores per anchor
      - Regression head predicts bbox offsets (tx, ty, tw, th)
      - Anchor matching assigns ground truth to best anchors
      - Positive/negative anchor balance handled
      - Multi-class detection (not just binary)
    pitfalls:
      - Anchor design mismatch with object sizes
      - Class imbalance (many background anchors)
      - Coordinate transformation errors
      - Ignoring small objects
    concepts:
      - Anchor generation
      - Bounding box regression
      - Anchor matching
      - Multi-class detection
    skills:
      - Anchor design
      - Detection head
      - Box regression
      - Class handling
    deliverables:
      - Anchor generator
      - Detection head
      - Anchor matching
      - Multi-class output
    estimated_hours: "12-16"

  - id: cv-pipeline-m3
    name: Loss Functions & Training
    description: >
      Implement detection loss combining classification and localization
      with proper handling of positive/negative samples.
    acceptance_criteria:
      - Classification loss: cross-entropy with class weights
      - Regression loss: smooth L1 or CIoU/DIoU for bboxes
      - Focal loss option for class imbalance
      - Combined loss = cls_loss + lambda * reg_loss
      - Only positive anchors contribute to regression loss
      - Hard negative mining for classification
    pitfalls:
      - Ignoring sample imbalance leads to poor training
      - Loss scale mismatch between cls and reg
      - Not handling empty detections in batch
      - Gradient explosion from large regression targets
    concepts:
      - Detection loss
      - Focal loss
      - Hard negative mining
      - Loss balancing
    skills:
      - Loss implementation
      - Class imbalance handling
      - Training stability
      - Loss monitoring
    deliverables:
      - Classification loss
      - Regression loss
      - Focal loss option
      - Combined loss
    estimated_hours: "10-14"

  - id: cv-pipeline-m4
    name: Non-Maximum Suppression & Inference
    description: >
      Implement NMS for post-processing and build inference pipeline
      for real-time detection.
    acceptance_criteria:
      - NMS removes duplicate detections based on IoU threshold
      - Class-aware NMS handles multi-class correctly
      - Soft-NMS option reduces false suppression
      - Inference pipeline: preprocess -> model -> postprocess
      - Batch inference for throughput
      - "Real-time performance target: > 30 FPS on GPU"
    pitfalls:
      - NMS threshold too low misses objects, too high keeps duplicates
      - Not handling overlapping objects of different classes
      - Preprocessing inconsistencies between train/inference
      - Memory leaks in inference loop
    concepts:
      - Non-maximum suppression
      - Post-processing
      - Inference pipeline
      - Real-time performance
    skills:
      - NMS implementation
      - Pipeline design
      - Performance optimization
      - Batch inference
    deliverables:
      - NMS implementation
      - Soft-NMS option
      - Inference pipeline
      - Performance benchmark
    estimated_hours: "10-14"

  - id: cv-pipeline-m5
    name: Evaluation & Data Augmentation
    description: >
      Implement COCO-style evaluation metrics and data augmentation
      for robust training.
    acceptance_criteria:
      - IoU calculation between predicted and ground truth boxes
      - mAP calculated at IoU thresholds 0.5:0.95
      - Precision-recall curves per class
      - Augmentation: random crop, flip, color jitter, scale
      - Mosaic/mixup augmentation for better generalization
      - Evaluation on validation set after each epoch
    pitfalls:
      - Incorrect IoU calculation
      - mAP calculation differs from standard (verify)
      - Aggressive augmentation destroys small objects
      - Not normalizing bounding box coordinates
    concepts:
      - IoU metrics
      - mAP evaluation
      - Data augmentation
      - Training robustness
    skills:
      - Metric implementation
      - Augmentation design
      - Evaluation pipeline
      - Result interpretation
    deliverables:
      - IoU calculation
      - mAP evaluation
      - Augmentation pipeline
      - Training improvements
    estimated_hours: "10-14"
