id: constitutional-ai-system
name: Constitutional AI System
description: >
  Build a Constitutional AI system that trains language models to follow explicit principles
  (a "constitution") without human feedback on every output. Implement critique-revision
  loops, red-teaming, and principle-based alignment.

difficulty: advanced
estimated_hours: 50-70
domain: ai-ml

essence: >
  Self-improvement through principle-based critique where an AI model evaluates and revises
  its own outputs against a set of constitutional principles, enabling alignment at scale
  without extensive human labeling.

why_important: >
  AI safety and alignment is one of the most important problems in ML. Constitutional AI
  (developed by Anthropic) is a key technique for training helpful, harmless, and honest
  AI systems. Skills in AI alignment are highly valued at AI labs ($250K-500K+).

learning_outcomes:
  - Implement Constitutional AI critique-revision loop
  - Design and encode constitutional principles
  - Build red-teaming and adversarial evaluation systems
  - Implement RLHF (Reinforcement Learning from Human Feedback)
  - Build preference datasets from AI-generated critiques
  - Implement Direct Preference Optimization (DPO) as alternative to RLHF
  - Evaluate model behavior on safety and helpfulness benchmarks
  - Design robust principles that generalize to unseen situations

skills:
  - AI Alignment
  - Constitutional AI
  - RLHF
  - Red-Teaming
  - Preference Learning
  - Safety Evaluation
  - Principle Design
  - Adversarial Testing

tags:
  - advanced
  - ai-safety
  - constitutional-ai
  - rlhf
  - alignment
  - llama

languages:
  recommended:
    - Python
  also_possible: []

resources:
  - name: "Constitutional AI Paper"
    url: https://arxiv.org/abs/2212.08073
    type: paper
  - name: "Training a Helpful and Harmless Assistant with RLHF"
    url: https://arxiv.org/abs/2204.05862
    type: paper
  - name: "Direct Preference Optimization"
    url: https://arxiv.org/abs/2305.18290
    type: paper
  - name: "Anthropic Collection on Constitutional AI"
    url: https://www.anthropic.com/research
    type: documentation

prerequisites:
  - type: project
    name: transformer-scratch or equivalent
  - type: skill
    name: LLM fine-tuning experience
  - type: skill
    name: PyTorch proficiency
  - type: skill
    name: Understanding of RL basics

milestones:
  - id: constitutional-m1
    name: Constitution Design & Encoding
    description: >
      Design and implement a system for encoding constitutional principles
      that guide AI behavior.
    acceptance_criteria:
      - Constitution is a list of natural language principles
      - Each principle has: name, description, examples of good/bad behavior
      - Principles cover: helpfulness, harmlessness, honesty, and custom categories
      - Principle renderer formats constitution for model input
      - Principles can be added, removed, or modified without code changes
      - Example principles match published Constitutional AI principles
    pitfalls:
      - Principles too vague -> inconsistent application
      - Principles too specific -> doesn't generalize
      - Conflicting principles -> model confused
      - Not including examples -> model doesn't understand intent
    concepts:
      - Principle-based alignment
      - Natural language constraints
      - Constitution design
      - Example-based guidance
    skills:
      - Principle design
      - Constitution formatting
      - Balance of specificity/generality
      - Conflict resolution
    deliverables:
      - Constitution data structure
      - Principle renderer
      - Example constitution with 10+ principles
      - Documentation on principle design
    estimated_hours: "8-10"

  - id: constitutional-m2
    name: Critique-Revision Loop
    description: >
      Implement the core Constitutional AI mechanism: AI critiques its own
      outputs against principles and revises them.
    acceptance_criteria:
      - Given (prompt, response, principle), model generates critique
      - Critique identifies specific violations of the principle
      - Given (prompt, response, critique), model generates revision
      - Revision addresses all issues identified in critique
      - Multiple revision rounds improve response quality
      - Process works for various principles without code changes
      - Critique-revision tested on adversarial prompts
    pitfalls:
      - Model may not critique its own errors honestly
      - Revisions can introduce new problems while fixing old ones
      - Over-correction makes responses unhelpful
      - Critique format inconsistency makes parsing difficult
    concepts:
      - Self-critique
      - Iterative refinement
      - Principle-guided revision
      - Quality improvement
    skills:
      - Critique generation
      - Revision generation
      - Iterative improvement
      - Prompt engineering
    deliverables:
      - Critique generation module
      - Revision generation module
      - Multi-round improvement loop
      - Evaluation on test prompts
    estimated_hours: "12-16"

  - id: constitutional-m3
    name: Preference Dataset Generation
    description: >
      Use the critique-revision loop to generate preference data for RLHF training.
    acceptance_criteria:
      - For each prompt, generate initial response and revised response
      - Revised response is labeled as "preferred" (or better yet, human verified)
      - Dataset format compatible with RLHF training (prompt, chosen, rejected)
      - Thousands of preference pairs generated efficiently
      - Data quality sampled and verified manually
      - Diversity of prompts and principles in dataset
    pitfalls:
      - Automated preferences may not match human preferences
      - Dataset bias towards certain types of issues
      - Not enough diversity in prompts
      - Scaling costs for large datasets
    concepts:
      - Preference data generation
      - AI-assisted labeling
      - Dataset quality
      - Scalable annotation
    skills:
      - Dataset generation
      - Quality sampling
      - Diversity analysis
      - Efficient generation
    deliverables:
      - Preference dataset generator
      - Sampled quality evaluation
      - Dataset statistics
      - Format conversion for RLHF
    estimated_hours: "10-14"

  - id: constitutional-m4
    name: RLHF Training (or DPO)
    description: >
      Train a model using the preference data through RLHF or the simpler
      Direct Preference Optimization (DPO) method.
    acceptance_criteria:
      - Reward model trained to predict human preferences
      - PPO (or DPO) fine-tunes policy model using reward model
      - KL penalty prevents policy from deviating too far from base
      - Training improves preference model accuracy
      - Fine-tuned model shows improved behavior on held-out prompts
      - Training curves show reward improvement without reward hacking
    pitfalls:
      - Reward hacking: model optimizes reward without improving quality
      - KL penalty too weak -> model drifts; too strong -> no improvement
      - Reward model overfits to training preferences
      - PPO is unstable - requires careful hyperparameter tuning
    concepts:
      - Reward modeling
      - PPO for RLHF
      - DPO as simpler alternative
      - KL regularization
    skills:
      - Reward model training
      - PPO/DPO implementation
      - Training stability
      - Evaluation
    deliverables:
      - Trained reward model
      - RLHF/DPO fine-tuned model
      - Training curves
      - Evaluation on test prompts
    estimated_hours: "12-16"

  - id: constitutional-m5
    name: Red-Teaming & Evaluation
    description: >
      Build adversarial evaluation system to test model safety and robustness
      before deployment.
    acceptance_criteria:
      - Red-team prompts generated or collected (adversarial, edge cases)
      - Model tested on harmful request refusal
      - Model tested on subtle manipulation attempts
      - Model tested on诚实 (honesty) when it doesn't know something
      - Automated evaluation scores model on multiple dimensions
      - Human evaluation on sample of responses
      - Report generation with failure modes and recommendations
    pitfalls:
      - Red-team prompts don't cover real-world adversarial inputs
      - Evaluation focuses on obvious failures, misses subtle ones
      - Not testing enough edge cases
      - Human evaluation is expensive - need good sampling
    concepts:
      - Red-teaming
      - Adversarial evaluation
      - Safety benchmarks
      - Failure mode analysis
    skills:
      - Red-team design
      - Evaluation metrics
      - Failure analysis
      - Safety reporting
    deliverables:
      - Red-team prompt collection
      - Automated evaluation system
      - Safety report
      - Recommendations for improvement
    estimated_hours: "8-12"
