id: edge-ml-runtime
name: Edge ML Runtime
description: >
  Build a machine learning runtime optimized for edge devices with limited
  resources. Implement model quantization, efficient memory management, and
  hardware acceleration for running models on microcontrollers and mobile devices.

difficulty: advanced
estimated_hours: 50-70
domain: specialized

essence: >
  Constrained execution through model quantization (int8/int4), memory-efficient
  inference, operator fusion for reduced overhead, and hardware backend abstraction
  for CPU/GPU/NPU deployment - enabling ML on devices with KB of RAM.

why_important: >
  Edge AI is growing rapidly (IoT, mobile, embedded). Understanding TinyML and
  edge deployment is valuable at $150K-280K+ for embedded ML engineers.

learning_outcomes:
  - Implement model quantization (PTQ, QAT)
  - Build memory-efficient tensor operations
  - Implement operator fusion for inference optimization
  - Build hardware abstraction for CPU/GPU/NPU
  - Handle constrained memory environments
  - Implement model compression (pruning, distillation)
  - Build efficient memory allocators for edge
  - Profile and optimize inference latency

skills:
  - Model Quantization
  - Memory Optimization
  - Operator Fusion
  - Hardware Abstraction
  - Edge Deployment
  - Model Compression
  - Memory Management
  - Latency Optimization

tags:
  - advanced
  - edge-ml
  - tinymml
  - quantization
  - embedded
  - inference
  - optimization

languages:
  recommended:
    - C++
    - Rust
  also_possible:
    - C

resources:
  - name: "TinyML Book"
    url: https://www.oreilly.com/library/view/tinyml/9781492052036/
    type: book
  - name: "TensorFlow Lite for Microcontrollers"
    url: https://www.tensorflow.org/lite/microcontrollers
    type: documentation
  - name: "ONNX Runtime"
    url: https://onnxruntime.ai/
    type: documentation
  - name: "Quantization Deep Learning"
    url: https://arxiv.org/abs/2103.13630
    type: paper

prerequisites:
  - type: skill
    name: C/C++ or Rust proficiency
  - type: skill
    name: Understanding of neural network inference
  - type: skill
    name: Memory management in constrained environments
  - type: project
    name: neural-network-basic or equivalent

milestones:
  - id: edge-ml-m1
    name: Tensor Runtime & Memory Management
    description: >
      Build the core tensor runtime with memory-efficient operations
      for constrained environments.
    acceptance_criteria:
      - Tensor representation with configurable data types
      - Arena allocator for deterministic memory usage
      - In-place operations where possible
      - Memory planning for entire inference graph
      - Peak memory calculation before execution
      - Works with < 256KB RAM constraint
    pitfalls:
      - Memory fragmentation over time
      - Peak memory exceeds device limit
      - Not accounting for temporary buffers
      - Alignment requirements for SIMD
    concepts:
      - Tensor representation
      - Arena allocation
      - Memory planning
      - Constrained execution
    skills:
      - Tensor implementation
      - Memory allocator
      - Planning algorithm
      - Constraint handling
    deliverables:
      - Tensor class
      - Arena allocator
      - Memory planner
      - Peak analyzer
    estimated_hours: "10-14"

  - id: edge-ml-m2
    name: Quantization (Post-Training)
    description: >
      Implement post-training quantization to convert float32 models
      to int8 for efficient inference.
    acceptance_criteria:
      - Dynamic range quantization with per-tensor scales
      - Per-channel quantization for better accuracy
      - Calibration with representative dataset
      - Quantize weights and activations separately
      - Dequantize outputs for comparison
      - < 2% accuracy drop on simple models (MNIST, CIFAR)
    pitfalls:
      - Quantization range too narrow clips values
      - Per-tensor less accurate than per-channel
      - Calibration dataset must be representative
      - Some ops don't quantize well
    concepts:
      - Quantization schemes
      - Scale and zero-point
      - Calibration
      - Accuracy tradeoff
    skills:
      - Quantization math
      - Calibration implementation
      - Scale calculation
      - Accuracy evaluation
    deliverables:
      - Quantizer
      - Calibrator
      - Dequantizer
      - Accuracy comparison
    estimated_hours: "12-16"

  - id: edge-ml-m3
    name: Operator Fusion & Optimization
    description: >
      Implement operator fusion to reduce memory bandwidth and
      improve inference latency.
    acceptance_criteria:
      - Pattern matching for fusable operators (conv+relu, etc.)
      - Fused operator implementation
      - Graph optimization pass
      - Memory bandwidth reduction measured
      - Latency improvement from fusion
      - Handle edge cases (padding, stride)
    pitfalls:
      - Not all patterns fuse correctly
      - Numerical differences after fusion
      - Fusion can increase code size
      - Some hardware benefits more than others
    concepts:
      - Operator fusion
      - Pattern matching
      - Graph optimization
      - Bandwidth reduction
    skills:
      - Pattern detection
      - Fused implementation
      - Optimization pass
      - Performance measurement
    deliverables:
      - Pattern matcher
      - Fused operators
      - Optimization pass
      - Bandwidth analysis
    estimated_hours: "10-14"

  - id: edge-ml-m4
    name: Hardware Backend Abstraction
    description: >
      Build hardware abstraction layer for CPU with optional
      SIMD and GPU/NPU backend support.
    acceptance_criteria:
      - Backend interface for different hardware
      - CPU reference implementation
      - SIMD-optimized convolution (optional NEON/SSE)
      - Backend selection at runtime
      - Fallback to CPU for unsupported ops
      - Performance comparison across backends
    pitfalls:
      - SIMD intrinsics not portable
      - GPU memory transfer overhead
      - Backend-specific numerical differences
      - Thread safety across backends
    concepts:
      - Hardware abstraction
      - SIMD optimization
      - Backend selection
      - Graceful fallback
    skills:
      - Interface design
      - SIMD implementation
      - Backend dispatch
      - Performance tuning
    deliverables:
      - Backend interface
      - CPU backend
      - SIMD optimizations
      - Backend comparison
    estimated_hours: "12-16"

  - id: edge-ml-m5
    name: Model Loading & Deployment
    description: >
      Build model loading and deployment pipeline for edge devices.
    acceptance_criteria:
      - Flatbuffer/binary model format
      - Minimal parsing overhead
      - Model validation before execution
      - Version compatibility checking
      - Memory-mapped model loading
      - Deployment package generation
    pitfalls:
      - Model format overhead
      - Parsing uses too much memory
      - Version mismatch crashes
      - File I/O on embedded systems
    concepts:
      - Model serialization
      - Binary format
      - Memory mapping
      - Deployment packaging
    skills:
      - Format design
      - Efficient parsing
      - Memory mapping
      - Package creation
    deliverables:
      - Model format
      - Loader
      - Validator
      - Deployment tools
    estimated_hours: "8-12"
