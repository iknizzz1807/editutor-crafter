id: etl-pipeline
name: "ETL Pipeline"
description: >-
  Build a production-grade ETL pipeline framework with DAG-based orchestration,
  incremental extraction with checkpointing, schema-validated transformations,
  and monitoring with retry/recovery capabilities.
difficulty: expert
estimated_hours: "45-60"
essence: >-
  Orchestrating data workflows as directed acyclic graphs with topological
  execution ordering, implementing incremental extraction with checkpointing for
  crash recovery, enforcing schema validation between pipeline stages, and
  building idempotent task execution with monitoring and failure recovery.
why_important: >-
  Building this teaches you production-grade data engineering patterns used at
  scale in modern data platforms, including workflow orchestration, distributed
  processing, and data quality management — skills essential for backend and
  infrastructure engineering roles.
learning_outcomes:
  - Implement DAG-based workflow execution with topological sorting and dependency resolution
  - Design incremental data extraction with watermark tracking and checkpointing
  - Build checkpoint/state persistence for crash recovery and pipeline resumption
  - Implement data transformation pipelines with inter-stage schema validation
  - Ensure task idempotency so re-runs produce identical results
  - Debug pipeline failures with structured logging, metrics, and retry strategies
  - Implement job scheduling with cron expressions and manual trigger support
  - Build connector abstractions for multiple data sources with unified interfaces
skills:
  - DAG Orchestration
  - Checkpointing and State Management
  - Schema Validation
  - Data Quality Enforcement
  - Pipeline Monitoring
  - Incremental Processing
  - Idempotent Operations
  - Workflow Scheduling
tags:
  - batch
  - data-engineering
  - etl
  - expert
  - pipelines
  - scheduling
  - transformation
architecture_doc: architecture-docs/etl-pipeline/index.md
languages:
  recommended:
    - Python
    - Scala
    - Java
  also_possible: []
resources:
  - name: Apache Airflow Documentation""
    url: https://airflow.apache.org/docs/
    type: documentation
  - name: Dagster Orchestration Platform""
    url: https://dagster.io
    type: tool
  - name: Data Validation in ETL Guide""
    url: https://www.integrate.io/blog/data-validation-etl/
    type: article
  - name: Schema Evolution Best Practices""
    url: https://dataterrain.com/handling-schema-evolution-etl-data-transformation
    type: article
prerequisites:
  - type: project
    id: job-scheduler
  - type: skill
    name: SQL and relational database basics
  - type: skill
    name: Python or JVM language proficiency
milestones:
  - id: etl-pipeline-m1
    name: "DAG Definition & Execution Engine"
    description: >-
      Implement pipeline definition as a DAG with task dependencies, validate the
      graph, topologically sort it, and execute tasks respecting dependency order
      with parallel execution of independent tasks.
    estimated_hours: "10-14"
    concepts:
      - Directed acyclic graphs for task dependencies
      - Topological sorting for execution order
      - Cycle detection (DFS-based or Kahn's algorithm)
      - Parallel execution of independent tasks
      - Task state machine (PENDING, RUNNING, SUCCESS, FAILED, SKIPPED)
    skills:
      - DAG modeling and graph algorithms
      - Task dependency resolution
      - Concurrent execution with thread pool or async
      - Configuration parsing (YAML or Python DSL)
    acceptance_criteria:
      - "Tasks are defined with name, type, configuration, and list of upstream dependencies in YAML or Python configuration"
      - "DAG parser validates no cycles exist using DFS-based cycle detection; circular dependencies produce an error listing the cycle path"
      - "Topological sort produces a valid execution order where every task runs after all its upstream dependencies"
      - "Independent tasks (no mutual dependency) execute in parallel using a configurable thread/process pool"
      - "Each task transitions through states PENDING → RUNNING → SUCCESS/FAILED with timestamps recorded for each transition"
      - "A task in FAILED state causes all downstream tasks to be marked SKIPPED, not executed"
      - "Parameterized tasks support runtime variable substitution (e.g., execution_date, custom params) in configuration fields"
      - "DAG of 20+ tasks with diamond dependencies executes correctly with parallel branches"
    pitfalls:
      - "Topological sort produces a valid order but doesn't encode parallelism — you need a separate scheduler that launches tasks whose dependencies are all complete"
      - "Thread-safe task state management is critical; multiple threads updating task states concurrently causes lost updates"
      - "Diamond dependencies (A→B, A→C, B→D, C→D) must not run D until both B and C complete"
      - "Long dependency chains limit parallelism — pipeline width (max parallel tasks) is bounded by the DAG's critical path"
    deliverables:
      - Task definition with name, type, configuration, and dependency list
      - DAG parser with cycle detection and topological sorting
      - Parallel execution engine with configurable concurrency limit
      - Task state machine with transition timestamps
      - DAG visualization rendering pipeline structure as text or graph

  - id: etl-pipeline-m2
    name: "Data Extraction, Loading & Checkpointing"
    description: >-
      Implement source connectors (database, API, file), destination loaders with
      batched writes, incremental extraction using watermarks, and checkpoint
      persistence for crash recovery.
    estimated_hours: "12-16"
    concepts:
      - Connector abstraction for heterogeneous sources
      - Incremental extraction with watermarks (timestamp or ID-based)
      - Checkpoint persistence (last processed offset/watermark)
      - Exactly-once loading via idempotent upserts
      - Batched writes for throughput
      - Cursor-based pagination for API sources
    skills:
      - Database connector implementation (SQL query execution)
      - REST API client with pagination
      - File reader (CSV, JSON, Parquet)
      - Checkpoint state persistence to file or database
      - Batched insert/upsert operations
    acceptance_criteria:
      - "Database extractor executes configurable SQL queries and streams results row-by-row or in configurable batch sizes to avoid OOM on large tables"
      - "API extractor handles cursor-based or offset-based pagination, following next-page tokens until exhaustion"
      - "File extractor reads CSV, JSON, or newline-delimited JSON files with configurable schema"
      - "Incremental extraction uses a watermark column (timestamp or auto-increment ID) to extract only rows changed since the last checkpoint"
      - "Checkpoint state (watermark value, task ID, run ID) is persisted to a durable store (file or database) atomically with the loaded data"
      - "On restart after crash, pipeline resumes from the last checkpointed watermark, not from the beginning"
      - "Destination loader performs batched inserts with configurable batch size (default 1000 rows); batch size is tunable per destination"
      - "Upsert mode uses a configurable conflict key to update existing rows and insert new ones, ensuring idempotency on re-runs"
      - "Schema mapping between source and destination is configurable with column renaming and type casting"
    pitfalls:
      - "Watermark must be saved atomically with the loaded data; saving watermark before load succeeds causes data loss on crash"
      - "API pagination cursors can become invalid during long extractions; implement cursor expiry detection and full re-extraction fallback"
      - "Large batch sizes improve throughput but increase memory usage and transaction size; tune based on available RAM"
      - "Upsert without a proper conflict key causes duplicate rows instead of updates"
      - "Streaming extraction (row-by-row) is memory-safe but slow; batched extraction is fast but risks OOM — provide both modes"
    deliverables:
      - Connector interface with extract() and load() methods and configurable parameters
      - Database source connector with SQL query execution and watermark filtering
      - API source connector with pagination and rate limiting
      - File source connector for CSV and JSON
      - Destination loader with batched insert and upsert modes
      - Checkpoint manager persisting and recovering watermark state
      - Schema mapping configuration for column renaming and type conversion

  - id: etl-pipeline-m3
    name: "Data Transformations with Schema Validation"
    description: >-
      Implement transformation operations (map, filter, aggregate, join) with
      inter-stage schema validation ensuring each transformation's output schema
      matches the next stage's expected input schema.
    estimated_hours: "10-14"
    concepts:
      - Row-level vs batch-level transformations
      - Schema definition and validation between stages
      - Type coercion rules and precision loss
      - Null value handling across transformations
      - Schema inference from transformation logic
    skills:
      - Data transformation function design
      - Schema validation and enforcement
      - Type conversion with precision awareness
      - Error handling at row level vs batch level
    acceptance_criteria:
      - "Map transformation applies a user-defined function to each row, producing a new row with potentially different schema"
      - "Filter transformation removes rows that don't match a predicate without changing schema"
      - "Aggregate transformation groups rows by key columns and computes aggregate functions (count, sum, avg, min, max)"
      - "Each transformation declares its output schema; the framework validates that the output schema of stage N matches the input schema of stage N+1"
      - "Schema validation catches type mismatches (e.g., string column fed to a stage expecting integer) before execution starts, not at runtime"
      - "Null values are handled explicitly — transformations declare whether each output column is nullable; non-nullable columns receiving null values produce a validation error"
      - "Type coercion is explicit and logged — implicit coercion (e.g., float to int truncation) produces a warning; precision-losing coercions require explicit cast"
      - "Invalid rows can be routed to an error output (dead letter table) instead of failing the entire pipeline, based on a configurable error threshold (e.g., fail if >1% of rows have errors)"
      - "Python UDF transformations are supported for custom logic not covered by built-in operations"
    pitfalls:
      - "Schema validation on every row is slow for large datasets — validate schema once at pipeline compile time, then spot-check at runtime via sampling"
      - "Type coercion from float to int silently loses precision; always warn or error on narrowing conversions"
      - "Null handling semantics differ between databases (SQL NULL) and programming languages (None, nil) — standardize on a canonical null representation"
      - "Aggregate transformations change row count and schema; downstream stages must handle the new shape"
      - "UDF exceptions must be caught per-row to prevent one bad record from crashing the entire pipeline"
    deliverables:
      - Transformation interface with declared input/output schemas
      - Built-in transformations for map, filter, aggregate, and join
      - Inter-stage schema validator checking type compatibility before execution
      - Null handling policy (nullable vs non-nullable columns) with enforcement
      - Error routing to dead letter table for invalid rows
      - Python UDF support for custom transformation logic

  - id: etl-pipeline-m4
    name: "Scheduling, Retry, Monitoring & Idempotency"
    description: >-
      Implement pipeline scheduling with cron expressions, task retry with
      exponential backoff, idempotent re-execution, and a monitoring API exposing
      run history, task status, and performance metrics.
    estimated_hours: "12-16"
    concepts:
      - Cron-based scheduling and manual triggers
      - Retry with exponential backoff and max attempts
      - Idempotent pipeline runs (re-run produces same result)
      - Structured logging and metrics collection
      - Partial pipeline re-execution (from failed task)
    skills:
      - Cron expression parsing and scheduling
      - Retry strategy implementation
      - Idempotency patterns (upsert, staging tables)
      - Metrics API design
      - Structured logging
    acceptance_criteria:
      - "Pipeline runs are triggered by cron expressions (e.g., '0 2 * * *' for daily at 2 AM) with a cron parser that calculates next run time"
      - "Manual trigger API starts a pipeline run with optional parameter overrides"
      - "Failed tasks are retried up to a configurable max_retries (default 3) with exponential backoff (base_delay * 2^attempt)"
      - "Pipeline re-execution for the same time window is idempotent — running the same pipeline twice for the same date produces identical output using upsert or truncate-and-reload strategy"
      - "Partial re-execution allows restarting from a specific failed task, skipping already-succeeded upstream tasks using checkpoint state"
      - "Monitoring API exposes per-run metadata — run_id, start_time, end_time, status, and per-task status with duration"
      - "Monitoring API exposes per-task metrics — rows_extracted, rows_transformed, rows_loaded, rows_errored"
      - "Run history is queryable — list last N runs with status, or filter by date range and status"
      - "Structured logs include run_id, task_id, and timestamp in every log entry for correlation"
      - "Alerting hook fires (webhook, email, or log) when a task fails after exhausting all retries"
    pitfalls:
      - "Retry delay should use exponential backoff with jitter to prevent thundering herd on shared resources"
      - "Idempotency via upsert requires a stable primary key; without it, re-runs create duplicates"
      - "Partial re-execution must re-validate that upstream task outputs still exist (they may have been cleaned up by retention policy)"
      - "Cancelled pipelines must clean up partial data to maintain idempotency — a half-loaded table breaks the next run"
      - "Cron scheduling must handle missed runs (e.g., server was down) — decide whether to catch up or skip"
    deliverables:
      - Cron scheduler with next-run-time calculation and missed-run handling
      - Manual trigger API with parameter overrides
      - Retry handler with configurable max attempts and exponential backoff with jitter
      - Idempotent execution using upsert or truncate-reload strategy
      - Partial re-execution from failed task using checkpoint state
      - Monitoring API with run history, task metrics, and alerting hooks
      - Structured logging with run_id/task_id correlation