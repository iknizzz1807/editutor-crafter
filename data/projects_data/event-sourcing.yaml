id: event-sourcing
name: Event Sourcing System
description: Event store with command handling, aggregate reconstitution, projections, snapshots, and event schema evolution (upcasting).
difficulty: advanced
estimated_hours: 55
essence: Append-only immutable event log serving as system-of-record, with explicit Command-to-Event separation enforcing business rules before facts are recorded, aggregate state materialization through event replay, optimistic concurrency control via stream version numbers, separate read model maintenance through asynchronous catch-up subscriptions, and event upcasting for schema evolution in long-lived streams.
why_important: Event sourcing provides complete audit trails, temporal queries, and robust distributed systems patterns. Used in banking, e-commerce, and enterprise systems where business rule traceability and system-of-record integrity are non-negotiable.
learning_outcomes:
- Implement an append-only event store with stream versioning and optimistic concurrency
- Design a Command handling layer that validates business rules before emitting events
- Build domain aggregates reconstituted from event history
- Implement asynchronous projections with idempotent event handlers and checkpointing
- Handle aggregate snapshots for performance optimization of long event streams
- Implement event upcasting for schema evolution in long-lived event streams
- Apply CQRS for read/write separation with eventually consistent read models
skills:
- Event Store Design
- Command Handling & Validation
- CQRS Pattern
- Aggregate Reconstitution
- Event-Driven Architecture
- Optimistic Concurrency
- Projection Building
- Snapshot Management
- Event Upcasting
- Domain Modeling
tags:
- advanced
- architecture
- backend
- cqrs
- distributed-systems
- event-store
- projections
- replay
- domain-driven-design
architecture_doc: architecture-docs/event-sourcing/index.md
languages:
  recommended:
  - Java
  - Kotlin
  - C#
  also_possible:
  - TypeScript
  - Python
resources:
- name: Martin Fowler on Event Sourcing
  url: https://martinfowler.com/eaaDev/EventSourcing.html
  type: article
- name: Microsoft CQRS Pattern Guide
  url: https://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs
  type: documentation
- name: Axon Framework Documentation
  url: https://www.axoniq.io/framework
  type: documentation
- name: EventStoreDB Official Docs
  url: https://developers.eventstore.com/
  type: documentation
- name: Event Sourcing in .NET Tutorial
  url: https://github.com/oskardudycz/EventSourcing.NetCore
  type: tutorial
- name: Versioning in an Event Sourced System (Greg Young)
  url: https://leanpub.com/esversioning
  type: book
prerequisites:
- type: project
  id: rest-api-design
- type: skill
  name: Domain-Driven Design basics
milestones:
- id: event-sourcing-m1
  name: Event Store
  description: Build an append-only event store with stream-level optimistic concurrency, global ordering, and event metadata. This is the foundational persistence layer.
  acceptance_criteria:
  - Append events to a named stream atomically, with each event assigned a monotonically increasing stream version starting from 0
  - Read events for a specific stream in version order, returning event type, payload, metadata, and version
  - Optimistic concurrency rejects an append if the provided expected version does not match the current stream version, returning a concurrency conflict error
  - Global position counter assigns a unique, monotonically increasing position to every event across all streams for cross-stream ordering
  - Event metadata includes event ID (UUID), event type name, timestamp, causation ID, and correlation ID
  - Event store sustains at least 1,000 appends per second on a single stream (measured via benchmark)
  pitfalls:
  - Using vector clocks instead of simple stream version numbers—event sourcing uses sequential integer versions per stream, not vector clocks
  - Gaps in stream versions make the stream unrecoverable—ensure atomic version increment
  - Missing global ordering breaks cross-stream projections; every event needs a global position
  - Large event payloads bloat storage; consider payload size limits and compression
  - Not storing causation/correlation IDs makes debugging event chains impossible in production
  concepts:
  - Append-only log architecture ensures immutability
  - Optimistic concurrency control via expected stream version number
  - Global position provides total ordering across all streams
  - Event metadata (causation ID, correlation ID) enables distributed tracing
  - Storage compaction and retention policies for long-lived stores
  skills:
  - Designing immutable data structures
  - Implementing optimistic locking
  - Database transaction isolation
  - Event serialization and schema design
  - Stream-based storage systems
  deliverables:
  - Event append operation adding events to a stream atomically with version enforcement
  - Event read operation returning events for a stream in version order
  - Global position assignment for cross-stream ordering
  - Event serialization using JSON with type discriminator and metadata envelope
  - Concurrency conflict detection and error reporting
  - Catch-up subscription reading events from a global position forward for projection use
  estimated_hours: 11
- id: event-sourcing-m2
  name: Command Handling & Aggregates
  description: Implement the Command layer and domain aggregates. Commands represent intent and may be rejected; Events represent immutable facts. Aggregates enforce business invariants before emitting events.
  acceptance_criteria:
  - Command objects are distinct from Events—Commands can fail validation; Events are immutable facts that have already occurred
  - Command handler loads aggregate state by replaying its event stream, validates business rules against current state, and emits new events only if validation passes
  - Command rejected by business rule validation returns a domain error without emitting any events
  - Events are applied sequentially to rebuild aggregate state via pure apply() functions that contain no side effects
  - Command deduplication using idempotency keys prevents duplicate events when the same command is retried
  - Aggregate root enforces a transactional boundary—all events from a single command are appended atomically
  - Test suite demonstrates at least 3 business rules enforced by command validation (e.g., cannot overdraw account, cannot add item to closed order)
  pitfalls:
  - Putting business logic in apply() methods—apply() must be a pure state transition; all validation belongs in command handlers
  - Forgetting to clear uncommitted events after successful save causes duplicate appends
  - Mutable event objects allow accidental modification after creation—events must be immutable
  - Missing command deduplication leads to duplicate events in distributed environments with retries
  - Aggregates growing too large; enforce aggregate boundaries aligned with consistency boundaries
  concepts:
  - Commands represent intent (CreateOrder); Events represent facts (OrderCreated)
  - Aggregate boundaries define transactional consistency scope
  - Event sourcing reconstitution replays events to rebuild current state
  - Command deduplication via idempotency keys ensures at-most-once processing
  - Domain invariant enforcement happens in command handlers, not event handlers
  skills:
  - Command/Event separation
  - Domain modeling with aggregates
  - Event replay and state reconstruction
  - Business rule validation
  - Idempotent command processing
  deliverables:
  - Command base class/interface with command ID, aggregate ID, and expected version
  - Command handler that loads aggregate, validates, and emits events
  - Aggregate base class with event application (apply) and uncommitted event tracking
  - Command validation returning domain errors without persisting events
  - Command deduplication store tracking processed command IDs
  - Integration test suite with at least 3 aggregate invariant scenarios
  estimated_hours: 11
- id: event-sourcing-m3
  name: Projections
  description: Build asynchronous read models (projections) that subscribe to the event store and maintain denormalized views for efficient querying.
  acceptance_criteria:
  - Projection handler subscribes to the event store via catch-up subscription starting from a stored checkpoint position
  - Each projection processes events idempotently—reprocessing the same event produces identical read model state (verified by replaying events twice and comparing output)
  - Checkpoint position is persisted atomically with the read model update, ensuring no events are lost or double-processed on crash recovery
  - Multiple independent projections can run concurrently on the same event stream without interference
  - Projection rebuild from position 0 produces identical read model state as incremental processing (verified by comparison test)
  - Read model queries return results within 500ms of event being appended (eventual consistency lag measured)
  pitfalls:
  - Saving checkpoint before processing the event loses events on crash—checkpoint must be saved atomically with the read model update
  - Processing same event twice without idempotency corrupts data—use event position as idempotency key
  - Single-threaded projection engine blocks all projections when one is slow—use independent processing per projection
  - Projection rebuild on large event stores is slow; consider parallelizing by stream or partition
  - Read model schema changes require full rebuild—design for forward compatibility
  concepts:
  - Event-driven materialized views updated asynchronously
  - Projection idempotency using event position as deduplication key
  - Checkpoint-based resumable event processing
  - Eventually consistent read models with measurable lag
  - Catch-up subscriptions for reliable event delivery
  skills:
  - Building denormalized read models
  - Implementing idempotent event handlers
  - Checkpoint persistence and recovery
  - Async event processing
  - Query optimization for read models
  deliverables:
  - Projection handler subscribing to event store catch-up subscription
  - Read model update logic transforming events into queryable denormalized views
  - Checkpoint persistence storing last processed global position per projection
  - Projection rebuild capability reprocessing all events from position 0
  - At least 2 independent projections on the same event stream (e.g., list view and summary view)
  - Consistency lag measurement reporting delay between event append and read model update
  estimated_hours: 11
- id: event-sourcing-m4
  name: Snapshots
  description: Implement aggregate snapshots to optimize loading of aggregates with long event histories, avoiding full replay on every command.
  acceptance_criteria:
  - Snapshot captures aggregate state at a specific stream version, serialized to a persistent store
  - Aggregate loading first checks for latest snapshot, then replays only events after the snapshot version
  - Aggregate state loaded from snapshot + subsequent events is byte-identical to state loaded from full event replay (verified by comparison test)
  - Automatic snapshot creation triggers after a configurable N events threshold (default 100)
  - Snapshot schema versioning allows old snapshots to be discarded and rebuilt rather than causing deserialization failures
  - Loading an aggregate with 10,000 events and a recent snapshot completes at least 10x faster than full replay (measured via benchmark)
  pitfalls:
  - Snapshot schema changes break deserialization—version snapshots and fall back to full replay when version mismatch detected
  - Creating a snapshot on every save destroys write performance; use event count thresholds
  - Snapshot without version metadata forces full replay from beginning—always store the stream version with the snapshot
  - Snapshot correctness is critical—an incorrect snapshot corrupts all subsequent aggregate loads silently
  concepts:
  - Snapshots trade storage space for reduced replay time
  - Snapshot versioning enables graceful handling of schema changes
  - Snapshot + tail replay pattern combines snapshot with events since snapshot
  - Snapshot invalidation on schema change triggers safe fallback to full replay
  skills:
  - Snapshot serialization and deserialization
  - Schema versioning strategies
  - Performance profiling and optimization
  - Correctness verification via comparison testing
  deliverables:
  - Snapshot creation serializing aggregate state with stream version metadata
  - Snapshot storage persisting snapshots keyed by aggregate ID and version
  - Aggregate loading from snapshot + tail events with full replay fallback
  - Automatic snapshot scheduling based on event count threshold
  - Snapshot version checking with graceful fallback on version mismatch
  - Benchmark comparing load time with and without snapshots on a 10,000-event aggregate
  estimated_hours: 8
- id: event-sourcing-m5
  name: Event Upcasting & Schema Evolution
  description: Implement event upcasting to handle schema evolution in long-lived event streams, ensuring old events can be read and processed by current code.
  acceptance_criteria:
  - Event upcaster transforms events from version N to version N+1 during deserialization, without modifying stored events
  - Upcaster chain applies multiple sequential transformations (v1 → v2 → v3) for events that are multiple versions behind
  - Upcasted events are indistinguishable from natively versioned events to aggregate apply() methods and projection handlers
  - Adding a new event field with a default value is handled by upcaster without breaking existing event replay
  - Renaming an event type is handled by a registered type mapping without modifying stored events
  - 'Test suite demonstrates at least 3 schema evolution scenarios: field addition, field rename, and event type rename'
  pitfalls:
  - Modifying stored events violates immutability—upcasting must happen at read time only
  - Upcaster ordering matters—applying v2→v3 before v1→v2 produces incorrect results; enforce sequential version chain
  - Removing a field without a default value breaks old events—always add with defaults, never remove without upcaster
  - Upcasting performance degrades with many versions; consider snapshotting to reduce the number of events that need upcasting
  - Event type renames require a registry mapping old type names to new ones; without this, deserialization fails silently
  concepts:
  - Event immutability means stored events are never modified
  - Upcasting transforms event shape at read time to match current schema
  - Weak schema (adding optional fields) vs strong schema (breaking changes requiring upcasters)
  - Event type registry maps historical type names to current handler
  - Copy-and-transform pattern creates new event version from old event data
  skills:
  - Schema evolution patterns
  - Serialization versioning
  - Upcaster chain design
  - Backward compatibility testing
  deliverables:
  - Event version annotation marking schema version on each event type
  - Upcaster interface transforming event payload from version N to N+1
  - Upcaster chain applying sequential transformations during event deserialization
  - Event type registry mapping historical type names to current types
  - Test suite covering field addition, field rename, and event type rename scenarios
  - Documentation of schema evolution policy and upcaster registration process
  estimated_hours: 14
domain: data-storage
