id: high-cardinality-metrics
name: Nova Metrics Engine
description: >-
  High-performance time-series database optimized for high-cardinality label
  data with Gorilla-style compression, inverted indexing, WAL durability,
  and real-time query aggregation.
difficulty: expert
estimated_hours: 65
essence: >-
  Time-series compression and inverted indexing targeting high-cardinality
  label sets, using delta-of-delta encoding and bit-packing to achieve
  sub-millisecond aggregations over millions of data points.
why_important: >-
  Modern monitoring systems (like Prometheus or VictoriaMetrics) struggle with
  high cardinality. Building this teaches you advanced compression algorithms
  and data structure optimization for massive-scale time-series data.
learning_outcomes:
  - Implement Delta-of-Delta compression for timestamp storage
  - Design XOR-based floating point compression (Gorilla encoding)
  - Build an inverted index for ultra-fast label-based searching
  - Implement a Write-Ahead Log (WAL) for durable ingestion at high volume
  - Design a query engine for real-time aggregations (Sum, Avg, Percentiles)
  - Implement data downsampling and retention policies
  - Optimize memory layout using mmap and specialized buffer pools
  - Handle ingestion backpressure and concurrent query processing
skills:
  - Systems Programming
  - Database Internals
  - Compression Algorithms
  - Go/Rust
  - Performance Tuning
tags:
  - databases
  - metrics
  - high-performance
  - time-series
architecture_doc: architecture-docs/high-cardinality-metrics/index.md
languages:
  recommended:
    - Go
    - Rust
    - C++
  also_possible:
    - Zig
resources:
  - name: Gorilla Paper
    url: http://www.vldb.org/pvldb/vol8/p1816-teller.pdf
    type: paper
  - name: Prometheus TSDB
    url: https://prometheus.io/docs/prometheus/latest/storage/
    type: documentation
  - name: VictoriaMetrics Design
    url: https://victoriametrics.github.io/
    type: documentation
prerequisites:
  - type: skill
    name: Go or Rust programming
  - type: skill
    name: Basic database concepts
  - type: skill
    name: Bit manipulation
milestones:
  - id: high-cardinality-metrics-m1
    name: TSDB Storage Layout & Compression
    description: >-
      Design the on-disk format and implement Gorilla-style compression for
      timestamps and values.
    acceptance_criteria:
      - "Implement Delta-of-Delta encoding for sequences of timestamps"
      - "Implement XOR compression for floating point value streams"
      - "Achieve at least 10x compression ratio compared to raw 64-bit storage"
      - "Successful serialization and deserialization of compressed data blocks"
      - "Handle out-of-order samples within a configurable time window"
    pitfalls:
      - "Rounding errors in XOR compression can accumulate—use controlled precision"
      - "Inefficient bit-packing logic slows down both compression and decompression"
      - "Out-of-order samples break delta-of-delta assumptions—buffer and reorder or document limitation"
    concepts:
      - Delta-of-Delta exploits timestamp locality
      - Gorilla XOR compression exploits float value similarity
      - Bit-packing reduces storage for small integers
    skills:
      - Compression algorithm implementation
      - Bit manipulation
      - Binary format design
    deliverables:
      - Timestamp compressor using delta-of-delta with variable-length encoding
      - Float value compressor using XOR-based Gorilla encoding
      - Bit-packed container for compressed data
      - Block reader/writer for serialized storage
    estimated_hours: 15

  - id: high-cardinality-metrics-m2
    name: Inverted Index for Labels
    description: >-
      Build the indexing layer to handle high-cardinality label metadata.
    acceptance_criteria:
      - "Create a mapping from label key-value pairs to series IDs"
      - "Implement posting list structure for efficient boolean query evaluation"
      - "Support sub-millisecond lookup for specific label combinations"
      - "Handle millions of unique series IDs without excessive memory usage"
      - "Support label value cardinality in the millions without index explosion"
    pitfalls:
      - "Index size explosion with high cardinality—use interning and compression"
      - "Slow index updates during high-throughput ingestion—batch updates"
      - "Lock contention on the index—use lock-free or fine-grained locking"
    concepts:
      - Inverted index maps terms to document (series) IDs
      - Posting lists store series IDs for each label combination
      - Cardinality management prevents memory exhaustion
    skills:
      - Index data structure design
      - Memory-efficient containers
      - Concurrent data structure design
    deliverables:
      - Label name/value interning to reduce string duplication
      - Inverted index mapping label pairs to series IDs
      - Posting list with compression for series ID storage
      - Boolean query evaluator for label matchers (equals, regex, not-equals)
    estimated_hours: 18

  - id: high-cardinality-metrics-m3
    name: Write-Ahead Log (WAL)
    description: >-
      Implement durable ingestion with write-ahead logging for crash recovery.
    acceptance_criteria:
      - "All ingested samples are written to WAL before being acknowledged"
      - "WAL supports fsync-based durability configuration"
      - "Crash recovery replays WAL to restore unflushed data"
      - "WAL segments are rotated and compacted to manage disk usage"
      - "High-throughput ingestion (100K+ samples/sec) is sustained with WAL enabled"
    pitfalls:
      - "Fsync on every write kills throughput—batch and sync periodically"
      - "WAL replay can be slow if segments are too large—segment by size or time"
      - "Corrupt WAL entries must be detected and skipped—use checksums"
    concepts:
      - WAL provides durability before data is flushed to main storage
      - fsync guarantees data is on disk
      - Checkpointing marks WAL entries as flushed
    skills:
      - File I/O and durability
      - Crash recovery logic
      - Throughput optimization
    deliverables:
      - WAL writer appending samples with checksums
      - WAL reader for crash recovery replay
      - Segment rotation and compaction
      - Configurable fsync policy (always, periodic, never)
    estimated_hours: 10

  - id: high-cardinality-metrics-m4
    name: Real-time Query Engine
    description: >-
      Implement aggregation logic over time-series blocks with precision
      handling.
    acceptance_criteria:
      - "Support range queries with Min, Max, Sum, Count, and Average aggregations"
      - "Implement streaming aggregation to minimize memory allocation"
      - "Use Kahan summation or similar for numerically stable floating-point aggregation"
      - "Query results are accurate within specified precision (not just 0.001)"
      - "Concurrent queries do not block ingestion"
    pitfalls:
      - "Allocating too much memory per query—stream results instead of buffering"
      - "Naive floating-point summation loses precision—use compensated summation"
      - "Blocking ingestion during queries—separate read and write paths"
    concepts:
      - Aggregation pipelines process data in stages
      - Streaming aggregation avoids memory allocation
      - Kahan summation maintains precision for large sums
    skills:
      - Query engine design
      - Numerical precision handling
      - Concurrent execution
    deliverables:
      - Range query executor scanning time-series blocks
      - Streaming aggregators for Sum, Count, Min, Max, Avg
      - Kahan summation for accurate floating-point totals
      - Query result iterator for memory-efficient result delivery
    estimated_hours: 15

  - id: high-cardinality-metrics-m5
    name: Downsampling & Retention
    description: >-
      Implement automatic data downsampling and retention policies.
    acceptance_criteria:
      - "Configure retention periods for raw and downsampled data"
      - "Downsample data using configurable aggregation (avg, max, min)"
      - "Automatic deletion of data beyond retention period"
      - "Downsampling reduces storage while preserving query accuracy for long-term trends"
    pitfalls:
      - "Downsampling too aggressively loses important detail"
      - "Retention deletion must not block queries—use background compaction"
      - "Multiple retention levels complicate query routing"
    concepts:
      - Downsampling reduces resolution for older data
      - Retention policies define data lifetime
      - Compaction reclaims space from deleted data
    skills:
      - Background task scheduling
      - Data lifecycle management
      - Storage optimization
    deliverables:
      - Retention policy configuration (raw, 5m, 1h resolutions)
      - Downsampling job aggregating older data
      - Background deletion of expired data
      - Query routing to appropriate resolution level
    estimated_hours: 7