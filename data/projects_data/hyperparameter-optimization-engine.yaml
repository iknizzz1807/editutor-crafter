id: automl-framework
name: AutoML Framework
description: >
  Build an AutoML framework that automatically searches for optimal hyperparameters
  and neural network architectures. Implement Bayesian optimization, neural architecture
  search, and automated model selection.

difficulty: advanced
estimated_hours: 50-70
domain: ai-ml

essence: >
  Automated machine learning through sequential model-based optimization, where a surrogate
  model (Gaussian Process or Tree Parzen Estimator) guides the search toward promising
  hyperparameter regions, combined with early stopping and pruning to maximize efficiency.

why_important: >
  AutoML democratizes ML by removing the need for expert hyperparameter tuning.
  Tools like Optuna, Ray Tune, and AutoGluon are widely used. Understanding AutoML
  is valuable for ML engineers at $150K-300K+, especially in MLOps and platform roles.

learning_outcomes:
  - Implement Bayesian optimization with Gaussian Process surrogate
  - Build Tree Parzen Estimator (TPE) for hyperparameter search
  - Implement neural architecture search (NAS) with search space definition
  - Build early stopping and pruning strategies (Successive Halving, Hyperband)
  - Design configuration spaces with conditional parameters
  - Implement parallel/distributed optimization
  - Build visualization for search trajectories and importance analysis
  - Create reproducible experiment tracking

skills:
  - Bayesian Optimization
  - Hyperparameter Search
  - Neural Architecture Search
  - Early Stopping Strategies
  - Experiment Tracking
  - Surrogate Models
  - Acquisition Functions
  - Parallel Optimization

tags:
  - advanced
  - automl
  - hyperparameter-optimization
  - bayesian-optimization
  - nas
  - mlops

languages:
  recommended:
    - Python
  also_possible: []

resources:
  - name: "Practical Bayesian Optimization of ML Algorithms"
    url: https://arxiv.org/abs/1206.2944
    type: paper
  - name: "Algorithms for Hyper-Parameter Optimization"
    url: https://papers.nips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html
    type: paper
  - name: "Optuna Documentation"
    url: https://optuna.readthedocs.io/
    type: documentation
  - name: "Ray Tune Documentation"
    url: https://docs.ray.io/en/latest/tune/
    type: documentation

prerequisites:
  - type: skill
    name: Deep learning training and evaluation
  - type: skill
    name: Python and NumPy/SciPy proficiency
  - type: skill
    name: Basic probability and statistics
  - type: project
    name: neural-network-basic or equivalent

milestones:
  - id: automl-m1
    name: Search Space & Configuration
    description: >
      Build the foundational configuration system for defining hyperparameter
      search spaces with various parameter types.
    acceptance_criteria:
      - Search space supports float, int, categorical, and boolean parameters
      - Parameters define ranges (log/linear scale) or choices
      - Conditional parameters supported (e.g., learning rate only if optimizer is 'adam')
      - Configuration sampler generates random configurations from search space
      - Configurations can be serialized/deserialized for reproducibility
      - Search space visualization shows parameter distributions
    pitfalls:
      - Log-scale parameters sampled linearly miss important regions
      - Conditional parameters complicate search - must handle dependencies
      - Not validating configurations leads to invalid hyperparameter combinations
      - Integer parameters need proper rounding after sampling
    concepts:
      - Hyperparameter search spaces
      - Parameter types and distributions
      - Conditional parameters
      - Configuration sampling
    skills:
      - Search space design
      - Parameter types
      - Configuration management
      - Sampling methods
    deliverables:
      - Search space definition API
      - Configuration sampler
      - Conditional parameter support
      - Configuration serialization
    estimated_hours: "8-10"

  - id: automl-m2
    name: Bayesian Optimization
    description: >
      Implement Bayesian optimization with Gaussian Process surrogate model
      and acquisition functions for sequential hyperparameter search.
    acceptance_criteria:
      - Gaussian Process surrogate model fits observed (config, objective) pairs
      - Kernel selection (RBF, Matern) with configurable hyperparameters
      - Acquisition functions: Expected Improvement (EI), Upper Confidence Bound (UCB)
      - Sequential optimization selects next config by maximizing acquisition
      - Balance exploration (high uncertainty) and exploitation (high predicted value)
      - Initial random samples (warm-up) before Bayesian optimization starts
      - Convergence detected when improvement stalls for N trials
    pitfalls:
      - GP doesn't scale to high dimensions - consider random embeddings or TPE
      - Numerical instability in GP with poor kernel hyperparameters
      - Acquisition function optimization is non-trivial - use L-BFGS or random sampling
      - Forgetting to normalize hyperparameters to similar scales
    concepts:
      - Gaussian Process regression
      - Acquisition functions
      - Exploration-exploitation tradeoff
      - Surrogate model optimization
    skills:
      - GP implementation/usage
      - Acquisition functions
      - Sequential optimization
      - Convergence detection
    deliverables:
      - GP surrogate model
      - EI and UCB acquisition
      - Sequential optimizer
      - Convergence detection
    estimated_hours: "12-16"

  - id: automl-m3
    name: Tree Parzen Estimator (TPE)
    description: >
      Implement Tree Parzen Estimator, an alternative to GP that scales better
      to high dimensions and is the core algorithm in Optuna and Hyperopt.
    acceptance_criteria:
      - TPE separates observations into "good" (below threshold) and "bad" (above)
      - Kernel density estimation models l(x) for good configs and g(x) for bad
      - Expected Improvement computed as ratio g(x)/l(x) for sampling
      - TPE handles categorical and conditional parameters naturally
      - Performance comparable to GP on low dimensions, better on high dimensions
      - Sampling from KDE uses inverse CDF transform
    pitfalls:
      - Threshold quantile affects exploration - typically 10-25% best configs
      - KDE bandwidth selection affects smoothness of distribution
      - TPE assumes independence between dimensions - can miss interactions
      - Initial samples before TPE starts are important for good threshold
    concepts:
      - Kernel density estimation
      - Threshold-based separation
      - Expected Improvement approximation
      - High-dimensional optimization
    skills:
      - KDE implementation
      - TPE algorithm
      - High-dimensional search
      - Performance comparison
    deliverables:
      - TPE sampler implementation
      - KDE for good/bad distributions
      - EI-based sampling
      - Comparison with GP optimizer
    estimated_hours: "10-14"

  - id: automl-m4
    name: Early Stopping & Pruning
    description: >
      Implement early stopping strategies to terminate poorly performing trials
      early, saving computation time for promising configurations.
    acceptance_criteria:
      - Successive Halving (SHA) prunes bottom fraction of trials at each rung
      - Hyperband combines SHA with different resource budgets
      - Median pruning stops trials if intermediate result is below median of completed
      - Pruning decisions based on partial training curves (e.g., epoch 5 of 50)
      - Significant compute savings measured vs no pruning
      - Best-found config quality maintained despite pruning
    pitfalls:
      - Too aggressive pruning discards late-bloomers (configs that improve slowly)
      - Resource allocation in Hyperband needs tuning for problem
      - Comparing trials at different epochs requires care - curves aren't comparable
      - Pruning based on training loss instead of validation loss is misleading
    concepts:
      - Early stopping strategies
      - Multi-fidelity optimization
      - Resource allocation
      - Pruning decisions
    skills:
      - SHA implementation
      - Hyperband scheduler
      - Median pruning
      - Compute savings analysis
    deliverables:
      - Successive Halving
      - Hyperband scheduler
      - Median pruner
      - Efficiency comparison
    estimated_hours: "10-14"

  - id: automl-m5
    name: Experiment Tracking & Visualization
    description: >
      Build comprehensive experiment tracking and visualization for AutoML
      experiments, enabling analysis and reproducibility.
    acceptance_criteria:
      - All trials logged with config, objective, and metadata
      - Trial history shows improvement over time (best-so-far curve)
      - Parameter importance analysis (fANOVA or permutation importance)
      - Parallel coordinate plots show config-to-objective relationships
      - Contour plots show 2D slices of search space
      - Experiments can be resumed from saved state
      - Export of best configuration and trained model
    pitfalls:
      - Not saving enough metadata makes post-hoc analysis impossible
      - Visualization at scale (1000s of trials) needs aggregation
      - Parameter importance can be misleading with correlated parameters
      - Resume from checkpoint needs exact environment reproduction
    concepts:
      - Experiment tracking
      - Visualization for optimization
      - Parameter importance
      - Reproducibility
    skills:
      - Experiment logging
      - Visualization design
      - Importance analysis
      - State management
    deliverables:
      - Trial logging system
      - Best-so-far and history plots
      - Parameter importance analysis
      - Resume from checkpoint
    estimated_hours: "8-12"
