id: linear-regression
name: Linear Regression from Scratch
description: Implement linear regression using both closed-form solution and gradient descent optimization, with proper evaluation methodology.
difficulty: beginner
estimated_hours: 10-16
essence: 'Convex optimization of a parametric model using calculus-based iterative weight updates (gradient descent) to minimize mean squared error, with proper train/test evaluation methodology and feature normalization for numerical stability.

  '
why_important: 'Linear regression is the foundation of machine learning optimization—mastering gradient descent here builds intuition for training neural networks and all gradient-based algorithms, while proper evaluation methodology prevents the most common ML mistakes.

  '
learning_outcomes:
- Implement the ordinary least squares closed-form solution for single-variable regression
- Derive and implement gradient descent parameter updates from the MSE cost function
- Apply feature normalization to ensure gradient descent convergence
- Extend single-variable regression to multiple features using matrix operations
- Evaluate model performance using train/test splits and appropriate metrics
- Debug convergence issues by tuning learning rates and visualizing cost curves
- Compare closed-form and iterative optimization approaches
skills:
- Gradient Descent Optimization
- Loss Function Design
- NumPy Vectorization
- Mathematical Derivation
- Model Evaluation
- Feature Normalization
- Data Splitting
tags:
- ai-ml
- beginner-friendly
- gradient-descent
- loss-function
- python
- evaluation
architecture_doc: architecture-docs/linear-regression/index.md
languages:
  recommended:
  - Python
  also_possible:
  - JavaScript
  - Julia
resources:
- name: Linear Regression Tutorial""
  url: https://scikit-learn.org/stable/modules/linear_model.html
  type: documentation
- name: Gradient Descent Explained (3Blue1Brown)""
  url: https://www.youtube.com/watch?v=sDv4f4s2SB8
  type: video
- name: Andrew Ng - Machine Learning (Linear Regression)""
  url: https://www.coursera.org/learn/machine-learning
  type: course
prerequisites:
- type: skill
  name: Python and NumPy basics
- type: skill
  name: Calculus (partial derivatives)
- type: skill
  name: Basic linear algebra (matrix multiplication)
milestones:
- id: linear-regression-m1
  name: Simple Linear Regression (Closed-Form)
  description: 'Implement single-variable linear regression using the ordinary least squares closed-form formula, with proper train/test evaluation.

    '
  acceptance_criteria:
  - Data is split into training set (80%) and test set (20%) with reproducible random seed
  - Closed-form solution computes slope (m) and intercept (b) directly using the OLS normal equation on training data only
  - Predictions are computed as y_hat = m * x + b for both training and test data
  - R-squared is computed on the test set; the implementation handles the case where R² can be negative for poor models
  - MSE (mean squared error) is computed on the test set as the primary loss metric
  - Scatter plot with regression line is generated showing data points and the fitted line
  pitfalls:
  - Computing R² on training data instead of test data gives an inflated estimate of model quality
  - Division by zero when all X values are identical (zero variance)—must check and handle this case
  - Using Python lists instead of NumPy arrays causes performance and precision issues
  - R² can be negative on test data for a bad model—don't assert it's in [0,1]
  concepts:
  - Ordinary least squares
  - Train/test split
  - R-squared and MSE metrics
  - Overfitting vs. generalization
  skills:
  - Implementing mathematical formulas in code
  - Computing statistical metrics
  - Data visualization with matplotlib
  - NumPy array operations
  deliverables:
  - Data loader reading feature and target columns from CSV or NumPy arrays
  - Train/test splitter with configurable ratio and random seed
  - OLS closed-form solution computing slope and intercept from training data
  - Prediction function computing y_hat for new x values
  - Evaluation module computing R², MSE, and MAE on test data
  - Visualization showing scatter plot with regression line
  estimated_hours: 3-4
- id: linear-regression-m2
  name: Gradient Descent Optimization
  description: 'Implement gradient descent to learn the same regression parameters iteratively, with feature normalization for convergence and learning rate visualization.

    '
  acceptance_criteria:
  - Cost function computes MSE as (1/2N) * Σ(y_hat - y)² over training data
  - Gradient computation returns correct partial derivatives ∂J/∂m and ∂J/∂b verified against numerical gradient (finite differences) within 1e-5 tolerance
  - Feature normalization (z-score standardization) is applied before gradient descent to ensure convergence
  - Iterative parameter updates reduce the cost function monotonically for an appropriately chosen learning rate
  - Convergence is detected when |J(t) - J(t-1)| < epsilon (default 1e-8) or max iterations reached
  - Gradient descent solution matches the closed-form solution from M1 within 1e-3 tolerance on the same dataset
  - Cost function vs. iteration number is plotted showing convergence behavior
  pitfalls:
  - Learning rate too high causes divergence (cost increases)—implement divergence detection and abort
  - Without feature normalization, gradient descent takes millions of iterations or diverges entirely
  - Forgetting the 1/N normalization in the gradient causes learning rate sensitivity to dataset size
  - Stopping too early before convergence gives a suboptimal solution
  concepts:
  - Gradient descent algorithm
  - Learning rate selection
  - Feature normalization (z-score)
  - Convergence criteria
  - Numerical gradient verification
  skills:
  - Iterative optimization implementation
  - Hyperparameter tuning (learning rate)
  - Convergence analysis and visualization
  - Numerical verification of analytical gradients
  deliverables:
  - Z-score feature normalizer computing mean and std from training data and applying to both train and test
  - MSE cost function computing average squared error over training set
  - Gradient computation module returning partial derivatives for slope and intercept
  - Gradient descent loop with configurable learning rate, max iterations, and convergence threshold
  - Convergence plot showing cost vs. iteration number
  - Comparison report showing gradient descent parameters vs. closed-form solution
  estimated_hours: 3-5
- id: linear-regression-m3
  name: Multiple Linear Regression
  description: 'Extend to multiple input features using matrix operations with vectorized gradient descent.

    '
  acceptance_criteria:
  - Design matrix X is constructed with an intercept column of ones prepended to the feature matrix
  - Predictions are computed as y_hat = X @ w using matrix-vector multiplication
  - 'Vectorized gradient descent updates the entire weight vector w in a single matrix operation per iteration: w = w - α * (1/N) * X.T @ (X @ w - y)'
  - Feature normalization scales each feature column independently to zero mean and unit variance using training set statistics
  - Model handles 2+ features and produces test-set R² comparable to sklearn's LinearRegression on the same data (within 0.01)
  - Normal equation (closed-form) solution w = (X^T X)^(-1) X^T y is implemented and verified against gradient descent results
  pitfalls:
  - Matrix dimension mismatches (NxD vs Dx1) cause cryptic NumPy errors—validate shapes explicitly
  - Forgetting the bias/intercept column in X causes the model to force through the origin
  - Applying test set statistics for normalization instead of training set statistics causes data leakage
  - Normal equation fails when X^T X is singular (collinear features)—detect and report this condition
  concepts:
  - Matrix formulation of linear regression
  - Vectorized gradient descent
  - Normal equation
  - Data leakage prevention
  skills:
  - Matrix multiplication and linear algebra in NumPy
  - Feature engineering and preprocessing
  - Multi-dimensional gradient descent
  - Vectorized computation
  deliverables:
  - Design matrix constructor prepending intercept column of ones
  - Vectorized gradient descent updating all weights simultaneously via matrix operations
  - Normal equation solver as closed-form alternative with singularity detection
  - Multi-feature normalization applying training set statistics to both train and test data
  - Comparison of gradient descent vs. normal equation vs. sklearn LinearRegression results
  estimated_hours: 3-5
domain: ai-ml
