id: llm-application-patterns
name: LLM Application Patterns
description: >
  Build production-ready LLM applications implementing structured output,
  function calling, multi-turn state management, and prompt chaining
  for reliable AI-powered features.

difficulty: intermediate
estimated_hours: 30-40
domain: ai-ml

essence: >
  Prompt engineering techniques for reliable outputs, structured output
  parsing (JSON, schemas), function/tool calling implementation, conversation
  state management, and error handling for LLM non-determinism.

why_important: >
  LLM applications are transforming every industry. Engineers who can build
  reliable LLM features are in high demand at $150K-300K+ for AI/ML engineers
  and full-stack developers with AI skills.

learning_outcomes:
  - Implement structured output parsing
  - Build function/tool calling systems
  - Manage multi-turn conversation state
  - Implement prompt chaining and routing
  - Handle LLM errors gracefully
  - Build retry and fallback mechanisms
  - Implement streaming responses
  - Design robust prompt templates

skills:
  - Structured Output
  - Function Calling
  - State Management
  - Prompt Chaining
  - Error Handling
  - Streaming
  - Prompt Templates
  - LLM Integration

tags:
  - ai
  - intermediate
  - llm
  - openai
  - prompts
  - rag
  - streaming
  - structured-output

languages:
  recommended:
    - Python
    - TypeScript
  also_possible:
    - JavaScript
    - Go

resources:
  - name: "OpenAI API Documentation"
    url: https://platform.openai.com/docs
    type: documentation
  - name: "Anthropic API Documentation"
    url: https://docs.anthropic.com/
    type: documentation
  - name: "Prompt Engineering Guide"
    url: https://www.promptingguide.ai/
    type: tutorial
  - name: "LangChain Documentation"
    url: https://python.langchain.com/docs/
    type: documentation

prerequisites:
  - type: skill
    name: Python or TypeScript proficiency
  - type: skill
    name: API integration experience
  - type: skill
    name: Basic understanding of LLMs
  - type: project
    name: chatbot-intent or equivalent

milestones:
  - id: llmapp-m1
    name: Structured Output
    description: >
      Implement reliable structured output parsing from LLM responses
      using JSON mode and schema validation.
    acceptance_criteria:
      - JSON mode API usage (OpenAI/Anthropic)
      - Schema definition for output structure
      - Validation of LLM output against schema
      - Retry logic for malformed outputs
      - Handling of edge cases (empty, truncated)
      - Support for nested structures
      - Type-safe output in application code
    pitfalls:
      - LLMs can still produce invalid JSON
      - Token limits truncate JSON output
      - Schema too complex confuses model
      - Not all models support JSON mode
      - Validation failures need retry strategy
    concepts:
      - JSON mode
      - Schema validation
      - Retry logic
      - Type safety
    skills:
      - Schema design
      - Validation
      - Error handling
      - API integration
    deliverables:
      - Structured output wrapper
      - Schema validator
      - Retry mechanism
      - Type definitions
    estimated_hours: "6-7"

  - id: llmapp-m2
    name: Function Calling
    description: >
      Implement function/tool calling for LLMs to execute actions and
      retrieve information.
    acceptance_criteria:
      - Function definition schema
      - Function call parsing from response
      - Function execution engine
      - Result formatting for LLM
      - Multiple function support
      - Parallel function calling
      - Error handling for function failures
    pitfalls:
      - Function definitions too complex
      - Not handling missing parameters
      - Function timeout handling
      - Infinite loops with function calls
      - Security: validate function inputs
    concepts:
      - Function schema
      - Execution engine
      - Result formatting
      - Security
    skills:
      - Schema design
      - Execution
      - Error handling
      - Security validation
    deliverables:
      - Function registry
      - Executor
      - Result formatter
      - Error handler
    estimated_hours: "6-8"

  - id: llmapp-m3
    name: Multi-Turn State Management
    description: >
      Implement conversation state management for coherent multi-turn
      interactions with LLMs.
    acceptance_criteria:
      - Message history tracking
      - Token counting and management
      - Context window limit handling
      - Conversation summarization for long chats
      - Session storage (memory, database)
      - System prompt management
      - User/assistant message alternation
    pitfalls:
      - Context overflow causes errors
      - Token counting varies by model
      - Summarization loses context
      - Not persisting state loses conversations
      - System prompt in wrong position
    concepts:
      - Message history
      - Token management
      - Summarization
      - State persistence
    skills:
      - State management
      - Token counting
      - Summarization
      - Persistence
    deliverables:
      - Conversation manager
      - Token counter
      - Summarizer
      - State storage
    estimated_hours: "6-8"

  - id: llmapp-m4
    name: Prompt Chaining & Routing
    description: >
      Implement prompt chaining for complex tasks and routing to
      appropriate prompts based on input.
    acceptance_criteria:
      - Chain multiple LLM calls
      - Pass outputs between chain steps
      - Conditional routing based on input
      - Parallel prompt execution
      - Fallback chains for reliability
      - Chain monitoring and debugging
      - Timeout handling for chains
    pitfalls:
      - Chain length increases latency
      - Error in middle of chain
      - Context loss between steps
      - Timeout with long chains
      - Debugging complex chains hard
    concepts:
      - Prompt chains
      - Routing
      - Parallel execution
      - Monitoring
    skills:
      - Chain design
      - Routing logic
      - Error recovery
      - Debugging
    deliverables:
      - Chain executor
      - Router
      - Parallel execution
      - Debug logging
    estimated_hours: "6-7"

  - id: llmapp-m5
    name: Streaming & Production Ready
    description: >
      Add streaming support and production features for a complete
      LLM application.
    acceptance_criteria:
      - SSE streaming for responses
      - Graceful degradation on errors
      - Rate limiting and cost tracking
      - Caching for repeated queries
      - Health checks and monitoring
      - Configuration management
      - Complete example application
    pitfalls:
      - Streaming errors mid-response
      - Rate limits from provider
      - Cost overruns from caching misses
      - Stale cache issues
      - Monitoring overhead
    concepts:
      - Streaming
      - Error handling
      - Cost management
      - Production features
    skills:
      - SSE implementation
      - Error handling
      - Caching
      - Monitoring
    deliverables:
      - Streaming handler
      - Error recovery
      - Cost tracker
      - Complete app
    estimated_hours: "6-8"
