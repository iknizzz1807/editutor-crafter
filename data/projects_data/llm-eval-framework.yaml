id: llm-eval-framework
name: LLM Evaluation Framework
description: >-
  Build a comprehensive evaluation system for LLM applications with automated
  metrics, LLM-as-judge evaluation, prompt versioning, and regression detection.
difficulty: advanced
estimated_hours: "45-65"
essence: >-
  Automated quality measurement of generative model outputs through embedding
  similarity, calibrated LLM-as-judge patterns, and reference-based metrics,
  combined with prompt and dataset versioning, parallel execution with caching,
  and statistical significance testing to detect regression in production systems.
why_important: >-
  Production LLM systems degrade silently through prompt drift, model updates,
  and data shifts. Building evaluation infrastructure teaches systematic quality
  measurement and regression detection essential for deploying AI reliably.
  "If you can't measure it, you can't improve it."
learning_outcomes:
  - Design versioned evaluation datasets with prompt versioning and stratified sampling
  - Implement reference-based metrics (BLEU, ROUGE) and understand their limitations
  - Build calibrated LLM-as-judge evaluation with consistency checking and position bias detection
  - Implement embedding-based semantic similarity scoring
  - Create parallel evaluation runners with caching, rate limiting, and crash recovery
  - Develop statistical analysis for metric aggregation and significance testing
  - Build automated regression detection with configurable thresholds
  - Design experiment tracking comparing prompt variations and model versions
skills:
  - LLM-as-judge patterns and calibration
  - Evaluation metrics design
  - Prompt and dataset versioning
  - Statistical analysis and significance testing
  - Parallel processing with rate limiting
  - Regression detection
tags:
  - advanced
  - ai-ml
  - evaluation
  - framework
  - metrics
  - prompts
  - python
  - llm
architecture_doc: architecture-docs/llm-eval-framework/index.md
languages:
  recommended:
    - Python
  also_possible:
    - TypeScript
resources:
  - name: Anthropic Eval Best Practices
    url: https://docs.anthropic.com/en/docs/build-with-claude/develop-tests
    type: documentation
  - name: LangSmith
    url: https://docs.smith.langchain.com/
    type: documentation
  - name: Braintrust
    url: https://www.braintrustdata.com/docs
    type: documentation
  - name: RAGAS (RAG Evaluation)
    url: https://docs.ragas.io/
    type: documentation
prerequisites:
  - type: skill
    name: LLM APIs (OpenAI, Anthropic)
  - type: skill
    name: Basic statistics (mean, variance, hypothesis testing)
  - type: skill
    name: Python (asyncio, dataclasses)
milestones:
  - id: llm-eval-framework-m1
    name: Dataset & Prompt Versioning
    description: >-
      Create and version evaluation datasets alongside the prompts they test.
      Prompt changes ARE code changes and must be tracked.
    acceptance_criteria:
      - "Test case schema validates required fields: input prompt, expected output (optional for reference-free), tags, and difficulty level"
      - "Datasets are versioned with content-hash-based IDs so identical datasets always produce the same version"
      - "Dataset changes are diffable showing added, removed, and modified test cases between versions"
      - "Prompt templates are versioned separately from test cases with their own version history"
      - "Each evaluation run records the exact dataset version AND prompt version used"
      - "Import from CSV, JSON, and JSONL files maps columns to the test case schema with validation errors on missing fields"
      - "Dataset splits into test and validation subsets with configurable ratios; stratification by tags ensures balanced representation"
      - "At least 50 test cases loaded for meaningful evaluation (warn if fewer)"
    pitfalls:
      - "Not versioning prompts separately from datasets means you can't tell if a metric change came from data or prompt changes"
      - "Version conflicts with concurrent edits—use content-hash versioning, not sequential integers"
      - "Large datasets slow to load—use lazy loading or memory-mapped files for 10K+ test cases"
      - "Not tracking dataset provenance (who created it, from what source) makes debugging impossible"
      - "Test cases with ambiguous or multiple valid answers break exact-match scoring—tag these cases"
    concepts:
      - Content-addressable dataset versioning
      - Prompt template versioning
      - Train/test splits with stratification
      - Data lineage and provenance
    skills:
      - Data serialization (JSON, JSONL, Parquet)
      - Content hashing for versioning
      - Schema validation
      - Stratified sampling
    deliverables:
      - Dataset loader supporting CSV, JSON, and JSONL with schema validation
      - Test case structure with input, expected output, tags, difficulty, and metadata
      - Dataset versioning system using content hashing with diff capability
      - Prompt template versioning system tracking prompt changes independently
      - Dataset splitter with configurable ratios and tag-stratified sampling
      - Version metadata recorder linking evaluation runs to exact dataset and prompt versions
    estimated_hours: "7-10"

  - id: llm-eval-framework-m2
    name: Evaluation Metrics & LLM-as-Judge
    description: >-
      Implement automated evaluation metrics from simple string matching to
      calibrated LLM-as-judge scoring. Understand the limitations of each.
    acceptance_criteria:
      - "Exact match and fuzzy match (Levenshtein ratio >= threshold) with whitespace normalization and case-insensitive option"
      - "BLEU and ROUGE computed as baseline metrics with documented limitations (penalizes valid paraphrases, rewards surface overlap)"
      - "Semantic similarity computes cosine similarity between embeddings of output and reference; calibration data establishes what score thresholds map to \"good\"/\"acceptable\"/\"poor\""
      - "LLM-as-judge sends output (and optionally reference) to a judge model with a rubric prompt and parses a structured score (1-5 scale)"
      - "LLM-as-judge consistency validated: same input scored 3 times, standard deviation < 0.5 for at least 90% of cases\"\""
      - "Position bias check: when judge evaluates two outputs, swapping their order should not change the winner more than 10% of the time\"\""
      - "Golden example calibration: judge scores on 10 pre-scored golden examples must correlate >= 0.8 with human scores"
      - "Custom metric functions registered as plugins, returning float in [0, 1] range, called automatically during evaluation"
      - "All metrics handle edge cases: empty strings return 0, None values return 0 with a warning"
    pitfalls:
      - "BLEU/ROUGE give false confidence—a grammatically different but semantically identical answer scores poorly; always pair with semantic similarity"
      - "LLM-as-judge is non-deterministic; without consistency checking, scores vary 20-30% across runs"
      - "Position bias in LLM judges means the first option is preferred regardless of quality; always test for this"
      - "Semantic similarity scores are not calibrated by default; 0.85 cosine similarity might be \"good\" or \"mediocre\" depending on the embedding model"
      - "Custom metrics not normalized to [0,1] break aggregate scoring and comparison"
    concepts:
      - Reference-based vs reference-free evaluation
      - LLM-as-judge calibration and consistency
      - Position bias in automated evaluation
      - Metric calibration with golden examples
    skills:
      - String similarity algorithms
      - Embedding-based semantic comparison
      - LLM API integration for evaluation
      - Statistical consistency analysis
    deliverables:
      - Exact match and fuzzy match scorers with normalization options
      - BLEU and ROUGE calculators (documented as baseline/legacy metrics)
      - Semantic similarity scorer with calibration threshold documentation
      - LLM-as-judge evaluator with rubric prompt, structured score parsing, and consistency checking
      - Position bias detector for LLM judge
      - Golden example calibrator validating judge correlation with human scores
      - Custom metric plugin system with [0,1] normalization enforcement
    estimated_hours: "10-14"

  - id: llm-eval-framework-m3
    name: Evaluation Runner
    description: >-
      Run evaluations efficiently with parallel execution, rate limiting,
      caching, and crash recovery.
    acceptance_criteria:
      - "Batch evaluation processes all test cases, records each model response and all metric scores"
      - "Parallel LLM calls execute with configurable concurrency limit (default 5) using asyncio"
      - "Rate limiting with exponential backoff handles HTTP 429 responses; retries up to 3 times with jitter"
      - "Result cache stores responses keyed by hash(prompt_version + test_case_id + model_name); cache invalidates when any component changes"
      - "Progress tracking displays live counter of completed, pending, failed, and cached evaluations"
      - "Checkpoint saves progress every N evaluations (default 50) so runs can resume after crash"
      - "Failed evaluations are logged with error details and retried once; persistent failures are recorded as errors, not skipped silently"
      - "Supports multiple LLM backends: OpenAI, Anthropic, and local model inference (e.g., vLLM, Ollama)"
    pitfalls:
      - "Rate limiting without exponential backoff and jitter causes thundering herd on retry"
      - "Cache keyed only by prompt text misses prompt template version changes"
      - "Memory issues when holding all results in memory for large evaluation sets; stream results to disk"
      - "Lost progress on crash without checkpointing wastes API budget"
      - "Not recording failed evaluations means you don't know which test cases are problematic"
    concepts:
      - Async parallel execution with concurrency control
      - Rate limiting with exponential backoff and jitter
      - Content-addressed result caching
      - Checkpointing for crash recovery
    skills:
      - Python asyncio and semaphore-based concurrency
      - Exponential backoff implementation
      - Hash-based caching strategies
      - Progress tracking and checkpointing
    deliverables:
      - Multi-backend LLM client supporting OpenAI, Anthropic, and local inference APIs
      - Parallel evaluation runner with configurable concurrency and semaphore-based limiting
      - Rate limiter with exponential backoff, jitter, and retry on HTTP 429
      - Result cache keyed by prompt version + test case + model with automatic invalidation
      - Checkpoint manager saving progress to disk for crash recovery
      - Progress tracker displaying live completion stats and ETA
    estimated_hours: "10-14"

  - id: llm-eval-framework-m4
    name: Reporting, Regression Detection & CI/CD
    description: >-
      Generate actionable reports, detect regressions with statistical rigor,
      and integrate into CI/CD pipelines.
    acceptance_criteria:
      - "Score breakdown groups results by tags/categories showing per-group mean, median, and standard deviation"
      - "Regression detection compares current run against a stored baseline; flags metrics that degraded by more than a configurable threshold (default 5%)"
      - "Statistical significance test (paired bootstrap or Wilcoxon signed-rank) determines if degradation is real (p < 0.05) vs random noise"
      - "Minimum sample size warning triggers when tag groups have fewer than 20 cases (insufficient for reliable comparison)"
      - "Failure analysis clusters common error patterns by embedding similarity and shows representative examples per cluster"
      - "Reports exported as self-contained HTML with charts, tables, and drill-down into individual test cases"
      - "CI/CD integration: script returns exit code 0 (pass) or 1 (fail) based on configurable metric thresholds"
      - "Comparison mode shows side-by-side diffs between two evaluation runs with per-metric delta and significance"
    pitfalls:
      - "Small sample sizes give unreliable comparisons; always report confidence intervals alongside point estimates"
      - "Multiple comparisons without Bonferroni or FDR correction inflates false positive rate"
      - "Ignoring variance in aggregate scores hides bimodal distributions where the model either succeeds or fails completely"
      - "Report generation that fails silently on chart rendering errors produces incomplete reports"
      - "CI threshold set too tight causes constant failures; set too loose misses real regressions"
    concepts:
      - Statistical significance testing for evaluation
      - Multiple comparison correction
      - Error pattern clustering
      - CI/CD integration patterns
    skills:
      - Statistical hypothesis testing (bootstrap, Wilcoxon)
      - Data visualization (matplotlib, plotly)
      - HTML report generation
      - CI/CD exit code integration
    deliverables:
      - Score aggregation module computing per-tag and overall summaries with confidence intervals
      - Regression detector comparing against stored baseline with statistical significance testing
      - Failure analysis tool clustering errors by embedding similarity with representative examples
      - HTML report generator with charts, tables, and per-case drill-down
      - Comparison report showing side-by-side metric diffs between two runs
      - CI/CD integration script with configurable pass/fail thresholds and exit codes
    estimated_hours: "10-14"