id: logging-structured
name: Structured Logging
description: JSON-structured logging library with level filtering, context propagation, correlation ID threading, log rotation, backpressure handling, and schema versioning for machine-parseable observability across distributed system boundaries.
difficulty: intermediate
estimated_hours: 14-20
essence: Log level-based event filtering, key-value serialization to JSON with schema versioning, context/correlation identifier threading through execution paths, backpressure-aware output dispatch, and log rotation/retention for machine-parseable observability across distributed system boundaries.
why_important: Production systems generate millions of log entries daily—structured logging enables automated parsing, aggregation, and correlation across microservices, making debugging and monitoring scalable where grep and text parsing fail. Without rotation, buffering, and schema discipline, logging itself becomes a production incident vector.
learning_outcomes:
- Implement log levels (DEBUG, INFO, WARN, ERROR, FATAL) with numeric severity filtering
- Design versioned JSON schema for structured log entries with timestamp, level, message, and contextual fields
- Build context propagation mechanisms using thread-local storage or context variables
- Implement correlation ID generation and propagation across service boundaries via HTTP headers
- Configure log output destinations with rotation policies and retention limits
- Implement backpressure handling and local buffering for unreliable remote destinations
- Design logging strategies that avoid performance bottlenecks through lazy evaluation, buffered writes, and sampling
- Redact sensitive data (PII, secrets, tokens) from structured log output
- Debug distributed system failures by tracing requests across multiple services using correlation IDs
skills:
- Structured logging
- JSON serialization
- Context propagation
- Correlation ID tracing
- Log aggregation patterns
- MDC (Mapped Diagnostic Context)
- Log level management
- Log rotation and retention
- Backpressure handling
- Sensitive data redaction
- Production observability
tags:
- aggregation
- go
- intermediate
- java
- json
- levels
- python
architecture_doc: architecture-docs/logging-structured/index.md
languages:
  recommended:
  - Python
  - Go
  - Java
  also_possible:
  - JavaScript
  - Rust
resources:
- name: 12 Factor App - Logs""
  url: https://12factor.net/logs
  type: article
- name: Structured Logging Guide""
  url: https://www.honeycomb.io/blog/structured-logging
  type: article
- name: Python structlog Documentation""
  url: https://www.structlog.org/en/stable/
  type: documentation
- name: Go slog Package""
  url: https://pkg.go.dev/log/slog
  type: documentation
prerequisites:
- type: skill
  name: JSON
- type: skill
  name: File I/O
- type: skill
  name: Basic debugging concepts
- type: skill
  name: Concurrency fundamentals (threads or goroutines)
milestones:
- id: logging-structured-m1
  name: Logger Core
  description: Build the core logging infrastructure with level filtering, thread-safe multi-destination dispatch, and runtime-configurable log levels.
  acceptance_criteria:
  - Log levels DEBUG, INFO, WARN, ERROR, and FATAL are defined with numeric ordering; messages below the configured minimum level are discarded before serialization
  - Minimum log level is configurable per logger instance and can be changed at runtime without process restart, taking effect on the next log call
  - 'Logging operations are thread-safe: 100 concurrent goroutines/threads writing simultaneously produce zero corrupted or interleaved JSON lines in output'
  - Log records are dispatched to multiple output handlers (stdout, file, remote HTTP/TCP) with independent failure handling per handler
  - Handler failures (e.g., file write error, remote collector timeout) do not block the calling application thread for more than a configurable timeout (default 50ms)
  pitfalls:
  - Interleaved partial JSON lines when multiple threads write to the same fd without atomic writes or mutex protection—test with high concurrency to verify
  - 'Blocking I/O in the log dispatch hot path: if a remote collector is slow, the application thread stalls. Use async dispatch with bounded queues.'
  - Forgetting to propagate log level changes to child loggers when parent level is updated at runtime
  - Using fmt.Sprintf or string concatenation eagerly for log messages that will be filtered—use lazy evaluation to avoid allocation overhead
  concepts:
  - Log levels and severity ordering
  - Thread safety via mutexes or lock-free structures
  - Handler/sink dispatch pattern
  - Lazy evaluation for log message construction
  skills:
  - Concurrent programming
  - Implementing thread-safe data structures
  - Building extensible handler architectures
  - Configuring log levels and filtering
  deliverables:
  - Log level manager supporting DEBUG, INFO, WARN, ERROR, and FATAL with numeric ordering and runtime-changeable minimum level
  - Logger hierarchy system that creates child loggers inheriting parent configuration and context, with override capability
  - Log record factory that constructs structured records with timestamp (UTC), level, message, logger name, and metadata map
  - Handler dispatch mechanism that routes log records to one or more configured output destinations with per-handler error isolation
  estimated_hours: 3-4
- id: logging-structured-m2
  name: Structured Output with Schema Versioning
  description: Format logs as structured JSON with a versioned schema, support custom formatters, and handle sensitive data redaction.
  acceptance_criteria:
  - JSON output produces valid single-line JSON per log record; each record includes a 'schema_version' field (e.g., '1.0') for forward-compatible evolution
  - 'Each log record includes at minimum: timestamp (ISO 8601 with timezone), level, message, logger_name, schema_version, and any attached context fields'
  - 'Non-serializable values (functions, circular references, binary data) are handled gracefully: replaced with a ''[non-serializable: <type>]'' placeholder, never causing a serialization exception'
  - 'Sensitive field redaction is supported: fields matching configurable patterns (e.g., ''password'', ''token'', ''ssn'') are replaced with ''[REDACTED]'' in output'
  - Pretty-print mode formats JSON with indentation for TTY-detected stdout; ANSI color codes are suppressed when output is not a TTY (piped to file or redirected)
  - Custom formatters can be registered by name and selected per handler to control log output appearance (e.g., logfmt, CSV, custom)
  pitfalls:
  - Non-serializable values (e.g., socket objects, lambda functions) in context fields cause silent log drops or crashes—always implement a safe fallback serializer
  - 'Forgetting to flush buffered writers: logs are lost on process crash if the buffer is not flushed on FATAL or at shutdown'
  - Large context objects (e.g., entire request bodies) attached to every log line cause massive I/O and storage bloat—enforce a max context field size
  - ANSI escape codes written to non-TTY outputs corrupt log files and break JSON parsers downstream
  - Schema version not included from day one means retroactive identification of log format is impossible
  concepts:
  - Structured logging with JSON
  - Schema versioning for log evolution
  - Formatter plugin pattern
  - Sensitive data redaction
  skills:
  - JSON serialization and safe encoding
  - Custom formatter implementation
  - Stream-based output handling
  - Sanitizing sensitive data from logs
  deliverables:
  - JSON formatter that serializes log records as single-line JSON with consistent field ordering and schema_version field
  - Key-value pair enricher that attaches arbitrary context fields to each log record with max field size enforcement
  - Timestamp formatter supporting ISO 8601 (default), Unix epoch millis, and custom strftime patterns
  - Sensitive field redactor that replaces values of fields matching configurable regex patterns with '[REDACTED]'
  - Custom formatter plugin system allowing user-registered output formats selectable per handler
  estimated_hours: 3-4
- id: logging-structured-m3
  name: Context & Correlation
  description: Add request context propagation and correlation ID threading through execution paths, including across async/await boundaries.
  acceptance_criteria:
  - A unique correlation ID (UUID v4 or equivalent) is automatically generated at request entry and injected into every log record within that request scope
  - Incoming HTTP requests with an existing correlation ID header (e.g., X-Request-ID, traceparent) reuse that ID instead of generating a new one
  - Context key-value pairs propagate through nested function calls without explicit parameter passing, using thread-local storage (Python threading.local, Go context.Context, Java MDC)
  - Child loggers inherit all context fields from their parent and can add additional fields without mutating the parent's context
  - 'Logging context is preserved correctly across async/await boundaries: a test with 50 concurrent async tasks each logging with distinct correlation IDs shows zero cross-contamination'
  - Context cleanup occurs automatically at request scope exit to prevent memory leaks in long-running server processes
  pitfalls:
  - Thread-local storage does not propagate to child threads or async tasks by default—each language has different semantics (Python contextvars vs threading.local, Go context.Context is explicit, Java InheritableThreadLocal has footguns)
  - Memory leaks from context not being cleaned up at end of request scope in long-running processes—use context managers or defer patterns
  - 'Async context loss: Python''s threading.local is NOT async-safe; must use contextvars. Go''s context.Context must be explicitly passed. Java''s MDC doesn''t propagate to CompletableFuture threads.'
  - 'Correlation ID collision: UUID v4 collision probability is negligible but using sequential IDs or short random strings can collide at scale'
  concepts:
  - Correlation IDs and distributed tracing context
  - Thread-local vs context-variable storage
  - Request lifecycle and scope management
  - W3C Trace Context / X-Request-ID propagation
  skills:
  - Thread-local storage patterns
  - Middleware and decorator patterns
  - Distributed tracing fundamentals
  - Managing request lifecycle context
  deliverables:
  - Context storage abstraction that maintains key-value pairs scoped to the current execution context (thread-local or contextvars)
  - Correlation ID middleware that extracts or generates a request ID from HTTP headers and attaches it to the logging context
  - Request context middleware that extracts request metadata (method, path, user agent) and attaches it to the logging context
  - Async context bridge that preserves logging context when crossing async/await task boundaries, verified with concurrent async test
  - Context cleanup hook that automatically removes context data at request scope exit
  estimated_hours: 3-4
- id: logging-structured-m4
  name: Log Rotation, Retention & Backpressure
  description: Implement file-based log rotation with retention policies, and add backpressure handling with local buffering for unreliable remote destinations.
  acceptance_criteria:
  - File-based log rotation triggers when log file exceeds a configurable size threshold (e.g., 100MB) or at configurable time intervals (e.g., daily)
  - Rotated log files are named with a timestamp suffix and optionally compressed (gzip); at most N rotated files are retained, older ones are deleted automatically
  - When a remote log collector is unavailable, log records are buffered locally in a bounded disk-backed queue (configurable max size, e.g., 500MB)
  - 'When the local buffer is full, the system applies a configurable backpressure policy: either drop oldest entries, drop newest entries, or block the caller with a timeout'
  - Buffered logs are replayed to the remote collector in order upon reconnection, with at-least-once delivery semantics
  - A health/status endpoint or log line reports the current buffer depth and remote collector connection status
  pitfalls:
  - 'Rotation race condition: if multiple processes write to the same log file, rotation by one process can cause the other to write to a stale file descriptor—use advisory locking or separate files per process'
  - Unbounded local buffer during prolonged remote outage will exhaust disk—always enforce a max buffer size
  - Replaying buffered logs too aggressively on reconnection can overwhelm the collector—implement rate-limited replay
  - Retention cleanup running concurrently with rotation can delete the file currently being written—use atomic rename and ordered cleanup
  - Forgetting to handle SIGHUP for log rotation in daemon processes (Unix convention for triggering log reopen)
  concepts:
  - File rotation strategies (size-based, time-based)
  - Retention policies and disk space management
  - Backpressure patterns (drop, block, buffer)
  - At-least-once delivery with disk-backed queues
  skills:
  - File system operations and atomic renames
  - Disk-backed queue implementation
  - Signal handling for log rotation
  - Backpressure and flow control
  deliverables:
  - Size-based and time-based file rotation handler that renames current log, opens new file, and optionally compresses the rotated file
  - Retention policy enforcer that deletes rotated log files exceeding the configured count or age threshold
  - Disk-backed buffer for remote handler that persists log records locally when the remote collector is unreachable
  - Replay mechanism that drains the local buffer to the remote collector upon reconnection with configurable rate limiting
  - Backpressure policy configuration allowing selection of drop-oldest, drop-newest, or block-with-timeout when buffer is full
  estimated_hours: 4-6
domain: software-engineering
