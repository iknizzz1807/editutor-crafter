id: mixture-of-experts-engine
name: Mixture of Experts Engine
description: >
  Build a Mixture of Experts (MoE) engine implementing sparse activation patterns
  like those used in GPT-4, Mixtral, and Grok. Learn how to route tokens to
  specialized experts while maintaining efficient training and inference.

difficulty: expert
estimated_hours: 60-80
domain: ai-ml

essence: >
  Sparse expert routing where each input token activates only a subset of experts
  (typically 2 out of 8), multiplying model capacity without proportional compute increase.
  Router learns to specialize experts while load balancing prevents collapse.

why_important: >
  MoE is the architecture behind GPT-4, Mixtral 8x7B, and Grok-1. It enables
  trillion-parameter models with reasonable inference costs. Understanding MoE
  is essential for LLM engineers at top AI labs ($250K-500K+).

learning_outcomes:
  - Implement sparse MoE layer with top-k routing
  - Build differentiable router with load balancing loss
  - Implement expert parallelism for distributed training
  - Handle token dropping and expert capacity constraints
  - Implement efficient MoE inference with expert caching
  - Compare dense vs sparse scaling on language modeling
  - Debug routing collapse and expert imbalance
  - Optimize communication patterns for expert parallelism

skills:
  - Mixture of Experts
  - Sparse Routing
  - Load Balancing
  - Expert Parallelism
  - Distributed Training
  - Capacity Constraints
  - Communication Optimization
  - MoE Debugging

tags:
  - expert
  - mixture-of-experts
  - moe
  - sparse-activation
  - llm
  - gpt-4
  - mixtral

languages:
  recommended:
    - Python
  also_possible: []

resources:
  - name: "Outrageously Large Neural Networks: The Sparsely-Gated MoE Layer"
    url: https://arxiv.org/abs/1701.06538
    type: paper
  - name: "GShard: Scaling Giant Models with Conditional Computation"
    url: https://arxiv.org/abs/2006.16668
    type: paper
  - name: "Switch Transformers"
    url: https://arxiv.org/abs/2101.03961
    type: paper
  - name: "Mixtral of Experts"
    url: https://arxiv.org/abs/2401.04088
    type: paper

prerequisites:
  - type: project
    name: transformer-scratch or equivalent
  - type: skill
    name: PyTorch proficiency
  - type: skill
    name: Multi-GPU training experience
  - type: skill
    name: Understanding of attention and feed-forward layers

milestones:
  - id: moe-m1
    name: Basic MoE Layer
    description: >
      Implement a basic Mixture of Experts layer with top-k routing
      and expert feed-forward networks.
    acceptance_criteria:
      - MoE layer replaces standard FFN in transformer block
      - Router outputs logits over E experts, selects top-k (typically k=2)
      - Each expert is a standard FFN (same architecture as dense layer)
      - Selected experts process tokens in parallel
      - Outputs weighted by router softmax probabilities
      - Works on single GPU with moderate number of experts (E=4-8)
      - Forward pass produces same shaped output as dense FFN
    pitfalls:
      - Not handling tokens with no experts selected (use k>=1 always)
      - Router logits too extreme -> hard routing, no gradients
      - Forgetting to weight expert outputs by router probabilities
      - Memory overhead scales with number of experts
    concepts:
      - Expert routing
      - Top-k selection
      - Sparse activation
      - Expert FFN architecture
    skills:
      - MoE layer implementation
      - Router design
      - Sparse operations
      - Integration with transformer
    deliverables:
      - Working MoE layer
      - Top-k router
      - Integration with transformer block
      - Single-GPU validation
    estimated_hours: "12-16"

  - id: moe-m2
    name: Load Balancing Loss
    description: >
      Implement auxiliary losses to encourage balanced expert utilization
      and prevent routing collapse where all tokens go to one expert.
    acceptance_criteria:
      - Router softmax produces differentiable expert selection probabilities
      - Auxiliary loss: load balancing loss from GShard/Switch Transformer
      - Loss encourages uniform expert utilization while maintaining performance
      - Router-Z loss (regularization) prevents logits from growing unbounded
      - Without auxiliary loss, routing collapses (all tokens to 1-2 experts)
      - With auxiliary loss, experts are roughly evenly utilized
      - Coefficient for auxiliary loss is configurable
    pitfalls:
      - Auxiliary loss too strong -> uniform random routing, no specialization
      - Auxiliary loss too weak -> routing collapse
      - Not accounting for expert capacity in load balancing
      - Router-Z loss coefficient needs tuning
    concepts:
      - Load balancing
      - Auxiliary losses
      - Routing collapse
      - Differentiable routing
    skills:
      - Load balancing loss
      - Routing regularization
      - Hyperparameter tuning
      - Expert utilization analysis
    deliverables:
      - Load balancing auxiliary loss
      - Router-Z regularization
      - Collapse detection/prevention
      - Utilization analysis
    estimated_hours: "10-14"

  - id: moe-m3
    name: Expert Capacity & Token Dropping
    description: >
      Implement capacity constraints to bound memory and compute,
      handling tokens that exceed expert capacity gracefully.
    acceptance_criteria:
      - Each expert has maximum capacity C = (tokens/batch) * k / E * capacity_factor
      - Tokens exceeding capacity are either dropped or routed to next-best expert
      - Token dropping tracked and reported
      - Capacity factor (e.g., 1.25) provides buffer for load imbalance
      - Differentiable capacity enforcement (soft routing) or hard dropping
      - Inference can use lower capacity factor than training
    pitfalls:
      - Capacity too low -> many tokens dropped, performance degrades
      - Capacity too high -> no benefit from sparsity
      - Token dropping at boundaries causes issues in autoregressive generation
      - Not tracking dropped tokens hides routing problems
    concepts:
      - Expert capacity
      - Token dropping
      - Capacity factor
      - Soft vs hard routing
    skills:
      - Capacity implementation
      - Token dropping handling
      - Capacity tuning
      - Dropping metrics
    deliverables:
      - Capacity-constrained routing
      - Token dropping mechanism
      - Capacity factor tuning
      - Dropping metrics tracking
    estimated_hours: "8-12"

  - id: moe-m4
    name: Expert Parallelism
    description: >
      Implement distributed training with expert parallelism, where different
      experts are placed on different GPUs to scale to many experts.
    acceptance_criteria:
      - Each GPU holds subset of experts (E_local = E_total / num_gpus)
      - All-to-all communication routes tokens to expert GPUs
      - All-to-all communication returns expert outputs to original GPUs
      - Gradient synchronization handles expert parameters correctly
      - Scaling efficiency measured: 4 GPUs should get ~3-4x throughput
      - Works with standard data parallelism for non-MoE parameters
    pitfalls:
      - All-to-all communication is expensive - batch tokens efficiently
      - Load imbalance between GPUs if routing is uneven
      - Communication overhead dominates if experts are too small
      - Not overlapping communication with computation
    concepts:
      - Expert parallelism
      - All-to-all communication
      - Distributed routing
      - Communication optimization
    skills:
      - Expert parallelism
      - All-to-all ops
      - Distributed routing
      - Scaling analysis
    deliverables:
      - Expert-parallel MoE
      - All-to-all routing
      - Multi-GPU training
      - Scaling efficiency report
    estimated_hours: "14-18"

  - id: moe-m5
    name: Training & Evaluation
    description: >
      Train a MoE language model and compare with dense baseline
      on perplexity, throughput, and expert specialization.
    acceptance_criteria:
      - MoE model trained on language modeling task (e.g., WikiText, C4)
      - Comparison baseline: dense model with same active parameters
      - MoE achieves similar or better perplexity than dense baseline
      - MoE has higher throughput (tokens/sec) than dense model
      - Expert specialization analysis shows different experts handle different patterns
      - Ablation on number of experts (E) and top-k shows tradeoffs
    pitfalls:
      - Comparing models with different total parameters (compare active params)
      - MoE needs more data to train effectively than dense model
      - Not accounting for communication overhead in throughput
      - Expert specialization may not emerge without enough data
    concepts:
      - MoE training
      - Dense baseline comparison
      - Throughput analysis
      - Expert specialization
    skills:
      - MoE training
      - Baseline comparison
      - Specialization analysis
      - Ablation studies
    deliverables:
      - Trained MoE model
      - Dense baseline comparison
      - Throughput benchmarks
      - Expert specialization analysis
    estimated_hours: "12-16"
