id: ml-model-serving
name: ML Model Serving API
description: Build a production model inference service with efficient batching, versioning, A/B testing, and real-time monitoring with drift detection.
difficulty: intermediate
estimated_hours: 45-60
essence: Dynamic request batching for inference throughput optimization, multi-version model deployment with traffic routing for A/B testing, and real-time drift detection through statistical comparison of live input distributions against training baselines.
why_important: Building this teaches production ML system design—handling inference optimization, serialization bottlenecks, traffic splitting strategies, and drift detection that are critical for any ML engineering role.
learning_outcomes:
- Load and serve ML models with proper device management and warmup
- Optimize request serialization and deserialization for throughput
- Implement dynamic request batching with configurable latency/throughput tradeoffs
- Manage multiple model versions with hot-swap and rollback capabilities
- Route traffic between model versions for A/B testing with statistical rigor
- Monitor inference latency, prediction distributions, and input data drift
skills:
- Model Loading & Inference
- Request Serialization (JSON, Protobuf)
- Dynamic Batching
- Model Versioning & Hot Reload
- Traffic Routing & A/B Testing
- Drift Detection & Monitoring
tags:
- ai-ml
- api
- batching
- devops
- inference
- intermediate
- optimization
- monitoring
architecture_doc: architecture-docs/ml-model-serving/index.md
languages:
  recommended:
  - Python
  - Go
  also_possible:
  - Rust
resources:
- name: TensorFlow Serving Guide
  url: https://www.tensorflow.org/tfx/guide/serving
  type: documentation
- name: PyTorch Serve Performance Checklist
  url: https://docs.pytorch.org/serve/performance_checklist.html
  type: documentation
- name: Ray Batch Inference Tutorial
  url: https://docs.ray.io/en/latest/data/batch_inference.html
  type: tutorial
- name: A/B Testing ML Models Guide
  url: https://mlinproduction.com/ab-test-ml-models-deployment-series-08/
  type: article
- name: Grafana ML Monitoring with Prometheus
  url: https://grafana.com/blog/2021/08/02/how-basisai-uses-grafana-and-prometheus-to-monitor-model-drift-in-machine-learning-workloads/
  type: article
prerequisites:
- type: skill
  name: REST API design
- type: skill
  name: Basic ML (model training and inference)
- type: skill
  name: Docker
milestones:
- id: ml-model-serving-m1
  name: Model Loading, Inference & Serialization
  description: Load a model, serve predictions via HTTP, and optimize request serialization. GPU warmup is part of the model loading lifecycle.
  acceptance_criteria:
  - Load a PyTorch model (primary) with ONNX Runtime as optional extension; model is set to eval mode with torch.no_grad() context
  - GPU inference is used when CUDA is available; fallback to CPU is automatic and logged
  - Model warmup runs N configurable dummy inference requests immediately after model loading completes (not separately) to trigger JIT compilation and CUDA kernel caching
  - Inference endpoint accepts JSON input, validates tensor shape and dtype against model's expected input schema, and returns JSON results
  - Request deserialization overhead is measured; response serialization uses efficient JSON encoding (e.g., orjson) with benchmarked improvement over stdlib json
  - Health endpoint returns 200 only after model is loaded and warmup is complete (readiness probe)
  - Liveness endpoint returns 200 as long as the process is running
  - Input validation rejects requests with wrong shape/dtype and returns 400 with descriptive error message
  - Single-request inference p50 latency is under 50ms for a standard image classification or text embedding model
  pitfalls:
  - Forgetting torch.no_grad() and model.eval() causes unnecessary gradient computation and incorrect BatchNorm/Dropout behavior
  - Loading the entire model into GPU memory without checking available VRAM causes OOM crashes
  - Blocking the main event loop during model loading causes HTTP timeout errors on health checks
  - stdlib json.dumps is 3-10x slower than orjson for numpy array serialization—measure this
  - Not preallocating output tensors causes repeated memory allocation during inference
  - Warmup must happen AFTER model loading; separating them as independent steps creates a race condition
  concepts:
  - Model serialization formats (PyTorch JIT, ONNX)
  - CUDA context initialization and JIT warmup
  - Request/response serialization performance
  - Readiness vs liveness probes
  - Inference mode and gradient disabling
  skills:
  - Model loading and device placement
  - JSON serialization optimization
  - HTTP API design (FastAPI/Flask)
  - Health check implementation
  deliverables:
  - Model loader supporting PyTorch (required) and ONNX Runtime (optional) with device auto-detection
  - Warmup module that runs dummy inferences immediately after model load completes
  - Prediction HTTP endpoint with JSON input validation and structured JSON response
  - Serialization benchmark comparing stdlib json vs orjson for inference payloads
  - Health endpoint (readiness) returning 200 only after model load and warmup complete
  - Liveness endpoint returning 200 when process is alive
  estimated_hours: 9-12
- id: ml-model-serving-m2
  name: Request Batching
  description: Batch incoming requests to maximize GPU utilization and inference throughput while bounding latency.
  acceptance_criteria:
  - Incoming requests are queued and grouped into batches up to a configurable max batch size (default 32)
  - Maximum wait timeout (default 50ms) flushes partial batches to bound worst-case latency
  - Variable-length inputs are padded to the longest input in the batch with proper attention masks
  - After batch inference, results are correctly un-batched and routed to each original requester
  - Backpressure rejects new requests with 503 when queue depth exceeds configurable limit
  - 'Metrics exposed: average batch size, queue depth, wait time, and requests per second'
  - 'Throughput improvement is demonstrated: batched serving achieves at least 2x RPS vs sequential serving on GPU'
  pitfalls:
  - Waiting too long to fill batches adds unacceptable latency for low-traffic periods
  - Forgetting to un-batch results in correct order causes response mismatch (user A gets user B's prediction)
  - Not padding variable-length inputs causes dimension mismatch errors in batch inference
  - Memory leaks from completed batches not being cleared from the queue
  - Max batch size too large causes GPU OOM; profile memory usage per batch size
  - Backpressure not implemented leads to unbounded queue growth and eventual OOM
  concepts:
  - Dynamic batching with adaptive timeout
  - Asynchronous request queuing
  - Input padding and masking for variable-length sequences
  - Backpressure mechanisms
  skills:
  - Async queue management (asyncio.Queue)
  - Batch formation and timeout handling
  - Tensor padding and attention mask creation
  - Throughput benchmarking
  deliverables:
  - Request queue buffering incoming requests for batch formation
  - Batch former grouping requests up to max size with timeout flush
  - Input padder aligning variable-length inputs with attention masks
  - Response router splitting batch outputs back to individual requesters
  - Backpressure handler rejecting requests when queue exceeds limit
  - Throughput benchmark comparing batched vs sequential serving RPS
  estimated_hours: 9-12
- id: ml-model-serving-m3
  name: Model Versioning & Hot Reload
  description: Manage multiple model versions with zero-downtime hot swapping and rollback.
  acceptance_criteria:
  - Model registry stores multiple versions with metadata (accuracy, training date, data hash, input/output schema)
  - Version-specific routing directs requests to a specified version via URL path or header
  - Default routing sends requests to the latest stable version when no version is specified
  - Hot swap loads a new model version into memory and atomically switches the default, with no dropped requests
  - In-flight requests to the old version drain completely before the old model is unloaded
  - Rollback reverts to the previous version within 5 seconds if the new version's error rate exceeds a threshold
  - Input/output schema compatibility is validated before a new version is promoted; incompatible schemas are rejected
  pitfalls:
  - Not draining in-flight requests before unloading a model version causes 500 errors
  - Race conditions between concurrent model loads and inference on the same version
  - Breaking API contracts when model input/output shapes change between versions
  - Loading both old and new models simultaneously doubles peak memory usage—account for this
  - Not preserving the previous model in memory makes rollback slow (requires reload from disk)
  concepts:
  - Model registry and metadata management
  - Graceful shutdown and request draining
  - Atomic pointer swap for zero-downtime updates
  - Schema compatibility validation
  skills:
  - Version management patterns
  - Concurrent model loading
  - Graceful request draining
  - Schema validation
  deliverables:
  - Model registry storing versions with metadata and schema definitions
  - Version-specific endpoint routing by URL path or request header
  - Hot swap mechanism loading new version and switching traffic atomically
  - Request draining logic completing in-flight requests before unloading old version
  - Rollback trigger reverting to previous version on error rate spike
  - Schema compatibility checker validating input/output shapes between versions
  estimated_hours: 9-11
- id: ml-model-serving-m4
  name: A/B Testing & Canary Deployment
  description: Split traffic between model versions for controlled experiments with statistical rigor.
  acceptance_criteria:
  - Traffic is split between model versions according to configured percentage weights (e.g., 90/10)
  - User routing is deterministic—same user ID always routes to the same version (consistent hashing)
  - Traffic weights are adjustable at runtime without server restart
  - 'Per-version metrics tracked: latency (p50, p95), error rate, and prediction distribution'
  - Gradual rollout increases canary traffic in configurable increments (e.g., 5% -> 10% -> 25% -> 50% -> 100%)
  - Automatic rollback triggers if canary error rate exceeds baseline by configurable threshold (default 2x)
  - Experiment metadata (start time, versions, weights) is logged for every prediction
  pitfalls:
  - Non-deterministic routing makes A/B test results invalid (same user sees different versions across requests)
  - Insufficient sample size leads to inconclusive results; compute required sample size before starting
  - Changing multiple variables (model + prompt + preprocessing) simultaneously makes results uninterpretable
  - Not logging experiment assignment with predictions makes post-hoc analysis impossible
  - Canary failure detection too slow allows bad models to serve significant traffic
  concepts:
  - Consistent hashing for deterministic traffic splitting
  - Statistical significance testing for experiment results
  - Canary deployment with automatic rollback
  - Experiment logging and analysis
  skills:
  - Consistent hashing implementation
  - Traffic weight management
  - Basic statistical testing (t-test, chi-square)
  - Experiment logging
  deliverables:
  - Traffic splitter routing requests by consistent hash of user ID to model versions
  - Weight configuration API allowing runtime adjustment of traffic percentages
  - Per-version metrics collector tracking latency, error rate, and prediction stats
  - Gradual rollout controller incrementing canary traffic on schedule
  - Automatic rollback trigger on canary error rate exceeding threshold
  - Experiment log recording version assignment, prediction, and timestamp per request
  estimated_hours: 9-11
- id: ml-model-serving-m5
  name: Monitoring & Drift Detection
  description: Monitor model performance in production and detect input data drift and prediction distribution shifts.
  acceptance_criteria:
  - Inference latency percentiles (p50, p90, p95, p99) tracked per model version and exposed as Prometheus metrics
  - Prediction output distribution is monitored using a sliding window histogram, compared against a stored baseline
  - Data drift detection computes KS test (Kolmogorov-Smirnov) between sampled live input feature distributions and stored training baseline distributions
  - 'Drift detection runs on a configurable sample (default: every 1000th request) to avoid performance overhead'
  - Alert fires when KS test p-value drops below configurable threshold (default 0.01) for any monitored feature
  - Structured request logging samples a configurable percentage of requests with full input/output for debugging
  - Dashboard (Grafana or equivalent) visualizes latency percentiles, throughput, error rates, and drift scores over time
  pitfalls:
  - Logging every request at full fidelity causes storage explosion and inference slowdown; sample judiciously
  - Alert fatigue from poorly tuned drift thresholds—start conservative and tighten based on data
  - KS test requires sufficient samples (~100+) per window to be reliable; small windows produce noisy p-values
  - Not monitoring actual prediction quality (only latency) misses silent model degradation
  - Training baseline must be stored at deployment time; forgetting this makes drift detection impossible
  - Not correlating infrastructure metrics (CPU, memory, GPU utilization) with latency spikes
  concepts:
  - Prometheus metrics exposition
  - KS test for distribution comparison
  - Prediction distribution monitoring
  - Structured logging with sampling
  - Alerting thresholds and fatigue management
  skills:
  - Prometheus metric instrumentation
  - Statistical distribution testing
  - Grafana dashboard configuration
  - Log sampling strategies
  deliverables:
  - Latency metrics tracker exposing p50/p90/p95/p99 per model version as Prometheus metrics
  - Throughput counter tracking RPS and error rates
  - Training baseline store capturing feature distributions at deployment time
  - KS test drift detector comparing sampled live features against training baseline
  - Prediction distribution monitor tracking output histogram shifts over sliding windows
  - Alert rule engine firing notifications when drift or latency thresholds are exceeded
  - Sampled request logger storing input/output pairs for a configurable percentage of requests
  estimated_hours: 10-14
domain: ai-ml
