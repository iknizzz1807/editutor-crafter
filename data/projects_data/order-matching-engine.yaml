id: order-matching-engine
name: Order Matching Engine (HFT-Grade)
description: >
  Build a high-frequency trading grade order matching engine with sub-microsecond latency.
  Implement lock-free data structures, cache-optimized memory layout, SIMD-accelerated operations,
  and NUMA-aware design patterns used in real quantitative trading systems at firms like Jane Street,
  Citadel, and Two Sigma.

difficulty: expert
estimated_hours: 120-160
domain: specialized

essence: >
  Lock-free order book using atomic operations and memory ordering for concurrent access without locks,
  cache-line aligned data structures to prevent false sharing, pre-allocated memory pools to eliminate
  allocation latency, SIMD batch processing for bulk order operations, and single-producer single-consumer
  ring buffers for message passing. Target: <1 microsecond end-to-end latency for order placement and matching.

why_important: >
  This is the core infrastructure that powers global financial markets. The skills developed - lock-free
  programming, cache optimization, latency engineering - are directly applicable to:
  - Quantitative trading (compensation: $300K-600K+ at top firms)
  - High-performance game servers (tick rate optimization)
  - Real-time databases (storage engine performance)
  - Ad tech (real-time bidding systems)
  These techniques separate senior systems programmers from junior developers.

learning_outcomes:
  - Design lock-free order book data structures using atomic operations, compare-and-swap, and memory ordering
  - Implement cache-line aligned structures to prevent false sharing and optimize CPU cache utilization
  - Build memory pools with pre-allocation to eliminate malloc/free overhead on the hot path
  - Apply SIMD intrinsics for batch order validation and bulk price level operations
  - Engineer for sub-microsecond latency with proper measurement methodology (p50, p99, p999)
  - Implement NUMA-aware memory allocation and thread pinning for multi-socket systems
  - Design deterministic matching that produces identical results on replay (crucial for compliance)
  - Build proper backpressure and load shedding to maintain latency SLAs under overload

skills:
  - Lock-free programming (atomics, CAS, memory ordering)
  - Cache optimization (cache-line alignment, prefetching)
  - Memory pool design (pre-allocation, slab allocators)
  - SIMD intrinsics (AVX2/AVX-512 for batch operations)
  - Latency engineering (measurement, profiling, optimization)
  - NUMA programming (memory affinity, CPU pinning)
  - Low-latency networking (kernel bypass, DPDK basics)
  - Performance profiling (perf, VTune, flame graphs)

tags:
  - expert
  - lock-free
  - low-latency
  - hft
  - quant
  - cache-optimized
  - simd
  - numa
  - order-book
  - trading

languages:
  recommended:
    - Rust
    - C++
    - C
  also_possible:
    - Java (with GraalVM native)
    - Go (with unsafe)

resources:
  - name: "Lock-Free Data Structures"
    url: https://preshing.com/20120612/an-introduction-to-lock-free-programming/
    type: tutorial
  - name: "LMAX Disruptor - High Performance Inter-Thread Messaging"
    url: https://lmax-exchange.github.io/disruptor/
    type: documentation
  - name: "Cache-Line Alignment in C++"
    url: https://www.aristeia.com/TalkNotes/codedive-CPUCachesHandouts.pdf
    type: documentation
  - name: "Intel Intrinsics Guide"
    url: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html
    type: reference
  - name: "Liquibook C++ Order Matching Engine"
    url: https://github.com/enewhuis/liquibook
    type: code
  - name: "How to Build a Fast Limit Order Book"
    url: https://gist.github.com/halfelf/db1ae032dc34278968f8bf31ee999a25
    type: tutorial
  - name: "NUMA Best Practices"
    url: https://queue.acm.org/detail.cfm?id=2543610
    type: article

prerequisites:
  - type: skill
    name: Advanced C/C++ or Rust (unsafe, raw pointers, memory layout)
  - type: skill
    name: x86 memory model and cache hierarchy understanding
  - type: skill
    name: Concurrency primitives (mutexes, condition variables, atomics)
  - type: skill
    name: Performance profiling tools (perf, valgrind, or similar)
  - type: project
    name: Any high-performance systems project (game server, database, etc.)

milestones:
  - id: order-matching-engine-m1
    name: Lock-Free Order Book Core
    description: >
      Build the foundational order book data structure using lock-free techniques.
      Implement atomic operations for concurrent access, cache-line alignment to prevent
      false sharing, and O(1) operations for all critical paths.
    acceptance_criteria:
      - Order book uses atomic operations (compare-and-swap, fetch-add) for all mutable state - no mutexes on hot path
      - All hot-path structures are aligned to 64-byte cache lines using alignas(64) or equivalent
      - Order lookup by ID is O(1) via concurrent hash map with lock-free or fine-grained locking
      - Price level operations (insert, remove, update) complete in O(log P) without blocking readers
      - No false sharing between bid and ask sides - each on separate cache lines
      - Memory layout is documented with sizeof() and alignment verification tests
      - Order struct fits within 1-2 cache lines (target: <128 bytes)
      - Thread sanitizer and address sanitizer tests pass with no data races detected
    pitfalls:
      - Using std::mutex on the hot path adds 20-100+ nanoseconds per contention - use atomics
      - False sharing between frequently modified fields causes 10x latency variance - align to cache lines
      - Oversized order structs (padding issues) cause cache pollution - use packed representations
      - Relaxed memory ordering where sequential consistency is needed causes subtle bugs - document ordering requirements
      - ABA problem in lock-free linked lists - use versioned pointers or hazard pointers
    concepts:
      - Lock-free vs wait-free semantics
      - x86 memory ordering (sequential consistency, acquire-release, relaxed)
      - Cache-line alignment and false sharing
      - Compare-and-swap loops and spin waiting
      - Memory layout optimization
    skills:
      - Atomic operations
      - Cache-line alignment
      - Lock-free data structures
      - Memory ordering
    deliverables:
      - Lock-free order book with atomic operations for all mutable state
      - Cache-line aligned structures with static_assert verification
      - Concurrent order ID lookup with O(1) average case
      - No-data-race verification via ThreadSanitizer
      - Memory layout documentation with sizeof() and alignment analysis
    estimated_hours: "24-32"

  - id: order-matching-engine-m2
    name: Memory Pool & Allocation System
    description: >
      Implement a custom memory allocator optimized for the order book's allocation patterns.
      Pre-allocate all order and trade objects at startup to eliminate malloc/free on the hot path.
      Design for cache locality and minimal fragmentation.
    acceptance_criteria:
      - Memory pool pre-allocates orders at startup based on configured max orders (e.g., 1M orders)
      - Order allocation from pool is O(1) and never calls malloc during matching
      - Pool uses slab allocation with fixed-size blocks matching order struct size
      - Freed orders return to pool and are reused without fragmentation
      - Pool supports thread-local caches to avoid atomic contention on allocation
      - Memory is NUMA-aware - allocated on the NUMA node where matching thread runs
      - Pool statistics track utilization, fragmentation, and allocation latency
      - Allocation latency is <10 nanoseconds (measured with high-resolution timer)
    pitfalls:
      - Calling malloc/new during matching causes 100+ nanosecond latency spikes - pre-allocate everything
      - Thread contention on shared pool causes atomic operations - use thread-local caches
      - Pool fragmentation from variable-size allocations - use fixed-size slabs
      - Not accounting for NUMA causes remote memory access (2-3x slower) - use numa_alloc_onnode
      - Memory leaks in pool return path - use RAII or comprehensive testing
    concepts:
      - Slab allocation and object pools
      - Thread-local storage for allocation caches
      - NUMA memory allocation
      - Memory fragmentation avoidance
      - Allocation latency measurement
    skills:
      - Custom allocator design
      - Slab allocation
      - NUMA programming
      - Memory profiling
    deliverables:
      - Pre-allocated memory pool for orders with O(1) allocation
      - Thread-local allocation caches for scalability
      - NUMA-aware memory allocation
      - Pool utilization statistics and monitoring
      - Allocation latency benchmarks showing <10ns average
    estimated_hours: "16-24"

  - id: order-matching-engine-m3
    name: Price-Time Priority Matching Engine
    description: >
      Implement the core matching algorithm with strict price-time priority.
      Optimize for single-pass matching with minimal branching and maximum cache utilization.
    acceptance_criteria:
      - Matching follows strict price-time priority: best price first, then FIFO within price level
      - Aggressive order walks resting book in single pass without backtracking
      - Each match generates trade execution with trade ID, price, quantity, aggressive/passive IDs, timestamp
      - Partial fills correctly update remaining quantities without allocation
      - Self-trade prevention detects same-participant matches and applies configurable policy
      - Matching is deterministic - same input sequence produces identical trades (verified by replay test)
      - Matching latency (place + match) is <500 nanoseconds for typical orders, <1 microsecond p99
      - Trade struct is allocated from pre-allocated pool, not malloc
    pitfalls:
      - Non-deterministic matching (using wall clock for timestamps) breaks replay - use logical sequence numbers
      - Branch misprediction in matching loop causes pipeline stalls - use branchless techniques where possible
      - Not validating tick size and lot size causes invalid prices - validate early, reject fast
      - Self-trade detection after matching rather than before causes unnecessary work - check during walk
      - Trade allocation on hot path - use pre-allocated trade pool
    concepts:
      - Price-time priority matching algorithm
      - Deterministic execution for replay
      - Branch prediction optimization
      - Self-trade prevention policies
      - Execution reporting semantics
    skills:
      - Matching algorithm implementation
      - Deterministic design
      - Branch optimization
      - Trade execution
    deliverables:
      - Price-time priority matching with deterministic output
      - Trade execution generation from pre-allocated pool
      - Self-trade prevention with configurable policy
      - Matching latency benchmarks (<500ns typical, <1µs p99)
      - Determinism verification via replay tests
    estimated_hours: "20-28"

  - id: order-matching-engine-m4
    name: SIMD Batch Processing
    description: >
      Implement SIMD-accelerated operations for bulk order processing.
      Use AVX2 or AVX-512 for batch validation, price level operations, and market data generation.
    acceptance_criteria:
      - Order validation (tick size, lot size, price bounds) uses SIMD to validate 8+ orders simultaneously
      - Price level aggregation for market data snapshots uses SIMD for sum/min/max operations
      - SIMD code path is guarded by CPU feature detection (cpuid) with scalar fallback
      - SIMD memory loads are aligned to 32 bytes (AVX2) or 64 bytes (AVX-512)
      - Benchmark shows SIMD validation is 4-8x faster than scalar for batch operations
      - No SIMD code on hot path that processes single orders - SIMD only for batch operations
      - Code compiles without SIMD on non-x86 platforms (ARM NEON optional)
    pitfalls:
      - Using SIMD for single-order operations adds overhead - SIMD is for batches only
      - Misaligned SIMD loads cause performance penalties or crashes - align allocations
      - Not checking CPU features causes SIGILL on older CPUs - use cpuid
      - SIMD code is hard to read - comment extensively and provide scalar reference
      - Ignoring ARM servers - provide scalar fallback or NEON alternative
    concepts:
      - SIMD intrinsics (AVX2, AVX-512)
      - Vectorized validation algorithms
      - CPU feature detection
      - Memory alignment for SIMD
      - Batch processing design
    skills:
      - SIMD intrinsics
      - Vectorized algorithms
      - CPU feature detection
      - Aligned memory allocation
    deliverables:
      - SIMD-accelerated batch order validation
      - SIMD price level aggregation for market data
      - CPU feature detection with fallback
      - Benchmarks showing 4-8x speedup over scalar
      - Portable code with ARM fallback
    estimated_hours: "16-24"

  - id: order-matching-engine-m5
    name: NUMA-Aware Design & Thread Architecture
    description: >
      Design the thread architecture for multi-socket systems with NUMA awareness.
      Pin threads to CPUs, allocate memory on local NUMA nodes, and minimize cross-NUMA access.
    acceptance_criteria:
      - Matching thread is pinned to a specific CPU core using pthread_setaffinity_np or equivalent
      - Memory for order book is allocated on the NUMA node where matching thread runs
      - I/O threads (network, WAL) run on separate cores, potentially on different NUMA nodes
      - Inter-thread communication uses lock-free ring buffers (SPSC pattern)
      - NUMA topology is detected at startup and logged for debugging
      - Cross-NUMA memory access is minimized (<5% of total access)
      - System supports configuration of CPU affinity and NUMA policy via config file
    pitfalls:
      - Cross-NUMA memory access is 2-3x slower - allocate on local node
      - Thread migration between cores destroys cache locality - pin threads
      - Shared data structures between NUMA nodes cause remote access - partition by NUMA
      - Not documenting NUMA requirements leads to misconfiguration in production
      - Hyperthreading siblings compete for cache - avoid placing critical threads on siblings
    concepts:
      - NUMA topology and memory affinity
      - CPU pinning and thread affinity
      - SPSC ring buffers for inter-thread communication
      - Memory allocation policies (MPOL_BIND, MPOL_PREFERRED)
      - NUMA balancing avoidance
    skills:
      - NUMA programming
      - Thread affinity
      - Lock-free ring buffers
      - NUMA profiling
    deliverables:
      - Thread pinning configuration and implementation
      - NUMA-aware memory allocation
      - SPSC ring buffers for inter-thread messaging
      - NUMA topology detection and logging
      - Documentation for production NUMA configuration
    estimated_hours: "12-16"

  - id: order-matching-engine-m6
    name: Latency Measurement & Optimization
    description: >
      Implement comprehensive latency measurement infrastructure and optimize to meet
      sub-microsecond targets. Use proper methodology: warm-up, isolated measurement,
      and tail latency focus.
    acceptance_criteria:
      - Latency measurement uses RDTSC or clock_gettime(CLOCK_MONOTONIC) with sub-nanosecond resolution
      - Measurement includes full round-trip: receive order → match → generate trades → send ack
      - Warm-up period runs before measurement to ensure CPU cache is hot
      - Latency histogram tracks p50, p90, p99, p99.9, p99.99, and max
      - Achieved latency: p50 <200ns, p99 <500ns, p99.9 <1µs, max <10µs
      - Latency outliers are investigated with tracing (record timestamps at each step)
      - Continuous benchmark runs in CI to catch latency regressions
      - Profiling with perf shows no unexpected hotspots (>5% of total time)
    pitfalls:
      - Using clock_gettime() without warm-up measures cold cache latency - always warm up
      - Measuring average latency hides tail latency problems - focus on p99 and beyond
      - Coordinated omission in load testing understates tail latency - use fixed-rate load generation
      - Not pinning the measurement thread causes frequency scaling artifacts - pin to performance core
      - Interrupts during measurement cause outliers - consider CPU isolation (isolcpus)
    concepts:
      - High-resolution timing (RDTSC, TSC calibration)
      - Latency histogram methodology
      - Warm-up and steady-state measurement
      - Coordinated omission problem
      - Tail latency optimization
    skills:
      - Latency measurement
      - Performance profiling (perf, VTune)
      - Histogram analysis
      - Optimization methodology
    deliverables:
      - High-resolution latency measurement harness
      - Latency histogram with p50 through p99.99
      - Achieved targets: p50 <200ns, p99 <500ns, p99.9 <1µs
      - CI benchmark for latency regression detection
      - Profiling analysis with identified hotspots
    estimated_hours: "16-24"

  - id: order-matching-engine-m7
    name: Crash Recovery & Durability
    description: >
      Implement write-ahead logging with sub-microsecond overhead and fast recovery.
      Design for minimal impact on matching latency while ensuring no data loss.
    acceptance_criteria:
      - WAL writes are batched and async - matching thread never blocks on disk I/O
      - WAL entries include monotonic sequence number, operation type, and full operation data
      - Recovery from WAL replay completes in <1 second for typical log sizes (<1M entries)
      - Periodic snapshots serialize full order book state with incremental snapshot support
      - fsync policy is configurable: per-entry (safest), time-based, or size-based
      - Recovery is tested with simulated crashes at random points (chaos testing)
      - WAL overhead on matching latency is <50 nanoseconds average (async write)
      - Deterministic replay produces identical order book state
    pitfalls:
      - Synchronous WAL writes on hot path kill latency - always async with batching
      - Not fsyncing means data loss on power failure - choose durability level explicitly
      - Non-deterministic replay breaks recovery - use logical timestamps, not wall clock
      - Snapshot during matching causes inconsistency - use copy-on-write or pause matching
      - WAL files grow unboundedly - implement snapshot-based truncation
    concepts:
      - Write-ahead logging for durability
      - Async I/O for latency isolation
      - Snapshot + incremental recovery
      - fsync durability levels
      - Deterministic replay
    skills:
      - WAL implementation
      - Async I/O
      - Crash recovery
      - Durability engineering
    deliverables:
      - Async WAL with batching and configurable fsync
      - Fast recovery (<1 second for typical logs)
      - Snapshot serialization with truncation
      - Chaos testing for crash recovery
      - WAL latency overhead <50ns on matching thread
    estimated_hours: "12-16"
