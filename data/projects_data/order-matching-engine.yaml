id: order-matching-engine
name: Order Matching Engine
description: >-
  Low-latency trading engine with order book management, price-time priority
  matching, market data dissemination, and crash recovery via write-ahead log.
difficulty: advanced
estimated_hours: 80
essence: >-
  Price-level order book management using a sorted price level structure
  (red-black tree or skip list) for O(log P) price level lookup where P is
  the number of distinct price levels, FIFO queues at each level for O(1)
  order insertion, and a hash map for O(1) order cancellation by ID.
  Implements price-time priority matching with trade execution in
  microseconds, single-threaded event loop design for deterministic
  low-latency processing, and write-ahead logging for crash recovery.
why_important: >-
  Building this develops critical skills in low-latency system design,
  efficient data structure selection, and understanding how modern financial
  exchanges operate - knowledge directly applicable to high-frequency trading,
  real-time systems, and performance-critical backend engineering.
learning_outcomes:
  - Design order book data structures with O(log P) price level operations and O(1) order cancellation
  - Implement price-time priority matching algorithm with partial fill handling
  - Handle concurrent order operations safely via single-threaded event loop or fine-grained synchronization
  - Build market data dissemination with incremental order book updates
  - Implement write-ahead logging for crash recovery and order book state reconstruction
  - Design self-trade prevention policies (cancel newest, cancel oldest, cancel both)
skills:
  - Order book data structures
  - Matching algorithms
  - Low-latency design
  - Single-threaded event loops
  - Market data feeds
  - Write-ahead logging
  - Crash recovery
tags:
  - advanced
  - fifo
  - low-latency
  - matching
  - order-book
  - fintech
  - recovery
architecture_doc: architecture-docs/order-matching-engine/index.md
languages:
  recommended:
    - Rust
    - C++
    - Go
  also_possible:
    - Java
    - Python
resources:
  - name: Liquibook C++ Order Matching Engine
    url: https://github.com/enewhuis/liquibook
    type: tool
  - name: Matching Engines Guide by Jelle Pelgrims
    url: https://jellepelgrims.com/posts/matching_engines
    type: tutorial
  - name: How to Build a Fast Limit Order Book
    url: https://gist.github.com/halfelf/db1ae032dc34278968f8bf31ee999a25
    type: tutorial
  - name: LMAX Disruptor
    url: https://lmax-exchange.github.io/disruptor/
    type: documentation
  - name: Price-Time Priority - MarketsWiki
    url: https://www.marketswiki.com/wiki/Price-time_priority
    type: documentation
prerequisites:
  - type: skill
    name: Data structures (trees, hash maps, queues)
  - type: skill
    name: Systems programming (memory layout, performance profiling)
  - type: skill
    name: Networking basics (TCP/WebSocket)
milestones:
  - id: order-matching-engine-m1
    name: Order Book Data Structure
    description: >-
      Build an efficient order book with sorted price levels, FIFO queues
      per level, and O(1) order lookup by ID.
    acceptance_criteria:
      - "Order book maintains two sides (bids sorted descending, asks sorted ascending) using a sorted data structure (red-black tree, skip list, or sorted map)"
      - "Each price level contains a FIFO queue of orders preserving insertion order (time priority)"
      - "Best bid and best ask are retrievable in O(1) via maintained pointers/references to the top of each side"
      - "Order insertion at an existing price level is O(1) (append to FIFO queue); new price level creation is O(log P) where P is the number of active price levels"
      - "Order cancellation by ID is O(1) via a hash map from order ID to order node, with O(1) removal from the doubly-linked FIFO queue"
      - "Empty price levels are removed immediately to prevent memory leaks and stale level iteration"
      - "Order struct contains order ID, side (bid/ask), price, quantity, remaining quantity, timestamp, and participant ID"
      - "Aggregate book depth (total quantity per price level) is maintained incrementally, not recomputed on each query"
    pitfalls:
      - "Not removing empty price levels causes memory leaks and incorrect best bid/ask after cancellations"
      - "Using a singly-linked list for the FIFO queue makes O(1) cancellation impossible - use a doubly-linked list"
      - "Storing full order copies at multiple locations causes consistency bugs - store orders once and reference via pointers/IDs"
      - "Breaking FIFO order when inserting into price level queues violates price-time priority"
      - "Forgetting to update aggregate depth when orders are added, cancelled, or partially filled"
    concepts:
      - Sorted map (red-black tree, skip list) for O(log P) price level operations
      - Doubly-linked FIFO queue at each price level for O(1) insertion and removal
      - Hash map from order ID to order node for O(1) cancellation
      - Incremental aggregate depth tracking
      - Memory-efficient order storage with minimal allocation
    skills:
      - Tree/skip list structures
      - Doubly-linked lists
      - Hash map design
      - Memory management
    deliverables:
      - Order struct with ID, side, price, quantity, remaining quantity, timestamp, and participant ID
      - Price level structure with doubly-linked FIFO queue and aggregate depth
      - Sorted bid and ask side structures with O(1) best price access
      - Hash map for O(1) order ID to order node lookup
      - Empty price level cleanup on last order removal
    estimated_hours: 14

  - id: order-matching-engine-m2
    name: Order Operations
    description: >-
      Implement order placement, cancellation, and modification with
      validation and execution reporting.
    acceptance_criteria:
      - "Limit order placement adds the order to the correct side and price level, or triggers immediate matching if it crosses the spread"
      - "Market order matches immediately against the best available price levels until fully filled or the book is exhausted"
      - "Cancel request removes the order from the book by ID and returns a cancellation acknowledgment; cancelling a non-existent or already-filled order returns an error, not a crash"
      - "Order modification (cancel-replace) cancels the existing order and places a new one; price change resets time priority, quantity reduction preserves it"
      - "All orders are validated against configurable tick size (price increment) and lot size (quantity increment) constraints"
      - "Duplicate order IDs are rejected with an error response"
      - "Execution reports are generated for every state change: acknowledgment, fill, partial fill, cancel, and reject"
    pitfalls:
      - "Not validating order price against tick size constraints leads to orders at impossible price levels"
      - "Allowing modification of an already partially-filled order without correctly adjusting remaining quantity causes accounting errors"
      - "Missing duplicate order ID checks cause state corruption - the hash map silently overwrites the previous order"
      - "Cancelling a fully filled order should return a clean error, not corrupt state"
      - "Modify with price change must lose time priority (cancel + new add); modify with only quantity reduction should keep time priority"
    concepts:
      - Limit order book crossing detection (new limit order that can match immediately)
      - Cancel-replace semantics and time priority implications
      - Tick size and lot size validation rules
      - Idempotent order operations for duplicate handling
      - Execution report generation for all state transitions
    skills:
      - Order lifecycle
      - Input validation
      - State management
      - Execution reporting
    deliverables:
      - Limit order placement with immediate match detection if crossing the spread
      - Market order execution against available book depth
      - Order cancellation by ID with acknowledgment and error handling
      - Order modification with correct time priority semantics
      - Tick size and lot size validation
      - Execution report generation for all order state changes
    estimated_hours: 14

  - id: order-matching-engine-m3
    name: Matching Engine
    description: >-
      Implement the price-time priority matching algorithm with partial fills,
      self-trade prevention, and trade execution records.
    acceptance_criteria:
      - "Incoming aggressive order walks the opposite side of the book matching against resting orders in strict price-time priority"
      - "Each individual fill generates a trade execution record with trade ID, price, quantity, aggressive order ID, passive order ID, and timestamp"
      - "Partial fills reduce the remaining quantity of both orders; the passive order stays on the book with its reduced quantity"
      - "Market orders that cannot be fully filled are cancelled (remaining unfilled portion) rather than left on the book"
      - "Self-trade prevention detects orders from the same participant on both sides and applies configurable policy (cancel newest, cancel oldest, cancel both)"
      - "Limit orders that cross the spread (bid >= best ask or ask <= best bid) trigger immediate matching before any residual is placed on the book"
      - "No order can match against itself"
      - "Matching produces deterministic results - same input sequence always produces same trades"
    pitfalls:
      - "Not checking for spread crossing before placing limit orders causes orders to rest on the book at marketable prices without executing"
      - "Incorrectly calculating remaining quantity after partial fills leads to phantom liquidity or negative quantities"
      - "Allowing same-participant self-trades is illegal on most exchanges and must be explicitly prevented"
      - "Processing orders out of time priority at the same price level violates exchange fairness rules"
      - "Market orders left on the book (instead of cancelled if unfilled) can match at arbitrary future prices"
    concepts:
      - Price-time priority matching with aggressive order walking the book
      - Partial fill mechanics and remaining quantity tracking
      - Self-trade prevention policies and implementation
      - Market vs limit order matching behavior differences
      - Deterministic matching for reproducibility and auditability
    skills:
      - Matching algorithms
      - Trade execution
      - Self-trade prevention
      - Deterministic processing
    deliverables:
      - Price-time priority matching algorithm implementation
      - Trade execution record generation with all required fields
      - Partial fill handling with correct remaining quantity updates
      - Self-trade prevention with configurable policy (cancel newest/oldest/both)
      - Market order cancellation of unfilled residual
      - Deterministic matching verification test suite
    estimated_hours: 14

  - id: order-matching-engine-m4
    name: Write-Ahead Log & Recovery
    description: >-
      Implement crash recovery via write-ahead logging and periodic
      snapshots to reconstruct order book state after restart.
    acceptance_criteria:
      - "Every inbound order operation (place, cancel, modify) is appended to a write-ahead log (WAL) on disk before being processed"
      - "WAL entries include a monotonic sequence number, operation type, and complete operation data"
      - "On startup, the engine replays the WAL from the last snapshot to reconstruct the complete order book state"
      - "Periodic snapshots serialize the full order book state to disk, allowing WAL truncation of entries before the snapshot"
      - "Recovery produces identical order book state as if the engine had never crashed (deterministic replay)"
      - "WAL fsync policy is configurable - fsync every entry (safest, slower) or fsync every N entries (faster, risk of losing up to N entries)"
      - "Recovery time is measured and reported; snapshot frequency is tunable to keep recovery time under a target (e.g., <5 seconds)"
    pitfalls:
      - "Not calling fsync after WAL writes means data can be lost in OS buffer cache during a crash - decide the durability-performance tradeoff explicitly"
      - "Non-deterministic matching (e.g., using wall clock time for timestamps) makes WAL replay produce different results - use logical timestamps"
      - "Snapshot must be taken atomically - a partial snapshot is worse than no snapshot since it corrupts recovery"
      - "WAL files grow unboundedly without snapshot-based truncation - implement compaction"
      - "Replay speed determines recovery time - optimize WAL entry parsing and avoid unnecessary allocations during replay"
    concepts:
      - Write-ahead logging for crash-safe state persistence
      - Periodic snapshots for WAL truncation and fast recovery
      - Deterministic replay ensuring identical state reconstruction
      - fsync durability guarantees and performance tradeoffs
      - Logical vs wall-clock timestamps for reproducibility
    skills:
      - Write-ahead logging
      - Snapshot serialization
      - Crash recovery
      - Deterministic replay
    deliverables:
      - Write-ahead log appending every operation with sequence number before processing
      - WAL replay engine reconstructing order book state from log entries
      - Periodic snapshot serializer capturing complete order book state to disk
      - WAL truncation after successful snapshot creation
      - Recovery time measurement and reporting
      - Configurable fsync policy (per-entry vs batched)
    estimated_hours: 12

  - id: order-matching-engine-m5
    name: Performance Optimization
    description: >-
      Optimize for low-latency with single-threaded event loop design,
      memory pre-allocation, and latency benchmarking.
    acceptance_criteria:
      - "Single-threaded event loop processes all order operations sequentially, eliminating lock contention"
      - "Memory pre-allocation pool for order and trade objects eliminates allocation overhead on the hot path"
      - "Average order processing latency (place + match) is under 10 microseconds on modern hardware"
      - "Throughput exceeds 100,000 orders per second sustained on a single core"
      - "Latency measurement harness tracks p50, p95, p99, and p999 latencies under load"
      - "Hot data structures (best bid/ask, top price levels) are cache-line aligned to minimize CPU cache misses"
      - "I/O operations (WAL writes, network) are batched or offloaded to separate threads to keep the matching thread unblocked"
    pitfalls:
      - "Using locks on the hot path adds microseconds of latency per contention point - single-threaded design eliminates this entirely"
      - "Memory allocation (malloc/new) during matching causes latency spikes from heap fragmentation - pre-allocate all objects"
      - "Not measuring tail latencies (p99, p999) hides worst-case performance that matters in production"
      - "Cache-line false sharing between threads causes invisible performance degradation - align data to 64-byte boundaries"
      - "WAL fsync on the hot path dominates latency - batch writes or use a dedicated I/O thread"
    concepts:
      - Single-threaded event loop (LMAX Disruptor pattern) for lock-free sequential processing
      - Memory pool pre-allocation to eliminate hot-path allocation
      - Cache-line alignment for CPU cache efficiency
      - I/O offloading to separate threads for non-blocking matching
      - Tail latency measurement and optimization
    skills:
      - Latency optimization
      - Memory pre-allocation
      - Cache-line alignment
      - Performance benchmarking
    deliverables:
      - Single-threaded event loop processing all matching operations
      - Memory pre-allocation pool for order and trade objects
      - Latency measurement harness tracking p50, p95, p99, p999
      - Throughput benchmarking suite measuring orders per second
      - Cache-line aligned data structures for hot-path objects
      - I/O batching or offloading for WAL writes and network
    estimated_hours: 12

  - id: order-matching-engine-m6
    name: Market Data & API
    description: >-
      Build market data feeds and a trading API for order submission,
      cancellation, and real-time book updates.
    acceptance_criteria:
      - "Level 2 market data snapshot provides all price levels with aggregate quantity for both sides"
      - "Incremental updates stream order book changes (add, remove, modify at price level) with sequence numbers"
      - "Sequence number gaps are detectable by subscribers, triggering a full snapshot re-sync"
      - "Trade feed broadcasts execution details (trade ID, price, quantity, timestamp) in real-time"
      - "REST API supports order submission, cancellation, modification, and status query"
      - "WebSocket endpoint streams market data and trade events to subscribers"
      - "Market data publication does not block the matching thread - events are published asynchronously"
    pitfalls:
      - "Sending full order book snapshots on every change instead of incremental deltas wastes enormous bandwidth"
      - "Missing sequence number validation causes clients to have inconsistent book state without knowing it"
      - "Blocking the matching thread with market data publication adds latency to order processing - always async"
      - "Not implementing WebSocket heartbeat and reconnection causes silent data loss on network interruptions"
      - "Rate limiting market data to prevent overwhelming slow subscribers is essential but often forgotten"
    concepts:
      - Level 1 (best bid/ask) vs Level 2 (full book depth) market data
      - Incremental updates with sequence numbers for consistency
      - Snapshot + delta synchronization protocol
      - Market data conflation under high message rates
      - Asynchronous publication to avoid blocking the matching thread
    skills:
      - Market data protocols
      - WebSocket streaming
      - REST API design
      - Async event publication
    deliverables:
      - Level 2 market data snapshot API with full book depth
      - Incremental market data update stream with sequence numbers
      - Real-time trade execution feed via WebSocket
      - REST API for order submission, cancellation, modification, and status
      - WebSocket endpoint for live market data subscription
      - Asynchronous publication ensuring matching thread is never blocked
    estimated_hours: 14