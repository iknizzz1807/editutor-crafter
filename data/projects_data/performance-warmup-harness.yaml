id: performance-warmup-harness
name: Performance Warmup Harness
description: >
  Build a scientific benchmarking tool designed for measuring tail latency
  (p99, p99.9, p99.99), implementing proper warmup strategies, and
  detecting performance anomalies in microbenchmarks.

difficulty: advanced
estimated_hours: 40-55
domain: performance-engineering

essence: >
  Statistical methodology for measuring tail latency with proper warmup
  phases, outlier classification (not just removal), JIT/compiler warmup
  handling, and actionable performance reporting for production systems.

why_important: >
  Tail latency is critical for SLAs and user experience. Companies like
  Google, Amazon, and HFT firms invest heavily in tail latency optimization.
  These skills are valued at $180K-350K+ for performance engineers.

learning_outcomes:
  - Implement proper warmup methodology
  - Measure and report tail latency percentiles
  - Classify outliers vs legitimate tail events
  - Detect performance anomalies and regressions
  - Handle JIT and compiler warmup effects
  - Build statistical confidence intervals
  - Create actionable performance reports
  - Implement continuous performance monitoring

skills:
  - Tail Latency Measurement
  - Warmup Methodology
  - Outlier Analysis
  - Statistical Reporting
  - Anomaly Detection
  - Performance Regression
  - JIT Warmup Handling
  - Monitoring Integration

tags:
  - advanced
  - benchmarking
  - latency
  - monitoring
  - performance
  - p99
  - slas
  - statistics

languages:
  recommended:
    - Rust
    - Go
    - Python
  also_possible:
    - Java
    - C++

resources:
  - name: "Latency Tip: Don't Average Percentiles"
    url: https://www.brendangregg.com/blog/2014-06-12/latency-tip-dont-average-percentiles.html
    type: article
  - name: "How NOT to Measure Latency (Gil Tene)"
    url: https://www.youtube.com/watch?v=lJ8ydIuPFeU
    type: video
  - name: "HdrHistogram"
    url: http://hdrhistogram.org/
    type: reference
  - name: "Coordinated Omission Problem"
    url: https://openjdk.org/projects/code-tools/jmh/
    type: documentation

prerequisites:
  - type: project
    id: benchmark-framework
    name: Benchmark Framework
  - type: skill
    name: Statistics fundamentals (percentiles, distributions)
  - type: skill
    name: Understanding of JIT compilation

milestones:
  - id: pwh-m1
    name: Tail Latency Measurement
    description: >
      Implement high-precision latency measurement with histogram-based
      percentile calculation optimized for tail latency.
    acceptance_criteria:
      - High-resolution timer (nanosecond precision)
      - Histogram with configurable precision (significant digits)
      - Correct percentile calculation (p50, p90, p99, p99.9, p99.99)
      - Memory-efficient histogram (not storing all samples)
      - Handles latency range from nanoseconds to minutes
      - Thread-safe recording for concurrent benchmarks
    pitfalls:
      - Timer overhead affects measurement
      - Naive percentile calculation loses precision
      - Memory explosion with high-precision histograms
      - Concurrent recording contention
      - Very long tails need log-scale buckets
    concepts:
      - Histogram design
      - Percentile calculation
      - Tail latency
      - High-resolution timing
    skills:
      - Histogram implementation
      - Percentile algorithms
      - Timer APIs
      - Memory efficiency
    deliverables:
      - HdrHistogram implementation
      - Percentile calculator
      - High-resolution timer
      - Thread-safe recorder
    estimated_hours: "8-10"

  - id: pwh-m2
    name: Warmup & Steady State Detection
    description: >
      Implement sophisticated warmup methodology that detects when a
      benchmark has reached steady-state performance.
    acceptance_criteria:
      - Configurable warmup duration and iterations
      - Automatic steady-state detection via variance analysis
      - Separate warmup and measurement phases
      - Detects and warns about benchmarks that never stabilize
      - Handles JIT warmup, cache warmup, and allocation warmup
      - Reports warmup statistics separately
    pitfalls:
      - JIT warmup can take thousands of iterations
      - Some benchmarks never stabilize
      - False positive steady-state detection
      - Warmup state leaks into measurement
      - Different warmup for different code paths
    concepts:
      - Steady-state detection
      - JIT warmup
      - Cache warming
      - Variance analysis
    skills:
      - Warmup implementation
      - Variance analysis
      - State separation
      - Detection algorithms
    deliverables:
      - Warmup executor
      - Steady-state detector
      - Phase separator
      - Warning system
    estimated_hours: "8-10"

  - id: pwh-m3
    name: Outlier Classification
    description: >
      Implement intelligent outlier classification to distinguish between
      measurement noise and legitimate tail latency events.
    acceptance_criteria:
      - Multiple outlier detection methods (IQR, z-score, MAD)
      - Classification: noise vs legitimate tail vs anomaly
      - Configurable sensitivity levels
      - Outlier frequency tracking over time
      - Correlation with system events (GC, compaction)
      - Reports outliers with context for debugging
    pitfalls:
      - Removing outliers hides real tail latency problems
      - Tail latency IS the measurement; don't over-filter
      - Different workloads have different tail profiles
      - Outlier thresholds are workload-specific
      - Correlation doesn't imply causation
    concepts:
      - Outlier detection
      - Noise classification
      - Anomaly identification
      - Context correlation
    skills:
      - Statistical detection
      - Classification algorithms
      - Context analysis
      - Debugging support
    deliverables:
      - Outlier detector
      - Classifier
      - Context correlator
      - Outlier report
    estimated_hours: "8-10"

  - id: pwh-m4
    name: Anomaly & Regression Detection
    description: >
      Build automated detection of performance anomalies and regressions
      with baseline comparison and alerting.
    acceptance_criteria:
      - Historical baseline storage
      - Statistical comparison (t-test, Mann-Whitney)
      - Regression threshold configuration
      - Automatic alerting on significant changes
      - Handles baseline drift over time
      - Weekly/monthly trend analysis
      - CI integration with pass/fail exit codes
    pitfalls:
      - Baseline drift makes old data irrelevant
      - False positives from normal variance
      - Multiple comparisons increase error rate
      - CI environment differs from production
      - Seasonal patterns (time-of-day, day-of-week)
    concepts:
      - Baseline management
      - Regression detection
      - Trend analysis
      - CI integration
    skills:
      - Statistical comparison
      - Baseline design
      - Trend analysis
      - CI/CD integration
    deliverables:
      - Baseline manager
      - Regression detector
      - Trend analyzer
      - CI integration
    estimated_hours: "8-10"

  - id: pwh-m5
    name: Reporting & Monitoring Integration
    description: >
      Create comprehensive reporting with actionable insights and
      integration with monitoring systems for continuous tracking.
    acceptance_criteria:
      - Human-readable reports with percentiles and trends
      - Machine-readable output (JSON, Prometheus)
      - Visualization data for dashboards
      - Alerts integrated with monitoring (Prometheus, Datadog)
      - Historical data archival and query
      - Comparison reports across versions/commits
      - SLA tracking (percentage of requests under threshold)
    pitfalls:
      - Too much data hides important signals
      - Dashboards need curation
      - Alert fatigue from too many false positives
      - Long-term storage costs
      - Different audiences need different reports
    concepts:
      - Report design
      - Monitoring integration
      - Alerting
      - SLA tracking
    skills:
      - Report generation
      - Monitoring APIs
      - Dashboard design
      - SLA metrics
    deliverables:
      - Report generator
      - Monitoring exporter
      - Alert integration
      - SLA tracker
    estimated_hours: "8-10"
