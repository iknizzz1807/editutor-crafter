id: query-optimizer
name: Query Optimizer
description: Build a cost-based query optimizer that transforms SQL query ASTs into efficient physical execution plans using statistics-driven cost estimation, rule-based logical rewrites, dynamic programming for join ordering, and physical operator selection.
difficulty: advanced
estimated_hours: 25-40
essence: Cost-based query plan generation through statistics collection (histograms, row counts), cardinality estimation, rule-based logical optimization (predicate pushdown, projection pruning), dynamic programming for join ordering, and selection between physical operators to minimize estimated execution cost.
why_important: Building a query optimizer teaches you the algorithmic foundations of modern databases and how to reason about computational complexity in systems with multiple valid execution paths — skills critical for backend engineering and data infrastructure roles.
learning_outcomes:
- Implement tree-based query plan representations with logical and physical operators
- Build a statistics collector (ANALYZE) that gathers histograms and row counts from tables
- Design cost models that estimate I/O and CPU usage using table statistics
- Apply rule-based logical optimizations (predicate pushdown, projection pruning, constant folding)
- Build dynamic programming algorithms for optimal join ordering
- Select between physical operators (sequential scan vs index scan, hash join vs merge join vs nested loop)
- Apply selectivity estimation using equi-depth histograms
- Debug query performance by analyzing and comparing execution plans
skills:
- Cost-based Optimization
- Dynamic Programming
- Cardinality Estimation
- Histogram Statistics
- Query Plan Trees
- Join Algorithms
- Rule-based Rewriting
- Performance Profiling
tags:
- advanced
- cost-estimation
- databases
- join-ordering
- optimization
- statistics
architecture_doc: architecture-docs/query-optimizer/index.md
languages:
  recommended:
  - Python
  - Java
  - Go
  also_possible:
  - Rust
  - C++
resources:
- name: CMU Database Systems Course (15-445)""
  url: https://15445.courses.cs.cmu.edu/
  type: course
- name: Access Path Selection in a Relational Database Management System (Selinger)""
  url: https://courses.cs.duke.edu/compsci516/cps216/spring03/papers/selinger-etal-1979.pdf
  type: paper
- name: Query Optimization Survey""
  url: https://www.vldb.org/pvldb/vol14/p3025-yang.pdf
  type: paper
prerequisites:
- type: project
  name: sql-parser
- type: skill
  name: Database fundamentals (tables, indexes, query execution)
- type: skill
  name: Algorithm complexity and dynamic programming
milestones:
- id: query-optimizer-m1
  name: Plan Representation & Statistics Collection
  description: Define query plan tree structure with logical and physical operator nodes, and implement a statistics collector (ANALYZE) that gathers row counts, distinct value counts, and histograms from table data.
  estimated_hours: 5-7
  concepts:
  - Logical operators (Scan, Filter, Join, Project, Aggregate)
  - Physical operators (SeqScan, IndexScan, HashJoin, MergeJoin, NLJoin)
  - Plan tree structure with parent-child relationships
  - Table statistics (row count, column distinct values, null fraction)
  - Equi-depth histograms for value distribution
  skills:
  - Tree data structure design
  - Operator pattern with polymorphism
  - Statistics collection algorithms
  - Histogram construction
  acceptance_criteria:
  - Plan tree has operator nodes with children representing inputs; each node carries estimated cost and output cardinality
  - Logical operators include Scan, Filter, Project, Join (with join type), and Aggregate
  - Physical operators include SeqScan, IndexScan, NestedLoopJoin, HashJoin, SortMergeJoin, and Sort
  - Each operator node is annotated with estimated row count (cardinality) and estimated cost
  - ANALYZE command scans a table and collects — total row count, per-column distinct value count, per-column null fraction, and per-column equi-depth histogram with configurable bucket count (default 100)
  - Histograms store bucket boundaries and per-bucket row counts for selectivity estimation
  - Statistics are stored in a catalog and can be retrieved by table name and column name
  - Plan tree pretty-printer outputs indented text showing operator type, estimated cardinality, and cost at each node
  pitfalls:
  - Logical and physical operators must be separate types; conflating them prevents exploring alternative physical implementations of the same logical plan
  - Histogram bucket count affects estimation accuracy — too few buckets miss value distribution details; too many waste memory
  - ANALYZE on a large table is expensive; support sampling (random N% of rows) for approximate statistics
  - Statistics become stale after data modifications; document that ANALYZE should be re-run after significant data changes
  deliverables:
  - Plan tree data structure with logical and physical operator node types
  - Cost and cardinality annotations per node
  - ANALYZE command collecting row counts, distinct values, null fractions, and histograms
  - Statistics catalog storing per-table, per-column statistics
  - Plan tree pretty-printer for debugging
- id: query-optimizer-m2
  name: Cost Estimation & Selectivity
  description: Implement cost models that estimate plan execution cost using table statistics. Estimate filter selectivity using histograms and join output cardinality using distinct value counts.
  estimated_hours: 5-8
  concepts:
  - Cost model (I/O pages + CPU tuples)
  - Filter selectivity estimation from histograms
  - Join cardinality estimation using min(distinct_A, distinct_B)
  - Independence assumption for compound predicates
  - Cost unit definition and weighting
  skills:
  - Statistical selectivity estimation
  - Probability-based cardinality prediction
  - Cost function design with I/O and CPU components
  - Histogram lookup and interpolation
  acceptance_criteria:
  - Cost model estimates total cost as weighted sum of I/O cost (pages read/written) and CPU cost (tuples processed); weights are configurable (default I/O weight = 1.0, CPU weight = 0.01)
  - Sequential scan cost = number_of_pages + (cpu_weight * number_of_rows)
  - Index scan cost = (selectivity * number_of_pages) + (selectivity * number_of_rows * cpu_weight) + index_lookup_cost
  - Filter selectivity for equality predicates (col = value) is estimated using histogram — bucket containing the value gives selectivity = bucket_frequency / total_rows
  - Filter selectivity for range predicates (col > value) is estimated by summing histogram buckets above the value with interpolation for partial buckets
  - Compound predicates with AND use multiplicative selectivity (independence assumption); OR uses additive minus intersection
  - Join output cardinality is estimated as (rows_A * rows_B) / max(distinct_A, distinct_B) for equi-joins
  - Cost estimates are within 2x of actual row counts on test datasets with uniform and skewed distributions (verified by running queries and counting actual rows)
  pitfalls:
  - Independence assumption for AND predicates is often wrong (correlated columns); acknowledge this as a known limitation
  - Histogram interpolation within a bucket assumes uniform distribution within the bucket; highly skewed data within buckets produces errors
  - Join cardinality estimation assumes key uniqueness on at least one side; many-to-many joins can produce much larger output than estimated
  - Statistics staleness after INSERT/DELETE causes cost estimates to diverge from reality; test with fresh statistics
  - Cost model weights (I/O vs CPU) depend on hardware; provide defaults but make them tunable
  deliverables:
  - Cost model with configurable I/O and CPU weight parameters
  - Sequential scan cost estimator
  - Index scan cost estimator with selectivity-based I/O reduction
  - Filter selectivity estimator using histogram lookup for equality and range predicates
  - Compound predicate selectivity using independence assumption
  - Join cardinality estimator using distinct value counts
  - Accuracy test comparing estimated vs actual row counts
- id: query-optimizer-m3
  name: Rule-Based Logical Optimization
  description: Implement rule-based logical rewrites that transform the logical plan tree to reduce cost before physical operator selection. Rules include predicate pushdown, projection pruning, and constant folding.
  estimated_hours: 5-8
  concepts:
  - Predicate pushdown (push filters below joins)
  - Projection pruning (remove unused columns early)
  - Constant folding (evaluate constant expressions at compile time)
  - Redundant predicate elimination
  - Rule application ordering
  skills:
  - AST/plan tree transformation
  - Pattern matching on tree nodes
  - Rule engine design
  - Algebraic equivalence rules
  acceptance_criteria:
  - Predicate pushdown moves filter operators below join operators when the filter references only one side of the join, reducing the number of rows entering the join
  - Predicate pushdown correctly handles filters that reference both sides of a join by leaving them above the join as join conditions
  - Projection pruning removes columns from intermediate results that are not referenced by any upstream operator, reducing tuple width
  - Constant folding evaluates expressions with only literal operands at optimization time (e.g., WHERE 1 = 1 is eliminated, WHERE 1 = 0 prunes the entire branch)
  - Redundant predicate elimination removes duplicate filter conditions (e.g., x > 5 AND x > 5 → x > 5)
  - Rules are applied iteratively until no more transformations are possible (fixed-point iteration)
  - Each rule transformation preserves query semantics — the logical plan produces the same result before and after transformation (verified by test)
  - Predicate pushdown demonstrably reduces estimated plan cost on a 3-table join query with selective filters
  pitfalls:
  - Predicate pushdown must check column provenance — pushing a filter that references table B below a join with table A is incorrect
  - Projection pruning must not remove columns needed by ORDER BY, GROUP BY, or HAVING even if they don't appear in SELECT
  - Rule ordering matters — predicate pushdown should run before join ordering so that pushed-down predicates reduce cardinality estimates used in join cost calculation
  - Fixed-point iteration can be infinite if rules are not monotonically reducing; add a max iteration limit (e.g., 100) as a safety valve
  deliverables:
  - Predicate pushdown rule moving filters below joins where applicable
  - Projection pruning rule removing unreferenced columns from intermediate plans
  - Constant folding rule evaluating compile-time constant expressions
  - Redundant predicate elimination rule
  - Rule engine applying transformations to fixed point
  - Test suite verifying semantic preservation and cost reduction
- id: query-optimizer-m4
  name: Join Ordering & Physical Plan Selection
  description: Find the optimal join order for multi-table queries using dynamic programming, and select physical operators (scan method, join algorithm) based on cost estimates to produce the final execution plan.
  estimated_hours: 7-12
  concepts:
  - Dynamic programming for join ordering (Selinger-style)
  - Left-deep vs bushy join trees
  - Physical operator selection based on cost
  - Access method selection (sequential scan vs index scan)
  - Join algorithm selection (nested loop, hash join, sort-merge join)
  - Sort order propagation (interesting orders)
  skills:
  - Dynamic programming implementation
  - Combinatorial optimization
  - Physical operator cost comparison
  - Memoization for subplan reuse
  acceptance_criteria:
  - Dynamic programming enumerates join subsets for multi-table queries; for N tables, considers all 2^N - 1 non-empty subsets with memoization
  - For each pair of subsets that combine to form a larger subset, the optimizer evaluates the cost of joining them and retains only the cheapest plan per subset
  - Both left-deep and bushy join tree shapes are considered (or configurable to restrict to left-deep only for reduced search space)
  - Cross products (joins without predicates) are penalized with a high cost multiplier and avoided when predicate-connected alternatives exist
  - Physical operator selection chooses between SeqScan and IndexScan based on filter selectivity — IndexScan is preferred when selectivity < configurable threshold (default 15%)
  - Join algorithm selection chooses between NestedLoopJoin (small outer, indexed inner), HashJoin (equi-join with large inputs), and SortMergeJoin (pre-sorted inputs or when sort order is needed downstream)
  - Final physical plan includes all operators selected with their cost estimates; total plan cost is the sum of all operator costs
  - For a 4-table join query, the optimizer produces a plan with demonstrably lower cost than a naive left-to-right join order (verified by comparing plan costs)
  - Optimization completes in under 1 second for queries with up to 8 tables; for more tables, a greedy heuristic fallback is used
  pitfalls:
  - Join ordering is exponential (2^N subsets); for >10 tables, DP is infeasible — implement a greedy or genetic algorithm fallback
  - Cross products must still be considered when no predicate connects two subsets; completely excluding them breaks queries with Cartesian products
  - IndexScan selection at low selectivity (e.g., 50%) is worse than SeqScan due to random I/O; the threshold depends on hardware
  - SortMergeJoin is advantageous when one or both inputs are already sorted (e.g., from an index scan on the join key); detecting this requires tracking 'interesting orders' through the plan
  - Hash join requires the smaller input to fit in memory; for very large joins, partition-based (grace) hash join is needed but is out of scope for this project
  deliverables:
  - Dynamic programming join ordering with subset enumeration and memoization
  - Left-deep and bushy join tree support
  - Cross product detection and penalization
  - Access method selector (SeqScan vs IndexScan based on selectivity)
  - Join algorithm selector (NLJ, HashJoin, SortMergeJoin based on input sizes and sort order)
  - Final physical plan generator with total cost annotation
  - Greedy heuristic fallback for queries exceeding DP table count limit
  - Comparison test showing optimizer plan cost vs naive join order cost
domain: data-storage
