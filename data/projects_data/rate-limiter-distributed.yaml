id: rate-limiter-distributed
name: Distributed Rate Limiter
description: Multi-node rate limiting with Redis backend
difficulty: intermediate
estimated_hours: 25-35
essence: 'Atomic counter operations and timestamp tracking across distributed nodes using Redis Lua scripts to enforce accurate rate limits while handling clock skew, race conditions, and burst traffic patterns, with multi-tier policies and graceful degradation when the shared store is unavailable.

  '
why_important: 'Rate limiting protects services from abuse and ensures fair resource usage. Building a distributed rate limiter teaches you how to coordinate state across multiple application nodes, design atomic operations in Redis, handle failure modes, and choose the right algorithm for different traffic patterns.

  '
learning_outcomes:
- Implement token bucket, sliding window log, and sliding window counter algorithms locally
- Design distributed rate limiting using Redis Lua scripts for atomicity
- Handle clock skew between application nodes using Redis server time
- Build multi-tier rate limiting (per-user, per-IP, per-endpoint, global)
- Implement graceful degradation when Redis is unavailable
- Measure and optimize rate limiter accuracy under concurrent load
- Emit standard rate limit response headers (RateLimit-Limit, RateLimit-Remaining, Retry-After)
skills:
- Token Bucket Algorithm
- Sliding Window Counter
- Redis Lua Scripting
- Distributed Synchronization
- Atomic Operations
- Multi-tier Rate Limit Policies
- Clock Skew Mitigation
- Graceful Degradation Patterns
tags:
- backend
- coordination
- distributed
- distributed-systems
- intermediate
- redis
- reliability
architecture_doc: architecture-docs/rate-limiter-distributed/index.md
languages:
  recommended:
  - Go
  - Java
  - Python
  also_possible: []
resources:
- name: Redis Rate Limiting Tutorial""
  url: https://redis.io/tutorials/howtos/ratelimiting/
  type: tutorial
- name: Rate Limiting Algorithms Explained""
  url: https://blog.algomaster.io/p/rate-limiting-algorithms-explained-with-code
  type: article
- name: Designing a Distributed Rate Limiter""
  url: https://blog.algomaster.io/p/designing-a-distributed-rate-limiter
  type: article
- name: Sliding Window Rate Limiter""
  url: https://arpitbhayani.me/blogs/sliding-window-ratelimiter/
  type: article
- name: IETF RateLimit Header Fields (draft)""
  url: https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers/
  type: documentation
prerequisites:
- type: project
  id: rate-limiter
  name: Rate Limiter (local)
- type: skill
  name: Redis basics (commands, data types, Lua eval)
- type: skill
  name: Distributed systems concepts (consistency, partitions)
milestones:
- id: rate-limiter-distributed-m1
  name: Local Rate Limiting Algorithms
  description: 'Implement token bucket, sliding window log, and sliding window counter algorithms as local (single-node) implementations. Benchmark accuracy and memory usage to understand trade-offs before distributing.

    '
  acceptance_criteria:
  - 'Token bucket algorithm: configurable capacity (burst size) and refill rate (tokens/second); a request is allowed if tokens > 0, otherwise rejected'
  - 'Sliding window log algorithm: stores exact timestamps of each request; counts requests in the trailing window; rejects if count exceeds limit'
  - 'Sliding window counter algorithm: uses weighted sum of current and previous fixed-window counters to approximate a sliding window; approximation error is bounded to at most 2x the limit at window boundaries'
  - All algorithms are thread-safe and correct under concurrent access from 100 goroutines/threads making simultaneous requests
  - 'Benchmark: measure memory usage per rate-limited key for each algorithm at 1000 RPS; sliding window log uses O(R) memory where R=requests in window; token bucket and sliding window counter use O(1)'
  - 'Accuracy test: at exactly the rate limit (e.g., 100 req/s), fewer than 2% of requests are incorrectly allowed or rejected over a 60-second run'
  pitfalls:
  - Token bucket allows instantaneous burst up to capacity, which may exceed the sustained rate limit; document this as expected behavior, not a bug
  - Sliding window log memory grows linearly with request rate per key; at 10K RPS with 1-minute window, that's 600K timestamps per key—use sorted sets with periodic cleanup
  - Sliding window counter approximation allows up to 2x the limit at the boundary between two fixed windows; this is the fundamental trade-off for O(1) memory
  - Using time.Now() instead of a monotonic clock source causes issues during NTP adjustments or leap seconds
  concepts:
  - Token bucket with refill rate and burst capacity
  - Sliding window log with exact timestamp tracking
  - Sliding window counter with weighted window approximation
  - Memory vs accuracy trade-offs in rate limiting
  deliverables:
  - Token bucket implementation with configurable rate and burst capacity
  - Sliding window log implementation with exact counting
  - Sliding window counter implementation with weighted approximation
  - Thread-safety verification test under concurrent load
  - Benchmark report comparing memory, accuracy, and latency of each algorithm
  estimated_hours: 5-7
- id: rate-limiter-distributed-m2
  name: Redis-Backed Distributed Rate Limiting
  description: 'Move rate limiting state to Redis using Lua scripts for atomic check-and-update operations. Handle clock skew by using Redis server time. Implement connection pooling, retry logic, and graceful degradation when Redis is unavailable.

    '
  acceptance_criteria:
  - Token bucket algorithm implemented as a single Redis Lua script that atomically reads current tokens, calculates refill based on elapsed time, decrements if allowed, and returns (allowed, remaining_tokens, retry_after_ms)
  - Sliding window counter implemented as a Redis Lua script using two hash keys for current and previous windows with atomic weighted-sum calculation
  - All Lua scripts use Redis TIME command (not application node time) as the time source, eliminating clock skew between application nodes
  - 'Clock skew test: two application nodes with 2-second clock difference both enforce the same rate limit against the same key without exceeding the limit'
  - Connection pool with configurable min/max connections, health checking, and automatic eviction of broken connections
  - 'Retry logic: transient Redis errors (connection reset, timeout) are retried once with 10ms delay before falling back'
  - 'Graceful degradation: when Redis is unreachable for >3 consecutive attempts, fall back to local in-memory rate limiting per node; log a warning; resume Redis-backed limiting when connectivity is restored'
  - 'Fallback test: disconnect Redis, verify requests are still rate-limited locally; reconnect Redis, verify shared state resumes within 5 seconds'
  pitfalls:
  - Without Lua scripts, MULTI/EXEC transactions in Redis do not support conditional logic (read-then-write); you must use EVAL for atomic check-and-update
  - Redis TIME returns seconds and microseconds, not milliseconds or nanoseconds; parse it correctly in the Lua script
  - Local fallback during Redis outage means each node enforces its own limit independently; with N nodes, the effective global limit becomes N * local_limit. Document this and set local_limit = global_limit / expected_nodes
  - Connection pool exhaustion under high load causes request queuing; monitor pool utilization and scale pool size accordingly
  - 'Redis single point of failure: for production, use Redis Sentinel or Redis Cluster for HA; this milestone uses a single instance for simplicity'
  concepts:
  - Redis Lua scripting for atomic operations
  - Server-side time source for clock skew elimination
  - Connection pooling and health checking
  - Graceful degradation with local fallback
  - Split-brain rate limiting during partitions
  deliverables:
  - Redis Lua script implementing token bucket with server-time-based refill
  - Redis Lua script implementing sliding window counter with weighted approximation
  - Connection pool manager with health checking and automatic reconnection
  - Local fallback rate limiter activated when Redis is unreachable
  - Clock skew verification test with simulated node time differences
  - Degradation and recovery integration test
  estimated_hours: 7-10
- id: rate-limiter-distributed-m3
  name: Multi-Tier Rate Limiting
  description: 'Implement hierarchical rate limiting with per-user, per-IP, per-endpoint, and global tiers. Evaluate tiers in priority order with short-circuit rejection.

    '
  acceptance_criteria:
  - 'Rate limit tiers are evaluated in order: global -> per-endpoint -> per-user (or per-IP for unauthenticated); if any tier rejects, the request is rejected immediately (short-circuit)'
  - 'Each tier has independently configurable limit and window (e.g., global: 10000/min, per-endpoint: 1000/min, per-user: 100/min)'
  - Per-user tier uses authenticated user ID as key; per-IP tier uses client IP as key for unauthenticated requests
  - Global tier aggregates all requests across all clients and all nodes via shared Redis counter
  - 'Burst allowance: per-user tier supports a configurable burst multiplier (e.g., 2x) allowing short spikes up to burst_limit before enforcing sustained rate'
  - 'Different rate limit thresholds per API endpoint are configurable (e.g., /api/search: 50/min vs /api/login: 5/min)'
  - 'Short-circuit test: when global tier rejects, per-user tier is not evaluated (verified by checking Redis command count)'
  - 'Response includes rate limit headers: RateLimit-Limit, RateLimit-Remaining, RateLimit-Reset (seconds until window reset), and Retry-After (seconds) when rejected—all from the most restrictive tier that triggered'
  pitfalls:
  - Evaluating all tiers even after one rejects wastes Redis round-trips; implement short-circuit evaluation that stops at the first rejection
  - 'Different window sizes per tier confuse clients: per-user window=1min but global window=1s means headers from different tiers show different reset times; always report the most restrictive tier''s headers'
  - Burst allowance combined with sliding window counter can allow up to burst * 2x limit at window boundaries; test this edge case
  - 'Rate limit key format must be carefully designed to avoid collisions: use structured keys like rl:{tier}:{endpoint}:{user_id}'
  concepts:
  - Hierarchical rate limit tiers with priority ordering
  - Short-circuit evaluation for performance
  - Rate limit key composition for multi-dimensional limiting
  - Standard rate limit response headers (IETF draft)
  - Burst allowance and sustained rate distinction
  deliverables:
  - Tier evaluator executing rate limit checks in priority order with short-circuit
  - Per-user, per-IP, per-endpoint, and global tier implementations
  - Configuration schema for per-tier and per-endpoint rate limit rules
  - Rate limit response header generator returning headers from the most restrictive tier
  - Burst allowance configuration and test
  - Integration test verifying multi-tier enforcement with short-circuit behavior
  estimated_hours: 6-8
- id: rate-limiter-distributed-m4
  name: Dynamic Configuration & Monitoring
  description: 'Build an API for dynamic rate limit rule management (CRUD) and a monitoring endpoint exposing current usage metrics per key and tier. Rules update without service restart.

    '
  acceptance_criteria:
  - 'REST API supports CRUD operations on rate limit rules: create, read, update, delete rules identified by tier + endpoint + key pattern'
  - Rule updates take effect within 5 seconds without requiring service restart; new requests use updated rules immediately
  - Rule validation rejects invalid configurations (e.g., negative limits, zero window, missing required fields) with descriptive error messages
  - Monitoring endpoint (/metrics or /stats) exposes per-key current usage, remaining quota, and time-to-reset for the top N most active keys
  - 'Prometheus-compatible metrics: rate_limit_requests_total (counter, labeled by tier, endpoint, decision=allowed|rejected), rate_limit_current_usage (gauge per key)'
  - 'Configuration propagation: in a multi-node deployment, a rule change on one node is visible to all nodes within 10 seconds (via shared Redis config or polling)'
  pitfalls:
  - Configuration propagation delay means different nodes enforce different rules briefly; document this eventual consistency window
  - Monitoring endpoint that queries Redis for every active key overwhelms Redis at scale; use local caching with periodic refresh
  - Allowing CRUD on rules without authentication or authorization lets anyone disable rate limiting; protect the API
  - High-cardinality per-key metrics can exhaust Prometheus memory; expose only top-N or aggregated metrics
  concepts:
  - Dynamic configuration management without restart
  - Configuration propagation in distributed systems
  - Prometheus metrics exposition
  - Rate limit rule validation
  deliverables:
  - REST API for rate limit rule CRUD with validation
  - Configuration store (Redis or database) with change propagation to all nodes
  - Prometheus metrics endpoint exposing rate limit decisions and current usage
  - Rule validation middleware rejecting invalid configurations
  - Propagation delay test verifying cross-node rule consistency
  estimated_hours: 5-7
domain: distributed
