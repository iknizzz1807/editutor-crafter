id: reverse-proxy
name: Reverse Proxy
description: HTTP reverse proxy with header manipulation, load balancing, connection pooling, caching, and TLS termination
difficulty: advanced
estimated_hours: 60-80
essence: 'HTTP/1.1 request parsing and forwarding between clients and backend servers with proper header manipulation (Host rewrite, X-Forwarded-* injection), streaming body relay without full buffering, TCP connection pooling for backend reuse, load distribution algorithms, response caching with HTTP semantics, and TLS termination with SNI-based certificate selection.

  '
why_important: 'Reverse proxies are fundamental infrastructure components used by every major web service at scale, teaching you low-level network programming, HTTP semantics, concurrency patterns, and performance optimization techniques directly applicable to distributed systems engineering roles.

  '
learning_outcomes:
- Parse and forward HTTP/1.1 requests with streaming body relay
- Implement proxy-specific header manipulation (Host, X-Forwarded-For, Via)
- Build connection pooling with keep-alive reuse and stale connection detection
- Implement load balancing algorithms (round-robin, least-connections, weighted)
- Design response caching respecting HTTP Cache-Control semantics
- Terminate TLS connections with SNI-based certificate selection
- Handle edge cases including chunked encoding, large bodies, and connection errors
skills:
- HTTP/1.1 parsing and semantics
- Header manipulation
- Load balancing algorithms
- Connection pooling
- TLS/SSL termination
- Response caching
- Streaming I/O
- Health checking
tags:
- advanced
- caching
- networking
- routing
- security
- service
- ssl-termination
architecture_doc: architecture-docs/reverse-proxy/index.md
languages:
  recommended:
  - Go
  - Rust
  - C
  also_possible:
  - Java
resources:
- name: RFC 9110 HTTP Semantics""
  url: https://httpwg.org/specs/rfc9110.html
  type: specification
- name: RFC 9111 HTTP Caching""
  url: https://httpwg.org/specs/rfc9111.html
  type: specification
- name: RFC 7239 Forwarded HTTP Extension""
  url: https://httpwg.org/specs/rfc7239.html
  type: specification
- name: NGINX Reverse Proxy Guide""
  url: https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/
  type: documentation
prerequisites:
- type: skill
  name: HTTP/1.1 request/response format
- type: skill
  name: TCP socket programming
- type: skill
  name: Async or multi-threaded I/O
milestones:
- id: reverse-proxy-m1
  name: HTTP/1.1 Proxy Core with Header Manipulation
  description: 'Accept HTTP/1.1 requests, rewrite proxy headers, forward to a single backend, and stream the response back. Handle both Content-Length and chunked transfer encoding without buffering entire bodies.

    '
  acceptance_criteria:
  - Accept incoming HTTP/1.1 requests on a configurable listen address and port
  - Parse request line (method, path, version) and all headers; reject malformed requests with 400
  - 'Rewrite Host header to match the backend''s host: port before forwarding'
  - Add X-Forwarded-For header with the client's IP address; append to existing header if present
  - Add X-Forwarded-Proto header indicating whether the client connection was HTTP or HTTPS
  - Add Via header indicating the proxy's identity per RFC 9110 §7.6.3
  - Forward the complete request (headers + body) to the configured backend server
  - Stream response body back to client using chunked transfer encoding or Content-Length without buffering the entire body in memory
  - Handle chunked Transfer-Encoding on both request and response sides; decode and re-encode if necessary
  - Return 502 Bad Gateway if the backend connection fails or times out; return 504 Gateway Timeout if the backend is too slow
  - 'Access log entry written for each request: timestamp, client IP, method, path, status, response size, duration'
  pitfalls:
  - 'Partial TCP reads: HTTP messages may arrive in multiple TCP segments; must buffer until headers are complete (look for \r\n\r\n)'
  - Buffering entire request/response bodies causes OOM on large uploads; stream chunks as they arrive
  - Not removing hop-by-hop headers (Connection, Keep-Alive, Transfer-Encoding, Proxy-*) before forwarding
  - Mixing up client-side and backend-side connection state; they are independent
  - 'Not handling Connection: close properly — if either side requests close, don''t reuse that connection'
  concepts:
  - HTTP/1.1 message format and parsing
  - Proxy header semantics (X-Forwarded-*, Via, Host)
  - Hop-by-hop vs end-to-end headers
  - Streaming body relay
  skills:
  - HTTP parsing
  - Header manipulation
  - Streaming I/O
  - Error handling
  deliverables:
  - HTTP request parser extracting method, path, headers
  - Header rewriter (Host, X-Forwarded-For, X-Forwarded-Proto, Via)
  - Hop-by-hop header stripper
  - Streaming body forwarder (chunked and Content-Length)
  - Backend connection with timeout
  - Error response generator (400, 502, 504)
  - Access logger
  estimated_hours: 12-16
- id: reverse-proxy-m2
  name: Load Balancing and Health Checks
  description: 'Distribute requests across multiple backends with configurable algorithms and automatic health checking.

    '
  acceptance_criteria:
  - Round-robin distributes requests evenly across all healthy backends; verified by sending N requests and checking distribution
  - Weighted round-robin distributes requests proportionally to configured weights (e.g., weight 3 gets 3x traffic of weight 1)
  - Least-connections algorithm routes each new request to the backend with the fewest active connections
  - Health check sends HTTP GET to each backend's health endpoint at configurable intervals (e.g., every 5s); backend marked unhealthy after N consecutive failures
  - Unhealthy backends are removed from the active pool; healthy backends are re-added when health checks pass again
  - If all backends are unhealthy, return 503 Service Unavailable to client
  - Backend configuration is loaded from a config file defining upstream servers with address, weight, and health check path
  pitfalls:
  - Using naive round-robin without health checks sends requests to dead backends causing client-visible errors
  - Not handling backend failure DURING request forwarding; must retry on a different backend or return 502
  - 'Health check thundering herd: if health checks are synchronized, all backends get checked simultaneously causing load spikes'
  - Least-connections counter must be updated atomically in multi-threaded proxy
  - 'Weight changes when backends go down: remaining backends should absorb proportionally, not equally'
  concepts:
  - Load balancing algorithms
  - Active health checking
  - Backend pool management
  - Failover and retry
  skills:
  - Algorithm implementation
  - Health check design
  - Configuration management
  deliverables:
  - Round-robin load balancer
  - Weighted round-robin load balancer
  - Least-connections load balancer
  - Health check runner with configurable interval and failure threshold
  - Backend pool manager (add/remove/mark healthy/unhealthy)
  - Configuration file parser for upstream definition
  estimated_hours: 10-14
- id: reverse-proxy-m3
  name: Connection Pooling
  description: 'Maintain persistent keep-alive connections to backends for reuse, reducing TCP handshake overhead.

    '
  acceptance_criteria:
  - Persistent HTTP/1.1 keep-alive connections to backends are maintained in per-backend pools
  - 'After forwarding a request/response, the backend connection is returned to the pool for reuse (if Connection: keep-alive)'
  - Stale connections are detected before reuse by checking for unexpected data or closure; stale connections are discarded and a new one is created
  - Configurable maximum pool size per backend (e.g., 10 connections); excess connections are closed after use
  - Idle timeout evicts connections that have been unused for a configurable duration (e.g., 60s)
  - 'Pool exhaustion handling: if all pooled connections are in use and max is reached, either queue the request or create a burst connection with a short keep-alive'
  - 'Pool metrics: active connections, idle connections, total created, total reused, total evicted'
  pitfalls:
  - Reusing a connection that the backend closed (half-open); attempt to read/write will fail. Must validate before use or handle the error and retry.
  - Not enforcing pool limits causes connection count to grow unbounded under load, exhausting file descriptors
  - Connection pool must be thread-safe; lock contention on the pool becomes a bottleneck under high concurrency
  - HTTP/1.1 pipelining is largely unsupported by backends; do not send a second request on a connection before the first response completes
  - Keep-alive timeout on the backend side may be shorter than the proxy's idle timeout; the proxy thinks the connection is alive but the backend closed it
  concepts:
  - Connection lifecycle management
  - Pool sizing and resource limits
  - Connection validation
  - Keep-alive semantics
  skills:
  - Connection pool implementation
  - Resource management
  - Thread-safe data structures
  deliverables:
  - Per-backend connection pool with configurable max size
  - Connection checkout/return lifecycle
  - Stale connection detection and eviction
  - Idle timeout eviction
  - Pool metrics reporting
  estimated_hours: 10-12
- id: reverse-proxy-m4
  name: Response Caching
  description: 'Cache backend responses in-memory respecting HTTP caching semantics.

    '
  acceptance_criteria:
  - Only GET and HEAD responses are eligible for caching; POST/PUT/DELETE responses are never cached
  - Cache key is generated from method + URL + relevant Vary header values (if Vary header is present, the specified request headers are included in the key)
  - 'Cache-Control directives are respected: no-store prevents caching entirely; no-cache allows storage but requires revalidation; max-age sets TTL; private prevents shared cache storage'
  - Expired entries are revalidated using conditional requests (If-None-Match with ETag, If-Modified-Since with Last-Modified); 304 Not Modified refreshes the cached entry without re-transferring the body
  - Cache size limit is enforced; LRU eviction removes least-recently-accessed entries when the cache is full
  - 'Cache stampede protection: when a cached entry expires, only one request is sent to the backend; other concurrent requests for the same key are queued and served from the refreshed cache'
  - 'Cache bypass for requests with Authorization header or Cache-Control: no-cache from client'
  - Cache hit/miss metrics tracked and reported
  pitfalls:
  - Ignoring the Vary header causes serving the wrong cached response (e.g., gzip-encoded content to a client that doesn't accept gzip)
  - Caching responses with Set-Cookie headers leaks session data to other users; these should be excluded
  - Not implementing conditional revalidation means every expired entry requires a full re-download
  - Cache stampede (thundering herd) when a popular cached entry expires; all concurrent requests hit the backend simultaneously
  - Serving stale content beyond max-age without revalidation violates HTTP semantics
  concepts:
  - HTTP caching semantics (RFC 9111)
  - Cache-Control directive parsing
  - Conditional requests and revalidation
  - Cache stampede prevention
  skills:
  - Cache implementation
  - HTTP header parsing
  - LRU eviction
  - Concurrent access patterns
  deliverables:
  - In-memory cache with configurable max size and LRU eviction
  - Cache key generator incorporating Vary headers
  - Cache-Control parser and policy enforcer
  - Conditional request support (ETag, Last-Modified)
  - Cache stampede protection (request coalescing)
  - Cache metrics (hits, misses, evictions, hit rate)
  estimated_hours: 12-14
- id: reverse-proxy-m5
  name: TLS Termination
  description: 'Terminate TLS at the proxy, forwarding decrypted HTTP to backends. Support SNI for multi-domain hosting.

    '
  acceptance_criteria:
  - Proxy accepts HTTPS connections and performs TLS handshake using a configured certificate and private key
  - SNI (Server Name Indication) is used to select the correct certificate when multiple domains are served; fallback to default cert if no SNI match
  - Minimum TLS version is enforced (TLS 1.2+); TLS 1.0 and 1.1 connections are rejected
  - Strong cipher suites are prioritized; weak ciphers (RC4, 3DES, export ciphers) are disabled
  - 'Decrypted HTTP request is forwarded to backend over plain HTTP; X-Forwarded-Proto: https is set'
  - 'Certificate reload without proxy restart: SIGHUP or config reload triggers re-reading cert/key files from disk'
  - 'HTTP-to-HTTPS redirect: plaintext HTTP requests on port 80 receive a 301 redirect to the HTTPS URL'
  - 'Certificate chain validation: intermediate certificates are included in the TLS handshake'
  pitfalls:
  - Storing private keys in source code, logs, or error messages; keys must be loaded from files with restricted permissions
  - 'SNI mismatch: if the certificate doesn''t match the SNI hostname, browsers show a warning; verify certificate CN/SAN matches'
  - Not including intermediate certificates in the chain causes validation failure on clients that don't have the intermediate cached
  - TLS session resumption (session tickets/IDs) must be handled to avoid expensive full handshakes on every connection
  - OCSP stapling improves client-side validation speed; not implementing it causes slower TLS handshakes as clients fetch OCSP themselves
  concepts:
  - TLS handshake and cipher negotiation
  - Server Name Indication (SNI)
  - Certificate chain and trust
  - TLS termination proxy pattern
  skills:
  - TLS/SSL configuration
  - Certificate management
  - Security hardening
  deliverables:
  - TLS listener with certificate and key loading
  - SNI-based certificate selection
  - TLS version and cipher suite configuration
  - Certificate chain loading (leaf + intermediates)
  - Certificate reload on signal (SIGHUP)
  - HTTP-to-HTTPS redirect handler
  - X-Forwarded-Proto injection for backend awareness
  estimated_hours: 12-16
domain: systems
