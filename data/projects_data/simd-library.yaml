id: simd-library
name: SIMD Optimization Library
description: SSE/AVX intrinsics for memcpy, string search, and vector math
difficulty: intermediate
estimated_hours: "20-30"
essence: >
  Data-parallel computation using CPU vector registers to process multiple
  values simultaneously, requiring explicit memory alignment, safe memory
  access patterns, and understanding of instruction-level parallelism to
  achieve significant performance gains over scalar code.
why_important: >
  SIMD optimization is critical for performance-sensitive applications and
  teaches low-level performance engineering skills that separate competent
  programmers from systems experts who can extract maximum hardware efficiency.
learning_outcomes:
  - Implement memory operations using 128-bit and 256-bit vector registers with proper alignment
  - Write intrinsic functions mapping directly to machine instructions
  - Design safe memory access patterns that avoid page-fault hazards
  - Benchmark SIMD code rigorously against scalar implementations
  - Analyze compiler auto-vectorization and identify when hand-written SIMD wins
  - Debug alignment faults and understand memory boundary requirements
  - Implement horizontal reduction operations efficiently using shuffle patterns
skills:
  - Vector Instructions (SSE2, SSE4.1, AVX2)
  - Memory Alignment
  - Performance Benchmarking
  - Assembly Analysis
  - CPU Architecture
  - Data-Parallel Programming
  - Compiler Optimization Analysis
tags:
  - simd
  - performance
  - vectorization
  - cpu-architecture
  - optimization
architecture_doc: architecture-docs/simd-library/index.md
languages:
  recommended:
    - C
    - Rust
  also_possible:
    - C++
resources:
  - name: Intel Intrinsics Guide
    url: "https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html"
    type: reference
  - name: SIMD for C++ Developers
    url: "http://const.me/articles/simd/simd.pdf"
    type: article
  - name: Agner Fog's Optimization Manuals
    url: "https://www.agner.org/optimize/"
    type: reference
prerequisites:
  - type: skill
    name: C programming (pointers, bitwise ops)
  - type: skill
    name: CPU architecture basics (registers, cache lines)
milestones:
  - id: simd-library-m1
    name: "SSE2 Basics: memset and memcpy"
    description: Implement basic memory operations using SSE2 128-bit registers.
    estimated_hours: "5-7"
    concepts:
      - SSE2 intrinsics
      - Memory alignment (16-byte)
      - Aligned vs unaligned loads/stores
      - Scalar prologue/epilogue pattern
    skills:
      - SSE2 intrinsics programming
      - Memory alignment optimization
      - SIMD register management
      - Performance benchmarking basics
    acceptance_criteria:
      - "Implement simd_memset using _mm_store_si128 processing 16 bytes per iteration for aligned regions"
      - "Implement simd_memcpy using _mm_load_si128 / _mm_store_si128 pairs for aligned 16-byte copies"
      - "Handle unaligned buffer edges with a scalar prologue (process bytes until 16-byte aligned) and scalar epilogue (process remaining bytes after last full 16-byte chunk)"
      - "For buffers smaller than 16 bytes, fall back entirely to scalar implementation"
      - "Benchmark against libc memset/memcpy and scalar loop across buffer sizes (64B, 1KB, 64KB, 1MB, 16MB)"
      - "Show measurable speedup over naive scalar loop for buffers >= 1KB; explain results where no speedup is observed (libc is already SIMD-optimized)"
      - "Use _mm_stream_si128 (non-temporal store) for buffers larger than L2 cache and benchmark the difference"
    pitfalls:
      - "_mm_store_si128 requires 16-byte aligned pointers; use _mm_storeu_si128 for unaligned, but prefer aligning the pointer"
      - "Compiler may auto-vectorize scalar code, making benchmarks unfair; use __attribute__((optimize(\"no-tree-vectorize\"))) or volatile for scalar baseline"
      - "libc memset/memcpy are heavily optimized (often using AVX+non-temporal stores); you're competing against expert-written code"
      - "Forgetting the scalar prologue/epilogue causes segfaults on unaligned inputs"
    deliverables:
      - SIMD memset with alignment handling and scalar fallback
      - SIMD memcpy with alignment handling and scalar fallback
      - Non-temporal store variant for large buffers
      - Benchmark suite with fair scalar baseline (auto-vectorization disabled)

  - id: simd-library-m2
    name: "String Operations: strlen and memchr"
    description: Implement SIMD-optimized string scanning with safe memory access patterns.
    estimated_hours: "5-7"
    concepts:
      - Parallel byte comparison
      - Bitmask extraction (movemask)
      - Page-boundary-safe memory access
    skills:
      - Bitwise operations for parallel comparison
      - Safe memory boundary handling
      - Efficient string scanning with SIMD
    acceptance_criteria:
      - "Implement simd_strlen using _mm_cmpeq_epi8 to compare 16 bytes against zero simultaneously"
      - "Implement simd_memchr using _mm_cmpeq_epi8 to find a target byte across 16 bytes simultaneously"
      - "Use _mm_movemask_epi8 to convert comparison results to a 16-bit bitmask; use __builtin_ctz (count trailing zeros) to find the first match position"
      - "Handle page-boundary safety: before the first aligned read, compute the distance from the input pointer to the next page boundary (4096-byte aligned). If the input is within 16 bytes of a page boundary, use scalar scanning until aligned. After alignment, 16-byte aligned reads within a page cannot fault."
      - "For the initial unaligned chunk (pointer not 16-byte aligned), either use scalar scanning OR read from the aligned-down address and mask out bytes before the actual start"
      - "Verify correctness with strings of lengths 0, 1, 15, 16, 17, 4095, 4096, 4097 (page boundary cases)"
      - "Benchmark against libc strlen/memchr across string lengths"
    pitfalls:
      - "CRITICAL: Reading 16 bytes starting at an arbitrary address can cross a page boundary into unmapped memory, causing a segfault. Always align reads to 16-byte boundaries; an aligned 16-byte read that starts within a mapped page cannot cross a page boundary."
      - "When reading from an aligned-down address, the first few bytes may be before the string start; mask these out of the movemask result to avoid false positives"
      - "movemask returns a 16-bit value where bit N corresponds to byte N; __builtin_ctz gives the index of the first set bit"
      - "For memchr with a size limit, must not scan past the end of the buffer"
    deliverables:
      - SIMD strlen with page-safe aligned reads
      - SIMD memchr with page-safe aligned reads
      - Bitmask-based first-match detection
      - Correctness test suite including page-boundary edge cases
      - Benchmark comparison against libc

  - id: simd-library-m3
    name: "Math Operations: Dot Product and Matrix Multiply"
    description: Implement SIMD-optimized floating-point math operations.
    estimated_hours: "5-8"
    concepts:
      - Floating-point SIMD (_mm_mul_ps, _mm256_mul_ps)
      - Efficient horizontal reduction via shuffle+add (NOT hadd)
      - AVX/SSE transition penalties
      - Data layout for vectorization
    skills:
      - AVX/SSE vector math programming
      - Matrix multiplication optimization
      - Cache-friendly data layout design
      - CPU feature detection
    acceptance_criteria:
      - "Implement SIMD dot product using _mm_mul_ps for element-wise multiply, then reduce using shuffle+add pattern (_mm_shuffle_ps + _mm_add_ps), NOT _mm_hadd_ps"
      - "Implement 4x4 float matrix multiplication using SSE, processing one row-column dot product per iteration"
      - "Support both row-major and column-major matrix layouts; benchmark both to demonstrate access pattern impact"
      - "Implement AVX (256-bit) variants of both operations using _mm256_* intrinsics"
      - "Insert _mm256_zeroupper() or use __attribute__((target(\"avx\"))) to avoid AVX-SSE transition penalties when mixing code paths"
      - "Implement runtime CPU feature detection using CPUID (or compiler builtins) to select SSE or AVX code path"
      - "Benchmark scalar vs SSE vs AVX across array sizes (4, 16, 64, 256, 1024 elements for dot product; various matrix sizes)"
      - "Show at least 2x speedup for SSE and 3x for AVX over scalar for large inputs (1024+ elements)"
    pitfalls:
      - "_mm_hadd_ps has 3-5 cycle latency on many Intel CPUs; the shuffle+vertical-add pattern is faster for horizontal reduction"
      - "Row-major vs column-major layout drastically affects cache performance for matrix multiply; column-major allows vectorized column access"
      - "AVX-SSE transition penalty (up to ~70 cycles on older Intel) occurs when mixing 128-bit SSE and 256-bit AVX without vzeroupper"
      - "-ffast-math may be needed for the compiler to auto-vectorize floating-point reductions (it changes semantics); document tradeoffs"
    deliverables:
      - SIMD dot product with shuffle+add horizontal reduction
      - 4x4 matrix multiply with SSE and AVX variants
      - Runtime CPU feature detection for code path selection
      - vzeroupper handling for clean AVX-SSE transitions
      - Benchmark suite comparing scalar, SSE, AVX across data sizes

  - id: simd-library-m4
    name: Auto-vectorization Analysis
    description: Study compiler auto-vectorization and compare with hand-written SIMD.
    estimated_hours: "5-8"
    concepts:
      - Compiler auto-vectorization
      - Vectorization reports
      - Assembly reading
      - Benchmark methodology
    skills:
      - Compiler optimization analysis
      - Assembly code reading
      - Rigorous benchmarking methodology
      - Compiler flag tuning
    acceptance_criteria:
      - "Write scalar versions of all functions structured for auto-vectorization (simple loops, no pointer aliasing, use restrict keyword)"
      - "Compile with -O3 -march=native -ftree-vectorize -fopt-info-vec-all (GCC) or -Rpass=loop-vectorize (Clang) and capture vectorization reports"
      - "Inspect generated assembly for at least 3 functions; annotate which SIMD instructions the compiler chose"
      - "Identify at least 2 cases where hand-written SIMD outperforms auto-vectorization and explain why (e.g., the compiler couldn't prove alignment, couldn't vectorize complex control flow, chose suboptimal instruction sequence)"
      - "Identify at least 1 case where auto-vectorization matches or beats hand-written SIMD and explain why"
      - "Benchmark methodology: pin CPU frequency (disable turbo boost and frequency scaling), warm up caches with 3 untimed runs, report median of 10+ timed runs with standard deviation"
      - "Disable auto-vectorization for scalar baselines using __attribute__((optimize(\"no-tree-vectorize\"))) or equivalent"
      - "Produce a written analysis document (markdown) summarizing findings with assembly excerpts and benchmark data"
    pitfalls:
      - "CPU frequency scaling and turbo boost cause massive benchmark variance; pin frequency with cpupower or equivalent"
      - "Compiler may not vectorize loops with potential pointer aliasing; use restrict and __builtin_assume_aligned to help"
      - "-ffast-math enables FP reassociation which allows vectorization but changes numerical results; document when used"
      - "Micro-benchmarks on tiny data may be dominated by function call overhead, not the actual computation"
      - "Different compiler versions produce different vectorization decisions; record compiler version"
    deliverables:
      - Scalar implementations annotated for auto-vectorization
      - Compiler vectorization reports with analysis
      - Annotated assembly excerpts for key functions
      - Comprehensive benchmark with statistical rigor (median, stddev, frequency pinning)
      - Written analysis document with conclusions