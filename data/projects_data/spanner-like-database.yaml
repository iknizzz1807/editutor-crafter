id: spanner-like-database
name: Spanner-like Database
description: >
  Build a globally distributed database with strong consistency like Google Spanner.
  Implement TrueTime-inspired timestamp ordering, distributed transactions, and
  cross-region replication with minimal latency impact.

difficulty: expert
estimated_hours: 100-130
domain: world-scale

essence: >
  Globally consistent distributed database using synchronized clocks (or clock uncertainty
  bounds) for external consistency, two-phase commit for distributed transactions, and
  Paxos/Raft for replication - achieving both horizontal scalability and ACID guarantees.

why_important: >
  Spanner is Google's globally distributed database powering Gmail, AdWords, and more.
  Understanding how to build globally consistent systems is essential for distributed
  systems engineers at $250K-500K+ at cloud providers and database companies.

learning_outcomes:
  - Implement timestamp ordering with clock uncertainty
  - Build distributed transactions with 2PC across shards
  - Implement Paxos or Raft for replica consensus
  - Handle cross-region latency with careful placement
  - Implement read-only transactions at specific timestamps
  - Build schema changes without downtime
  - Handle clock synchronization and uncertainty
  - Implement directory-based data placement

skills:
  - Distributed Transactions
  - Timestamp Ordering
  - Clock Synchronization
  - Paxos/Raft Replication
  - Cross-Region Latency
  - Schema Evolution
  - External Consistency
  - Data Placement

tags:
  - expert
  - distributed-database
  - spanner
  - truetype
  - distributed-transactions
  - global-consistency

languages:
  recommended:
    - Go
    - Rust
  also_possible:
    - Java
    - C++

resources:
  - name: "Spanner: Google's Globally-Distributed Database"
    url: https://research.google/pubs/pub39966/
    type: paper
  - name: "Spanner: Becoming a SQL System"
    url: https://research.google/pubs/pub46103/
    type: paper
  - name: "CockroachDB Architecture"
    url: https://www.cockroachlabs.com/docs/architecture/overview.html
    type: documentation
  - name: "TiDB Architecture"
    url: https://docs.pingcap.com/tidb/stable/tidb-architecture
    type: documentation

prerequisites:
  - type: project
    name: build-raft or equivalent consensus experience
  - type: project
    name: 2pc-impl or equivalent transaction experience
  - type: skill
    name: Distributed systems fundamentals
  - type: skill
    name: Database internals understanding

milestones:
  - id: spanner-m1
    name: Timestamp & Clock Uncertainty
    description: >
      Implement a timestamp system that handles clock uncertainty
      across distributed nodes.
    acceptance_criteria:
      - Each node maintains local clock with uncertainty bound (epsilon)
      - TrueTime abstraction: returns [earliest, latest] interval
      - Timestamps are globally unique (node ID + sequence)
      - Events ordered by timestamp, respecting uncertainty bounds
      - Wait-for-commit delays ensure timestamp is definitively in the past
      - Simulated clock drift for testing uncertainty handling
    pitfalls:
      - Clock drift can exceed assumed uncertainty bounds
      - Leap seconds cause time to move backward
      - NTP synchronization has latency and jitter
      - Clock uncertainty creates commit wait latency
    concepts:
      - Clock uncertainty
      - TrueTime abstraction
      - Timestamp ordering
      - Commit wait
    skills:
      - Clock handling
      - Timestamp generation
      - Uncertainty bounds
      - Time ordering
    deliverables:
      - TrueTime abstraction
      - Timestamp generation
      - Commit wait implementation
      - Clock drift simulation
    estimated_hours: "16-20"

  - id: spanner-m2
    name: Replicated Log with Paxos/Raft
    description: >
      Implement replicated log per shard using Paxos or Raft for
      fault tolerance and consistency.
    acceptance_criteria:
      - Each shard (tablet) replicated across multiple replicas
      - Paxos or Raft maintains consistency across replicas
      - Leader handles all writes, followers serve stale reads
      - Leader election on failure within seconds
      - Replication works across simulated regions/zones
      - Log entries include operation, timestamp, and client info
    pitfalls:
      - Split-brain if leader election fails
      - Unbounded log growth - need snapshotting
      - Cross-region latency impacts write throughput
      - Not handling network partitions correctly
    concepts:
      - Consensus protocols
      - Replicated logs
      - Leader election
      - Cross-region replication
    skills:
      - Raft/Paxos implementation
      - Replica management
      - Failure handling
      - Cross-region optimization
    deliverables:
      - Replicated log per shard
      - Leader election
      - Cross-region replication
      - Failure recovery
    estimated_hours: "20-26"

  - id: spanner-m3
    name: Distributed Transactions
    description: >
      Implement distributed transactions spanning multiple shards
      using two-phase commit with timestamp ordering.
    acceptance_criteria:
      - Transactions can span multiple shards (tablets)
      - Two-phase commit coordinates across participant shards
      - Timestamp assigned at commit time, respects clock uncertainty
      - Read-only transactions read at specific timestamp (snapshot)
      - Write-write conflicts detected and resolved
      - Deadlock detection across distributed locks
    pitfalls:
      - 2PC coordinator failure blocks participants
      - Distributed deadlocks are hard to detect
      - Cross-shard transactions have higher latency
      - Long-running transactions hold locks too long
    concepts:
      - Distributed 2PC
      - Timestamp-based commit
      - Snapshot isolation
      - Distributed deadlock
    skills:
      - 2PC implementation
      - Timestamp assignment
      - Snapshot reads
      - Deadlock detection
    deliverables:
      - Distributed transactions
      - Timestamp-based commit
      - Snapshot isolation
      - Deadlock detection
    estimated_hours: "20-26"

  - id: spanner-m4
    name: SQL Layer & Query Processing
    description: >
      Implement SQL query layer that compiles queries to distributed
      execution plans.
    acceptance_criteria:
      - SQL parser supports SELECT, INSERT, UPDATE, DELETE
      - Query planner generates distributed execution plans
      - Distributed joins across shards
      - Secondary indexes supported with index-only scans
      - Query results respect transaction snapshot timestamp
      - Explain plan shows distributed execution strategy
    pitfalls:
      - Cross-shard joins are expensive - co-location helps
      - Query optimizer mistakes cause poor performance
      - Not pushing down predicates wastes network bandwidth
      - Large result sets need streaming
    concepts:
      - SQL parsing
      - Query planning
      - Distributed execution
      - Secondary indexes
    skills:
      - SQL layer
      - Query planning
      - Distributed execution
      - Index design
    deliverables:
      - SQL parser
      - Query planner
      - Distributed executor
      - Secondary indexes
    estimated_hours: "18-24"

  - id: spanner-m5
    name: Schema Changes & Data Placement
    description: >
      Implement online schema changes and directory-based data placement
      for multi-tenant scenarios.
    acceptance_criteria:
      - Schema changes without downtime (add column, add index)
      - Schema change progresses through states (delete-only, write-only, public)
      - Directories (groups of related data) placed across regions
      - Data movement between regions with minimal disruption
      - Multi-tenant isolation at directory level
      - Placement policy respects latency and compliance requirements
    pitfalls:
      - Schema change races with ongoing queries
      - Data movement creates inconsistent intermediate states
      - Placement decisions affect latency and cost
      - Compliance requirements constrain placement
    concepts:
      - Online schema change
      - Directory abstraction
      - Data placement
      - Multi-tenancy
    skills:
      - Schema evolution
      - Data placement
      - Multi-tenant design
      - Compliance handling
    deliverables:
      - Online schema change
      - Directory abstraction
      - Data placement
      - Multi-tenant isolation
    estimated_hours: "16-20"
