id: stream-processing-engine
name: Stream Processing Engine
description: Real-time data processing engine with windowing, event-time watermarks, stateful operators with checkpointing, backpressure, and exactly-once processing guarantees.
difficulty: expert
estimated_hours: 105
essence: Distributed dataflow DAG with typed operator chains, serialization/deserialization for network data exchange, credit-based backpressure for flow control, watermark-driven event-time semantics for out-of-order events, barrier-based checkpointing for globally consistent state snapshots, and two-phase commit sinks for exactly-once output guarantees.
why_important: Building this teaches distributed systems fault tolerance, the complexities of time in distributed computing, state consistency mechanisms, and flow control—skills that underpin production streaming platforms like Flink, Kafka Streams, and real-time analytics systems.
learning_outcomes:
- Design a dataflow execution graph with typed operators and parallel execution
- Implement serialization/deserialization for data exchange between operators
- Build credit-based backpressure for flow control across operator chains
- Implement tumbling, sliding, and session windows with configurable triggers
- Handle out-of-order events using watermarks and allowed lateness
- Build keyed state with checkpointing for fault-tolerant stateful processing
- Implement exactly-once semantics via barrier-based checkpointing and transactional sinks
skills:
- Dataflow graph execution
- Serialization/Deserialization
- Backpressure and flow control
- Event time processing
- Windowing
- State management
- Checkpointing
- Watermarks
- Exactly-once semantics
tags:
- aggregation
- expert
- framework
- streaming
- watermarks
- windowing
- fault-tolerance
architecture_doc: architecture-docs/stream-processing-engine/index.md
languages:
  recommended:
  - Java
  - Scala
  - Go
  also_possible:
  - Rust
resources:
- name: Apache Flink Documentation
  url: https://nightlies.apache.org/flink/flink-docs-lts/
  type: documentation
- name: Streaming Systems Book (Akidau, Chernyak, Lax)""
  url: https://www.oreilly.com/library/view/streaming-systems/9781491983867/
  type: book
- name: Lightweight Asynchronous Snapshots for Distributed Dataflows (Chandy-Lamport variant)""
  url: https://arxiv.org/abs/1506.08603
  type: paper
- name: Exactly-Once Semantics in Kafka
  url: https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/
  type: article
prerequisites:
- type: skill
  name: Distributed systems fundamentals
- type: skill
  name: Message queues and pub/sub
- type: skill
  name: State machines and concurrency
- type: skill
  name: Serialization frameworks (protobuf, Avro, or similar)
milestones:
- id: stream-processing-engine-m1
  name: Dataflow Graph, SerDe & Backpressure
  description: Build the core stream abstraction with typed operators, serialization for data exchange, execution graph compilation, and credit-based backpressure for flow control.
  acceptance_criteria:
  - DataStream represents an unbounded sequence of typed records with a schema
  - Map, filter, and flatMap operators transform stream elements with correct semantics (verified by unit tests on at least 10 transformation scenarios)
  - KeyBy operator partitions stream elements by extracted key for downstream keyed operations
  - Operator chain compiles into an execution DAG with source, transformation, and sink nodes
  - Serialization/deserialization (SerDe) framework encodes records for network transfer between operators; at least two formats supported (JSON and binary)
  - 'Credit-based backpressure: downstream operators signal available capacity to upstream; upstream stops sending when downstream credits are exhausted'
  - 'Backpressure propagates end-to-end: a slow sink causes the source to throttle (verified by test with artificial slow sink)'
  - Parallel execution of keyed streams distributes records across parallel operator instances by key hash
  pitfalls:
  - Materializing entire streams in memory instead of lazy/pull-based processing causes OOM on unbounded streams
  - SerDe must be schema-aware; serializing with one schema and deserializing with another causes silent data corruption
  - Credit-based backpressure must propagate across all operators in the chain, not just adjacent pairs
  - KeyBy partition function must be deterministic—same key always routes to same parallel instance
  - Creating stateful closures in stateless operators (map/filter) causes correctness issues during rescaling
  concepts:
  - Functional transformations (map, filter, flatMap) on infinite data streams
  - Lazy evaluation processes records on-demand without materializing the full stream
  - Serialization converts typed records to bytes for network transfer
  - Credit-based flow control prevents fast producers from overwhelming slow consumers
  - Execution graph compiles operator chains into a parallel DAG
  skills:
  - Stream API design
  - SerDe framework implementation
  - Backpressure mechanisms
  - DAG compilation
  - Parallel execution
  deliverables:
  - DataStream class with typed record schema
  - Map, filter, flatMap operators with unit tests
  - KeyBy partitioning operator
  - Execution graph builder compiling operator chain into DAG
  - SerDe framework with JSON and binary format support
  - Credit-based backpressure between operators
  - Source and sink interfaces for pluggable I/O
  - Parallel operator instances with key-based routing
  estimated_hours: 24
- id: stream-processing-engine-m2
  name: Windowing
  description: Implement time-based windowing for aggregations with configurable triggers and late-data handling.
  acceptance_criteria:
  - Tumbling windows group elements into fixed non-overlapping time intervals (e.g., every 5 minutes)
  - Sliding windows group elements into overlapping intervals with configurable window size and slide (e.g., 10-minute window, 5-minute slide)
  - Session windows merge when gap between consecutive events for the same key is below a configurable inactivity threshold
  - 'Window triggers fire computation at specified conditions: on watermark passing window end, on element count, or on processing time timer'
  - Allowed lateness parameter specifies how long after the watermark passes the window end to accept late events; late events update the window result and re-fire the trigger
  - Elements arriving after allowed lateness are routed to a configurable side output (late data stream)
  - Window state is properly cleaned up after window end + allowed lateness to prevent memory leaks
  pitfalls:
  - Session windows that never close (continuously arriving events) consume unbounded memory—enforce a maximum session duration
  - Sliding windows with small slide relative to window size create many overlapping windows, each consuming memory
  - Trigger firing before the window has meaningful data produces noisy partial results
  - Allowed lateness that is too generous retains window state for too long, increasing memory usage
  - Window boundary alignment (e.g., does a 1-hour window start at the top of the hour or at the first event?) must be configurable
  concepts:
  - Tumbling windows partition time into disjoint fixed-width intervals
  - Sliding windows create overlapping intervals for smoothed aggregation
  - Session windows merge event clusters separated by inactivity gaps
  - Triggers determine when window computation fires
  - Allowed lateness extends window lifetime for late-arriving events
  skills:
  - Window assigner implementation
  - Trigger design
  - Late data handling
  - Memory management for window state
  deliverables:
  - Tumbling window assigner with configurable interval
  - Sliding window assigner with configurable size and slide
  - Session window assigner with configurable gap threshold
  - Trigger implementations (watermark, count, processing time)
  - Allowed lateness configuration with late data side output
  - Window state cleanup after expiration
  estimated_hours: 20
- id: stream-processing-engine-m3
  name: Event Time & Watermarks
  description: Implement event-time processing with watermark generation and propagation for handling out-of-order events.
  acceptance_criteria:
  - Event time extractor pulls timestamps from record payload fields (configurable per source)
  - Periodic watermark generator emits watermarks at configurable intervals (e.g., every 200ms) based on the maximum observed event timestamp minus a bounded out-of-orderness tolerance
  - 'Watermarks are monotonically non-decreasing: a watermark value never decreases over time'
  - Watermark propagation across parallel operators forwards the minimum watermark from all input partitions
  - Idle source detection advances watermarks for partitions with no new data after a configurable idle timeout, preventing watermark stall
  - Window triggers fire when the watermark passes the window end timestamp
  - 'Test suite demonstrates correct ordering: events arriving before watermark are processed in-window; events arriving after watermark but within allowed lateness are late-processed; events arriving after allowed lateness are side-output'
  pitfalls:
  - Generating watermarks based on processing time instead of event time produces incorrect window assignments under load spikes
  - One stalled partition with no data prevents watermark advancement for the entire operator—idle source detection is mandatory
  - Bounded out-of-orderness tolerance too small drops legitimate late events; too large delays all window computations
  - Watermarks must be injected into the data stream between records, not as separate channels, to maintain ordering guarantees
  - Clock skew between event producers causes some partitions to consistently have earlier event times, skewing watermarks
  concepts:
  - Event time is the timestamp when the event actually occurred (embedded in the record)
  - Watermark asserts that no events with timestamp ≤ watermark will arrive in the future
  - Bounded out-of-orderness: watermark = max_event_time - tolerance
  - Watermark propagation takes the minimum across all input partitions
  - Idle source detection prevents watermark stall from inactive partitions
  skills:
  - Event time extraction
  - Watermark generation strategies
  - Watermark propagation logic
  - Idle detection
  deliverables:
  - Configurable event time extractor per source
  - Periodic watermark generator with bounded out-of-orderness
  - Monotonically non-decreasing watermark enforcement
  - Multi-partition watermark propagation (min across inputs)
  - Idle source detection with configurable timeout
  - Integration test demonstrating in-window, late, and dropped event handling
  estimated_hours: 18
- id: stream-processing-engine-m4
  name: Stateful Processing & Checkpointing
  description: Implement keyed operator state with periodic barrier-based checkpointing for fault-tolerant stateful stream processing.
  acceptance_criteria:
  - ValueState, ListState, and MapState abstractions provide per-key state accessible within keyed operators
  - 'State is isolated per key: accessing state in a keyed operator returns only the state for the current record''s key'
  - Checkpoint barriers are injected into the data stream at configurable intervals (e.g., every 10 seconds)
  - 'Barrier alignment: an operator with multiple inputs waits for barriers from ALL inputs before snapshotting state (ensuring globally consistent snapshot)'
  - State snapshot is written asynchronously to a configurable state backend (memory or filesystem) without blocking record processing
  - State recovery restores exact pre-failure operator state from the latest checkpoint; processing resumes from the checkpoint offset
  - State TTL (time-to-live) automatically clears entries that haven't been accessed within a configurable duration, preventing unbounded state growth
  - 'Checkpoint completion is verified: all operators successfully snapshot before the checkpoint is marked as complete'
  pitfalls:
  - Not clearing state causes memory leaks over time—state TTL is not optional for production workloads
  - Barrier alignment blocks processing on the faster input while waiting for the slower input's barrier—this increases latency; document the tradeoff
  - Mixing keyed and operator (non-keyed) state in the same operator complicates checkpointing—separate them clearly
  - Checkpoint failure from slow I/O or large state causes checkpoint timeout; monitor checkpoint duration and size
  - 'State backend choice dramatically affects performance: memory is fast but limited; filesystem is larger but slower'
  concepts:
  - Keyed state is scoped to individual keys within a keyed stream
  - Checkpoint barriers are special records injected into the stream to trigger state snapshots
  - Barrier alignment ensures globally consistent snapshots across parallel operators (Chandy-Lamport variant)
  - Asynchronous checkpointing writes snapshots without blocking the data processing path
  - State TTL prevents unbounded state growth for long-running streaming jobs
  skills:
  - Keyed state implementation
  - Barrier-based checkpointing
  - Asynchronous snapshot writing
  - State recovery
  - State TTL management
  deliverables:
  - ValueState, ListState, MapState abstractions for keyed operators
  - State backend interface with memory and filesystem implementations
  - Checkpoint barrier injection at configurable intervals
  - Barrier alignment logic for multi-input operators
  - Asynchronous state snapshot writing
  - State recovery from latest checkpoint with offset replay
  - State TTL configuration and automatic cleanup
  - Checkpoint completion tracking and timeout handling
  estimated_hours: 22
- id: stream-processing-engine-m5
  name: Exactly-Once Semantics
  description: Guarantee exactly-once processing from source through sink using checkpointing, idempotent/transactional sinks, and source offset management.
  acceptance_criteria:
  - Source connectors commit their read offset (e.g., Kafka offset) as part of the checkpoint; on recovery, they resume from the checkpointed offset
  - Transactional sink connector pre-commits output during processing and finalizes (commits) only when the checkpoint completes successfully
  - If a checkpoint fails, the transactional sink rolls back the pre-committed output
  - Two-phase commit protocol coordinates commit/abort across multiple sink operator instances
  - After a simulated crash and recovery, no duplicate output records are observed in the sink (verified by end-to-end test counting output records)
  - 'At-least-once mode is available as a lower-overhead alternative: disabling barrier alignment allows faster processing with potential duplicates'
  - 'End-to-end exactly-once test: inject N records from source, crash after partial processing, recover, and verify exactly N records in sink'
  pitfalls:
  - Exactly-once is only possible if BOTH source (replayable) and sink (transactional or idempotent) support it—document sink requirements
  - Two-phase commit adds latency equal to the checkpoint interval before output becomes visible
  - Sink transactions timing out during long checkpoint intervals cause duplicate output—set transaction timeout > checkpoint interval
  - Barrier alignment (required for exactly-once) increases latency compared to unaligned mode—this is an inherent tradeoff
  - Idempotent sinks (using unique keys) are simpler than transactional sinks but require the sink to support upsert/dedup semantics
  concepts:
  - Exactly-once = consistent checkpointing + replayable sources + transactional/idempotent sinks
  - Two-phase commit: pre-commit on barrier, finalize on checkpoint completion, rollback on failure
  - Replayable sources can re-read from a specific offset (e.g., Kafka consumer group)
  - Idempotent sinks produce the same output on replay (e.g., upsert by unique key)
  - Barrier alignment ensures no records are processed twice across checkpoint boundaries
  skills:
  - Two-phase commit protocol
  - Transactional sink design
  - Source offset management
  - End-to-end exactly-once verification
  deliverables:
  - Source connector with checkpointable offset management
  - Two-phase commit sink connector (pre-commit, commit, rollback)
  - Checkpoint completion callback notifying sources and sinks
  - Transaction coordinator managing distributed commit across sinks
  - At-least-once mode as lower-overhead alternative
  - End-to-end exactly-once test with crash recovery and output verification
  estimated_hours: 21
domain: data-storage
