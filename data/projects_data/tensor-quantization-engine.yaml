id: tensor-quantization-engine
name: Tensor Quantization Engine
description: Implement model quantization techniques for efficient LLM inference including PTQ and GPTQ.
difficulty: advanced
domain: ai-ml
estimated_hours: 50-70
essence: Reducing model precision from FP32 to INT8 or INT4 to decrease memory and accelerate inference.
why_important: Quantization is essential for production ML deployment on edge devices and reducing cloud costs.
learning_outcomes:
- Implement symmetric and asymmetric quantization
- Build calibration pipeline
- Implement per-tensor and per-channel quantization
- Implement basic GPTQ weight quantization
- Measure accuracy vs compression tradeoff
skills:
- Model Quantization
- Calibration Techniques
- GPTQ Implementation
- Edge Deployment
tags:
- advanced
- quantization
- optimization
- inference
- edge-ml
languages:
  recommended:
  - Python
  also_possible:
  - C++
prerequisites:
- type: skill
  name: PyTorch proficiency
- type: skill
  name: Neural network fundamentals
resources:
- name: Quantization Deep Dive Paper
  type: paper
  url: https://arxiv.org/abs/2106.08295
- name: GPTQ Paper
  type: paper
  url: https://arxiv.org/abs/2210.17323
- name: PyTorch Quantization Docs
  type: documentation
  url: https://pytorch.org/docs/stable/quantization.html
milestones:
- id: quant-m1
  name: Quantization Fundamentals
  description: Implement basic quantization and dequantization operations.
  estimated_hours: 8-10
  concepts:
  - Affine transform quantization
  - Symmetric vs asymmetric
  - Scale and zero-point
  skills:
  - Quantization math
  - Tensor operations
  acceptance_criteria:
  - Implement symmetric quantization function
  - Implement asymmetric quantization with zero-point
  - Implement dequantization function
  - Quantize FP32 tensor to INT8 and back
  - Quantization error MSE below 0.01
  - Support configurable bit widths
  pitfalls:
  - Integer overflow in quantized ops
  - Scale too large or small causes errors
  deliverables:
  - Quantize and dequantize functions
  - Scale calculators
  - Error measurement utilities
- id: quant-m2
  name: Per-Tensor vs Per-Channel
  description: Implement different quantization granularities and compare accuracy.
  estimated_hours: 10-12
  concepts:
  - Per-tensor quantization
  - Per-channel quantization
  - Granularity tradeoffs
  skills:
  - Granular quantization
  - Accuracy measurement
  acceptance_criteria:
  - Implement per-tensor quantization for weights
  - Implement per-channel quantization per output dimension
  - Apply to sample model
  - Per-channel has 30 percent lower MSE than per-tensor
  - Report weight statistics
  - Document memory footprint comparison
  pitfalls:
  - Per-channel needs more scale storage
  - Channel dimension convention matters
  deliverables:
  - Both quantization modes
  - Comparison benchmark
  - Memory analysis
- id: quant-m3
  name: Calibration Pipeline
  description: Build calibration to determine optimal scales for activations.
  estimated_hours: 12-16
  concepts:
  - Activation distributions
  - Min/max calibration
  - Percentile calibration
  skills:
  - Calibration pipeline
  - Histogram analysis
  acceptance_criteria:
  - Run calibration on representative dataset
  - Collect activation statistics per layer
  - Implement min/max calibration
  - Implement percentile calibration
  - Percentile has 10 percent lower MSE
  - Store calibration parameters
  - Accuracy drop below 2 percent
  pitfalls:
  - Calibration data must match production
  - Outliers inflate scale
  deliverables:
  - Calibration pipeline
  - Both calibration methods
  - Parameter storage
- id: quant-m4
  name: Post-Training Static Quantization
  description: Implement full PTQ combining weight and activation quantization.
  estimated_hours: 12-16
  concepts:
  - Static quantization
  - Quantized inference
  - Layer fusion
  skills:
  - Full PTQ pipeline
  - Quantized model construction
  acceptance_criteria:
  - Quantize small transformer or CNN end-to-end
  - Apply per-channel weights and per-tensor activations
  - Implement quantized Linear layer
  - Model size reduction 4x
  - Accuracy drop below 3 percent for images
  - CPU inference speedup 2x
  - Export quantized model state
  pitfalls:
  - Wrong layer order breaks residuals
  - Some ops should stay FP32
  deliverables:
  - Full PTQ pipeline
  - Quantized model
  - Benchmarks
- id: quant-m5
  name: GPTQ Weight Quantization
  description: Implement GPTQ algorithm for LLM weight quantization.
  estimated_hours: 12-16
  concepts:
  - GPTQ algorithm
  - Hessian approximation
  - Layer-wise quantization
  skills:
  - GPTQ implementation
  - Layer optimization
  acceptance_criteria:
  - Implement GPTQ for linear layers
  - Use Hessian diagonal approximation
  - Quantize weights column by column
  - Support INT4 with group size 128
  - Apply to small language model
  - Perplexity increase below 1.0
  - Model size reduction 4x
  - GPTQ better than naive by 30 percent
  pitfalls:
  - Sensitive to calibration quality
  - Group size affects tradeoff
  deliverables:
  - GPTQ implementation
  - INT4 quantized model
  - Perplexity comparison
