id: tokenizer
name: Tokenizer / Lexer
description: Character-level lexer for a simple C-like language using finite state machine scanning with maximal munch.
difficulty: beginner
estimated_hours: 8-15
essence: 'Character-by-character finite-state-machine traversal to recognize lexemes via maximal munch and single-character lookahead, transforming linear source text into categorized token sequences while handling whitespace, comments, escape sequences, and lexical error boundaries.

  '
why_important: 'Building a lexer demystifies how programming languages work at the lowest level and provides the foundation for understanding parsers, compilers, and interpreters—skills directly applicable to developer tools, DSLs, configuration languages, and language server implementations.

  '
learning_outcomes:
- Design a token type system for a simple programming language
- Implement character-by-character scanning with peek and advance
- Apply the maximal munch principle to disambiguate multi-character tokens
- Handle string literals with escape sequences
- Distinguish keywords from identifiers using a lookup table
- Implement comment filtering (single-line and multi-line)
- Track source position (line, column) for error reporting
- Test tokenizer against complete multi-line input programs
skills:
- Finite State Machines
- Pattern Matching (Maximal Munch)
- String Processing
- Error Handling
- Character Encoding
- Compiler Theory Basics
tags:
- beginner-friendly
- compilers
- go
- javascript
- parsing
- python
architecture_doc: architecture-docs/tokenizer/index.md
languages:
  recommended:
  - Python
  - JavaScript
  - Go
  also_possible:
  - Rust
  - C
resources:
- name: Crafting Interpreters - Scanning
  url: https://craftinginterpreters.com/scanning.html
  type: book
- name: Let's Build a Compiler
  url: https://compilers.iecc.com/crenshaw/
  type: tutorial
prerequisites:
- type: skill
  name: Basic string manipulation
- type: skill
  name: Conditional logic and loops
milestones:
- id: tokenizer-m1
  name: Token Types & Scanner Foundation
  description: 'Define the target language''s token types, the Token data structure, and the core scanner infrastructure (character consumption, peek, position tracking). Implement scanning for single-character tokens and whitespace.

    '
  target_language_note: 'The tokenizer targets a simple C-like language with: integer and float literals, string literals, identifiers, keywords (if, else, while, return, true, false, null), arithmetic operators (+, -, *, /), comparison operators (==, !=, <, >, <=, >=), assignment (=), grouping ({, }, (, ), [, ]), delimiters (, ;), and comments (// single-line, /* */ multi-line). Identifiers are ASCII-only.

    '
  acceptance_criteria:
  - 'Token type enumeration defines all categories: Number, String, Identifier, Keyword, Operator, Punctuation, EOF, Error'
  - 'Token struct stores: type, lexeme (raw text), line number, and column number'
  - Scanner has advance() consuming and returning current character, peek() inspecting next character without consuming
  - Scanner tracks current line (incremented on newline) and column (reset on newline)
  - Single-character tokens (+, -, *, /, (, ), {, }, [, ], ;, ,) are recognized and emitted
  - Whitespace (space, tab, \r, \n) is consumed without emitting tokens
  - EOF token is emitted when input is exhausted
  - Invalid/unrecognized characters produce an Error token with position
  pitfalls:
  - Forgetting EOF token causes downstream parser to crash
  - Column number not resetting on newline causes incorrect error positions
  - Tab characters advancing column by 1 instead of configurable tab width
  - Windows line endings (\r\n) counted as two newlines
  concepts:
  - Token type design
  - Scanner infrastructure
  - Position tracking
  skills:
  - Enum and data structure design
  - Character stream processing
  - Source position tracking
  deliverables:
  - Token type enumeration
  - Token struct with type, lexeme, line, column
  - Scanner with advance(), peek(), is_at_end() methods
  - Single-character token recognition
  - Whitespace consumption
  - EOF and Error token emission
  estimated_hours: 2-3
- id: tokenizer-m2
  name: Multi-Character Tokens & Maximal Munch
  description: 'Implement scanning for multi-character tokens: two-character operators, number literals, identifiers, and keywords. Apply maximal munch to always prefer the longest matching token.

    '
  acceptance_criteria:
  - 'Two-character operators: ==, !=, <=, >= are recognized as single tokens (not two separate tokens)'
  - 'Maximal munch: "''=='' is emitted as Equals-Equals, never as Assign + Assign"'
  - Single '=' is emitted as Assign only when not followed by '='
  - '''<'' is emitted as LessThan; ''<='' is emitted as LessEqual (maximal munch applied)'
  - 'Integer literals: "sequences of digits (e.g., ''42'', ''0'') are scanned as Number tokens"'
  - 'Float literals: "digit sequences with one decimal point (e.g., ''3.14'') are scanned as Number tokens"'
  - 'Identifiers: sequences starting with letter or underscore, followed by alphanumeric or underscore'
  - Keywords (if, else, while, return, true, false, null) are identified by lookup table after scanning identifier
  - Keywords are emitted as Keyword tokens, not Identifier tokens
  - 'Input ''>=='' is tokenized as GreaterEqual + Assign (maximal munch: "''>='' then ''='')"'
  pitfalls:
  - Not peeking ahead causes '==' to be scanned as '=' + '='
  - 'Float with trailing dot (''3.'') or leading dot (''.5'')—decide: accept or reject, and document'
  - Keyword 'if' inside identifier 'iffy' must NOT match—keyword check only on exact match
  - 'Number immediately followed by identifier (''42abc'') must be handled: scan as Number then Identifier, or error'
  concepts:
  - Maximal munch principle
  - Lookahead (peek)
  - Keyword tables
  - Number scanning
  skills:
  - Single-character lookahead implementation
  - Hash table keyword lookup
  - Number literal scanning
  - Identifier scanning
  deliverables:
  - Two-character operator scanning with lookahead
  - Number literal scanner (integer and float)
  - Identifier scanner with keyword table lookup
  - Maximal munch applied to all ambiguous token pairs
  estimated_hours: 3-4
- id: tokenizer-m3
  name: Strings & Comments
  description: 'Handle string literals with escape sequences and both single-line and multi-line comments.

    '
  acceptance_criteria:
  - String literals between double quotes are scanned as String tokens
  - Escape sequences \n, \t, \r, \\\", \\\\ are interpreted correctly within strings
  - Unterminated string (no closing quote before EOF or newline) produces Error token with position of opening quote
  - Single-line comments (//) skip all characters until end of line
  - Multi-line comments (/* */) skip all characters between delimiters including newlines
  - Unterminated multi-line comment (no closing */) produces Error token with position of opening /*
  - 'Multi-line comments do NOT nest: /* /* */ ends at the first */ encountered'
  - 'Comments inside strings are NOT treated as comments: \"hello // world\" is a string'
  - Line numbers are correctly updated inside multi-line comments and strings
  pitfalls:
  - String scanning consuming the closing quote as part of the next token
  - 'Escape backslash at end of string: "''\"hello\\\"'' is unterminated, not a valid string"'
  - 'Nested /* */ comments: deciding to not support nesting but accidentally doing so with a counter'
  - Comment starting with '/' but followed by neither '/' nor '*'—emit '/' as Division operator
  concepts:
  - String literal scanning
  - Escape sequence handling
  - Comment filtering
  - State machine for quote/comment matching
  skills:
  - Multi-character state tracking
  - Escape sequence processing
  - Comment filtering without token emission
  - Error detection for unterminated constructs
  deliverables:
  - String literal scanner with escape sequence handling
  - Single-line comment scanner (// to end of line)
  - Multi-line comment scanner (/* to */)
  - Error tokens for unterminated strings and comments
  estimated_hours: 2-3
- id: tokenizer-m4
  name: Integration Testing & Error Recovery
  description: 'Test the tokenizer against complete multi-line programs, implement error recovery to continue scanning after errors, and verify full token stream output.

    '
  acceptance_criteria:
  - Complete multi-line test program is tokenized into correct token stream (verified token-by-token)
  - 'Error recovery: after encountering an invalid character, scanning continues from the next character'
  - Multiple errors in a single input are all reported (not just the first)
  - 'Token stream for ''if (x >= 42) { return true; }'' produces exactly: Keyword(if), LParen, Ident(x), GreaterEqual, Number(42), RParen, LBrace, Keyword(return), Keyword(true), Semicolon, RBrace, EOF'
  - 'Edge cases tested: empty input (produces only EOF), single character input, maximum-length identifiers'
  - 'Performance: tokenizing a 10,000-line input completes in under 1 second'
  - All token positions (line, column) are accurate for a multi-line test file
  pitfalls:
  - Error recovery consuming too many characters and skipping valid tokens
  - Not testing with real multi-line input—only testing individual token types in isolation
  - 'Position tracking drift: errors in line/column accumulate over long inputs'
  - Off-by-one in column tracking after tab characters or multi-byte sequences
  concepts:
  - Error recovery
  - Integration testing
  - Token stream validation
  skills:
  - Test case design for lexers
  - Error recovery strategies
  - End-to-end validation
  - Performance awareness
  deliverables:
  - Integration test with complete multi-line program and expected token stream
  - Error recovery: continue scanning after invalid characters
  - Multi-error reporting in single input
  - Edge case test suite: empty input, single char, long identifiers
  - Position accuracy verification across multi-line input
  estimated_hours: 2-3
domain: compilers
