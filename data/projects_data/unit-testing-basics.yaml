id: unit-testing-basics
name: Unit Testing Fundamentals
description: Learn to write, organize, and run automated unit tests using a modern test framework
difficulty: beginner
estimated_hours: "8-14"
essence: >
  Isolated verification of individual functions and methods through programmatic
  assertions that validate expected behavior against actual output, combined with
  test doubles (mocks, stubs) that replace external dependencies to ensure
  deterministic, repeatable validation independent of databases, networks, or
  filesystem state.
why_important: >
  Unit testing is foundational to professional software development — it catches
  bugs early, enables confident refactoring, and is expected in virtually every
  modern development workflow from startups to enterprise teams.
learning_outcomes:
  - Write isolated test cases using assertion frameworks (pytest, Jest, JUnit)
  - Understand how test runners discover and execute tests in a project structure
  - Design test fixtures and setup/teardown patterns to manage test state
  - Test stateful objects and classes with methods that modify internal state
  - Implement mocking and stubbing to isolate units from external dependencies
  - Structure test suites with clear naming and logical organization
  - Apply the Arrange-Act-Assert pattern for readable test code
  - Measure test coverage and identify untested code paths
  - Debug failing tests by analyzing assertion messages and stack traces
skills:
  - Test Automation
  - Assertion Libraries
  - Test Fixtures
  - Mocking and Stubbing
  - Code Coverage Analysis
  - Test Organization
  - Debugging Test Failures
  - Test Discovery
tags:
  - assertions
  - beginner-friendly
  - fixtures
  - java
  - javascript
  - mocking
  - python
  - test-runner
architecture_doc: architecture-docs/unit-testing-basics/index.md
languages:
  recommended:
    - Python
    - JavaScript
    - Java
  also_possible:
    - Go
    - Rust
resources:
  - name: pytest Documentation""
    url: "https://docs.pytest.org/"
    type: documentation
  - name: Jest Documentation""
    url: "https://jestjs.io/docs/getting-started"
    type: documentation
  - name: JUnit 5 User Guide""
    url: "https://junit.org/junit5/docs/current/user-guide/"
    type: documentation
prerequisites:
  - type: skill
    name: Basic programming (functions, conditionals, loops)
  - type: skill
    name: Classes and objects
milestones:
  - id: unit-testing-basics-m1
    name: First Tests and Test Discovery
    description: >
      Set up a test framework, understand how test discovery works, and
      write your first unit tests for pure functions.
    acceptance_criteria:
      - "Test framework is installed and configured in the project (e.g., pytest installed, jest in package.json, JUnit in build config)"
      - "Test discovery rules are understood and documented: which files are scanned (e.g., test_*.py for pytest, *.test.js for Jest) and which functions/methods are recognized as tests (e.g., functions starting with test_ for pytest, test() blocks for Jest)"
      - "At least 3 test functions are written for a pure function, verifying expected return values for normal inputs"
      - "Edge cases are tested: empty input, boundary values (0, negative numbers, maximum values), and null/None/undefined arguments"
      - "Exception/error cases are tested: verify that invalid inputs raise the expected exception type (e.g., pytest.raises, expect().toThrow(), assertThrows)"
      - "Tests are run from the command line and output shows pass count, failure count, and failure details including assertion messages and line numbers"
      - "At least one test is intentionally written to FAIL, and the failure output is examined to understand how the framework reports assertion mismatches"
      - "Arrange-Act-Assert pattern is followed in each test: setup input, call the function, assert the result"
    pitfalls:
      - "Testing implementation details (HOW a function works) rather than behavior (WHAT it returns) makes tests fragile and break on refactoring"
      - "Not testing edge cases leads to false confidence; the happy path often works but boundaries reveal bugs"
      - "Test file not matching the discovery pattern (wrong name, wrong directory) causes tests to silently not run; verify test count matches expectations"
      - "Fragile assertions that depend on exact string formatting, dictionary ordering, or floating-point equality break unexpectedly"
      - "Floating-point comparison requires approximate assertions (pytest.approx, toBeCloseTo) rather than exact equality"
    concepts:
      - Unit testing purpose and value
      - Test discovery rules
      - Assertion types (equality, truthiness, exception)
      - Arrange-Act-Assert pattern
      - Edge case identification
    skills:
      - Writing test functions
      - Using assertion methods/keywords
      - Running test suites from command line
      - Reading test failure output
      - Identifying edge cases
    deliverables:
      - Test framework installation and project configuration
      - Test file(s) with correct naming for discovery
      - Test functions for pure functions covering normal, edge, and error cases
      - Exception testing using framework-appropriate assertion
      - Documented test discovery rules for chosen framework
    estimated_hours: "2-3"

  - id: unit-testing-basics-m2
    name: Test Organization and Fixtures
    description: >
      Organize tests into logical groups, use setup/teardown fixtures,
      and write parameterized tests for data-driven testing.
    acceptance_criteria:
      - "Related tests are grouped in test classes or describe blocks for logical organization"
      - "Setup fixtures initialize shared test data before each test (e.g., pytest fixtures, beforeEach in Jest, @BeforeEach in JUnit)"
      - "Teardown logic cleans up resources after each test (e.g., closing files, resetting state)"
      - "Each test is independent: running tests in any order or individually produces the same result (no test interdependencies)"
      - "Parameterized tests run the same assertion logic across at least 5 different input-output pairs using the framework's parameterization feature (e.g., @pytest.mark.parametrize, test.each in Jest, @ParameterizedTest in JUnit)"
      - "Test names are descriptive: each test name documents the scenario and expected behavior (e.g., test_divide_by_zero_raises_error, 'should return empty array for null input')"
      - "Suite-level setup (run once before all tests in a group) is implemented for expensive initialization (e.g., database connection, file loading)"
    pitfalls:
      - "Fixtures with side effects (modifying shared mutable state) cause test interdependencies where test order affects results"
      - "Shared mutable state between tests causes mysterious failures when tests run in parallel or different order"
      - "Over-complicated fixtures hide test intent; prefer simple, explicit setup within each test when possible"
      - "Not cleaning up in teardown causes resource leaks (open files, database connections) that affect subsequent tests"
      - "Parameterized tests with too many parameters become hard to debug when one case fails; include descriptive IDs for each parameter set"
    concepts:
      - Test fixtures and lifecycle hooks
      - Test isolation and independence
      - Parameterized/data-driven testing
      - Test naming conventions
      - Setup/teardown lifecycle
    skills:
      - Creating and using fixtures
      - Setup and teardown methods
      - Parameterizing test cases
      - Managing test data
      - Ensuring test isolation
    deliverables:
      - Test classes/modules grouping related tests
      - Setup fixtures providing test data and teardown cleaning up
      - Parameterized tests covering multiple input-output pairs
      - Descriptive test names documenting scenario and expected behavior
      - Demonstration of test isolation (tests pass in any order)
    estimated_hours: "2-3"

  - id: unit-testing-basics-m3
    name: Testing Stateful Objects and Dependencies
    description: >
      Test classes and objects with internal state, understand why mocking
      is needed, and learn dependency injection basics before introducing
      mock objects.
    acceptance_criteria:
      - "Tests are written for a class with internal state (e.g., a shopping cart, bank account, or counter) verifying that methods correctly modify and query internal state"
      - "State transitions are tested: verify that a sequence of method calls produces the expected final state (e.g., add item → add item → remove item → check total)"
      - "A function/class with an external dependency (e.g., reads from a file, calls an API, queries a database) is identified as needing isolation for unit testing"
      - "Dependency injection is used to replace the external dependency with a simple fake/stub implementation in tests (e.g., pass a fake file reader or in-memory database to the constructor)"
      - "Tests using dependency injection run without any real external system (no network calls, no file I/O, no database)"
      - "The difference between testing WITH the real dependency (integration test) and WITHOUT it (unit test using injection) is understood and documented"
    pitfalls:
      - "Testing only the final state misses intermediate state bugs; test state after each significant operation"
      - "Forgetting to reset state between tests causes one test's modifications to affect the next; use setup/teardown or create fresh instances per test"
      - "Dependency injection requires designing code with testability in mind; tightly coupled code that creates its own dependencies is hard to test"
      - "Writing a 'fake' implementation that doesn't match the real interface causes tests to pass but production code to fail"
      - "Testing internal private state (reaching into private fields) couples tests to implementation; prefer testing through public methods"
    concepts:
      - Testing stateful objects
      - State transition verification
      - External dependency identification
      - Dependency injection for testability
      - Unit test vs integration test distinction
    skills:
      - Testing classes with mutable state
      - Designing code for testability
      - Creating simple fake/stub implementations
      - Dependency injection patterns
    deliverables:
      - Tests for a stateful class verifying state transitions through method calls
      - Identification of external dependencies in a code module
      - Fake/stub implementation of an external dependency for testing
      - Tests using dependency injection running without external systems
      - Documentation explaining unit test vs integration test for the tested code
    estimated_hours: "2-3"

  - id: unit-testing-basics-m4
    name: Mocking, Coverage, and Best Practices
    description: >
      Use the framework's mocking library for advanced isolation, measure
      test coverage, and apply testing best practices.
    acceptance_criteria:
      - "Mock objects replace external dependencies using the framework's mocking library (e.g., unittest.mock.patch in Python, jest.mock in Jest, Mockito in Java)"
      - "Mock return values are configured to control the dependency's behavior during tests (e.g., mock.return_value, mockResolvedValue)"
      - "Mock interactions are verified: assert that the mock was called with expected arguments and the expected number of times"
      - "Patching is applied at the correct import location (in Python: patch where the name is LOOKED UP in the module under test, not where it is DEFINED)"
      - "Code coverage is measured using the framework's coverage tool (e.g., pytest-cov, jest --coverage, JaCoCo) and a coverage report is generated"
      - "Coverage report is analyzed to identify untested code paths; at least one new test is written to cover a previously untested branch"
      - "Overall line coverage exceeds 80% for the tested module"
      - "Tests follow the principle: mock external dependencies (I/O, network, time) but do NOT mock the code under test itself"
    pitfalls:
      - "Mocking too much creates tests that verify the mock configuration rather than actual behavior; mock only external boundaries"
      - "In Python, patching the wrong location (where the function is defined vs where it's imported) is the #1 mocking bug; always patch where the name is used"
      - "Not verifying mock interactions means the mock could be completely ignored and the test still passes"
      - "100% coverage does not mean 100% correctness; coverage measures which lines execute, not whether assertions are meaningful"
      - "Over-reliance on mocks creates brittle tests that break when internal implementation changes even though behavior is unchanged"
      - "Mock return values that don't match the real dependency's return type cause tests to pass with incorrect assumptions"
    concepts:
      - Mock objects and test doubles (mock, stub, spy, fake)
      - Patching and import-level mocking
      - Code coverage measurement and interpretation
      - Testing best practices and anti-patterns
    skills:
      - Creating and configuring mock objects
      - Patching external dependencies at correct import level
      - Verifying mock interactions (calls, arguments, count)
      - Measuring and interpreting code coverage
      - Distinguishing meaningful coverage from line counting
    deliverables:
      - Tests using framework mocking library to isolate external dependencies
      - Mock configuration with controlled return values
      - Mock verification asserting correct call arguments and counts
      - Coverage report generated with framework coverage tool
      - New test(s) written to cover previously untested code paths
      - Summary of testing best practices and anti-patterns learned
    estimated_hours: "2-4"