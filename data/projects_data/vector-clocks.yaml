id: vector-clocks
name: Vector Clocks
description: Implement vector clocks for causal ordering, conflict detection in replicated data, dominated-version pruning, and integration into a distributed key-value store.
difficulty: intermediate
estimated_hours: 12-18
essence: Logical timestamp data structure maintaining per-node counters to establish partial ordering between distributed events, enabling detection of concurrent (conflicting) updates without physical clock synchronization, with dominated-version pruning for bounded memory growth.
why_important: Vector clocks are fundamental to eventually-consistent distributed databases (DynamoDB, Cassandra, Riak). Building them teaches you causal ordering, conflict detection, and the critical distinction between concurrent and causally-ordered events — concepts essential for designing systems that operate without strong consensus.
learning_outcomes:
- Implement vector clock data structures with increment, merge, and comparison operations
- Formalize the happens-before partial order and concurrent event detection
- Build conflict detection for replicated key-value data using vector clock comparison
- Implement dominated-version pruning to prevent unbounded version growth
- Integrate vector clocks into a multi-node key-value store with replication
- Design application-level merge functions for concurrent versions (semantic merge)
- Understand why LWW defeats the purpose of vector clocks
- Analyze trade-offs between vector clocks and dotted version vectors
skills:
- Causality Tracking
- Logical Timestamps
- Conflict Detection
- Partial Ordering
- Dominated Version Pruning
- Replicated Data Structures
- Application-Level Merge
- Eventually Consistent Design
tags:
- causality
- distributed
- go
- intermediate
- java
- ordering
- python
- timestamps
- version-vectors
architecture_doc: architecture-docs/vector-clocks/index.md
languages:
  recommended:
  - Python
  - Go
  - Java
  also_possible:
  - Rust
  - JavaScript
resources:
- name: Vector Clocks - Wikipedia
  url: https://en.wikipedia.org/wiki/Vector_clock
  type: article
- name: Understanding Vector Clocks
  url: https://sookocheff.com/post/time/vector-clocks/
  type: article
- name: 'Princeton COS 418: Vector Clocks'
  url: https://www.cs.princeton.edu/courses/archive/fall18/cos418/docs/L4-vc.pdf
  type: paper
- name: Dotted Version Vectors (DVV) Paper
  url: https://gsd.di.uminho.pt/members/vff/dotted-version-vectors-2012.pdf
  type: paper
prerequisites:
- type: skill
  name: Basic distributed systems concepts
- type: skill
  name: Hash maps and linked lists
- type: skill
  name: Understanding of causality and partial ordering
milestones:
- id: vector-clocks-m1
  name: Vector Clock Data Structure and Operations
  description: 'Implement the vector clock data structure with three core operations: increment (local event), merge (on message receive), and comparison (happens-before, happens-after, or concurrent). Define the formal comparison algorithm.'
  acceptance_criteria:
  - 'Initialize a vector clock for a node as a map/dictionary of {node_id: counter}, defaulting unseen nodes to 0'
  - increment(node_id) increments only the local node's counter by 1
  - merge(other_clock) computes element-wise maximum of all components from both clocks, then increments the local node's counter by 1
  - compare(a, b) returns HAPPENS_BEFORE if all components of a ≤ b AND at least one component of a < b
  - compare(a, b) returns HAPPENS_AFTER if all components of b ≤ a AND at least one component of b < a
  - compare(a, b) returns CONCURRENT if neither a ≤ b nor b ≤ a (at least one component of a > corresponding in b, and vice versa)
  - 'Deep copy of clock on send: the sent message carries a copy, not a reference to the sender''s mutable clock'
  - Unit tests verify all comparison outcomes with at least 10 scenarios including edge cases (identical clocks, single-node, unknown nodes)
  pitfalls:
  - 'Not incrementing after merge on receive: the standard rule is merge THEN increment. Forgetting the increment loses the ''receive'' event from the causal history.'
  - 'Shallow copy on send: if the sent clock is a reference, subsequent increments by the sender corrupt the message in transit'
  - 'Comparison logic error: the most common bug is returning HAPPENS_BEFORE when clocks are equal (equal clocks are CONCURRENT — they represent the same event, not a causal relationship). Actually, equal clocks can be treated as EQUAL/IDENTICAL, a fourth category.'
  - 'Not handling clocks with different key sets: if clock A has node X but clock B doesn''t, B''s component for X is implicitly 0'
  concepts:
  - Vector clock operations (increment, merge, compare)
  - 'Formal happens-before definition: VC(a) < VC(b) iff ∀i: a[i] ≤ b[i] ∧ ∃j: a[j] < b[j]'
  - Deep copy semantics for message passing
  - Implicit zero for missing components
  skills:
  - Map/dictionary operations with default values
  - Deep copy implementation
  - Partial order comparison algorithms
  - Comprehensive unit testing
  deliverables:
  - VectorClock class/struct with increment, merge, and compare methods
  - Formal comparison function returning HAPPENS_BEFORE, HAPPENS_AFTER, CONCURRENT, or EQUAL
  - Deep copy method for safe message passing
  - Comprehensive test suite with 10+ comparison scenarios
  estimated_hours: 3-4
- id: vector-clocks-m2
  name: Versioned Key-Value Store with Conflict Detection
  description: Build a single-node key-value store where each value is stored alongside its vector clock. On write, compare the incoming clock with the stored clock to detect conflicts. On read, return all concurrent versions (siblings) for the client to resolve.
  acceptance_criteria:
  - put(key, value, context_clock) stores the value with its vector clock. If context_clock is HAPPENS_AFTER stored clock, the old version is replaced.
  - If context_clock is CONCURRENT with the stored clock, both versions are kept as siblings (multi-value register)
  - If context_clock is HAPPENS_BEFORE the stored clock, the write is rejected as stale (409 Conflict or equivalent)
  - get(key) returns a list of all current siblings with their vector clocks. A single entry means no conflict.
  - 'Client-side merge: get returns siblings, client merges them, put writes the merged result with a new clock that dominates all sibling clocks'
  - After a successful merge-put, subsequent get returns only the single merged version
  - LWW (last-writer-wins using wall-clock timestamp) is available as a degenerate conflict resolution option but documented as defeating the purpose of vector clocks
  pitfalls:
  - 'Growing sibling lists without bounds: if many concurrent writes happen without reads/merges, siblings accumulate. Set a max sibling count with a warning log.'
  - 'Using LWW by default: this eliminates all benefits of vector clocks. It should be opt-in and documented as lossy.'
  - 'Lost updates: if a client reads version A, another client reads version A, both write updates without knowing about each other — the second write should create a sibling, not silently overwrite'
  - 'Comparison direction error: comparing the incoming clock against the stored clock in the wrong direction causes incorrect conflict/overwrite decisions'
  concepts:
  - Multi-value register (sibling values)
  - Context clock for read-modify-write
  - Semantic merge by application
  - LWW as a degenerate strategy
  skills:
  - Versioned data storage
  - Sibling management and bounded lists
  - Conflict detection via clock comparison
  - Read-modify-write patterns
  deliverables:
  - Versioned key-value store with put(key, value, context) and get(key) → [(value, clock)]
  - Conflict detection on put using vector clock comparison
  - Sibling storage for concurrent versions
  - Merge-put operation collapsing siblings after client-side resolution
  estimated_hours: 3-4
- id: vector-clocks-m3
  name: Dominated-Version Pruning
  description: Implement safe pruning strategies to prevent unbounded growth of version history. Use dominated-version pruning (not naive age-based pruning, which breaks causality). Optionally explore Dotted Version Vectors.
  acceptance_criteria:
  - 'Dominated version detection: version A is dominated if there exists another version B where compare(A, B) = HAPPENS_BEFORE. Dominated versions are safe to prune.'
  - On every put, after adding the new version, scan siblings and remove any version dominated by the new version
  - 'Pruning never removes concurrent versions: only versions causally superseded by another stored version are pruned'
  - 'Configurable max sibling count (default 10): if exceeded, a warning is logged. Optionally, the oldest sibling by insertion time is dropped with an explicit data-loss warning.'
  - 'Garbage collection for departed nodes: clock entries for nodes that have permanently left the cluster can be folded into a ''base'' counter after operator confirmation'
  - 'Metrics endpoint reports: current sibling count per key, total pruning events, forced-drop events (max sibling exceeded)'
  pitfalls:
  - 'Naive age-based pruning breaks causality: deleting ''the oldest version'' may remove a version that is concurrent with (not dominated by) newer versions, losing conflict information'
  - 'Aggressive pruning losing conflict info: if you prune too much, you can''t detect conflicts that the application needs to resolve'
  - 'Clock entries for departed nodes growing the clock forever: without garbage collection, the clock size grows with the total number of nodes that ever existed'
  - 'Not running pruning on every write: if pruning only runs periodically, siblings accumulate between runs'
  concepts:
  - Dominated version detection (causally superseded)
  - Safe pruning preserving concurrent versions
  - Node departure garbage collection
  - Dotted Version Vectors (advanced alternative)
  skills:
  - Dominance relationship algorithms
  - Memory management for versioned data
  - Garbage collection strategies
  - Metrics and observability
  deliverables:
  - Dominated-version detector and pruner running on each put
  - Max sibling safety limit with warning on overflow
  - Node departure GC for compacting clock entries
  - Metrics for sibling counts and pruning activity
  estimated_hours: 2-3
- id: vector-clocks-m4
  name: Multi-Node Replication
  description: Extend the key-value store to multiple nodes with full replication (all nodes store all keys). Writes to any node are propagated to all others. Read-repair resolves detected conflicts.
  acceptance_criteria:
  - 3-node cluster where every write is asynchronously replicated to all other nodes
  - Each node increments its own component in the vector clock when accepting a write
  - When a replicated write arrives, the receiving node compares clocks and either replaces (dominated), adds sibling (concurrent), or rejects (stale)
  - 'get(key) from any node triggers read-repair: if the responding node has fewer siblings than another node, it fetches the missing siblings'
  - After network partition heals, nodes exchange keys and reconcile using vector clock comparison (anti-entropy)
  - 'Configurable conflict resolution per key: application-merge callback or LWW (opt-in)'
  - 'Integration test: two nodes write different values for the same key during a partition. After heal, both nodes show both values as siblings until a client merges them.'
  pitfalls:
  - 'Network partition causing permanent divergence: without anti-entropy, partitioned nodes never reconcile after healing'
  - 'Replication lag during high write load: async replication means a read from a slow replica may miss recent writes'
  - 'Clock synchronization across restarts: if a node restarts and forgets its clock, it starts from 0 and creates false conflicts. Persist the clock.'
  - 'Anti-entropy scanning all keys is expensive: use Merkle trees or hash-based comparison to identify differing keys efficiently'
  concepts:
  - Full replication with asynchronous propagation
  - Read repair for consistency on read
  - Anti-entropy for partition recovery
  - Eventual consistency guarantees
  skills:
  - Asynchronous replication protocol
  - Read-repair implementation
  - Anti-entropy mechanism
  - Partition testing
  deliverables:
  - Multi-node replication with async write propagation
  - Read-repair mechanism comparing and merging on read
  - Anti-entropy background process for post-partition reconciliation
  - Integration tests demonstrating partition, divergence, heal, and reconciliation
  estimated_hours: 5-7
domain: distributed
