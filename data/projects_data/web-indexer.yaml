id: web-indexer
name: Web Indexer
description: >
  Build a web crawler and indexer capable of processing billions of web pages.
  Implement distributed crawling, politeness policies, inverted indexing, and
  PageRank-style ranking for a mini search engine.

difficulty: expert
estimated_hours: 80-100
domain: world-scale

essence: >
  Massive-scale web crawling with politeness-constrained parallelism, content extraction
  from diverse HTML structures, inverted index construction with compression, and link-based
  ranking algorithms - the core components behind Google-scale search.

why_important: >
  Web indexing is foundational to search engines, and the techniques (crawling, indexing,
  ranking) apply to many large-scale data processing systems. Understanding billion-scale
  systems is valuable for infrastructure engineers at $200K-400K+.

learning_outcomes:
  - Implement distributed web crawler with politeness and deduplication
  - Build content extraction pipeline for diverse HTML structures
  - Implement inverted index with compression (varint, PForDelta)
  - Build distributed index serving with sharding
  - Implement PageRank or similar link-based ranking
  - Handle crawl frontier management at scale
  - Implement URL normalization and deduplication
  - Build real-time index updates

skills:
  - Web Crawling
  - Inverted Indexing
  - Index Compression
  - Distributed Systems
  - Link Analysis
  - Content Extraction
  - URL Normalization
  - Large-Scale Data Processing

tags:
  - expert
  - web-crawling
  - search-engine
  - pagerank
  - inverted-index
  - distributed-systems
  - scale

languages:
  recommended:
    - Go
    - Python
  also_possible:
    - Rust
    - Java

resources:
  - name: "The Anatomy of a Large-Scale Hypertextual Web Search Engine"
    url: https://research.google/pubs/pub334/
    type: paper
  - name: "Google's PageRank and Beyond"
    url: https://press.princeton.edu/books/hardcover/9780691152660/googles-pagerank-and-beyond
    type: book
  - name: "Managing Gigabytes"
    url: https://www.cs.otago.ac.nz/homepages/andrew/papers/books.html
    type: book
  - name: "Common Crawl"
    url: https://commoncrawl.org/
    type: dataset

prerequisites:
  - type: skill
    name: Distributed systems fundamentals
  - type: skill
    name: Data structures (hash tables, trees)
  - type: skill
    name: HTTP and web fundamentals
  - type: project
    name: search-engine or equivalent

milestones:
  - id: web-indexer-m1
    name: Crawler Core & Politeness
    description: >
      Build a web crawler with proper politeness policies and
      robots.txt handling.
    acceptance_criteria:
      - Crawler fetches pages with configurable parallelism
      - Robots.txt parsed and respected per domain
      - Per-domain rate limiting (min delay between requests)
      - Exponential backoff on errors (429, 5xx)
      - User-Agent header identifies crawler
      - URL frontier (queue) manages crawl order
      - Handles redirects (follows with limit), malformed URLs (skips)
    pitfalls:
      - Overwhelming servers causes IP blocks - strict politeness
      - Crawl traps (infinite URL spaces) - depth/visit limits
      - Not respecting robots.txt is unethical and may be illegal
      - Memory leaks from unclosed connections
    concepts:
      - Polite crawling
      - Robots.txt
      - Rate limiting
      - Crawl frontier
    skills:
      - Crawler implementation
      - Politeness policies
      - Error handling
      - Queue management
    deliverables:
      - Working crawler
      - Robots.txt parser
      - Rate limiter
      - URL frontier
    estimated_hours: "14-18"

  - id: web-indexer-m2
    name: URL Normalization & Deduplication
    description: >
      Implement URL normalization and near-duplicate detection
      to avoid redundant crawling and indexing.
    acceptance_criteria:
      - URL normalization: scheme lowercase, path normalization, query sort
      - Canonical URL detection from link tags
      - URL fingerprinting (hash) for O(1) seen-URL lookup
      - Simhash or MinHash for near-duplicate content detection
      - Bloom filter for memory-efficient seen-URL tracking
      - Handles URL encoding/decoding edge cases
    pitfalls:
      - Different URLs pointing to same content - canonical detection needed
      - Session IDs in URLs create false uniqueness
      - Hash collisions in fingerprinting
      - Bloom filter false positives cause missed pages
    concepts:
      - URL normalization
      - Content fingerprinting
      - Near-duplicate detection
      - Bloom filters
    skills:
      - URL processing
      - Hash functions
      - Simhash/MinHash
      - Memory-efficient data structures
    deliverables:
      - URL normalizer
      - Content fingerprinter
      - Near-duplicate detector
      - Bloom filter for seen URLs
    estimated_hours: "10-14"

  - id: web-indexer-m3
    name: Content Extraction & Parsing
    description: >
      Build content extraction pipeline to get clean text from
      diverse HTML structures.
    acceptance_criteria:
      - HTML parsing handles malformed markup gracefully
      - Main content extraction (boilerplate removal)
      - Text extraction from JavaScript-rendered pages (optional)
      - Metadata extraction: title, description, OG tags, schema.org
      - Link extraction with anchor text
      - Language detection for content
    pitfalls:
      - HTML parsers differ in handling malformed HTML
      - JavaScript-rendered content requires browser (expensive)
      - Boilerplate removal may remove legitimate content
      - Encoding detection is tricky (HTTP header vs meta tag)
    concepts:
      - HTML parsing
      - Content extraction
      - Boilerplate removal
      - Metadata extraction
    skills:
      - HTML parsing
      - Content extraction
      - Metadata handling
      - Encoding detection
    deliverables:
      - HTML parser wrapper
      - Content extractor
      - Metadata extractor
      - Link extractor
    estimated_hours: "12-16"

  - id: web-indexer-m4
    name: Inverted Index Construction
    description: >
      Build an inverted index with compression for efficient
      storage and retrieval.
    acceptance_criteria:
      - Inverted index maps terms to (doc_id, positions) postings
      - Posting lists compressed with varint or PForDelta
      - Dictionary (term -> posting list offset) fits in memory
      - Index sharded across multiple files/nodes
      - Supports phrase queries (position indexing)
      - Index construction can be parallelized (MapReduce style)
      - Index size < 30% of raw text size
    pitfalls:
      - Dictionary doesn't fit in memory for large corpora
      - Compression ratio vs decompression speed tradeoff
      - Index updates require rebuilding or complex merge
      - Position indexing significantly increases index size
    concepts:
      - Inverted index structure
      - Posting list compression
      - Dictionary management
      - Distributed indexing
    skills:
      - Index construction
      - Compression algorithms
      - Distributed processing
      - Storage optimization
    deliverables:
      - Inverted index builder
      - Compressed posting lists
      - In-memory dictionary
      - Sharded index files
    estimated_hours: "16-20"

  - id: web-indexer-m5
    name: Link Analysis & Ranking
    description: >
      Implement PageRank or similar link-based ranking algorithm
      for result relevance.
    acceptance_criteria:
      - Link graph constructed from extracted links
      - PageRank computed iteratively with damping factor
      - Handles dangling nodes (pages with no outlinks)
      - Ranking combined with text relevance (BM25)
      - Convergence detected (change < threshold)
      - Works on graph with millions of nodes
    pitfalls:
      - Dangling nodes require special handling
      - PageRank doesn't converge without damping
      - Link farms manipulate ranking - need spam detection
      - Memory for graph can be enormous
    concepts:
      - Link graph
      - PageRank algorithm
      - Damping factor
      - Rank aggregation
    skills:
      - Graph construction
      - PageRank implementation
      - Iterative algorithms
      - Rank combination
    deliverables:
      - Link graph builder
      - PageRank calculator
      - Rank aggregation
      - Convergence analysis
    estimated_hours: "14-18"

  - id: web-indexer-m6
    name: Distributed Architecture
    description: >
      Scale the system to handle billions of pages with
      distributed crawling and indexing.
    acceptance_criteria:
      - Crawler workers distributed across multiple machines
      - URL frontier distributed with consistent hashing
      - Index shards distributed across multiple nodes
      - Query routing to relevant shards
      - Handles node failures gracefully
      - Monitoring tracks crawl rate, index size, query latency
    pitfalls:
      - Distributed coordination overhead
      - Load imbalance across shards
      - Network bandwidth becomes bottleneck
      - Single points of failure in coordination
    concepts:
      - Distributed crawling
      - Shard distribution
      - Query routing
      - Fault tolerance
    skills:
      - Distributed architecture
      - Load balancing
      - Fault tolerance
      - Monitoring
    deliverables:
      - Distributed crawler
      - Sharded index
      - Query router
      - Monitoring dashboard
    estimated_hours: "14-18"
