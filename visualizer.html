<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EduTutor Crafter - Project Explorer</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-primary: #0a0a0f;
            --bg-secondary: #12121a;
            --bg-card: #1a1a24;
            --bg-hover: #22222e;
            --text-primary: #ffffff;
            --text-secondary: #a0a0b0;
            --text-muted: #606070;
            --accent-1: #6366f1;
            --accent-2: #8b5cf6;
            --accent-3: #ec4899;
            --accent-4: #06b6d4;
            --beginner: #22c55e;
            --intermediate: #eab308;
            --advanced: #f97316;
            --expert: #ef4444;
            --border: rgba(255,255,255,0.08);
            --glow: rgba(99, 102, 241, 0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            min-height: 100vh;
            overflow-x: hidden;
        }

        /* Animated Background */
        .bg-gradient {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background:
                radial-gradient(ellipse at 20% 20%, rgba(99, 102, 241, 0.15) 0%, transparent 50%),
                radial-gradient(ellipse at 80% 80%, rgba(139, 92, 246, 0.1) 0%, transparent 50%),
                radial-gradient(ellipse at 50% 50%, rgba(236, 72, 153, 0.05) 0%, transparent 70%);
            pointer-events: none;
            z-index: 0;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 24px;
            position: relative;
            z-index: 1;
        }

        /* Header */
        header {
            padding: 40px 0 30px;
            text-align: center;
        }

        .logo {
            font-size: 14px;
            font-weight: 600;
            letter-spacing: 3px;
            text-transform: uppercase;
            color: var(--accent-1);
            margin-bottom: 16px;
        }

        h1 {
            font-size: clamp(2.5rem, 5vw, 4rem);
            font-weight: 800;
            background: linear-gradient(135deg, var(--text-primary) 0%, var(--accent-1) 50%, var(--accent-2) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 16px;
        }

        .subtitle {
            font-size: 1.1rem;
            color: var(--text-secondary);
            max-width: 600px;
            margin: 0 auto;
        }

        /* Search & Filters */
        .search-container {
            margin: 40px 0;
            display: flex;
            gap: 16px;
            flex-wrap: wrap;
            justify-content: center;
            align-items: center;
        }

        .search-box {
            position: relative;
            flex: 1;
            max-width: 400px;
        }

        .search-box input {
            width: 100%;
            padding: 14px 20px 14px 48px;
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 12px;
            color: var(--text-primary);
            font-size: 15px;
            transition: all 0.3s ease;
        }

        .search-box input:focus {
            outline: none;
            border-color: var(--accent-1);
            box-shadow: 0 0 0 3px var(--glow);
        }

        .search-box::before {
            content: '';
            position: absolute;
            left: 18px;
            top: 50%;
            transform: translateY(-50%);
            width: 18px;
            height: 18px;
            background: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 24 24' stroke='%23606070'%3E%3Cpath stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z'%3E%3C/path%3E%3C/svg%3E") center/contain no-repeat;
        }

        .filter-pills {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }

        .filter-pill {
            padding: 10px 18px;
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 100px;
            font-size: 13px;
            font-weight: 500;
            color: var(--text-secondary);
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .filter-pill:hover {
            background: var(--bg-hover);
            border-color: var(--accent-1);
            color: var(--text-primary);
        }

        .filter-pill.active {
            background: linear-gradient(135deg, var(--accent-1), var(--accent-2));
            border-color: transparent;
            color: white;
        }

        /* Stats Bar */
        .stats-bar {
            display: flex;
            justify-content: center;
            gap: 40px;
            padding: 24px;
            margin-bottom: 40px;
            flex-wrap: wrap;
        }

        .stat {
            text-align: center;
        }

        .stat-value {
            font-size: 2rem;
            font-weight: 700;
            background: linear-gradient(135deg, var(--accent-1), var(--accent-4));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .stat-label {
            font-size: 13px;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        /* Domain Cards Grid */
        .domains-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
            gap: 24px;
            margin-bottom: 60px;
        }

        .domain-card {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 20px;
            padding: 28px;
            cursor: pointer;
            transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            overflow: hidden;
        }

        .domain-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 4px;
            background: linear-gradient(90deg, var(--accent-1), var(--accent-2), var(--accent-3));
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .domain-card:hover {
            transform: translateY(-6px);
            border-color: var(--accent-1);
            box-shadow:
                0 20px 40px rgba(0, 0, 0, 0.3),
                0 0 60px var(--glow);
        }

        .domain-card:hover::before {
            opacity: 1;
        }

        .domain-icon {
            width: 56px;
            height: 56px;
            border-radius: 14px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 26px;
            margin-bottom: 20px;
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.2), rgba(139, 92, 246, 0.2));
        }

        .domain-name {
            font-size: 1.25rem;
            font-weight: 700;
            margin-bottom: 8px;
        }

        .domain-subdomains {
            font-size: 13px;
            color: var(--text-muted);
            margin-bottom: 20px;
            line-height: 1.6;
        }

        .project-counts {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }

        .count-badge {
            padding: 6px 12px;
            border-radius: 6px;
            font-size: 12px;
            font-weight: 600;
        }

        .count-badge.beginner { background: rgba(34, 197, 94, 0.15); color: var(--beginner); }
        .count-badge.intermediate { background: rgba(234, 179, 8, 0.15); color: var(--intermediate); }
        .count-badge.advanced { background: rgba(249, 115, 22, 0.15); color: var(--advanced); }
        .count-badge.expert { background: rgba(239, 68, 68, 0.15); color: var(--expert); }

        /* Modal */
        .modal-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: rgba(0, 0, 0, 0.8);
            backdrop-filter: blur(8px);
            z-index: 1000;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
        }

        .modal-overlay.active {
            opacity: 1;
            visibility: visible;
        }

        .modal {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%) scale(0.9);
            width: 90%;
            max-width: 900px;
            max-height: 85vh;
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: 24px;
            overflow: hidden;
            z-index: 1001;
            opacity: 0;
            visibility: hidden;
            transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
        }

        .modal.active {
            opacity: 1;
            visibility: visible;
            transform: translate(-50%, -50%) scale(1);
        }

        .modal-header {
            padding: 24px 28px;
            background: var(--bg-card);
            border-bottom: 1px solid var(--border);
            display: flex;
            align-items: center;
            justify-content: space-between;
        }

        .modal-header h2 {
            font-size: 1.5rem;
            display: flex;
            align-items: center;
            gap: 12px;
        }

        .close-btn {
            width: 40px;
            height: 40px;
            border-radius: 10px;
            background: var(--bg-hover);
            border: none;
            color: var(--text-secondary);
            cursor: pointer;
            font-size: 20px;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s ease;
        }

        .close-btn:hover {
            background: var(--accent-1);
            color: white;
        }

        .modal-content {
            padding: 28px;
            overflow-y: auto;
            max-height: calc(85vh - 80px);
        }

        /* Project List */
        .level-section {
            margin-bottom: 32px;
        }

        .level-header {
            display: flex;
            align-items: center;
            gap: 12px;
            margin-bottom: 16px;
        }

        .level-indicator {
            padding: 6px 14px;
            border-radius: 6px;
            font-size: 12px;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .level-indicator.beginner { background: var(--beginner); color: #000; }
        .level-indicator.intermediate { background: var(--intermediate); color: #000; }
        .level-indicator.advanced { background: var(--advanced); color: #000; }
        .level-indicator.expert { background: var(--expert); color: #fff; }

        .projects-list {
            display: grid;
            gap: 12px;
        }

        .project-item {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 16px 20px;
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .project-item:hover {
            border-color: var(--accent-1);
            transform: translateX(8px);
        }

        .project-item.detailed {
            border-left: 3px solid var(--accent-1);
        }

        .project-info h4 {
            font-size: 15px;
            font-weight: 600;
            margin-bottom: 4px;
        }

        .project-info p {
            font-size: 13px;
            color: var(--text-muted);
        }

        .project-arrow {
            color: var(--text-muted);
            transition: all 0.3s ease;
        }

        .project-item:hover .project-arrow {
            color: var(--accent-1);
            transform: translateX(4px);
        }

        /* Project Detail View */
        .project-detail {
            animation: slideIn 0.4s ease;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateX(20px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        .back-btn {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 10px 16px;
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 8px;
            color: var(--text-secondary);
            font-size: 13px;
            cursor: pointer;
            margin-bottom: 24px;
            transition: all 0.3s ease;
        }

        .back-btn:hover {
            border-color: var(--accent-1);
            color: var(--text-primary);
        }

        .detail-header {
            margin-bottom: 32px;
        }

        .detail-header h3 {
            font-size: 1.75rem;
            margin-bottom: 8px;
        }

        .detail-meta {
            display: flex;
            gap: 16px;
            flex-wrap: wrap;
            margin-top: 12px;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 6px;
            font-size: 13px;
            color: var(--text-muted);
        }

        .section {
            margin-bottom: 32px;
        }

        .section-title {
            font-size: 14px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--accent-1);
            margin-bottom: 16px;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border);
        }

        /* Milestones */
        .milestone {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 16px;
            padding: 24px;
            margin-bottom: 16px;
            transition: all 0.3s ease;
        }

        .milestone:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }

        .milestone-header {
            display: flex;
            align-items: flex-start;
            gap: 16px;
            margin-bottom: 16px;
        }

        .milestone-number {
            width: 36px;
            height: 36px;
            background: linear-gradient(135deg, var(--accent-1), var(--accent-2));
            border-radius: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            font-size: 14px;
            flex-shrink: 0;
        }

        .milestone-title {
            font-size: 1.1rem;
            font-weight: 600;
        }

        .milestone-desc {
            color: var(--text-secondary);
            font-size: 14px;
            line-height: 1.6;
            margin-bottom: 16px;
        }

        .milestone-details {
            display: none;
        }

        .milestone.expanded .milestone-details {
            display: block;
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .expand-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 14px;
            background: var(--bg-hover);
            border: 1px solid var(--border);
            border-radius: 8px;
            color: var(--text-secondary);
            font-size: 12px;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .expand-btn:hover {
            background: var(--accent-1);
            border-color: var(--accent-1);
            color: white;
        }

        .criteria-list, .pitfalls-list, .concepts-list {
            list-style: none;
            margin-top: 12px;
        }

        .criteria-list li, .pitfalls-list li, .concepts-list li {
            padding: 8px 0 8px 24px;
            position: relative;
            font-size: 13px;
            color: var(--text-secondary);
            border-bottom: 1px solid var(--border);
        }

        .criteria-list li:last-child, .pitfalls-list li:last-child, .concepts-list li:last-child {
            border-bottom: none;
        }

        .criteria-list li::before {
            content: '‚úì';
            position: absolute;
            left: 0;
            color: var(--beginner);
        }

        .pitfalls-list li::before {
            content: '‚ö†';
            position: absolute;
            left: 0;
            color: var(--advanced);
        }

        .concepts-list li::before {
            content: '‚óÜ';
            position: absolute;
            left: 0;
            color: var(--accent-1);
        }

        /* Hints */
        .hints-container {
            margin-top: 16px;
        }

        .hint-level {
            background: var(--bg-hover);
            border-radius: 10px;
            margin-bottom: 8px;
            overflow: hidden;
        }

        .hint-header {
            padding: 12px 16px;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
            font-size: 13px;
            font-weight: 500;
        }

        .hint-header:hover {
            background: rgba(99, 102, 241, 0.1);
        }

        .hint-content {
            display: none;
            padding: 0 16px 16px;
            font-size: 13px;
            color: var(--text-secondary);
            line-height: 1.7;
        }

        .hint-content pre {
            background: var(--bg-primary);
            padding: 12px;
            border-radius: 8px;
            overflow-x: auto;
            margin-top: 8px;
            font-size: 12px;
        }

        .hint-level.open .hint-content {
            display: block;
        }

        /* Resources */
        .resources-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 12px;
        }

        .resource-link {
            display: block;
            padding: 16px;
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 10px;
            text-decoration: none;
            transition: all 0.3s ease;
        }

        .resource-link:hover {
            border-color: var(--accent-1);
            transform: translateY(-2px);
        }

        .resource-link .type {
            font-size: 10px;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--accent-2);
            margin-bottom: 4px;
        }

        .resource-link .name {
            font-size: 14px;
            color: var(--text-primary);
            font-weight: 500;
        }

        /* Tags */
        .tags {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
            margin-top: 12px;
        }

        .tag {
            padding: 4px 10px;
            background: rgba(99, 102, 241, 0.15);
            border-radius: 4px;
            font-size: 12px;
            color: var(--accent-1);
        }

        /* Empty State */
        .empty-state {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-muted);
        }

        .empty-state .icon {
            font-size: 48px;
            margin-bottom: 16px;
        }

        /* Footer */
        footer {
            text-align: center;
            padding: 40px 0;
            color: var(--text-muted);
            font-size: 13px;
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .domains-grid {
                grid-template-columns: 1fr;
            }

            .search-container {
                flex-direction: column;
            }

            .search-box {
                max-width: 100%;
            }

            .stats-bar {
                gap: 24px;
            }
        }
    </style>
</head>
<body>
    <div class="bg-gradient"></div>

    <div class="container">
        <header>
            <div class="logo">EduTutor Crafter</div>
            <h1>Project Explorer</h1>
            <p class="subtitle">Explore 100+ "Build Your Own X" projects across 11 domains, from beginner to expert level</p>
        </header>

        <div class="search-container">
            <div class="search-box">
                <input type="text" id="searchInput" placeholder="Search projects, domains, or technologies...">
            </div>
            <div class="filter-pills">
                <button class="filter-pill active" data-filter="all">All Levels</button>
                <button class="filter-pill" data-filter="beginner">Beginner</button>
                <button class="filter-pill" data-filter="intermediate">Intermediate</button>
                <button class="filter-pill" data-filter="advanced">Advanced</button>
                <button class="filter-pill" data-filter="expert">Expert</button>
            </div>
        </div>

        <div class="stats-bar">
            <div class="stat">
                <div class="stat-value" id="totalDomains">11</div>
                <div class="stat-label">Domains</div>
            </div>
            <div class="stat">
                <div class="stat-value" id="totalProjects">0</div>
                <div class="stat-label">Projects</div>
            </div>
            <div class="stat">
                <div class="stat-value" id="detailedProjects">0</div>
                <div class="stat-label">Detailed Specs</div>
            </div>
            <div class="stat">
                <div class="stat-value" id="totalMilestones">0</div>
                <div class="stat-label">Milestones</div>
            </div>
        </div>

        <div class="domains-grid" id="domainsGrid"></div>

        <footer>
            Built with care for learners everywhere. Data from EduTutor Crafter curriculum v2.0
        </footer>
    </div>

    <!-- Modal -->
    <div class="modal-overlay" id="modalOverlay"></div>
    <div class="modal" id="modal">
        <div class="modal-header">
            <h2 id="modalTitle"></h2>
            <button class="close-btn" id="closeModal">&times;</button>
        </div>
        <div class="modal-content" id="modalContent"></div>
    </div>

    <script>
    // Data from projects.yaml
    const projectsData = {
        domains: [
            {
                id: "app-dev",
                name: "Application Development",
                icon: "üåê",
                subdomains: ["Web Frontend", "Web Backend", "Full-stack", "Mobile", "Desktop & CLI"],
                projects: {
                    beginner: [
                        { id: "todo-app", name: "Todo App", description: "Basic CRUD application", detailed: true },
                        { id: "weather-app", name: "Weather App", description: "API consumption, async data", detailed: true },
                        { id: "portfolio-site", name: "Portfolio Website", description: "Static site, responsive design", detailed: true },
                        { id: "calculator", name: "Calculator", description: "UI state management", detailed: true }
                    ],
                    intermediate: [
                        { id: "blog-platform", name: "Blog Platform", description: "Full CRUD, auth, markdown", detailed: true },
                        { id: "chat-app", name: "Real-time Chat", description: "WebSocket, real-time updates", detailed: true },
                        { id: "ecommerce-basic", name: "E-commerce (Basic)", description: "Cart, checkout flow", detailed: true }
                    ],
                    advanced: [
                        { id: "social-network", name: "Social Network", description: "Feed, followers, notifications", detailed: true },
                        { id: "video-streaming", name: "Video Streaming", description: "HLS, adaptive bitrate", detailed: true }
                    ],
                    expert: [
                        { id: "build-react", name: "Build Your Own React", description: "Virtual DOM, reconciliation, hooks, fiber", detailed: true },
                        { id: "build-bundler", name: "Build Your Own Bundler", description: "Module resolution, tree shaking, code splitting", detailed: true },
                        { id: "build-spreadsheet", name: "Build Your Own Spreadsheet", description: "Excel-like app with formulas", detailed: true },
                        { id: "build-web-framework", name: "Build Your Own Web Framework", description: "Express/Django clone", detailed: true }
                    ]
                }
            },
            {
                id: "systems",
                name: "Systems & Low-Level",
                icon: "‚öôÔ∏è",
                subdomains: ["Systems Programming", "Networking", "Operating Systems"],
                projects: {
                    beginner: [
                        { id: "cat-clone", name: "Cat Clone", description: "File reading, stdout", detailed: true },
                        { id: "wc-clone", name: "Wc Clone", description: "Line/word/byte counting", detailed: true },
                        { id: "grep-clone", name: "Grep Clone", description: "Pattern matching basics", detailed: true },
                        { id: "file-copy", name: "File Copy (cp clone)", description: "File I/O, permissions", detailed: true }
                    ],
                    intermediate: [
                        { id: "shell-basic", name: "Shell (Basic)", description: "Pipes, redirects", detailed: true },
                        { id: "http-server-basic", name: "HTTP Server (Basic)", description: "Static file serving", detailed: true },
                        { id: "memory-pool", name: "Memory Pool Allocator", description: "Fixed-size block allocation", detailed: true },
                        { id: "process-spawner", name: "Process Spawner", description: "fork/exec, process lifecycle", bridge: true, detailed: true },
                        { id: "signal-handler", name: "Signal Handler", description: "SIGINT, SIGTERM, signal masks", bridge: true, detailed: true }
                    ],
                    advanced: [
                        { id: "http2-server", name: "HTTP/2 Server", description: "Multiplexing, HPACK", detailed: true },
                        { id: "container-basic", name: "Container (Basic)", description: "Namespaces isolation", detailed: true },
                        { id: "mini-shell", name: "Mini Shell", description: "Job control, background processes", bridge: true, detailed: true },
                        { id: "virtual-memory-sim", name: "Virtual Memory Simulator", description: "Page tables, TLB", bridge: true, detailed: true }
                    ],
                    expert: [
                        { id: "build-docker", name: "Build Your Own Docker", description: "Container runtime with namespaces, cgroups", detailed: true, languages: ["Go", "Rust", "C"] },
                        { id: "build-shell", name: "Build Your Own Shell", description: "Full Unix shell with job control", detailed: true, languages: ["C", "Rust", "Go"] },
                        { id: "build-allocator", name: "Build Your Own Memory Allocator", description: "malloc/free implementation", detailed: true, languages: ["C", "Rust", "Zig"] },
                        { id: "build-os", name: "Build Your Own OS", description: "Operating system kernel", detailed: true, languages: ["C", "Rust", "Zig"] },
                        { id: "build-tcp-stack", name: "Build Your Own TCP/IP Stack", description: "Network stack implementation", detailed: true, languages: ["C", "Rust", "Go"] }
                    ]
                }
            },
            {
                id: "data-storage",
                name: "Data & Storage",
                icon: "üóÑÔ∏è",
                subdomains: ["Databases", "Data Engineering"],
                projects: {
                    beginner: [
                        { id: "json-db", name: "JSON File Database", description: "File-based storage", detailed: true },
                        { id: "kv-memory", name: "In-memory Key-Value Store", description: "Hash map based", detailed: true }
                    ],
                    intermediate: [
                        { id: "btree-impl", name: "B-tree Implementation", description: "Insert, delete, rebalance", detailed: true },
                        { id: "sql-parser", name: "SQL Parser", description: "SELECT, WHERE, JOIN", detailed: true }
                    ],
                    advanced: [
                        { id: "query-optimizer", name: "Query Optimizer", description: "Cost estimation", detailed: true },
                        { id: "wal-impl", name: "WAL Implementation", description: "Write-ahead logging", detailed: true }
                    ],
                    expert: [
                        { id: "build-redis", name: "Build Your Own Redis", description: "In-memory data store with RESP protocol", detailed: true },
                        { id: "build-sqlite", name: "Build Your Own SQLite", description: "Embedded SQL database", detailed: true },
                        { id: "build-kafka", name: "Build Your Own Kafka", description: "Distributed message queue", detailed: true }
                    ]
                }
            },
            {
                id: "distributed",
                name: "Distributed & Cloud",
                icon: "‚òÅÔ∏è",
                subdomains: ["Distributed Systems", "Cloud & DevOps", "Consensus", "Blockchain"],
                projects: {
                    beginner: [
                        { id: "service-discovery", name: "Service Discovery", description: "Registry, health checks", detailed: true },
                        { id: "rpc-basic", name: "RPC Framework (Basic)", description: "Remote procedure calls", bridge: true, detailed: true }
                    ],
                    intermediate: [
                        { id: "load-balancer-basic", name: "Load Balancer (Basic)", description: "Round-robin", detailed: true },
                        { id: "rate-limiter", name: "Rate Limiter", description: "Token bucket", detailed: true },
                        { id: "leader-election", name: "Leader Election", description: "Bully algorithm, ring election", bridge: true, detailed: true },
                        { id: "replicated-log", name: "Replicated Log", description: "Append-only log, basic replication", bridge: true, detailed: true },
                        { id: "vector-clocks", name: "Vector Clocks", description: "Logical time, causality", bridge: true, detailed: true }
                    ],
                    advanced: [
                        { id: "distributed-cache", name: "Distributed Cache", description: "Consistent hashing", detailed: true },
                        { id: "gossip-protocol", name: "Gossip Protocol", description: "Epidemic broadcast", bridge: true, detailed: true },
                        { id: "2pc-impl", name: "Two-Phase Commit", description: "Distributed transactions", bridge: true, detailed: true }
                    ],
                    expert: [
                        { id: "build-raft", name: "Build Your Own Raft", description: "Consensus algorithm", detailed: true, languages: ["Go", "Rust", "Java"] },
                        { id: "build-blockchain", name: "Build Your Own Blockchain", description: "Proof of work, P2P network", detailed: true, languages: ["Python", "Go", "Rust"] },
                        { id: "build-distributed-kv", name: "Build Your Own Distributed KV Store", description: "Partitioning, replication", detailed: true }
                    ]
                }
            },
            {
                id: "ai-ml",
                name: "AI & Machine Learning",
                icon: "üß†",
                subdomains: ["Classical ML", "Deep Learning", "NLP"],
                projects: {
                    beginner: [
                        { id: "linear-regression", name: "Linear Regression", description: "Gradient descent", detailed: true },
                        { id: "knn", name: "KNN Classifier", description: "Distance metrics", detailed: true }
                    ],
                    intermediate: [
                        { id: "neural-network-basic", name: "Neural Network (micrograd)", description: "Forward/backward pass", detailed: true },
                        { id: "word2vec", name: "Word Embeddings", description: "Skip-gram, CBOW", detailed: true }
                    ],
                    advanced: [
                        { id: "transformer-scratch", name: "Transformer from Scratch", description: "Attention mechanism", detailed: true },
                        { id: "gan", name: "GAN", description: "Generator, discriminator", detailed: true }
                    ],
                    expert: [
                        { id: "build-nn-framework", name: "Build Your Own Neural Network Framework", description: "PyTorch/TensorFlow clone", detailed: true },
                        { id: "build-transformer", name: "Build Your Own Transformer", description: "Full GPT implementation", detailed: true }
                    ]
                }
            },
            {
                id: "game-dev",
                name: "Game Development",
                icon: "üéÆ",
                subdomains: ["Game Programming", "Graphics", "Engine Development"],
                projects: {
                    beginner: [
                        { id: "pong", name: "Pong", description: "Basic game loop", detailed: true },
                        { id: "snake", name: "Snake", description: "Grid movement", detailed: true },
                        { id: "tetris", name: "Tetris", description: "Rotation, line clearing", detailed: true }
                    ],
                    intermediate: [
                        { id: "platformer", name: "Platformer", description: "Gravity, jumping", detailed: true },
                        { id: "topdown-shooter", name: "Top-down Shooter", description: "Enemies, projectiles", detailed: true }
                    ],
                    advanced: [
                        { id: "software-3d", name: "Software 3D Renderer", description: "No GPU, pure math", detailed: true },
                        { id: "ecs-arch", name: "ECS Architecture", description: "Entity-component-system", detailed: true }
                    ],
                    expert: [
                        { id: "build-game-engine", name: "Build Your Own Game Engine", description: "Full 2D/3D engine", detailed: true },
                        { id: "build-raytracer", name: "Build Your Own Ray Tracer", description: "Path tracing renderer", detailed: true }
                    ]
                }
            },
            {
                id: "compilers",
                name: "Languages & Compilers",
                icon: "üìù",
                subdomains: ["Parsing & Lexing", "Interpreters", "Compilers", "Runtime Systems"],
                projects: {
                    beginner: [
                        { id: "calculator-parser", name: "Calculator Parser", description: "Arithmetic expressions", detailed: true },
                        { id: "json-parser", name: "JSON Parser", description: "Recursive descent", detailed: true },
                        { id: "tokenizer", name: "Tokenizer/Lexer", description: "Token types, state machine", bridge: true, detailed: true }
                    ],
                    intermediate: [
                        { id: "lisp-interp", name: "Lisp Interpreter", description: "S-expressions, eval", detailed: true },
                        { id: "bytecode-vm", name: "Bytecode VM", description: "Stack-based", detailed: true },
                        { id: "ast-builder", name: "AST Builder", description: "Parse expressions to AST", bridge: true, detailed: true },
                        { id: "ast-interpreter", name: "AST Tree-Walking Interpreter", description: "Evaluate AST directly", bridge: true, detailed: true }
                    ],
                    advanced: [
                        { id: "type-checker", name: "Type Checker", description: "Type inference", detailed: true },
                        { id: "simple-gc", name: "Simple GC", description: "Mark-sweep", detailed: true },
                        { id: "bytecode-compiler", name: "Bytecode Compiler", description: "AST to bytecode", bridge: true, detailed: true },
                        { id: "wasm-emitter", name: "WebAssembly Emitter", description: "Emit .wasm from AST", bridge: true, detailed: true }
                    ],
                    expert: [
                        { id: "build-interpreter", name: "Build Your Own Interpreter (Lox)", description: "Crafting Interpreters", detailed: true, languages: ["Java", "C", "Rust", "Go"] },
                        { id: "build-gc", name: "Build Your Own Garbage Collector", description: "Memory management", detailed: true, languages: ["C", "Rust", "Zig"] },
                        { id: "build-regex", name: "Build Your Own Regex Engine", description: "NFA/DFA, Thompson construction", detailed: true, languages: ["C", "Rust", "Go", "Python"] }
                    ]
                }
            },
            {
                id: "security",
                name: "Security",
                icon: "üîê",
                subdomains: ["Cryptography", "Web Security"],
                projects: {
                    beginner: [
                        { id: "hash-impl", name: "Hash Function", description: "SHA-256 from spec", detailed: true },
                        { id: "password-hashing", name: "Password Hashing", description: "bcrypt, salt", detailed: true }
                    ],
                    intermediate: [
                        { id: "aes-impl", name: "AES Implementation", description: "Block cipher", detailed: true },
                        { id: "jwt-impl", name: "JWT Library", description: "Sign, verify", detailed: true }
                    ],
                    advanced: [
                        { id: "https-client", name: "HTTPS Client", description: "TLS handshake", detailed: true }
                    ],
                    expert: [
                        { id: "build-tls", name: "Build Your Own TLS", description: "TLS 1.3 implementation", detailed: true }
                    ]
                }
            },
            {
                id: "cs-fundamentals",
                name: "CS Fundamentals",
                icon: "üéì",
                subdomains: ["Data Structures", "Algorithms"],
                projects: {
                    beginner: [
                        { id: "linked-list", name: "Linked List", description: "Single, double, circular", detailed: true },
                        { id: "stack-queue", name: "Stack & Queue", description: "Array and linked", detailed: true }
                    ],
                    intermediate: [
                        { id: "bst", name: "Binary Search Tree", description: "Insert, delete, traversal", detailed: true },
                        { id: "hash-table", name: "Hash Table", description: "Chaining, open addressing", detailed: true }
                    ],
                    advanced: [
                        { id: "red-black-tree", name: "Red-Black Tree", description: "Balanced tree", detailed: true },
                        { id: "graph-algos", name: "Graph Algorithms", description: "BFS, DFS, Dijkstra", detailed: true }
                    ],
                    expert: [
                        { id: "build-btree", name: "Build Your Own B-tree", description: "Disk-friendly tree", detailed: true }
                    ]
                }
            },
            {
                id: "specialized",
                name: "Specialized",
                icon: "üîß",
                subdomains: ["Developer Tools", "Advanced Networking", "Embedded & Emulation"],
                projects: {
                    beginner: [],
                    intermediate: [],
                    advanced: [],
                    expert: [
                        { id: "build-git", name: "Build Your Own Git", description: "Version control", detailed: true },
                        { id: "build-text-editor", name: "Build Your Own Text Editor", description: "Vim-like editor", detailed: true },
                        { id: "build-bittorrent", name: "Build Your Own BitTorrent", description: "P2P file sharing", detailed: true },
                        { id: "build-dns", name: "Build Your Own DNS Server", description: "Recursive resolver", detailed: true },
                        { id: "build-debugger", name: "Build Your Own Debugger", description: "GDB-like", detailed: true },
                        { id: "build-lsp", name: "Build Your Own LSP Server", description: "Language server protocol", detailed: true },
                        { id: "build-emulator", name: "Build Your Own Emulator", description: "NES/GameBoy/CHIP-8", detailed: true },
                        { id: "build-browser", name: "Build Your Own Browser", description: "Browser engine", detailed: true }
                    ]
                }
            },
            {
                id: "software-engineering",
                name: "Software Engineering Practices",
                icon: "‚úÖ",
                subdomains: ["Testing & Quality", "CI/CD", "Observability", "Engineering Practices"],
                projects: {
                    beginner: [
                        { id: "unit-testing-basics", name: "Unit Testing Fundamentals", description: "pytest/jest basics", detailed: true },
                        { id: "git-workflow", name: "Git Workflow Mastery", description: "Branching, PRs, rebasing", detailed: true },
                        { id: "documentation-project", name: "Documentation Project", description: "README, API docs", detailed: true }
                    ],
                    intermediate: [
                        { id: "tdd-kata", name: "TDD Kata Series", description: "Red-green-refactor", detailed: true },
                        { id: "ci-pipeline", name: "CI Pipeline Setup", description: "GitHub Actions/Jenkins", detailed: true },
                        { id: "logging-structured", name: "Structured Logging", description: "JSON logs, aggregation", detailed: true },
                        { id: "code-review-practice", name: "Code Review Practice", description: "Review real PRs", detailed: true }
                    ],
                    advanced: [
                        { id: "integration-testing", name: "Integration Testing Suite", description: "Test containers, mocking", detailed: true },
                        { id: "cd-deployment", name: "CD with Blue-Green Deployment", description: "Zero-downtime deploys", detailed: true },
                        { id: "metrics-dashboard", name: "Metrics & Alerting Dashboard", description: "Prometheus, Grafana", detailed: true },
                        { id: "distributed-tracing", name: "Distributed Tracing", description: "OpenTelemetry, Jaeger", detailed: true }
                    ],
                    expert: [
                        { id: "build-test-framework", name: "Build Your Own Test Framework", description: "pytest/jest clone", detailed: true },
                        { id: "build-ci-system", name: "Build Your Own CI System", description: "Pipeline executor", detailed: true },
                        { id: "build-observability-platform", name: "Build Your Own Observability Platform", description: "Logs/metrics/traces", detailed: true }
                    ]
                }
            }
        ],

        // Detailed Expert Projects
        expertProjects: {
            "build-redis": {
                name: "Build Your Own Redis",
                description: "Build an in-memory data structure store that implements the Redis protocol. You'll learn about TCP servers, protocol parsing, data structures, and persistence.",
                difficulty: "expert",
                estimatedHours: "40-60",
                prerequisites: ["TCP/IP networking basics", "Hash tables and data structures", "Concurrency fundamentals", "File I/O"],
                languages: { recommended: ["Go", "Rust", "C"], also: ["Python", "Java", "TypeScript"] },
                resources: [
                    { name: "CodeCrafters Redis Challenge", url: "https://app.codecrafters.io/courses/redis/overview", type: "interactive" },
                    { name: "Build Your Own Redis (Book)", url: "https://build-your-own.org/redis/", type: "book" },
                    { name: "Redis Protocol Specification (RESP)", url: "https://redis.io/docs/reference/protocol-spec/", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "TCP Server + RESP Protocol",
                        description: "Create a TCP server that listens on port 6379 and responds to PING with +PONG. Implement basic RESP parsing.",
                        criteria: ["Server binds to port 6379", "Accepts TCP connections", "Responds to PING with +PONG\\r\\n", "Handles multiple sequential commands", "Clean shutdown on SIGINT"],
                        hints: {
                            level1: "Start with a simple TCP server using your language's net library. Accept connections in a loop.",
                            level2: "RESP Simple Strings start with '+', errors with '-'. PING expects '+PONG\\r\\n' response.",
                            level3: "RESP format examples:\n- Simple String: +OK\\r\\n\n- Error: -ERR unknown command\\r\\n\n- Integer: :1000\\r\\n\n- Bulk String: $5\\r\\nhello\\r\\n\n- Array: *2\\r\\n$4\\r\\nPING\\r\\n$4\\r\\nPONG\\r\\n"
                        },
                        pitfalls: ["Forgetting \\r\\n line endings (CRLF, not just LF)", "Not handling partial reads (TCP is a stream)", "Blocking the main thread on single client"],
                        concepts: ["TCP sockets and server lifecycle", "RESP protocol encoding/decoding", "Basic network I/O"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "GET/SET/DEL Commands",
                        description: "Implement basic key-value operations. Store data in an in-memory hash map.",
                        criteria: ["SET key value stores the key-value pair", "GET key returns the value or nil", "DEL key removes the key and returns 1 (or 0)", "Keys are case-sensitive", "Values can be any string (including binary)"],
                        hints: {
                            level1: "Use a hash map (dict/map) to store key-value pairs.",
                            level2: "Parse RESP arrays for commands. SET comes as *3\\r\\n$3\\r\\nSET\\r\\n$3\\r\\nkey\\r\\n$5\\r\\nvalue\\r\\n",
                            level3: "GET returns Bulk String ($length\\r\\nvalue\\r\\n) or Null Bulk String ($-1\\r\\n)"
                        },
                        pitfalls: ["Not handling binary-safe strings", "Forgetting null response for non-existent keys", "Case sensitivity issues"],
                        concepts: ["Hash table implementation", "RESP array parsing", "Command dispatch pattern"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Expiration (TTL)",
                        description: "Add key expiration support. Keys can be set with PX (milliseconds) or EX (seconds) options.",
                        criteria: ["SET key value PX milliseconds", "SET key value EX seconds", "GET returns nil for expired keys", "TTL key returns remaining time", "Expired keys are cleaned up"],
                        hints: {
                            level1: "Store expiration timestamp alongside each value.",
                            level2: "Check expiration on GET (lazy deletion). Optionally run background cleanup.",
                            level3: "Two deletion strategies:\n1. Lazy: Check on access, delete if expired\n2. Active: Background goroutine periodically scans\nRedis uses both."
                        },
                        pitfalls: ["Using relative time instead of absolute timestamp", "Not handling clock drift", "Memory leaks from never-accessed expired keys"],
                        concepts: ["TTL and expiration strategies", "Lazy vs active deletion", "Time handling and precision"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 4,
                        name: "Data Structures (List, Set, Hash)",
                        description: "Implement Redis data structure commands beyond simple strings.",
                        criteria: ["LPUSH/RPUSH adds elements to list", "LPOP/RPOP removes and returns elements", "LRANGE returns range", "SADD/SMEMBERS for sets", "HSET/HGET for hashes"],
                        hints: {
                            level1: "Use native data structures: arrays for lists, sets for sets, nested maps for hashes.",
                            level2: "Lists in Redis are doubly-linked for O(1) push/pop at both ends.",
                            level3: "For sorted sets (ZADD), consider skip list or balanced tree for O(log n) operations."
                        },
                        pitfalls: ["Using array for list (O(n) insert at head)", "Not handling type errors", "LRANGE negative indices"],
                        concepts: ["Linked lists vs arrays", "Set data structure", "Skip lists (for sorted sets)"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 5,
                        name: "Persistence (RDB Snapshots)",
                        description: "Implement point-in-time snapshots using RDB format. SAVE blocks, BGSAVE forks.",
                        criteria: ["SAVE command creates RDB file synchronously", "BGSAVE forks child process", "Server loads RDB on startup", "RDB file format is binary with checksums"],
                        hints: {
                            level1: "Start with custom binary format. Later match Redis RDB format.",
                            level2: "BGSAVE uses fork() - child inherits memory snapshot via copy-on-write.",
                            level3: "RDB file structure:\n- Magic: \"REDIS\" + version\n- Database selector\n- Key-value pairs with type byte\n- EOF marker\n- CRC64 checksum"
                        },
                        pitfalls: ["Blocking main thread during save", "Not using fork() for BGSAVE", "Corrupted RDB from incomplete writes"],
                        concepts: ["Binary serialization", "Process forking and copy-on-write", "Atomic file operations"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 6,
                        name: "Persistence (AOF)",
                        description: "Implement Append-Only File logging for durability.",
                        criteria: ["All write commands appended to AOF file", "AOF file contains RESP-formatted commands", "Server replays AOF on startup", "BGREWRITEAOF compacts the file", "Configurable fsync policy"],
                        hints: {
                            level1: "Simply append each write command to a file in RESP format.",
                            level2: "BGREWRITEAOF: fork, iterate current state, write minimal commands.",
                            level3: "fsync strategies:\n- always: fsync after every write (safest, slowest)\n- everysec: fsync once per second (good balance)\n- no: let OS decide (fastest, riskier)"
                        },
                        pitfalls: ["AOF growing unbounded without rewrite", "Concurrent writes during BGREWRITEAOF", "Data loss from buffered writes"],
                        concepts: ["Write-ahead logging", "fsync and durability guarantees", "Log compaction"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 7,
                        name: "Pub/Sub",
                        description: "Implement publish/subscribe messaging pattern.",
                        criteria: ["SUBSCRIBE channel subscribes client", "PUBLISH channel message sends to all", "UNSUBSCRIBE removes subscription", "Pattern subscriptions (PSUBSCRIBE) optional"],
                        hints: {
                            level1: "Maintain map of channel -> list of subscriber connections.",
                            level2: "Subscribed clients enter special mode - only pub/sub commands allowed.",
                            level3: "Message format: *3\\r\\n$7\\r\\nmessage\\r\\n$channel\\r\\n$message\\r\\n"
                        },
                        pitfalls: ["Not blocking other commands in subscribed state", "Memory leaks from disconnected subscribers", "Race conditions in publish"],
                        concepts: ["Pub/Sub pattern", "Observer pattern", "Connection state management"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 8,
                        name: "Cluster Mode (Sharding)",
                        description: "Implement horizontal scaling with hash slot based sharding.",
                        criteria: ["16384 hash slots distributed across nodes", "CLUSTER SLOTS returns distribution", "-MOVED redirect for keys on other nodes", "Hash tags for multi-key operations"],
                        hints: {
                            level1: "CRC16(key) % 16384 gives the slot. Each node owns a range.",
                            level2: "On wrong node, return -MOVED slot ip:port. Client should retry.",
                            level3: "Hash tags: {user123}.profile and {user123}.settings go to same slot."
                        },
                        pitfalls: ["Not handling key migration", "Cross-slot operations", "Network partitions and split brain"],
                        concepts: ["Consistent hashing", "Hash slots and key routing", "Distributed systems fundamentals"],
                        estimatedHours: "8-12"
                    }
                ]
            },
            "build-interpreter": {
                name: "Build Your Own Interpreter (Lox)",
                description: "Build a complete interpreter for the Lox programming language. Based on the excellent 'Crafting Interpreters' book by Bob Nystrom.",
                difficulty: "expert",
                estimatedHours: "40-80",
                prerequisites: ["Basic programming language concepts", "Recursion", "Tree data structures", "Object-oriented programming"],
                languages: { recommended: ["Java", "C", "Rust"], also: ["Python", "Go", "TypeScript"] },
                resources: [
                    { name: "Crafting Interpreters (Free Online Book)", url: "https://craftinginterpreters.com", type: "book" },
                    { name: "Writing An Interpreter In Go", url: "https://interpreterbook.com", type: "book" },
                    { name: "CodeCrafters Interpreter Challenge", url: "https://app.codecrafters.io/courses/interpreter/overview", type: "interactive" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Scanner (Lexer)",
                        description: "Build a scanner that converts Lox source code into tokens. Chapter 4 of Crafting Interpreters.",
                        criteria: ["Recognizes all Lox tokens", "Handles string and number literals", "Tracks line numbers", "Ignores whitespace and comments", "Reports lexical errors"],
                        hints: {
                            level1: "Consume characters one by one. Match against known patterns.",
                            level2: "Token types: ( ) { } , . - + ; / * ! != = == > >= < <=\nKeywords: and class else false for fun if nil or print return super this true var while",
                            level3: "Use a switch on first character, then peek ahead for two-char tokens."
                        },
                        pitfalls: ["Confusing = and ==", "Not handling unterminated strings", "Newlines inside strings"],
                        concepts: ["Lexical analysis", "Token representation", "Error handling"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Representing Code (AST)",
                        description: "Define the abstract syntax tree classes for Lox. Chapter 5.",
                        criteria: ["Expression classes for binary, unary, grouping, literal", "Pretty-printer using visitor pattern", "Statement classes for print, expression, var"],
                        hints: {
                            level1: "Use the Visitor pattern for operations on AST nodes.",
                            level2: "Generate classes from grammar rules. Expr -> Binary, Unary, Literal, Grouping...",
                            level3: "Expr types:\n- Binary: left op right (1 + 2)\n- Unary: op right (-1, !true)\n- Literal: value (42, \"hello\")\n- Grouping: expression ((1 + 2))"
                        },
                        pitfalls: ["Visitor pattern boilerplate", "Mutable vs immutable AST nodes", "Parent/child references creating cycles"],
                        concepts: ["Abstract Syntax Trees", "Visitor pattern", "Expression vs Statement"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Parsing Expressions",
                        description: "Build a recursive descent parser for expressions. Chapter 6.",
                        criteria: ["Parses arithmetic with correct precedence", "Handles parentheses for grouping", "Parses comparison and equality", "Reports syntax errors with context"],
                        hints: {
                            level1: "One function per precedence level. Lower precedence = higher in call stack.",
                            level2: "Precedence (low to high):\nequality (== !=)\ncomparison (< > <= >=)\nterm (+ -)\nfactor (* /)\nunary (! -)\nprimary (literals, grouping)",
                            level3: "Pratt parsing is elegant alternative. Each token has prefix/infix rules + precedence."
                        },
                        pitfalls: ["Left recursion causes infinite loop", "Forgetting closing paren", "Error recovery"],
                        concepts: ["Recursive descent parsing", "Operator precedence", "Error recovery"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Evaluating Expressions",
                        description: "Build a tree-walking interpreter to evaluate expressions. Chapter 7.",
                        criteria: ["Evaluates arithmetic operations", "Handles unary minus and negation", "Implements truthiness", "String concatenation with +", "Runtime error for type mismatches"],
                        hints: {
                            level1: "Implement Visitor that returns evaluated value. Recursively evaluate children.",
                            level2: "Lox is dynamically typed. Operations check types at runtime.",
                            level3: "Truthiness: false and nil are falsy, everything else truthy.\nEquality: nil == nil, numbers by value, strings by content."
                        },
                        pitfalls: ["Division by zero", "String + non-string", "Java null vs Lox nil"],
                        concepts: ["Tree-walking interpretation", "Dynamic typing", "Runtime type checking"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 5,
                        name: "Statements and State",
                        description: "Add statements, variables, and assignment. Chapter 8.",
                        criteria: ["Print statement outputs to stdout", "Variable declarations with var", "Assignment expressions", "Expression statements", "Environment stores variable bindings"],
                        hints: {
                            level1: "Environment is a map from variable name to value.",
                            level2: "var a = 1; creates binding. a = 2; updates existing binding.",
                            level3: "Assignment is an expression (returns value) but lowest precedence."
                        },
                        pitfalls: ["Using undeclared variable", "Assignment to non-variable", "Scoping issues"],
                        concepts: ["Statement vs Expression", "Environment and bindings", "Assignment as expression"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 6,
                        name: "Control Flow",
                        description: "Add if, while, for, and logical operators. Chapter 9.",
                        criteria: ["if/else with proper scoping", "while loops", "for loops (desugared to while)", "Logical and/or with short-circuit"],
                        hints: {
                            level1: "for (init; cond; incr) body -> { init; while (cond) { body; incr; } }",
                            level2: "Short-circuit: a and b returns a if falsy, else b. a or b returns a if truthy, else b.",
                            level3: "No 'break' in Lox. Challenge: add it (requires unwinding)."
                        },
                        pitfalls: ["Dangling else", "Forgetting short-circuit", "Infinite loops without timeout"],
                        concepts: ["Conditional execution", "Loop desugaring", "Short-circuit evaluation"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 7,
                        name: "Functions",
                        description: "Add function declarations and calls. Chapter 10.",
                        criteria: ["fun keyword declares functions", "Parameters and return values", "Return statement exits early", "Functions are first-class values", "Call stack for recursion"],
                        hints: {
                            level1: "Function value stores: name, parameters, body AST, and defining environment.",
                            level2: "Call: create new environment for local vars, execute body, return result.",
                            level3: "Return implemented as exception/longjmp - unwinds call stack to caller."
                        },
                        pitfalls: ["Stack overflow from infinite recursion", "Not restoring environment after call", "Return outside function"],
                        concepts: ["First-class functions", "Call frames", "Return values"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 8,
                        name: "Closures",
                        description: "Implement lexical scoping and closures. Chapter 11.",
                        criteria: ["Functions capture enclosing environment", "Nested functions work correctly", "Closures persist after outer function returns", "Resolving variables at compile time"],
                        hints: {
                            level1: "Closure = function + captured environment. Environment chains to enclosing scopes.",
                            level2: "Resolver pass: determine which declaration each variable refers to.",
                            level3: "Example:\nfun makeCounter() {\n  var i = 0;\n  fun count() {\n    i = i + 1;\n    return i;\n  }\n  return count;\n}\nvar counter = makeCounter();\ncounter(); // 1\ncounter(); // 2"
                        },
                        pitfalls: ["Capturing variable vs capturing value", "Variable resolution with shadowing", "This in closures"],
                        concepts: ["Lexical scoping", "Closures", "Environment chains"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 9,
                        name: "Classes",
                        description: "Add class declarations, instances, and methods. Chapter 12.",
                        criteria: ["class keyword declares classes", "Instances created with ClassName()", "Properties accessed with dot notation", "Methods with implicit 'this'", "Initializer method: init()"],
                        hints: {
                            level1: "Class is a map of method names to functions. Instance has class + fields map.",
                            level2: "this bound when method is accessed. Method call binds this to instance.",
                            level3: "init() is constructor. Called automatically on instance creation."
                        },
                        pitfalls: ["this outside method", "Returning from init()", "Method vs function"],
                        concepts: ["Classes and instances", "This binding", "Constructors"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 10,
                        name: "Inheritance",
                        description: "Add class inheritance and super calls. Chapter 13.",
                        criteria: ["class Derived < Base syntax", "Methods inherited from superclass", "super.method() calls parent", "Proper method resolution order"],
                        hints: {
                            level1: "Subclass stores reference to superclass. Method lookup chains upward.",
                            level2: "super is looked up in enclosing class, not dynamically.",
                            level3: "class Doughnut {\n  cook() { print \"Fry\"; }\n}\nclass BostonCream < Doughnut {\n  cook() {\n    super.cook();\n    print \"Pipe cream\";\n  }\n}"
                        },
                        pitfalls: ["super outside class", "Inheriting from non-class", "Diamond inheritance"],
                        concepts: ["Single inheritance", "Super calls", "Method resolution"],
                        estimatedHours: "3-4"
                    }
                ]
            },
            "build-docker": {
                name: "Build Your Own Docker",
                description: "Build a container runtime that can run isolated processes using Linux primitives. You'll learn about namespaces, cgroups, and filesystem isolation.",
                difficulty: "expert",
                estimatedHours: "30-50",
                prerequisites: ["Linux system administration", "Process management (fork, exec)", "Filesystem concepts", "Basic networking"],
                languages: { recommended: ["Go", "C", "Rust"], also: [] },
                resources: [
                    { name: "Containers from Scratch", url: "https://ericchiang.github.io/post/containers-from-scratch/", type: "blog" },
                    { name: "Containers from Scratch (Video) - Liz Rice", url: "https://www.youtube.com/watch?v=8fi7uSYlOdc", type: "video" },
                    { name: "Linux Namespaces man page", url: "https://man7.org/linux/man-pages/man7/namespaces.7.html", type: "documentation" },
                    { name: "OCI Runtime Specification", url: "https://github.com/opencontainers/runtime-spec", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Process Isolation (Namespaces)",
                        description: "Isolate a process using Linux namespaces (PID, UTS, mount).",
                        criteria: ["Process runs with its own PID namespace (PID 1)", "Process has its own hostname (UTS)", "Process has its own mount namespace", "Uses clone() or unshare() syscalls"],
                        hints: {
                            level1: "Use CLONE_NEWPID, CLONE_NEWUTS, CLONE_NEWNS flags with clone().",
                            level2: "unshare command in bash: unshare --pid --uts --mount --fork /bin/bash",
                            level3: "Linux namespace types:\n- PID: Process IDs\n- UTS: Hostname\n- Mount: Filesystem mounts\n- Net: Network stack\n- User: User/group IDs\n- IPC: Inter-process comm\n- Cgroup: Cgroup root"
                        },
                        pitfalls: ["Not using CLONE_NEWPID correctly", "/proc still showing host PIDs", "Permission issues (usually need root)"],
                        concepts: ["Linux namespaces", "Process isolation", "clone() syscall"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 2,
                        name: "Resource Limits (cgroups)",
                        description: "Limit container resources using cgroups (CPU, memory).",
                        criteria: ["Can limit memory usage (OOM-killed if exceeds)", "Can limit CPU shares/quota", "Cgroup filesystem properly set up", "Process added to cgroup before exec"],
                        hints: {
                            level1: "Create directory in /sys/fs/cgroup/memory/mycontainer. Write PID to tasks file.",
                            level2: "Set memory.limit_in_bytes for memory limit. Use cpu.cfs_quota_us for CPU.",
                            level3: "cgroup v2 paths:\n- /sys/fs/cgroup/mycontainer/memory.max\n- /sys/fs/cgroup/mycontainer/cpu.max\n- /sys/fs/cgroup/mycontainer/cgroup.procs"
                        },
                        pitfalls: ["cgroup v1 vs v2 differences", "Not cleaning up cgroups on exit", "Memory limits not accounting kernel memory"],
                        concepts: ["Control groups", "Resource limiting", "OOM killer"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Filesystem Isolation (chroot/pivot_root)",
                        description: "Give container its own root filesystem using chroot or pivot_root.",
                        criteria: ["Container sees only its own filesystem", "Host filesystem is not accessible", "Proc filesystem mounted in container", "Works with any root filesystem (Alpine, Ubuntu)"],
                        hints: {
                            level1: "chroot /path/to/rootfs /bin/sh - simplest form",
                            level2: "pivot_root is more secure than chroot. Requires mount namespace.",
                            level3: "Setup steps:\n1. Create mount namespace\n2. Mount new root somewhere\n3. pivot_root newroot putold\n4. Unmount putold\n5. Mount /proc, /sys, etc."
                        },
                        pitfalls: ["chroot is escapable without mount namespace", "Forgetting to mount /proc", "Not having required binaries in rootfs"],
                        concepts: ["chroot jails", "pivot_root", "Root filesystem"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Layered Filesystem (OverlayFS)",
                        description: "Implement layered filesystem for efficient image storage.",
                        criteria: ["Multiple read-only layers merged into single view", "Write layer captures all modifications", "Copy-on-write semantics", "Layers can be shared between containers"],
                        hints: {
                            level1: "OverlayFS: mount -t overlay overlay -o lowerdir=base,upperdir=changes,workdir=work target",
                            level2: "Multiple lower dirs: lowerdir=layer3:layer2:layer1 (later = higher priority)",
                            level3: "Docker layer structure:\n/var/lib/docker/overlay2/\n  <layer-id>/\n    diff/    # Layer contents\n    lower    # Link to parent\n    work/    # OverlayFS work dir"
                        },
                        pitfalls: ["OverlayFS doesn't support all filesystems", "Renaming directories has copy-up overhead", "Open file handles survive layer changes"],
                        concepts: ["Union filesystems", "Copy-on-write", "Docker image layers"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 5,
                        name: "Container Networking",
                        description: "Set up network namespace with virtual ethernet pair.",
                        criteria: ["Container has its own network stack", "veth pair connects container to host bridge", "Container can reach the internet (via NAT)", "Containers can communicate with each other"],
                        hints: {
                            level1: "ip netns add container; ip link add veth0 type veth peer name veth1",
                            level2: "Move one end of veth into container namespace. Connect other to bridge.",
                            level3: "Network setup:\n1. Create network namespace\n2. Create veth pair\n3. Move veth1 to container\n4. Attach veth0 to docker0 bridge\n5. Configure IP addresses\n6. Set up iptables NAT"
                        },
                        pitfalls: ["DNS resolution (need /etc/resolv.conf)", "iptables rules for masquerading", "Bridge vs host network"],
                        concepts: ["Network namespaces", "Virtual ethernet", "Linux bridge networking"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 6,
                        name: "Image Format and CLI",
                        description: "Implement OCI image format and Docker-compatible CLI.",
                        criteria: ["Pull images from Docker Hub", "Parse OCI image manifest", "Extract and layer filesystem", "docker run equivalent command"],
                        hints: {
                            level1: "OCI image is tarball with manifest.json pointing to layer tarballs.",
                            level2: "Docker Hub API: GET /v2/<name>/manifests/<tag>",
                            level3: "CLI commands to implement:\n- run: Create and start container\n- exec: Run command in existing container\n- ps: List running containers\n- images: List local images"
                        },
                        pitfalls: ["Image manifest v1 vs v2", "Content-addressable storage", "Image and layer deduplication"],
                        concepts: ["OCI specification", "Container registry protocol", "CLI design"],
                        estimatedHours: "6-10"
                    }
                ]
            },
            "build-git": {
                name: "Build Your Own Git",
                description: "Build a version control system that implements core Git operations. Understand content-addressable storage and the Git object model.",
                difficulty: "expert",
                estimatedHours: "30-50",
                prerequisites: ["File I/O and hashing", "Tree data structures", "Basic compression (zlib)", "Graph algorithms"],
                languages: { recommended: ["Python", "Rust", "Go", "C"], also: [] },
                resources: [
                    { name: "Write yourself a Git!", url: "https://wyag.thb.lt/", type: "tutorial" },
                    { name: "CodeCrafters Git Challenge", url: "https://app.codecrafters.io/courses/git/overview", type: "interactive" },
                    { name: "Git Internals - Git Objects", url: "https://git-scm.com/book/en/v2/Git-Internals-Git-Objects", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Repository Initialization",
                        description: "Implement git init - create .git directory structure.",
                        criteria: ["Creates .git directory", "Creates .git/objects directory", "Creates .git/refs/heads directory", "Creates .git/HEAD file pointing to refs/heads/master"],
                        hints: {
                            level1: "mkdir -p for nested directories. HEAD contains 'ref: refs/heads/master'",
                            level2: ".git/objects stores all content. .git/refs stores branch pointers.",
                            level3: ".git structure:\n.git/\n  HEAD           # ref: refs/heads/master\n  objects/       # blob, tree, commit objects\n  refs/\n    heads/       # branch refs\n    tags/        # tag refs"
                        },
                        pitfalls: [],
                        concepts: ["Git repository structure", "References (HEAD, branches)"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 2,
                        name: "Object Storage (Blobs)",
                        description: "Implement hash-object and cat-file for blob objects.",
                        criteria: ["hash-object computes SHA-1 of content", "Stores compressed object in .git/objects/xx/yyyy", "cat-file retrieves and decompresses", "Object format: 'blob {size}\\0{content}'"],
                        hints: {
                            level1: "SHA-1 hash of 'blob {size}\\0{content}'. Store zlib-compressed.",
                            level2: "Object path: first 2 chars of hash = directory, rest = filename.",
                            level3: "def hash_object(data, type='blob'):\n  header = f'{type} {len(data)}\\0'.encode()\n  store = header + data\n  sha = hashlib.sha1(store).hexdigest()\n  path = f'.git/objects/{sha[:2]}/{sha[2:]}'\n  compressed = zlib.compress(store)\n  write_file(path, compressed)\n  return sha"
                        },
                        pitfalls: ["Forgetting null byte between header and content", "Not compressing before storage", "Binary vs text content"],
                        concepts: ["Content-addressable storage", "SHA-1 hashing", "Zlib compression"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Tree Objects",
                        description: "Implement tree objects that represent directory structure.",
                        criteria: ["Tree object stores list of (mode, name, hash) entries", "ls-tree displays tree contents", "write-tree creates tree from working directory", "Nested trees for subdirectories"],
                        hints: {
                            level1: "Tree entries: mode (100644 for file, 40000 for dir), name, 20-byte hash.",
                            level2: "Entries sorted by name. Each entry is binary: mode + space + name + null + hash.",
                            level3: "Tree format:\n\"tree {size}\\0\"\n{mode} {name}\\0{20-byte-sha}\n{mode} {name}\\0{20-byte-sha}\n..."
                        },
                        pitfalls: ["Hash stored as binary (20 bytes), not hex (40 chars)", "Sorting rules", "Mode formatting"],
                        concepts: ["Tree data structure", "Directory representation", "Binary formats"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Commit Objects",
                        description: "Implement commit objects with tree, parent, author, message.",
                        criteria: ["commit-tree creates commit object", "Commit has tree hash, parent(s), author, committer, message", "Timestamp in Unix format with timezone", "Chained commits form history"],
                        hints: {
                            level1: "Commit points to tree. First commit has no parent.",
                            level2: "Format: tree, parent (optional), author, committer, blank line, message.",
                            level3: "Commit format:\ntree {tree-sha}\nparent {parent-sha}  # optional\nauthor {name} <{email}> {timestamp} {tz}\ncommitter {name} <{email}> {timestamp} {tz}\n\n{commit message}"
                        },
                        pitfalls: ["Timestamp format", "Handling merge commits", "Message can be multi-line"],
                        concepts: ["Commit graph", "Directed acyclic graph (DAG)", "Immutable history"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 5,
                        name: "References and Branches",
                        description: "Implement branches as references to commits.",
                        criteria: ["branch creates new branch (ref file)", "branch -d deletes branch", "Branches stored in .git/refs/heads/", "HEAD points to current branch", "Detached HEAD when pointing directly to commit"],
                        hints: {
                            level1: "Branch = file containing commit hash. HEAD = file containing 'ref: refs/heads/xxx'.",
                            level2: "update-ref command safely updates ref files.",
                            level3: "# Create branch\necho {commit-sha} > .git/refs/heads/{branch-name}\n# Switch branch (update HEAD)\necho \"ref: refs/heads/{branch}\" > .git/HEAD"
                        },
                        pitfalls: ["Symbolic refs vs direct refs", "Deleting branch you're on", "Ref file locking"],
                        concepts: ["Git references", "Symbolic references", "Branch management"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 6,
                        name: "Index (Staging Area)",
                        description: "Implement the staging area for preparing commits.",
                        criteria: ["add stages files to index", "status shows staged/unstaged changes", "Index is binary file .git/index", "Stores (mode, sha, flags, path) per entry"],
                        hints: {
                            level1: "Index caches file info for quick status checks.",
                            level2: "Binary format with header, entries, and optional extensions.",
                            level3: "Index format (simplified):\nHeader: DIRC, version (2-4), entry count\nEntries: ctime, mtime, dev, ino, mode, uid, gid, size, sha, flags, path"
                        },
                        pitfalls: ["Index is binary, not text", "Stat info for detecting changes", "Path encoding and sorting"],
                        concepts: ["Staging area concept", "Binary file formats", "File metadata caching"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 7,
                        name: "Diff Algorithm",
                        description: "Implement diff to show changes between versions.",
                        criteria: ["Shows line-by-line differences", "Unified diff format", "diff between working tree and index", "diff between commits"],
                        hints: {
                            level1: "Use Myers diff algorithm or Longest Common Subsequence.",
                            level2: "Unified diff: @@ -start,count +start,count @@ then - and + lines.",
                            level3: "Myers algorithm finds shortest edit script.\nKey insight: d-path on k-diagonal.\nO((N+M)D) where D is edit distance."
                        },
                        pitfalls: ["Binary files", "Line ending differences", "Large files / long diffs"],
                        concepts: ["Diff algorithms (Myers, LCS)", "Edit distance", "Unified diff format"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 8,
                        name: "Merge (Three-way)",
                        description: "Implement three-way merge with conflict detection.",
                        criteria: ["Finds common ancestor (merge base)", "Applies non-conflicting changes automatically", "Marks conflicts with <<<< ==== >>>> markers", "Creates merge commit with two parents"],
                        hints: {
                            level1: "Merge base = lowest common ancestor in commit graph.",
                            level2: "Three-way: if only one side changed, take that. If both changed same, conflict.",
                            level3: "For each file:\nbase = file in merge-base\nours = file in current branch\ntheirs = file in other branch\nif ours == theirs: keep ours\nelif ours == base: take theirs\nelif theirs == base: take ours\nelse: conflict"
                        },
                        pitfalls: ["Finding merge base in complex history", "Handling renames during merge", "Nested conflicts"],
                        concepts: ["Three-way merge", "Merge base calculation", "Conflict resolution"],
                        estimatedHours: "6-10"
                    }
                ]
            },
            "build-shell": {
                name: "Build Your Own Shell",
                description: "Build a Unix shell that can execute commands, handle pipes, redirections, and job control. A fundamental systems programming project.",
                difficulty: "advanced",
                estimatedHours: "25-40",
                prerequisites: ["C programming", "Unix process model (fork, exec)", "File descriptors", "Signal handling basics"],
                languages: { recommended: ["C", "Rust", "Go"], also: [] },
                resources: [
                    { name: "Write a Shell in C", url: "https://brennan.io/2015/01/16/write-a-shell-in-c/", type: "blog" },
                    { name: "CodeCrafters Shell Challenge", url: "https://app.codecrafters.io/courses/shell/overview", type: "interactive" },
                    { name: "GNU Implementing a Shell", url: "https://www.gnu.org/software/libc/manual/html_node/Implementing-a-Shell.html", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic REPL and Command Execution",
                        description: "Read input, parse into command and arguments, execute with fork/exec.",
                        criteria: ["Displays prompt and reads user input", "Parses command and arguments", "Forks child process to execute command", "Parent waits for child to complete", "Handles command not found errors"],
                        hints: {
                            level1: "Use fgets() to read input, strtok() to split by spaces.",
                            level2: "fork() returns 0 in child. In child, call execvp(cmd, args). Parent calls waitpid().",
                            level3: "Basic loop:\nwhile (1) {\n  printf(\"> \");\n  fgets(line, sizeof(line), stdin);\n  char *args[] = parse(line);\n  pid_t pid = fork();\n  if (pid == 0) execvp(args[0], args);\n  else waitpid(pid, &status, 0);\n}"
                        },
                        pitfalls: ["Forgetting to null-terminate args array", "Not handling newline from fgets()", "Zombie processes"],
                        concepts: ["REPL pattern", "fork() and exec() family", "Process creation and waiting"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Built-in Commands",
                        description: "Implement cd, exit, pwd, export, and other built-in commands.",
                        criteria: ["cd changes current directory", "cd with no args goes to $HOME", "exit terminates shell", "pwd prints working directory", "export sets environment variables"],
                        hints: {
                            level1: "Built-ins don't fork - they modify shell state directly.",
                            level2: "Use chdir() for cd, getcwd() for pwd, setenv() for export.",
                            level3: "Check if command is built-in BEFORE forking. Handle in parent process."
                        },
                        pitfalls: ["Trying to cd in child process (only affects child)", "Not expanding ~ to HOME", "Environment variables not inherited"],
                        concepts: ["Built-in vs external commands", "Working directory", "Environment variables"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "I/O Redirection",
                        description: "Implement input (<), output (>), and append (>>) redirection.",
                        criteria: ["cmd > file redirects stdout to file", "cmd >> file appends to file", "cmd < file reads stdin from file", "cmd 2> file redirects stderr", "Combinations work (cmd < in > out)"],
                        hints: {
                            level1: "Use open() to get file descriptor, dup2() to redirect.",
                            level2: "dup2(fd, STDOUT_FILENO) makes stdout point to fd. Do this in child before exec.",
                            level3: "In child, before exec:\nint fd = open(filename, O_WRONLY | O_CREAT | O_TRUNC, 0644);\ndup2(fd, STDOUT_FILENO);\nclose(fd);\nexecvp(cmd, args);"
                        },
                        pitfalls: ["Forgetting to close original fd after dup2", "Wrong open() flags", "File permissions on created files"],
                        concepts: ["File descriptors", "dup2() system call", "Unix I/O model"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 4,
                        name: "Pipes",
                        description: "Implement command pipelines (cmd1 | cmd2 | cmd3).",
                        criteria: ["Two command pipeline works (ls | grep foo)", "Multi-stage pipelines work", "Proper cleanup of file descriptors", "All pipeline stages run concurrently"],
                        hints: {
                            level1: "pipe() creates fd pair. pipe[0] for reading, pipe[1] for writing.",
                            level2: "Fork for each command. Connect stdout of cmd1 to stdin of cmd2 via pipe.",
                            level3: "For cmd1 | cmd2:\nint pipefd[2];\npipe(pipefd);\nif (fork() == 0) {  // cmd1\n  dup2(pipefd[1], STDOUT_FILENO);\n  close(pipefd[0]); close(pipefd[1]);\n  exec(cmd1);\n}\nif (fork() == 0) {  // cmd2\n  dup2(pipefd[0], STDIN_FILENO);\n  close(pipefd[0]); close(pipefd[1]);\n  exec(cmd2);\n}\nclose(pipefd[0]); close(pipefd[1]);\nwait(); wait();"
                        },
                        pitfalls: ["Not closing unused pipe ends (causes hang)", "Parent not closing pipe fds", "Order of fork/close operations"],
                        concepts: ["Unix pipes", "Process communication", "File descriptor inheritance"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 5,
                        name: "Background Jobs",
                        description: "Run commands in background with &, implement job listing.",
                        criteria: ["cmd & runs in background, shell returns immediately", "jobs command lists background processes", "Shell notifies when background job completes", "Background jobs have job numbers [1], [2], etc."],
                        hints: {
                            level1: "Don't waitpid() immediately for background jobs. Store PID in job list.",
                            level2: "Use SIGCHLD handler to detect when background jobs finish.",
                            level3: "Use waitpid(-1, &status, WNOHANG) in SIGCHLD handler to reap zombies."
                        },
                        pitfalls: ["Zombie processes from background jobs", "Race condition in SIGCHLD handler", "Reaping wrong child"],
                        concepts: ["Background execution", "Asynchronous process management", "Zombie processes"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 6,
                        name: "Job Control (fg, bg, Ctrl+Z)",
                        description: "Full job control with suspend, resume, foreground/background.",
                        criteria: ["Ctrl+Z suspends foreground job", "fg brings job to foreground", "bg resumes suspended job in background", "Proper terminal control group management"],
                        hints: {
                            level1: "Handle SIGTSTP (Ctrl+Z). Suspended jobs need to be resumed with SIGCONT.",
                            level2: "Use setpgid() to put job in its own process group. Use tcsetpgrp() for foreground.",
                            level3: "Job control flow:\n1. Create job, put in new process group\n2. If foreground: tcsetpgrp(terminal, job_pgid)\n3. On Ctrl+Z: shell regains terminal, job is stopped\n4. fg: tcsetpgrp to job, send SIGCONT, wait\n5. bg: send SIGCONT, don't wait"
                        },
                        pitfalls: ["Terminal control group confusion", "Orphaned process groups", "Signal handling race conditions"],
                        concepts: ["Process groups", "Terminal control", "Signal handling (SIGTSTP, SIGCONT)"],
                        estimatedHours: "5-8"
                    }
                ]
            },
            "build-text-editor": {
                name: "Build Your Own Text Editor",
                description: "Build a terminal-based text editor from scratch. Based on the kilo editor (~1000 lines of C).",
                difficulty: "advanced",
                estimatedHours: "20-30",
                prerequisites: ["Terminal I/O", "C or systems language", "Basic data structures"],
                languages: { recommended: ["C", "Rust", "Go"], also: [] },
                resources: [
                    { name: "Build Your Own Text Editor", url: "https://viewsourcecode.org/snaptoken/kilo/", type: "tutorial" },
                    { name: "antirez/kilo source", url: "https://github.com/antirez/kilo", type: "reference" },
                    { name: "Hecto (Rust version)", url: "https://philippflenker.com/hecto/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Raw Mode and Input",
                        description: "Put terminal in raw mode and read keypresses.",
                        criteria: ["Disables canonical mode (line buffering)", "Disables echo", "Reads individual keypresses", "Handles Ctrl+Q to quit", "Restores terminal state on exit"],
                        hints: {
                            level1: "Use tcgetattr/tcsetattr to modify terminal settings.",
                            level2: "Disable ICANON and ECHO flags. Save original termios to restore later.",
                            level3: "struct termios raw;\ntcgetattr(STDIN_FILENO, &raw);\nraw.c_lflag &= ~(ECHO | ICANON | ISIG | IEXTEN);\nraw.c_iflag &= ~(IXON | ICRNL | BRKINT | INPCK | ISTRIP);\nraw.c_oflag &= ~(OPOST);\ntcsetattr(STDIN_FILENO, TCSAFLUSH, &raw);"
                        },
                        pitfalls: ["Not restoring terminal on crash", "Ctrl+C killing before cleanup", "Different terminal emulators"],
                        concepts: ["Terminal modes (canonical vs raw)", "termios structure", "Signal handling for cleanup"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Screen Refresh",
                        description: "Clear screen and position cursor using escape sequences.",
                        criteria: ["Clears screen on refresh", "Draws rows with ~ for empty lines", "Positions cursor correctly", "Uses escape sequences (VT100)"],
                        hints: {
                            level1: "\\x1b[2J clears screen. \\x1b[H positions cursor at top-left.",
                            level2: "\\x1b[K clears line from cursor. Build screen in buffer, write once.",
                            level3: "Common escape sequences:\n\\x1b[2J    - Clear entire screen\n\\x1b[H     - Move cursor to 1,1\n\\x1b[{r};{c}H - Move cursor to row,col\n\\x1b[K     - Clear line from cursor\n\\x1b[?25l  - Hide cursor\n\\x1b[?25h  - Show cursor"
                        },
                        pitfalls: ["Flickering (write small chunks vs one big write)", "Off-by-one in cursor positioning (1-indexed)", "Screen size detection"],
                        concepts: ["VT100 escape sequences", "Screen buffers", "Terminal graphics"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "File Viewing",
                        description: "Load and display file contents with scrolling.",
                        criteria: ["Opens file from command line argument", "Displays file contents", "Scrolls with arrow keys", "Shows filename in status bar", "Handles files larger than screen"],
                        hints: {
                            level1: "Store lines in dynamic array. Track row offset for scrolling.",
                            level2: "Only render visible rows (offset to offset+screen_rows).",
                            level3: "typedef struct {\n  char *chars;\n  int size;\n} Row;\nRow *rows;\nint numrows;\nint rowoff;  // scroll offset"
                        },
                        pitfalls: ["Memory allocation for lines", "Horizontal scrolling (long lines)", "Tabs rendering"],
                        concepts: ["File I/O", "Dynamic arrays", "Viewport scrolling"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Text Editing",
                        description: "Insert and delete characters, handle Enter and Backspace.",
                        criteria: ["Insert character at cursor", "Delete with Backspace and Delete keys", "Enter creates new line", "Track 'dirty' state (modified)"],
                        hints: {
                            level1: "Insert: shift chars right, insert at cursor. Delete: shift chars left.",
                            level2: "Enter: split current line at cursor, insert new row.",
                            level3: "void insertChar(Row *row, int at, int c) {\n  row->chars = realloc(row->chars, row->size + 2);\n  memmove(&row->chars[at + 1], &row->chars[at], row->size - at + 1);\n  row->chars[at] = c;\n  row->size++;\n}"
                        },
                        pitfalls: ["Cursor at end of line edge cases", "Deleting at beginning of line (join with previous)", "Memory reallocation"],
                        concepts: ["Text buffer operations", "Gap buffer or simple array", "Change tracking"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 5,
                        name: "Save and Undo",
                        description: "Save file to disk and implement undo functionality.",
                        criteria: ["Ctrl+S saves file", "Prompts for filename if new file", "Confirms before quit with unsaved changes", "Basic undo with Ctrl+Z"],
                        hints: {
                            level1: "Convert rows back to string with newlines, write to file.",
                            level2: "Undo: keep stack of operations. Each edit pushes inverse operation.",
                            level3: "Undo approaches:\n1. Memento: save entire state (memory heavy)\n2. Command pattern: save operations and inverse\n3. Piece table: inherently supports undo"
                        },
                        pitfalls: ["Handling write errors", "Undo across multiple edits", "Memory for undo history"],
                        concepts: ["File writing", "Undo architectures", "Command pattern"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 6,
                        name: "Search",
                        description: "Implement incremental search functionality.",
                        criteria: ["Ctrl+F enters search mode", "Highlights matches as you type", "Enter goes to next match", "Escape cancels search"],
                        hints: {
                            level1: "Simple strstr() for each line.",
                            level2: "Incremental: search on each keypress, restore cursor on cancel.",
                            level3: "For incremental search:\n- Save original cursor position\n- On each char: search forward from current position\n- If found: move cursor, highlight\n- On Enter: stay at match\n- On Escape: restore original position"
                        },
                        pitfalls: ["Search wrapping at end of file", "Case sensitivity", "Restoring state on cancel"],
                        concepts: ["Text searching", "Incremental search UX", "State management"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 7,
                        name: "Syntax Highlighting",
                        description: "Add syntax highlighting for common languages.",
                        criteria: ["Highlights keywords in different color", "Highlights strings and comments", "Different rules per file type", "Multi-line comment handling"],
                        hints: {
                            level1: "For each row, track highlighting state per character.",
                            level2: "State machine: normal, string, comment. Different color per state.",
                            level3: "Highlight types:\n- HL_NORMAL\n- HL_KEYWORD1 (if, while, for)\n- HL_KEYWORD2 (int, char, void)\n- HL_STRING\n- HL_COMMENT\n- HL_MLCOMMENT\n- HL_NUMBER"
                        },
                        pitfalls: ["Multi-line strings and comments", "Escape sequences in strings", "Performance on large files"],
                        concepts: ["Syntax highlighting algorithms", "State machines", "ANSI color codes"],
                        estimatedHours: "4-6"
                    }
                ]
            },
            "build-raytracer": {
                name: "Build Your Own Ray Tracer",
                description: "Build a path tracing renderer following Ray Tracing in One Weekend. Renders photorealistic images with reflections, refractions, and soft shadows.",
                difficulty: "advanced",
                estimatedHours: "20-40",
                prerequisites: ["Linear algebra (vectors, matrices)", "Basic geometry", "Some physics (light, optics)"],
                languages: { recommended: ["C++", "Rust", "Go"], also: ["Python", "JavaScript"] },
                resources: [
                    { name: "Ray Tracing in One Weekend", url: "https://raytracing.github.io/books/RayTracingInOneWeekend.html", type: "book" },
                    { name: "Ray Tracing: The Next Week", url: "https://raytracing.github.io/books/RayTracingTheNextWeek.html", type: "book" },
                    { name: "Physically Based Rendering (PBRT)", url: "https://pbr-book.org/", type: "reference" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Output an Image",
                        description: "Generate a simple PPM image with gradient colors.",
                        criteria: ["Outputs valid PPM image file", "Shows color gradient", "Correct image dimensions", "Opens in image viewer"],
                        hints: {
                            level1: "PPM format: P3 header, width height, max color, then R G B values.",
                            level2: "Iterate y from top to bottom, x from left to right.",
                            level3: "PPM format:\nP3\n{width} {height}\n255\nr g b r g b r g b ..."
                        },
                        pitfalls: [],
                        concepts: ["Image file formats", "Color representation", "Coordinate systems"],
                        estimatedHours: "1"
                    },
                    {
                        id: 2,
                        name: "Ray Class and Background",
                        description: "Define ray class and render background gradient.",
                        criteria: ["Ray has origin and direction", "point_at_t(t) returns position along ray", "Background gradient based on ray direction", "Camera shoots rays through pixel centers"],
                        hints: {
                            level1: "Ray: P(t) = origin + t * direction. t=0 at origin, t=1 at origin+direction.",
                            level2: "Background: lerp between white and blue based on y component of unit direction.",
                            level3: "Color ray_color(Ray& r) {\n  Vec3 unit_dir = normalize(r.direction);\n  float t = 0.5 * (unit_dir.y + 1.0);\n  return (1-t)*white + t*blue;\n}"
                        },
                        pitfalls: [],
                        concepts: ["Ray representation", "Linear interpolation (lerp)", "Camera model basics"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 3,
                        name: "Sphere Intersection",
                        description: "Add a sphere and test ray-sphere intersection.",
                        criteria: ["Sphere defined by center and radius", "Ray-sphere intersection using quadratic formula", "Returns closest positive hit", "Sphere visible in rendered image"],
                        hints: {
                            level1: "Sphere equation: |P - C|^2 = r^2. Substitute ray equation, solve quadratic.",
                            level2: "at^2 + bt + c = 0 where a=d.d, b=2*d.(o-c), c=(o-c).(o-c)-r^2",
                            level3: "discriminant = b*b - 4*a*c\nif (discriminant < 0) return false;\nt = (-b - sqrt(discriminant)) / (2*a);\nif (t < 0) t = (-b + sqrt(discriminant)) / (2*a);"
                        },
                        pitfalls: ["Choosing correct root (closest positive)", "Numeric precision issues", "Sphere behind camera"],
                        concepts: ["Ray-sphere intersection", "Quadratic formula", "Hit detection"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 4,
                        name: "Surface Normals and Multiple Objects",
                        description: "Compute normals and support multiple objects with closest hit.",
                        criteria: ["Normal at hit point computed correctly", "Normals visualized as colors", "Multiple spheres in scene", "Finds closest intersection"],
                        hints: {
                            level1: "Normal at hit point on sphere: (hit_point - center) / radius",
                            level2: "Hittable interface with hit() method. Scene is list of hittables.",
                            level3: "Track t_min, t_max. Update t_max when hit found to find closest."
                        },
                        pitfalls: [],
                        concepts: ["Surface normals", "Object-oriented design", "Closest hit algorithm"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 5,
                        name: "Antialiasing",
                        description: "Add antialiasing by shooting multiple rays per pixel.",
                        criteria: ["Multiple samples per pixel", "Random offset within pixel", "Average color of all samples", "Smooth edges on spheres"],
                        hints: {
                            level1: "For each pixel, shoot N rays with random offset. Average results.",
                            level2: "Offset: (x + random()) / width instead of x / width",
                            level3: "for (int s = 0; s < samples_per_pixel; s++) {\n  float u = (x + random_float()) / (width - 1);\n  float v = (y + random_float()) / (height - 1);\n  color += ray_color(camera.get_ray(u, v));\n}\ncolor /= samples_per_pixel;"
                        },
                        pitfalls: [],
                        concepts: ["Antialiasing", "Monte Carlo sampling", "Random number generation"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 6,
                        name: "Diffuse Materials",
                        description: "Implement Lambertian (diffuse) material.",
                        criteria: ["Scattered ray goes in random hemisphere direction", "Color attenuates with each bounce", "Maximum bounce depth", "Soft shadows from diffuse surfaces"],
                        hints: {
                            level1: "On hit, spawn new ray in random direction in hemisphere around normal.",
                            level2: "Lambertian: direction = normal + random_unit_vector()",
                            level3: "Color ray_color(Ray& r, int depth) {\n  if (depth <= 0) return black;\n  if (world.hit(r, hit_record)) {\n    Vec3 target = hit_point + normal + random_unit_vector();\n    return 0.5 * ray_color(Ray(hit_point, target - hit_point), depth-1);\n  }\n  return background;\n}"
                        },
                        pitfalls: ["Infinite recursion without depth limit", "Shadow acne (self-intersection)", "Correct hemisphere sampling"],
                        concepts: ["Lambertian reflection", "Recursive ray tracing", "Material scattering"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 7,
                        name: "Metal and Reflections",
                        description: "Implement metallic (reflective) materials with fuzz.",
                        criteria: ["Perfect reflections (mirror)", "Fuzzy reflections with adjustable roughness", "Reflection formula: r = d - 2*(d.n)*n", "Material assigned per object"],
                        hints: {
                            level1: "Reflect direction around normal. Incident = incoming ray direction.",
                            level2: "Fuzz: add random vector scaled by fuzziness to reflected direction.",
                            level3: "Vec3 reflect(Vec3& v, Vec3& n) {\n  return v - 2*dot(v, n)*n;\n}\nVec3 scattered = reflect(ray.dir, normal) + fuzz*random_in_unit_sphere();"
                        },
                        pitfalls: [],
                        concepts: ["Specular reflection", "Roughness/fuzz", "Material system"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 8,
                        name: "Dielectrics (Glass)",
                        description: "Implement glass with refraction and Fresnel effects.",
                        criteria: ["Snell's law for refraction", "Total internal reflection at critical angle", "Schlick approximation for Fresnel", "Realistic glass appearance"],
                        hints: {
                            level1: "Snell's law: n1*sin(theta1) = n2*sin(theta2). Glass IOR ~ 1.5",
                            level2: "Total internal reflection when sin(theta2) > 1. Just reflect.",
                            level3: "Schlick approximation for reflectance:\nr0 = ((n1-n2)/(n1+n2))^2\nreflectance = r0 + (1-r0)*(1-cos(theta))^5"
                        },
                        pitfalls: ["Getting the IOR ratio right", "Hollow glass spheres (negative radius trick)", "Total internal reflection check"],
                        concepts: ["Refraction (Snell's law)", "Total internal reflection", "Fresnel equations"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 9,
                        name: "Positionable Camera",
                        description: "Implement camera with position, look-at, and field of view.",
                        criteria: ["Camera positioned anywhere in scene", "Points at specified target", "Adjustable field of view", "Up vector for camera orientation"],
                        hints: {
                            level1: "Camera basis vectors: w (back), u (right), v (up).",
                            level2: "vfov = vertical field of view. viewport_height = 2 * tan(vfov/2)",
                            level3: "w = normalize(lookfrom - lookat);\nu = normalize(cross(vup, w));\nv = cross(w, u);\nhorizontal = viewport_width * u;\nvertical = viewport_height * v;\nlower_left = origin - horizontal/2 - vertical/2 - w;"
                        },
                        pitfalls: [],
                        concepts: ["Camera coordinate system", "Field of view", "Look-at transformation"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 10,
                        name: "Depth of Field",
                        description: "Add defocus blur (depth of field) effect.",
                        criteria: ["Adjustable aperture size", "Focus distance setting", "Objects at focus distance are sharp", "Objects closer/farther are blurred"],
                        hints: {
                            level1: "Random origin on lens disk, ray still goes through focus point.",
                            level2: "Larger aperture = more blur. Aperture = 0 = pinhole camera.",
                            level3: "Vec3 rd = lens_radius * random_in_unit_disk();\nVec3 offset = u * rd.x + v * rd.y;\nreturn Ray(origin + offset, lower_left + s*horizontal + t*vertical - origin - offset);"
                        },
                        pitfalls: [],
                        concepts: ["Thin lens model", "Depth of field", "Aperture and focus"],
                        estimatedHours: "2-3"
                    }
                ]
            },
            "build-sqlite": {
                name: "Build Your Own SQLite",
                description: "Build an embedded SQL database that stores data in a single file. You'll learn about SQL parsing, B-trees, query execution, and transactions.",
                difficulty: "expert",
                estimatedHours: "60-100",
                prerequisites: ["B-tree data structure", "SQL basics", "File I/O and binary formats", "Basic compiler concepts"],
                languages: { recommended: ["C", "Rust", "Go"], also: ["Python", "Java"] },
                resources: [
                    { name: "Let's Build a Simple Database", url: "https://cstack.github.io/db_tutorial/", type: "tutorial" },
                    { name: "CodeCrafters SQLite Challenge", url: "https://app.codecrafters.io/courses/sqlite/overview", type: "interactive" },
                    { name: "SQLite File Format", url: "https://www.sqlite.org/fileformat2.html", type: "documentation" },
                    { name: "CMU 15-445 Database Systems", url: "https://15445.courses.cs.cmu.edu", type: "course" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "SQL Tokenizer",
                        description: "Build a lexer that converts SQL text into tokens.",
                        criteria: ["Tokenizes keywords (SELECT, FROM, WHERE, INSERT, etc.)", "Handles identifiers (table names, column names)", "Parses string and numeric literals", "Handles operators", "Ignores whitespace and comments"],
                        hints: {
                            level1: "Use a state machine or regular expressions for each token type.",
                            level2: "SQL keywords are case-insensitive. Normalize to uppercase.",
                            level3: "Token types to implement:\n- KEYWORD: SELECT, FROM, WHERE, INSERT, UPDATE, DELETE, CREATE\n- IDENTIFIER: table/column names\n- STRING: 'hello'\n- NUMBER: 123, 3.14\n- OPERATOR: =, <>, <, >\n- PUNCTUATION: (, ), ,, ;"
                        },
                        pitfalls: ["Not handling escaped quotes in strings ('it''s')", "Case sensitivity", "Unicode identifiers"],
                        concepts: ["Lexical analysis", "Finite state machines", "Token representation"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "SQL Parser (AST)",
                        description: "Build a parser that converts tokens into an Abstract Syntax Tree.",
                        criteria: ["Parses SELECT statements with columns, FROM, WHERE", "Parses INSERT statements", "Parses CREATE TABLE with column definitions", "Handles expressions", "Reports syntax errors with line/column"],
                        hints: {
                            level1: "Use recursive descent parsing - one function per grammar rule.",
                            level2: "For expressions, use Pratt parsing or precedence climbing for operators.",
                            level3: "Grammar sketch:\nselect := SELECT columns FROM table [WHERE expr]\ncolumns := * | column (, column)*\nexpr := term ((AND|OR) term)*\nterm := value (op value)?"
                        },
                        pitfalls: ["Left recursion in grammar", "Operator precedence (AND vs OR)", "Not handling parentheses"],
                        concepts: ["Recursive descent parsing", "AST design", "Operator precedence"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 3,
                        name: "B-tree Page Format",
                        description: "Implement the on-disk B-tree page structure.",
                        criteria: ["Fixed page size (4096 bytes default)", "Page header with cell count, free space pointer", "Cell pointer array for variable-size cells", "Interior nodes store keys + child page pointers", "Leaf nodes store keys + values"],
                        hints: {
                            level1: "Each page is a node in the B-tree. Start with leaf nodes only.",
                            level2: "Page layout:\n[Header (8-12 bytes)]\n[Cell Pointer Array (2 bytes each)]\n[Free space]\n[Cells (grow from bottom)]",
                            level3: "SQLite uses B+ tree variant where all data is in leaves, interior nodes only have keys."
                        },
                        pitfalls: ["Cell overflow (value too large for page)", "Page fragmentation after deletions", "Endianness"],
                        concepts: ["B-tree structure", "Page-based storage", "Variable-length records"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 4,
                        name: "Table Storage",
                        description: "Store table rows in B-tree leaves with rowid as key.",
                        criteria: ["Each table has its own B-tree", "Rowid is auto-incrementing primary key", "Rows stored as serialized column values", "CREATE TABLE creates new B-tree root page", "Table schema stored in sqlite_master table"],
                        hints: {
                            level1: "Simplest: rowid -> serialized row bytes. Key is rowid, value is row.",
                            level2: "sqlite_master stores schema: CREATE TABLE statements as text.",
                            level3: "Record format (simplified):\n[Header size][Type1][Type2]...[Value1][Value2]...\nTypes: NULL, INT8/16/32/64, FLOAT, BLOB, TEXT"
                        },
                        pitfalls: ["Not handling NULL values", "Rowid gaps after deletes", "Variable-length integer encoding"],
                        concepts: ["Row storage formats", "Schema management", "Type serialization"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 5,
                        name: "SELECT Execution (Table Scan)",
                        description: "Execute SELECT queries by scanning all rows.",
                        criteria: ["SELECT * FROM table returns all rows", "SELECT col1, col2 returns specific columns", "Rows returned in rowid order", "Handle non-existent tables with error"],
                        hints: {
                            level1: "Iterate B-tree leaves from leftmost to rightmost.",
                            level2: "Deserialize each row, project requested columns.",
                            level3: "Use cursor abstraction: open, next, get_column, close."
                        },
                        pitfalls: ["Column name case sensitivity", "NULL handling in output", "Memory management for large result sets"],
                        concepts: ["B-tree traversal", "Projection operator", "Cursor pattern"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 6,
                        name: "INSERT/UPDATE/DELETE",
                        description: "Implement data modification operations.",
                        criteria: ["INSERT INTO table VALUES (...) adds row", "INSERT with column list", "UPDATE table SET col=val WHERE condition", "DELETE FROM table WHERE condition", "Proper rowid assignment for inserts"],
                        hints: {
                            level1: "INSERT: serialize row, insert into B-tree with next rowid.",
                            level2: "DELETE: find matching rows, remove from B-tree, handle rebalancing.",
                            level3: "UPDATE: can be DELETE + INSERT, or in-place if size unchanged."
                        },
                        pitfalls: ["B-tree rebalancing after delete", "Updating primary key", "Constraint violations"],
                        concepts: ["B-tree insertion and deletion", "Record modification", "Constraint checking"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 7,
                        name: "WHERE Clause and Indexes",
                        description: "Implement filtering and secondary indexes.",
                        criteria: ["WHERE with comparison operators", "WHERE with AND/OR", "CREATE INDEX creates secondary B-tree", "Query uses index when beneficial", "EXPLAIN shows query plan"],
                        hints: {
                            level1: "Full table scan with filter: scan all, test predicate, output matches.",
                            level2: "Index B-tree: key=indexed column, value=rowid. Lookup, then fetch row.",
                            level3: "Simple query planner: if WHERE on indexed column, use index seek."
                        },
                        pitfalls: ["Index not used for non-equality predicates", "Maintaining index on INSERT/UPDATE/DELETE", "Choosing between index scan vs table scan"],
                        concepts: ["Secondary indexes", "Query planning basics", "Filter predicates"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 8,
                        name: "Query Planner",
                        description: "Implement cost-based query optimization.",
                        criteria: ["Estimates row counts for tables", "Chooses between table scan and index scan", "Handles simple JOINs", "EXPLAIN QUERY PLAN shows chosen plan"],
                        hints: {
                            level1: "Store row count per table. Estimate selectivity of predicates.",
                            level2: "Cost model: table scan = N rows, index lookup = log(N) + matching rows.",
                            level3: "For JOINs, consider nested loop vs hash join. Start with nested loop."
                        },
                        pitfalls: ["Stale statistics", "Wrong cardinality estimates", "Exponential plan space for many JOINs"],
                        concepts: ["Cost-based optimization", "Cardinality estimation", "Join algorithms"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 9,
                        name: "Transactions (BEGIN/COMMIT/ROLLBACK)",
                        description: "Implement ACID transactions.",
                        criteria: ["BEGIN starts transaction", "COMMIT makes changes permanent", "ROLLBACK undoes all changes since BEGIN", "Changes not visible to other connections until commit", "Crash recovery maintains consistency"],
                        hints: {
                            level1: "Shadow paging: copy pages before modify, commit = swap pointers.",
                            level2: "Or WAL: write changes to separate log, replay on recovery.",
                            level3: "ACID properties:\n- Atomicity: all or nothing (rollback support)\n- Consistency: constraints always valid\n- Isolation: concurrent txns don't interfere\n- Durability: committed = permanent (fsync)"
                        },
                        pitfalls: ["Partial writes (torn pages)", "Lock ordering deadlocks", "Long-running transactions blocking others"],
                        concepts: ["ACID properties", "Shadow paging vs WAL", "Crash recovery"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 10,
                        name: "WAL Mode",
                        description: "Implement Write-Ahead Logging for better concurrency.",
                        criteria: ["WAL file stores changes before applying to main DB", "Readers don't block writers", "Checkpointing merges WAL into main DB", "Crash recovery replays committed WAL entries"],
                        hints: {
                            level1: "WAL append-only: each commit appends frames with changed pages.",
                            level2: "Read: check WAL first (most recent), fallback to main DB.",
                            level3: "WAL advantages over rollback journal:\n- Readers and writer concurrent\n- Faster commits (sequential writes)\n- Better crash recovery"
                        },
                        pitfalls: ["WAL growing unbounded without checkpoint", "Readers pinning old WAL frames", "WAL file corruption detection"],
                        concepts: ["Write-ahead logging", "MVCC basics", "Checkpointing"],
                        estimatedHours: "8-12"
                    }
                ]
            },

            // APPLICATION DEVELOPMENT - BEGINNER
            "todo-app": {
                name: "Todo App",
                description: "Build a classic todo application with CRUD operations. Learn fundamental web development patterns including state management, local storage, and DOM manipulation.",
                difficulty: "beginner",
                estimatedHours: "8-12",
                prerequisites: ["HTML/CSS basics", "JavaScript fundamentals", "DOM manipulation basics"],
                languages: { recommended: ["JavaScript", "TypeScript"], also: ["Python", "React", "Vue"] },
                resources: [
                    { name: "MDN Todo App Tutorial", url: "https://developer.mozilla.org/en-US/docs/Learn/Tools_and_testing/Client-side_JavaScript_frameworks/React_todo_list_beginning", type: "tutorial" },
                    { name: "JavaScript Todo App - freeCodeCamp", url: "https://www.freecodecamp.org/news/how-to-build-a-todo-app-with-javascript/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Project Setup & Basic HTML Structure",
                        description: "Set up the project and create the basic HTML structure for the todo app.",
                        criteria: ["HTML page with input field for new todos", "Add button to create todos", "Empty list container for todos", "Basic CSS styling applied"],
                        hints: {
                            level1: "Start with a simple HTML file with an input, button, and ul element.",
                            level2: "Use semantic HTML: main, section, form elements for better structure.",
                            level3: "HTML structure:\n<form id=\"todo-form\">\n  <input type=\"text\" id=\"todo-input\" placeholder=\"Add a todo...\">\n  <button type=\"submit\">Add</button>\n</form>\n<ul id=\"todo-list\"></ul>"
                        },
                        pitfalls: ["Forgetting to prevent default form submission", "Not using semantic HTML elements", "Hardcoding styles instead of using CSS classes"],
                        concepts: ["HTML forms", "Semantic HTML", "CSS basics"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 2,
                        name: "Add Todo Functionality",
                        description: "Implement the ability to add new todos to the list.",
                        criteria: ["User can type in input field", "Pressing Enter or clicking Add creates new todo", "New todo appears in the list", "Input field clears after adding", "Empty todos are not allowed"],
                        hints: {
                            level1: "Listen for form submit event, get input value, create list item.",
                            level2: "Use createElement or template literals to build todo HTML dynamically.",
                            level3: "function addTodo(text) {\n  const li = document.createElement('li');\n  li.textContent = text;\n  todoList.appendChild(li);\n}"
                        },
                        pitfalls: ["Not trimming whitespace from input", "Memory leaks from not removing event listeners", "XSS vulnerability if using innerHTML without sanitization"],
                        concepts: ["Event handling", "DOM manipulation", "Input validation"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 3,
                        name: "Complete & Delete Todos",
                        description: "Add ability to mark todos as complete and delete them.",
                        criteria: ["Clicking a todo toggles its completed state", "Completed todos have visual distinction (strikethrough)", "Delete button removes todo from list", "Confirmation before delete (optional)"],
                        hints: {
                            level1: "Add click event to toggle a 'completed' class on the todo item.",
                            level2: "Use event delegation: listen on parent, check event.target.",
                            level3: "todoList.addEventListener('click', (e) => {\n  if (e.target.matches('.delete-btn')) {\n    e.target.parentElement.remove();\n  } else if (e.target.matches('li')) {\n    e.target.classList.toggle('completed');\n  }\n});"
                        },
                        pitfalls: ["Adding event listeners to each item (use delegation instead)", "Not handling click on child elements of li", "Removing wrong element from DOM"],
                        concepts: ["Event delegation", "CSS classes for state", "DOM element removal"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 4,
                        name: "Local Storage Persistence",
                        description: "Save todos to localStorage so they persist across page refreshes.",
                        criteria: ["Todos saved to localStorage on every change", "Todos loaded from localStorage on page load", "Completed state persists", "Works even if localStorage is empty"],
                        hints: {
                            level1: "Use JSON.stringify to save array, JSON.parse to load.",
                            level2: "Create a save function, call it after every add/delete/toggle.",
                            level3: "function saveTodos() {\n  const todos = Array.from(todoList.children).map(li => ({\n    text: li.textContent,\n    completed: li.classList.contains('completed')\n  }));\n  localStorage.setItem('todos', JSON.stringify(todos));\n}"
                        },
                        pitfalls: ["Forgetting to handle null from localStorage.getItem", "Not updating localStorage after every change", "Storing DOM elements instead of data"],
                        concepts: ["localStorage API", "JSON serialization", "Data persistence"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 5,
                        name: "Filter & Polish",
                        description: "Add filtering options and polish the user experience.",
                        criteria: ["Filter buttons: All, Active, Completed", "Show count of remaining todos", "Clear completed button", "Smooth animations for add/remove", "Responsive design"],
                        hints: {
                            level1: "Filter by showing/hiding items based on their completed state.",
                            level2: "Keep track of current filter, re-render list when filter changes.",
                            level3: "function filterTodos(filter) {\n  const todos = todoList.querySelectorAll('li');\n  todos.forEach(todo => {\n    const isCompleted = todo.classList.contains('completed');\n    const show = filter === 'all' ||\n      (filter === 'active' && !isCompleted) ||\n      (filter === 'completed' && isCompleted);\n    todo.style.display = show ? '' : 'none';\n  });\n}"
                        },
                        pitfalls: ["Not updating count when filtering", "CSS transitions causing layout shifts", "Accessibility issues with filter buttons"],
                        concepts: ["UI state management", "CSS transitions", "Responsive design"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "weather-app": {
                name: "Weather App",
                description: "Build a weather application that fetches real-time weather data from an API. Learn about async JavaScript, API consumption, and handling loading/error states.",
                difficulty: "beginner",
                estimatedHours: "10-15",
                prerequisites: ["HTML/CSS basics", "JavaScript fundamentals", "Understanding of async/await"],
                languages: { recommended: ["JavaScript", "TypeScript"], also: ["Python", "React", "Vue"] },
                resources: [
                    { name: "OpenWeatherMap API", url: "https://openweathermap.org/api", type: "documentation" },
                    { name: "Build a Weather App with Vanilla JS", url: "https://www.youtube.com/watch?v=WZNG8UomjSI", type: "video" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "API Setup & Basic Fetch",
                        description: "Set up API key and make basic weather API calls.",
                        criteria: ["Sign up for OpenWeatherMap API key", "Successfully fetch weather data for a hardcoded city", "Log weather data to console", "Handle API errors gracefully"],
                        hints: {
                            level1: "Use fetch() with your API key as a query parameter.",
                            level2: "Store API key in a config file, not hardcoded in fetch URL.",
                            level3: "const API_KEY = 'your_api_key';\nasync function getWeather(city) {\n  const response = await fetch(\n    `https://api.openweathermap.org/data/2.5/weather?q=${city}&appid=${API_KEY}&units=metric`\n  );\n  if (!response.ok) throw new Error('City not found');\n  return response.json();\n}"
                        },
                        pitfalls: ["Exposing API key in client-side code (use env vars in production)", "Not handling network errors", "Forgetting units parameter (default is Kelvin)"],
                        concepts: ["REST APIs", "Fetch API", "Async/await", "Error handling"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Search Functionality",
                        description: "Allow users to search for weather by city name.",
                        criteria: ["Search input field for city name", "Search triggers on Enter or button click", "Loading indicator while fetching", "Error message for invalid city"],
                        hints: {
                            level1: "Add event listener for form submit, get input value, call API.",
                            level2: "Show a loading spinner while waiting for API response.",
                            level3: "searchForm.addEventListener('submit', async (e) => {\n  e.preventDefault();\n  const city = cityInput.value.trim();\n  if (!city) return;\n  showLoading();\n  try {\n    const data = await getWeather(city);\n    displayWeather(data);\n  } catch (error) {\n    showError(error.message);\n  } finally {\n    hideLoading();\n  }\n});"
                        },
                        pitfalls: ["Not debouncing rapid searches", "Not clearing previous errors", "Accessibility: missing form labels"],
                        concepts: ["Form handling", "Loading states", "Error states"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Display Weather Data",
                        description: "Create a beautiful UI to display weather information.",
                        criteria: ["Display city name and country", "Show current temperature", "Show weather condition with icon", "Display humidity, wind speed", "Show feels-like temperature"],
                        hints: {
                            level1: "Map API response fields to UI elements.",
                            level2: "Use weather icon codes from API to show appropriate icons.",
                            level3: "function displayWeather(data) {\n  cityEl.textContent = `${data.name}, ${data.sys.country}`;\n  tempEl.textContent = `${Math.round(data.main.temp)}¬∞C`;\n  iconEl.src = `https://openweathermap.org/img/wn/${data.weather[0].icon}@2x.png`;\n  descEl.textContent = data.weather[0].description;\n}"
                        },
                        pitfalls: ["Not handling missing data fields", "Icon URL format changes with API version", "Not rounding temperature values"],
                        concepts: ["Data mapping", "Dynamic content", "API response structure"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 4,
                        name: "Geolocation & Current Location",
                        description: "Use browser geolocation to get weather for current location.",
                        criteria: ["Button to get current location weather", "Request geolocation permission", "Fetch weather using lat/lon coordinates", "Handle permission denied gracefully"],
                        hints: {
                            level1: "Use navigator.geolocation.getCurrentPosition() API.",
                            level2: "API supports lat/lon params instead of city name.",
                            level3: "function getCurrentLocationWeather() {\n  navigator.geolocation.getCurrentPosition(\n    async (position) => {\n      const { latitude, longitude } = position.coords;\n      const data = await getWeatherByCoords(latitude, longitude);\n      displayWeather(data);\n    },\n    (error) => showError('Location access denied')\n  );\n}"
                        },
                        pitfalls: ["Geolocation requires HTTPS in production", "Not handling timeout for slow GPS", "High accuracy option drains battery"],
                        concepts: ["Geolocation API", "Permissions", "Coordinate-based queries"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "calculator": {
                name: "Calculator",
                description: "Build a functional calculator with a clean UI. Learn about UI state management, event handling, and handling edge cases in user input.",
                difficulty: "beginner",
                estimatedHours: "8-12",
                prerequisites: ["HTML/CSS basics", "JavaScript fundamentals"],
                languages: { recommended: ["JavaScript", "TypeScript"], also: ["Python", "React", "Vue"] },
                resources: [
                    { name: "Build a Calculator - The Odin Project", url: "https://www.theodinproject.com/lessons/foundations-calculator", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "UI Layout",
                        description: "Create the calculator interface with buttons and display.",
                        criteria: ["Display area for input and result", "Number buttons (0-9)", "Operator buttons (+, -, *, /)", "Equals and clear buttons", "Responsive grid layout"],
                        hints: {
                            level1: "Use CSS Grid for button layout - perfect for calculators.",
                            level2: "Use data attributes to store button values.",
                            level3: ".calculator {\n  display: grid;\n  grid-template-columns: repeat(4, 1fr);\n  gap: 4px;\n}\n.display { grid-column: 1 / -1; }\n.btn-zero { grid-column: span 2; }"
                        },
                        pitfalls: ["Buttons too small for touch", "Display overflow for long numbers", "Not aligning numbers to the right"],
                        concepts: ["CSS Grid", "Button styling", "Layout design"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Basic Operations",
                        description: "Implement basic arithmetic operations.",
                        criteria: ["Numbers append to display when clicked", "Operators store current value and operation", "Equals performs calculation", "Clear resets calculator", "Only one decimal point allowed"],
                        hints: {
                            level1: "Track: currentValue, previousValue, currentOperator.",
                            level2: "Calculate when equals pressed or new operator selected.",
                            level3: "function calculate(a, b, operator) {\n  switch(operator) {\n    case '+': return a + b;\n    case '-': return a - b;\n    case '*': return a * b;\n    case '/': return b !== 0 ? a / b : 'Error';\n  }\n}"
                        },
                        pitfalls: ["Division by zero", "Multiple decimal points", "Floating point precision (0.1 + 0.2)"],
                        concepts: ["State management", "Arithmetic operations", "Edge case handling"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Chained Operations",
                        description: "Allow chaining multiple operations (e.g., 5 + 3 * 2).",
                        criteria: ["Pressing operator after operator changes operation", "Result shown after each operator press", "Can continue calculating without equals", "Backspace/delete functionality"],
                        hints: {
                            level1: "When new operator pressed, first calculate previous if exists.",
                            level2: "Track display mode: 'input' vs 'result' to know when to clear.",
                            level3: "function handleOperator(op) {\n  if (previousValue && currentOperator && displayMode === 'input') {\n    currentValue = calculate(previousValue, parseFloat(display), currentOperator);\n    updateDisplay(currentValue);\n  }\n  previousValue = parseFloat(display);\n  currentOperator = op;\n  displayMode = 'result';\n}"
                        },
                        pitfalls: ["Losing previous value when changing operators", "Not updating display after intermediate calculation", "Order of operations (this calculator is left-to-right)"],
                        concepts: ["State transitions", "Calculator logic", "User experience"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 4,
                        name: "Keyboard Support & Polish",
                        description: "Add keyboard input and polish the calculator.",
                        criteria: ["Number keys type numbers", "Operator keys work (+, -, *, /)", "Enter = equals, Escape = clear", "Backspace deletes last digit", "Visual feedback on button press"],
                        hints: {
                            level1: "Listen for keydown events, map key to button action.",
                            level2: "Add visual feedback: briefly highlight button on keypress.",
                            level3: "document.addEventListener('keydown', (e) => {\n  if (e.key >= '0' && e.key <= '9') handleNumber(e.key);\n  else if (['+', '-', '*', '/'].includes(e.key)) handleOperator(e.key);\n  else if (e.key === 'Enter') handleEquals();\n  else if (e.key === 'Escape') handleClear();\n  else if (e.key === 'Backspace') handleBackspace();\n});"
                        },
                        pitfalls: ["Preventing default for some keys (like backspace navigating away)", "Different key codes for numpad vs main keys", "Focus management"],
                        concepts: ["Keyboard events", "Event key codes", "Visual feedback"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // GAME DEVELOPMENT - BEGINNER
            "snake": {
                name: "Snake",
                description: "Build the classic Snake game. Learn about grid-based movement, growing arrays, and collision detection with self.",
                difficulty: "beginner",
                estimatedHours: "10-14",
                prerequisites: ["Basic programming", "Array manipulation", "HTML5 Canvas or similar"],
                languages: { recommended: ["JavaScript", "Python", "C#"], also: ["Rust", "Go", "Lua"] },
                resources: [
                    { name: "Snake with JavaScript", url: "https://www.youtube.com/watch?v=7Azlj0f9vas", type: "video" },
                    { name: "Python Snake with Pygame", url: "https://realpython.com/pygame-a-primer/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Grid Setup & Snake Rendering",
                        description: "Set up the grid and render the snake.",
                        criteria: ["Game grid drawn on canvas", "Snake represented as array of segments", "Snake renders as connected blocks", "Grid-based positioning", "Clear visual distinction for head"],
                        hints: {
                            level1: "Snake is array of {x, y} coordinates. Head is index 0.",
                            level2: "Grid cell size (e.g., 20px). Snake position in grid units.",
                            level3: "const GRID_SIZE = 20;\nlet snake = [\n  { x: 15, y: 10 }, // head\n  { x: 14, y: 10 },\n  { x: 13, y: 10 }\n];\n\nfunction draw() {\n  ctx.clearRect(0, 0, canvas.width, canvas.height);\n  snake.forEach((segment, i) => {\n    ctx.fillStyle = i === 0 ? 'darkgreen' : 'green';\n    ctx.fillRect(segment.x * GRID_SIZE, segment.y * GRID_SIZE, GRID_SIZE - 1, GRID_SIZE - 1);\n  });\n}"
                        },
                        pitfalls: ["Drawing snake in wrong order", "Off-by-one in grid positioning", "Not leaving gap between segments"],
                        concepts: ["Grid-based games", "Array representation", "Canvas rendering"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Movement & Direction",
                        description: "Implement snake movement with keyboard controls.",
                        criteria: ["Snake moves automatically in current direction", "Arrow keys change direction", "Cannot reverse direction (no 180¬∞ turns)", "Movement updates at regular interval", "Smooth, grid-aligned movement"],
                        hints: {
                            level1: "Use setInterval for fixed movement timing, not requestAnimationFrame.",
                            level2: "Queue direction changes, apply at next movement tick.",
                            level3: "let direction = { x: 1, y: 0 };\nlet nextDirection = { x: 1, y: 0 };\n\ndocument.addEventListener('keydown', e => {\n  switch(e.key) {\n    case 'ArrowUp': if (direction.y !== 1) nextDirection = {x: 0, y: -1}; break;\n    case 'ArrowDown': if (direction.y !== -1) nextDirection = {x: 0, y: 1}; break;\n    case 'ArrowLeft': if (direction.x !== 1) nextDirection = {x: -1, y: 0}; break;\n    case 'ArrowRight': if (direction.x !== -1) nextDirection = {x: 1, y: 0}; break;\n  }\n});\n\nfunction moveSnake() {\n  direction = nextDirection;\n  const head = { x: snake[0].x + direction.x, y: snake[0].y + direction.y };\n  snake.unshift(head);\n  snake.pop();\n}"
                        },
                        pitfalls: ["Reversing and dying immediately", "Multiple direction changes between ticks", "Movement too fast or slow"],
                        concepts: ["Timer-based updates", "Direction queuing", "Movement constraints"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Food & Growth",
                        description: "Add food spawning and snake growth.",
                        criteria: ["Food appears at random location", "Food not spawned on snake", "Eating food grows snake", "New food spawns after eating", "Score tracking"],
                        hints: {
                            level1: "To grow: don't remove tail when eating food.",
                            level2: "Generate food position, check it's not on snake, regenerate if needed.",
                            level3: "function spawnFood() {\n  let pos;\n  do {\n    pos = {\n      x: Math.floor(Math.random() * GRID_WIDTH),\n      y: Math.floor(Math.random() * GRID_HEIGHT)\n    };\n  } while (snake.some(s => s.x === pos.x && s.y === pos.y));\n  return pos;\n}\n\nfunction moveSnake() {\n  const head = { x: snake[0].x + direction.x, y: snake[0].y + direction.y };\n  snake.unshift(head);\n  if (head.x === food.x && head.y === food.y) {\n    score++;\n    food = spawnFood();\n  } else {\n    snake.pop();\n  }\n}"
                        },
                        pitfalls: ["Food spawning on snake", "Infinite loop if snake fills grid", "Growth happening wrong direction"],
                        concepts: ["Random positioning", "Collision with objects", "Array growth"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 4,
                        name: "Collision & Game Over",
                        description: "Implement collision detection and game over.",
                        criteria: ["Collision with walls ends game", "Collision with self ends game", "Game over screen with score", "Restart option", "High score tracking"],
                        hints: {
                            level1: "Wall collision: head.x < 0 || head.x >= GRID_WIDTH.",
                            level2: "Self collision: check if head matches any other segment.",
                            level3: "function checkCollision() {\n  const head = snake[0];\n  // Wall collision\n  if (head.x < 0 || head.x >= GRID_WIDTH || head.y < 0 || head.y >= GRID_HEIGHT) {\n    return true;\n  }\n  // Self collision (skip head at index 0)\n  for (let i = 1; i < snake.length; i++) {\n    if (head.x === snake[i].x && head.y === snake[i].y) return true;\n  }\n  return false;\n}"
                        },
                        pitfalls: ["Checking collision before movement", "Self collision including head", "High score not persisting"],
                        concepts: ["Self-collision", "Game states", "Local storage"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "pong": {
                name: "Pong",
                description: "Build the classic Pong game. Learn game loop fundamentals, collision detection, and basic game AI.",
                difficulty: "beginner",
                estimatedHours: "8-12",
                prerequisites: ["Basic programming", "HTML5 Canvas or similar graphics"],
                languages: { recommended: ["JavaScript", "Python", "C#"], also: ["Rust", "Go", "Lua"] },
                resources: [
                    { name: "Pong with JavaScript", url: "https://www.youtube.com/watch?v=nl0KXCa5pJk", type: "video" },
                    { name: "HTML5 Canvas Tutorial", url: "https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Game Setup & Ball Movement",
                        description: "Set up the game canvas and implement ball movement.",
                        criteria: ["Canvas renders at 60fps", "Ball appears on screen", "Ball moves in a direction", "Ball bounces off top and bottom walls", "Ball resets when going off sides"],
                        hints: {
                            level1: "Use requestAnimationFrame for smooth 60fps game loop.",
                            level2: "Ball velocity: { vx, vy }. Update position: x += vx, y += vy.",
                            level3: "const ball = { x: 400, y: 300, vx: 5, vy: 3, radius: 10 };\n\nfunction update() {\n  ball.x += ball.vx;\n  ball.y += ball.vy;\n  // Bounce off top/bottom\n  if (ball.y - ball.radius < 0 || ball.y + ball.radius > canvas.height) {\n    ball.vy = -ball.vy;\n  }\n  // Reset if off sides\n  if (ball.x < 0 || ball.x > canvas.width) {\n    resetBall();\n  }\n}"
                        },
                        pitfalls: ["Ball getting stuck in walls", "Variable frame rate affecting speed", "Not clearing canvas each frame"],
                        concepts: ["Game loop", "Velocity and position", "Wall collision"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Paddles & Controls",
                        description: "Add player paddles with keyboard controls.",
                        criteria: ["Two paddles on left and right", "Player 1 uses W/S keys", "Player 2 uses Arrow keys", "Paddles stay within screen bounds", "Smooth paddle movement"],
                        hints: {
                            level1: "Track key states: keyDown sets flag, keyUp clears it.",
                            level2: "Move paddles in update loop based on key states, not in event handler.",
                            level3: "const keys = {};\ndocument.addEventListener('keydown', e => keys[e.key] = true);\ndocument.addEventListener('keyup', e => keys[e.key] = false);\n\nfunction updatePaddles() {\n  if (keys['w'] && paddle1.y > 0) paddle1.y -= paddle1.speed;\n  if (keys['s'] && paddle1.y < canvas.height - paddle1.height) paddle1.y += paddle1.speed;\n}"
                        },
                        pitfalls: ["Paddle going off screen", "Jerky movement from event-based updates", "Key repeat delay issues"],
                        concepts: ["Keyboard input", "Input buffering", "Bounds checking"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Paddle Collision & Scoring",
                        description: "Implement ball-paddle collision and scoring system.",
                        criteria: ["Ball bounces off paddles", "Angle changes based on hit position", "Score increments when ball passes paddle", "Score displayed on screen", "Game resets after score"],
                        hints: {
                            level1: "AABB collision: check if ball rectangle overlaps paddle rectangle.",
                            level2: "Hit position affects angle: hit at edge = steeper angle.",
                            level3: "function checkPaddleCollision(paddle) {\n  if (ball.x - ball.radius < paddle.x + paddle.width &&\n      ball.x + ball.radius > paddle.x &&\n      ball.y > paddle.y && ball.y < paddle.y + paddle.height) {\n    ball.vx = -ball.vx;\n    const hitPos = (ball.y - paddle.y) / paddle.height;\n    const angle = (hitPos - 0.5) * Math.PI / 3;\n    const speed = Math.sqrt(ball.vx * ball.vx + ball.vy * ball.vy);\n    ball.vy = Math.sin(angle) * speed;\n  }\n}"
                        },
                        pitfalls: ["Ball passing through paddle at high speed", "Collision detected multiple times", "Infinite speed increase"],
                        concepts: ["AABB collision", "Hit detection", "Score system"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 4,
                        name: "AI Opponent & Polish",
                        description: "Add single-player mode with AI and polish the game.",
                        criteria: ["AI controls one paddle", "AI difficulty adjustable", "Start/pause menu", "Sound effects", "Win condition (first to 10)"],
                        hints: {
                            level1: "Simple AI: move paddle toward ball Y position.",
                            level2: "Add reaction delay and imperfection for difficulty levels.",
                            level3: "function updateAI() {\n  const targetY = ball.y - paddle2.height / 2;\n  const diff = targetY - paddle2.y;\n  const difficulty = 0.8;\n  const maxSpeed = paddle2.speed * difficulty;\n  if (Math.random() > 0.9) return; // Sometimes don't move\n  if (Math.abs(diff) > 5) {\n    paddle2.y += Math.sign(diff) * Math.min(Math.abs(diff), maxSpeed);\n  }\n}"
                        },
                        pitfalls: ["Perfect AI is unbeatable", "AI reacting to ball behind it", "Sound playing too frequently"],
                        concepts: ["Basic game AI", "Difficulty scaling", "Game states"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "tetris": {
                name: "Tetris",
                description: "Build the classic Tetris game. Learn about piece rotation, line clearing, and more complex game state management.",
                difficulty: "beginner",
                estimatedHours: "15-20",
                prerequisites: ["Basic programming", "2D arrays", "HTML5 Canvas or similar"],
                languages: { recommended: ["JavaScript", "Python", "C#"], also: ["Rust", "Go", "C++"] },
                resources: [
                    { name: "Tetris with JavaScript", url: "https://www.youtube.com/watch?v=rAUn1Lom6dw", type: "video" },
                    { name: "Coding Challenges Tetris", url: "https://codingchallenges.fyi/challenges/challenge-tetris/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Board & Tetrominoes",
                        description: "Create the game board and define tetromino shapes.",
                        criteria: ["10x20 game board", "7 standard tetromino shapes (I, O, T, S, Z, J, L)", "Each shape has distinct color", "Shapes defined as 2D arrays", "Board renders empty grid"],
                        hints: {
                            level1: "Board: 2D array of cell states (0=empty, 1-7=piece colors).",
                            level2: "Define pieces as 2D arrays, use rotation matrices or store all rotations.",
                            level3: "const PIECES = {\n  I: { shape: [[1,1,1,1]], color: 'cyan' },\n  O: { shape: [[1,1],[1,1]], color: 'yellow' },\n  T: { shape: [[0,1,0],[1,1,1]], color: 'purple' },\n  S: { shape: [[0,1,1],[1,1,0]], color: 'green' },\n  Z: { shape: [[1,1,0],[0,1,1]], color: 'red' },\n  J: { shape: [[1,0,0],[1,1,1]], color: 'blue' },\n  L: { shape: [[0,0,1],[1,1,1]], color: 'orange' }\n};"
                        },
                        pitfalls: ["Piece definition orientation inconsistent", "Board dimensions swapped", "Off-by-one in grid rendering"],
                        concepts: ["2D arrays", "Piece representation", "Grid rendering"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Piece Falling & Controls",
                        description: "Implement falling pieces and player controls.",
                        criteria: ["Current piece falls automatically", "Left/Right moves piece", "Down accelerates fall", "Space hard drops", "Cannot move outside board"],
                        hints: {
                            level1: "Track current piece position separately from board state.",
                            level2: "Validate move before applying: check bounds and collisions.",
                            level3: "function isValidPosition(piece, offsetX = 0, offsetY = 0) {\n  const shape = PIECES[piece.type].shape;\n  for (let row = 0; row < shape.length; row++) {\n    for (let col = 0; col < shape[row].length; col++) {\n      if (shape[row][col]) {\n        const newX = piece.x + col + offsetX;\n        const newY = piece.y + row + offsetY;\n        if (newX < 0 || newX >= 10 || newY >= 20) return false;\n        if (newY >= 0 && board[newY][newX]) return false;\n      }\n    }\n  }\n  return true;\n}"
                        },
                        pitfalls: ["Negative Y during spawn", "Piece moving into placed blocks", "Hard drop going through floor"],
                        concepts: ["Position validation", "Input handling", "Collision detection"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Piece Rotation",
                        description: "Implement piece rotation with wall kicks.",
                        criteria: ["Up arrow rotates piece clockwise", "Rotation works for all pieces", "Cannot rotate into walls or pieces", "Wall kick: try offset positions if direct rotation fails", "O piece doesn't rotate"],
                        hints: {
                            level1: "Transpose and reverse rows to rotate 90¬∞ clockwise.",
                            level2: "If rotation fails, try offsets: (1,0), (-1,0), (0,-1).",
                            level3: "function rotateShape(shape) {\n  const rows = shape.length;\n  const cols = shape[0].length;\n  const rotated = Array(cols).fill(null).map(() => Array(rows).fill(0));\n  for (let r = 0; r < rows; r++) {\n    for (let c = 0; c < cols; c++) {\n      rotated[c][rows - 1 - r] = shape[r][c];\n    }\n  }\n  return rotated;\n}"
                        },
                        pitfalls: ["O piece rotation changing position", "I piece wall kick needs special handling", "Rotating into ceiling"],
                        concepts: ["Matrix rotation", "Wall kicks", "SRS (Super Rotation System)"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Line Clearing & Scoring",
                        description: "Implement line clearing and scoring system.",
                        criteria: ["Filled rows are detected", "Filled rows are removed", "Rows above fall down", "Score based on lines cleared (1/2/3/4 = 100/300/500/800)", "Level increases with lines", "Speed increases with level"],
                        hints: {
                            level1: "Check each row: if all cells filled, remove and shift above down.",
                            level2: "Track lines cleared, increase level every 10 lines.",
                            level3: "function clearLines() {\n  let linesCleared = 0;\n  for (let row = board.length - 1; row >= 0; row--) {\n    if (board[row].every(cell => cell !== 0)) {\n      board.splice(row, 1);\n      board.unshift(Array(10).fill(0));\n      linesCleared++;\n      row++;\n    }\n  }\n  if (linesCleared > 0) {\n    const points = [0, 100, 300, 500, 800][linesCleared];\n    score += points * level;\n  }\n}"
                        },
                        pitfalls: ["Iterating wrong direction when removing", "Not re-checking row after splice", "Tetris (4 lines) scoring wrong"],
                        concepts: ["Row clearing", "Score systems", "Difficulty progression"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // APPLICATION DEVELOPMENT - INTERMEDIATE
            "blog-platform": {
                name: "Blog Platform",
                description: "Build a full-featured blog platform with authentication, markdown support, and CRUD operations. Learn full-stack web development fundamentals.",
                difficulty: "intermediate",
                estimatedHours: "25-35",
                prerequisites: ["HTML/CSS/JavaScript", "Basic backend knowledge (Node.js or Python)", "Database basics (SQL or MongoDB)"],
                languages: { recommended: ["JavaScript", "Python", "TypeScript"], also: ["Go", "Ruby", "PHP"] },
                resources: [
                    { name: "Build a Blog with Next.js", url: "https://nextjs.org/learn/basics/create-nextjs-app", type: "tutorial" },
                    { name: "Flask Mega-Tutorial", url: "https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Project Setup & Database Schema",
                        description: "Set up the project structure and design the database.",
                        criteria: ["Project initialized with chosen framework", "Database connection established", "User table (id, email, password_hash, name)", "Post table (id, title, content, author_id, created_at, updated_at)", "Migration system in place"],
                        hints: {
                            level1: "Start with SQLite for development, switch to PostgreSQL for production.",
                            level2: "Use an ORM (Prisma, SQLAlchemy, TypeORM) for database operations.",
                            level3: "// Prisma schema example\nmodel User {\n  id       Int    @id @default(autoincrement())\n  email    String @unique\n  name     String\n  password String\n  posts    Post[]\n}\nmodel Post {\n  id        Int      @id @default(autoincrement())\n  title     String\n  content   String\n  author    User     @relation(fields: [authorId], references: [id])\n  authorId  Int\n  createdAt DateTime @default(now())\n}"
                        },
                        pitfalls: ["Not indexing frequently queried columns", "Storing passwords in plain text", "Not using foreign key constraints"],
                        concepts: ["Database design", "ORM usage", "Migrations"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "User Authentication",
                        description: "Implement user registration and login with secure password handling.",
                        criteria: ["Registration with email/password", "Password hashed with bcrypt", "Login returns JWT or session", "Protected routes require authentication", "Logout functionality"],
                        hints: {
                            level1: "Use bcrypt with cost factor 10-12 for password hashing.",
                            level2: "JWT: sign with secret, include user id and expiry.",
                            level3: "app.post('/login', async (req, res) => {\n  const user = await User.findByEmail(req.body.email);\n  if (!user || !await bcrypt.compare(req.body.password, user.password)) {\n    return res.status(401).json({ error: 'Invalid credentials' });\n  }\n  const token = jwt.sign({ userId: user.id }, SECRET, { expiresIn: '7d' });\n  res.json({ token });\n});"
                        },
                        pitfalls: ["Storing JWT in localStorage (XSS vulnerable)", "Not validating email format", "Exposing whether email exists on failed login"],
                        concepts: ["Password hashing", "JWT authentication", "HTTP-only cookies", "CSRF protection"],
                        estimatedHours: "5-6"
                    },
                    {
                        id: 3,
                        name: "Blog CRUD Operations",
                        description: "Implement create, read, update, delete for blog posts.",
                        criteria: ["Create post with title and content", "View all posts (paginated)", "View single post", "Edit own posts only", "Delete own posts only", "Markdown support for content"],
                        hints: {
                            level1: "Use marked or markdown-it library for markdown rendering.",
                            level2: "Pagination: OFFSET/LIMIT or cursor-based for large datasets.",
                            level3: "app.get('/posts', async (req, res) => {\n  const page = parseInt(req.query.page) || 1;\n  const limit = 10;\n  const offset = (page - 1) * limit;\n  const [posts, total] = await Promise.all([\n    Post.findMany({ skip: offset, take: limit, orderBy: { createdAt: 'desc' } }),\n    Post.count()\n  ]);\n  res.json({ posts, pagination: { page, limit, total, pages: Math.ceil(total / limit) } });\n});"
                        },
                        pitfalls: ["XSS from rendering user markdown (sanitize HTML)", "N+1 queries when loading posts with authors", "Not checking ownership on edit/delete"],
                        concepts: ["CRUD operations", "Authorization", "Markdown rendering", "Pagination"],
                        estimatedHours: "6-8"
                    },
                    {
                        id: 4,
                        name: "Frontend UI",
                        description: "Build the frontend interface for the blog.",
                        criteria: ["Homepage with post list", "Post detail page", "Login/register forms", "Post editor with live preview", "Responsive design"],
                        hints: {
                            level1: "Use a component library or build minimal components.",
                            level2: "Split editor and preview into side-by-side panels.",
                            level3: "function PostEditor({ value, onChange }) {\n  return (\n    <div className=\"editor-container\">\n      <textarea value={value} onChange={(e) => onChange(e.target.value)} className=\"editor\" />\n      <div className=\"preview\" dangerouslySetInnerHTML={{ __html: marked(value) }} />\n    </div>\n  );\n}"
                        },
                        pitfalls: ["Not handling loading states", "No error boundaries", "SEO issues with client-side rendering"],
                        concepts: ["Component architecture", "Form handling", "State management"],
                        estimatedHours: "6-8"
                    }
                ]
            },

            "chat-app": {
                name: "Real-time Chat",
                description: "Build a real-time chat application using WebSockets. Learn about bi-directional communication, presence, and message persistence.",
                difficulty: "intermediate",
                estimatedHours: "25-35",
                prerequisites: ["JavaScript/Node.js", "Basic HTML/CSS", "Understanding of HTTP"],
                languages: { recommended: ["JavaScript", "TypeScript"], also: ["Go", "Python", "Rust"] },
                resources: [
                    { name: "Socket.io Chat Tutorial", url: "https://socket.io/get-started/chat", type: "tutorial" },
                    { name: "WebSocket API - MDN", url: "https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "WebSocket Server Setup",
                        description: "Set up WebSocket server and basic connection handling.",
                        criteria: ["Server accepts WebSocket connections", "Clients can connect and disconnect", "Server logs connection events", "Basic error handling"],
                        hints: {
                            level1: "Use ws library (Node.js) or gorilla/websocket (Go).",
                            level2: "Track connected clients in a Map or Set.",
                            level3: "const WebSocket = require('ws');\nconst wss = new WebSocket.Server({ port: 8080 });\nconst clients = new Set();\n\nwss.on('connection', (ws) => {\n  clients.add(ws);\n  console.log('Client connected', clients.size);\n  ws.on('close', () => {\n    clients.delete(ws);\n    console.log('Client disconnected', clients.size);\n  });\n});"
                        },
                        pitfalls: ["Not handling connection errors", "Memory leaks from not cleaning up closed connections", "Not implementing heartbeat/ping-pong"],
                        concepts: ["WebSocket protocol", "Connection lifecycle", "Event-driven architecture"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Message Broadcasting",
                        description: "Implement sending and receiving messages.",
                        criteria: ["Client can send text messages", "Server broadcasts to all connected clients", "Messages include sender and timestamp", "Message format is JSON"],
                        hints: {
                            level1: "Parse incoming messages as JSON, broadcast to all clients.",
                            level2: "Don't send message back to sender (unless you want echo).",
                            level3: "ws.on('message', (data) => {\n  const message = JSON.parse(data);\n  const broadcast = JSON.stringify({\n    type: 'message',\n    user: message.user,\n    text: message.text,\n    timestamp: Date.now()\n  });\n  clients.forEach(client => {\n    if (client !== ws && client.readyState === WebSocket.OPEN) {\n      client.send(broadcast);\n    }\n  });\n});"
                        },
                        pitfalls: ["Not checking readyState before sending", "Invalid JSON crashing server", "Missing message validation"],
                        concepts: ["Broadcasting pattern", "JSON message protocol", "Error handling"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Chat Rooms",
                        description: "Implement multiple chat rooms/channels.",
                        criteria: ["Users can join/leave rooms", "Messages scoped to room", "List available rooms", "Create new rooms", "Show users in current room"],
                        hints: {
                            level1: "Use a Map: roomName -> Set of client connections.",
                            level2: "Each message includes room name, only broadcast to room members.",
                            level3: "const rooms = new Map();\n\nfunction joinRoom(ws, roomName) {\n  if (!rooms.has(roomName)) {\n    rooms.set(roomName, new Set());\n  }\n  rooms.get(roomName).add(ws);\n  ws.currentRoom = roomName;\n}\n\nfunction broadcastToRoom(roomName, message, sender) {\n  const room = rooms.get(roomName);\n  if (!room) return;\n  room.forEach(client => {\n    if (client !== sender && client.readyState === WebSocket.OPEN) {\n      client.send(JSON.stringify(message));\n    }\n  });\n}"
                        },
                        pitfalls: ["Not removing user from room on disconnect", "Room names not sanitized", "Empty rooms accumulating"],
                        concepts: ["Room-based messaging", "Namespaces", "User presence"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 4,
                        name: "User Authentication & Persistence",
                        description: "Add user authentication and message history.",
                        criteria: ["Users authenticate before connecting", "Messages stored in database", "Load message history on join", "User typing indicators", "Online/offline status"],
                        hints: {
                            level1: "Send JWT token on WebSocket connect, verify before accepting.",
                            level2: "Store last N messages per room, load on join.",
                            level3: "// Typing indicator with debounce\nws.on('message', (data) => {\n  const msg = JSON.parse(data);\n  if (msg.type === 'typing') {\n    broadcastToRoom(ws.currentRoom, { type: 'typing', user: ws.user.name }, ws);\n    clearTimeout(ws.typingTimeout);\n    ws.typingTimeout = setTimeout(() => {\n      broadcastToRoom(ws.currentRoom, { type: 'stopped_typing', user: ws.user.name }, ws);\n    }, 3000);\n  }\n});"
                        },
                        pitfalls: ["Loading too much history (pagination needed)", "Typing indicator spam", "Race conditions with presence"],
                        concepts: ["WebSocket authentication", "Message persistence", "Presence system"],
                        estimatedHours: "6-8"
                    }
                ]
            },

            // DISTRIBUTED & CLOUD - INTERMEDIATE
            "rate-limiter": {
                name: "Rate Limiter",
                description: "Build a rate limiter using the token bucket algorithm. Learn about request throttling and protecting services from abuse.",
                difficulty: "intermediate",
                estimatedHours: "10-15",
                prerequisites: ["Basic web server knowledge", "Concurrency basics", "Time handling"],
                languages: { recommended: ["Python", "Go", "JavaScript"], also: ["Java", "Rust"] },
                resources: [
                    { name: "Token Bucket Algorithm", url: "https://en.wikipedia.org/wiki/Token_bucket", type: "documentation" },
                    { name: "Rate Limiting Strategies", url: "https://blog.bytebytego.com/p/rate-limiting-fundamentals", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Token Bucket Implementation",
                        description: "Implement the core token bucket algorithm.",
                        criteria: ["Bucket has configurable capacity", "Tokens added at configurable rate", "Consume tokens for requests", "Return allow/deny decision", "Thread-safe implementation"],
                        hints: {
                            level1: "Track tokens and last refill time.",
                            level2: "Refill tokens on each request based on elapsed time.",
                            level3: "import time, threading\n\nclass TokenBucket:\n    def __init__(self, capacity, refill_rate):\n        self.capacity = capacity\n        self.tokens = capacity\n        self.refill_rate = refill_rate\n        self.last_refill = time.time()\n        self.lock = threading.Lock()\n\n    def _refill(self):\n        now = time.time()\n        elapsed = now - self.last_refill\n        tokens_to_add = elapsed * self.refill_rate\n        self.tokens = min(self.capacity, self.tokens + tokens_to_add)\n        self.last_refill = now\n\n    def consume(self, tokens=1):\n        with self.lock:\n            self._refill()\n            if self.tokens >= tokens:\n                self.tokens -= tokens\n                return True\n            return False"
                        },
                        pitfalls: ["Race conditions without locking", "Integer overflow in token calculation", "Clock drift issues"],
                        concepts: ["Token bucket algorithm", "Thread safety", "Rate calculations"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Per-Client Rate Limiting",
                        description: "Track rate limits per client (IP or API key).",
                        criteria: ["Each client has own bucket", "Identify clients by IP or key", "Clean up old buckets", "Memory-efficient storage", "Configurable per-client limits"],
                        hints: {
                            level1: "Use dict/map: client_id -> TokenBucket.",
                            level2: "Expire buckets not used for N minutes to save memory.",
                            level3: "class RateLimiter:\n    def __init__(self, capacity, refill_rate):\n        self.capacity = capacity\n        self.refill_rate = refill_rate\n        self.buckets = {}\n        self.lock = threading.Lock()\n\n    def is_allowed(self, client_id):\n        with self.lock:\n            if client_id not in self.buckets:\n                self.buckets[client_id] = TokenBucket(self.capacity, self.refill_rate)\n            return self.buckets[client_id].consume()"
                        },
                        pitfalls: ["Memory leak from never cleaning buckets", "Lock contention under load", "Client spoofing bypassing limits"],
                        concepts: ["Per-client tracking", "Memory management", "Background cleanup"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "HTTP Middleware Integration",
                        description: "Integrate rate limiter as HTTP middleware.",
                        criteria: ["Middleware intercepts requests", "Returns 429 Too Many Requests when limited", "Includes Retry-After header", "X-RateLimit headers show limit/remaining", "Works with Express/Flask/etc"],
                        hints: {
                            level1: "Middleware checks rate limiter, proceeds or returns 429.",
                            level2: "Calculate Retry-After from time until next token.",
                            level3: "# Flask middleware example\n@app.before_request\ndef rate_limit():\n    client_ip = request.remote_addr\n    if not limiter.is_allowed(client_ip):\n        response = jsonify({'error': 'Too many requests'})\n        response.status_code = 429\n        response.headers['Retry-After'] = str(limiter.get_retry_after(client_ip))\n        response.headers['X-RateLimit-Limit'] = str(limiter.capacity)\n        response.headers['X-RateLimit-Remaining'] = '0'\n        return response"
                        },
                        pitfalls: ["Not returning proper status code", "Missing Retry-After header", "Rate limit headers only on 429"],
                        concepts: ["HTTP middleware", "Rate limit headers", "429 response"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 4,
                        name: "Distributed Rate Limiting",
                        description: "Scale rate limiter across multiple server instances.",
                        criteria: ["Rate limit shared across instances", "Redis-backed storage", "Atomic operations", "Handles Redis failures gracefully", "Consistent under high concurrency"],
                        hints: {
                            level1: "Store bucket state in Redis instead of memory.",
                            level2: "Use Redis transactions (MULTI/EXEC) or Lua scripts.",
                            level3: "# Redis Lua script for atomic token bucket\nSCRIPT = '''\nlocal key = KEYS[1]\nlocal capacity = tonumber(ARGV[1])\nlocal refill_rate = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\nlocal requested = tonumber(ARGV[4])\n\nlocal bucket = redis.call('HMGET', key, 'tokens', 'last_refill')\nlocal tokens = tonumber(bucket[1]) or capacity\nlocal last_refill = tonumber(bucket[2]) or now\n\nlocal elapsed = now - last_refill\ntokens = math.min(capacity, tokens + elapsed * refill_rate)\n\nlocal allowed = 0\nif tokens >= requested then\n    tokens = tokens - requested\n    allowed = 1\nend\n\nredis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)\nredis.call('EXPIRE', key, 3600)\nreturn {allowed, tokens}\n'''"
                        },
                        pitfalls: ["Non-atomic read-modify-write", "Redis connection failures", "Clock sync between servers"],
                        concepts: ["Distributed state", "Redis Lua scripts", "Atomic operations"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            "load-balancer-basic": {
                name: "Load Balancer (Basic)",
                description: "Build a basic application load balancer with round-robin distribution. Learn about reverse proxying and server health management.",
                difficulty: "intermediate",
                estimatedHours: "15-20",
                prerequisites: ["HTTP protocol", "TCP networking", "Concurrency basics"],
                languages: { recommended: ["Go", "Python", "JavaScript"], also: ["Rust", "Java"] },
                resources: [
                    { name: "Build Your Own Load Balancer", url: "https://codingchallenges.fyi/challenges/challenge-load-balancer/", type: "tutorial" },
                    { name: "Load Balancer in Go", url: "https://kasvith.me/posts/lets-create-a-simple-lb-go/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "HTTP Proxy Foundation",
                        description: "Build basic HTTP reverse proxy functionality.",
                        criteria: ["Accept incoming HTTP requests", "Forward to single backend server", "Return backend response to client", "Handle connection errors", "Log requests"],
                        hints: {
                            level1: "Read request from client, write to backend, read response, write to client.",
                            level2: "Use HTTP library to handle request/response parsing.",
                            level3: "from flask import Flask, request, Response\nimport requests\n\napp = Flask(__name__)\nBACKEND = 'http://localhost:8001'\n\n@app.route('/<path:path>', methods=['GET', 'POST', 'PUT', 'DELETE'])\ndef proxy(path):\n    url = f'{BACKEND}/{path}'\n    resp = requests.request(\n        method=request.method,\n        url=url,\n        headers={k: v for k, v in request.headers if k != 'Host'},\n        data=request.get_data(),\n        allow_redirects=False\n    )\n    return Response(resp.content, status=resp.status_code, headers=dict(resp.headers))"
                        },
                        pitfalls: ["Not forwarding all headers", "Not handling request body", "Connection timeouts"],
                        concepts: ["Reverse proxy", "HTTP forwarding", "Request/response handling"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Round Robin Distribution",
                        description: "Distribute requests across multiple backends.",
                        criteria: ["Configure list of backend servers", "Round-robin selection algorithm", "Even distribution across backends", "Thread-safe counter", "Skip unavailable backends"],
                        hints: {
                            level1: "Keep counter, increment mod number of backends.",
                            level2: "Use atomic operations for thread safety.",
                            level3: "class RoundRobinBalancer:\n    def __init__(self, backends):\n        self.backends = backends\n        self.current = 0\n        self.lock = threading.Lock()\n\n    def get_next(self):\n        with self.lock:\n            backend = self.backends[self.current]\n            self.current = (self.current + 1) % len(self.backends)\n            return backend"
                        },
                        pitfalls: ["Race condition in counter", "Modulo with zero backends", "Uneven distribution after backend changes"],
                        concepts: ["Round robin algorithm", "Atomic operations", "Backend pool"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Health Checks",
                        description: "Implement active health checking of backends.",
                        criteria: ["Periodic health check requests", "Mark unhealthy backends", "Skip unhealthy backends in routing", "Recover backends when healthy", "Configurable check interval and threshold"],
                        hints: {
                            level1: "Background thread pings each backend periodically.",
                            level2: "Track consecutive failures, mark unhealthy after threshold.",
                            level3: "class HealthChecker:\n    def __init__(self, backends, threshold=3):\n        self.backends = backends\n        self.healthy = {b: True for b in backends}\n        self.failures = {b: 0 for b in backends}\n        self.threshold = threshold\n\n    def check_backend(self, backend):\n        try:\n            resp = requests.get(f'{backend}/health', timeout=2)\n            if resp.status_code == 200:\n                self.failures[backend] = 0\n                self.healthy[backend] = True\n                return\n        except: pass\n        self.failures[backend] += 1\n        if self.failures[backend] >= self.threshold:\n            self.healthy[backend] = False"
                        },
                        pitfalls: ["Health checks overwhelming backends", "No healthy backends available", "Thundering herd on recovery"],
                        concepts: ["Health checking", "Failure detection", "Graceful degradation"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Additional Algorithms",
                        description: "Implement additional load balancing algorithms.",
                        criteria: ["Weighted round robin", "Least connections", "IP hash (sticky sessions)", "Random selection", "Algorithm configurable at runtime"],
                        hints: {
                            level1: "Least connections: track active requests per backend.",
                            level2: "IP hash: hash(client_ip) % len(backends) for consistency.",
                            level3: "class LeastConnectionsBalancer:\n    def __init__(self, backends):\n        self.backends = backends\n        self.connections = {b: 0 for b in backends}\n        self.lock = threading.Lock()\n\n    def get_next(self):\n        with self.lock:\n            healthy = [b for b in self.backends if health_checker.healthy[b]]\n            return min(healthy, key=lambda b: self.connections[b])\n\nclass IPHashBalancer:\n    def get_next(self, client_ip):\n        healthy = [b for b in self.backends if health_checker.healthy[b]]\n        return healthy[hash(client_ip) % len(healthy)]"
                        },
                        pitfalls: ["Least connections not updating on response", "IP hash inconsistent after backend changes", "Weighted round robin integer overflow"],
                        concepts: ["Load balancing algorithms", "Session affinity", "Algorithm tradeoffs"],
                        estimatedHours: "4-5"
                    }
                ]
            },

            // SECURITY - BEGINNER
            "hash-impl": {
                name: "Hash Function (SHA-256)",
                description: "Implement SHA-256 hash function from the NIST specification. Learn about cryptographic primitives and bitwise operations.",
                difficulty: "beginner",
                estimatedHours: "10-15",
                prerequisites: ["Binary and hexadecimal representation", "Bitwise operations", "Basic understanding of cryptography"],
                languages: { recommended: ["Python", "JavaScript", "C"], also: ["Rust", "Go", "Java"] },
                resources: [
                    { name: "SHA-256 Step by Step", url: "https://blog.boot.dev/cryptography/how-sha-2-works-step-by-step-sha-256/", type: "tutorial" },
                    { name: "NIST SHA-256 Specification", url: "https://csrc.nist.gov/publications/detail/fips/180/4/final", type: "specification" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Message Preprocessing",
                        description: "Implement message padding and parsing.",
                        criteria: ["Convert message to binary", "Append '1' bit", "Pad with zeros to 448 mod 512", "Append original length as 64-bit integer", "Parse into 512-bit blocks"],
                        hints: {
                            level1: "Message length in bits must end at 64 bits from 512 boundary.",
                            level2: "Padding: 1 + zeros + 64-bit length. Total = multiple of 512.",
                            level3: "def preprocess(message):\n    if isinstance(message, str):\n        message = message.encode()\n    original_bit_len = len(message) * 8\n    message += b'\\x80'\n    while (len(message) % 64) != 56:\n        message += b'\\x00'\n    message += original_bit_len.to_bytes(8, 'big')\n    return [message[i:i+64] for i in range(0, len(message), 64)]"
                        },
                        pitfalls: ["Wrong bit/byte conversion", "Endianness errors", "Off-by-one in padding calculation"],
                        concepts: ["Bit padding", "Message blocks", "Binary representation"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Message Schedule",
                        description: "Generate the message schedule from each 512-bit block.",
                        criteria: ["Parse 512-bit block into 16 words (32-bit each)", "Extend to 64 words using SHA-256 schedule", "Implement œÉ0 and œÉ1 functions", "Words stored as 32-bit unsigned integers"],
                        hints: {
                            level1: "First 16 words: direct from message block (big-endian).",
                            level2: "Words 16-63: W[i] = œÉ1(W[i-2]) + W[i-7] + œÉ0(W[i-15]) + W[i-16].",
                            level3: "def rotr(x, n):\n    return ((x >> n) | (x << (32 - n))) & 0xffffffff\n\ndef sigma0(x):\n    return rotr(x, 7) ^ rotr(x, 18) ^ (x >> 3)\n\ndef sigma1(x):\n    return rotr(x, 17) ^ rotr(x, 19) ^ (x >> 10)\n\ndef message_schedule(block):\n    W = [int.from_bytes(block[i*4:(i+1)*4], 'big') for i in range(16)]\n    for i in range(16, 64):\n        W.append((sigma1(W[i-2]) + W[i-7] + sigma0(W[i-15]) + W[i-16]) & 0xffffffff)\n    return W"
                        },
                        pitfalls: ["Not masking to 32 bits", "Right rotate vs right shift", "Wrong œÉ function parameters"],
                        concepts: ["Rotate operations", "XOR operations", "Word expansion"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Compression Function",
                        description: "Implement the main compression function.",
                        criteria: ["Initialize working variables (a-h) from hash values", "64 rounds of compression", "Implement Œ£0, Œ£1, Ch, Maj functions", "Use correct K constants", "Update hash values after compression"],
                        hints: {
                            level1: "Each round: T1 = h + Œ£1(e) + Ch(e,f,g) + K[i] + W[i].",
                            level2: "Ch(x,y,z) = (x AND y) XOR (NOT x AND z). Maj(x,y,z) = (x AND y) XOR (x AND z) XOR (y AND z).",
                            level3: "def compress(H, W):\n    a, b, c, d, e, f, g, h = H\n    for i in range(64):\n        S1 = rotr(e, 6) ^ rotr(e, 11) ^ rotr(e, 25)\n        ch = (e & f) ^ (~e & g)\n        T1 = (h + S1 + ch + K[i] + W[i]) & 0xffffffff\n        S0 = rotr(a, 2) ^ rotr(a, 13) ^ rotr(a, 22)\n        maj = (a & b) ^ (a & c) ^ (b & c)\n        T2 = (S0 + maj) & 0xffffffff\n        h, g, f, e, d, c, b, a = g, f, e, (d + T1) & 0xffffffff, c, b, a, (T1 + T2) & 0xffffffff\n    return [(H[i] + v) & 0xffffffff for i, v in enumerate([a,b,c,d,e,f,g,h])]"
                        },
                        pitfalls: ["Mixing up Œ£ and œÉ functions", "Wrong K constant values", "Not masking intermediate results"],
                        concepts: ["Compression function", "Round functions", "Hash state"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 4,
                        name: "Final Hash Output",
                        description: "Produce the final 256-bit hash output.",
                        criteria: ["Process all message blocks", "Concatenate final hash values", "Output as 64-character hex string", "Test against known vectors", "Handle empty input correctly"],
                        hints: {
                            level1: "Process blocks sequentially, each updates hash state.",
                            level2: "Final hash: concatenate all 8 hash values as big-endian.",
                            level3: "def sha256(message):\n    blocks = preprocess(message)\n    H = initial_hash_values.copy()\n    for block in blocks:\n        W = message_schedule(block)\n        H = compress(H, W)\n    return ''.join(h.to_bytes(4, 'big').hex() for h in H)\n\n# Test vectors\nassert sha256('') == 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\nassert sha256('abc') == 'ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad'"
                        },
                        pitfalls: ["Wrong hash for empty string", "Endianness in output", "Not resetting state between calls"],
                        concepts: ["Hash finalization", "Test vectors", "Hex encoding"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // CS FUNDAMENTALS - BEGINNER
            "linked-list": {
                name: "Linked List",
                description: "Implement singly, doubly, and circular linked lists. Understand pointer manipulation and memory management.",
                difficulty: "beginner",
                estimatedHours: "8-12",
                prerequisites: ["Basic programming", "Understanding of pointers/references"],
                languages: { recommended: ["Python", "JavaScript", "C", "Java"], also: ["Rust", "Go", "C++"] },
                resources: [
                    { name: "Linked List - GeeksforGeeks", url: "https://www.geeksforgeeks.org/linked-list-data-structure/", type: "tutorial" },
                    { name: "Visualgo - Linked List", url: "https://visualgo.net/en/list", type: "interactive" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Singly Linked List Basics",
                        description: "Implement a basic singly linked list with insert and traversal.",
                        criteria: ["Node class with value and next pointer", "LinkedList class with head reference", "Insert at beginning (prepend)", "Insert at end (append)", "Print/traverse list", "Get length"],
                        hints: {
                            level1: "Node has two properties: value and next (reference to next node).",
                            level2: "Keep track of head. For append, traverse to end first.",
                            level3: "class Node:\n    def __init__(self, value):\n        self.value = value\n        self.next = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def prepend(self, value):\n        new_node = Node(value)\n        new_node.next = self.head\n        self.head = new_node\n\n    def append(self, value):\n        new_node = Node(value)\n        if not self.head:\n            self.head = new_node\n            return\n        current = self.head\n        while current.next:\n            current = current.next\n        current.next = new_node"
                        },
                        pitfalls: ["Forgetting to handle empty list", "Losing reference to head", "Off-by-one in traversal"],
                        concepts: ["Node structure", "Head pointer", "Traversal"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Search & Delete",
                        description: "Implement search and delete operations.",
                        criteria: ["Find node by value", "Delete node by value", "Delete at index", "Handle not found cases", "Handle single-element list"],
                        hints: {
                            level1: "To delete, keep track of previous node.",
                            level2: "Special case: deleting head node.",
                            level3: "def find(self, value):\n    current = self.head\n    while current:\n        if current.value == value:\n            return current\n        current = current.next\n    return None\n\ndef delete(self, value):\n    if not self.head:\n        return False\n    if self.head.value == value:\n        self.head = self.head.next\n        return True\n    current = self.head\n    while current.next:\n        if current.next.value == value:\n            current.next = current.next.next\n            return True\n        current = current.next\n    return False"
                        },
                        pitfalls: ["Not handling delete at head", "Memory leak (in manual memory languages)", "Deleting wrong node"],
                        concepts: ["Two-pointer technique", "Edge cases", "Memory management"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Doubly Linked List",
                        description: "Implement a doubly linked list with bidirectional traversal.",
                        criteria: ["Node has prev and next pointers", "Maintain both head and tail", "Insert at beginning and end O(1)", "Delete from both ends O(1)", "Traverse forward and backward"],
                        hints: {
                            level1: "Each node has three fields: value, prev, next.",
                            level2: "Keep tail pointer for O(1) append.",
                            level3: "class DoublyNode:\n    def __init__(self, value):\n        self.value = value\n        self.prev = None\n        self.next = None\n\nclass DoublyLinkedList:\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    def append(self, value):\n        new_node = DoublyNode(value)\n        if not self.tail:\n            self.head = self.tail = new_node\n        else:\n            new_node.prev = self.tail\n            self.tail.next = new_node\n            self.tail = new_node"
                        },
                        pitfalls: ["Not updating both prev and next", "Forgetting to update tail", "Inconsistent head/tail on single element"],
                        concepts: ["Bidirectional links", "Head and tail pointers", "Constant time operations"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 4,
                        name: "Circular & Advanced Operations",
                        description: "Implement circular linked list and advanced operations.",
                        criteria: ["Circular linked list (last points to first)", "Reverse a linked list", "Detect cycle (Floyd's algorithm)", "Find middle element", "Merge two sorted lists"],
                        hints: {
                            level1: "Circular: tail.next = head instead of null.",
                            level2: "Reverse: three pointers - prev, current, next.",
                            level3: "def reverse(self):\n    prev = None\n    current = self.head\n    while current:\n        next_node = current.next\n        current.next = prev\n        prev = current\n        current = next_node\n    self.head = prev\n\ndef has_cycle(self):\n    slow = fast = self.head\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n        if slow == fast:\n            return True\n    return False\n\ndef find_middle(self):\n    slow = fast = self.head\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n    return slow"
                        },
                        pitfalls: ["Infinite loop in circular list traversal", "Reverse losing nodes", "Cycle detection on empty list"],
                        concepts: ["Circular structures", "In-place reversal", "Floyd's cycle detection", "Two-pointer technique"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // COMPILERS - BEGINNER
            "json-parser": {
                name: "JSON Parser",
                description: "Build a JSON parser using recursive descent parsing. Learn about tokenization and AST construction.",
                difficulty: "beginner",
                estimatedHours: "8-12",
                prerequisites: ["Basic programming", "Understanding of JSON format"],
                languages: { recommended: ["Python", "JavaScript", "C"], also: ["Rust", "Go"] },
                resources: [
                    { name: "JSON Specification", url: "https://www.json.org/json-en.html", type: "specification" },
                    { name: "Crafting Interpreters", url: "https://craftinginterpreters.com/", type: "book" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Tokenizer",
                        description: "Build a lexer that converts JSON string into tokens.",
                        criteria: ["Tokenize strings, numbers, booleans, null", "Tokenize punctuation ({, }, [, ], :, ,)", "Skip whitespace", "Handle escape sequences in strings"],
                        hints: {
                            level1: "Read char by char, emit tokens based on current char.",
                            level2: "Handle escape sequences in strings: \\n, \\t, \\\\, etc.",
                            level3: "class Tokenizer:\n    def __init__(self, text):\n        self.text = text\n        self.pos = 0\n\n    def tokenize(self):\n        tokens = []\n        while self.pos < len(self.text):\n            char = self.text[self.pos]\n            if char.isspace():\n                self.pos += 1\n            elif char == '\"':\n                tokens.append(self.read_string())\n            elif char.isdigit() or char == '-':\n                tokens.append(self.read_number())\n            elif char in '{}[]:,':\n                tokens.append(('PUNCT', char))\n                self.pos += 1\n            elif self.text[self.pos:self.pos+4] == 'true':\n                tokens.append(('BOOL', True))\n                self.pos += 4\n        return tokens"
                        },
                        pitfalls: ["Escape sequences in strings", "Negative numbers", "Scientific notation"],
                        concepts: ["Lexical analysis", "Token types", "State machine"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Parser",
                        description: "Parse tokens into native data structures.",
                        criteria: ["Parse objects and arrays", "Handle nested structures", "Return native data types", "Handle syntax errors gracefully"],
                        hints: {
                            level1: "Recursive descent: parse_value calls parse_object/parse_array.",
                            level2: "Object: {key: value, ...}. Array: [value, ...].",
                            level3: "class Parser:\n    def parse_value(self):\n        token = self.current()\n        if token[0] == 'PUNCT' and token[1] == '{':\n            return self.parse_object()\n        elif token[0] == 'PUNCT' and token[1] == '[':\n            return self.parse_array()\n        elif token[0] == 'STRING':\n            self.advance()\n            return token[1]\n        elif token[0] == 'NUMBER':\n            self.advance()\n            return token[1]\n        elif token[0] == 'BOOL':\n            self.advance()\n            return token[1]\n\n    def parse_object(self):\n        self.expect('{')\n        obj = {}\n        while self.current()[1] != '}':\n            key = self.expect_string()\n            self.expect(':')\n            value = self.parse_value()\n            obj[key] = value\n            if self.current()[1] == ',':\n                self.advance()\n        self.expect('}')\n        return obj"
                        },
                        pitfalls: ["Trailing comma handling", "Deep nesting stack overflow", "Empty object/array edge case"],
                        concepts: ["Recursive descent", "Grammar rules", "AST construction"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Error Handling & Edge Cases",
                        description: "Add robust error handling and edge case support.",
                        criteria: ["Meaningful error messages with position", "Handle all JSON types correctly", "Validate number format", "Unicode string support"],
                        hints: {
                            level1: "Track line and column for error messages.",
                            level2: "Number format: optional minus, integer or decimal, optional exponent.",
                            level3: "class JSONError(Exception):\n    def __init__(self, message, line, col):\n        super().__init__(f'{message} at line {line}, column {col}')\n        self.line = line\n        self.col = col\n\ndef read_number(self):\n    start = self.pos\n    if self.current_char() == '-':\n        self.advance()\n    if self.current_char() == '0':\n        self.advance()\n    else:\n        while self.current_char().isdigit():\n            self.advance()\n    if self.current_char() == '.':\n        self.advance()\n        while self.current_char().isdigit():\n            self.advance()\n    if self.current_char() in 'eE':\n        self.advance()\n        if self.current_char() in '+-':\n            self.advance()\n        while self.current_char().isdigit():\n            self.advance()\n    return float(self.text[start:self.pos])"
                        },
                        pitfalls: ["Numbers with leading zeros (invalid in JSON)", "Unicode escapes \\u0000", "Duplicate keys in objects"],
                        concepts: ["Error recovery", "JSON specification compliance", "Unicode handling"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // APPLICATION DEVELOPMENT - ADVANCED
            "social-network": {
                name: "Social Network",
                description: "Build a social network with user profiles, feeds, followers, and notifications. Learn about complex data relationships and real-time features at scale.",
                difficulty: "advanced",
                estimatedHours: "60-80",
                prerequisites: ["Full-stack web development", "Database design", "Caching concepts", "Real-time systems"],
                languages: { recommended: ["JavaScript", "Python", "Go"], also: ["Ruby", "Java", "Elixir"] },
                resources: [
                    { name: "Feed Architecture", url: "https://www.youtube.com/watch?v=QmX2NPkJTKg", type: "video" },
                    { name: "System Design Social Network", url: "https://www.youtube.com/results?search_query=system+design+social+network", type: "video" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "User Profiles & Follow System",
                        description: "Build user profiles and the follow/follower relationship.",
                        criteria: ["User profile with bio, avatar, links", "Follow/unfollow users", "Follower and following lists", "Follower/following counts", "Profile editing"],
                        hints: {
                            level1: "Follow is a self-referential many-to-many relationship.",
                            level2: "Store counts denormalized for performance.",
                            level3: "// Follow relationship\nmodel Follow {\n  id          Int      @id @default(autoincrement())\n  followerId  Int\n  followingId Int\n  createdAt   DateTime @default(now())\n  follower    User     @relation(\"follower\", ...)\n  following   User     @relation(\"following\", ...)\n  @@unique([followerId, followingId])\n  @@index([followingId])\n}\n\n// Denormalized counts on User\nmodel User {\n  followerCount  Int @default(0)\n  followingCount Int @default(0)\n}"
                        },
                        pitfalls: ["Self-follow allowed", "Count drift from denormalization", "N+1 queries for follower lists"],
                        concepts: ["Self-referential relations", "Denormalization", "Count caching"],
                        estimatedHours: "8-10"
                    },
                    {
                        id: 2,
                        name: "Posts & Feed (Fan-out on Write)",
                        description: "Implement posts and the home feed using fan-out on write.",
                        criteria: ["Create text/image posts", "User's own post list", "Home feed (posts from followed users)", "Feed pagination (cursor-based)", "Post timestamps"],
                        hints: {
                            level1: "Fan-out on write: when user posts, add to each follower's feed.",
                            level2: "Use a Feed table: { userId, postId, createdAt }.",
                            level3: "async function createPost(authorId, content) {\n  const post = await Post.create({ authorId, content });\n  const followers = await Follow.findMany({\n    where: { followingId: authorId },\n    select: { followerId: true }\n  });\n  await FeedItem.createMany({\n    data: followers.map(f => ({\n      userId: f.followerId,\n      postId: post.id,\n      createdAt: post.createdAt\n    }))\n  });\n  return post;\n}"
                        },
                        pitfalls: ["Fan-out blocking post creation (use queue)", "Celebrity problem (millions of followers)", "Offset pagination performance"],
                        concepts: ["Fan-out on write vs read", "Feed generation", "Cursor pagination"],
                        estimatedHours: "10-12"
                    },
                    {
                        id: 3,
                        name: "Likes, Comments & Interactions",
                        description: "Add social interactions to posts.",
                        criteria: ["Like/unlike posts", "Comment on posts", "Like counts (real-time update)", "Comment threads", "Share/repost"],
                        hints: {
                            level1: "Like: simple user-post relation. Comment: text + user + post.",
                            level2: "Use optimistic UI updates for likes.",
                            level3: "async function likePost(postId) {\n  // Optimistic update\n  setLiked(true);\n  setLikeCount(prev => prev + 1);\n  try {\n    await api.post(`/posts/${postId}/like`);\n  } catch (error) {\n    // Rollback on error\n    setLiked(false);\n    setLikeCount(prev => prev - 1);\n  }\n}"
                        },
                        pitfalls: ["Double-like race condition", "Comment ordering (newest vs oldest)", "Deep nested replies complexity"],
                        concepts: ["Social interactions", "Optimistic updates", "Comment systems"],
                        estimatedHours: "8-10"
                    },
                    {
                        id: 4,
                        name: "Notifications",
                        description: "Build a notification system for social activity.",
                        criteria: ["Notification for new follower", "Notification for likes/comments", "Notification badge count", "Mark as read", "Real-time notification delivery"],
                        hints: {
                            level1: "Notification table: { userId, type, data, read, createdAt }.",
                            level2: "Use WebSocket or SSE for real-time delivery.",
                            level3: "model Notification {\n  id        Int      @id\n  userId    Int      // recipient\n  type      String   // 'follow', 'like', 'comment'\n  actorId   Int      // who triggered it\n  targetId  Int?     // post id for likes/comments\n  read      Boolean  @default(false)\n  createdAt DateTime @default(now())\n  @@index([userId, read, createdAt])\n}\n\nasync function createLikeNotification(postId, likerId) {\n  const post = await Post.findUnique({ where: { id: postId } });\n  if (post.authorId === likerId) return;\n  await Notification.create({\n    data: { userId: post.authorId, type: 'like', actorId: likerId, targetId: postId }\n  });\n  notificationService.push(post.authorId, { type: 'like', ... });\n}"
                        },
                        pitfalls: ["Notification spam (batch similar notifications)", "Notifying yourself", "Notification count going negative"],
                        concepts: ["Notification system", "Real-time delivery", "Batching"],
                        estimatedHours: "8-10"
                    },
                    {
                        id: 5,
                        name: "Search & Discovery",
                        description: "Add search and content discovery features.",
                        criteria: ["Search users by name/username", "Search posts by content", "Trending posts/hashtags", "Suggested users to follow", "Explore page"],
                        hints: {
                            level1: "Simple search with LIKE. Use Elasticsearch for scale.",
                            level2: "Trending: count hashtags/posts in sliding window.",
                            level3: "async function getSuggestedUsers(userId) {\n  // Users followed by people I follow, that I don't follow\n  const suggestions = await prisma.$queryRaw`\n    SELECT u.*, COUNT(*) as mutual\n    FROM users u\n    JOIN follows f1 ON f1.following_id = u.id\n    JOIN follows f2 ON f2.follower_id = f1.follower_id\n    WHERE f2.following_id = ${userId}\n      AND u.id != ${userId}\n      AND u.id NOT IN (\n        SELECT following_id FROM follows WHERE follower_id = ${userId}\n      )\n    GROUP BY u.id\n    ORDER BY mutual DESC\n    LIMIT 10\n  `;\n  return suggestions;\n}"
                        },
                        pitfalls: ["Search too slow on large datasets", "Trending manipulation (spam)", "Recommendation bubbles"],
                        concepts: ["Search implementation", "Trending algorithms", "Recommendation systems"],
                        estimatedHours: "10-12"
                    },
                    {
                        id: 6,
                        name: "Performance & Scaling",
                        description: "Optimize for performance and prepare for scale.",
                        criteria: ["Redis caching for feeds", "Background job processing", "CDN for media", "Database indexing optimized", "Load testing done"],
                        hints: {
                            level1: "Cache hot data: user profiles, feed, follower counts.",
                            level2: "Use background jobs for fan-out, notifications, emails.",
                            level3: "async function getFeed(userId, cursor) {\n  const cacheKey = `feed:${userId}`;\n  let feedIds = await redis.zrevrange(cacheKey, 0, 19);\n  if (!feedIds.length) {\n    const feed = await FeedItem.findMany({\n      where: { userId },\n      orderBy: { createdAt: 'desc' },\n      take: 100\n    });\n    await redis.zadd(cacheKey, ...feed.flatMap(f => [f.createdAt.getTime(), f.postId]));\n    feedIds = feed.map(f => f.postId).slice(0, 20);\n  }\n  return Post.findMany({ where: { id: { in: feedIds } } });\n}"
                        },
                        pitfalls: ["Cache invalidation complexity", "Celebrity user problem", "Hot partition on popular content"],
                        concepts: ["Caching strategies", "Background jobs", "Performance optimization"],
                        estimatedHours: "12-15"
                    }
                ]
            },

            // AI/ML - BEGINNER
            "linear-regression": {
                name: "Linear Regression",
                description: "Implement linear regression with gradient descent from scratch. Understand the fundamentals of machine learning optimization.",
                difficulty: "beginner",
                estimatedHours: "8-12",
                prerequisites: ["Basic Python/NumPy", "High school math (derivatives)"],
                languages: { recommended: ["Python"], also: ["JavaScript", "Julia"] },
                resources: [
                    { name: "Linear Regression Tutorial", url: "https://scikit-learn.org/stable/modules/linear_model.html", type: "documentation" },
                    { name: "Gradient Descent Explained", url: "https://www.youtube.com/watch?v=sDv4f4s2SB8", type: "video" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Simple Linear Regression",
                        description: "Implement single-variable linear regression.",
                        criteria: ["Model y = mx + b", "Fit using closed-form solution", "Calculate predictions", "Calculate R¬≤ score"],
                        hints: {
                            level1: "Closed-form: m = cov(x,y)/var(x), b = mean(y) - m*mean(x)",
                            level2: "Use numpy for vectorized operations.",
                            level3: "def fit(X, y):\n    x_mean, y_mean = np.mean(X), np.mean(y)\n    numerator = np.sum((X - x_mean) * (y - y_mean))\n    denominator = np.sum((X - x_mean) ** 2)\n    m = numerator / denominator\n    b = y_mean - m * x_mean\n    return m, b\n\ndef r2_score(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    return 1 - (ss_res / ss_tot)"
                        },
                        pitfalls: ["Division by zero with constant X", "Data not being proper arrays", "Using float instead of numpy"],
                        concepts: ["Linear relationship", "Least squares", "Model evaluation"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Gradient Descent",
                        description: "Implement gradient descent optimization.",
                        criteria: ["Cost function (MSE)", "Gradient calculation", "Iterative parameter update", "Convergence detection"],
                        hints: {
                            level1: "Cost = (1/n) * sum((y_pred - y)¬≤)",
                            level2: "Update: param = param - learning_rate * gradient",
                            level3: "def gradient_descent(X, y, learning_rate=0.01, iterations=1000):\n    m, b = 0, 0\n    n = len(X)\n    for _ in range(iterations):\n        y_pred = m * X + b\n        dm = -(2/n) * np.sum(X * (y - y_pred))\n        db = -(2/n) * np.sum(y - y_pred)\n        m -= learning_rate * dm\n        b -= learning_rate * db\n    return m, b"
                        },
                        pitfalls: ["Learning rate too high (divergence)", "Not normalizing features", "Stopping too early"],
                        concepts: ["Gradient descent", "Learning rate", "Cost function"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Multiple Linear Regression",
                        description: "Extend to multiple input features.",
                        criteria: ["Handle multiple features", "Matrix form: y = Xw", "Vectorized gradient descent", "Feature normalization"],
                        hints: {
                            level1: "Add bias term as column of 1s in X matrix.",
                            level2: "Gradient: ‚àáJ = (1/n) * X^T * (Xw - y)",
                            level3: "def multi_gradient_descent(X, y, lr=0.01, iters=1000):\n    X = np.c_[np.ones(len(X)), X]  # Add bias column\n    w = np.zeros(X.shape[1])\n    n = len(y)\n    for _ in range(iters):\n        y_pred = X @ w\n        gradient = (1/n) * (X.T @ (y_pred - y))\n        w -= lr * gradient\n    return w"
                        },
                        pitfalls: ["Features on different scales", "Matrix dimension mismatches", "Forgetting bias term"],
                        concepts: ["Matrix operations", "Feature scaling", "Vectorization"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "knn": {
                name: "KNN Classifier",
                description: "Implement K-Nearest Neighbors classification algorithm. Learn about distance metrics and non-parametric models.",
                difficulty: "beginner",
                estimatedHours: "6-10",
                prerequisites: ["Basic Python/NumPy", "Distance formulas"],
                languages: { recommended: ["Python"], also: ["JavaScript", "Julia"] },
                resources: [
                    { name: "KNN Explained", url: "https://scikit-learn.org/stable/modules/neighbors.html", type: "documentation" },
                    { name: "KNN from Scratch", url: "https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Distance Calculation",
                        description: "Implement distance metrics for KNN.",
                        criteria: ["Euclidean distance", "Manhattan distance", "Distance to all training points", "Efficient computation"],
                        hints: {
                            level1: "Euclidean: sqrt(sum((a-b)¬≤))",
                            level2: "Manhattan: sum(|a-b|)",
                            level3: "def euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b) ** 2))\n\ndef manhattan_distance(a, b):\n    return np.sum(np.abs(a - b))\n\ndef compute_distances(X_train, x_test, metric='euclidean'):\n    distances = []\n    for x_train in X_train:\n        if metric == 'euclidean':\n            d = euclidean_distance(x_train, x_test)\n        else:\n            d = manhattan_distance(x_train, x_test)\n        distances.append(d)\n    return np.array(distances)"
                        },
                        pitfalls: ["Negative under square root", "Feature scaling differences", "Integer vs float types"],
                        concepts: ["Distance metrics", "Vector operations", "NumPy arrays"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 2,
                        name: "K-Nearest Neighbors Classification",
                        description: "Implement the full KNN classifier.",
                        criteria: ["Find k nearest neighbors", "Majority voting for classification", "Predict on new data", "Calculate accuracy"],
                        hints: {
                            level1: "Sort by distance, take first k, count labels.",
                            level2: "Use Counter for majority voting.",
                            level3: "from collections import Counter\n\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def predict_one(self, x):\n        distances = compute_distances(self.X_train, x)\n        k_indices = np.argsort(distances)[:self.k]\n        k_labels = self.y_train[k_indices]\n        most_common = Counter(k_labels).most_common(1)\n        return most_common[0][0]\n\n    def predict(self, X):\n        return [self.predict_one(x) for x in X]\n\n    def score(self, X, y):\n        predictions = self.predict(X)\n        return np.mean(predictions == y)"
                        },
                        pitfalls: ["K larger than dataset", "Ties in voting", "Empty neighbors"],
                        concepts: ["Classification", "Majority voting", "Accuracy metric"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Improvements & Evaluation",
                        description: "Add improvements and proper evaluation.",
                        criteria: ["Weighted voting by distance", "Cross-validation", "Confusion matrix", "Finding optimal k"],
                        hints: {
                            level1: "Weight by 1/distance for closer neighbors.",
                            level2: "Use k-fold cross-validation to find best k.",
                            level3: "def weighted_predict(self, x):\n    distances = compute_distances(self.X_train, x)\n    k_indices = np.argsort(distances)[:self.k]\n    k_distances = distances[k_indices]\n    k_labels = self.y_train[k_indices]\n    # Weighted voting\n    weights = 1 / (k_distances + 1e-8)  # Avoid div by zero\n    label_weights = {}\n    for label, weight in zip(k_labels, weights):\n        label_weights[label] = label_weights.get(label, 0) + weight\n    return max(label_weights, key=label_weights.get)\n\ndef find_best_k(X, y, k_range=range(1, 21)):\n    best_k, best_score = 1, 0\n    for k in k_range:\n        knn = KNN(k=k)\n        score = cross_val_score(knn, X, y)  # Implement k-fold CV\n        if score > best_score:\n            best_k, best_score = k, score\n    return best_k"
                        },
                        pitfalls: ["Overfitting with small k", "Underfitting with large k", "Computational cost"],
                        concepts: ["Weighted voting", "Cross-validation", "Hyperparameter tuning"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // SYSTEMS - BEGINNER
            "cat-clone": {
                name: "Cat Clone",
                description: "Build your own version of the Unix cat command. Learn file I/O, command-line argument parsing, and basic systems programming.",
                difficulty: "beginner",
                estimatedHours: "4-6",
                prerequisites: ["Basic C/Rust/Go knowledge", "Understanding of file systems"],
                languages: { recommended: ["C", "Rust", "Go"], also: ["Python", "Zig"] },
                resources: [
                    { name: "Cat Man Page", url: "https://man7.org/linux/man-pages/man1/cat.1.html", type: "documentation" },
                    { name: "File I/O in C", url: "https://www.geeksforgeeks.org/basics-file-handling-c/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic File Reading",
                        description: "Read a single file and output to stdout.",
                        criteria: ["Accept filename as argument", "Read file contents", "Print to stdout", "Handle file not found error"],
                        hints: {
                            level1: "Use fopen/fread in C or File::open in Rust.",
                            level2: "Read in chunks using a buffer, not entire file at once.",
                            level3: "#include <stdio.h>\n\nint main(int argc, char *argv[]) {\n    if (argc < 2) {\n        fprintf(stderr, \"Usage: cat <file>\\n\");\n        return 1;\n    }\n    FILE *f = fopen(argv[1], \"r\");\n    if (!f) { perror(\"cat\"); return 1; }\n    char buf[4096];\n    size_t n;\n    while ((n = fread(buf, 1, sizeof(buf), f)) > 0) {\n        fwrite(buf, 1, n, stdout);\n    }\n    fclose(f);\n    return 0;\n}"
                        },
                        pitfalls: ["Not checking if file exists", "Reading entire file into memory", "Forgetting to close file"],
                        concepts: ["File descriptors", "Buffered I/O", "Error handling"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 2,
                        name: "Multiple Files & Stdin",
                        description: "Handle multiple files and reading from stdin.",
                        criteria: ["Concatenate multiple files", "Read from stdin when no args", "Handle - as stdin specifier", "Continue on error for remaining files"],
                        hints: {
                            level1: "Loop through argv[1] to argv[argc-1].",
                            level2: "Use stdin when filename is \"-\" or no arguments.",
                            level3: "for (int i = 1; i < argc; i++) {\n    FILE *f;\n    if (strcmp(argv[i], \"-\") == 0) {\n        f = stdin;\n    } else {\n        f = fopen(argv[i], \"r\");\n        if (!f) { perror(argv[i]); continue; }\n    }\n    // ... read and write ...\n    if (f != stdin) fclose(f);\n}"
                        },
                        pitfalls: ["Closing stdin", "Not continuing after error", "Order of file processing"],
                        concepts: ["stdin/stdout", "Command-line conventions", "Error recovery"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 3,
                        name: "Options and Flags",
                        description: "Implement common cat options.",
                        criteria: ["-n to number lines", "-b to number non-blank lines only", "-s to squeeze blank lines", "-E to show $ at end of lines"],
                        hints: {
                            level1: "Parse options before processing files.",
                            level2: "Track line number as state across files.",
                            level3: "Use getopt() for argument parsing:\nint opt;\nwhile ((opt = getopt(argc, argv, \"nbsE\")) != -1) {\n    switch (opt) {\n        case 'n': number_lines = 1; break;\n        case 'b': number_nonblank = 1; break;\n        case 's': squeeze_blank = 1; break;\n        case 'E': show_ends = 1; break;\n    }\n}"
                        },
                        pitfalls: ["Option parsing edge cases", "Line counting across files", "Blank line detection"],
                        concepts: ["getopt parsing", "State machines", "Character-by-character processing"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "wc-clone": {
                name: "Wc Clone",
                description: "Build your own word count utility. Learn text processing, counting algorithms, and Unix conventions.",
                difficulty: "beginner",
                estimatedHours: "4-6",
                prerequisites: ["Basic programming", "File I/O"],
                languages: { recommended: ["C", "Rust", "Go"], also: ["Python", "Zig"] },
                resources: [
                    { name: "Wc Man Page", url: "https://man7.org/linux/man-pages/man1/wc.1.html", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Counting",
                        description: "Count lines, words, and bytes in a file.",
                        criteria: ["Count newline characters (lines)", "Count words (whitespace-separated)", "Count total bytes", "Output in standard wc format"],
                        hints: {
                            level1: "Word = sequence of non-whitespace chars. Newline = \\n.",
                            level2: "Track 'in_word' state to count word transitions.",
                            level3: "int lines=0, words=0, bytes=0, in_word=0;\nint c;\nwhile ((c = fgetc(f)) != EOF) {\n    bytes++;\n    if (c == '\\n') lines++;\n    if (isspace(c)) {\n        in_word = 0;\n    } else if (!in_word) {\n        in_word = 1;\n        words++;\n    }\n}\nprintf(\"%7d %7d %7d\\n\", lines, words, bytes);"
                        },
                        pitfalls: ["Off-by-one in word counting", "Files without trailing newline", "Different whitespace characters"],
                        concepts: ["State machines", "Text parsing", "Whitespace handling"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 2,
                        name: "Options and Multiple Files",
                        description: "Add flag options and handle multiple files.",
                        criteria: ["-l for lines only", "-w for words only", "-c for bytes only", "-m for character count (UTF-8)", "Total line for multiple files"],
                        hints: {
                            level1: "No flags = show all three counts.",
                            level2: "Character count differs from byte count for UTF-8.",
                            level3: "// For UTF-8 character counting:\nint chars = 0;\nwhile ((c = fgetc(f)) != EOF) {\n    // Count start bytes, not continuation bytes\n    if ((c & 0xC0) != 0x80) chars++;\n}"
                        },
                        pitfalls: ["UTF-8 multi-byte characters", "Showing totals only with multiple files", "Flag combination logic"],
                        concepts: ["UTF-8 encoding", "Option handling", "Output formatting"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "grep-clone": {
                name: "Grep Clone",
                description: "Build a pattern matching utility. Learn regular expressions, text search, and efficient file scanning.",
                difficulty: "beginner",
                estimatedHours: "8-12",
                prerequisites: ["Basic programming", "Understanding of regular expressions"],
                languages: { recommended: ["C", "Rust", "Go"], also: ["Python"] },
                resources: [
                    { name: "Grep Man Page", url: "https://man7.org/linux/man-pages/man1/grep.1.html", type: "documentation" },
                    { name: "Regex Tutorial", url: "https://regexone.com/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Simple String Matching",
                        description: "Match literal strings in files.",
                        criteria: ["Search for literal string", "Print matching lines", "Support multiple files", "Show filename with multiple files"],
                        hints: {
                            level1: "Use strstr() for substring matching.",
                            level2: "Read line by line with fgets() or getline().",
                            level3: "char line[4096];\nwhile (fgets(line, sizeof(line), f)) {\n    if (strstr(line, pattern)) {\n        if (show_filename) printf(\"%s:\", filename);\n        printf(\"%s\", line);\n    }\n}"
                        },
                        pitfalls: ["Lines longer than buffer", "Missing newline at end", "Binary files"],
                        concepts: ["String searching", "Line-oriented processing", "Output formatting"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Common Options",
                        description: "Add essential grep options.",
                        criteria: ["-i case insensitive", "-v invert match", "-n show line numbers", "-c count matches only", "-l files with matches only"],
                        hints: {
                            level1: "For case insensitive, convert both pattern and line to lowercase.",
                            level2: "Use strcasestr() or implement case-folding.",
                            level3: "int matches = 0;\nint line_num = 0;\nwhile (fgets(line, sizeof(line), f)) {\n    line_num++;\n    int match = (opt_i ? strcasestr(line, pattern) : strstr(line, pattern)) != NULL;\n    if (opt_v) match = !match;\n    if (match) {\n        matches++;\n        if (opt_l) { printf(\"%s\\n\", filename); break; }\n        if (!opt_c) {\n            if (opt_n) printf(\"%d:\", line_num);\n            printf(\"%s\", line);\n        }\n    }\n}\nif (opt_c) printf(\"%d\\n\", matches);"
                        },
                        pitfalls: ["Case folding for non-ASCII", "Combining -v with -c", "-l should stop after first match"],
                        concepts: ["Case insensitive matching", "Inverted logic", "Counting vs printing"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Basic Regex Support",
                        description: "Add regular expression matching.",
                        criteria: [". matches any char", "* matches zero or more", "^ and $ anchors", "Character classes [abc]", "-E for extended regex"],
                        hints: {
                            level1: "Use POSIX regex library (regex.h) or implement simple patterns.",
                            level2: "regcomp() to compile, regexec() to match.",
                            level3: "#include <regex.h>\n\nregex_t regex;\nint flags = REG_NOSUB | (opt_i ? REG_ICASE : 0) | (opt_E ? REG_EXTENDED : 0);\nif (regcomp(&regex, pattern, flags) != 0) {\n    fprintf(stderr, \"Invalid pattern\\n\");\n    return 1;\n}\n// Then use:\nint match = regexec(&regex, line, 0, NULL, 0) == 0;\nregfree(&regex);"
                        },
                        pitfalls: ["Regex compilation errors", "BRE vs ERE differences", "Newline in regex"],
                        concepts: ["Regular expressions", "POSIX regex API", "Pattern compilation"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            "shell-basic": {
                name: "Shell (Basic)",
                description: "Build a basic Unix shell with pipes and redirects. Learn process management, file descriptors, and Unix IPC.",
                difficulty: "intermediate",
                estimatedHours: "15-25",
                prerequisites: ["C programming", "fork/exec", "File descriptors"],
                languages: { recommended: ["C", "Rust"], also: ["Go", "Zig"] },
                resources: [
                    { name: "Writing a Shell in C", url: "https://brennan.io/2015/01/16/write-a-shell-in-c/", type: "tutorial" },
                    { name: "Shell Command Language", url: "https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html", type: "specification" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "REPL and Simple Commands",
                        description: "Basic read-eval-print loop executing simple commands.",
                        criteria: ["Print prompt and read input", "Parse command and arguments", "Fork and exec the command", "Wait for child and show exit status", "Handle Ctrl+D (EOF)"],
                        hints: {
                            level1: "Use fork() to create child, execvp() to run command.",
                            level2: "Split input by whitespace for arguments.",
                            level3: "while (1) {\n    printf(\"$ \");\n    if (!fgets(line, sizeof(line), stdin)) break;\n    char *args[64];\n    int argc = 0;\n    char *tok = strtok(line, \" \\t\\n\");\n    while (tok && argc < 63) {\n        args[argc++] = tok;\n        tok = strtok(NULL, \" \\t\\n\");\n    }\n    args[argc] = NULL;\n    if (argc == 0) continue;\n    pid_t pid = fork();\n    if (pid == 0) {\n        execvp(args[0], args);\n        perror(args[0]);\n        exit(127);\n    }\n    int status;\n    waitpid(pid, &status, 0);\n}"
                        },
                        pitfalls: ["Not null-terminating args array", "Child not calling exit after exec fails", "Not handling empty input"],
                        concepts: ["Process creation", "exec family", "Wait and status"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Built-in Commands",
                        description: "Implement shell built-ins that can't be external commands.",
                        criteria: ["cd changes directory", "pwd prints working directory", "exit terminates shell", "export sets environment variables"],
                        hints: {
                            level1: "Built-ins run in shell process, not forked.",
                            level2: "Use chdir() for cd, getcwd() for pwd.",
                            level3: "if (strcmp(args[0], \"cd\") == 0) {\n    char *dir = args[1] ? args[1] : getenv(\"HOME\");\n    if (chdir(dir) != 0) perror(\"cd\");\n    continue;  // Don't fork\n}\nif (strcmp(args[0], \"exit\") == 0) {\n    int code = args[1] ? atoi(args[1]) : 0;\n    exit(code);\n}"
                        },
                        pitfalls: ["cd with no args should go to HOME", "exit status from built-ins", "Environment inheritance"],
                        concepts: ["Shell built-ins", "Working directory", "Environment variables"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "I/O Redirection",
                        description: "Implement input/output redirection.",
                        criteria: ["> redirects stdout to file", "< redirects stdin from file", ">> appends to file", "2> redirects stderr"],
                        hints: {
                            level1: "In child, before exec: close fd, open file, dup2 to 0/1/2.",
                            level2: "Parse redirections before executing.",
                            level3: "// In child process, before execvp:\nif (output_file) {\n    int fd = open(output_file, O_WRONLY | O_CREAT | (append ? O_APPEND : O_TRUNC), 0644);\n    if (fd < 0) { perror(output_file); exit(1); }\n    dup2(fd, STDOUT_FILENO);\n    close(fd);\n}\nif (input_file) {\n    int fd = open(input_file, O_RDONLY);\n    if (fd < 0) { perror(input_file); exit(1); }\n    dup2(fd, STDIN_FILENO);\n    close(fd);\n}"
                        },
                        pitfalls: ["File permissions on create", "dup2 error handling", "Closing original fd after dup2"],
                        concepts: ["File descriptors", "dup2 system call", "Standard streams"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Pipes",
                        description: "Implement command pipelines.",
                        criteria: ["cmd1 | cmd2 pipes stdout to stdin", "Multiple pipes: cmd1 | cmd2 | cmd3", "Error handling in pipeline", "Exit status from last command"],
                        hints: {
                            level1: "Use pipe() to create fd pair. Fork twice.",
                            level2: "First child: stdout -> pipe write. Second: stdin <- pipe read.",
                            level3: "int pipefd[2];\npipe(pipefd);\npid_t pid1 = fork();\nif (pid1 == 0) {\n    close(pipefd[0]);\n    dup2(pipefd[1], STDOUT_FILENO);\n    close(pipefd[1]);\n    execvp(cmd1[0], cmd1);\n    exit(127);\n}\npid_t pid2 = fork();\nif (pid2 == 0) {\n    close(pipefd[1]);\n    dup2(pipefd[0], STDIN_FILENO);\n    close(pipefd[0]);\n    execvp(cmd2[0], cmd2);\n    exit(127);\n}\nclose(pipefd[0]);\nclose(pipefd[1]);\nwaitpid(pid1, NULL, 0);\nint status;\nwaitpid(pid2, &status, 0);"
                        },
                        pitfalls: ["Not closing all pipe ends", "Deadlock from blocking read", "Order of fork and close"],
                        concepts: ["Pipes and IPC", "File descriptor inheritance", "Pipeline coordination"],
                        estimatedHours: "4-6"
                    }
                ]
            },

            "http-server-basic": {
                name: "HTTP Server (Basic)",
                description: "Build a static file HTTP server. Learn TCP networking, HTTP protocol, and concurrent request handling.",
                difficulty: "intermediate",
                estimatedHours: "12-20",
                prerequisites: ["TCP/IP basics", "Socket programming", "File I/O"],
                languages: { recommended: ["C", "Go", "Rust"], also: ["Python", "Java"] },
                resources: [
                    { name: "HTTP/1.1 Specification (RFC 7230)", url: "https://tools.ietf.org/html/rfc7230", type: "specification" },
                    { name: "Beej's Guide to Network Programming", url: "https://beej.us/guide/bgnet/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "TCP Server Basics",
                        description: "Create a listening TCP server.",
                        criteria: ["Bind to port 8080", "Accept connections", "Read request data", "Send hardcoded response", "Close connection"],
                        hints: {
                            level1: "socket(), bind(), listen(), accept() sequence.",
                            level2: "Set SO_REUSEADDR to avoid 'address already in use'.",
                            level3: "int server_fd = socket(AF_INET, SOCK_STREAM, 0);\nint opt = 1;\nsetsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));\nstruct sockaddr_in addr = { .sin_family = AF_INET, .sin_addr.s_addr = INADDR_ANY, .sin_port = htons(8080) };\nbind(server_fd, (struct sockaddr*)&addr, sizeof(addr));\nlisten(server_fd, 10);\nwhile (1) {\n    int client_fd = accept(server_fd, NULL, NULL);\n    char buf[1024];\n    read(client_fd, buf, sizeof(buf));\n    char *resp = \"HTTP/1.1 200 OK\\r\\nContent-Length: 5\\r\\n\\r\\nHello\";\n    write(client_fd, resp, strlen(resp));\n    close(client_fd);\n}"
                        },
                        pitfalls: ["Byte order (htons)", "Not handling partial reads", "Forgetting to close client fd"],
                        concepts: ["Sockets", "TCP connection lifecycle", "Bind/Listen/Accept"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "HTTP Request Parsing",
                        description: "Parse HTTP request line and headers.",
                        criteria: ["Parse method, path, version", "Parse headers into key-value pairs", "Handle GET requests", "Extract Host header"],
                        hints: {
                            level1: "Request line: GET /path HTTP/1.1\\r\\n",
                            level2: "Headers end at \\r\\n\\r\\n (blank line).",
                            level3: "// Parse request line\nchar method[16], path[256], version[16];\nsscanf(request, \"%s %s %s\", method, path, version);\n\n// Parse headers\nchar *line = strstr(request, \"\\r\\n\") + 2;\nwhile (line && strncmp(line, \"\\r\\n\", 2) != 0) {\n    char *colon = strchr(line, ':');\n    if (colon) {\n        *colon = '\\0';\n        char *value = colon + 1;\n        while (*value == ' ') value++;\n        // Store header: line -> value\n    }\n    line = strstr(line, \"\\r\\n\");\n    if (line) line += 2;\n}"
                        },
                        pitfalls: ["CRLF vs LF line endings", "Whitespace around header values", "Buffer overflow on long lines"],
                        concepts: ["HTTP message format", "Request parsing", "Header handling"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Static File Serving",
                        description: "Serve files from a directory.",
                        criteria: ["Map URL path to file system", "Detect and set Content-Type", "Send file contents", "Handle 404 Not Found", "Prevent directory traversal"],
                        hints: {
                            level1: "Concatenate document root with URL path.",
                            level2: "Use realpath() to resolve and validate path.",
                            level3: "char filepath[512];\nsnprintf(filepath, sizeof(filepath), \"%s%s\", docroot, path);\nchar resolved[PATH_MAX];\nif (!realpath(filepath, resolved) || strncmp(resolved, docroot, strlen(docroot)) != 0) {\n    send_404(client_fd);\n    return;\n}\nFILE *f = fopen(resolved, \"rb\");\nif (!f) { send_404(client_fd); return; }\nfseek(f, 0, SEEK_END);\nlong size = ftell(f);\nfseek(f, 0, SEEK_SET);\nchar *mime = get_mime_type(resolved);\ndprintf(client_fd, \"HTTP/1.1 200 OK\\r\\nContent-Type: %s\\r\\nContent-Length: %ld\\r\\n\\r\\n\", mime, size);\nchar buf[8192];\nsize_t n;\nwhile ((n = fread(buf, 1, sizeof(buf), f)) > 0) {\n    write(client_fd, buf, n);\n}"
                        },
                        pitfalls: ["Directory traversal (../)", "Missing Content-Type", "Binary file handling"],
                        concepts: ["Path resolution", "MIME types", "Security validation"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Concurrent Connections",
                        description: "Handle multiple clients concurrently.",
                        criteria: ["Thread-per-connection model", "Thread pool (optional)", "Non-blocking with select/poll (optional)", "Graceful shutdown"],
                        hints: {
                            level1: "Simple: fork() per connection or spawn thread.",
                            level2: "Better: thread pool with work queue.",
                            level3: "// Thread per connection\nwhile (1) {\n    int client_fd = accept(server_fd, NULL, NULL);\n    pthread_t thread;\n    int *arg = malloc(sizeof(int));\n    *arg = client_fd;\n    pthread_create(&thread, NULL, handle_client, arg);\n    pthread_detach(thread);\n}\n\nvoid *handle_client(void *arg) {\n    int client_fd = *(int*)arg;\n    free(arg);\n    // ... handle request ...\n    close(client_fd);\n    return NULL;\n}"
                        },
                        pitfalls: ["Thread safety", "File descriptor leaks", "Resource exhaustion"],
                        concepts: ["Concurrency models", "Threading", "Connection handling"],
                        estimatedHours: "4-6"
                    }
                ]
            },

            "memory-pool": {
                name: "Memory Pool Allocator",
                description: "Build a fixed-size block memory allocator. Learn memory management, fragmentation, and allocation strategies.",
                difficulty: "intermediate",
                estimatedHours: "10-15",
                prerequisites: ["C pointers", "Memory layout", "Data structures"],
                languages: { recommended: ["C", "Rust", "Zig"], also: ["C++"] },
                resources: [
                    { name: "Memory Pool Design", url: "https://en.wikipedia.org/wiki/Memory_pool", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Fixed-Size Pool",
                        description: "Allocate a pool of fixed-size blocks.",
                        criteria: ["Initialize pool with N blocks of size S", "Allocate returns block or NULL", "Free returns block to pool", "O(1) alloc and free"],
                        hints: {
                            level1: "Use free list: each free block points to next free block.",
                            level2: "Store next pointer in the free block itself (no extra memory).",
                            level3: "typedef struct Pool {\n    void *memory;\n    void *free_list;\n    size_t block_size;\n    size_t count;\n} Pool;\n\nvoid pool_init(Pool *p, size_t block_size, size_t count) {\n    p->block_size = block_size < sizeof(void*) ? sizeof(void*) : block_size;\n    p->count = count;\n    p->memory = malloc(p->block_size * count);\n    p->free_list = p->memory;\n    // Chain all blocks\n    char *block = p->memory;\n    for (size_t i = 0; i < count - 1; i++) {\n        *(void**)(block) = block + p->block_size;\n        block += p->block_size;\n    }\n    *(void**)block = NULL;\n}\n\nvoid *pool_alloc(Pool *p) {\n    if (!p->free_list) return NULL;\n    void *block = p->free_list;\n    p->free_list = *(void**)block;\n    return block;\n}\n\nvoid pool_free(Pool *p, void *block) {\n    *(void**)block = p->free_list;\n    p->free_list = block;\n}"
                        },
                        pitfalls: ["Block size smaller than pointer", "Double free", "Use after free"],
                        concepts: ["Free list", "Pointer aliasing", "Memory layout"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Pool Growing",
                        description: "Allow pool to grow when exhausted.",
                        criteria: ["Allocate new chunk when pool empty", "Chain chunks together", "Track all chunks for cleanup", "Optional max size limit"],
                        hints: {
                            level1: "Keep linked list of chunks.",
                            level2: "Each chunk contains blocks, chain new blocks to free list.",
                            level3: "typedef struct Chunk {\n    void *memory;\n    struct Chunk *next;\n} Chunk;\n\ntypedef struct GrowablePool {\n    Chunk *chunks;\n    void *free_list;\n    size_t block_size;\n    size_t chunk_blocks;\n} GrowablePool;\n\nvoid *gpool_alloc(GrowablePool *p) {\n    if (!p->free_list) {\n        // Allocate new chunk\n        Chunk *chunk = malloc(sizeof(Chunk));\n        chunk->memory = malloc(p->block_size * p->chunk_blocks);\n        chunk->next = p->chunks;\n        p->chunks = chunk;\n        // Add blocks to free list\n        char *block = chunk->memory;\n        for (size_t i = 0; i < p->chunk_blocks; i++) {\n            *(void**)block = p->free_list;\n            p->free_list = block;\n            block += p->block_size;\n        }\n    }\n    void *block = p->free_list;\n    p->free_list = *(void**)block;\n    return block;\n}"
                        },
                        pitfalls: ["Memory leak on destroy", "Infinite growth", "Fragmentation across chunks"],
                        concepts: ["Dynamic allocation", "Chunk management", "Resource limits"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Thread Safety & Debugging",
                        description: "Add thread safety and debugging features.",
                        criteria: ["Mutex protection for alloc/free", "Detect double free", "Memory poisoning on free", "Statistics tracking"],
                        hints: {
                            level1: "Lock around free list operations.",
                            level2: "Use magic number to detect double free.",
                            level3: "#define MAGIC_FREE 0xDEADBEEF\n#define MAGIC_USED 0xBEEFCAFE\n\ntypedef struct DebugBlock {\n    uint32_t magic;\n    char data[];\n} DebugBlock;\n\nvoid *pool_alloc_debug(Pool *p) {\n    pthread_mutex_lock(&p->lock);\n    DebugBlock *block = pool_alloc_internal(p);\n    if (block) {\n        assert(block->magic == MAGIC_FREE);\n        block->magic = MAGIC_USED;\n        memset(block->data, 0xCC, p->block_size - sizeof(DebugBlock));\n    }\n    pthread_mutex_unlock(&p->lock);\n    return block ? block->data : NULL;\n}\n\nvoid pool_free_debug(Pool *p, void *ptr) {\n    DebugBlock *block = (DebugBlock*)((char*)ptr - offsetof(DebugBlock, data));\n    pthread_mutex_lock(&p->lock);\n    assert(block->magic == MAGIC_USED);  // Detect double free\n    block->magic = MAGIC_FREE;\n    memset(block->data, 0xDD, p->block_size - sizeof(DebugBlock));  // Poison\n    pool_free_internal(p, block);\n    pthread_mutex_unlock(&p->lock);\n}"
                        },
                        pitfalls: ["Lock ordering deadlocks", "Overhead of debugging", "False positives in detection"],
                        concepts: ["Thread safety", "Memory debugging", "Defensive programming"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // DATA STORAGE - BEGINNER
            "json-db": {
                name: "JSON File Database",
                description: "Build a simple file-based database using JSON. Learn data persistence, CRUD operations, and basic query patterns.",
                difficulty: "beginner",
                estimatedHours: "8-12",
                prerequisites: ["JSON knowledge", "File I/O", "Basic programming"],
                languages: { recommended: ["Python", "JavaScript", "Go"], also: ["Ruby", "PHP"] },
                resources: [
                    { name: "JSON Specification", url: "https://www.json.org", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic CRUD Operations",
                        description: "Implement Create, Read, Update, Delete.",
                        criteria: ["Create/insert new records", "Read by ID", "Update existing records", "Delete records", "Persist to JSON file"],
                        hints: {
                            level1: "Load entire file into memory, modify, write back.",
                            level2: "Use UUIDs or auto-increment for IDs.",
                            level3: "import json\nimport uuid\n\nclass JsonDB:\n    def __init__(self, filepath):\n        self.filepath = filepath\n        self.data = self._load()\n    \n    def _load(self):\n        try:\n            with open(self.filepath, 'r') as f:\n                return json.load(f)\n        except FileNotFoundError:\n            return {'records': {}}\n    \n    def _save(self):\n        with open(self.filepath, 'w') as f:\n            json.dump(self.data, f, indent=2)\n    \n    def create(self, record):\n        id = str(uuid.uuid4())\n        record['id'] = id\n        self.data['records'][id] = record\n        self._save()\n        return id\n    \n    def read(self, id):\n        return self.data['records'].get(id)\n    \n    def update(self, id, updates):\n        if id in self.data['records']:\n            self.data['records'][id].update(updates)\n            self._save()\n            return True\n        return False\n    \n    def delete(self, id):\n        if id in self.data['records']:\n            del self.data['records'][id]\n            self._save()\n            return True\n        return False"
                        },
                        pitfalls: ["Race conditions with concurrent access", "Data loss on crash during write", "Memory issues with large files"],
                        concepts: ["CRUD operations", "Data persistence", "File-based storage"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Querying",
                        description: "Add simple query capabilities.",
                        criteria: ["Find all matching a condition", "Support multiple conditions (AND)", "Sort results", "Limit and offset"],
                        hints: {
                            level1: "Filter records by iterating and checking conditions.",
                            level2: "Use lambda functions or dicts for conditions.",
                            level3: "def query(self, conditions=None, sort_by=None, limit=None, offset=0):\n    results = list(self.data['records'].values())\n    \n    if conditions:\n        for key, value in conditions.items():\n            results = [r for r in results if r.get(key) == value]\n    \n    if sort_by:\n        reverse = sort_by.startswith('-')\n        key = sort_by.lstrip('-')\n        results.sort(key=lambda r: r.get(key, ''), reverse=reverse)\n    \n    if offset:\n        results = results[offset:]\n    if limit:\n        results = results[:limit]\n    \n    return results"
                        },
                        pitfalls: ["Inefficient full scans", "Sorting nulls/missing fields", "Type comparison issues"],
                        concepts: ["Query filtering", "Sorting", "Pagination"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Collections & Indexing",
                        description: "Support multiple collections and basic indexes.",
                        criteria: ["Multiple named collections", "Create index on field", "Use index for queries", "Maintain index on writes"],
                        hints: {
                            level1: "Store collections as top-level keys.",
                            level2: "Index is a map from field value to list of record IDs.",
                            level3: "class Collection:\n    def __init__(self, name, db):\n        self.name = name\n        self.db = db\n        self.indexes = {}  # field -> {value -> [ids]}\n    \n    def create_index(self, field):\n        index = {}\n        for id, record in self.db.data[self.name].items():\n            value = record.get(field)\n            if value not in index:\n                index[value] = []\n            index[value].append(id)\n        self.indexes[field] = index\n    \n    def query(self, conditions):\n        # Try to use index\n        for field, value in conditions.items():\n            if field in self.indexes:\n                ids = self.indexes[field].get(value, [])\n                results = [self.db.data[self.name][id] for id in ids]\n                # Filter by remaining conditions\n                return [r for r in results if all(r.get(k) == v for k, v in conditions.items())]\n        # Fall back to full scan\n        return [r for r in self.db.data[self.name].values() if all(r.get(k) == v for k, v in conditions.items())]"
                        },
                        pitfalls: ["Index staleness", "Memory overhead", "Index not used"],
                        concepts: ["Indexing", "Query optimization", "Collection design"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            "kv-memory": {
                name: "In-Memory Key-Value Store",
                description: "Build an in-memory key-value store with a hash map. Learn hash table implementation and caching concepts.",
                difficulty: "beginner",
                estimatedHours: "6-10",
                prerequisites: ["Data structures", "Hash functions"],
                languages: { recommended: ["C", "Go", "Rust"], also: ["Python", "Java"] },
                resources: [
                    { name: "Hash Table Wikipedia", url: "https://en.wikipedia.org/wiki/Hash_table", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Hash Map",
                        description: "Implement core get/set/delete operations.",
                        criteria: ["Set key-value pairs", "Get value by key", "Delete by key", "Handle collisions with chaining", "Dynamic resizing"],
                        hints: {
                            level1: "Hash key to bucket index, store in linked list.",
                            level2: "Resize when load factor exceeds threshold.",
                            level3: "typedef struct Entry {\n    char *key;\n    void *value;\n    struct Entry *next;\n} Entry;\n\ntypedef struct HashMap {\n    Entry **buckets;\n    size_t capacity;\n    size_t size;\n} HashMap;\n\nuint32_t hash(const char *key) {\n    uint32_t h = 5381;\n    while (*key) h = ((h << 5) + h) + *key++;\n    return h;\n}\n\nvoid *hm_get(HashMap *m, const char *key) {\n    uint32_t idx = hash(key) % m->capacity;\n    Entry *e = m->buckets[idx];\n    while (e) {\n        if (strcmp(e->key, key) == 0) return e->value;\n        e = e->next;\n    }\n    return NULL;\n}\n\nvoid hm_set(HashMap *m, const char *key, void *value) {\n    if ((float)m->size / m->capacity > 0.75) resize(m);\n    uint32_t idx = hash(key) % m->capacity;\n    // Check existing\n    Entry *e = m->buckets[idx];\n    while (e) {\n        if (strcmp(e->key, key) == 0) { e->value = value; return; }\n        e = e->next;\n    }\n    // Insert new\n    Entry *new = malloc(sizeof(Entry));\n    new->key = strdup(key);\n    new->value = value;\n    new->next = m->buckets[idx];\n    m->buckets[idx] = new;\n    m->size++;\n}"
                        },
                        pitfalls: ["Hash function quality", "Memory leaks", "Resizing while iterating"],
                        concepts: ["Hash functions", "Collision resolution", "Load factor"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "TTL and Expiration",
                        description: "Add time-to-live for keys.",
                        criteria: ["Set with expiration time", "Get returns null for expired", "Background cleanup (optional)", "TTL command returns remaining time"],
                        hints: {
                            level1: "Store expiration timestamp with each entry.",
                            level2: "Check expiration on get (lazy deletion).",
                            level3: "typedef struct Entry {\n    char *key;\n    void *value;\n    time_t expires;  // 0 = no expiration\n    struct Entry *next;\n} Entry;\n\nvoid hm_set_ex(HashMap *m, const char *key, void *value, int ttl_secs) {\n    // ... set entry ...\n    entry->expires = ttl_secs > 0 ? time(NULL) + ttl_secs : 0;\n}\n\nvoid *hm_get(HashMap *m, const char *key) {\n    Entry *e = find_entry(m, key);\n    if (!e) return NULL;\n    if (e->expires && time(NULL) > e->expires) {\n        hm_delete(m, key);  // Lazy delete\n        return NULL;\n    }\n    return e->value;\n}\n\nint hm_ttl(HashMap *m, const char *key) {\n    Entry *e = find_entry(m, key);\n    if (!e || !e->expires) return -1;\n    int remaining = e->expires - time(NULL);\n    return remaining > 0 ? remaining : -2;  // -2 = expired\n}"
                        },
                        pitfalls: ["Clock skew", "Memory not freed for expired", "Precision of time"],
                        concepts: ["TTL mechanics", "Lazy vs eager deletion", "Time handling"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // COMPILERS - BEGINNER
            "calculator-parser": {
                name: "Calculator Parser",
                description: "Build a calculator that parses and evaluates arithmetic expressions. Learn expression parsing and operator precedence.",
                difficulty: "beginner",
                estimatedHours: "6-10",
                prerequisites: ["Basic programming", "Understanding of precedence"],
                languages: { recommended: ["Python", "JavaScript", "C"], also: ["Go", "Rust"] },
                resources: [
                    { name: "Recursive Descent Parsing", url: "https://craftinginterpreters.com/parsing-expressions.html", type: "book" },
                    { name: "Pratt Parsing", url: "https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Arithmetic",
                        description: "Evaluate simple arithmetic expressions.",
                        criteria: ["Parse numbers", "Handle + - * /", "Correct precedence (* / before + -)", "Handle parentheses"],
                        hints: {
                            level1: "One function per precedence level: expr() calls term() calls factor().",
                            level2: "factor handles numbers and parentheses, term handles * /, expr handles + -.",
                            level3: "class Calculator:\n    def __init__(self, text):\n        self.text = text\n        self.pos = 0\n    \n    def parse(self):\n        return self.expr()\n    \n    def expr(self):\n        result = self.term()\n        while self.pos < len(self.text) and self.text[self.pos] in '+-':\n            op = self.text[self.pos]\n            self.pos += 1\n            if op == '+':\n                result += self.term()\n            else:\n                result -= self.term()\n        return result\n    \n    def term(self):\n        result = self.factor()\n        while self.pos < len(self.text) and self.text[self.pos] in '*/':\n            op = self.text[self.pos]\n            self.pos += 1\n            if op == '*':\n                result *= self.factor()\n            else:\n                result /= self.factor()\n        return result\n    \n    def factor(self):\n        self.skip_whitespace()\n        if self.text[self.pos] == '(':\n            self.pos += 1\n            result = self.expr()\n            self.pos += 1  # skip ')'\n            return result\n        return self.number()"
                        },
                        pitfalls: ["Left vs right associativity", "Division by zero", "Whitespace handling"],
                        concepts: ["Recursive descent", "Operator precedence", "Expression trees"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Unary and Power",
                        description: "Add unary operators and exponentiation.",
                        criteria: ["Unary minus: -5", "Unary plus: +5", "Exponentiation: 2^3", "Correct right-associativity for ^"],
                        hints: {
                            level1: "Unary goes in factor(). Power needs its own level.",
                            level2: "Power is right-associative: 2^3^2 = 2^(3^2) = 512.",
                            level3: "def factor(self):\n    self.skip_whitespace()\n    if self.text[self.pos] in '+-':\n        op = self.text[self.pos]\n        self.pos += 1\n        value = self.factor()  # Recursive for --5\n        return -value if op == '-' else value\n    return self.power()\n\ndef power(self):\n    base = self.primary()\n    if self.pos < len(self.text) and self.text[self.pos] == '^':\n        self.pos += 1\n        exp = self.factor()  # Right-associative: recurse up\n        return base ** exp\n    return base"
                        },
                        pitfalls: ["--5 (double negative)", "Power precedence vs unary", "Right associativity"],
                        concepts: ["Unary operators", "Associativity", "Precedence climbing"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Variables and Functions",
                        description: "Add variables and built-in functions.",
                        criteria: ["Variable assignment: x = 5", "Variable reference: x + 2", "Functions: sin(x), sqrt(x)", "Error on undefined variable"],
                        hints: {
                            level1: "Store variables in a dict. Look up on identifier.",
                            level2: "Check if identifier followed by '(' for function call.",
                            level3: "class Calculator:\n    def __init__(self):\n        self.variables = {}\n        self.functions = {\n            'sin': math.sin,\n            'cos': math.cos,\n            'sqrt': math.sqrt,\n            'abs': abs\n        }\n    \n    def statement(self):\n        self.skip_whitespace()\n        # Check for assignment\n        if self.peek_identifier():\n            name = self.identifier()\n            self.skip_whitespace()\n            if self.pos < len(self.text) and self.text[self.pos] == '=':\n                self.pos += 1\n                value = self.expr()\n                self.variables[name] = value\n                return value\n            # Put back and parse as expression\n            self.pos -= len(name)\n        return self.expr()\n    \n    def primary(self):\n        self.skip_whitespace()\n        if self.text[self.pos].isalpha():\n            name = self.identifier()\n            if self.text[self.pos] == '(':  # Function call\n                self.pos += 1\n                arg = self.expr()\n                self.pos += 1  # skip ')'\n                if name not in self.functions:\n                    raise ValueError(f'Unknown function: {name}')\n                return self.functions[name](arg)\n            if name not in self.variables:\n                raise ValueError(f'Undefined variable: {name}')\n            return self.variables[name]\n        return self.number()"
                        },
                        pitfalls: ["Function vs variable ambiguity", "Assignment in expression", "Scope issues"],
                        concepts: ["Symbol tables", "Function calls", "Variable binding"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // CS FUNDAMENTALS
            "stack-queue": {
                name: "Stack & Queue",
                description: "Implement stack and queue data structures. Learn LIFO/FIFO principles and their applications.",
                difficulty: "beginner",
                estimatedHours: "4-6",
                prerequisites: ["Basic programming", "Arrays"],
                languages: { recommended: ["Python", "Java", "C"], also: ["JavaScript", "Go"] },
                resources: [
                    { name: "Stack and Queue - GeeksforGeeks", url: "https://www.geeksforgeeks.org/stack-data-structure/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Array-based Stack",
                        description: "Implement a stack using an array.",
                        criteria: ["push() adds to top", "pop() removes and returns top", "peek() returns top without removing", "isEmpty() checks if empty", "Handle stack overflow/underflow"],
                        hints: {
                            level1: "Track top index. Push increments, pop decrements.",
                            level2: "Use dynamic array for unbounded stack.",
                            level3: "class Stack:\n    def __init__(self, capacity=100):\n        self.items = [None] * capacity\n        self.top = -1\n        self.capacity = capacity\n    \n    def push(self, item):\n        if self.top >= self.capacity - 1:\n            raise OverflowError('Stack overflow')\n        self.top += 1\n        self.items[self.top] = item\n    \n    def pop(self):\n        if self.top < 0:\n            raise IndexError('Stack underflow')\n        item = self.items[self.top]\n        self.top -= 1\n        return item\n    \n    def peek(self):\n        if self.top < 0:\n            raise IndexError('Stack is empty')\n        return self.items[self.top]\n    \n    def is_empty(self):\n        return self.top < 0"
                        },
                        pitfalls: ["Off-by-one errors", "Not checking bounds", "Memory leaks in C"],
                        concepts: ["LIFO principle", "Array indexing", "Bounds checking"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 2,
                        name: "Array-based Queue",
                        description: "Implement a queue using a circular array.",
                        criteria: ["enqueue() adds to back", "dequeue() removes from front", "front() peeks front element", "Circular array for efficiency", "Handle full/empty states"],
                        hints: {
                            level1: "Track front and rear indices. Wrap around with modulo.",
                            level2: "Distinguish full from empty: track count or use sentinel.",
                            level3: "class CircularQueue:\n    def __init__(self, capacity):\n        self.items = [None] * capacity\n        self.capacity = capacity\n        self.front = 0\n        self.rear = 0\n        self.count = 0\n    \n    def enqueue(self, item):\n        if self.count >= self.capacity:\n            raise OverflowError('Queue is full')\n        self.items[self.rear] = item\n        self.rear = (self.rear + 1) % self.capacity\n        self.count += 1\n    \n    def dequeue(self):\n        if self.count == 0:\n            raise IndexError('Queue is empty')\n        item = self.items[self.front]\n        self.front = (self.front + 1) % self.capacity\n        self.count -= 1\n        return item\n    \n    def peek_front(self):\n        if self.count == 0:\n            raise IndexError('Queue is empty')\n        return self.items[self.front]"
                        },
                        pitfalls: ["Full vs empty with same indices", "Modulo arithmetic errors", "Growing circular buffer"],
                        concepts: ["FIFO principle", "Circular buffer", "Modulo arithmetic"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 3,
                        name: "Linked List Implementations",
                        description: "Implement stack and queue using linked lists.",
                        criteria: ["Linked stack with O(1) push/pop", "Linked queue with O(1) enqueue/dequeue", "No fixed capacity", "Memory efficient"],
                        hints: {
                            level1: "Stack: push/pop at head. Queue: enqueue at tail, dequeue at head.",
                            level2: "Keep tail pointer for O(1) enqueue.",
                            level3: "class Node:\n    def __init__(self, value):\n        self.value = value\n        self.next = None\n\nclass LinkedStack:\n    def __init__(self):\n        self.top = None\n    \n    def push(self, value):\n        node = Node(value)\n        node.next = self.top\n        self.top = node\n    \n    def pop(self):\n        if not self.top:\n            raise IndexError('Stack is empty')\n        value = self.top.value\n        self.top = self.top.next\n        return value\n\nclass LinkedQueue:\n    def __init__(self):\n        self.front = self.rear = None\n    \n    def enqueue(self, value):\n        node = Node(value)\n        if self.rear:\n            self.rear.next = node\n        self.rear = node\n        if not self.front:\n            self.front = node\n    \n    def dequeue(self):\n        if not self.front:\n            raise IndexError('Queue is empty')\n        value = self.front.value\n        self.front = self.front.next\n        if not self.front:\n            self.rear = None\n        return value"
                        },
                        pitfalls: ["Null pointer on empty", "Memory management", "Updating both front and rear"],
                        concepts: ["Linked nodes", "Dynamic memory", "Pointer manipulation"],
                        estimatedHours: "1-2"
                    }
                ]
            },

            "bst": {
                name: "Binary Search Tree",
                description: "Implement a binary search tree with standard operations. Learn tree traversals and recursive algorithms.",
                difficulty: "intermediate",
                estimatedHours: "8-12",
                prerequisites: ["Recursion", "Trees basics"],
                languages: { recommended: ["Python", "Java", "C++"], also: ["Go", "Rust"] },
                resources: [
                    { name: "BST - GeeksforGeeks", url: "https://www.geeksforgeeks.org/binary-search-tree-data-structure/", type: "tutorial" },
                    { name: "Visualgo BST", url: "https://visualgo.net/en/bst", type: "interactive" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Insert and Search",
                        description: "Implement basic insert and search operations.",
                        criteria: ["Insert maintaining BST property", "Search returns node or null", "Handle duplicates (left or ignore)", "Recursive implementation"],
                        hints: {
                            level1: "BST property: left < node < right.",
                            level2: "Recursive: if less go left, if greater go right.",
                            level3: "class Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = self.right = None\n\nclass BST:\n    def __init__(self):\n        self.root = None\n    \n    def insert(self, value):\n        self.root = self._insert(self.root, value)\n    \n    def _insert(self, node, value):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(node.left, value)\n        elif value > node.value:\n            node.right = self._insert(node.right, value)\n        return node\n    \n    def search(self, value):\n        return self._search(self.root, value)\n    \n    def _search(self, node, value):\n        if node is None or node.value == value:\n            return node\n        if value < node.value:\n            return self._search(node.left, value)\n        return self._search(node.right, value)"
                        },
                        pitfalls: ["Not returning updated node", "Handling empty tree", "Duplicate handling inconsistency"],
                        concepts: ["BST property", "Recursive tree operations", "Node creation"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Traversals",
                        description: "Implement tree traversal methods.",
                        criteria: ["Inorder traversal (sorted output)", "Preorder traversal", "Postorder traversal", "Level-order (BFS) traversal", "Iterative versions (optional)"],
                        hints: {
                            level1: "Inorder: left, node, right. Preorder: node, left, right.",
                            level2: "Level-order uses a queue.",
                            level3: "def inorder(self):\n    result = []\n    self._inorder(self.root, result)\n    return result\n\ndef _inorder(self, node, result):\n    if node:\n        self._inorder(node.left, result)\n        result.append(node.value)\n        self._inorder(node.right, result)\n\ndef level_order(self):\n    if not self.root:\n        return []\n    result = []\n    queue = [self.root]\n    while queue:\n        node = queue.pop(0)\n        result.append(node.value)\n        if node.left:\n            queue.append(node.left)\n        if node.right:\n            queue.append(node.right)\n    return result\n\n# Iterative inorder using stack\ndef inorder_iterative(self):\n    result = []\n    stack = []\n    node = self.root\n    while stack or node:\n        while node:\n            stack.append(node)\n            node = node.left\n        node = stack.pop()\n        result.append(node.value)\n        node = node.right\n    return result"
                        },
                        pitfalls: ["Stack overflow on deep trees", "Queue vs stack confusion", "Forgetting to visit all nodes"],
                        concepts: ["Tree traversals", "Recursion vs iteration", "BFS vs DFS"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Delete Operation",
                        description: "Implement node deletion with all cases.",
                        criteria: ["Delete leaf node", "Delete node with one child", "Delete node with two children (successor)", "Maintain BST property"],
                        hints: {
                            level1: "Three cases: leaf, one child, two children.",
                            level2: "Two children: replace with inorder successor (min of right subtree).",
                            level3: "def delete(self, value):\n    self.root = self._delete(self.root, value)\n\ndef _delete(self, node, value):\n    if node is None:\n        return None\n    if value < node.value:\n        node.left = self._delete(node.left, value)\n    elif value > node.value:\n        node.right = self._delete(node.right, value)\n    else:\n        # Node to delete found\n        if node.left is None:\n            return node.right\n        if node.right is None:\n            return node.left\n        # Two children: find inorder successor\n        successor = self._min_node(node.right)\n        node.value = successor.value\n        node.right = self._delete(node.right, successor.value)\n    return node\n\ndef _min_node(self, node):\n    while node.left:\n        node = node.left\n    return node"
                        },
                        pitfalls: ["Not handling all cases", "Memory leaks", "Breaking parent links"],
                        concepts: ["Node deletion", "Inorder successor", "Tree restructuring"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 4,
                        name: "Tree Properties",
                        description: "Implement utility methods for tree properties.",
                        criteria: ["Height of tree", "Size (node count)", "Min and max values", "Is valid BST check", "Balance factor (optional)"],
                        hints: {
                            level1: "Height = 1 + max(left_height, right_height).",
                            level2: "Valid BST: each node must be within a range.",
                            level3: "def height(self):\n    return self._height(self.root)\n\ndef _height(self, node):\n    if node is None:\n        return -1  # or 0, depending on convention\n    return 1 + max(self._height(node.left), self._height(node.right))\n\ndef is_valid_bst(self):\n    return self._is_valid(self.root, float('-inf'), float('inf'))\n\ndef _is_valid(self, node, min_val, max_val):\n    if node is None:\n        return True\n    if node.value <= min_val or node.value >= max_val:\n        return False\n    return (self._is_valid(node.left, min_val, node.value) and\n            self._is_valid(node.right, node.value, max_val))\n\ndef size(self):\n    return self._size(self.root)\n\ndef _size(self, node):\n    if node is None:\n        return 0\n    return 1 + self._size(node.left) + self._size(node.right)"
                        },
                        pitfalls: ["Height of empty tree", "BST validation range", "Counting correctly"],
                        concepts: ["Tree properties", "Range validation", "Recursive counting"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "hash-table": {
                name: "Hash Table",
                description: "Implement a hash table from scratch. Learn hash functions, collision resolution, and performance trade-offs.",
                difficulty: "intermediate",
                estimatedHours: "8-12",
                prerequisites: ["Arrays", "Understanding of hashing"],
                languages: { recommended: ["C", "Python", "Java"], also: ["Go", "Rust"] },
                resources: [
                    { name: "Hash Table - Wikipedia", url: "https://en.wikipedia.org/wiki/Hash_table", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Chaining Implementation",
                        description: "Implement hash table with separate chaining.",
                        criteria: ["put(key, value)", "get(key) returns value", "remove(key)", "Linked list for collisions", "Good hash function"],
                        hints: {
                            level1: "Each bucket is a linked list of key-value pairs.",
                            level2: "Use djb2 or FNV hash for strings.",
                            level3: "class Entry:\n    def __init__(self, key, value):\n        self.key = key\n        self.value = value\n        self.next = None\n\nclass HashTable:\n    def __init__(self, capacity=16):\n        self.capacity = capacity\n        self.size = 0\n        self.buckets = [None] * capacity\n    \n    def _hash(self, key):\n        h = 5381\n        for c in key:\n            h = ((h << 5) + h) + ord(c)\n        return h % self.capacity\n    \n    def put(self, key, value):\n        idx = self._hash(key)\n        entry = self.buckets[idx]\n        while entry:\n            if entry.key == key:\n                entry.value = value\n                return\n            entry = entry.next\n        new_entry = Entry(key, value)\n        new_entry.next = self.buckets[idx]\n        self.buckets[idx] = new_entry\n        self.size += 1\n    \n    def get(self, key):\n        idx = self._hash(key)\n        entry = self.buckets[idx]\n        while entry:\n            if entry.key == key:\n                return entry.value\n            entry = entry.next\n        return None"
                        },
                        pitfalls: ["Poor hash function", "Not updating existing keys", "Memory leaks on remove"],
                        concepts: ["Hash functions", "Collision resolution", "Linked lists"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Open Addressing",
                        description: "Implement hash table with open addressing.",
                        criteria: ["Linear probing", "Quadratic probing (optional)", "Double hashing (optional)", "Proper deletion (tombstones)", "Load factor monitoring"],
                        hints: {
                            level1: "On collision, try next slot. Wrap around.",
                            level2: "Deleted entries need tombstones to not break search.",
                            level3: "DELETED = object()  # Tombstone sentinel\n\nclass OpenHashTable:\n    def __init__(self, capacity=16):\n        self.keys = [None] * capacity\n        self.values = [None] * capacity\n        self.capacity = capacity\n        self.size = 0\n    \n    def put(self, key, value):\n        if self.size >= self.capacity * 0.7:\n            self._resize()\n        idx = self._hash(key)\n        while self.keys[idx] is not None and self.keys[idx] is not DELETED:\n            if self.keys[idx] == key:\n                self.values[idx] = value\n                return\n            idx = (idx + 1) % self.capacity\n        self.keys[idx] = key\n        self.values[idx] = value\n        self.size += 1\n    \n    def get(self, key):\n        idx = self._hash(key)\n        start = idx\n        while self.keys[idx] is not None:\n            if self.keys[idx] == key:\n                return self.values[idx]\n            idx = (idx + 1) % self.capacity\n            if idx == start:\n                break\n        return None\n    \n    def remove(self, key):\n        idx = self._hash(key)\n        while self.keys[idx] is not None:\n            if self.keys[idx] == key:\n                self.keys[idx] = DELETED\n                self.values[idx] = None\n                self.size -= 1\n                return True\n            idx = (idx + 1) % self.capacity\n        return False"
                        },
                        pitfalls: ["Infinite loop without tombstones", "Clustering", "High load factor performance"],
                        concepts: ["Open addressing", "Tombstones", "Probing sequences"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Dynamic Resizing",
                        description: "Implement automatic resizing for performance.",
                        criteria: ["Grow when load factor > 0.75", "Shrink when load factor < 0.25 (optional)", "Rehash all entries", "Amortized O(1) operations"],
                        hints: {
                            level1: "Double capacity and reinsert all entries.",
                            level2: "Rehash needed because bucket index depends on capacity.",
                            level3: "def _resize(self, new_capacity=None):\n    if new_capacity is None:\n        new_capacity = self.capacity * 2\n    old_buckets = self.buckets\n    self.capacity = new_capacity\n    self.buckets = [None] * new_capacity\n    self.size = 0\n    \n    for bucket in old_buckets:\n        entry = bucket\n        while entry:\n            self.put(entry.key, entry.value)\n            entry = entry.next\n\ndef put(self, key, value):\n    if self.size >= self.capacity * 0.75:\n        self._resize()\n    # ... rest of put ..."
                        },
                        pitfalls: ["Rehashing during resize", "Not updating size correctly", "Memory during resize"],
                        concepts: ["Amortized analysis", "Load factor", "Rehashing"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // SECURITY
            "password-hashing": {
                name: "Password Hashing",
                description: "Implement secure password hashing with salt. Learn cryptographic security concepts and why plain hashing is insufficient.",
                difficulty: "beginner",
                estimatedHours: "4-6",
                prerequisites: ["Basic programming", "Understanding of hashing"],
                languages: { recommended: ["Python", "Go", "JavaScript"], also: ["Java", "C#"] },
                resources: [
                    { name: "How to Safely Store Passwords", url: "https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Hashing with Salt",
                        description: "Implement salted password hashing.",
                        criteria: ["Generate random salt", "Hash password with salt", "Store salt with hash", "Verify password against stored hash"],
                        hints: {
                            level1: "Salt prevents rainbow table attacks.",
                            level2: "Concatenate salt + password, then hash.",
                            level3: "import hashlib\nimport os\n\ndef hash_password(password):\n    salt = os.urandom(16)  # 16 bytes of random salt\n    combined = salt + password.encode()\n    hash_bytes = hashlib.sha256(combined).digest()\n    # Store salt + hash together\n    return salt + hash_bytes\n\ndef verify_password(password, stored):\n    salt = stored[:16]\n    stored_hash = stored[16:]\n    combined = salt + password.encode()\n    computed_hash = hashlib.sha256(combined).digest()\n    return computed_hash == stored_hash"
                        },
                        pitfalls: ["Using predictable salt", "Not storing salt", "Timing attacks in comparison"],
                        concepts: ["Salting", "Rainbow tables", "Secure random generation"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 2,
                        name: "Key Stretching",
                        description: "Implement iterated hashing for slow verification.",
                        criteria: ["Multiple hash iterations", "Configurable iteration count", "PBKDF2 algorithm", "Constant-time comparison"],
                        hints: {
                            level1: "More iterations = slower brute force.",
                            level2: "PBKDF2 is standard key derivation function.",
                            level3: "import hashlib\nimport hmac\n\ndef pbkdf2_hash(password, salt, iterations=100000):\n    # PBKDF2-HMAC-SHA256\n    return hashlib.pbkdf2_hmac(\n        'sha256',\n        password.encode(),\n        salt,\n        iterations\n    )\n\ndef constant_time_compare(a, b):\n    \"\"\"Prevent timing attacks\"\"\"\n    if len(a) != len(b):\n        return False\n    result = 0\n    for x, y in zip(a, b):\n        result |= x ^ y\n    return result == 0"
                        },
                        pitfalls: ["Too few iterations", "Not using constant-time compare", "iteration count not stored"],
                        concepts: ["Key stretching", "PBKDF2", "Timing attacks"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 3,
                        name: "Modern Password Hashing",
                        description: "Implement or use bcrypt/Argon2.",
                        criteria: ["Bcrypt format understanding", "Work factor configuration", "Argon2 (optional)", "Migration strategy for old hashes"],
                        hints: {
                            level1: "Bcrypt includes salt and cost in output.",
                            level2: "Argon2 is memory-hard, resistant to GPU attacks.",
                            level3: "# Using bcrypt library (recommended for production)\nimport bcrypt\n\ndef hash_password_bcrypt(password):\n    # Cost factor of 12 is reasonable for 2024\n    return bcrypt.hashpw(password.encode(), bcrypt.gensalt(rounds=12))\n\ndef verify_password_bcrypt(password, hashed):\n    return bcrypt.checkpw(password.encode(), hashed)\n\n# Bcrypt output format: $2b$12$salthere...hashhere\n# $2b$ = algorithm, 12 = cost factor, then 22-char salt + 31-char hash\n\n# Migration strategy\ndef verify_with_migration(password, stored_hash):\n    if stored_hash.startswith('$2b$'):\n        return verify_password_bcrypt(password, stored_hash)\n    else:\n        # Old format - verify then upgrade\n        if verify_old_format(password, stored_hash):\n            return True, hash_password_bcrypt(password)  # New hash to store\n        return False, None"
                        },
                        pitfalls: ["Implementing crypto yourself", "Too low work factor", "Not planning for algorithm upgrades"],
                        concepts: ["Bcrypt", "Argon2", "Algorithm agility"],
                        estimatedHours: "1-2"
                    }
                ]
            },

            // SOFTWARE ENGINEERING
            "unit-testing-basics": {
                name: "Unit Testing Fundamentals",
                description: "Learn unit testing principles and practices. Write effective tests for confidence in your code.",
                difficulty: "beginner",
                estimatedHours: "6-10",
                prerequisites: ["Basic programming", "Functions and classes"],
                languages: { recommended: ["Python", "JavaScript", "Java"], also: ["Go", "Rust"] },
                resources: [
                    { name: "pytest Documentation", url: "https://docs.pytest.org/", type: "documentation" },
                    { name: "Jest Documentation", url: "https://jestjs.io/docs/getting-started", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "First Tests",
                        description: "Write your first unit tests.",
                        criteria: ["Set up test framework", "Write test for pure function", "Use assertions (assertEqual, assertTrue)", "Run tests and see results", "Test edge cases"],
                        hints: {
                            level1: "Test function should start with test_.",
                            level2: "Arrange-Act-Assert pattern.",
                            level3: "# calculator.py\ndef add(a, b):\n    return a + b\n\ndef divide(a, b):\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\n# test_calculator.py\nimport pytest\nfrom calculator import add, divide\n\ndef test_add_positive_numbers():\n    # Arrange\n    a, b = 2, 3\n    # Act\n    result = add(a, b)\n    # Assert\n    assert result == 5\n\ndef test_add_negative_numbers():\n    assert add(-1, -1) == -2\n\ndef test_add_zero():\n    assert add(0, 5) == 5\n\ndef test_divide_normal():\n    assert divide(10, 2) == 5\n\ndef test_divide_by_zero():\n    with pytest.raises(ValueError):\n        divide(10, 0)"
                        },
                        pitfalls: ["Testing implementation not behavior", "Not testing edge cases", "Fragile assertions"],
                        concepts: ["Unit testing", "Assertions", "Test structure"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Test Organization",
                        description: "Organize tests with fixtures and setup.",
                        criteria: ["Group related tests in classes", "Use setup/teardown", "Share fixtures between tests", "Parameterized tests"],
                        hints: {
                            level1: "Fixtures provide reusable test data.",
                            level2: "Parameterize to test multiple inputs.",
                            level3: "import pytest\n\n# Fixtures\n@pytest.fixture\ndef user():\n    return {'name': 'John', 'email': 'john@example.com'}\n\n@pytest.fixture\ndef database():\n    db = connect_test_db()\n    yield db  # Test runs here\n    db.close()  # Cleanup\n\ndef test_user_creation(user):\n    assert user['name'] == 'John'\n\n# Parameterized tests\n@pytest.mark.parametrize('input,expected', [\n    (1, 1),\n    (2, 4),\n    (3, 9),\n    (-2, 4),\n])\ndef test_square(input, expected):\n    assert input ** 2 == expected\n\n# Test class for grouping\nclass TestStringMethods:\n    def test_upper(self):\n        assert 'hello'.upper() == 'HELLO'\n    \n    def test_split(self):\n        assert 'a,b,c'.split(',') == ['a', 'b', 'c']"
                        },
                        pitfalls: ["Fixtures with side effects", "Shared mutable state", "Over-complicated fixtures"],
                        concepts: ["Fixtures", "Parameterization", "Test isolation"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Mocking and Isolation",
                        description: "Test code with external dependencies.",
                        criteria: ["Mock external API calls", "Mock file system operations", "Verify mock was called", "Patch at correct level"],
                        hints: {
                            level1: "Mock replaces real object with fake.",
                            level2: "Patch where it's used, not where it's defined.",
                            level3: "from unittest.mock import Mock, patch\n\n# Code under test\ndef get_user_data(user_id):\n    response = requests.get(f'https://api.example.com/users/{user_id}')\n    return response.json()\n\ndef process_user(user_id):\n    data = get_user_data(user_id)\n    return data['name'].upper()\n\n# Test with mocking\n@patch('mymodule.requests.get')\ndef test_process_user(mock_get):\n    # Setup mock response\n    mock_response = Mock()\n    mock_response.json.return_value = {'name': 'john', 'id': 1}\n    mock_get.return_value = mock_response\n    \n    # Act\n    result = process_user(1)\n    \n    # Assert\n    assert result == 'JOHN'\n    mock_get.assert_called_once_with('https://api.example.com/users/1')\n\n# Mock file operations\n@patch('builtins.open', mock_open(read_data='file content'))\ndef test_read_config():\n    result = read_config('config.txt')\n    assert result == 'file content'"
                        },
                        pitfalls: ["Mocking too much", "Wrong patch location", "Not verifying interactions"],
                        concepts: ["Mocking", "Dependency injection", "Test isolation"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "tdd-kata": {
                name: "TDD Kata Series",
                description: "Practice Test-Driven Development with coding katas. Learn the red-green-refactor cycle.",
                difficulty: "intermediate",
                estimatedHours: "10-15",
                prerequisites: ["Unit testing basics", "Refactoring concepts"],
                languages: { recommended: ["Python", "JavaScript", "Java"], also: ["Ruby", "C#"] },
                resources: [
                    { name: "TDD by Example - Kent Beck", url: "https://www.amazon.com/Test-Driven-Development-Kent-Beck/dp/0321146530", type: "book" },
                    { name: "Kata Catalog", url: "https://kata-log.rocks/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "String Calculator Kata",
                        description: "Classic TDD kata for beginners.",
                        criteria: ["Empty string returns 0", "Single number returns that number", "Two numbers comma-separated", "Handle newlines as delimiter", "Custom delimiters"],
                        hints: {
                            level1: "Start with simplest case: empty string.",
                            level2: "Add one test, make it pass, refactor. Repeat.",
                            level3: "# Step by step TDD\n\n# RED: Write failing test\ndef test_empty_string():\n    assert add('') == 0\n\n# GREEN: Minimal code to pass\ndef add(numbers):\n    return 0\n\n# RED: Next test\ndef test_single_number():\n    assert add('1') == 1\n\n# GREEN: Make it pass\ndef add(numbers):\n    if not numbers:\n        return 0\n    return int(numbers)\n\n# RED: Two numbers\ndef test_two_numbers():\n    assert add('1,2') == 3\n\n# GREEN + REFACTOR\ndef add(numbers):\n    if not numbers:\n        return 0\n    return sum(int(n) for n in numbers.split(','))\n\n# Continue: newlines, custom delimiters, negatives..."
                        },
                        pitfalls: ["Writing too much code at once", "Skipping refactor step", "Not running tests after each change"],
                        concepts: ["Red-green-refactor", "Baby steps", "Incremental design"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Bowling Game Kata",
                        description: "Calculate bowling scores with TDD.",
                        criteria: ["Score a gutter game (all zeros)", "Score all ones", "Handle spare", "Handle strike", "Perfect game (all strikes)"],
                        hints: {
                            level1: "Track rolls, calculate score after game.",
                            level2: "Spare: bonus = next roll. Strike: bonus = next two rolls.",
                            level3: "# Test progression:\n# 1. Gutter game\ndef test_gutter_game():\n    game = BowlingGame()\n    for _ in range(20):\n        game.roll(0)\n    assert game.score() == 0\n\n# 2. All ones\ndef test_all_ones():\n    game = BowlingGame()\n    for _ in range(20):\n        game.roll(1)\n    assert game.score() == 20\n\n# 3. One spare\ndef test_one_spare():\n    game = BowlingGame()\n    game.roll(5)\n    game.roll(5)  # Spare\n    game.roll(3)\n    for _ in range(17):\n        game.roll(0)\n    assert game.score() == 16  # 10 + 3 + 3\n\n# Implementation\nclass BowlingGame:\n    def __init__(self):\n        self.rolls = []\n    \n    def roll(self, pins):\n        self.rolls.append(pins)\n    \n    def score(self):\n        total = 0\n        roll_index = 0\n        for frame in range(10):\n            if self._is_strike(roll_index):\n                total += 10 + self.rolls[roll_index+1] + self.rolls[roll_index+2]\n                roll_index += 1\n            elif self._is_spare(roll_index):\n                total += 10 + self.rolls[roll_index+2]\n                roll_index += 2\n            else:\n                total += self.rolls[roll_index] + self.rolls[roll_index+1]\n                roll_index += 2\n        return total"
                        },
                        pitfalls: ["Premature optimization", "Not testing edge cases", "Overcomplicating frame handling"],
                        concepts: ["Domain modeling", "State management", "Emergent design"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Roman Numerals Kata",
                        description: "Convert numbers to/from Roman numerals.",
                        criteria: ["Convert 1-9 to I-IX", "Convert 10-90 to X-XC", "Convert 100-900 to C-CM", "Handle 1-3999 range", "Convert Roman to integer"],
                        hints: {
                            level1: "Start with 1=I, build up to subtraction cases.",
                            level2: "Use lookup table for values.",
                            level3: "# Test-driven approach\nclass TestRomanNumerals:\n    def test_1(self): assert to_roman(1) == 'I'\n    def test_2(self): assert to_roman(2) == 'II'\n    def test_3(self): assert to_roman(3) == 'III'\n    def test_4(self): assert to_roman(4) == 'IV'\n    def test_5(self): assert to_roman(5) == 'V'\n    def test_9(self): assert to_roman(9) == 'IX'\n    def test_10(self): assert to_roman(10) == 'X'\n    # ... continue\n\n# Final implementation\nVALUES = [\n    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'),\n    (100, 'C'), (90, 'XC'), (50, 'L'), (40, 'XL'),\n    (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\n]\n\ndef to_roman(num):\n    result = ''\n    for value, numeral in VALUES:\n        while num >= value:\n            result += numeral\n            num -= value\n    return result\n\ndef from_roman(roman):\n    result = 0\n    i = 0\n    for value, numeral in VALUES:\n        while roman[i:i+len(numeral)] == numeral:\n            result += value\n            i += len(numeral)\n    return result"
                        },
                        pitfalls: ["Not handling subtraction cases", "Off-by-one in ranges", "Hardcoding without pattern"],
                        concepts: ["Lookup tables", "Greedy algorithm", "Bi-directional conversion"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "ci-pipeline": {
                name: "CI Pipeline Setup",
                description: "Set up continuous integration pipelines. Automate testing and build verification.",
                difficulty: "intermediate",
                estimatedHours: "8-12",
                prerequisites: ["Git", "Unit testing", "Command line"],
                languages: { recommended: ["YAML", "Bash"], also: [] },
                resources: [
                    { name: "GitHub Actions Documentation", url: "https://docs.github.com/en/actions", type: "documentation" },
                    { name: "Jenkins Pipeline", url: "https://www.jenkins.io/doc/book/pipeline/", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic GitHub Actions",
                        description: "Set up basic CI with GitHub Actions.",
                        criteria: ["Create workflow YAML file", "Trigger on push and PR", "Run tests", "Show status badge"],
                        hints: {
                            level1: "Workflow files go in .github/workflows/",
                            level2: "Use actions/checkout and actions/setup-* for language.",
                            level3: "# .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      \n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install pytest\n      \n      - name: Run tests\n        run: pytest\n\n# Badge in README:\n# ![CI](https://github.com/user/repo/actions/workflows/ci.yml/badge.svg)"
                        },
                        pitfalls: ["YAML indentation", "Not caching dependencies", "Secrets in logs"],
                        concepts: ["CI/CD basics", "Workflow syntax", "Job runners"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Multi-Job Pipeline",
                        description: "Create pipeline with multiple jobs and stages.",
                        criteria: ["Lint job", "Test job", "Build job", "Job dependencies", "Matrix builds"],
                        hints: {
                            level1: "Use 'needs' to order jobs.",
                            level2: "Matrix strategy for multiple versions.",
                            level3: "jobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: pip install flake8 && flake8 .\n  \n  test:\n    needs: lint\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: ['3.9', '3.10', '3.11']\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n      - run: pip install -r requirements.txt && pytest\n  \n  build:\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: docker build -t myapp .\n      - uses: actions/upload-artifact@v3\n        with:\n          name: docker-image\n          path: ./image.tar"
                        },
                        pitfalls: ["Circular dependencies", "Long-running matrices", "Artifact retention costs"],
                        concepts: ["Job dependencies", "Matrix builds", "Artifacts"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Advanced CI Features",
                        description: "Add caching, secrets, and conditional jobs.",
                        criteria: ["Cache dependencies", "Use secrets safely", "Conditional execution", "Reusable workflows"],
                        hints: {
                            level1: "Cache pip/npm packages to speed up builds.",
                            level2: "Never echo secrets, they're masked.",
                            level3: "jobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Cache pip packages\n        uses: actions/cache@v3\n        with:\n          path: ~/.cache/pip\n          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}\n          restore-keys: |\n            ${{ runner.os }}-pip-\n      \n      - name: Run tests\n        run: pytest\n      \n      - name: Deploy to staging\n        if: github.ref == 'refs/heads/main'\n        env:\n          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }}\n        run: ./deploy.sh staging\n\n  # Conditional job\n  deploy-prod:\n    if: github.event_name == 'release'\n    needs: test\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - run: ./deploy.sh production"
                        },
                        pitfalls: ["Cache invalidation", "Secret rotation", "Conditional syntax"],
                        concepts: ["Caching", "Secrets management", "Environments"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // AI/ML - INTERMEDIATE
            "neural-network-basic": {
                name: "Neural Network (micrograd)",
                description: "Build a minimal neural network library with automatic differentiation. Inspired by Andrej Karpathy's micrograd.",
                difficulty: "intermediate",
                estimatedHours: "15-25",
                prerequisites: ["Calculus (derivatives)", "Python basics", "Linear algebra basics"],
                languages: { recommended: ["Python"], also: ["Julia", "JavaScript"] },
                resources: [
                    { name: "Micrograd Repository", url: "https://github.com/karpathy/micrograd", type: "code" },
                    { name: "Micrograd Video Tutorial", url: "https://www.youtube.com/watch?v=VMj-3S1tku0", type: "video" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Value Class with Autograd",
                        description: "Create Value class that tracks computation graph for automatic differentiation.",
                        criteria: ["Value wraps scalar number", "Tracks children (operands)", "Tracks operation type", "Stores gradient", "Supports +, *, -, /, **"],
                        hints: {
                            level1: "Value stores: data, grad, _backward function, _prev set.",
                            level2: "Each operation creates new Value with backward function.",
                            level3: "class Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n    \n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out"
                        },
                        pitfalls: ["Forgetting += for gradients (accumulation)", "Not handling scalar + Value", "Backward called before forward"],
                        concepts: ["Computational graphs", "Chain rule", "Gradient accumulation"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 2,
                        name: "Backward Pass",
                        description: "Implement backpropagation through the computation graph.",
                        criteria: ["Topological sort of nodes", "Backward pass in reverse order", "Zero gradients before backward", "Handle multiple uses of same value"],
                        hints: {
                            level1: "Topological sort: visit children before parents.",
                            level2: "Start with output.grad = 1, then propagate.",
                            level3: "def backward(self):\n    # Topological sort\n    topo = []\n    visited = set()\n    def build_topo(v):\n        if v not in visited:\n            visited.add(v)\n            for child in v._prev:\n                build_topo(child)\n            topo.append(v)\n    build_topo(self)\n    \n    # Backward pass\n    self.grad = 1\n    for v in reversed(topo):\n        v._backward()"
                        },
                        pitfalls: ["Not zeroing gradients", "Wrong topological order", "Gradient through constants"],
                        concepts: ["Backpropagation", "Topological sort", "Reverse-mode AD"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Neuron and Layer",
                        description: "Build neural network components using Value.",
                        criteria: ["Neuron with weights and bias", "Activation function (tanh/ReLU)", "Layer of neurons", "MLP (Multi-Layer Perceptron)"],
                        hints: {
                            level1: "Neuron: sum(w*x) + b, then activation.",
                            level2: "Layer: list of neurons. MLP: list of layers.",
                            level3: "import random\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(0)\n    \n    def __call__(self, x):\n        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        return act.tanh()\n    \n    def parameters(self):\n        return self.w + [self.b]\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n    \n    def __call__(self, x):\n        return [n(x) for n in self.neurons]\n    \n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n    \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x[0] if len(x) == 1 else x"
                        },
                        pitfalls: ["Not initializing weights properly", "Missing activation on last layer", "Parameter collection incomplete"],
                        concepts: ["Neural network architecture", "Activation functions", "Parameter management"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Training Loop",
                        description: "Train the neural network with gradient descent.",
                        criteria: ["Forward pass (prediction)", "Loss calculation (MSE)", "Backward pass", "Gradient descent update", "Multiple epochs"],
                        hints: {
                            level1: "loss = sum((ypred - ytrue)**2)",
                            level2: "After backward: param.data -= learning_rate * param.grad",
                            level3: "# Training data\nxs = [[2.0, 3.0], [-1.0, -2.0], [3.0, -1.0]]\nys = [1.0, -1.0, 1.0]\n\n# Training loop\nmodel = MLP(2, [4, 4, 1])  # 2 inputs, 2 hidden layers of 4, 1 output\nlearning_rate = 0.05\n\nfor epoch in range(100):\n    # Forward pass\n    ypred = [model(x) for x in xs]\n    loss = sum((yp - yt)**2 for yp, yt in zip(ypred, ys))\n    \n    # Zero gradients\n    for p in model.parameters():\n        p.grad = 0\n    \n    # Backward pass\n    loss.backward()\n    \n    # Update weights\n    for p in model.parameters():\n        p.data -= learning_rate * p.grad\n    \n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Loss: {loss.data}')"
                        },
                        pitfalls: ["Forgetting to zero gradients", "Learning rate too high", "Not enough epochs"],
                        concepts: ["Training loop", "Gradient descent", "Loss functions"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // GAME DEV - INTERMEDIATE
            "platformer": {
                name: "Platformer",
                description: "Build a 2D platformer with physics, jumping, and collision detection. Learn game physics and level design.",
                difficulty: "intermediate",
                estimatedHours: "20-30",
                prerequisites: ["Basic game loop", "2D graphics", "Basic physics"],
                languages: { recommended: ["JavaScript", "Python", "C#"], also: ["C++", "Lua"] },
                resources: [
                    { name: "2D Platformer Tutorial", url: "https://www.youtube.com/results?search_query=2d+platformer+tutorial", type: "video" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Movement and Gravity",
                        description: "Implement player movement with gravity.",
                        criteria: ["Left/right movement with arrow keys", "Gravity pulls player down", "Ground collision stops falling", "Velocity-based movement"],
                        hints: {
                            level1: "velocity.y += gravity each frame. position += velocity.",
                            level2: "Check if player bottom > ground level to detect landing.",
                            level3: "class Player:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        self.vx = 0\n        self.vy = 0\n        self.width = 32\n        self.height = 48\n        self.on_ground = False\n    \n    def update(self, dt):\n        # Gravity\n        self.vy += GRAVITY * dt\n        \n        # Horizontal movement\n        if keys['left']:\n            self.vx = -MOVE_SPEED\n        elif keys['right']:\n            self.vx = MOVE_SPEED\n        else:\n            self.vx = 0\n        \n        # Apply velocity\n        self.x += self.vx * dt\n        self.y += self.vy * dt\n        \n        # Ground collision\n        if self.y + self.height > GROUND_Y:\n            self.y = GROUND_Y - self.height\n            self.vy = 0\n            self.on_ground = True"
                        },
                        pitfalls: ["Gravity too strong or weak", "Not using delta time", "Ground clipping"],
                        concepts: ["Physics simulation", "Velocity and acceleration", "Delta time"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Jumping",
                        description: "Implement jump mechanics with variable height.",
                        criteria: ["Jump when on ground", "Variable jump height (hold to jump higher)", "Coyote time (jump just after leaving edge)", "Jump buffer (press before landing)"],
                        hints: {
                            level1: "Jump = set negative vy. Only allow when on_ground.",
                            level2: "Cut jump short by reducing vy when button released.",
                            level3: "JUMP_VELOCITY = -300\nJUMP_CUT_MULTIPLIER = 0.5\nCOYOTE_TIME = 0.1  # seconds\nJUMP_BUFFER = 0.1\n\nclass Player:\n    def __init__(self):\n        self.coyote_timer = 0\n        self.jump_buffer_timer = 0\n    \n    def update(self, dt):\n        # Coyote time\n        if self.on_ground:\n            self.coyote_timer = COYOTE_TIME\n        else:\n            self.coyote_timer -= dt\n        \n        # Jump buffer\n        if keys_pressed['jump']:\n            self.jump_buffer_timer = JUMP_BUFFER\n        else:\n            self.jump_buffer_timer -= dt\n        \n        # Jump\n        can_jump = self.coyote_timer > 0\n        want_jump = self.jump_buffer_timer > 0\n        if can_jump and want_jump:\n            self.vy = JUMP_VELOCITY\n            self.coyote_timer = 0\n            self.jump_buffer_timer = 0\n        \n        # Variable jump height\n        if keys_released['jump'] and self.vy < 0:\n            self.vy *= JUMP_CUT_MULTIPLIER"
                        },
                        pitfalls: ["Double jump without intending", "Jump feels floaty", "Coyote time too generous"],
                        concepts: ["Jump mechanics", "Coyote time", "Input buffering"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Tile-based Collision",
                        description: "Implement collision with tile-based level.",
                        criteria: ["Load tile map", "Collision with solid tiles", "Separate X and Y collision resolution", "Slope support (optional)"],
                        hints: {
                            level1: "Check which tiles player overlaps with.",
                            level2: "Resolve X and Y separately to handle corners.",
                            level3: "def resolve_collisions(self, tilemap):\n    # Move X, resolve X collisions\n    self.x += self.vx * dt\n    for tile in tilemap.get_colliding_tiles(self.rect):\n        if self.vx > 0:  # Moving right\n            self.x = tile.left - self.width\n        elif self.vx < 0:  # Moving left\n            self.x = tile.right\n        self.vx = 0\n    \n    # Move Y, resolve Y collisions\n    self.y += self.vy * dt\n    self.on_ground = False\n    for tile in tilemap.get_colliding_tiles(self.rect):\n        if self.vy > 0:  # Falling\n            self.y = tile.top - self.height\n            self.on_ground = True\n        elif self.vy < 0:  # Jumping\n            self.y = tile.bottom\n        self.vy = 0\n\nclass Tilemap:\n    def get_colliding_tiles(self, rect):\n        tiles = []\n        x1, y1 = int(rect.left // TILE_SIZE), int(rect.top // TILE_SIZE)\n        x2, y2 = int(rect.right // TILE_SIZE), int(rect.bottom // TILE_SIZE)\n        for y in range(y1, y2 + 1):\n            for x in range(x1, x2 + 1):\n                if self.is_solid(x, y):\n                    tiles.append(Rect(x * TILE_SIZE, y * TILE_SIZE, TILE_SIZE, TILE_SIZE))\n        return tiles"
                        },
                        pitfalls: ["Corner clipping", "Tunneling at high speeds", "Off-by-one in tile lookup"],
                        concepts: ["AABB collision", "Collision resolution", "Tile maps"],
                        estimatedHours: "5-6"
                    },
                    {
                        id: 4,
                        name: "Enemies and Hazards",
                        description: "Add enemies and death/respawn system.",
                        criteria: ["Simple enemy with patrol behavior", "Player dies on enemy contact", "Respawn at checkpoint", "Stomping enemies (optional)"],
                        hints: {
                            level1: "Enemy walks left/right, reverses at edges or walls.",
                            level2: "Check player-enemy overlap for damage.",
                            level3: "class Enemy:\n    def __init__(self, x, y, patrol_distance):\n        self.x = x\n        self.y = y\n        self.start_x = x\n        self.direction = 1\n        self.patrol_distance = patrol_distance\n    \n    def update(self, dt, tilemap):\n        self.x += ENEMY_SPEED * self.direction * dt\n        \n        # Reverse at patrol bounds\n        if abs(self.x - self.start_x) > self.patrol_distance:\n            self.direction *= -1\n        \n        # Reverse at walls or edges\n        next_tile_x = int((self.x + self.width/2 + self.direction * self.width/2) / TILE_SIZE)\n        floor_tile_y = int((self.y + self.height + 1) / TILE_SIZE)\n        if not tilemap.is_solid(next_tile_x, floor_tile_y):  # No floor ahead\n            self.direction *= -1\n\nclass Player:\n    def check_enemy_collision(self, enemies):\n        for enemy in enemies:\n            if self.rect.colliderect(enemy.rect):\n                if self.vy > 0 and self.y + self.height < enemy.y + 10:\n                    # Stomped enemy\n                    enemy.die()\n                    self.vy = BOUNCE_VELOCITY\n                else:\n                    # Player takes damage\n                    self.die()"
                        },
                        pitfalls: ["Stomp detection window", "Death during invincibility", "Enemy stuck at patrol bounds"],
                        concepts: ["AI patrol behavior", "Health systems", "Collision types"],
                        estimatedHours: "4-6"
                    }
                ]
            },

            // DISTRIBUTED - BEGINNER
            "service-discovery": {
                name: "Service Discovery",
                description: "Build a service registry with health checking. Learn how microservices find and communicate with each other.",
                difficulty: "beginner",
                estimatedHours: "10-15",
                prerequisites: ["HTTP basics", "Networking concepts"],
                languages: { recommended: ["Go", "Python", "JavaScript"], also: ["Java", "Rust"] },
                resources: [
                    { name: "Service Discovery Patterns", url: "https://microservices.io/patterns/service-registry.html", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Service Registry",
                        description: "Build basic service registration and lookup.",
                        criteria: ["Register service with name, host, port", "Deregister service", "List services by name", "Get all registered services"],
                        hints: {
                            level1: "Store services in a dict: name -> list of instances.",
                            level2: "Each instance has unique ID, host, port, metadata.",
                            level3: "import uuid\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n@dataclass\nclass ServiceInstance:\n    id: str\n    name: str\n    host: str\n    port: int\n    metadata: dict = None\n\nclass ServiceRegistry:\n    def __init__(self):\n        self.services: Dict[str, Dict[str, ServiceInstance]] = {}\n    \n    def register(self, name: str, host: str, port: int, metadata: dict = None) -> str:\n        instance_id = str(uuid.uuid4())\n        instance = ServiceInstance(instance_id, name, host, port, metadata or {})\n        if name not in self.services:\n            self.services[name] = {}\n        self.services[name][instance_id] = instance\n        return instance_id\n    \n    def deregister(self, name: str, instance_id: str):\n        if name in self.services and instance_id in self.services[name]:\n            del self.services[name][instance_id]\n    \n    def get_instances(self, name: str) -> List[ServiceInstance]:\n        return list(self.services.get(name, {}).values())"
                        },
                        pitfalls: ["Not handling duplicate registrations", "Race conditions", "Memory growth without cleanup"],
                        concepts: ["Service registry pattern", "Instance identity", "Lookup by name"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Health Checking",
                        description: "Add health checks to detect failed services.",
                        criteria: ["HTTP health endpoint on services", "Registry polls health endpoint", "Remove unhealthy instances", "Configurable check interval"],
                        hints: {
                            level1: "Each service exposes /health returning 200.",
                            level2: "Background thread checks all instances periodically.",
                            level3: "import threading\nimport requests\nimport time\n\nclass ServiceRegistry:\n    def __init__(self, health_check_interval=10):\n        self.services = {}\n        self.health_check_interval = health_check_interval\n        self._start_health_checker()\n    \n    def _start_health_checker(self):\n        def check_loop():\n            while True:\n                self._check_all_health()\n                time.sleep(self.health_check_interval)\n        thread = threading.Thread(target=check_loop, daemon=True)\n        thread.start()\n    \n    def _check_all_health(self):\n        for name, instances in list(self.services.items()):\n            for instance_id, instance in list(instances.items()):\n                try:\n                    url = f'http://{instance.host}:{instance.port}/health'\n                    resp = requests.get(url, timeout=5)\n                    if resp.status_code != 200:\n                        self._mark_unhealthy(name, instance_id)\n                except Exception:\n                    self._mark_unhealthy(name, instance_id)\n    \n    def _mark_unhealthy(self, name, instance_id):\n        # Could implement retry logic or immediate removal\n        self.deregister(name, instance_id)\n        print(f'Removed unhealthy instance: {name}/{instance_id}')"
                        },
                        pitfalls: ["Network partition false positives", "Check timeout too short", "Not handling transient failures"],
                        concepts: ["Health checking", "Failure detection", "Background tasks"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "HTTP API",
                        description: "Expose registry operations via HTTP API.",
                        criteria: ["POST /services to register", "DELETE /services/{name}/{id} to deregister", "GET /services/{name} to list instances", "GET /services to list all"],
                        hints: {
                            level1: "Use Flask/FastAPI/Express for HTTP server.",
                            level2: "Return JSON responses with proper status codes.",
                            level3: "from flask import Flask, request, jsonify\n\napp = Flask(__name__)\nregistry = ServiceRegistry()\n\n@app.route('/services', methods=['POST'])\ndef register_service():\n    data = request.json\n    instance_id = registry.register(\n        name=data['name'],\n        host=data['host'],\n        port=data['port'],\n        metadata=data.get('metadata')\n    )\n    return jsonify({'id': instance_id}), 201\n\n@app.route('/services/<name>/<instance_id>', methods=['DELETE'])\ndef deregister_service(name, instance_id):\n    registry.deregister(name, instance_id)\n    return '', 204\n\n@app.route('/services/<name>', methods=['GET'])\ndef get_service(name):\n    instances = registry.get_instances(name)\n    return jsonify([{\n        'id': i.id,\n        'host': i.host,\n        'port': i.port,\n        'metadata': i.metadata\n    } for i in instances])\n\n@app.route('/services', methods=['GET'])\ndef list_all_services():\n    return jsonify(list(registry.services.keys()))"
                        },
                        pitfalls: ["Missing validation", "Not thread-safe", "Error handling"],
                        concepts: ["REST API design", "HTTP methods", "JSON serialization"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // DISTRIBUTED - INTERMEDIATE (already have rate-limiter, load-balancer)
            "vector-clocks": {
                name: "Vector Clocks",
                description: "Implement vector clocks for tracking causality in distributed systems. Learn about logical time and partial ordering.",
                difficulty: "intermediate",
                estimatedHours: "8-12",
                prerequisites: ["Distributed systems basics", "Understanding of causality"],
                languages: { recommended: ["Python", "Go", "Java"], also: ["Rust", "JavaScript"] },
                resources: [
                    { name: "Vector Clocks Paper", url: "https://en.wikipedia.org/wiki/Vector_clock", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Vector Clock",
                        description: "Implement vector clock data structure and operations.",
                        criteria: ["Initialize clock for N nodes", "Increment local clock on event", "Merge clocks on message receive", "Compare clocks (before, after, concurrent)"],
                        hints: {
                            level1: "Vector clock is dict/array of counters, one per node.",
                            level2: "Merge: take max of each component.",
                            level3: "class VectorClock:\n    def __init__(self, node_id, num_nodes):\n        self.node_id = node_id\n        self.clock = [0] * num_nodes\n    \n    def increment(self):\n        \"\"\"Called on local event\"\"\"\n        self.clock[self.node_id] += 1\n    \n    def update(self, other_clock):\n        \"\"\"Called when receiving message\"\"\"\n        for i in range(len(self.clock)):\n            self.clock[i] = max(self.clock[i], other_clock[i])\n        self.increment()  # Receive is also an event\n    \n    def send(self):\n        \"\"\"Called when sending message, returns clock to attach\"\"\"\n        self.increment()\n        return self.clock.copy()\n    \n    def compare(self, other):\n        \"\"\"Returns 'before', 'after', 'concurrent', or 'equal'\"\"\"\n        less = all(a <= b for a, b in zip(self.clock, other))\n        greater = all(a >= b for a, b in zip(self.clock, other))\n        if less and greater:\n            return 'equal'\n        if less:\n            return 'before'\n        if greater:\n            return 'after'\n        return 'concurrent'"
                        },
                        pitfalls: ["Forgetting to increment on receive", "Not copying clock on send", "Comparison logic"],
                        concepts: ["Vector clocks", "Causality", "Partial ordering"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Conflict Detection",
                        description: "Use vector clocks to detect conflicting updates.",
                        criteria: ["Detect concurrent writes to same key", "Store version with each value", "Return all concurrent versions on read", "Last-writer-wins (optional)"],
                        hints: {
                            level1: "Concurrent clocks = neither happened-before the other.",
                            level2: "Keep list of (value, clock) pairs for conflicts.",
                            level3: "class VersionedStore:\n    def __init__(self):\n        self.data = {}  # key -> list of (value, clock)\n    \n    def put(self, key, value, clock):\n        if key not in self.data:\n            self.data[key] = []\n        \n        # Remove versions this one supersedes\n        new_versions = []\n        for old_value, old_clock in self.data[key]:\n            relation = self._compare(old_clock, clock)\n            if relation == 'after' or relation == 'equal':\n                # Old version supersedes or equals new, keep it\n                new_versions.append((old_value, old_clock))\n            elif relation == 'concurrent':\n                # Conflict! Keep both\n                new_versions.append((old_value, old_clock))\n            # 'before' means old is superseded, don't keep\n        \n        new_versions.append((value, clock))\n        self.data[key] = new_versions\n    \n    def get(self, key):\n        \"\"\"Returns list of (value, clock) - multiple means conflict\"\"\"\n        return self.data.get(key, [])"
                        },
                        pitfalls: ["Growing version lists without cleanup", "Comparison direction", "Lost updates"],
                        concepts: ["Conflict detection", "Version vectors", "Multi-value registers"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // SECURITY - INTERMEDIATE
            "jwt-impl": {
                name: "JWT Library",
                description: "Implement JSON Web Token signing and verification. Learn about authentication tokens and cryptographic signatures.",
                difficulty: "intermediate",
                estimatedHours: "8-12",
                prerequisites: ["JSON", "Base64", "HMAC basics"],
                languages: { recommended: ["Python", "JavaScript", "Go"], also: ["Java", "Rust"] },
                resources: [
                    { name: "JWT Specification (RFC 7519)", url: "https://tools.ietf.org/html/rfc7519", type: "specification" },
                    { name: "JWT.io Debugger", url: "https://jwt.io/", type: "tool" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "JWT Structure",
                        description: "Implement JWT encoding without signing.",
                        criteria: ["Create header with algorithm", "Encode payload claims", "Base64url encoding", "Combine header.payload.signature"],
                        hints: {
                            level1: "JWT = base64url(header).base64url(payload).signature",
                            level2: "Base64url: base64 with + -> -, / -> _, no padding.",
                            level3: "import json\nimport base64\n\ndef base64url_encode(data: bytes) -> str:\n    return base64.urlsafe_b64encode(data).rstrip(b'=').decode('ascii')\n\ndef base64url_decode(data: str) -> bytes:\n    padding = 4 - len(data) % 4\n    if padding != 4:\n        data += '=' * padding\n    return base64.urlsafe_b64decode(data)\n\ndef encode_jwt(payload: dict, secret: str, algorithm='HS256') -> str:\n    header = {'alg': algorithm, 'typ': 'JWT'}\n    header_b64 = base64url_encode(json.dumps(header).encode())\n    payload_b64 = base64url_encode(json.dumps(payload).encode())\n    \n    message = f'{header_b64}.{payload_b64}'\n    signature = sign(message, secret, algorithm)\n    signature_b64 = base64url_encode(signature)\n    \n    return f'{message}.{signature_b64}'"
                        },
                        pitfalls: ["Regular base64 vs base64url", "Padding removal/addition", "JSON key ordering"],
                        concepts: ["JWT structure", "Base64url", "Token format"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "HMAC Signing",
                        description: "Implement HS256 signing and verification.",
                        criteria: ["Sign with HMAC-SHA256", "Verify signature matches", "Constant-time comparison", "Handle invalid tokens"],
                        hints: {
                            level1: "HMAC-SHA256: hmac.new(key, message, sha256).",
                            level2: "Compare signatures with constant-time function.",
                            level3: "import hmac\nimport hashlib\n\ndef sign(message: str, secret: str, algorithm: str) -> bytes:\n    if algorithm != 'HS256':\n        raise ValueError(f'Unsupported algorithm: {algorithm}')\n    return hmac.new(\n        secret.encode(),\n        message.encode(),\n        hashlib.sha256\n    ).digest()\n\ndef verify_jwt(token: str, secret: str) -> dict:\n    parts = token.split('.')\n    if len(parts) != 3:\n        raise ValueError('Invalid token format')\n    \n    header_b64, payload_b64, signature_b64 = parts\n    \n    # Decode header to get algorithm\n    header = json.loads(base64url_decode(header_b64))\n    algorithm = header.get('alg')\n    \n    # Compute expected signature\n    message = f'{header_b64}.{payload_b64}'\n    expected_sig = sign(message, secret, algorithm)\n    actual_sig = base64url_decode(signature_b64)\n    \n    # Constant-time comparison\n    if not hmac.compare_digest(expected_sig, actual_sig):\n        raise ValueError('Invalid signature')\n    \n    return json.loads(base64url_decode(payload_b64))"
                        },
                        pitfalls: ["Timing attacks in comparison", "Algorithm confusion attacks", "Key encoding"],
                        concepts: ["HMAC", "Digital signatures", "Token verification"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Claims Validation",
                        description: "Implement standard JWT claim validation.",
                        criteria: ["exp: token expiration", "nbf: not before", "iat: issued at", "iss: issuer", "aud: audience"],
                        hints: {
                            level1: "exp and nbf are Unix timestamps.",
                            level2: "Add clock skew tolerance for exp/nbf.",
                            level3: "import time\n\ndef validate_claims(payload: dict, audience: str = None, issuer: str = None, clock_skew: int = 60):\n    now = time.time()\n    \n    # Expiration\n    if 'exp' in payload:\n        if now > payload['exp'] + clock_skew:\n            raise ValueError('Token expired')\n    \n    # Not Before\n    if 'nbf' in payload:\n        if now < payload['nbf'] - clock_skew:\n            raise ValueError('Token not yet valid')\n    \n    # Issuer\n    if issuer and payload.get('iss') != issuer:\n        raise ValueError(f'Invalid issuer: {payload.get(\"iss\")}')\n    \n    # Audience\n    if audience:\n        token_aud = payload.get('aud')\n        if isinstance(token_aud, list):\n            if audience not in token_aud:\n                raise ValueError('Invalid audience')\n        elif token_aud != audience:\n            raise ValueError('Invalid audience')\n\ndef decode_jwt(token: str, secret: str, audience: str = None, issuer: str = None) -> dict:\n    payload = verify_jwt(token, secret)\n    validate_claims(payload, audience, issuer)\n    return payload"
                        },
                        pitfalls: ["Clock skew handling", "Audience as list", "Missing required claims"],
                        concepts: ["JWT claims", "Token validation", "Time-based security"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // APP-DEV - BEGINNER
            "portfolio-site": {
                name: "Portfolio Website",
                description: "Build a personal portfolio site with responsive design. Learn HTML/CSS fundamentals and web design principles.",
                difficulty: "beginner",
                estimatedHours: "8-12",
                prerequisites: ["HTML basics", "CSS basics"],
                languages: { recommended: ["HTML", "CSS", "JavaScript"], also: [] },
                resources: [
                    { name: "MDN Web Docs", url: "https://developer.mozilla.org/en-US/docs/Learn", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "HTML Structure",
                        description: "Create semantic HTML structure for the portfolio.",
                        criteria: ["Header with navigation", "About/intro section", "Projects section", "Contact section", "Footer"],
                        hints: {
                            level1: "Use semantic tags: header, nav, main, section, footer.",
                            level2: "Add proper meta tags for SEO.",
                            level3: "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <meta name=\"description\" content=\"Portfolio of [Your Name] - Web Developer\">\n    <title>Your Name - Portfolio</title>\n    <link rel=\"stylesheet\" href=\"styles.css\">\n</head>\n<body>\n    <header>\n        <nav>\n            <a href=\"#\" class=\"logo\">YN</a>\n            <ul class=\"nav-links\">\n                <li><a href=\"#about\">About</a></li>\n                <li><a href=\"#projects\">Projects</a></li>\n                <li><a href=\"#contact\">Contact</a></li>\n            </ul>\n        </nav>\n    </header>\n    <main>\n        <section id=\"hero\">...</section>\n        <section id=\"about\">...</section>\n        <section id=\"projects\">...</section>\n        <section id=\"contact\">...</section>\n    </main>\n    <footer>...</footer>\n</body>\n</html>"
                        },
                        pitfalls: ["Missing viewport meta", "Non-semantic div soup", "Broken anchor links"],
                        concepts: ["Semantic HTML", "Document structure", "Accessibility basics"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "CSS Styling",
                        description: "Style the portfolio with modern CSS.",
                        criteria: ["Consistent color scheme", "Typography hierarchy", "Spacing and layout", "Hover effects", "Smooth scrolling"],
                        hints: {
                            level1: "Use CSS variables for colors and spacing.",
                            level2: "Create utility classes for common patterns.",
                            level3: ":root {\n    --primary: #3b82f6;\n    --text: #1f2937;\n    --bg: #ffffff;\n    --spacing-sm: 0.5rem;\n    --spacing-md: 1rem;\n    --spacing-lg: 2rem;\n}\n\n* {\n    box-sizing: border-box;\n    margin: 0;\n    padding: 0;\n}\n\nhtml {\n    scroll-behavior: smooth;\n}\n\nbody {\n    font-family: system-ui, sans-serif;\n    color: var(--text);\n    line-height: 1.6;\n}\n\n.container {\n    max-width: 1200px;\n    margin: 0 auto;\n    padding: 0 var(--spacing-md);\n}\n\nsection {\n    padding: var(--spacing-lg) 0;\n}\n\na {\n    color: var(--primary);\n    text-decoration: none;\n    transition: opacity 0.2s;\n}\n\na:hover {\n    opacity: 0.8;\n}"
                        },
                        pitfalls: ["Inconsistent spacing", "Too many fonts", "Low contrast text"],
                        concepts: ["CSS variables", "Box model", "Typography"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Responsive Design",
                        description: "Make the site work on all screen sizes.",
                        criteria: ["Mobile-first approach", "Responsive navigation", "Flexible images", "Grid/flexbox layouts", "Media queries for breakpoints"],
                        hints: {
                            level1: "Start with mobile styles, add media queries for larger screens.",
                            level2: "Use flexbox for navigation, CSS grid for project cards.",
                            level3: ".nav-links {\n    display: flex;\n    gap: var(--spacing-md);\n}\n\n.projects-grid {\n    display: grid;\n    grid-template-columns: 1fr;\n    gap: var(--spacing-lg);\n}\n\n@media (min-width: 768px) {\n    .projects-grid {\n        grid-template-columns: repeat(2, 1fr);\n    }\n}\n\n@media (min-width: 1024px) {\n    .projects-grid {\n        grid-template-columns: repeat(3, 1fr);\n    }\n}\n\n/* Mobile nav toggle */\n.nav-toggle {\n    display: none;\n}\n\n@media (max-width: 767px) {\n    .nav-toggle {\n        display: block;\n    }\n    .nav-links {\n        display: none;\n        flex-direction: column;\n    }\n    .nav-links.active {\n        display: flex;\n    }\n}"
                        },
                        pitfalls: ["Fixed widths breaking layout", "Horizontal scroll on mobile", "Touch target too small"],
                        concepts: ["Responsive design", "Flexbox/Grid", "Media queries"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // DATA-STORAGE - INTERMEDIATE
            "btree-impl": {
                name: "B-tree Implementation",
                description: "Implement a B-tree data structure. Learn disk-friendly tree structures used in databases and file systems.",
                difficulty: "intermediate",
                estimatedHours: "15-25",
                prerequisites: ["Trees basics", "Algorithm complexity"],
                languages: { recommended: ["C", "Python", "Rust"], also: ["Go", "Java"] },
                resources: [
                    { name: "B-tree Wikipedia", url: "https://en.wikipedia.org/wiki/B-tree", type: "article" },
                    { name: "B-tree Visualization", url: "https://www.cs.usfca.edu/~galles/visualization/BTree.html", type: "interactive" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "B-tree Node Structure",
                        description: "Define B-tree node with keys and children.",
                        criteria: ["Node holds up to 2t-1 keys", "Node has up to 2t children", "Keys are sorted within node", "Leaf indicator"],
                        hints: {
                            level1: "t = minimum degree. Node has t-1 to 2t-1 keys (except root).",
                            level2: "Children array has one more element than keys array.",
                            level3: "class BTreeNode:\n    def __init__(self, t, is_leaf=True):\n        self.t = t  # Minimum degree\n        self.keys = []  # List of keys\n        self.children = []  # List of child nodes\n        self.is_leaf = is_leaf\n    \n    @property\n    def is_full(self):\n        return len(self.keys) == 2 * self.t - 1\n    \n    @property\n    def is_underflow(self):\n        return len(self.keys) < self.t - 1\n\nclass BTree:\n    def __init__(self, t):\n        self.t = t  # Minimum degree\n        self.root = BTreeNode(t)"
                        },
                        pitfalls: ["Off-by-one in capacity", "Not tracking leaf status", "Children count mismatch"],
                        concepts: ["B-tree properties", "Node capacity", "Tree structure"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Search",
                        description: "Implement search operation.",
                        criteria: ["Binary search within node", "Recurse to appropriate child", "Return key and node if found", "Handle not found"],
                        hints: {
                            level1: "Find position where key would be, then check or descend.",
                            level2: "Use bisect for binary search in Python.",
                            level3: "import bisect\n\ndef search(self, key, node=None):\n    if node is None:\n        node = self.root\n    \n    # Find position where key would be\n    i = bisect.bisect_left(node.keys, key)\n    \n    # Check if found\n    if i < len(node.keys) and node.keys[i] == key:\n        return (node, i)\n    \n    # If leaf, key not in tree\n    if node.is_leaf:\n        return None\n    \n    # Recurse to appropriate child\n    return self.search(key, node.children[i])"
                        },
                        pitfalls: ["Index bounds", "Leaf check", "Key comparison"],
                        concepts: ["Binary search", "Tree traversal", "Key lookup"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Insert with Split",
                        description: "Implement insertion with node splitting.",
                        criteria: ["Insert into non-full leaf", "Split full nodes proactively", "Split root to grow tree", "Maintain sorted order"],
                        hints: {
                            level1: "Split before descending if child is full (proactive split).",
                            level2: "Split: median goes to parent, left/right halves become children.",
                            level3: "def insert(self, key):\n    root = self.root\n    if root.is_full:\n        # Create new root\n        new_root = BTreeNode(self.t, is_leaf=False)\n        new_root.children.append(self.root)\n        self._split_child(new_root, 0)\n        self.root = new_root\n    self._insert_non_full(self.root, key)\n\ndef _split_child(self, parent, i):\n    t = self.t\n    child = parent.children[i]\n    new_node = BTreeNode(t, child.is_leaf)\n    \n    # Middle key goes to parent\n    mid = t - 1\n    parent.keys.insert(i, child.keys[mid])\n    \n    # Right half goes to new node\n    new_node.keys = child.keys[mid+1:]\n    child.keys = child.keys[:mid]\n    \n    if not child.is_leaf:\n        new_node.children = child.children[t:]\n        child.children = child.children[:t]\n    \n    parent.children.insert(i + 1, new_node)\n\ndef _insert_non_full(self, node, key):\n    i = bisect.bisect_left(node.keys, key)\n    if node.is_leaf:\n        node.keys.insert(i, key)\n    else:\n        if node.children[i].is_full:\n            self._split_child(node, i)\n            if key > node.keys[i]:\n                i += 1\n        self._insert_non_full(node.children[i], key)"
                        },
                        pitfalls: ["Split at wrong position", "Not updating root", "Child index after split"],
                        concepts: ["Node splitting", "Proactive split", "Tree growth"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 4,
                        name: "Delete with Rebalancing",
                        description: "Implement deletion with borrowing and merging.",
                        criteria: ["Delete from leaf", "Delete from internal node (predecessor/successor)", "Borrow from sibling", "Merge nodes when underflow"],
                        hints: {
                            level1: "Internal delete: replace with predecessor/successor, delete from leaf.",
                            level2: "Before descending, ensure child has >= t keys.",
                            level3: "def delete(self, key):\n    self._delete(self.root, key)\n    # Shrink tree if root is empty\n    if len(self.root.keys) == 0 and not self.root.is_leaf:\n        self.root = self.root.children[0]\n\ndef _delete(self, node, key):\n    i = bisect.bisect_left(node.keys, key)\n    \n    if i < len(node.keys) and node.keys[i] == key:\n        if node.is_leaf:\n            node.keys.pop(i)\n        else:\n            self._delete_internal(node, i)\n    elif not node.is_leaf:\n        self._ensure_child_not_minimal(node, i)\n        # Recompute i after potential restructuring\n        i = bisect.bisect_left(node.keys, key)\n        if i > len(node.keys):\n            i = len(node.keys)\n        self._delete(node.children[i], key)\n\ndef _ensure_child_not_minimal(self, parent, i):\n    child = parent.children[i]\n    if len(child.keys) >= self.t:\n        return\n    # Try borrowing from siblings or merge\n    # ... (borrow from left, borrow from right, or merge)"
                        },
                        pitfalls: ["Merge vs borrow decision", "Predecessor vs successor choice", "Root shrinking"],
                        concepts: ["Node merging", "Sibling borrowing", "Tree shrinking"],
                        estimatedHours: "5-8"
                    }
                ]
            },

            // AI/ML - INTERMEDIATE
            "word2vec": {
                name: "Word Embeddings (Word2Vec)",
                description: "Implement Word2Vec from scratch using Skip-gram or CBOW. Learn how words become dense vectors capturing semantic meaning.",
                difficulty: "intermediate",
                estimatedHours: "15-25",
                prerequisites: ["Neural network basics", "Linear algebra", "Python/NumPy"],
                languages: { recommended: ["Python"], also: ["Julia", "C++"] },
                resources: [
                    { name: "Word2Vec Paper", url: "https://arxiv.org/abs/1301.3781", type: "paper" },
                    { name: "Word2Vec Tutorial", url: "https://www.tensorflow.org/tutorials/text/word2vec", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Data Preprocessing",
                        description: "Prepare text corpus for training.",
                        criteria: ["Tokenize text into words", "Build vocabulary with word-to-index mapping", "Create training pairs (context, target)", "Implement subsampling of frequent words"],
                        hints: {
                            level1: "Skip-gram: given center word, predict context words.",
                            level2: "For window size 2: 'the cat sat' -> (cat, the), (cat, sat).",
                            level3: "import re\nfrom collections import Counter\n\ndef preprocess(text, min_count=5):\n    # Tokenize\n    words = re.findall(r'\\w+', text.lower())\n    \n    # Build vocab\n    word_counts = Counter(words)\n    vocab = {w: i for i, (w, c) in enumerate(word_counts.items()) if c >= min_count}\n    \n    # Convert to indices\n    data = [vocab[w] for w in words if w in vocab]\n    return data, vocab\n\ndef generate_training_pairs(data, window_size=2):\n    pairs = []\n    for i, center in enumerate(data):\n        for j in range(max(0, i-window_size), min(len(data), i+window_size+1)):\n            if i != j:\n                pairs.append((center, data[j]))\n    return pairs"
                        },
                        pitfalls: ["Vocabulary too large", "Not removing rare words", "Memory issues with large corpus"],
                        concepts: ["Tokenization", "Vocabulary building", "Skip-gram pairs"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Skip-gram Model",
                        description: "Implement the Skip-gram neural network.",
                        criteria: ["Embedding layer (input)", "Output layer (context prediction)", "Softmax over vocabulary", "Forward pass implementation"],
                        hints: {
                            level1: "Two matrices: W_in (vocab x embed_dim), W_out (embed_dim x vocab).",
                            level2: "Forward: embed = W_in[word], scores = embed @ W_out, probs = softmax(scores).",
                            level3: "import numpy as np\n\nclass SkipGram:\n    def __init__(self, vocab_size, embed_dim):\n        self.W_in = np.random.randn(vocab_size, embed_dim) * 0.01\n        self.W_out = np.random.randn(embed_dim, vocab_size) * 0.01\n    \n    def forward(self, center_word):\n        # Get embedding\n        self.embed = self.W_in[center_word]  # (embed_dim,)\n        # Compute scores\n        self.scores = self.embed @ self.W_out  # (vocab_size,)\n        # Softmax\n        exp_scores = np.exp(self.scores - np.max(self.scores))\n        self.probs = exp_scores / exp_scores.sum()\n        return self.probs"
                        },
                        pitfalls: ["Softmax numerical stability", "Wrong matrix dimensions", "Embedding lookup indexing"],
                        concepts: ["Word embeddings", "Softmax classification", "Neural network layers"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Training with Negative Sampling",
                        description: "Implement efficient training with negative sampling.",
                        criteria: ["Cross-entropy loss", "Negative sampling instead of full softmax", "Gradient computation", "SGD parameter updates"],
                        hints: {
                            level1: "Full softmax is O(vocab_size). Negative sampling is O(k) where k ~ 5-20.",
                            level2: "Sample negatives according to word frequency ^ 0.75.",
                            level3: "def negative_sampling_loss(self, center, context, neg_samples):\n    # Positive sample\n    pos_embed = self.W_in[center]\n    pos_context = self.W_out[:, context]\n    pos_score = sigmoid(pos_embed @ pos_context)\n    pos_loss = -np.log(pos_score + 1e-10)\n    \n    # Negative samples\n    neg_loss = 0\n    for neg in neg_samples:\n        neg_context = self.W_out[:, neg]\n        neg_score = sigmoid(-pos_embed @ neg_context)\n        neg_loss -= np.log(neg_score + 1e-10)\n    \n    return pos_loss + neg_loss\n\ndef train_step(self, center, context, neg_samples, lr=0.01):\n    # Compute gradients and update\n    pos_embed = self.W_in[center]\n    pos_context = self.W_out[:, context]\n    \n    # Gradient for positive\n    pos_score = sigmoid(pos_embed @ pos_context)\n    grad_pos = (pos_score - 1) * pos_context\n    \n    # Update embeddings\n    self.W_in[center] -= lr * grad_pos\n    self.W_out[:, context] -= lr * (pos_score - 1) * pos_embed"
                        },
                        pitfalls: ["Sampling distribution", "Gradient computation errors", "Learning rate too high"],
                        concepts: ["Negative sampling", "Cross-entropy loss", "Stochastic gradient descent"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 4,
                        name: "Evaluation & Visualization",
                        description: "Evaluate embeddings and visualize results.",
                        criteria: ["Find similar words (cosine similarity)", "Analogy task: king - man + woman = queen", "Visualize with t-SNE/PCA", "Save and load embeddings"],
                        hints: {
                            level1: "Cosine similarity = dot(a, b) / (norm(a) * norm(b)).",
                            level2: "Analogy: vec(king) - vec(man) + vec(woman) ‚âà vec(queen).",
                            level3: "def most_similar(self, word, top_k=10):\n    if word not in self.vocab:\n        return []\n    word_vec = self.W_in[self.vocab[word]]\n    word_vec = word_vec / np.linalg.norm(word_vec)\n    \n    similarities = self.W_in @ word_vec\n    norms = np.linalg.norm(self.W_in, axis=1)\n    similarities = similarities / (norms + 1e-10)\n    \n    top_indices = similarities.argsort()[-top_k-1:-1][::-1]\n    idx_to_word = {i: w for w, i in self.vocab.items()}\n    return [(idx_to_word[i], similarities[i]) for i in top_indices]\n\ndef analogy(self, a, b, c):\n    # a is to b as c is to ?\n    vec = self.W_in[self.vocab[b]] - self.W_in[self.vocab[a]] + self.W_in[self.vocab[c]]\n    return self.most_similar_vec(vec)"
                        },
                        pitfalls: ["Normalizing vectors", "Including query word in results", "Memory for large vocab"],
                        concepts: ["Cosine similarity", "Word analogies", "Dimensionality reduction"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // AI/ML - ADVANCED
            "transformer-scratch": {
                name: "Transformer from Scratch",
                description: "Implement the Transformer architecture from 'Attention Is All You Need'. Learn self-attention and modern NLP architectures.",
                difficulty: "advanced",
                estimatedHours: "30-50",
                prerequisites: ["Neural networks", "Linear algebra", "Python/PyTorch basics"],
                languages: { recommended: ["Python"], also: ["Julia"] },
                resources: [
                    { name: "Attention Is All You Need Paper", url: "https://arxiv.org/abs/1706.03762", type: "paper" },
                    { name: "The Illustrated Transformer", url: "https://jalammar.github.io/illustrated-transformer/", type: "article" },
                    { name: "Annotated Transformer", url: "https://nlp.seas.harvard.edu/2018/04/03/attention.html", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Scaled Dot-Product Attention",
                        description: "Implement the core attention mechanism.",
                        criteria: ["Compute Q, K, V from input", "Scaled dot-product: softmax(QK^T / sqrt(d_k))V", "Handle attention mask for padding/causal", "Vectorized implementation"],
                        hints: {
                            level1: "Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V",
                            level2: "Mask: set masked positions to -inf before softmax.",
                            level3: "import torch\nimport torch.nn.functional as F\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    # Q, K, V: (batch, seq_len, d_k)\n    d_k = Q.size(-1)\n    \n    # Compute attention scores\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)\n    # scores: (batch, seq_len, seq_len)\n    \n    # Apply mask\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    \n    # Softmax and apply to values\n    attn_weights = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn_weights, V)\n    \n    return output, attn_weights"
                        },
                        pitfalls: ["Forgetting to scale by sqrt(d_k)", "Wrong mask application", "Dimension mismatches"],
                        concepts: ["Attention mechanism", "Query-Key-Value", "Masking"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 2,
                        name: "Multi-Head Attention",
                        description: "Implement multi-head attention with multiple parallel heads.",
                        criteria: ["Split into h heads", "Project Q, K, V for each head", "Parallel attention computation", "Concatenate and project output"],
                        hints: {
                            level1: "Split d_model into h heads of d_k = d_model/h each.",
                            level2: "Linear projections: W_q, W_k, W_v, W_o.",
                            level3: "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        # Linear projections and reshape to (batch, heads, seq, d_k)\n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Attention\n        attn_output, _ = scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Concatenate heads\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n        \n        return self.W_o(attn_output)"
                        },
                        pitfalls: ["Reshape vs view errors", "Transpose dimensions wrong", "Not using contiguous()"],
                        concepts: ["Multi-head attention", "Parallel computation", "Linear projections"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 3,
                        name: "Position-wise Feed-Forward & Embeddings",
                        description: "Implement FFN layer and positional embeddings.",
                        criteria: ["Two-layer FFN with ReLU", "Positional encoding (sinusoidal)", "Token embeddings", "Embedding scaling"],
                        hints: {
                            level1: "FFN: Linear(d_model, d_ff) -> ReLU -> Linear(d_ff, d_model).",
                            level2: "Positional: PE(pos, 2i) = sin(pos/10000^(2i/d_model)).",
                            level3: "class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
                        },
                        pitfalls: ["Positional encoding dimensions", "Forgetting dropout", "Embedding scale factor"],
                        concepts: ["Positional encoding", "Feed-forward networks", "Residual connections"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 4,
                        name: "Encoder & Decoder Layers",
                        description: "Combine components into encoder/decoder layers.",
                        criteria: ["Encoder: self-attention + FFN", "Decoder: masked self-attn + cross-attn + FFN", "Layer normalization", "Residual connections"],
                        hints: {
                            level1: "Each sublayer: LayerNorm(x + Sublayer(x)).",
                            level2: "Decoder has extra cross-attention to encoder output.",
                            level3: "class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Self attention with residual\n        attn_out = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        # FFN with residual\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_out))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)"
                        },
                        pitfalls: ["Pre-norm vs post-norm", "Causal mask in decoder", "Cross-attention key/value source"],
                        concepts: ["Encoder-decoder architecture", "Layer normalization", "Residual connections"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 5,
                        name: "Full Transformer & Training",
                        description: "Assemble full transformer and train on a task.",
                        criteria: ["Stack N encoder/decoder layers", "Output projection to vocabulary", "Label smoothing (optional)", "Train on translation or LM task"],
                        hints: {
                            level1: "Stack 6 encoder layers + 6 decoder layers (original paper).",
                            level2: "For language modeling, decoder-only with causal mask.",
                            level3: "class Transformer(nn.Module):\n    def __init__(self, src_vocab, tgt_vocab, d_model=512, num_heads=8, num_layers=6, d_ff=2048):\n        super().__init__()\n        self.encoder_embed = nn.Embedding(src_vocab, d_model)\n        self.decoder_embed = nn.Embedding(tgt_vocab, d_model)\n        self.pos_encoding = PositionalEncoding(d_model)\n        \n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, 0.1) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, 0.1) for _ in range(num_layers)])\n        \n        self.output_proj = nn.Linear(d_model, tgt_vocab)\n        self.scale = d_model ** 0.5\n    \n    def encode(self, src, src_mask):\n        x = self.pos_encoding(self.encoder_embed(src) * self.scale)\n        for layer in self.encoder_layers:\n            x = layer(x, src_mask)\n        return x\n    \n    def decode(self, tgt, enc_out, src_mask, tgt_mask):\n        x = self.pos_encoding(self.decoder_embed(tgt) * self.scale)\n        for layer in self.decoder_layers:\n            x = layer(x, enc_out, src_mask, tgt_mask)\n        return self.output_proj(x)"
                        },
                        pitfalls: ["Mask generation", "Teacher forcing", "Learning rate schedule"],
                        concepts: ["Full transformer", "Training loop", "Inference/generation"],
                        estimatedHours: "8-12"
                    }
                ]
            },

            // GAME DEV - INTERMEDIATE
            "topdown-shooter": {
                name: "Top-down Shooter",
                description: "Build a top-down shooter with enemies, projectiles, and waves. Learn game AI, collision systems, and game feel.",
                difficulty: "intermediate",
                estimatedHours: "20-30",
                prerequisites: ["Basic game loop", "2D graphics", "Vector math"],
                languages: { recommended: ["JavaScript", "Python", "C#"], also: ["C++", "Lua"] },
                resources: [
                    { name: "Game Programming Patterns", url: "https://gameprogrammingpatterns.com/", type: "book" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Player Movement & Aiming",
                        description: "Implement player with 8-directional movement and mouse aiming.",
                        criteria: ["WASD movement (8 directions)", "Mouse aiming (player faces cursor)", "Smooth movement with acceleration", "Screen bounds checking"],
                        hints: {
                            level1: "Normalize diagonal movement to avoid faster speed.",
                            level2: "Angle to mouse: atan2(mouse.y - player.y, mouse.x - player.x).",
                            level3: "class Player:\n    def update(self, dt):\n        # Input\n        dx = (keys['d'] - keys['a'])\n        dy = (keys['s'] - keys['w'])\n        \n        # Normalize diagonal\n        if dx != 0 and dy != 0:\n            dx *= 0.707  # 1/sqrt(2)\n            dy *= 0.707\n        \n        # Apply movement\n        self.vx = dx * SPEED\n        self.vy = dy * SPEED\n        self.x += self.vx * dt\n        self.y += self.vy * dt\n        \n        # Aim at mouse\n        self.angle = math.atan2(mouse_y - self.y, mouse_x - self.x)\n        \n        # Bounds\n        self.x = max(0, min(SCREEN_WIDTH, self.x))\n        self.y = max(0, min(SCREEN_HEIGHT, self.y))"
                        },
                        pitfalls: ["Diagonal speed boost", "Angle in wrong units", "Jittery movement"],
                        concepts: ["8-directional movement", "Mouse aiming", "Vector normalization"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Shooting & Projectiles",
                        description: "Implement projectile system with firing and collision.",
                        criteria: ["Click to fire projectile", "Projectile travels in aimed direction", "Fire rate limiting", "Projectile-enemy collision", "Remove off-screen projectiles"],
                        hints: {
                            level1: "Projectile velocity = (cos(angle), sin(angle)) * speed.",
                            level2: "Track last fire time for fire rate.",
                            level3: "class Projectile:\n    def __init__(self, x, y, angle, speed=500):\n        self.x = x\n        self.y = y\n        self.vx = math.cos(angle) * speed\n        self.vy = math.sin(angle) * speed\n        self.radius = 5\n        self.damage = 10\n    \n    def update(self, dt):\n        self.x += self.vx * dt\n        self.y += self.vy * dt\n    \n    def is_offscreen(self):\n        return self.x < 0 or self.x > SCREEN_WIDTH or self.y < 0 or self.y > SCREEN_HEIGHT\n\nclass Player:\n    def __init__(self):\n        self.fire_cooldown = 0\n        self.fire_rate = 0.1  # seconds between shots\n    \n    def shoot(self, projectiles):\n        if self.fire_cooldown <= 0:\n            proj = Projectile(self.x, self.y, self.angle)\n            projectiles.append(proj)\n            self.fire_cooldown = self.fire_rate\n    \n    def update(self, dt):\n        self.fire_cooldown -= dt"
                        },
                        pitfalls: ["Projectile spawn position", "Fire rate timing", "Memory from many projectiles"],
                        concepts: ["Projectile physics", "Fire rate", "Object pooling"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Enemies & AI",
                        description: "Add enemies with simple AI behaviors.",
                        criteria: ["Enemy chases player", "Different enemy types", "Health and damage system", "Death and spawning"],
                        hints: {
                            level1: "Chase: direction = normalize(player.pos - enemy.pos).",
                            level2: "Add variety: fast/weak, slow/strong, ranged.",
                            level3: "class Enemy:\n    def __init__(self, x, y, enemy_type='basic'):\n        self.x = x\n        self.y = y\n        if enemy_type == 'basic':\n            self.speed = 100\n            self.health = 30\n            self.damage = 10\n        elif enemy_type == 'fast':\n            self.speed = 200\n            self.health = 15\n            self.damage = 5\n        elif enemy_type == 'tank':\n            self.speed = 50\n            self.health = 100\n            self.damage = 25\n    \n    def update(self, dt, player):\n        # Chase player\n        dx = player.x - self.x\n        dy = player.y - self.y\n        dist = math.sqrt(dx*dx + dy*dy)\n        if dist > 0:\n            self.x += (dx/dist) * self.speed * dt\n            self.y += (dy/dist) * self.speed * dt\n    \n    def take_damage(self, amount):\n        self.health -= amount\n        return self.health <= 0  # Returns True if dead"
                        },
                        pitfalls: ["Division by zero in normalize", "Enemy stacking", "Instant kill on spawn"],
                        concepts: ["Enemy AI", "Health systems", "Enemy types"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 4,
                        name: "Waves & Scoring",
                        description: "Implement wave system and scoring.",
                        criteria: ["Wave-based spawning", "Difficulty increases each wave", "Score for kills", "High score tracking", "Wave clear bonus"],
                        hints: {
                            level1: "Wave spawns N enemies. N increases each wave.",
                            level2: "Track enemies alive, start next wave when zero.",
                            level3: "class WaveManager:\n    def __init__(self):\n        self.wave = 0\n        self.enemies_to_spawn = 0\n        self.spawn_timer = 0\n        self.spawn_interval = 1.0\n    \n    def start_wave(self, enemies):\n        self.wave += 1\n        self.enemies_to_spawn = 5 + self.wave * 2\n        self.spawn_timer = 0\n    \n    def update(self, dt, enemies):\n        if self.enemies_to_spawn > 0:\n            self.spawn_timer -= dt\n            if self.spawn_timer <= 0:\n                self.spawn_enemy(enemies)\n                self.enemies_to_spawn -= 1\n                self.spawn_timer = self.spawn_interval\n        elif len(enemies) == 0:\n            self.start_wave(enemies)\n    \n    def spawn_enemy(self, enemies):\n        # Spawn at random edge\n        side = random.randint(0, 3)\n        if side == 0: x, y = random.randint(0, WIDTH), 0\n        elif side == 1: x, y = WIDTH, random.randint(0, HEIGHT)\n        # ...\n        enemy_type = random.choice(['basic', 'basic', 'fast', 'tank'])\n        enemies.append(Enemy(x, y, enemy_type))"
                        },
                        pitfalls: ["Infinite spawn loop", "No break between waves", "Unfair difficulty spike"],
                        concepts: ["Wave systems", "Difficulty scaling", "Score systems"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // GAME DEV - ADVANCED
            "ecs-arch": {
                name: "ECS Architecture",
                description: "Implement an Entity-Component-System architecture. Learn data-oriented design for games.",
                difficulty: "advanced",
                estimatedHours: "20-35",
                prerequisites: ["Game programming basics", "Data structures", "Performance concepts"],
                languages: { recommended: ["C++", "Rust", "C"], also: ["C#", "Go"] },
                resources: [
                    { name: "ECS FAQ", url: "https://github.com/SanderMertens/ecs-faq", type: "article" },
                    { name: "Overwatch GDC Talk", url: "https://www.youtube.com/watch?v=W3aieHjyNvw", type: "video" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Entity Manager",
                        description: "Create entity ID management system.",
                        criteria: ["Generate unique entity IDs", "Track alive/dead entities", "Recycle deleted IDs", "Generation counter for ID reuse safety"],
                        hints: {
                            level1: "Entity = just an ID (integer). Components stored separately.",
                            level2: "Use generation to detect stale references.",
                            level3: "struct EntityId {\n    uint32_t index;\n    uint32_t generation;\n};\n\nclass EntityManager {\n    std::vector<uint32_t> generations;\n    std::queue<uint32_t> free_indices;\n    \npublic:\n    EntityId create() {\n        uint32_t index;\n        if (!free_indices.empty()) {\n            index = free_indices.front();\n            free_indices.pop();\n        } else {\n            index = generations.size();\n            generations.push_back(0);\n        }\n        return {index, generations[index]};\n    }\n    \n    void destroy(EntityId id) {\n        if (is_alive(id)) {\n            generations[id.index]++;\n            free_indices.push(id.index);\n        }\n    }\n    \n    bool is_alive(EntityId id) {\n        return id.index < generations.size() && \n               generations[id.index] == id.generation;\n    }\n};"
                        },
                        pitfalls: ["Stale entity references", "Index overflow", "Not recycling IDs"],
                        concepts: ["Entity IDs", "Generation counters", "ID recycling"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Component Storage",
                        description: "Implement cache-friendly component storage.",
                        criteria: ["Contiguous array storage", "Sparse set for entity->component mapping", "Add/remove components dynamically", "Type-safe component access"],
                        hints: {
                            level1: "Sparse set: sparse[entity] = dense_index, dense[index] = component.",
                            level2: "Dense array is contiguous for cache efficiency.",
                            level3: "template<typename T>\nclass ComponentArray {\n    std::vector<T> dense;           // Contiguous component data\n    std::vector<uint32_t> sparse;   // Entity ID -> dense index\n    std::vector<uint32_t> dense_to_entity;  // dense index -> entity ID\n    \npublic:\n    void add(uint32_t entity, T component) {\n        if (entity >= sparse.size())\n            sparse.resize(entity + 1, INVALID);\n        sparse[entity] = dense.size();\n        dense_to_entity.push_back(entity);\n        dense.push_back(component);\n    }\n    \n    void remove(uint32_t entity) {\n        uint32_t index = sparse[entity];\n        uint32_t last = dense.size() - 1;\n        \n        // Swap with last\n        std::swap(dense[index], dense[last]);\n        std::swap(dense_to_entity[index], dense_to_entity[last]);\n        \n        // Update sparse\n        sparse[dense_to_entity[index]] = index;\n        sparse[entity] = INVALID;\n        \n        dense.pop_back();\n        dense_to_entity.pop_back();\n    }\n    \n    T* get(uint32_t entity) {\n        if (entity >= sparse.size() || sparse[entity] == INVALID)\n            return nullptr;\n        return &dense[sparse[entity]];\n    }\n};"
                        },
                        pitfalls: ["Swap-remove ordering", "Sparse array growth", "Invalid index sentinel"],
                        concepts: ["Sparse sets", "Data-oriented design", "Cache efficiency"],
                        estimatedHours: "5-7"
                    },
                    {
                        id: 3,
                        name: "System Interface",
                        description: "Create system execution framework.",
                        criteria: ["Systems operate on component sets", "Query entities with specific components", "System registration and ordering", "Delta time passing"],
                        hints: {
                            level1: "System = function that iterates entities with certain components.",
                            level2: "Query: find all entities with Position AND Velocity.",
                            level3: "class World {\n    std::unordered_map<std::type_index, std::unique_ptr<ComponentArrayBase>> components;\n    \n    template<typename... Components>\n    class View {\n        World& world;\n    public:\n        class Iterator {\n            // Iterate entities that have all Components\n        };\n        \n        void each(std::function<void(EntityId, Components&...)> func) {\n            // For each entity with all components, call func\n            for (auto entity : get_matching_entities()) {\n                func(entity, *world.get<Components>(entity)...);\n            }\n        }\n    };\n    \n    template<typename... Components>\n    View<Components...> view() {\n        return View<Components...>(*this);\n    }\n};\n\n// Usage:\nvoid movement_system(World& world, float dt) {\n    world.view<Position, Velocity>().each([dt](EntityId e, Position& pos, Velocity& vel) {\n        pos.x += vel.x * dt;\n        pos.y += vel.y * dt;\n    });\n}"
                        },
                        pitfalls: ["Component iteration invalidation", "System ordering dependencies", "Thread safety"],
                        concepts: ["System execution", "Component queries", "Iteration patterns"],
                        estimatedHours: "5-7"
                    },
                    {
                        id: 4,
                        name: "Archetypes (Optional Advanced)",
                        description: "Implement archetype-based storage for better performance.",
                        criteria: ["Group entities by component combination", "Move entities between archetypes", "Archetype graph for fast lookup", "Chunk-based storage"],
                        hints: {
                            level1: "Archetype = unique combination of component types.",
                            level2: "Entities with same components stored together.",
                            level3: "struct Archetype {\n    std::set<std::type_index> component_types;\n    std::vector<void*> component_arrays;  // One array per type\n    std::vector<EntityId> entities;\n    \n    // All entities in same archetype have identical component layout\n};\n\nclass ArchetypeWorld {\n    std::unordered_map<ComponentMask, Archetype> archetypes;\n    std::unordered_map<EntityId, ArchetypeLocation> entity_locations;\n    \n    void add_component(EntityId entity, Component c) {\n        // 1. Find current archetype\n        // 2. Find or create target archetype (current + new component)\n        // 3. Move entity data to new archetype\n    }\n};"
                        },
                        pitfalls: ["Archetype explosion", "Move overhead", "Complex implementation"],
                        concepts: ["Archetypes", "Data locality", "Component masks"],
                        estimatedHours: "6-10"
                    }
                ]
            },

            // COMPILERS - INTERMEDIATE
            "lisp-interp": {
                name: "Lisp Interpreter",
                description: "Build an interpreter for a minimal Lisp. Learn S-expressions, environments, and functional programming concepts.",
                difficulty: "intermediate",
                estimatedHours: "15-25",
                prerequisites: ["Recursion", "Basic parsing", "Functional programming concepts"],
                languages: { recommended: ["Python", "JavaScript", "Ruby"], also: ["Go", "Rust", "C"] },
                resources: [
                    { name: "Make a Lisp", url: "https://github.com/kanaka/mal", type: "tutorial" },
                    { name: "SICP", url: "https://mitpress.mit.edu/sites/default/files/sicp/index.html", type: "book" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "S-Expression Parser",
                        description: "Parse Lisp S-expressions into data structures.",
                        criteria: ["Parse atoms (numbers, symbols)", "Parse lists (nested parentheses)", "Handle whitespace and comments", "Return native data structures"],
                        hints: {
                            level1: "Tokenize first: (, ), atoms. Then parse recursively.",
                            level2: "( starts list, ) ends it, atoms are leaf nodes.",
                            level3: "def tokenize(code):\n    tokens = []\n    i = 0\n    while i < len(code):\n        if code[i] in '()':\n            tokens.append(code[i])\n            i += 1\n        elif code[i].isspace():\n            i += 1\n        elif code[i] == ';':  # Comment\n            while i < len(code) and code[i] != '\\n':\n                i += 1\n        else:\n            j = i\n            while j < len(code) and code[j] not in '() \\t\\n;':\n                j += 1\n            tokens.append(code[i:j])\n            i = j\n    return tokens\n\ndef parse(tokens):\n    token = tokens.pop(0)\n    if token == '(':\n        lst = []\n        while tokens[0] != ')':\n            lst.append(parse(tokens))\n        tokens.pop(0)  # Remove ')'\n        return lst\n    elif token == ')':\n        raise SyntaxError('Unexpected )')\n    else:\n        return parse_atom(token)\n\ndef parse_atom(token):\n    try: return int(token)\n    except: pass\n    try: return float(token)\n    except: pass\n    return token  # Symbol"
                        },
                        pitfalls: ["Unbalanced parentheses", "Quote syntax", "Negative numbers"],
                        concepts: ["S-expressions", "Tokenization", "Recursive parsing"],
                        estimatedHours: "2-4"
                    },
                    {
                        id: 2,
                        name: "Basic Evaluation",
                        description: "Evaluate arithmetic and conditionals.",
                        criteria: ["Evaluate numbers to themselves", "Arithmetic: +, -, *, /", "Comparison: <, >, =, <=, >=", "Conditionals: if"],
                        hints: {
                            level1: "Numbers self-evaluate. Lists: first element is operator.",
                            level2: "Environment maps symbols to values.",
                            level3: "def eval_expr(expr, env):\n    if isinstance(expr, (int, float)):\n        return expr\n    if isinstance(expr, str):  # Symbol\n        if expr not in env:\n            raise NameError(f'Undefined: {expr}')\n        return env[expr]\n    if isinstance(expr, list):\n        if len(expr) == 0:\n            raise SyntaxError('Empty list')\n        \n        op = expr[0]\n        \n        # Special forms\n        if op == 'if':\n            _, test, then, else_ = expr\n            return eval_expr(then if eval_expr(test, env) else else_, env)\n        \n        # Function call\n        func = eval_expr(op, env)\n        args = [eval_expr(arg, env) for arg in expr[1:]]\n        return func(*args)\n\n# Built-in environment\nimport operator\nglobal_env = {\n    '+': operator.add,\n    '-': operator.sub,\n    '*': operator.mul,\n    '/': operator.truediv,\n    '<': operator.lt,\n    '>': operator.gt,\n    '=': operator.eq,\n}"
                        },
                        pitfalls: ["Special forms vs functions", "Short-circuit evaluation", "Environment lookup"],
                        concepts: ["Evaluation rules", "Environments", "Special forms"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Variables and Functions",
                        description: "Add define, lambda, and lexical scope.",
                        criteria: ["define binds variables", "lambda creates functions", "Lexical scoping (closures)", "let for local bindings"],
                        hints: {
                            level1: "Lambda captures its defining environment.",
                            level2: "Closure = (params, body, env).",
                            level3: "class Closure:\n    def __init__(self, params, body, env):\n        self.params = params\n        self.body = body\n        self.env = env\n    \n    def __call__(self, *args):\n        # Create new env with params bound to args\n        local_env = dict(self.env)\n        local_env.update(zip(self.params, args))\n        return eval_expr(self.body, local_env)\n\ndef eval_expr(expr, env):\n    # ... previous code ...\n    \n    if op == 'define':\n        _, name, value = expr\n        env[name] = eval_expr(value, env)\n        return None\n    \n    if op == 'lambda':\n        _, params, body = expr\n        return Closure(params, body, env.copy())\n    \n    if op == 'let':\n        # (let ((x 1) (y 2)) body)\n        _, bindings, body = expr\n        local_env = dict(env)\n        for name, value in bindings:\n            local_env[name] = eval_expr(value, env)\n        return eval_expr(body, local_env)"
                        },
                        pitfalls: ["Mutation vs shadowing", "Closure capturing", "Let vs let*"],
                        concepts: ["Closures", "Lexical scope", "Variable binding"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 4,
                        name: "List Operations & Recursion",
                        description: "Add list primitives and support recursion.",
                        criteria: ["cons, car, cdr operations", "list constructor", "null? predicate", "Recursive functions work"],
                        hints: {
                            level1: "cons builds pairs, car/cdr destructure them.",
                            level2: "For recursion, function must be in scope when called.",
                            level3: "# List operations\nglobal_env.update({\n    'cons': lambda a, b: [a] + (b if isinstance(b, list) else [b]),\n    'car': lambda lst: lst[0],\n    'cdr': lambda lst: lst[1:],\n    'list': lambda *args: list(args),\n    'null?': lambda lst: lst == [],\n    'length': len,\n})\n\n# Example: factorial\n# (define fact (lambda (n) (if (= n 0) 1 (* n (fact (- n 1))))))\n\n# Example: map\n# (define map (lambda (f lst)\n#   (if (null? lst)\n#       (list)\n#       (cons (f (car lst)) (map f (cdr lst))))))"
                        },
                        pitfalls: ["Proper vs improper lists", "Stack overflow on deep recursion", "Empty list handling"],
                        concepts: ["List processing", "Recursion", "Higher-order functions"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // CS FUNDAMENTALS - ADVANCED
            "graph-algos": {
                name: "Graph Algorithms",
                description: "Implement fundamental graph algorithms. Learn BFS, DFS, shortest paths, and spanning trees.",
                difficulty: "advanced",
                estimatedHours: "15-25",
                prerequisites: ["Basic data structures", "Recursion", "Algorithm complexity"],
                languages: { recommended: ["Python", "Java", "C++"], also: ["Go", "Rust"] },
                resources: [
                    { name: "CLRS - Introduction to Algorithms", url: "https://mitpress.mit.edu/books/introduction-algorithms-fourth-edition", type: "book" },
                    { name: "Visualgo Graphs", url: "https://visualgo.net/en/dfsbfs", type: "interactive" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Graph Representation",
                        description: "Implement adjacency list and matrix representations.",
                        criteria: ["Adjacency list for sparse graphs", "Adjacency matrix for dense graphs", "Add/remove vertices and edges", "Support weighted and directed graphs"],
                        hints: {
                            level1: "Adjacency list: dict of vertex -> list of neighbors.",
                            level2: "Adjacency matrix: 2D array, matrix[i][j] = weight or 0.",
                            level3: "class Graph:\n    def __init__(self, directed=False):\n        self.adj = {}  # vertex -> [(neighbor, weight), ...]\n        self.directed = directed\n    \n    def add_vertex(self, v):\n        if v not in self.adj:\n            self.adj[v] = []\n    \n    def add_edge(self, u, v, weight=1):\n        self.add_vertex(u)\n        self.add_vertex(v)\n        self.adj[u].append((v, weight))\n        if not self.directed:\n            self.adj[v].append((u, weight))\n    \n    def neighbors(self, v):\n        return self.adj.get(v, [])\n    \n    def vertices(self):\n        return self.adj.keys()"
                        },
                        pitfalls: ["Forgetting undirected edge goes both ways", "Self-loops", "Parallel edges"],
                        concepts: ["Graph representation", "Space complexity", "Edge weights"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "BFS and DFS",
                        description: "Implement breadth-first and depth-first search.",
                        criteria: ["BFS with queue", "DFS with recursion or stack", "Track visited vertices", "Return traversal order or path"],
                        hints: {
                            level1: "BFS uses queue (FIFO), DFS uses stack (LIFO).",
                            level2: "Mark visited BEFORE adding to queue/stack to avoid duplicates.",
                            level3: "from collections import deque\n\ndef bfs(graph, start):\n    visited = {start}\n    queue = deque([start])\n    order = []\n    \n    while queue:\n        v = queue.popleft()\n        order.append(v)\n        for neighbor, _ in graph.neighbors(v):\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append(neighbor)\n    \n    return order\n\ndef dfs(graph, start):\n    visited = set()\n    order = []\n    \n    def visit(v):\n        if v in visited:\n            return\n        visited.add(v)\n        order.append(v)\n        for neighbor, _ in graph.neighbors(v):\n            visit(neighbor)\n    \n    visit(start)\n    return order"
                        },
                        pitfalls: ["Infinite loop without visited check", "Stack overflow in DFS", "Disconnected components"],
                        concepts: ["Graph traversal", "BFS vs DFS", "Visited tracking"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Shortest Path (Dijkstra)",
                        description: "Implement Dijkstra's algorithm for shortest paths.",
                        criteria: ["Priority queue for efficiency", "Handle weighted edges", "Reconstruct shortest path", "Detect unreachable vertices"],
                        hints: {
                            level1: "Greedy: always expand closest unvisited vertex.",
                            level2: "Use heap: (distance, vertex). Update distances when shorter found.",
                            level3: "import heapq\n\ndef dijkstra(graph, start):\n    dist = {v: float('inf') for v in graph.vertices()}\n    dist[start] = 0\n    prev = {v: None for v in graph.vertices()}\n    pq = [(0, start)]\n    visited = set()\n    \n    while pq:\n        d, u = heapq.heappop(pq)\n        if u in visited:\n            continue\n        visited.add(u)\n        \n        for v, weight in graph.neighbors(u):\n            if v not in visited:\n                new_dist = dist[u] + weight\n                if new_dist < dist[v]:\n                    dist[v] = new_dist\n                    prev[v] = u\n                    heapq.heappush(pq, (new_dist, v))\n    \n    return dist, prev\n\ndef reconstruct_path(prev, start, end):\n    path = []\n    current = end\n    while current is not None:\n        path.append(current)\n        current = prev[current]\n    return path[::-1] if path[-1] == start else []"
                        },
                        pitfalls: ["Negative weights (use Bellman-Ford)", "Duplicate entries in heap", "Path reconstruction direction"],
                        concepts: ["Greedy algorithms", "Priority queues", "Shortest paths"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Minimum Spanning Tree",
                        description: "Implement Kruskal's or Prim's MST algorithm.",
                        criteria: ["Sort edges by weight (Kruskal)", "Union-Find for cycle detection", "Build MST edges", "Handle disconnected graphs"],
                        hints: {
                            level1: "Kruskal: sort edges, add if doesn't create cycle.",
                            level2: "Union-Find: disjoint set with union by rank, path compression.",
                            level3: "class UnionFind:\n    def __init__(self, n):\n        self.parent = list(range(n))\n        self.rank = [0] * n\n    \n    def find(self, x):\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])  # Path compression\n        return self.parent[x]\n    \n    def union(self, x, y):\n        px, py = self.find(x), self.find(y)\n        if px == py:\n            return False  # Already connected\n        if self.rank[px] < self.rank[py]:\n            px, py = py, px\n        self.parent[py] = px\n        if self.rank[px] == self.rank[py]:\n            self.rank[px] += 1\n        return True\n\ndef kruskal(graph):\n    # Get all edges\n    edges = []\n    for u in graph.vertices():\n        for v, w in graph.neighbors(u):\n            if u < v:  # Avoid duplicates in undirected\n                edges.append((w, u, v))\n    edges.sort()\n    \n    vertices = list(graph.vertices())\n    uf = UnionFind(len(vertices))\n    v_to_i = {v: i for i, v in enumerate(vertices)}\n    \n    mst = []\n    for w, u, v in edges:\n        if uf.union(v_to_i[u], v_to_i[v]):\n            mst.append((u, v, w))\n    \n    return mst"
                        },
                        pitfalls: ["Undirected edge duplication", "Union-Find without optimization", "Disconnected graph check"],
                        concepts: ["Minimum spanning tree", "Union-Find", "Greedy algorithms"],
                        estimatedHours: "4-5"
                    }
                ]
            },

            // SOFTWARE ENGINEERING - ADVANCED
            "integration-testing": {
                name: "Integration Testing Suite",
                description: "Build comprehensive integration tests. Learn to test components working together with real dependencies.",
                difficulty: "advanced",
                estimatedHours: "12-20",
                prerequisites: ["Unit testing", "Docker basics", "Database knowledge"],
                languages: { recommended: ["Python", "JavaScript", "Java"], also: ["Go", "Ruby"] },
                resources: [
                    { name: "Testcontainers", url: "https://www.testcontainers.org/", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Test Database Setup",
                        description: "Set up isolated database for testing.",
                        criteria: ["Spin up test database (Docker)", "Migrations run before tests", "Clean state between tests", "Tear down after test suite"],
                        hints: {
                            level1: "Use Docker Compose or Testcontainers.",
                            level2: "Transaction rollback for fast cleanup.",
                            level3: "import pytest\nimport docker\n\n@pytest.fixture(scope='session')\ndef postgres_container():\n    client = docker.from_env()\n    container = client.containers.run(\n        'postgres:15',\n        environment={'POSTGRES_PASSWORD': 'test'},\n        ports={'5432/tcp': None},\n        detach=True\n    )\n    # Wait for ready\n    import time\n    time.sleep(3)\n    port = container.ports['5432/tcp'][0]['HostPort']\n    yield f'postgresql://postgres:test@localhost:{port}/postgres'\n    container.stop()\n    container.remove()\n\n@pytest.fixture\ndef db_session(postgres_container):\n    engine = create_engine(postgres_container)\n    # Run migrations\n    Base.metadata.create_all(engine)\n    session = Session(engine)\n    yield session\n    session.rollback()  # Clean up\n    session.close()"
                        },
                        pitfalls: ["Port conflicts", "Container startup time", "Data leaking between tests"],
                        concepts: ["Test isolation", "Containers in testing", "Database fixtures"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "API Integration Tests",
                        description: "Test API endpoints with real HTTP requests.",
                        criteria: ["Start test server", "Make real HTTP requests", "Verify response codes and bodies", "Test authentication flow"],
                        hints: {
                            level1: "Use requests library or test client.",
                            level2: "Fixtures for authenticated users.",
                            level3: "import pytest\nfrom fastapi.testclient import TestClient\nfrom app import app, get_db\n\n@pytest.fixture\ndef client(db_session):\n    def override_get_db():\n        yield db_session\n    app.dependency_overrides[get_db] = override_get_db\n    yield TestClient(app)\n    app.dependency_overrides.clear()\n\n@pytest.fixture\ndef auth_client(client, db_session):\n    # Create test user\n    user = User(email='test@example.com')\n    user.set_password('password')\n    db_session.add(user)\n    db_session.commit()\n    \n    # Login and get token\n    response = client.post('/auth/login', json={\n        'email': 'test@example.com',\n        'password': 'password'\n    })\n    token = response.json()['access_token']\n    client.headers['Authorization'] = f'Bearer {token}'\n    return client\n\ndef test_create_post(auth_client):\n    response = auth_client.post('/posts', json={'title': 'Test', 'content': 'Content'})\n    assert response.status_code == 201\n    assert response.json()['title'] == 'Test'"
                        },
                        pitfalls: ["Test order dependencies", "Shared state", "Slow tests"],
                        concepts: ["API testing", "Test clients", "Authentication in tests"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 3,
                        name: "External Service Mocking",
                        description: "Mock external APIs while testing real internal integration.",
                        criteria: ["Mock HTTP responses for external APIs", "Verify external API was called correctly", "Simulate error responses", "Test retry logic"],
                        hints: {
                            level1: "Use responses (Python) or nock (Node.js) to mock HTTP.",
                            level2: "Record real responses for realistic mocks.",
                            level3: "import responses\n\n@responses.activate\ndef test_payment_integration(auth_client):\n    # Mock Stripe API\n    responses.add(\n        responses.POST,\n        'https://api.stripe.com/v1/charges',\n        json={'id': 'ch_123', 'status': 'succeeded'},\n        status=200\n    )\n    \n    # Make request that triggers Stripe call\n    response = auth_client.post('/orders/123/pay', json={\n        'card_token': 'tok_visa'\n    })\n    \n    assert response.status_code == 200\n    assert response.json()['payment_status'] == 'paid'\n    \n    # Verify Stripe was called correctly\n    assert len(responses.calls) == 1\n    assert 'amount' in responses.calls[0].request.body\n\n@responses.activate\ndef test_payment_failure_handling(auth_client):\n    # Mock Stripe failure\n    responses.add(\n        responses.POST,\n        'https://api.stripe.com/v1/charges',\n        json={'error': {'message': 'Card declined'}},\n        status=402\n    )\n    \n    response = auth_client.post('/orders/123/pay', json={'card_token': 'tok_bad'})\n    assert response.status_code == 400\n    assert 'declined' in response.json()['error']"
                        },
                        pitfalls: ["Missing mock causes real API call", "Mock doesn't match real API", "Order of mock setup"],
                        concepts: ["HTTP mocking", "Service virtualization", "Error simulation"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // AI/ML - ADVANCED
            "gan": {
                name: "Generative Adversarial Network",
                description: "Implement a GAN from scratch. Learn adversarial training, generator/discriminator architecture, and image generation.",
                difficulty: "advanced",
                estimatedHours: "25-40",
                prerequisites: ["Neural networks", "CNNs", "PyTorch/TensorFlow basics", "Linear algebra"],
                languages: { recommended: ["Python"], also: ["Julia"] },
                resources: [
                    { name: "Original GAN Paper", url: "https://arxiv.org/abs/1406.2661", type: "paper" },
                    { name: "GAN Hacks", url: "https://github.com/soumith/ganhacks", type: "article" },
                    { name: "PyTorch DCGAN Tutorial", url: "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Discriminator Network",
                        description: "Build the discriminator that classifies real vs fake images.",
                        criteria: ["CNN architecture for image classification", "Binary output (real/fake)", "Leaky ReLU activations", "Batch normalization"],
                        hints: {
                            level1: "Discriminator: Image -> probability it's real.",
                            level2: "Use strided convolutions instead of pooling. LeakyReLU prevents dead neurons.",
                            level3: "import torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels=1, features_d=64):\n        super().__init__()\n        self.disc = nn.Sequential(\n            # Input: N x img_channels x 64 x 64\n            nn.Conv2d(img_channels, features_d, 4, 2, 1),  # 32x32\n            nn.LeakyReLU(0.2),\n            self._block(features_d, features_d * 2, 4, 2, 1),  # 16x16\n            self._block(features_d * 2, features_d * 4, 4, 2, 1),  # 8x8\n            self._block(features_d * 4, features_d * 8, 4, 2, 1),  # 4x4\n            nn.Conv2d(features_d * 8, 1, 4, 2, 0),  # 1x1\n            nn.Sigmoid()\n        )\n    \n    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2)\n        )\n    \n    def forward(self, x):\n        return self.disc(x).view(-1)"
                        },
                        pitfalls: ["Using ReLU instead of LeakyReLU", "Missing batch norm", "Wrong output size"],
                        concepts: ["CNNs for classification", "Strided convolutions", "LeakyReLU"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 2,
                        name: "Generator Network",
                        description: "Build the generator that creates images from noise.",
                        criteria: ["Transposed convolutions for upsampling", "Takes random noise as input", "Output matches image dimensions", "Tanh activation for output"],
                        hints: {
                            level1: "Generator: Random noise -> fake image.",
                            level2: "Use transposed convolutions (deconvolutions) to upsample.",
                            level3: "class Generator(nn.Module):\n    def __init__(self, z_dim=100, img_channels=1, features_g=64):\n        super().__init__()\n        self.gen = nn.Sequential(\n            # Input: N x z_dim x 1 x 1\n            self._block(z_dim, features_g * 16, 4, 1, 0),  # 4x4\n            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # 8x8\n            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # 16x16\n            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # 32x32\n            nn.ConvTranspose2d(features_g * 2, img_channels, 4, 2, 1),  # 64x64\n            nn.Tanh()  # Output in [-1, 1]\n        )\n    \n    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n    \n    def forward(self, x):\n        return self.gen(x)"
                        },
                        pitfalls: ["Checkerboard artifacts", "Wrong noise dimension", "Not using Tanh"],
                        concepts: ["Transposed convolutions", "Upsampling", "Latent space"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 3,
                        name: "Adversarial Training Loop",
                        description: "Implement the minimax training procedure.",
                        criteria: ["Alternate D and G training", "Use appropriate loss functions", "Separate optimizers for D and G", "Track training progress"],
                        hints: {
                            level1: "D tries to maximize log(D(real)) + log(1-D(G(z))). G tries to minimize log(1-D(G(z))).",
                            level2: "In practice, train G to maximize log(D(G(z))) for better gradients.",
                            level3: "def train_step(real_images, D, G, opt_d, opt_g, criterion, z_dim, device):\n    batch_size = real_images.size(0)\n    real_images = real_images.to(device)\n    \n    # Train Discriminator: max log(D(real)) + log(1 - D(G(z)))\n    noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n    fake = G(noise)\n    \n    disc_real = D(real_images)\n    disc_fake = D(fake.detach())\n    \n    loss_d_real = criterion(disc_real, torch.ones_like(disc_real))\n    loss_d_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n    loss_d = (loss_d_real + loss_d_fake) / 2\n    \n    opt_d.zero_grad()\n    loss_d.backward()\n    opt_d.step()\n    \n    # Train Generator: max log(D(G(z))) <-> min -log(D(G(z)))\n    output = D(fake)\n    loss_g = criterion(output, torch.ones_like(output))\n    \n    opt_g.zero_grad()\n    loss_g.backward()\n    opt_g.step()\n    \n    return loss_d.item(), loss_g.item()"
                        },
                        pitfalls: ["Training D too much vs G", "Mode collapse", "Forgetting detach()"],
                        concepts: ["Adversarial training", "Minimax game", "Loss functions"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 4,
                        name: "Training Stability & Results",
                        description: "Stabilize training and generate quality images.",
                        criteria: ["Implement label smoothing", "Add noise to discriminator inputs", "Track FID or visual quality", "Save generated samples during training"],
                        hints: {
                            level1: "Training instability is common. Use tricks from GAN Hacks.",
                            level2: "Label smoothing: use 0.9 instead of 1.0 for real labels.",
                            level3: "# Training improvements\n\n# 1. Label smoothing\nreal_label = 0.9  # instead of 1.0\nfake_label = 0.0\n\n# 2. Add noise to discriminator inputs\ndef add_noise(images, std=0.1):\n    return images + torch.randn_like(images) * std\n\n# 3. Learning rates\nlr_d = 0.0002\nlr_g = 0.0002\nbetas = (0.5, 0.999)  # Adam betas\n\n# 4. Track progress\ndef save_samples(G, fixed_noise, epoch, path):\n    G.eval()\n    with torch.no_grad():\n        fake = G(fixed_noise)\n        # Denormalize from [-1,1] to [0,1]\n        fake = (fake + 1) / 2\n        save_image(fake, f'{path}/epoch_{epoch}.png', nrow=8)\n    G.train()\n\n# 5. Fixed noise for consistent comparison\nfixed_noise = torch.randn(64, z_dim, 1, 1, device=device)"
                        },
                        pitfalls: ["Mode collapse (G produces same image)", "D becomes too strong", "Vanishing gradients"],
                        concepts: ["Training stability", "Label smoothing", "Progress tracking"],
                        estimatedHours: "6-10"
                    }
                ]
            },

            // COMPILERS - BEGINNER/INTERMEDIATE
            "tokenizer": {
                name: "Tokenizer/Lexer",
                description: "Build a lexer that converts source code into tokens. Foundation for all compilers and interpreters.",
                difficulty: "beginner",
                estimatedHours: "8-15",
                prerequisites: ["Regular expressions basics", "String manipulation"],
                languages: { recommended: ["Python", "JavaScript", "Go"], also: ["Rust", "C"] },
                resources: [
                    { name: "Crafting Interpreters - Scanning", url: "https://craftinginterpreters.com/scanning.html", type: "book" },
                    { name: "Let's Build a Compiler", url: "https://compilers.iecc.com/crenshaw/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Token Types",
                        description: "Define token types and basic structure.",
                        criteria: ["Token class with type and value", "Support numbers and identifiers", "Support operators (+, -, *, /)", "Track position (line, column)"],
                        hints: {
                            level1: "Token = (type, value, position). Start simple.",
                            level2: "Use an enum or constants for token types.",
                            level3: "from enum import Enum, auto\nfrom dataclasses import dataclass\n\nclass TokenType(Enum):\n    # Literals\n    NUMBER = auto()\n    IDENTIFIER = auto()\n    STRING = auto()\n    \n    # Operators\n    PLUS = auto()\n    MINUS = auto()\n    STAR = auto()\n    SLASH = auto()\n    \n    # Delimiters\n    LPAREN = auto()\n    RPAREN = auto()\n    SEMICOLON = auto()\n    \n    # Keywords\n    IF = auto()\n    ELSE = auto()\n    WHILE = auto()\n    \n    # Special\n    EOF = auto()\n\n@dataclass\nclass Token:\n    type: TokenType\n    value: str\n    line: int\n    column: int\n    \n    def __repr__(self):\n        return f'Token({self.type.name}, {repr(self.value)}, {self.line}:{self.column})'"
                        },
                        pitfalls: ["Forgetting EOF token", "Not tracking position", "Inconsistent naming"],
                        concepts: ["Tokens", "Lexemes", "Token types"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 2,
                        name: "Scanning Logic",
                        description: "Implement the main scanning loop.",
                        criteria: ["Iterate through source code", "Match single-character tokens", "Handle whitespace and newlines", "Report lexical errors with position"],
                        hints: {
                            level1: "Maintain current position, peek ahead when needed.",
                            level2: "advance() returns current char and moves forward. peek() looks ahead without moving.",
                            level3: "class Lexer:\n    def __init__(self, source):\n        self.source = source\n        self.pos = 0\n        self.line = 1\n        self.column = 1\n        self.tokens = []\n    \n    def is_at_end(self):\n        return self.pos >= len(self.source)\n    \n    def advance(self):\n        char = self.source[self.pos]\n        self.pos += 1\n        if char == '\\n':\n            self.line += 1\n            self.column = 1\n        else:\n            self.column += 1\n        return char\n    \n    def peek(self):\n        if self.is_at_end():\n            return '\\0'\n        return self.source[self.pos]\n    \n    def add_token(self, type, value=''):\n        self.tokens.append(Token(type, value, self.line, self.column))\n    \n    def scan_tokens(self):\n        while not self.is_at_end():\n            self.scan_token()\n        self.add_token(TokenType.EOF)\n        return self.tokens\n    \n    def scan_token(self):\n        char = self.advance()\n        \n        if char in ' \\t\\r\\n':\n            return  # Skip whitespace\n        elif char == '+':\n            self.add_token(TokenType.PLUS, char)\n        elif char == '-':\n            self.add_token(TokenType.MINUS, char)\n        # ... more cases\n        else:\n            raise LexerError(f'Unexpected character: {char}', self.line, self.column)"
                        },
                        pitfalls: ["Off-by-one errors", "Not handling newlines", "Consuming too much"],
                        concepts: ["Scanning", "Character stream", "Position tracking"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Multi-character Tokens",
                        description: "Handle numbers, identifiers, strings, and multi-char operators.",
                        criteria: ["Scan integers and floats", "Scan identifiers (alphanumeric + underscore)", "Distinguish keywords from identifiers", "Handle ==, !=, <=, >= operators"],
                        hints: {
                            level1: "When you see a digit, keep consuming digits. Same for identifiers.",
                            level2: "Use a keyword lookup table to convert identifiers to keywords.",
                            level3: "KEYWORDS = {\n    'if': TokenType.IF,\n    'else': TokenType.ELSE,\n    'while': TokenType.WHILE,\n    'for': TokenType.FOR,\n    'return': TokenType.RETURN,\n    'true': TokenType.TRUE,\n    'false': TokenType.FALSE,\n}\n\ndef scan_number(self):\n    start = self.pos - 1\n    while self.peek().isdigit():\n        self.advance()\n    \n    # Look for decimal part\n    if self.peek() == '.' and self.peek_next().isdigit():\n        self.advance()  # consume '.'\n        while self.peek().isdigit():\n            self.advance()\n    \n    value = self.source[start:self.pos]\n    self.add_token(TokenType.NUMBER, value)\n\ndef scan_identifier(self):\n    start = self.pos - 1\n    while self.peek().isalnum() or self.peek() == '_':\n        self.advance()\n    \n    text = self.source[start:self.pos]\n    token_type = KEYWORDS.get(text, TokenType.IDENTIFIER)\n    self.add_token(token_type, text)\n\ndef match(self, expected):\n    '''Consume next char if it matches expected'''\n    if self.is_at_end() or self.source[self.pos] != expected:\n        return False\n    self.pos += 1\n    self.column += 1\n    return True\n\n# In scan_token:\nif char == '=':\n    if self.match('='):\n        self.add_token(TokenType.EQUAL_EQUAL, '==')\n    else:\n        self.add_token(TokenType.EQUAL, '=')"
                        },
                        pitfalls: ["Not handling '.' in floats correctly", "Keywords vs identifiers", "Multi-char operators"],
                        concepts: ["Maximal munch", "Keyword tables", "Lookahead"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Strings and Comments",
                        description: "Handle string literals and comments.",
                        criteria: ["String literals with escape sequences", "Single-line comments (//)", "Multi-line comments (/* */)", "Handle unterminated strings/comments"],
                        hints: {
                            level1: "Strings: consume until closing quote. Handle \\n, \\t, \\\\.",
                            level2: "For multi-line comments, track nesting or just find */.",
                            level3: "def scan_string(self):\n    start_line = self.line\n    value = ''\n    \n    while self.peek() != '\"' and not self.is_at_end():\n        if self.peek() == '\\\\':\n            self.advance()  # consume backslash\n            escape = self.advance()\n            if escape == 'n':\n                value += '\\n'\n            elif escape == 't':\n                value += '\\t'\n            elif escape == '\\\\':\n                value += '\\\\'\n            elif escape == '\"':\n                value += '\"'\n            else:\n                raise LexerError(f'Invalid escape: \\\\{escape}', self.line, self.column)\n        else:\n            value += self.advance()\n    \n    if self.is_at_end():\n        raise LexerError('Unterminated string', start_line, self.column)\n    \n    self.advance()  # closing \"\n    self.add_token(TokenType.STRING, value)\n\ndef skip_comment(self):\n    if self.peek() == '/':\n        # Single line comment\n        while self.peek() != '\\n' and not self.is_at_end():\n            self.advance()\n    elif self.peek() == '*':\n        # Multi-line comment\n        self.advance()  # consume *\n        while True:\n            if self.is_at_end():\n                raise LexerError('Unterminated comment', self.line, self.column)\n            if self.peek() == '*' and self.peek_next() == '/':\n                self.advance()  # *\n                self.advance()  # /\n                break\n            self.advance()"
                        },
                        pitfalls: ["Unterminated strings across lines", "Nested comments", "Escape sequences"],
                        concepts: ["String literals", "Comments", "Escape sequences"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "bytecode-vm": {
                name: "Bytecode Virtual Machine",
                description: "Build a stack-based virtual machine that executes bytecode. Learn about instruction sets and execution models.",
                difficulty: "intermediate",
                estimatedHours: "15-25",
                prerequisites: ["Basic assembly concepts", "Stack data structure", "Binary representation"],
                languages: { recommended: ["C", "Rust", "Go"], also: ["Python", "Java"] },
                resources: [
                    { name: "Crafting Interpreters - Bytecode VM", url: "https://craftinginterpreters.com/a-bytecode-virtual-machine.html", type: "book" },
                    { name: "Writing a Simple VM", url: "https://felix.engineer/blogs/virtual-machine-in-c", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Instruction Set Design",
                        description: "Define opcodes and bytecode format.",
                        criteria: ["Define opcode enum", "Design instruction encoding", "Support operand encoding", "Document instruction format"],
                        hints: {
                            level1: "Keep it simple: 1 byte opcode, optional operands follow.",
                            level2: "Common opcodes: PUSH, POP, ADD, SUB, JUMP, CALL, RETURN.",
                            level3: "from enum import IntEnum\n\nclass OpCode(IntEnum):\n    # Stack operations\n    CONST = 0x01      # Push constant: CONST <index>\n    POP = 0x02        # Pop top of stack\n    DUP = 0x03        # Duplicate top\n    \n    # Arithmetic\n    ADD = 0x10\n    SUB = 0x11\n    MUL = 0x12\n    DIV = 0x13\n    NEG = 0x14\n    \n    # Comparison\n    EQ = 0x20\n    LT = 0x21\n    GT = 0x22\n    \n    # Control flow\n    JUMP = 0x30       # JUMP <offset>\n    JUMP_IF_FALSE = 0x31\n    \n    # Variables\n    LOAD_LOCAL = 0x40  # LOAD_LOCAL <slot>\n    STORE_LOCAL = 0x41\n    LOAD_GLOBAL = 0x42\n    STORE_GLOBAL = 0x43\n    \n    # Functions\n    CALL = 0x50       # CALL <arg_count>\n    RETURN = 0x51\n    \n    # Special\n    HALT = 0xFF\n\nclass Chunk:\n    '''Bytecode container'''\n    def __init__(self):\n        self.code = bytearray()\n        self.constants = []\n        self.lines = []  # For error reporting\n    \n    def write(self, byte, line):\n        self.code.append(byte)\n        self.lines.append(line)\n    \n    def add_constant(self, value):\n        self.constants.append(value)\n        return len(self.constants) - 1"
                        },
                        pitfalls: ["Too complex instruction set", "Forgetting HALT", "Operand encoding issues"],
                        concepts: ["Opcodes", "Instruction encoding", "Bytecode format"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Stack-Based Execution",
                        description: "Implement the execution loop with value stack.",
                        criteria: ["Value stack for operands", "Instruction pointer", "Fetch-decode-execute cycle", "Arithmetic operations"],
                        hints: {
                            level1: "Stack machine: operands pushed, operators pop and push result.",
                            level2: "IP points to next instruction. advance() returns byte and increments IP.",
                            level3: "class VM:\n    STACK_MAX = 256\n    \n    def __init__(self):\n        self.stack = []\n        self.ip = 0\n        self.chunk = None\n    \n    def push(self, value):\n        if len(self.stack) >= self.STACK_MAX:\n            raise RuntimeError('Stack overflow')\n        self.stack.append(value)\n    \n    def pop(self):\n        if not self.stack:\n            raise RuntimeError('Stack underflow')\n        return self.stack.pop()\n    \n    def read_byte(self):\n        byte = self.chunk.code[self.ip]\n        self.ip += 1\n        return byte\n    \n    def run(self, chunk):\n        self.chunk = chunk\n        self.ip = 0\n        self.stack = []\n        \n        while True:\n            op = self.read_byte()\n            \n            if op == OpCode.CONST:\n                index = self.read_byte()\n                self.push(self.chunk.constants[index])\n            \n            elif op == OpCode.ADD:\n                b = self.pop()\n                a = self.pop()\n                self.push(a + b)\n            \n            elif op == OpCode.SUB:\n                b = self.pop()\n                a = self.pop()\n                self.push(a - b)\n            \n            elif op == OpCode.MUL:\n                b = self.pop()\n                a = self.pop()\n                self.push(a * b)\n            \n            elif op == OpCode.DIV:\n                b = self.pop()\n                a = self.pop()\n                self.push(a / b)\n            \n            elif op == OpCode.NEG:\n                self.push(-self.pop())\n            \n            elif op == OpCode.HALT:\n                return self.pop() if self.stack else None"
                        },
                        pitfalls: ["Stack over/underflow", "Off-by-one in IP", "Order of operands for SUB/DIV"],
                        concepts: ["Stack machines", "Instruction pointer", "Fetch-decode-execute"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 3,
                        name: "Control Flow",
                        description: "Add jumps and conditionals.",
                        criteria: ["Unconditional JUMP", "Conditional JUMP_IF_FALSE", "Comparison operators", "Handle forward and backward jumps"],
                        hints: {
                            level1: "JUMP modifies IP directly. Jump offset can be relative or absolute.",
                            level2: "JUMP_IF_FALSE: pop condition, jump if falsy, else continue.",
                            level3: "# In VM.run():\n\nelif op == OpCode.EQ:\n    b = self.pop()\n    a = self.pop()\n    self.push(a == b)\n\nelif op == OpCode.LT:\n    b = self.pop()\n    a = self.pop()\n    self.push(a < b)\n\nelif op == OpCode.GT:\n    b = self.pop()\n    a = self.pop()\n    self.push(a > b)\n\nelif op == OpCode.JUMP:\n    offset = self.read_short()  # 2-byte offset\n    self.ip = offset\n\nelif op == OpCode.JUMP_IF_FALSE:\n    offset = self.read_short()\n    if not self.peek_stack():  # Check without popping\n        self.ip = offset\n    self.pop()  # Pop condition after check\n\ndef read_short(self):\n    '''Read 2-byte offset'''\n    high = self.read_byte()\n    low = self.read_byte()\n    return (high << 8) | low\n\ndef peek_stack(self):\n    return self.stack[-1] if self.stack else None\n\n# Example: if-else compilation\n# if (x < 10) { a } else { b }\n# Compiles to:\n#   LOAD x\n#   CONST 10\n#   LT\n#   JUMP_IF_FALSE else_label\n#   <a code>\n#   JUMP end_label\n# else_label:\n#   <b code>\n# end_label:"
                        },
                        pitfalls: ["Jump offset calculation", "Forgetting to pop condition", "Infinite loops"],
                        concepts: ["Control flow", "Jumps", "Conditional execution"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Variables and Functions",
                        description: "Add local variables and function calls.",
                        criteria: ["Local variable slots on stack", "CALL pushes return address", "RETURN pops frame", "Pass arguments via stack"],
                        hints: {
                            level1: "Locals: use stack slots relative to frame pointer.",
                            level2: "Call frame: save IP, base pointer. Return: restore them.",
                            level3: "class CallFrame:\n    def __init__(self, function, ip, base):\n        self.function = function  # Function being called\n        self.ip = ip              # Return address\n        self.base = base          # Stack base for locals\n\nclass VM:\n    def __init__(self):\n        self.stack = []\n        self.frames = []  # Call stack\n    \n    @property\n    def frame(self):\n        return self.frames[-1]\n    \n    def run(self, main_function):\n        self.frames = [CallFrame(main_function, 0, 0)]\n        \n        while self.frames:\n            op = self.read_byte()\n            \n            if op == OpCode.LOAD_LOCAL:\n                slot = self.read_byte()\n                self.push(self.stack[self.frame.base + slot])\n            \n            elif op == OpCode.STORE_LOCAL:\n                slot = self.read_byte()\n                self.stack[self.frame.base + slot] = self.peek_stack()\n            \n            elif op == OpCode.CALL:\n                arg_count = self.read_byte()\n                function = self.stack[-arg_count - 1]  # Function is below args\n                frame = CallFrame(\n                    function,\n                    self.frame.ip,\n                    len(self.stack) - arg_count - 1\n                )\n                self.frames.append(frame)\n                self.frame.ip = 0\n            \n            elif op == OpCode.RETURN:\n                result = self.pop()\n                # Discard frame\n                old_frame = self.frames.pop()\n                # Pop locals and function\n                while len(self.stack) > old_frame.base:\n                    self.pop()\n                self.push(result)\n                \n                if not self.frames:\n                    return result"
                        },
                        pitfalls: ["Frame pointer calculation", "Argument passing order", "Return value handling"],
                        concepts: ["Call frames", "Local variables", "Function calls"],
                        estimatedHours: "5-7"
                    }
                ]
            },

            "ast-builder": {
                name: "AST Builder (Parser)",
                description: "Build a parser that converts tokens into an Abstract Syntax Tree. Learn grammar rules and tree structures.",
                difficulty: "intermediate",
                estimatedHours: "12-20",
                prerequisites: ["Tokenizer/Lexer", "Recursion", "Tree data structures"],
                languages: { recommended: ["Python", "JavaScript", "Java"], also: ["Go", "Rust", "C"] },
                resources: [
                    { name: "Crafting Interpreters - Parsing", url: "https://craftinginterpreters.com/parsing-expressions.html", type: "book" },
                    { name: "Pratt Parsing", url: "https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "AST Node Definitions",
                        description: "Define AST node types for your language.",
                        criteria: ["Expression nodes (literals, binary, unary)", "Statement nodes (if, while, return)", "Use visitor pattern or tagged unions", "Include source location in nodes"],
                        hints: {
                            level1: "AST = tree of nodes, each representing a language construct.",
                            level2: "Use inheritance or tagged unions. Visitor pattern for traversal.",
                            level3: "from dataclasses import dataclass\nfrom typing import List, Optional, Any\n\n@dataclass\nclass Expr:\n    '''Base expression class'''\n    pass\n\n@dataclass\nclass Literal(Expr):\n    value: Any\n\n@dataclass\nclass Identifier(Expr):\n    name: str\n\n@dataclass\nclass Binary(Expr):\n    left: Expr\n    operator: str\n    right: Expr\n\n@dataclass\nclass Unary(Expr):\n    operator: str\n    operand: Expr\n\n@dataclass\nclass Call(Expr):\n    callee: Expr\n    arguments: List[Expr]\n\n@dataclass\nclass Stmt:\n    '''Base statement class'''\n    pass\n\n@dataclass\nclass ExprStmt(Stmt):\n    expression: Expr\n\n@dataclass\nclass VarDecl(Stmt):\n    name: str\n    initializer: Optional[Expr]\n\n@dataclass\nclass If(Stmt):\n    condition: Expr\n    then_branch: Stmt\n    else_branch: Optional[Stmt]\n\n@dataclass\nclass While(Stmt):\n    condition: Expr\n    body: Stmt\n\n@dataclass\nclass Block(Stmt):\n    statements: List[Stmt]\n\n@dataclass\nclass Function(Stmt):\n    name: str\n    params: List[str]\n    body: List[Stmt]"
                        },
                        pitfalls: ["Missing node types", "No location info", "Mutable vs immutable nodes"],
                        concepts: ["AST nodes", "Expression vs statement", "Tree representation"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Recursive Descent Parser",
                        description: "Implement recursive descent parsing for expressions.",
                        criteria: ["One function per grammar rule", "Handle operator precedence", "Parse function calls", "Consume tokens correctly"],
                        hints: {
                            level1: "Each grammar rule = one parsing function. Lower precedence = called first.",
                            level2: "expression -> equality -> comparison -> term -> factor -> unary -> primary",
                            level3: "class Parser:\n    def __init__(self, tokens):\n        self.tokens = tokens\n        self.current = 0\n    \n    def peek(self):\n        return self.tokens[self.current]\n    \n    def is_at_end(self):\n        return self.peek().type == TokenType.EOF\n    \n    def advance(self):\n        if not self.is_at_end():\n            self.current += 1\n        return self.tokens[self.current - 1]\n    \n    def check(self, type):\n        if self.is_at_end():\n            return False\n        return self.peek().type == type\n    \n    def match(self, *types):\n        for type in types:\n            if self.check(type):\n                self.advance()\n                return True\n        return False\n    \n    def consume(self, type, message):\n        if self.check(type):\n            return self.advance()\n        raise ParseError(message, self.peek())\n    \n    # Grammar: expression -> equality\n    def expression(self):\n        return self.equality()\n    \n    # equality -> comparison (('==' | '!=') comparison)*\n    def equality(self):\n        expr = self.comparison()\n        while self.match(TokenType.EQUAL_EQUAL, TokenType.BANG_EQUAL):\n            op = self.tokens[self.current - 1].value\n            right = self.comparison()\n            expr = Binary(expr, op, right)\n        return expr\n    \n    # comparison -> term (('<' | '>' | '<=' | '>=') term)*\n    def comparison(self):\n        expr = self.term()\n        while self.match(TokenType.LESS, TokenType.GREATER, TokenType.LESS_EQUAL, TokenType.GREATER_EQUAL):\n            op = self.tokens[self.current - 1].value\n            right = self.term()\n            expr = Binary(expr, op, right)\n        return expr\n    \n    # term -> factor (('+' | '-') factor)*\n    def term(self):\n        expr = self.factor()\n        while self.match(TokenType.PLUS, TokenType.MINUS):\n            op = self.tokens[self.current - 1].value\n            right = self.factor()\n            expr = Binary(expr, op, right)\n        return expr"
                        },
                        pitfalls: ["Wrong precedence order", "Left recursion", "Infinite loops"],
                        concepts: ["Recursive descent", "Operator precedence", "Grammar rules"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 3,
                        name: "Statement Parsing",
                        description: "Parse statements and declarations.",
                        criteria: ["Variable declarations", "If/else statements", "While loops", "Block statements with scoping"],
                        hints: {
                            level1: "statement = exprStmt | ifStmt | whileStmt | block | declaration",
                            level2: "Block: '{' statement* '}'. Declaration: 'var' NAME ('=' expr)? ';'",
                            level3: "def statement(self):\n    if self.match(TokenType.IF):\n        return self.if_statement()\n    if self.match(TokenType.WHILE):\n        return self.while_statement()\n    if self.match(TokenType.LBRACE):\n        return Block(self.block())\n    if self.match(TokenType.RETURN):\n        return self.return_statement()\n    return self.expression_statement()\n\ndef if_statement(self):\n    self.consume(TokenType.LPAREN, \"Expect '(' after 'if'.\")\n    condition = self.expression()\n    self.consume(TokenType.RPAREN, \"Expect ')' after condition.\")\n    \n    then_branch = self.statement()\n    else_branch = None\n    if self.match(TokenType.ELSE):\n        else_branch = self.statement()\n    \n    return If(condition, then_branch, else_branch)\n\ndef while_statement(self):\n    self.consume(TokenType.LPAREN, \"Expect '(' after 'while'.\")\n    condition = self.expression()\n    self.consume(TokenType.RPAREN, \"Expect ')' after condition.\")\n    body = self.statement()\n    return While(condition, body)\n\ndef block(self):\n    statements = []\n    while not self.check(TokenType.RBRACE) and not self.is_at_end():\n        statements.append(self.declaration())\n    self.consume(TokenType.RBRACE, \"Expect '}' after block.\")\n    return statements\n\ndef declaration(self):\n    if self.match(TokenType.VAR):\n        return self.var_declaration()\n    if self.match(TokenType.FUN):\n        return self.function('function')\n    return self.statement()\n\ndef var_declaration(self):\n    name = self.consume(TokenType.IDENTIFIER, 'Expect variable name.').value\n    initializer = None\n    if self.match(TokenType.EQUAL):\n        initializer = self.expression()\n    self.consume(TokenType.SEMICOLON, \"Expect ';' after variable declaration.\")\n    return VarDecl(name, initializer)"
                        },
                        pitfalls: ["Dangling else ambiguity", "Missing semicolons", "Block scope boundaries"],
                        concepts: ["Statement parsing", "Control flow", "Declarations"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 4,
                        name: "Error Recovery",
                        description: "Implement error handling and recovery.",
                        criteria: ["Report multiple errors (don't stop at first)", "Synchronize after errors", "Meaningful error messages", "Track error locations"],
                        hints: {
                            level1: "Panic mode: on error, skip tokens until synchronization point.",
                            level2: "Sync points: statement boundaries (semicolons, keywords like if/while/class).",
                            level3: "class ParseError(Exception):\n    def __init__(self, message, token):\n        self.message = message\n        self.token = token\n        super().__init__(f'[line {token.line}] Error at {repr(token.value)}: {message}')\n\nclass Parser:\n    def __init__(self, tokens):\n        self.tokens = tokens\n        self.current = 0\n        self.errors = []\n        self.had_error = False\n    \n    def error(self, message, token=None):\n        token = token or self.peek()\n        error = ParseError(message, token)\n        self.errors.append(error)\n        self.had_error = True\n        return error\n    \n    def synchronize(self):\n        '''Skip tokens until we reach a statement boundary'''\n        self.advance()\n        \n        while not self.is_at_end():\n            # After semicolon, we're at statement boundary\n            if self.tokens[self.current - 1].type == TokenType.SEMICOLON:\n                return\n            \n            # These keywords start statements\n            if self.peek().type in (\n                TokenType.CLASS, TokenType.FUN, TokenType.VAR,\n                TokenType.FOR, TokenType.IF, TokenType.WHILE,\n                TokenType.RETURN\n            ):\n                return\n            \n            self.advance()\n    \n    def declaration(self):\n        try:\n            if self.match(TokenType.VAR):\n                return self.var_declaration()\n            return self.statement()\n        except ParseError:\n            self.synchronize()\n            return None"
                        },
                        pitfalls: ["Stopping at first error", "Bad sync points", "Cascading errors"],
                        concepts: ["Error recovery", "Panic mode", "Synchronization"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "type-checker": {
                name: "Type Checker",
                description: "Build a type checker for a statically typed language. Learn type inference, type rules, and semantic analysis.",
                difficulty: "advanced",
                estimatedHours: "20-35",
                prerequisites: ["AST Builder", "Type systems basics", "Recursive algorithms"],
                languages: { recommended: ["OCaml", "Haskell", "Rust"], also: ["Python", "TypeScript"] },
                resources: [
                    { name: "Types and Programming Languages", url: "https://www.cis.upenn.edu/~bcpierce/tapl/", type: "book" },
                    { name: "Write You a Haskell", url: "http://dev.stephendiehl.com/fun/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Type Representation",
                        description: "Define type representations and type environment.",
                        criteria: ["Primitive types (int, bool, string)", "Function types", "Type variables for generics", "Type environment mapping"],
                        hints: {
                            level1: "Types are data structures: Int, Bool, Function(params, return), TypeVar(name).",
                            level2: "Type environment: name -> type mapping, with scoping.",
                            level3: "from dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n@dataclass\nclass Type:\n    pass\n\n@dataclass\nclass TInt(Type):\n    def __repr__(self): return 'int'\n\n@dataclass\nclass TBool(Type):\n    def __repr__(self): return 'bool'\n\n@dataclass\nclass TString(Type):\n    def __repr__(self): return 'string'\n\n@dataclass\nclass TFunction(Type):\n    params: List[Type]\n    ret: Type\n    def __repr__(self): return f'({\", \".join(map(str, self.params))}) -> {self.ret}'\n\n@dataclass\nclass TVar(Type):\n    '''Type variable for polymorphism'''\n    name: str\n    def __repr__(self): return self.name\n\nclass TypeEnv:\n    def __init__(self, parent=None):\n        self.bindings: Dict[str, Type] = {}\n        self.parent = parent\n    \n    def define(self, name: str, type: Type):\n        self.bindings[name] = type\n    \n    def lookup(self, name: str) -> Optional[Type]:\n        if name in self.bindings:\n            return self.bindings[name]\n        if self.parent:\n            return self.parent.lookup(name)\n        return None\n    \n    def child(self):\n        return TypeEnv(parent=self)"
                        },
                        pitfalls: ["Forgetting unit/void type", "Mutable type variables", "Scope handling"],
                        concepts: ["Type representations", "Type environments", "Scoping"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Basic Type Checking",
                        description: "Check types for expressions and statements.",
                        criteria: ["Literals have known types", "Binary operators check operand types", "Function calls check argument types", "Assignments check type compatibility"],
                        hints: {
                            level1: "check(node, env) -> Type. Recursively check children.",
                            level2: "Binary +: both operands int -> result int. ==: same types -> bool.",
                            level3: "class TypeChecker:\n    def __init__(self):\n        self.errors = []\n    \n    def error(self, message, node):\n        self.errors.append(TypeError(message, node))\n    \n    def check_expr(self, expr, env) -> Type:\n        if isinstance(expr, Literal):\n            if isinstance(expr.value, int):\n                return TInt()\n            elif isinstance(expr.value, bool):\n                return TBool()\n            elif isinstance(expr.value, str):\n                return TString()\n        \n        elif isinstance(expr, Identifier):\n            type = env.lookup(expr.name)\n            if type is None:\n                self.error(f'Undefined variable: {expr.name}', expr)\n                return TInt()  # Return something to continue\n            return type\n        \n        elif isinstance(expr, Binary):\n            left_type = self.check_expr(expr.left, env)\n            right_type = self.check_expr(expr.right, env)\n            \n            if expr.operator in ['+', '-', '*', '/']:\n                if not isinstance(left_type, TInt) or not isinstance(right_type, TInt):\n                    self.error(f'Operator {expr.operator} requires int operands', expr)\n                return TInt()\n            \n            elif expr.operator in ['<', '>', '<=', '>=']:\n                if not isinstance(left_type, TInt) or not isinstance(right_type, TInt):\n                    self.error(f'Comparison requires int operands', expr)\n                return TBool()\n            \n            elif expr.operator in ['==', '!=']:\n                if type(left_type) != type(right_type):\n                    self.error(f'Cannot compare {left_type} and {right_type}', expr)\n                return TBool()\n        \n        elif isinstance(expr, Call):\n            callee_type = self.check_expr(expr.callee, env)\n            if not isinstance(callee_type, TFunction):\n                self.error('Cannot call non-function', expr)\n                return TInt()\n            \n            if len(expr.arguments) != len(callee_type.params):\n                self.error(f'Expected {len(callee_type.params)} arguments, got {len(expr.arguments)}', expr)\n            \n            for arg, param_type in zip(expr.arguments, callee_type.params):\n                arg_type = self.check_expr(arg, env)\n                if not self.types_match(arg_type, param_type):\n                    self.error(f'Expected {param_type}, got {arg_type}', arg)\n            \n            return callee_type.ret"
                        },
                        pitfalls: ["Operator type rules", "Function arity", "Return type tracking"],
                        concepts: ["Type checking", "Type rules", "Error reporting"],
                        estimatedHours: "5-7"
                    },
                    {
                        id: 3,
                        name: "Type Inference",
                        description: "Infer types where not explicitly annotated.",
                        criteria: ["Infer variable types from initializers", "Infer function return types", "Unification algorithm", "Handle type constraints"],
                        hints: {
                            level1: "Inference: generate constraints, then solve (unify).",
                            level2: "Unification: TVar = concrete type, or TVar = TVar.",
                            level3: "class TypeInferrer:\n    def __init__(self):\n        self.type_var_counter = 0\n        self.substitution = {}  # TVar name -> Type\n    \n    def fresh_type_var(self):\n        self.type_var_counter += 1\n        return TVar(f't{self.type_var_counter}')\n    \n    def unify(self, t1: Type, t2: Type):\n        '''Make t1 and t2 the same type'''\n        t1 = self.apply_substitution(t1)\n        t2 = self.apply_substitution(t2)\n        \n        if isinstance(t1, TVar):\n            if t1.name != getattr(t2, 'name', None):\n                # Occurs check\n                if self.occurs(t1, t2):\n                    raise TypeError(f'Infinite type: {t1} = {t2}')\n                self.substitution[t1.name] = t2\n        elif isinstance(t2, TVar):\n            self.unify(t2, t1)\n        elif type(t1) == type(t2):\n            if isinstance(t1, TFunction):\n                if len(t1.params) != len(t2.params):\n                    raise TypeError(f'Arity mismatch')\n                for p1, p2 in zip(t1.params, t2.params):\n                    self.unify(p1, p2)\n                self.unify(t1.ret, t2.ret)\n        else:\n            raise TypeError(f'Cannot unify {t1} with {t2}')\n    \n    def apply_substitution(self, type: Type) -> Type:\n        if isinstance(type, TVar):\n            if type.name in self.substitution:\n                return self.apply_substitution(self.substitution[type.name])\n            return type\n        elif isinstance(type, TFunction):\n            return TFunction(\n                [self.apply_substitution(p) for p in type.params],\n                self.apply_substitution(type.ret)\n            )\n        return type\n    \n    def occurs(self, tvar: TVar, type: Type) -> bool:\n        '''Check if tvar occurs in type (infinite type check)'''\n        type = self.apply_substitution(type)\n        if isinstance(type, TVar):\n            return tvar.name == type.name\n        elif isinstance(type, TFunction):\n            return any(self.occurs(tvar, p) for p in type.params) or self.occurs(tvar, type.ret)\n        return False"
                        },
                        pitfalls: ["Infinite types", "Substitution application", "Occurs check"],
                        concepts: ["Type inference", "Unification", "Constraints"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 4,
                        name: "Polymorphism",
                        description: "Add support for generic/polymorphic types.",
                        criteria: ["Let polymorphism", "Generalize types at let bindings", "Instantiate polymorphic types at use sites", "Type schemes (forall quantification)"],
                        hints: {
                            level1: "Polymorphism: 'id' has type forall a. a -> a.",
                            level2: "Generalize: collect free type vars. Instantiate: replace with fresh vars.",
                            level3: "@dataclass\nclass Scheme:\n    '''Type scheme: forall [vars] . type'''\n    vars: List[str]  # Quantified variables\n    type: Type\n\nclass TypeInferrer:\n    def generalize(self, type: Type, env: TypeEnv) -> Scheme:\n        '''Generalize type by quantifying free variables not in env'''\n        type = self.apply_substitution(type)\n        free_in_type = self.free_vars(type)\n        free_in_env = self.free_vars_env(env)\n        quantified = free_in_type - free_in_env\n        return Scheme(list(quantified), type)\n    \n    def instantiate(self, scheme: Scheme) -> Type:\n        '''Create fresh type variables for quantified vars'''\n        subst = {var: self.fresh_type_var() for var in scheme.vars}\n        return self.apply_subst_to_type(scheme.type, subst)\n    \n    def free_vars(self, type: Type) -> set:\n        if isinstance(type, TVar):\n            return {type.name}\n        elif isinstance(type, TFunction):\n            result = set()\n            for p in type.params:\n                result |= self.free_vars(p)\n            result |= self.free_vars(type.ret)\n            return result\n        return set()\n\n# Algorithm W for let-polymorphism:\n# let x = e1 in e2\n# 1. Infer type of e1\n# 2. Generalize to get scheme\n# 3. Add x: scheme to env\n# 4. Infer type of e2 in extended env"
                        },
                        pitfalls: ["Value restriction", "Generalization timing", "Monomorphization"],
                        concepts: ["Polymorphism", "Type schemes", "Generalization"],
                        estimatedHours: "6-10"
                    }
                ]
            },

            // SOFTWARE ENGINEERING - More projects
            "git-workflow": {
                name: "Git Workflow Mastery",
                description: "Master advanced Git workflows. Learn branching strategies, rebasing, and collaborative patterns.",
                difficulty: "intermediate",
                estimatedHours: "8-12",
                prerequisites: ["Basic Git commands", "Version control concepts"],
                languages: { recommended: ["Bash"], also: [] },
                resources: [
                    { name: "Pro Git Book", url: "https://git-scm.com/book/en/v2", type: "book" },
                    { name: "Git Flight Rules", url: "https://github.com/k88hudson/git-flight-rules", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Branching Strategies",
                        description: "Implement and practice branching strategies.",
                        criteria: ["Feature branch workflow", "Git Flow (develop, feature, release, hotfix)", "Trunk-based development", "Understand trade-offs of each"],
                        hints: {
                            level1: "Feature branches: branch per feature, merge to main.",
                            level2: "Git Flow: main (releases), develop (integration), feature/* (work).",
                            level3: "# Feature Branch Workflow\ngit checkout -b feature/user-auth main\n# ... work ...\ngit push -u origin feature/user-auth\n# Create PR, review, merge\n\n# Git Flow\ngit checkout -b develop main\ngit checkout -b feature/login develop\n# ... work ...\ngit checkout develop && git merge --no-ff feature/login\n\n# Release\ngit checkout -b release/1.0 develop\n# ... fix bugs ...\ngit checkout main && git merge --no-ff release/1.0\ngit tag -a v1.0\ngit checkout develop && git merge --no-ff release/1.0\n\n# Hotfix\ngit checkout -b hotfix/critical-bug main\n# ... fix ...\ngit checkout main && git merge --no-ff hotfix/critical-bug\ngit checkout develop && git merge --no-ff hotfix/critical-bug"
                        },
                        pitfalls: ["Long-lived branches", "Merge conflicts", "Choosing wrong strategy"],
                        concepts: ["Branching strategies", "Git Flow", "Trunk-based development"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Rebasing and History",
                        description: "Master rebasing and history rewriting.",
                        criteria: ["Interactive rebase for cleanup", "Squash commits before merge", "Rebase vs merge trade-offs", "Handle rebase conflicts"],
                        hints: {
                            level1: "Rebase: replay commits on new base. Cleaner history.",
                            level2: "Interactive rebase: pick, squash, reword, edit, drop.",
                            level3: "# Rebase feature branch onto updated main\ngit checkout feature/my-feature\ngit fetch origin\ngit rebase origin/main\n\n# Interactive rebase to clean up last 5 commits\ngit rebase -i HEAD~5\n\n# In editor:\n# pick abc123 Add user model\n# squash def456 Fix typo\n# squash ghi789 Add tests\n# reword jkl012 Implement auth <- change commit message\n# drop mno345 WIP commit\n\n# Handle conflicts during rebase\n# 1. Fix conflicts in files\ngit add <resolved-files>\ngit rebase --continue\n# Or abort\ngit rebase --abort\n\n# Squash merge (alternative to rebasing)\ngit checkout main\ngit merge --squash feature/my-feature\ngit commit -m 'Add user authentication (#123)'"
                        },
                        pitfalls: ["Rebasing public branches", "Lost commits", "Complex conflicts"],
                        concepts: ["Rebasing", "Interactive rebase", "History rewriting"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Advanced Operations",
                        description: "Learn advanced Git operations and recovery.",
                        criteria: ["Cherry-pick specific commits", "Bisect to find bugs", "Stash management", "Recover lost commits with reflog"],
                        hints: {
                            level1: "reflog shows all HEAD movements. Commits aren't truly lost.",
                            level2: "bisect: binary search through history to find bug introduction.",
                            level3: "# Cherry-pick commit from another branch\ngit cherry-pick abc123\ngit cherry-pick abc123..def456  # Range\n\n# Bisect to find bug\ngit bisect start\ngit bisect bad  # Current commit is bad\ngit bisect good v1.0  # This version was good\n# Git checks out middle commit\n# Test and mark:\ngit bisect good  # or\ngit bisect bad\n# Repeat until found\ngit bisect reset\n\n# Stash management\ngit stash push -m 'WIP: auth changes'\ngit stash list\ngit stash apply stash@{0}\ngit stash pop  # Apply and remove\ngit stash drop stash@{1}\n\n# Recover 'lost' commits with reflog\ngit reflog\n# Shows:\n# abc123 HEAD@{0}: commit: Latest\n# def456 HEAD@{1}: reset: moving to HEAD~1\n# ghi789 HEAD@{2}: commit: The 'lost' commit\ngit checkout ghi789\n# Or\ngit branch recovered ghi789"
                        },
                        pitfalls: ["Cherry-pick conflicts", "Stash corruption", "Not using reflog in time"],
                        concepts: ["Cherry-pick", "Bisect", "Reflog", "Stash"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            "logging-structured": {
                name: "Structured Logging System",
                description: "Implement production-grade structured logging. Learn log levels, formats, and log aggregation patterns.",
                difficulty: "intermediate",
                estimatedHours: "10-15",
                prerequisites: ["JSON", "File I/O", "Basic debugging concepts"],
                languages: { recommended: ["Python", "Go", "Java"], also: ["JavaScript", "Rust"] },
                resources: [
                    { name: "12 Factor App - Logs", url: "https://12factor.net/logs", type: "article" },
                    { name: "Structured Logging Guide", url: "https://www.honeycomb.io/blog/structured-logging", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Logger Core",
                        description: "Build the core logging infrastructure.",
                        criteria: ["Log levels (DEBUG, INFO, WARN, ERROR)", "Configurable minimum level", "Thread-safe logging", "Output to multiple destinations"],
                        hints: {
                            level1: "Logger accepts level + message + context. Filters by min level.",
                            level2: "Use mutex/lock for thread safety. Handler pattern for outputs.",
                            level3: "import threading\nimport sys\nfrom enum import IntEnum\nfrom typing import Dict, Any, List, Callable\nfrom datetime import datetime\n\nclass Level(IntEnum):\n    DEBUG = 10\n    INFO = 20\n    WARN = 30\n    ERROR = 40\n    FATAL = 50\n\nclass LogRecord:\n    def __init__(self, level: Level, message: str, **context):\n        self.timestamp = datetime.utcnow()\n        self.level = level\n        self.message = message\n        self.context = context\n\nclass Handler:\n    def emit(self, record: LogRecord):\n        raise NotImplementedError\n\nclass Logger:\n    def __init__(self, name: str = 'root', level: Level = Level.INFO):\n        self.name = name\n        self.level = level\n        self.handlers: List[Handler] = []\n        self._lock = threading.Lock()\n    \n    def add_handler(self, handler: Handler):\n        self.handlers.append(handler)\n    \n    def log(self, level: Level, message: str, **context):\n        if level < self.level:\n            return\n        \n        record = LogRecord(level, message, logger=self.name, **context)\n        \n        with self._lock:\n            for handler in self.handlers:\n                handler.emit(record)\n    \n    def debug(self, message: str, **ctx): self.log(Level.DEBUG, message, **ctx)\n    def info(self, message: str, **ctx): self.log(Level.INFO, message, **ctx)\n    def warn(self, message: str, **ctx): self.log(Level.WARN, message, **ctx)\n    def error(self, message: str, **ctx): self.log(Level.ERROR, message, **ctx)"
                        },
                        pitfalls: ["Race conditions", "Blocking I/O in hot path", "Missing context"],
                        concepts: ["Log levels", "Thread safety", "Handler pattern"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Structured Output",
                        description: "Format logs as structured JSON.",
                        criteria: ["JSON output format", "Include timestamp, level, message, context", "Support custom formatters", "Pretty-print for development"],
                        hints: {
                            level1: "JSON format allows log aggregation tools to parse logs.",
                            level2: "Include: timestamp (ISO8601), level, message, all context fields.",
                            level3: "import json\n\nclass JSONHandler(Handler):\n    def __init__(self, stream=sys.stdout):\n        self.stream = stream\n    \n    def emit(self, record: LogRecord):\n        log_dict = {\n            'timestamp': record.timestamp.isoformat() + 'Z',\n            'level': record.level.name,\n            'message': record.message,\n            **record.context\n        }\n        line = json.dumps(log_dict, default=str) + '\\n'\n        self.stream.write(line)\n        self.stream.flush()\n\nclass PrettyHandler(Handler):\n    '''Human-readable format for development'''\n    COLORS = {\n        Level.DEBUG: '\\033[36m',  # Cyan\n        Level.INFO: '\\033[32m',   # Green\n        Level.WARN: '\\033[33m',   # Yellow\n        Level.ERROR: '\\033[31m',  # Red\n    }\n    RESET = '\\033[0m'\n    \n    def emit(self, record: LogRecord):\n        color = self.COLORS.get(record.level, '')\n        ts = record.timestamp.strftime('%H:%M:%S.%f')[:-3]\n        \n        ctx_str = ''\n        if record.context:\n            ctx_parts = [f'{k}={v!r}' for k, v in record.context.items() if k != 'logger']\n            if ctx_parts:\n                ctx_str = f' | {\" \".join(ctx_parts)}'\n        \n        print(f'{ts} {color}{record.level.name:5}{self.RESET} {record.message}{ctx_str}')"
                        },
                        pitfalls: ["Non-serializable values", "Missing flush", "Large context objects"],
                        concepts: ["Structured logging", "JSON format", "Formatters"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Context & Correlation",
                        description: "Add request context and correlation IDs.",
                        criteria: ["Automatic request ID injection", "Context propagation across functions", "Child loggers with inherited context", "Async-safe context"],
                        hints: {
                            level1: "Correlation ID: unique ID per request, included in all logs.",
                            level2: "Use thread-local or context variables for automatic injection.",
                            level3: "import contextvars\nimport uuid\n\n# Context variable for request context\nrequest_context: contextvars.ContextVar[Dict[str, Any]] = contextvars.ContextVar('request_context', default={})\n\nclass ContextLogger(Logger):\n    def log(self, level: Level, message: str, **context):\n        # Merge request context\n        ctx = {**request_context.get(), **context}\n        super().log(level, message, **ctx)\n\ndef with_request_context(**ctx):\n    '''Decorator to set request context'''\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            token = request_context.set({**request_context.get(), **ctx})\n            try:\n                return func(*args, **kwargs)\n            finally:\n                request_context.reset(token)\n        return wrapper\n    return decorator\n\n# Middleware example\nclass LoggingMiddleware:\n    def __call__(self, request, next_handler):\n        request_id = request.headers.get('X-Request-ID', str(uuid.uuid4()))\n        \n        token = request_context.set({\n            'request_id': request_id,\n            'method': request.method,\n            'path': request.path,\n        })\n        \n        try:\n            logger.info('Request started')\n            response = next_handler(request)\n            logger.info('Request completed', status=response.status)\n            return response\n        except Exception as e:\n            logger.error('Request failed', error=str(e))\n            raise\n        finally:\n            request_context.reset(token)"
                        },
                        pitfalls: ["Context not propagating", "Memory leaks", "Async context issues"],
                        concepts: ["Correlation IDs", "Context propagation", "Request tracing"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // SECURITY - More projects
            "aes-impl": {
                name: "AES Encryption Implementation",
                description: "Implement AES encryption from scratch. Learn symmetric cryptography, block ciphers, and modes of operation.",
                difficulty: "intermediate",
                estimatedHours: "15-25",
                prerequisites: ["Binary operations", "Modular arithmetic", "Basic cryptography concepts"],
                languages: { recommended: ["C", "Rust", "Python"], also: ["Go", "Java"] },
                resources: [
                    { name: "FIPS 197 (AES Spec)", url: "https://csrc.nist.gov/publications/detail/fips/197/final", type: "paper" },
                    { name: "A Stick Figure Guide to AES", url: "http://www.moserware.com/2009/09/stick-figure-guide-to-advanced.html", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Galois Field Arithmetic",
                        description: "Implement GF(2^8) operations used in AES.",
                        criteria: ["Addition (XOR)", "Multiplication in GF(2^8)", "Multiplicative inverse", "Pre-compute lookup tables"],
                        hints: {
                            level1: "GF(2^8) addition = XOR. Multiplication involves polynomial math.",
                            level2: "Use irreducible polynomial x^8 + x^4 + x^3 + x + 1 (0x11B).",
                            level3: "def gf_add(a, b):\n    '''Addition in GF(2^8) is XOR'''\n    return a ^ b\n\ndef gf_mul(a, b):\n    '''Multiplication in GF(2^8) with irreducible polynomial 0x11B'''\n    p = 0\n    for _ in range(8):\n        if b & 1:\n            p ^= a\n        hi_bit = a & 0x80\n        a = (a << 1) & 0xFF\n        if hi_bit:\n            a ^= 0x1B  # Reduce by x^8 + x^4 + x^3 + x + 1\n        b >>= 1\n    return p\n\n# Pre-compute lookup tables for efficiency\nMUL2 = [gf_mul(i, 2) for i in range(256)]\nMUL3 = [gf_mul(i, 3) for i in range(256)]\n\ndef gf_inverse(a):\n    '''Find multiplicative inverse using extended Euclidean algorithm'''\n    if a == 0:\n        return 0\n    # Or use: a^254 = a^(-1) in GF(2^8)\n    result = a\n    for _ in range(6):\n        result = gf_mul(result, result)\n        result = gf_mul(result, a)\n    return gf_mul(result, result)"
                        },
                        pitfalls: ["Overflow handling", "Wrong irreducible polynomial", "Off-by-one in tables"],
                        concepts: ["Galois fields", "Polynomial arithmetic", "Lookup tables"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "AES Core Operations",
                        description: "Implement the four AES round operations.",
                        criteria: ["SubBytes (S-box substitution)", "ShiftRows", "MixColumns", "AddRoundKey"],
                        hints: {
                            level1: "AES state is 4x4 byte matrix. Each round applies 4 transformations.",
                            level2: "SubBytes: apply S-box to each byte. ShiftRows: rotate rows.",
                            level3: "# S-box (pre-computed)\nSBOX = [\n    0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5,\n    # ... (256 values total)\n]\n\ndef sub_bytes(state):\n    '''Apply S-box to each byte'''\n    for i in range(4):\n        for j in range(4):\n            state[i][j] = SBOX[state[i][j]]\n\ndef shift_rows(state):\n    '''Rotate rows: row 0 by 0, row 1 by 1, row 2 by 2, row 3 by 3'''\n    state[1] = state[1][1:] + state[1][:1]\n    state[2] = state[2][2:] + state[2][:2]\n    state[3] = state[3][3:] + state[3][:3]\n\ndef mix_columns(state):\n    '''Mix each column using GF(2^8) matrix multiplication'''\n    for j in range(4):\n        a = [state[i][j] for i in range(4)]\n        state[0][j] = MUL2[a[0]] ^ MUL3[a[1]] ^ a[2] ^ a[3]\n        state[1][j] = a[0] ^ MUL2[a[1]] ^ MUL3[a[2]] ^ a[3]\n        state[2][j] = a[0] ^ a[1] ^ MUL2[a[2]] ^ MUL3[a[3]]\n        state[3][j] = MUL3[a[0]] ^ a[1] ^ a[2] ^ MUL2[a[3]]\n\ndef add_round_key(state, round_key):\n    '''XOR state with round key'''\n    for i in range(4):\n        for j in range(4):\n            state[i][j] ^= round_key[i][j]"
                        },
                        pitfalls: ["State matrix orientation", "ShiftRows direction", "MixColumns coefficients"],
                        concepts: ["Substitution", "Permutation", "Diffusion"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 3,
                        name: "Key Expansion",
                        description: "Implement the key schedule for AES-128/192/256.",
                        criteria: ["Expand 128-bit key to round keys", "RotWord and SubWord functions", "Round constants (Rcon)", "Support different key sizes"],
                        hints: {
                            level1: "Key expansion creates 11 round keys (128-bit) from original key.",
                            level2: "Each round key derived from previous. Special transform every 4th word.",
                            level3: "RCON = [0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80, 0x1B, 0x36]\n\ndef rot_word(word):\n    '''Rotate 4-byte word left by 1'''\n    return word[1:] + word[:1]\n\ndef sub_word(word):\n    '''Apply S-box to each byte of word'''\n    return [SBOX[b] for b in word]\n\ndef key_expansion(key):\n    '''Expand 16-byte key to 44 words (11 round keys)'''\n    # Key is 16 bytes = 4 words\n    w = [list(key[i:i+4]) for i in range(0, 16, 4)]\n    \n    for i in range(4, 44):  # Generate 40 more words\n        temp = w[i-1].copy()\n        if i % 4 == 0:\n            temp = sub_word(rot_word(temp))\n            temp[0] ^= RCON[i // 4 - 1]\n        w.append([w[i-4][j] ^ temp[j] for j in range(4)])\n    \n    # Convert to round keys (4 words each)\n    round_keys = []\n    for r in range(11):\n        round_key = [[w[r*4 + j][i] for j in range(4)] for i in range(4)]\n        round_keys.append(round_key)\n    \n    return round_keys"
                        },
                        pitfalls: ["Rcon indexing", "Word vs byte ordering", "Key length handling"],
                        concepts: ["Key schedule", "Round constants", "Key expansion"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Encryption & Modes",
                        description: "Complete encryption and implement cipher modes.",
                        criteria: ["Full AES encrypt function", "ECB mode (for learning)", "CBC mode with IV", "Padding (PKCS7)"],
                        hints: {
                            level1: "10 rounds for AES-128. Last round skips MixColumns.",
                            level2: "CBC: each block XORed with previous ciphertext before encryption.",
                            level3: "def aes_encrypt_block(plaintext, key):\n    '''Encrypt 16-byte block'''\n    # Convert to state matrix (column-major)\n    state = [[plaintext[i + 4*j] for j in range(4)] for i in range(4)]\n    round_keys = key_expansion(key)\n    \n    # Initial round\n    add_round_key(state, round_keys[0])\n    \n    # Main rounds\n    for r in range(1, 10):\n        sub_bytes(state)\n        shift_rows(state)\n        mix_columns(state)\n        add_round_key(state, round_keys[r])\n    \n    # Final round (no MixColumns)\n    sub_bytes(state)\n    shift_rows(state)\n    add_round_key(state, round_keys[10])\n    \n    # Convert back to bytes\n    return bytes([state[i][j] for j in range(4) for i in range(4)])\n\ndef pkcs7_pad(data, block_size=16):\n    pad_len = block_size - (len(data) % block_size)\n    return data + bytes([pad_len] * pad_len)\n\ndef aes_cbc_encrypt(plaintext, key, iv):\n    '''CBC mode encryption'''\n    plaintext = pkcs7_pad(plaintext)\n    ciphertext = b''\n    prev_block = iv\n    \n    for i in range(0, len(plaintext), 16):\n        block = bytes(a ^ b for a, b in zip(plaintext[i:i+16], prev_block))\n        encrypted = aes_encrypt_block(block, key)\n        ciphertext += encrypted\n        prev_block = encrypted\n    \n    return ciphertext"
                        },
                        pitfalls: ["ECB mode vulnerabilities", "IV reuse", "Padding oracle attacks"],
                        concepts: ["Block cipher modes", "CBC", "Padding"],
                        estimatedHours: "4-6"
                    }
                ]
            },

            // DISTRIBUTED - More projects
            "gossip-protocol": {
                name: "Gossip Protocol",
                description: "Implement a gossip-based protocol for eventually consistent data dissemination. Learn epidemic algorithms.",
                difficulty: "advanced",
                estimatedHours: "15-25",
                prerequisites: ["Networking basics", "Distributed systems concepts", "Probability"],
                languages: { recommended: ["Go", "Rust", "Python"], also: ["Java", "Erlang"] },
                resources: [
                    { name: "Gossip Protocol Explained", url: "https://www.cs.cornell.edu/home/rvr/papers/flowgossip.pdf", type: "paper" },
                    { name: "SWIM Paper", url: "https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf", type: "paper" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Peer Management",
                        description: "Manage cluster membership and peer list.",
                        criteria: ["Maintain list of known peers", "Random peer selection", "Handle peer join/leave", "Periodic peer list exchange"],
                        hints: {
                            level1: "Each node maintains list of known peers. Gossip updates this list.",
                            level2: "Random selection: pick k random peers to gossip with each round.",
                            level3: "import random\nimport threading\nfrom dataclasses import dataclass\nfrom typing import Dict, Set\nimport time\n\n@dataclass\nclass Peer:\n    address: str\n    port: int\n    last_seen: float\n    state: str = 'alive'  # alive, suspected, dead\n\nclass PeerManager:\n    def __init__(self, self_addr: str, self_port: int):\n        self.self_id = f'{self_addr}:{self_port}'\n        self.peers: Dict[str, Peer] = {}\n        self.lock = threading.Lock()\n        self.fanout = 3  # Number of peers to gossip with\n    \n    def add_peer(self, addr: str, port: int):\n        peer_id = f'{addr}:{port}'\n        with self.lock:\n            if peer_id != self.self_id and peer_id not in self.peers:\n                self.peers[peer_id] = Peer(addr, port, time.time())\n    \n    def select_random_peers(self, k: int = None) -> list:\n        '''Select k random alive peers'''\n        k = k or self.fanout\n        with self.lock:\n            alive = [p for p in self.peers.values() if p.state == 'alive']\n            return random.sample(alive, min(k, len(alive)))\n    \n    def mark_alive(self, peer_id: str):\n        with self.lock:\n            if peer_id in self.peers:\n                self.peers[peer_id].last_seen = time.time()\n                self.peers[peer_id].state = 'alive'\n    \n    def check_timeouts(self, timeout: float = 30.0):\n        '''Mark peers as dead if not seen recently'''\n        now = time.time()\n        with self.lock:\n            for peer in self.peers.values():\n                if now - peer.last_seen > timeout:\n                    peer.state = 'dead'"
                        },
                        pitfalls: ["Not handling own address", "Thread safety", "Stale peer info"],
                        concepts: ["Membership", "Random selection", "Peer state"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Push Gossip",
                        description: "Implement push-based gossip dissemination.",
                        criteria: ["Periodic gossip rounds", "Push updates to random peers", "Version/timestamp for updates", "Infection-style spreading"],
                        hints: {
                            level1: "Push: send your data to random peers. They forward to others.",
                            level2: "Version numbers prevent old data from overwriting new.",
                            level3: "import json\nimport socket\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass GossipMessage:\n    sender: str\n    msg_type: str  # 'push', 'pull', 'ack'\n    key: str\n    value: str\n    version: int\n    \n    def to_json(self):\n        return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls, data):\n        return cls(**json.loads(data))\n\nclass GossipNode:\n    def __init__(self, addr, port):\n        self.addr = addr\n        self.port = port\n        self.peer_manager = PeerManager(addr, port)\n        self.data: Dict[str, tuple] = {}  # key -> (value, version)\n        self.lock = threading.Lock()\n    \n    def set(self, key: str, value: str):\n        '''Set local value and prepare for gossip'''\n        with self.lock:\n            current = self.data.get(key, (None, 0))\n            self.data[key] = (value, current[1] + 1)\n    \n    def gossip_round(self):\n        '''Push all data to random peers'''\n        peers = self.peer_manager.select_random_peers()\n        \n        with self.lock:\n            items = list(self.data.items())\n        \n        for peer in peers:\n            for key, (value, version) in items:\n                msg = GossipMessage(\n                    sender=f'{self.addr}:{self.port}',\n                    msg_type='push',\n                    key=key,\n                    value=value,\n                    version=version\n                )\n                self.send_to_peer(peer, msg)\n    \n    def handle_push(self, msg: GossipMessage):\n        '''Receive pushed data'''\n        with self.lock:\n            current = self.data.get(msg.key, (None, 0))\n            if msg.version > current[1]:\n                self.data[msg.key] = (msg.value, msg.version)\n        \n        self.peer_manager.mark_alive(msg.sender)"
                        },
                        pitfalls: ["Version conflicts", "Infinite propagation", "Network floods"],
                        concepts: ["Push gossip", "Versioning", "Epidemic spread"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 3,
                        name: "Pull Gossip & Anti-Entropy",
                        description: "Add pull-based reconciliation for consistency.",
                        criteria: ["Pull mechanism to request missing data", "Anti-entropy: periodic full sync", "Combine push-pull for efficiency", "Handle network partitions"],
                        hints: {
                            level1: "Pull: request data you're missing. Anti-entropy: compare full state.",
                            level2: "Push-pull: push digest, peer responds with missing items.",
                            level3: "def create_digest(self) -> Dict[str, int]:\n    '''Create digest of local data (key -> version)'''\n    with self.lock:\n        return {k: v[1] for k, v in self.data.items()}\n\ndef pull_from_peer(self, peer: Peer):\n    '''Request peer's digest and pull missing/newer items'''\n    # Send our digest\n    our_digest = self.create_digest()\n    msg = GossipMessage(\n        sender=f'{self.addr}:{self.port}',\n        msg_type='pull_request',\n        key='__digest__',\n        value=json.dumps(our_digest),\n        version=0\n    )\n    self.send_to_peer(peer, msg)\n\ndef handle_pull_request(self, msg: GossipMessage):\n    '''Respond with items peer is missing or has old versions of'''\n    their_digest = json.loads(msg.value)\n    our_digest = self.create_digest()\n    \n    # Find items to send\n    to_send = []\n    with self.lock:\n        for key, (value, version) in self.data.items():\n            their_version = their_digest.get(key, 0)\n            if version > their_version:\n                to_send.append((key, value, version))\n    \n    # Send missing items\n    for key, value, version in to_send:\n        response = GossipMessage(\n            sender=f'{self.addr}:{self.port}',\n            msg_type='pull_response',\n            key=key,\n            value=value,\n            version=version\n        )\n        # Send to requester\n\ndef anti_entropy(self):\n    '''Periodic full state synchronization'''\n    peer = self.peer_manager.select_random_peers(1)\n    if peer:\n        self.pull_from_peer(peer[0])"
                        },
                        pitfalls: ["Digest size", "Sync storms", "Stale data during partition"],
                        concepts: ["Pull gossip", "Anti-entropy", "State reconciliation"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 4,
                        name: "Failure Detection",
                        description: "Implement gossip-based failure detection (SWIM-style).",
                        criteria: ["Probe random peers periodically", "Indirect probing through other peers", "Suspicion mechanism before declaring dead", "Disseminate membership changes"],
                        hints: {
                            level1: "SWIM: ping peer, if no response, ask others to ping (indirect probe).",
                            level2: "Suspicion: don't immediately mark dead. Wait for confirmation.",
                            level3: "def failure_detection_round(self):\n    '''SWIM-style failure detection'''\n    peer = self.peer_manager.select_random_peers(1)\n    if not peer:\n        return\n    \n    target = peer[0]\n    \n    # Direct probe\n    if self.ping(target):\n        self.peer_manager.mark_alive(f'{target.address}:{target.port}')\n        return\n    \n    # Direct probe failed, try indirect\n    helpers = self.peer_manager.select_random_peers(3)\n    responses = []\n    \n    for helper in helpers:\n        # Ask helper to ping target\n        msg = GossipMessage(\n            sender=f'{self.addr}:{self.port}',\n            msg_type='ping_req',\n            key='target',\n            value=f'{target.address}:{target.port}',\n            version=0\n        )\n        if self.send_and_wait(helper, msg, timeout=2.0):\n            responses.append(True)\n    \n    if any(responses):\n        # Target reachable through helper\n        self.peer_manager.mark_alive(f'{target.address}:{target.port}')\n    else:\n        # Start suspicion\n        self.suspect_peer(target)\n\ndef suspect_peer(self, peer: Peer):\n    '''Mark peer as suspected, wait before declaring dead'''\n    peer_id = f'{peer.address}:{peer.port}'\n    with self.lock:\n        if peer_id in self.peers:\n            self.peers[peer_id].state = 'suspected'\n    \n    # Schedule dead declaration if not refuted\n    threading.Timer(10.0, lambda: self.confirm_dead(peer_id)).start()\n\ndef confirm_dead(self, peer_id: str):\n    '''Confirm death if still suspected'''\n    with self.lock:\n        if peer_id in self.peers and self.peers[peer_id].state == 'suspected':\n            self.peers[peer_id].state = 'dead'\n            # Gossip membership change\n            self.broadcast_membership_change(peer_id, 'dead')"
                        },
                        pitfalls: ["False positives", "Suspicion timeout tuning", "Split brain"],
                        concepts: ["SWIM protocol", "Failure detection", "Suspicion"],
                        estimatedHours: "4-6"
                    }
                ]
            },

            // SYSTEMS - BEGINNER
            "file-copy": {
                name: "File Copy (cp clone)",
                description: "Build a file copy utility. Learn file I/O, permissions, and error handling.",
                difficulty: "beginner",
                estimatedHours: "4-8",
                prerequisites: ["Basic file I/O", "Command-line arguments"],
                languages: { recommended: ["C", "Rust", "Go"], also: ["Python"] },
                resources: [
                    { name: "POSIX File Operations", url: "https://pubs.opengroup.org/onlinepubs/9699919799/functions/read.html", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic File Copy",
                        description: "Copy a single file to a destination.",
                        criteria: ["Open source file for reading", "Create destination file", "Copy contents in chunks", "Close files properly"],
                        hints: {
                            level1: "Open source (read-only), open/create dest (write-only), loop read/write.",
                            level2: "Use buffer size of 4KB-64KB for efficiency. Check return values.",
                            level3: "#include <stdio.h>\n#include <stdlib.h>\n#include <fcntl.h>\n#include <unistd.h>\n\n#define BUF_SIZE 4096\n\nint copy_file(const char *src, const char *dst) {\n    int fd_src = open(src, O_RDONLY);\n    if (fd_src < 0) {\n        perror(\"open source\");\n        return -1;\n    }\n    \n    int fd_dst = open(dst, O_WRONLY | O_CREAT | O_TRUNC, 0644);\n    if (fd_dst < 0) {\n        perror(\"open dest\");\n        close(fd_src);\n        return -1;\n    }\n    \n    char buf[BUF_SIZE];\n    ssize_t bytes_read;\n    \n    while ((bytes_read = read(fd_src, buf, BUF_SIZE)) > 0) {\n        ssize_t bytes_written = write(fd_dst, buf, bytes_read);\n        if (bytes_written != bytes_read) {\n            perror(\"write\");\n            close(fd_src);\n            close(fd_dst);\n            return -1;\n        }\n    }\n    \n    close(fd_src);\n    close(fd_dst);\n    return 0;\n}"
                        },
                        pitfalls: ["Not handling partial writes", "Forgetting to close files", "Wrong file permissions"],
                        concepts: ["File descriptors", "Buffered I/O", "Error handling"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 2,
                        name: "Preserve Permissions",
                        description: "Copy file permissions and metadata.",
                        criteria: ["Preserve file mode (rwx)", "Copy timestamps", "Handle special files gracefully"],
                        hints: {
                            level1: "Use stat() to get source file info, then set on destination.",
                            level2: "chmod() for permissions, utimes() for timestamps.",
                            level3: "#include <sys/stat.h>\n#include <sys/time.h>\n\nint preserve_metadata(const char *src, const char *dst) {\n    struct stat st;\n    if (stat(src, &st) < 0) {\n        perror(\"stat\");\n        return -1;\n    }\n    \n    // Preserve permissions\n    if (chmod(dst, st.st_mode) < 0) {\n        perror(\"chmod\");\n        return -1;\n    }\n    \n    // Preserve timestamps\n    struct timeval times[2];\n    times[0].tv_sec = st.st_atime;  // Access time\n    times[0].tv_usec = 0;\n    times[1].tv_sec = st.st_mtime;  // Modification time\n    times[1].tv_usec = 0;\n    \n    if (utimes(dst, times) < 0) {\n        perror(\"utimes\");\n        return -1;\n    }\n    \n    return 0;\n}"
                        },
                        pitfalls: ["Forgetting executable bit", "Not checking stat return", "Platform differences"],
                        concepts: ["File metadata", "Permissions", "Timestamps"],
                        estimatedHours: "1-2"
                    },
                    {
                        id: 3,
                        name: "Directory Copy",
                        description: "Recursively copy directories.",
                        criteria: ["Detect if source is directory", "Create destination directory", "Recursively copy contents", "Handle symbolic links option"],
                        hints: {
                            level1: "Check S_ISDIR(st.st_mode). Use opendir/readdir to iterate.",
                            level2: "Skip . and .. entries. Handle -r flag for recursive.",
                            level3: "#include <dirent.h>\n#include <string.h>\n\nint copy_directory(const char *src, const char *dst) {\n    struct stat st;\n    stat(src, &st);\n    \n    // Create destination directory\n    if (mkdir(dst, st.st_mode) < 0 && errno != EEXIST) {\n        perror(\"mkdir\");\n        return -1;\n    }\n    \n    DIR *dir = opendir(src);\n    if (!dir) {\n        perror(\"opendir\");\n        return -1;\n    }\n    \n    struct dirent *entry;\n    while ((entry = readdir(dir)) != NULL) {\n        // Skip . and ..\n        if (strcmp(entry->d_name, \".\") == 0 || strcmp(entry->d_name, \"..\") == 0)\n            continue;\n        \n        // Build full paths\n        char src_path[PATH_MAX], dst_path[PATH_MAX];\n        snprintf(src_path, sizeof(src_path), \"%s/%s\", src, entry->d_name);\n        snprintf(dst_path, sizeof(dst_path), \"%s/%s\", dst, entry->d_name);\n        \n        struct stat entry_st;\n        if (lstat(src_path, &entry_st) < 0) continue;\n        \n        if (S_ISDIR(entry_st.st_mode)) {\n            copy_directory(src_path, dst_path);  // Recurse\n        } else {\n            copy_file(src_path, dst_path);\n        }\n    }\n    \n    closedir(dir);\n    return 0;\n}"
                        },
                        pitfalls: ["Infinite recursion with symlinks", "Path buffer overflow", "Permission denied errors"],
                        concepts: ["Directory traversal", "Recursion", "Path handling"],
                        estimatedHours: "2-3"
                    }
                ]
            },

            // DATA STORAGE - INTERMEDIATE
            "sql-parser": {
                name: "SQL Parser",
                description: "Build a SQL parser for SELECT, INSERT, UPDATE queries. Learn query parsing and AST construction.",
                difficulty: "intermediate",
                estimatedHours: "15-25",
                prerequisites: ["Tokenizer basics", "Recursive descent parsing", "Tree data structures"],
                languages: { recommended: ["Python", "Go", "Rust"], also: ["JavaScript", "Java"] },
                resources: [
                    { name: "SQLite Grammar", url: "https://www.sqlite.org/lang.html", type: "documentation" },
                    { name: "Writing a SQL Parser", url: "https://blog.subnetzero.io/post/building-a-sql-parser/", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "SQL Tokenizer",
                        description: "Tokenize SQL statements into keywords, identifiers, literals.",
                        criteria: ["Recognize SQL keywords (SELECT, FROM, WHERE)", "Handle identifiers and numbers", "Parse string literals", "Support operators (=, <, >, AND, OR)"],
                        hints: {
                            level1: "Keywords are case-insensitive. Identifiers can be quoted.",
                            level2: "Handle both single and double quotes for strings. Whitespace separates tokens.",
                            level3: "import re\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\n\nclass TokenType(Enum):\n    SELECT = auto()\n    FROM = auto()\n    WHERE = auto()\n    INSERT = auto()\n    INTO = auto()\n    VALUES = auto()\n    UPDATE = auto()\n    SET = auto()\n    DELETE = auto()\n    AND = auto()\n    OR = auto()\n    NOT = auto()\n    NULL = auto()\n    IDENTIFIER = auto()\n    NUMBER = auto()\n    STRING = auto()\n    STAR = auto()\n    COMMA = auto()\n    LPAREN = auto()\n    RPAREN = auto()\n    EQ = auto()\n    NE = auto()\n    LT = auto()\n    GT = auto()\n    LE = auto()\n    GE = auto()\n    EOF = auto()\n\nKEYWORDS = {\n    'SELECT': TokenType.SELECT, 'FROM': TokenType.FROM,\n    'WHERE': TokenType.WHERE, 'AND': TokenType.AND,\n    'OR': TokenType.OR, 'INSERT': TokenType.INSERT,\n    'INTO': TokenType.INTO, 'VALUES': TokenType.VALUES,\n    'UPDATE': TokenType.UPDATE, 'SET': TokenType.SET,\n    'DELETE': TokenType.DELETE, 'NULL': TokenType.NULL,\n    'NOT': TokenType.NOT,\n}\n\n@dataclass\nclass Token:\n    type: TokenType\n    value: str\n    \nclass SQLTokenizer:\n    def __init__(self, sql: str):\n        self.sql = sql\n        self.pos = 0\n    \n    def tokenize(self) -> list:\n        tokens = []\n        while self.pos < len(self.sql):\n            self.skip_whitespace()\n            if self.pos >= len(self.sql):\n                break\n            token = self.next_token()\n            if token:\n                tokens.append(token)\n        tokens.append(Token(TokenType.EOF, ''))\n        return tokens"
                        },
                        pitfalls: ["Case sensitivity", "Escape sequences in strings", "Multi-char operators"],
                        concepts: ["Lexical analysis", "Token types", "SQL syntax"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "SELECT Parser",
                        description: "Parse SELECT statements into AST.",
                        criteria: ["Parse column list or *", "Parse FROM clause with table name", "Handle multiple columns", "Parse aliases (AS)"],
                        hints: {
                            level1: "SELECT columns FROM table. columns = * | column_list.",
                            level2: "column_list = column (, column)*. column can have alias.",
                            level3: "@dataclass\nclass SelectColumn:\n    name: str\n    alias: str = None\n\n@dataclass\nclass SelectStmt:\n    columns: list  # [SelectColumn] or ['*']\n    table: str\n    table_alias: str = None\n    where: 'Expr' = None\n\nclass SQLParser:\n    def __init__(self, tokens: list):\n        self.tokens = tokens\n        self.pos = 0\n    \n    def parse(self):\n        if self.current().type == TokenType.SELECT:\n            return self.parse_select()\n        # ... other statements\n    \n    def parse_select(self):\n        self.expect(TokenType.SELECT)\n        \n        # Parse columns\n        if self.match(TokenType.STAR):\n            columns = ['*']\n        else:\n            columns = self.parse_column_list()\n        \n        # Parse FROM\n        self.expect(TokenType.FROM)\n        table = self.expect(TokenType.IDENTIFIER).value\n        \n        # Optional alias\n        table_alias = None\n        if self.check(TokenType.IDENTIFIER) or self.match_keyword('AS'):\n            table_alias = self.expect(TokenType.IDENTIFIER).value\n        \n        # Optional WHERE\n        where = None\n        if self.match(TokenType.WHERE):\n            where = self.parse_expr()\n        \n        return SelectStmt(columns, table, table_alias, where)\n    \n    def parse_column_list(self):\n        columns = [self.parse_column()]\n        while self.match(TokenType.COMMA):\n            columns.append(self.parse_column())\n        return columns\n    \n    def parse_column(self):\n        name = self.expect(TokenType.IDENTIFIER).value\n        alias = None\n        if self.match_keyword('AS') or self.check(TokenType.IDENTIFIER):\n            alias = self.expect(TokenType.IDENTIFIER).value\n        return SelectColumn(name, alias)"
                        },
                        pitfalls: ["Missing commas", "Alias without AS", "Reserved words as identifiers"],
                        concepts: ["Recursive descent", "AST construction", "Grammar rules"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 3,
                        name: "WHERE Clause",
                        description: "Parse WHERE expressions with operators and logic.",
                        criteria: ["Comparison operators (=, <, >, !=)", "Boolean operators (AND, OR, NOT)", "Parentheses for grouping", "NULL checks (IS NULL, IS NOT NULL)"],
                        hints: {
                            level1: "Precedence: NOT > AND > OR. Use recursive descent.",
                            level2: "or_expr -> and_expr (OR and_expr)*. and_expr -> not_expr (AND not_expr)*.",
                            level3: "@dataclass\nclass BinaryExpr:\n    left: 'Expr'\n    op: str\n    right: 'Expr'\n\n@dataclass\nclass UnaryExpr:\n    op: str\n    operand: 'Expr'\n\n@dataclass\nclass Comparison:\n    left: 'Expr'\n    op: str  # =, <, >, <=, >=, !=\n    right: 'Expr'\n\n@dataclass\nclass IsNull:\n    expr: 'Expr'\n    negated: bool  # IS NOT NULL\n\ndef parse_expr(self):\n    return self.parse_or()\n\ndef parse_or(self):\n    left = self.parse_and()\n    while self.match(TokenType.OR):\n        right = self.parse_and()\n        left = BinaryExpr(left, 'OR', right)\n    return left\n\ndef parse_and(self):\n    left = self.parse_not()\n    while self.match(TokenType.AND):\n        right = self.parse_not()\n        left = BinaryExpr(left, 'AND', right)\n    return left\n\ndef parse_not(self):\n    if self.match(TokenType.NOT):\n        return UnaryExpr('NOT', self.parse_not())\n    return self.parse_comparison()\n\ndef parse_comparison(self):\n    left = self.parse_primary()\n    \n    # IS NULL / IS NOT NULL\n    if self.match_keyword('IS'):\n        negated = self.match(TokenType.NOT)\n        self.expect(TokenType.NULL)\n        return IsNull(left, negated)\n    \n    # Comparison operators\n    if self.current().type in (TokenType.EQ, TokenType.NE, TokenType.LT, TokenType.GT, TokenType.LE, TokenType.GE):\n        op = self.advance().value\n        right = self.parse_primary()\n        return Comparison(left, op, right)\n    \n    return left\n\ndef parse_primary(self):\n    if self.match(TokenType.LPAREN):\n        expr = self.parse_expr()\n        self.expect(TokenType.RPAREN)\n        return expr\n    \n    if self.check(TokenType.NUMBER):\n        return Literal(int(self.advance().value))\n    if self.check(TokenType.STRING):\n        return Literal(self.advance().value)\n    if self.check(TokenType.IDENTIFIER):\n        return Identifier(self.advance().value)\n    \n    raise ParseError(f'Unexpected token: {self.current()}')"
                        },
                        pitfalls: ["Operator precedence", "Unbalanced parentheses", "NULL comparisons"],
                        concepts: ["Expression parsing", "Precedence climbing", "Boolean logic"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 4,
                        name: "INSERT/UPDATE/DELETE",
                        description: "Parse modification statements.",
                        criteria: ["INSERT INTO table (cols) VALUES (vals)", "UPDATE table SET col=val WHERE...", "DELETE FROM table WHERE...", "Handle multiple rows in INSERT"],
                        hints: {
                            level1: "INSERT: columns optional. VALUES can have multiple tuples.",
                            level2: "UPDATE: SET clause is comma-separated assignments.",
                            level3: "@dataclass\nclass InsertStmt:\n    table: str\n    columns: list  # Optional column list\n    values: list   # List of value tuples\n\n@dataclass\nclass UpdateStmt:\n    table: str\n    assignments: list  # [(column, value), ...]\n    where: Expr = None\n\n@dataclass\nclass DeleteStmt:\n    table: str\n    where: Expr = None\n\ndef parse_insert(self):\n    self.expect(TokenType.INSERT)\n    self.expect(TokenType.INTO)\n    table = self.expect(TokenType.IDENTIFIER).value\n    \n    # Optional column list\n    columns = None\n    if self.match(TokenType.LPAREN):\n        columns = []\n        columns.append(self.expect(TokenType.IDENTIFIER).value)\n        while self.match(TokenType.COMMA):\n            columns.append(self.expect(TokenType.IDENTIFIER).value)\n        self.expect(TokenType.RPAREN)\n    \n    self.expect(TokenType.VALUES)\n    values = []\n    values.append(self.parse_value_tuple())\n    while self.match(TokenType.COMMA):\n        values.append(self.parse_value_tuple())\n    \n    return InsertStmt(table, columns, values)\n\ndef parse_update(self):\n    self.expect(TokenType.UPDATE)\n    table = self.expect(TokenType.IDENTIFIER).value\n    self.expect(TokenType.SET)\n    \n    assignments = []\n    col = self.expect(TokenType.IDENTIFIER).value\n    self.expect(TokenType.EQ)\n    val = self.parse_primary()\n    assignments.append((col, val))\n    \n    while self.match(TokenType.COMMA):\n        col = self.expect(TokenType.IDENTIFIER).value\n        self.expect(TokenType.EQ)\n        val = self.parse_primary()\n        assignments.append((col, val))\n    \n    where = None\n    if self.match(TokenType.WHERE):\n        where = self.parse_expr()\n    \n    return UpdateStmt(table, assignments, where)"
                        },
                        pitfalls: ["Column/value count mismatch", "Missing WHERE in DELETE", "Type mismatches"],
                        concepts: ["DML parsing", "Statement types", "Data modification"],
                        estimatedHours: "4-5"
                    }
                ]
            },

            // GAME DEV - ADVANCED
            "software-3d": {
                name: "Software 3D Renderer",
                description: "Build a 3D renderer without GPU acceleration. Learn the math behind 3D graphics from scratch.",
                difficulty: "advanced",
                estimatedHours: "30-50",
                prerequisites: ["Linear algebra (matrices, vectors)", "Basic trigonometry", "2D graphics basics"],
                languages: { recommended: ["C", "C++", "Rust"], also: ["Python", "JavaScript"] },
                resources: [
                    { name: "tinyrenderer", url: "https://github.com/ssloy/tinyrenderer/wiki", type: "tutorial" },
                    { name: "3D Math Primer", url: "https://gamemath.com/", type: "book" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Line Drawing",
                        description: "Implement Bresenham's line algorithm.",
                        criteria: ["Draw lines between any two points", "Handle all octants correctly", "Optimize for integer-only math", "Draw to a pixel buffer"],
                        hints: {
                            level1: "Bresenham: use error accumulation instead of floats.",
                            level2: "Handle steep lines by swapping x/y. Handle direction with step.",
                            level3: "void draw_line(int x0, int y0, int x1, int y1, uint32_t *buffer, int width) {\n    int steep = abs(y1 - y0) > abs(x1 - x0);\n    if (steep) {\n        // Swap x and y\n        int tmp = x0; x0 = y0; y0 = tmp;\n        tmp = x1; x1 = y1; y1 = tmp;\n    }\n    if (x0 > x1) {\n        int tmp = x0; x0 = x1; x1 = tmp;\n        tmp = y0; y0 = y1; y1 = tmp;\n    }\n    \n    int dx = x1 - x0;\n    int dy = abs(y1 - y0);\n    int error = dx / 2;\n    int ystep = (y0 < y1) ? 1 : -1;\n    int y = y0;\n    \n    for (int x = x0; x <= x1; x++) {\n        if (steep)\n            buffer[x * width + y] = 0xFFFFFF;\n        else\n            buffer[y * width + x] = 0xFFFFFF;\n        \n        error -= dy;\n        if (error < 0) {\n            y += ystep;\n            error += dx;\n        }\n    }\n}"
                        },
                        pitfalls: ["Integer overflow", "Division by zero", "Missing octants"],
                        concepts: ["Rasterization", "Bresenham's algorithm", "Pixel buffers"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Triangle Rasterization",
                        description: "Fill triangles using scanline or barycentric methods.",
                        criteria: ["Fill solid triangles", "Handle edge cases (flat top/bottom)", "Implement barycentric coordinates", "Anti-aliasing (optional)"],
                        hints: {
                            level1: "Barycentric: point P in triangle if all weights positive.",
                            level2: "Compute bounding box, test each pixel with barycentric coords.",
                            level3: "typedef struct { float x, y; } Vec2;\n\nfloat edge_function(Vec2 a, Vec2 b, Vec2 c) {\n    return (c.x - a.x) * (b.y - a.y) - (c.y - a.y) * (b.x - a.x);\n}\n\nvoid draw_triangle(Vec2 v0, Vec2 v1, Vec2 v2, uint32_t color, uint32_t *buffer, int w, int h) {\n    // Bounding box\n    int minX = max(0, (int)min(v0.x, min(v1.x, v2.x)));\n    int maxX = min(w-1, (int)max(v0.x, max(v1.x, v2.x)));\n    int minY = max(0, (int)min(v0.y, min(v1.y, v2.y)));\n    int maxY = min(h-1, (int)max(v0.y, max(v1.y, v2.y)));\n    \n    float area = edge_function(v0, v1, v2);\n    \n    for (int y = minY; y <= maxY; y++) {\n        for (int x = minX; x <= maxX; x++) {\n            Vec2 p = {x + 0.5f, y + 0.5f};\n            \n            float w0 = edge_function(v1, v2, p);\n            float w1 = edge_function(v2, v0, p);\n            float w2 = edge_function(v0, v1, p);\n            \n            // Point inside triangle if all same sign\n            if (w0 >= 0 && w1 >= 0 && w2 >= 0) {\n                // Barycentric coordinates\n                w0 /= area; w1 /= area; w2 /= area;\n                buffer[y * w + x] = color;\n            }\n        }\n    }\n}"
                        },
                        pitfalls: ["Winding order", "Gaps between triangles", "Subpixel precision"],
                        concepts: ["Barycentric coordinates", "Rasterization", "Fill rules"],
                        estimatedHours: "5-7"
                    },
                    {
                        id: 3,
                        name: "3D Transformations",
                        description: "Implement model, view, and projection matrices.",
                        criteria: ["4x4 matrix multiplication", "Translation, rotation, scaling", "Look-at camera matrix", "Perspective projection"],
                        hints: {
                            level1: "MVP: Model * View * Projection. Apply to each vertex.",
                            level2: "Perspective divide: after projection, divide x,y,z by w.",
                            level3: "typedef struct { float m[4][4]; } Mat4;\n\nMat4 mat4_perspective(float fov, float aspect, float near, float far) {\n    Mat4 m = {0};\n    float tanHalfFov = tan(fov / 2.0f);\n    m.m[0][0] = 1.0f / (aspect * tanHalfFov);\n    m.m[1][1] = 1.0f / tanHalfFov;\n    m.m[2][2] = -(far + near) / (far - near);\n    m.m[2][3] = -1.0f;\n    m.m[3][2] = -(2.0f * far * near) / (far - near);\n    return m;\n}\n\nMat4 mat4_look_at(Vec3 eye, Vec3 target, Vec3 up) {\n    Vec3 f = vec3_normalize(vec3_sub(target, eye));\n    Vec3 r = vec3_normalize(vec3_cross(f, up));\n    Vec3 u = vec3_cross(r, f);\n    \n    Mat4 m = mat4_identity();\n    m.m[0][0] = r.x; m.m[1][0] = r.y; m.m[2][0] = r.z;\n    m.m[0][1] = u.x; m.m[1][1] = u.y; m.m[2][1] = u.z;\n    m.m[0][2] = -f.x; m.m[1][2] = -f.y; m.m[2][2] = -f.z;\n    m.m[3][0] = -vec3_dot(r, eye);\n    m.m[3][1] = -vec3_dot(u, eye);\n    m.m[3][2] = vec3_dot(f, eye);\n    return m;\n}\n\nVec3 project_vertex(Vec3 v, Mat4 mvp, int width, int height) {\n    // Apply MVP\n    float w = mvp.m[0][3]*v.x + mvp.m[1][3]*v.y + mvp.m[2][3]*v.z + mvp.m[3][3];\n    float x = mvp.m[0][0]*v.x + mvp.m[1][0]*v.y + mvp.m[2][0]*v.z + mvp.m[3][0];\n    float y = mvp.m[0][1]*v.x + mvp.m[1][1]*v.y + mvp.m[2][1]*v.z + mvp.m[3][1];\n    \n    // Perspective divide\n    x /= w; y /= w;\n    \n    // Viewport transform\n    return (Vec3){\n        (x + 1.0f) * width / 2.0f,\n        (1.0f - y) * height / 2.0f,\n        0\n    };\n}"
                        },
                        pitfalls: ["Matrix multiplication order", "Homogeneous coordinates", "Y-axis flip"],
                        concepts: ["Linear transformations", "Projection", "Camera space"],
                        estimatedHours: "6-8"
                    },
                    {
                        id: 4,
                        name: "Depth Buffer & Lighting",
                        description: "Add z-buffering and basic shading.",
                        criteria: ["Z-buffer for hidden surface removal", "Flat shading with surface normals", "Gouraud shading (vertex interpolation)", "Basic diffuse lighting"],
                        hints: {
                            level1: "Z-buffer: store depth per pixel, only draw if closer.",
                            level2: "Interpolate z using barycentric coordinates.",
                            level3: "float *zbuffer;  // Initialize to infinity\n\nvoid draw_triangle_zbuf(Vec3 v0, Vec3 v1, Vec3 v2, uint32_t color,\n                         uint32_t *buffer, float *zbuffer, int w, int h) {\n    // ... bounding box code ...\n    \n    for (int y = minY; y <= maxY; y++) {\n        for (int x = minX; x <= maxX; x++) {\n            Vec2 p = {x + 0.5f, y + 0.5f};\n            \n            float w0 = edge_function(v1, v2, p) / area;\n            float w1 = edge_function(v2, v0, p) / area;\n            float w2 = edge_function(v0, v1, p) / area;\n            \n            if (w0 >= 0 && w1 >= 0 && w2 >= 0) {\n                // Interpolate z\n                float z = w0 * v0.z + w1 * v1.z + w2 * v2.z;\n                \n                int idx = y * w + x;\n                if (z < zbuffer[idx]) {\n                    zbuffer[idx] = z;\n                    buffer[idx] = color;\n                }\n            }\n        }\n    }\n}\n\n// Flat shading\nVec3 compute_normal(Vec3 v0, Vec3 v1, Vec3 v2) {\n    Vec3 edge1 = vec3_sub(v1, v0);\n    Vec3 edge2 = vec3_sub(v2, v0);\n    return vec3_normalize(vec3_cross(edge1, edge2));\n}\n\nfloat compute_lighting(Vec3 normal, Vec3 light_dir) {\n    float intensity = vec3_dot(normal, light_dir);\n    return max(0.0f, intensity);  // Clamp to [0,1]\n}"
                        },
                        pitfalls: ["Z-fighting", "Normal direction", "Interpolation artifacts"],
                        concepts: ["Z-buffering", "Surface normals", "Lighting models"],
                        estimatedHours: "8-12"
                    }
                ]
            },

            // CS FUNDAMENTALS - ADVANCED
            "red-black-tree": {
                name: "Red-Black Tree",
                description: "Implement a self-balancing red-black tree. Learn the invariants and rotations that maintain balance.",
                difficulty: "advanced",
                estimatedHours: "15-25",
                prerequisites: ["Binary search trees", "Tree rotations", "Recursion"],
                languages: { recommended: ["C", "Java", "Python"], also: ["Rust", "Go"] },
                resources: [
                    { name: "CLRS - Red-Black Trees", url: "https://mitpress.mit.edu/books/introduction-algorithms-fourth-edition", type: "book" },
                    { name: "Red-Black Tree Visualization", url: "https://www.cs.usfca.edu/~galles/visualization/RedBlack.html", type: "interactive" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Tree Structure",
                        description: "Define the red-black tree node structure.",
                        criteria: ["Node with key, value, color, parent, left, right", "NIL sentinel node (black)", "Root is always black", "Implement basic BST search"],
                        hints: {
                            level1: "RB properties: 1) Nodes red or black, 2) Root black, 3) Leaves (NIL) black, 4) Red nodes have black children, 5) Equal black height on all paths.",
                            level2: "Use a single NIL sentinel to simplify code.",
                            level3: "from enum import Enum\n\nclass Color(Enum):\n    RED = 0\n    BLACK = 1\n\nclass Node:\n    def __init__(self, key, value=None):\n        self.key = key\n        self.value = value\n        self.color = Color.RED  # New nodes are red\n        self.left = None\n        self.right = None\n        self.parent = None\n\nclass RedBlackTree:\n    def __init__(self):\n        self.NIL = Node(None)\n        self.NIL.color = Color.BLACK\n        self.root = self.NIL\n    \n    def search(self, key):\n        node = self.root\n        while node != self.NIL:\n            if key == node.key:\n                return node\n            elif key < node.key:\n                node = node.left\n            else:\n                node = node.right\n        return None\n    \n    def minimum(self, node):\n        while node.left != self.NIL:\n            node = node.left\n        return node"
                        },
                        pitfalls: ["Forgetting NIL sentinel", "Null pointer errors", "Color initialization"],
                        concepts: ["RB tree properties", "Sentinel nodes", "BST operations"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Rotations",
                        description: "Implement left and right rotations.",
                        criteria: ["Left rotation around a node", "Right rotation around a node", "Maintain parent pointers", "Handle root rotation"],
                        hints: {
                            level1: "Rotation preserves BST property. Child becomes parent, parent becomes child.",
                            level2: "Update three parent pointers: rotated node, its new parent, subtree moved.",
                            level3: "def left_rotate(self, x):\n    '''Rotate x down to the left'''\n    y = x.right\n    x.right = y.left\n    \n    if y.left != self.NIL:\n        y.left.parent = x\n    \n    y.parent = x.parent\n    \n    if x.parent == self.NIL:\n        self.root = y\n    elif x == x.parent.left:\n        x.parent.left = y\n    else:\n        x.parent.right = y\n    \n    y.left = x\n    x.parent = y\n\ndef right_rotate(self, y):\n    '''Rotate y down to the right'''\n    x = y.left\n    y.left = x.right\n    \n    if x.right != self.NIL:\n        x.right.parent = y\n    \n    x.parent = y.parent\n    \n    if y.parent == self.NIL:\n        self.root = x\n    elif y == y.parent.right:\n        y.parent.right = x\n    else:\n        y.parent.left = x\n    \n    x.right = y\n    y.parent = x"
                        },
                        pitfalls: ["Wrong direction", "Missing parent updates", "Root special case"],
                        concepts: ["Tree rotations", "Pointer manipulation", "BST preservation"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Insertion",
                        description: "Insert nodes while maintaining RB properties.",
                        criteria: ["BST insert (new node is red)", "Fix violations (red-red)", "Handle uncle cases", "Recolor and rotate as needed"],
                        hints: {
                            level1: "After insert: if parent red, fix. Cases depend on uncle's color.",
                            level2: "Red uncle: recolor. Black uncle: rotate and recolor.",
                            level3: "def insert(self, key, value=None):\n    node = Node(key, value)\n    node.left = self.NIL\n    node.right = self.NIL\n    \n    # BST insert\n    parent = self.NIL\n    current = self.root\n    while current != self.NIL:\n        parent = current\n        if key < current.key:\n            current = current.left\n        else:\n            current = current.right\n    \n    node.parent = parent\n    if parent == self.NIL:\n        self.root = node\n    elif key < parent.key:\n        parent.left = node\n    else:\n        parent.right = node\n    \n    self._insert_fixup(node)\n\ndef _insert_fixup(self, z):\n    while z.parent.color == Color.RED:\n        if z.parent == z.parent.parent.left:\n            uncle = z.parent.parent.right\n            if uncle.color == Color.RED:\n                # Case 1: Uncle is red - recolor\n                z.parent.color = Color.BLACK\n                uncle.color = Color.BLACK\n                z.parent.parent.color = Color.RED\n                z = z.parent.parent\n            else:\n                if z == z.parent.right:\n                    # Case 2: Uncle black, z is right child\n                    z = z.parent\n                    self.left_rotate(z)\n                # Case 3: Uncle black, z is left child\n                z.parent.color = Color.BLACK\n                z.parent.parent.color = Color.RED\n                self.right_rotate(z.parent.parent)\n        else:\n            # Mirror cases for right side\n            uncle = z.parent.parent.left\n            # ... (symmetric)\n    \n    self.root.color = Color.BLACK"
                        },
                        pitfalls: ["Symmetric cases", "Root color", "Grandparent access"],
                        concepts: ["RB insert fixup", "Case analysis", "Recoloring"],
                        estimatedHours: "5-7"
                    },
                    {
                        id: 4,
                        name: "Deletion",
                        description: "Delete nodes while maintaining RB properties.",
                        criteria: ["BST delete (find successor if needed)", "Handle double-black case", "Fix violations after delete", "Transplant helper function"],
                        hints: {
                            level1: "Deletion is complex. Track the 'extra black' that moves up.",
                            level2: "Fixup cases depend on sibling and its children colors.",
                            level3: "def _transplant(self, u, v):\n    '''Replace subtree rooted at u with subtree rooted at v'''\n    if u.parent == self.NIL:\n        self.root = v\n    elif u == u.parent.left:\n        u.parent.left = v\n    else:\n        u.parent.right = v\n    v.parent = u.parent\n\ndef delete(self, key):\n    z = self.search(key)\n    if z is None:\n        return\n    \n    y = z\n    y_original_color = y.color\n    \n    if z.left == self.NIL:\n        x = z.right\n        self._transplant(z, z.right)\n    elif z.right == self.NIL:\n        x = z.left\n        self._transplant(z, z.left)\n    else:\n        y = self.minimum(z.right)  # Successor\n        y_original_color = y.color\n        x = y.right\n        if y.parent == z:\n            x.parent = y\n        else:\n            self._transplant(y, y.right)\n            y.right = z.right\n            y.right.parent = y\n        self._transplant(z, y)\n        y.left = z.left\n        y.left.parent = y\n        y.color = z.color\n    \n    if y_original_color == Color.BLACK:\n        self._delete_fixup(x)\n\ndef _delete_fixup(self, x):\n    while x != self.root and x.color == Color.BLACK:\n        if x == x.parent.left:\n            sibling = x.parent.right\n            # Case 1: Sibling is red\n            if sibling.color == Color.RED:\n                sibling.color = Color.BLACK\n                x.parent.color = Color.RED\n                self.left_rotate(x.parent)\n                sibling = x.parent.right\n            # Case 2: Sibling black, both children black\n            if sibling.left.color == Color.BLACK and sibling.right.color == Color.BLACK:\n                sibling.color = Color.RED\n                x = x.parent\n            else:\n                # Case 3: Sibling black, left child red\n                if sibling.right.color == Color.BLACK:\n                    sibling.left.color = Color.BLACK\n                    sibling.color = Color.RED\n                    self.right_rotate(sibling)\n                    sibling = x.parent.right\n                # Case 4: Sibling black, right child red\n                sibling.color = x.parent.color\n                x.parent.color = Color.BLACK\n                sibling.right.color = Color.BLACK\n                self.left_rotate(x.parent)\n                x = self.root\n        else:\n            # Mirror cases\n            pass\n    x.color = Color.BLACK"
                        },
                        pitfalls: ["Four delete cases", "Double-black propagation", "NIL parent handling"],
                        concepts: ["RB delete", "Transplant", "Case analysis"],
                        estimatedHours: "6-8"
                    }
                ]
            },

            // SECURITY - ADVANCED
            "https-client": {
                name: "HTTPS Client",
                description: "Build an HTTPS client that performs TLS handshake. Learn certificate validation and encrypted communication.",
                difficulty: "advanced",
                estimatedHours: "20-35",
                prerequisites: ["TCP sockets", "Cryptography basics", "X.509 certificates"],
                languages: { recommended: ["Python", "Go", "Rust"], also: ["C", "Java"] },
                resources: [
                    { name: "TLS 1.3 RFC 8446", url: "https://datatracker.ietf.org/doc/html/rfc8446", type: "specification" },
                    { name: "Illustrated TLS 1.3", url: "https://tls13.xargs.org/", type: "tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "TCP Socket & Record Layer",
                        description: "Establish TCP connection and implement TLS record layer.",
                        criteria: ["TCP connect to port 443", "TLS record format (type, version, length, data)", "Fragment handling", "Record layer parsing"],
                        hints: {
                            level1: "TLS record: 1 byte type, 2 bytes version, 2 bytes length, then payload.",
                            level2: "Content types: 20=ChangeCipherSpec, 21=Alert, 22=Handshake, 23=Application.",
                            level3: "import socket\nimport struct\n\nclass TLSRecord:\n    CHANGE_CIPHER_SPEC = 20\n    ALERT = 21\n    HANDSHAKE = 22\n    APPLICATION_DATA = 23\n\nclass TLSConnection:\n    def __init__(self, host, port=443):\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.connect((host, port))\n        self.host = host\n    \n    def send_record(self, content_type, data):\n        # TLS 1.2 version for compatibility (actual version in handshake)\n        version = (3, 3)\n        header = struct.pack('!BHH', content_type, (version[0] << 8) | version[1], len(data))\n        self.sock.sendall(header + data)\n    \n    def recv_record(self):\n        header = self.sock.recv(5)\n        if len(header) < 5:\n            raise ConnectionError('Connection closed')\n        \n        content_type, version, length = struct.unpack('!BHH', header)\n        data = b''\n        while len(data) < length:\n            chunk = self.sock.recv(length - len(data))\n            if not chunk:\n                raise ConnectionError('Connection closed')\n            data += chunk\n        \n        return content_type, data"
                        },
                        pitfalls: ["Endianness", "Partial reads", "Record fragmentation"],
                        concepts: ["TLS records", "Network protocols", "Binary parsing"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "ClientHello",
                        description: "Send ClientHello with supported cipher suites.",
                        criteria: ["Random bytes generation", "Session ID", "Cipher suite list", "SNI extension for hostname"],
                        hints: {
                            level1: "ClientHello starts handshake. Include TLS version, random, extensions.",
                            level2: "SNI (Server Name Indication) tells server which hostname you want.",
                            level3: "import os\n\ndef build_client_hello(hostname):\n    # Random (32 bytes)\n    client_random = os.urandom(32)\n    \n    # Session ID (empty for new connection)\n    session_id = b''\n    \n    # Cipher suites (TLS 1.3)\n    cipher_suites = struct.pack('!H', 2)  # Length\n    cipher_suites += struct.pack('!H', 0x1301)  # TLS_AES_128_GCM_SHA256\n    \n    # Compression methods (null only)\n    compression = struct.pack('!B', 1) + struct.pack('!B', 0)\n    \n    # Extensions\n    extensions = build_sni_extension(hostname)\n    extensions += build_supported_versions_extension()\n    extensions += build_key_share_extension()\n    \n    # Build handshake message\n    body = struct.pack('!H', 0x0303)  # Legacy version TLS 1.2\n    body += client_random\n    body += struct.pack('!B', len(session_id)) + session_id\n    body += cipher_suites\n    body += compression\n    body += struct.pack('!H', len(extensions)) + extensions\n    \n    # Handshake header: type (1) + length (3)\n    handshake = struct.pack('!B', 1)  # ClientHello\n    handshake += struct.pack('!I', len(body))[1:]  # 3-byte length\n    handshake += body\n    \n    return handshake, client_random\n\ndef build_sni_extension(hostname):\n    hostname_bytes = hostname.encode()\n    # SNI list entry\n    entry = struct.pack('!B', 0)  # DNS hostname type\n    entry += struct.pack('!H', len(hostname_bytes))\n    entry += hostname_bytes\n    # SNI list\n    sni_list = struct.pack('!H', len(entry)) + entry\n    # Extension\n    ext = struct.pack('!H', 0)  # SNI extension type\n    ext += struct.pack('!H', len(sni_list)) + sni_list\n    return ext"
                        },
                        pitfalls: ["Extension ordering", "Length field sizes", "Version negotiation"],
                        concepts: ["TLS handshake", "Cipher negotiation", "Extensions"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 3,
                        name: "Key Exchange",
                        description: "Perform ECDHE key exchange and derive keys.",
                        criteria: ["Generate ephemeral key pair", "Parse ServerHello and key share", "ECDH shared secret computation", "Key derivation (HKDF)"],
                        hints: {
                            level1: "ECDHE: both sides generate key pairs, share public keys, compute shared secret.",
                            level2: "TLS 1.3 uses HKDF for key derivation from shared secret.",
                            level3: "from cryptography.hazmat.primitives.asymmetric import x25519\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.hkdf import HKDFExpand, HKDF\nimport hashlib\n\nclass KeyExchange:\n    def __init__(self):\n        self.private_key = x25519.X25519PrivateKey.generate()\n        self.public_key = self.private_key.public_key()\n    \n    def get_public_bytes(self):\n        return self.public_key.public_bytes_raw()\n    \n    def compute_shared_secret(self, peer_public_bytes):\n        peer_public = x25519.X25519PublicKey.from_public_bytes(peer_public_bytes)\n        return self.private_key.exchange(peer_public)\n\ndef derive_keys(shared_secret, client_hello_hash, server_hello_hash):\n    # Early secret\n    early_secret = hkdf_extract(b'\\x00' * 32, b'\\x00' * 32)\n    \n    # Handshake secret\n    derived = hkdf_expand_label(early_secret, b'derived', b'', 32)\n    handshake_secret = hkdf_extract(derived, shared_secret)\n    \n    # Traffic secrets\n    messages_hash = hashlib.sha256(client_hello_hash + server_hello_hash).digest()\n    \n    client_handshake_traffic_secret = hkdf_expand_label(\n        handshake_secret, b'c hs traffic', messages_hash, 32\n    )\n    server_handshake_traffic_secret = hkdf_expand_label(\n        handshake_secret, b's hs traffic', messages_hash, 32\n    )\n    \n    # Derive keys and IVs\n    client_key = hkdf_expand_label(client_handshake_traffic_secret, b'key', b'', 16)\n    client_iv = hkdf_expand_label(client_handshake_traffic_secret, b'iv', b'', 12)\n    \n    return client_key, client_iv"
                        },
                        pitfalls: ["Curve negotiation", "Key derivation order", "Hash transcript"],
                        concepts: ["ECDHE", "Key derivation", "Forward secrecy"],
                        estimatedHours: "6-8"
                    },
                    {
                        id: 4,
                        name: "Encrypted Communication",
                        description: "Encrypt/decrypt application data with derived keys.",
                        criteria: ["AEAD encryption (AES-GCM)", "Sequence number handling", "Finish message verification", "Send/receive application data"],
                        hints: {
                            level1: "TLS 1.3 uses AEAD. Nonce = IV XOR sequence number.",
                            level2: "Record header is additional authenticated data (AAD).",
                            level3: "from cryptography.hazmat.primitives.ciphers.aead import AESGCM\n\nclass TLS13Cipher:\n    def __init__(self, key, iv):\n        self.aesgcm = AESGCM(key)\n        self.iv = iv\n        self.seq_num = 0\n    \n    def _compute_nonce(self):\n        # XOR IV with sequence number (padded to IV length)\n        seq_bytes = self.seq_num.to_bytes(len(self.iv), 'big')\n        nonce = bytes(a ^ b for a, b in zip(self.iv, seq_bytes))\n        self.seq_num += 1\n        return nonce\n    \n    def encrypt(self, plaintext, content_type):\n        # Add content type byte (inner plaintext)\n        inner = plaintext + bytes([content_type])\n        \n        # Build AAD (record header with encrypted content type)\n        aad = struct.pack('!BHH', 23, 0x0303, len(inner) + 16)  # +16 for auth tag\n        \n        nonce = self._compute_nonce()\n        ciphertext = self.aesgcm.encrypt(nonce, inner, aad)\n        \n        return aad + ciphertext\n    \n    def decrypt(self, record_data):\n        # AAD is the record header\n        aad = record_data[:5]\n        ciphertext = record_data[5:]\n        \n        nonce = self._compute_nonce()\n        plaintext = self.aesgcm.decrypt(nonce, ciphertext, aad)\n        \n        # Last byte is real content type\n        content_type = plaintext[-1]\n        data = plaintext[:-1].rstrip(b'\\x00')  # Remove padding\n        \n        return content_type, data"
                        },
                        pitfalls: ["Nonce reuse", "Padding removal", "Alert handling"],
                        concepts: ["AEAD encryption", "Sequence numbers", "TLS records"],
                        estimatedHours: "6-8"
                    }
                ]
            },

            // DATA STORAGE - ADVANCED
            "wal-impl": {
                name: "WAL Implementation",
                description: "Implement Write-Ahead Logging for durability. Learn crash recovery and log-structured storage.",
                difficulty: "advanced",
                estimatedHours: "15-25",
                prerequisites: ["File I/O", "Database basics", "Crash recovery concepts"],
                languages: { recommended: ["Rust", "Go", "C"], also: ["Python", "Java"] },
                resources: [
                    { name: "ARIES Paper", url: "https://cs.stanford.edu/people/chr101/cs345/aries.pdf", type: "paper" },
                    { name: "SQLite WAL", url: "https://sqlite.org/wal.html", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Log Record Format",
                        description: "Design and implement log record structure.",
                        criteria: ["LSN (Log Sequence Number)", "Transaction ID", "Operation type (INSERT, UPDATE, DELETE)", "Before/after images for undo/redo"],
                        hints: {
                            level1: "Log record = LSN + TxID + Type + Data. Append-only file.",
                            level2: "Include enough info to redo OR undo the operation.",
                            level3: "import struct\nfrom enum import IntEnum\n\nclass LogType(IntEnum):\n    BEGIN = 1\n    COMMIT = 2\n    ABORT = 3\n    INSERT = 4\n    UPDATE = 5\n    DELETE = 6\n    CHECKPOINT = 7\n\nclass LogRecord:\n    def __init__(self, lsn, tx_id, log_type, table=None, key=None, \n                 before_value=None, after_value=None):\n        self.lsn = lsn\n        self.tx_id = tx_id\n        self.log_type = log_type\n        self.table = table\n        self.key = key\n        self.before_value = before_value  # For undo\n        self.after_value = after_value    # For redo\n    \n    def serialize(self):\n        # Format: LSN(8) + TxID(4) + Type(1) + DataLen(4) + Data\n        data = b''\n        if self.table:\n            data += self.table.encode() + b'\\x00'\n        if self.key is not None:\n            data += struct.pack('!I', self.key)\n        if self.before_value is not None:\n            before_bytes = self.before_value.encode() if isinstance(self.before_value, str) else self.before_value\n            data += struct.pack('!I', len(before_bytes)) + before_bytes\n        if self.after_value is not None:\n            after_bytes = self.after_value.encode() if isinstance(self.after_value, str) else self.after_value\n            data += struct.pack('!I', len(after_bytes)) + after_bytes\n        \n        header = struct.pack('!QIB I', self.lsn, self.tx_id, self.log_type, len(data))\n        return header + data"
                        },
                        pitfalls: ["Variable-length fields", "Endianness", "Corruption detection"],
                        concepts: ["Log records", "LSN", "Undo/redo logging"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Log Writer",
                        description: "Implement append-only log file with fsync.",
                        criteria: ["Append records atomically", "Force log to disk (fsync)", "Handle concurrent writers", "Log file rotation"],
                        hints: {
                            level1: "Append to log file, fsync before acknowledging commit.",
                            level2: "Group commit: batch multiple commits into one fsync.",
                            level3: "import os\nimport threading\n\nclass WALWriter:\n    def __init__(self, log_path):\n        self.log_path = log_path\n        self.log_file = open(log_path, 'ab')\n        self.lock = threading.Lock()\n        self.current_lsn = self._recover_lsn()\n        self.buffer = bytearray()\n        self.pending_commits = []\n    \n    def _recover_lsn(self):\n        # Read last LSN from log file\n        try:\n            size = os.path.getsize(self.log_path)\n            if size == 0:\n                return 0\n            # Read last record's LSN\n            # ... implementation ...\n        except FileNotFoundError:\n            return 0\n    \n    def append(self, record):\n        with self.lock:\n            record.lsn = self.current_lsn\n            self.current_lsn += 1\n            \n            data = record.serialize()\n            self.buffer.extend(data)\n            \n            if record.log_type == LogType.COMMIT:\n                self.pending_commits.append(record.tx_id)\n            \n            return record.lsn\n    \n    def flush(self):\n        '''Force buffered records to disk'''\n        with self.lock:\n            if self.buffer:\n                self.log_file.write(bytes(self.buffer))\n                self.log_file.flush()\n                os.fsync(self.log_file.fileno())\n                self.buffer.clear()\n                committed = self.pending_commits.copy()\n                self.pending_commits.clear()\n                return committed\n            return []"
                        },
                        pitfalls: ["Partial writes", "fsync semantics", "Torn pages"],
                        concepts: ["Durability", "fsync", "Group commit"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 3,
                        name: "Crash Recovery",
                        description: "Implement ARIES-style recovery (redo then undo).",
                        criteria: ["Scan log to find transactions", "Redo all committed changes", "Undo incomplete transactions", "Track active transactions"],
                        hints: {
                            level1: "Recovery: 1) Analysis - find active txs, 2) Redo - replay, 3) Undo - rollback.",
                            level2: "Redo: apply all operations. Undo: reverse uncommitted ops.",
                            level3: "class WALRecovery:\n    def recover(self, wal_reader, database):\n        # Phase 1: Analysis\n        active_txs = set()\n        committed_txs = set()\n        \n        for record in wal_reader.scan():\n            if record.log_type == LogType.BEGIN:\n                active_txs.add(record.tx_id)\n            elif record.log_type == LogType.COMMIT:\n                active_txs.discard(record.tx_id)\n                committed_txs.add(record.tx_id)\n            elif record.log_type == LogType.ABORT:\n                active_txs.discard(record.tx_id)\n        \n        # Phase 2: Redo (all operations from committed transactions)\n        for record in wal_reader.scan():\n            if record.tx_id in committed_txs:\n                if record.log_type == LogType.INSERT:\n                    database.insert(record.table, record.key, record.after_value)\n                elif record.log_type == LogType.UPDATE:\n                    database.update(record.table, record.key, record.after_value)\n                elif record.log_type == LogType.DELETE:\n                    database.delete(record.table, record.key)\n        \n        # Phase 3: Undo (reverse uncommitted transactions)\n        # Scan backwards\n        for record in wal_reader.scan_reverse():\n            if record.tx_id in active_txs:\n                if record.log_type == LogType.INSERT:\n                    database.delete(record.table, record.key)\n                elif record.log_type == LogType.UPDATE:\n                    database.update(record.table, record.key, record.before_value)\n                elif record.log_type == LogType.DELETE:\n                    database.insert(record.table, record.key, record.before_value)\n        \n        # Log abort records for incomplete transactions\n        for tx_id in active_txs:\n            wal_writer.append(LogRecord(0, tx_id, LogType.ABORT))"
                        },
                        pitfalls: ["Idempotent operations", "Log corruption", "Partial recovery"],
                        concepts: ["ARIES recovery", "Redo/undo", "Crash consistency"],
                        estimatedHours: "5-7"
                    },
                    {
                        id: 4,
                        name: "Checkpointing",
                        description: "Implement checkpoints to speed up recovery.",
                        criteria: ["Fuzzy checkpoints (non-blocking)", "Record dirty pages/active txs", "Truncate old log entries", "Restart from checkpoint"],
                        hints: {
                            level1: "Checkpoint records state so recovery starts from there.",
                            level2: "Fuzzy checkpoint: don't block, just record dirty pages.",
                            level3: "class Checkpoint:\n    def __init__(self, lsn, active_txs, dirty_pages):\n        self.lsn = lsn\n        self.active_txs = active_txs  # {tx_id: first_lsn}\n        self.dirty_pages = dirty_pages  # {page_id: recovery_lsn}\n\nclass WALManager:\n    def create_checkpoint(self):\n        with self.lock:\n            # Get current state\n            active_txs = dict(self.transaction_table)\n            dirty_pages = dict(self.dirty_page_table)\n            \n            # Write checkpoint begin\n            begin_record = LogRecord(0, 0, LogType.CHECKPOINT_BEGIN)\n            self.wal_writer.append(begin_record)\n            \n            # Flush dirty pages in background\n            # (fuzzy - don't wait)\n            \n            # Write checkpoint end with state\n            checkpoint = Checkpoint(\n                begin_record.lsn,\n                active_txs,\n                dirty_pages\n            )\n            end_record = LogRecord(0, 0, LogType.CHECKPOINT_END,\n                                    data=checkpoint.serialize())\n            self.wal_writer.append(end_record)\n            self.wal_writer.flush()\n            \n            # Update master record\n            self._write_master_record(begin_record.lsn)\n    \n    def recover_from_checkpoint(self):\n        # Read master record to find checkpoint\n        checkpoint_lsn = self._read_master_record()\n        \n        if checkpoint_lsn:\n            # Start analysis from checkpoint\n            checkpoint = self._read_checkpoint(checkpoint_lsn)\n            self.transaction_table = checkpoint.active_txs\n            self.dirty_page_table = checkpoint.dirty_pages\n            start_lsn = checkpoint_lsn\n        else:\n            start_lsn = 0\n        \n        # Continue with normal recovery from start_lsn\n        self._redo_phase(start_lsn)\n        self._undo_phase()"
                        },
                        pitfalls: ["Checkpoint consistency", "Log truncation timing", "Master record"],
                        concepts: ["Checkpointing", "Fuzzy checkpoints", "Log truncation"],
                        estimatedHours: "4-5"
                    }
                ]
            },

            // COMPILERS - ADVANCED
            "simple-gc": {
                name: "Simple Garbage Collector",
                description: "Implement a mark-sweep garbage collector. Learn memory management and object traversal.",
                difficulty: "advanced",
                estimatedHours: "15-25",
                prerequisites: ["Memory management basics", "Graph traversal", "C pointers"],
                languages: { recommended: ["C", "Rust", "Go"], also: ["Python (educational)"] },
                resources: [
                    { name: "The Garbage Collection Handbook", url: "https://gchandbook.org/", type: "book" },
                    { name: "Baby's First GC", url: "https://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Object Model",
                        description: "Define object representation with GC metadata.",
                        criteria: ["Object header with type info", "Mark bit for GC", "Object size tracking", "Support different object types"],
                        hints: {
                            level1: "Every object needs header: marked flag, type, size.",
                            level2: "Use a common header struct that precedes object data.",
                            level3: "typedef enum {\n    OBJ_INT,\n    OBJ_PAIR,\n    OBJ_STRING,\n} ObjectType;\n\ntypedef struct Object {\n    ObjectType type;\n    unsigned char marked;\n    struct Object* next;  // For allocation list\n    \n    union {\n        int int_value;\n        struct {\n            struct Object* head;\n            struct Object* tail;\n        } pair;\n        struct {\n            char* chars;\n            int length;\n        } string;\n    } as;\n} Object;\n\ntypedef struct {\n    Object* first_object;  // Head of all allocated objects\n    Object** stack;        // Root stack\n    int stack_size;\n    int stack_capacity;\n    int num_objects;\n    int max_objects;       // Threshold for GC\n} VM;\n\nObject* new_object(VM* vm, ObjectType type) {\n    if (vm->num_objects >= vm->max_objects) {\n        gc(vm);\n    }\n    \n    Object* obj = malloc(sizeof(Object));\n    obj->type = type;\n    obj->marked = 0;\n    obj->next = vm->first_object;\n    vm->first_object = obj;\n    vm->num_objects++;\n    \n    return obj;\n}"
                        },
                        pitfalls: ["Header alignment", "Type safety", "Object size calculation"],
                        concepts: ["Object headers", "Type tags", "Allocation lists"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Root Discovery",
                        description: "Identify and traverse GC roots.",
                        criteria: ["Track stack roots", "Global variable roots", "Register roots (if applicable)", "Precise vs conservative roots"],
                        hints: {
                            level1: "Roots = live references not reachable from heap. Stack, globals.",
                            level2: "Maintain explicit stack of roots for simplicity.",
                            level3: "void push_root(VM* vm, Object* obj) {\n    if (vm->stack_size >= vm->stack_capacity) {\n        vm->stack_capacity *= 2;\n        vm->stack = realloc(vm->stack, sizeof(Object*) * vm->stack_capacity);\n    }\n    vm->stack[vm->stack_size++] = obj;\n}\n\nObject* pop_root(VM* vm) {\n    return vm->stack[--vm->stack_size];\n}\n\nvoid mark_roots(VM* vm) {\n    // Mark all objects on the stack\n    for (int i = 0; i < vm->stack_size; i++) {\n        mark_object(vm->stack[i]);\n    }\n    \n    // Mark globals (if any)\n    // mark_object(vm->globals);\n}\n\n// For conservative GC (scan memory for pointers)\nvoid scan_stack_conservative(VM* vm, void* stack_top, void* stack_bottom) {\n    for (void** p = stack_top; p < stack_bottom; p++) {\n        void* potential_ptr = *p;\n        // Check if this looks like a valid object pointer\n        Object* obj = find_object_at_address(vm, potential_ptr);\n        if (obj) {\n            mark_object(obj);\n        }\n    }\n}"
                        },
                        pitfalls: ["Missing roots", "Stack scanning direction", "Interior pointers"],
                        concepts: ["Root set", "Precise vs conservative", "Stack scanning"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Mark Phase",
                        description: "Traverse and mark all reachable objects.",
                        criteria: ["Recursive marking", "Handle cycles", "Mark bit manipulation", "Worklist-based marking (optional)"],
                        hints: {
                            level1: "Mark: start from roots, follow pointers, set mark bit.",
                            level2: "Use recursion or explicit worklist to avoid stack overflow.",
                            level3: "void mark_object(Object* obj) {\n    if (obj == NULL) return;\n    if (obj->marked) return;  // Already visited\n    \n    obj->marked = 1;\n    \n    // Recursively mark referenced objects\n    switch (obj->type) {\n        case OBJ_INT:\n            // No references\n            break;\n        case OBJ_PAIR:\n            mark_object(obj->as.pair.head);\n            mark_object(obj->as.pair.tail);\n            break;\n        case OBJ_STRING:\n            // No object references (just char*)\n            break;\n    }\n}\n\n// Worklist version (avoids deep recursion)\nvoid mark_object_worklist(VM* vm, Object* obj) {\n    if (obj == NULL || obj->marked) return;\n    \n    Object** worklist = malloc(sizeof(Object*) * vm->num_objects);\n    int worklist_size = 0;\n    \n    worklist[worklist_size++] = obj;\n    \n    while (worklist_size > 0) {\n        Object* current = worklist[--worklist_size];\n        if (current->marked) continue;\n        current->marked = 1;\n        \n        if (current->type == OBJ_PAIR) {\n            if (current->as.pair.head && !current->as.pair.head->marked)\n                worklist[worklist_size++] = current->as.pair.head;\n            if (current->as.pair.tail && !current->as.pair.tail->marked)\n                worklist[worklist_size++] = current->as.pair.tail;\n        }\n    }\n    \n    free(worklist);\n}"
                        },
                        pitfalls: ["Stack overflow on deep structures", "Forgetting object types", "Double marking"],
                        concepts: ["Graph traversal", "Mark bits", "Tri-color marking"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Sweep Phase",
                        description: "Reclaim unmarked objects and reset marks.",
                        criteria: ["Walk allocation list", "Free unmarked objects", "Reset mark bits on live objects", "Update allocation list"],
                        hints: {
                            level1: "Sweep: walk all objects, free unmarked, clear marks on marked.",
                            level2: "Update linked list as you go - skip freed objects.",
                            level3: "void sweep(VM* vm) {\n    Object** obj_ptr = &vm->first_object;\n    \n    while (*obj_ptr != NULL) {\n        if (!(*obj_ptr)->marked) {\n            // Unreachable - free it\n            Object* unreached = *obj_ptr;\n            *obj_ptr = unreached->next;  // Unlink\n            \n            // Free any additional memory\n            if (unreached->type == OBJ_STRING) {\n                free(unreached->as.string.chars);\n            }\n            \n            free(unreached);\n            vm->num_objects--;\n        } else {\n            // Reachable - clear mark for next GC\n            (*obj_ptr)->marked = 0;\n            obj_ptr = &(*obj_ptr)->next;\n        }\n    }\n}\n\nvoid gc(VM* vm) {\n    int num_before = vm->num_objects;\n    \n    // Mark phase\n    mark_roots(vm);\n    \n    // Sweep phase\n    sweep(vm);\n    \n    // Adjust threshold\n    vm->max_objects = vm->num_objects * 2;\n    \n    printf(\"Collected %d objects, %d remaining.\\n\",\n           num_before - vm->num_objects, vm->num_objects);\n}"
                        },
                        pitfalls: ["List corruption", "Forgetting to clear marks", "Memory leaks in objects"],
                        concepts: ["Memory reclamation", "Linked list manipulation", "GC thresholds"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // SYSTEMS - ADVANCED
            "container-basic": {
                name: "Container (Basic)",
                description: "Build a simple container using Linux namespaces. Learn process isolation and cgroups basics.",
                difficulty: "advanced",
                estimatedHours: "15-25",
                prerequisites: ["Linux system calls", "Process management", "Filesystem basics"],
                languages: { recommended: ["C", "Go", "Rust"], also: ["Python"] },
                resources: [
                    { name: "Linux Namespaces", url: "https://man7.org/linux/man-pages/man7/namespaces.7.html", type: "documentation" },
                    { name: "Containers from Scratch", url: "https://ericchiang.github.io/post/containers-from-scratch/", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Process Namespace",
                        description: "Isolate process tree with PID namespace.",
                        criteria: ["Create new PID namespace", "Child sees itself as PID 1", "Parent sees real PID", "Handle zombie processes"],
                        hints: {
                            level1: "clone() with CLONE_NEWPID creates new PID namespace.",
                            level2: "First process in namespace is PID 1, must reap children.",
                            level3: "#define _GNU_SOURCE\n#include <sched.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/wait.h>\n\nstatic int child_fn(void *arg) {\n    printf(\"Child PID: %d\\n\", getpid());  // Will print 1\n    \n    // Mount proc for accurate /proc\n    mount(\"proc\", \"/proc\", \"proc\", 0, NULL);\n    \n    // Execute the command\n    char *argv[] = {\"/bin/sh\", NULL};\n    execv(argv[0], argv);\n    return 1;\n}\n\nint main() {\n    #define STACK_SIZE (1024 * 1024)\n    char *stack = malloc(STACK_SIZE);\n    \n    pid_t pid = clone(\n        child_fn,\n        stack + STACK_SIZE,  // Stack grows down\n        CLONE_NEWPID | SIGCHLD,\n        NULL\n    );\n    \n    printf(\"Parent sees child PID: %d\\n\", pid);  // Real PID\n    waitpid(pid, NULL, 0);\n    \n    return 0;\n}"
                        },
                        pitfalls: ["Stack direction", "PID 1 responsibilities", "Zombie processes"],
                        concepts: ["PID namespaces", "clone()", "Process isolation"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Mount Namespace",
                        description: "Isolate filesystem mounts.",
                        criteria: ["New mount namespace", "Private mount propagation", "Pivot root to new filesystem", "Mount essential filesystems"],
                        hints: {
                            level1: "CLONE_NEWNS isolates mounts. pivot_root changes root.",
                            level2: "Need to mount /proc, /sys, /dev inside container.",
                            level3: "void setup_mount_namespace(const char *rootfs) {\n    // Make all mounts private\n    mount(NULL, \"/\", NULL, MS_REC | MS_PRIVATE, NULL);\n    \n    // Mount the new root filesystem\n    mount(rootfs, rootfs, \"bind\", MS_BIND | MS_REC, NULL);\n    \n    // Create mount points\n    char path[256];\n    snprintf(path, sizeof(path), \"%s/proc\", rootfs);\n    mkdir(path, 0755);\n    snprintf(path, sizeof(path), \"%s/sys\", rootfs);\n    mkdir(path, 0755);\n    snprintf(path, sizeof(path), \"%s/dev\", rootfs);\n    mkdir(path, 0755);\n    \n    // Mount essential filesystems\n    snprintf(path, sizeof(path), \"%s/proc\", rootfs);\n    mount(\"proc\", path, \"proc\", 0, NULL);\n    \n    snprintf(path, sizeof(path), \"%s/sys\", rootfs);\n    mount(\"sysfs\", path, \"sysfs\", 0, NULL);\n    \n    // Pivot root\n    char old_root[256];\n    snprintf(old_root, sizeof(old_root), \"%s/.old_root\", rootfs);\n    mkdir(old_root, 0755);\n    \n    if (pivot_root(rootfs, old_root) < 0) {\n        perror(\"pivot_root\");\n        exit(1);\n    }\n    \n    chdir(\"/\");\n    \n    // Unmount old root\n    umount2(\"/.old_root\", MNT_DETACH);\n    rmdir(\"/.old_root\");\n}"
                        },
                        pitfalls: ["Mount propagation", "pivot_root requirements", "Device nodes"],
                        concepts: ["Mount namespaces", "pivot_root", "Filesystem isolation"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 3,
                        name: "Network Namespace",
                        description: "Isolate network stack.",
                        criteria: ["Create network namespace", "Set up veth pair", "Configure IP addresses", "Enable container networking"],
                        hints: {
                            level1: "CLONE_NEWNET gives isolated network stack. veth connects to host.",
                            level2: "veth pair: one end in host, one in container namespace.",
                            level3: "// Note: This typically requires root and netlink or ip commands\n\nvoid setup_network(pid_t child_pid) {\n    char cmd[256];\n    \n    // Create veth pair\n    snprintf(cmd, sizeof(cmd), \n             \"ip link add veth0 type veth peer name veth1\");\n    system(cmd);\n    \n    // Move veth1 to container namespace\n    snprintf(cmd, sizeof(cmd),\n             \"ip link set veth1 netns %d\", child_pid);\n    system(cmd);\n    \n    // Configure host side\n    system(\"ip addr add 10.0.0.1/24 dev veth0\");\n    system(\"ip link set veth0 up\");\n    \n    // Enable IP forwarding and NAT\n    system(\"echo 1 > /proc/sys/net/ipv4/ip_forward\");\n    system(\"iptables -t nat -A POSTROUTING -s 10.0.0.0/24 -j MASQUERADE\");\n}\n\nvoid setup_network_inside_container() {\n    // Run inside container namespace\n    system(\"ip addr add 10.0.0.2/24 dev veth1\");\n    system(\"ip link set veth1 up\");\n    system(\"ip link set lo up\");\n    system(\"ip route add default via 10.0.0.1\");\n}"
                        },
                        pitfalls: ["Namespace timing", "veth cleanup", "NAT configuration"],
                        concepts: ["Network namespaces", "veth pairs", "Container networking"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 4,
                        name: "Cgroups (Resource Limits)",
                        description: "Limit CPU, memory, and other resources.",
                        criteria: ["Create cgroup for container", "Set memory limit", "Set CPU quota", "Clean up cgroup on exit"],
                        hints: {
                            level1: "cgroups v2: create directory in /sys/fs/cgroup, write limits.",
                            level2: "Memory limit: memory.max. CPU: cpu.max (quota period).",
                            level3: "void setup_cgroups(pid_t pid, int memory_limit_mb, int cpu_percent) {\n    char path[256];\n    char value[64];\n    \n    // Create cgroup directory (cgroups v2)\n    snprintf(path, sizeof(path), \"/sys/fs/cgroup/container_%d\", pid);\n    mkdir(path, 0755);\n    \n    // Set memory limit\n    snprintf(path, sizeof(path), \n             \"/sys/fs/cgroup/container_%d/memory.max\", pid);\n    snprintf(value, sizeof(value), \"%d\", memory_limit_mb * 1024 * 1024);\n    write_file(path, value);\n    \n    // Set CPU quota (e.g., 50% = 50000 100000)\n    snprintf(path, sizeof(path),\n             \"/sys/fs/cgroup/container_%d/cpu.max\", pid);\n    snprintf(value, sizeof(value), \"%d 100000\", cpu_percent * 1000);\n    write_file(path, value);\n    \n    // Add process to cgroup\n    snprintf(path, sizeof(path),\n             \"/sys/fs/cgroup/container_%d/cgroup.procs\", pid);\n    snprintf(value, sizeof(value), \"%d\", pid);\n    write_file(path, value);\n}\n\nvoid cleanup_cgroups(pid_t pid) {\n    char path[256];\n    snprintf(path, sizeof(path), \"/sys/fs/cgroup/container_%d\", pid);\n    rmdir(path);  // Only works if empty (process exited)\n}"
                        },
                        pitfalls: ["cgroups v1 vs v2", "Controller availability", "Cleanup order"],
                        concepts: ["cgroups", "Resource limits", "Container resources"],
                        estimatedHours: "4-5"
                    }
                ]
            },

            // DISTRIBUTED - BEGINNER
            "rpc-basic": {
                name: "RPC Framework (Basic)",
                description: "Build a simple Remote Procedure Call framework. Learn serialization, network protocols, and client-server patterns.",
                difficulty: "beginner",
                estimatedHours: "10-15",
                prerequisites: ["TCP sockets", "JSON/serialization", "Client-server architecture"],
                languages: { recommended: ["Python", "Go", "Java"], also: ["JavaScript", "Rust"] },
                resources: [
                    { name: "gRPC Concepts", url: "https://grpc.io/docs/what-is-grpc/core-concepts/", type: "documentation" },
                    { name: "JSON-RPC Spec", url: "https://www.jsonrpc.org/specification", type: "specification" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Message Protocol",
                        description: "Define request/response message format.",
                        criteria: ["Request: method name, parameters, request ID", "Response: result or error, request ID", "JSON serialization", "Handle different parameter types"],
                        hints: {
                            level1: "Request = {method, params, id}. Response = {result, error, id}.",
                            level2: "Use JSON for simplicity. Match request ID in response.",
                            level3: "import json\nfrom dataclasses import dataclass, asdict\nfrom typing import Any, Optional, List\n\n@dataclass\nclass RPCRequest:\n    method: str\n    params: List[Any]\n    id: int\n    \n    def to_json(self) -> str:\n        return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls, data: str) -> 'RPCRequest':\n        d = json.loads(data)\n        return cls(d['method'], d['params'], d['id'])\n\n@dataclass\nclass RPCResponse:\n    id: int\n    result: Any = None\n    error: Optional[str] = None\n    \n    def to_json(self) -> str:\n        return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls, data: str) -> 'RPCResponse':\n        d = json.loads(data)\n        return cls(d['id'], d.get('result'), d.get('error'))"
                        },
                        pitfalls: ["ID mismatch", "Serialization errors", "Missing error handling"],
                        concepts: ["RPC protocol", "Message serialization", "Request-response pattern"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Server Implementation",
                        description: "Build RPC server that handles method calls.",
                        criteria: ["Register callable methods", "Parse incoming requests", "Execute method and return result", "Handle errors gracefully"],
                        hints: {
                            level1: "Server maintains registry of method_name -> function.",
                            level2: "Receive request, lookup method, call with params, send response.",
                            level3: "import socket\nimport threading\n\nclass RPCServer:\n    def __init__(self, host='localhost', port=8000):\n        self.host = host\n        self.port = port\n        self.methods = {}\n    \n    def register(self, name: str, func):\n        self.methods[name] = func\n    \n    def handle_client(self, conn, addr):\n        try:\n            while True:\n                data = conn.recv(4096)\n                if not data:\n                    break\n                \n                request = RPCRequest.from_json(data.decode())\n                response = self.execute(request)\n                conn.sendall(response.to_json().encode())\n        finally:\n            conn.close()\n    \n    def execute(self, request: RPCRequest) -> RPCResponse:\n        try:\n            if request.method not in self.methods:\n                return RPCResponse(request.id, error=f'Method not found: {request.method}')\n            \n            func = self.methods[request.method]\n            result = func(*request.params)\n            return RPCResponse(request.id, result=result)\n        except Exception as e:\n            return RPCResponse(request.id, error=str(e))\n    \n    def serve(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        sock.bind((self.host, self.port))\n        sock.listen(5)\n        print(f'RPC Server listening on {self.host}:{self.port}')\n        \n        while True:\n            conn, addr = sock.accept()\n            thread = threading.Thread(target=self.handle_client, args=(conn, addr))\n            thread.start()"
                        },
                        pitfalls: ["Method not found", "Parameter count mismatch", "Blocking calls"],
                        concepts: ["Method registry", "Request dispatching", "Error handling"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Client Implementation",
                        description: "Build RPC client with method proxying.",
                        criteria: ["Connect to server", "Send requests and wait for response", "Proxy object for natural method calls", "Handle timeouts"],
                        hints: {
                            level1: "Client sends request, blocks until response with matching ID.",
                            level2: "Use __getattr__ to proxy method calls dynamically.",
                            level3: "class RPCClient:\n    def __init__(self, host='localhost', port=8000):\n        self.host = host\n        self.port = port\n        self.sock = None\n        self.request_id = 0\n    \n    def connect(self):\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.connect((self.host, self.port))\n    \n    def close(self):\n        if self.sock:\n            self.sock.close()\n    \n    def call(self, method: str, *params, timeout=30):\n        self.request_id += 1\n        request = RPCRequest(method, list(params), self.request_id)\n        \n        self.sock.settimeout(timeout)\n        self.sock.sendall(request.to_json().encode())\n        \n        data = self.sock.recv(4096)\n        response = RPCResponse.from_json(data.decode())\n        \n        if response.id != self.request_id:\n            raise Exception('Response ID mismatch')\n        \n        if response.error:\n            raise Exception(response.error)\n        \n        return response.result\n    \n    def __getattr__(self, name):\n        '''Proxy method calls: client.add(1, 2) -> client.call('add', 1, 2)'''\n        def method(*args):\n            return self.call(name, *args)\n        return method\n\n# Usage:\nclient = RPCClient()\nclient.connect()\nresult = client.add(1, 2)  # Calls remote 'add' method\nprint(result)  # 3"
                        },
                        pitfalls: ["Connection management", "Timeout handling", "ID tracking"],
                        concepts: ["RPC client", "Method proxying", "Connection pooling"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // DISTRIBUTED - INTERMEDIATE
            "leader-election": {
                name: "Leader Election",
                description: "Implement leader election algorithms. Learn distributed coordination and failure handling.",
                difficulty: "intermediate",
                estimatedHours: "12-20",
                prerequisites: ["Distributed systems basics", "Network programming", "Failure detection"],
                languages: { recommended: ["Go", "Python", "Java"], also: ["Rust", "Erlang"] },
                resources: [
                    { name: "Bully Algorithm", url: "https://en.wikipedia.org/wiki/Bully_algorithm", type: "article" },
                    { name: "Ring Election", url: "https://www.cs.colostate.edu/~cs551/CourseNotes/Synchronization/LeijdElect.html", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Node Communication",
                        description: "Set up inter-node messaging.",
                        criteria: ["Node identity (unique ID)", "Point-to-point messaging", "Broadcast to all nodes", "Handle node failures"],
                        hints: {
                            level1: "Each node has unique ID and knows other nodes' addresses.",
                            level2: "Use TCP or UDP for messaging. Handle connection failures.",
                            level3: "import socket\nimport threading\nimport json\nfrom typing import Dict, Callable\n\nclass Node:\n    def __init__(self, node_id: int, port: int, peers: Dict[int, tuple]):\n        self.node_id = node_id\n        self.port = port\n        self.peers = peers  # {node_id: (host, port)}\n        self.leader_id = None\n        self.handlers: Dict[str, Callable] = {}\n        self.running = True\n    \n    def send_to(self, target_id: int, msg_type: str, data: dict) -> bool:\n        if target_id not in self.peers:\n            return False\n        \n        host, port = self.peers[target_id]\n        message = json.dumps({'type': msg_type, 'from': self.node_id, **data})\n        \n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.settimeout(2.0)\n            sock.connect((host, port))\n            sock.sendall(message.encode())\n            sock.close()\n            return True\n        except:\n            return False  # Node unreachable\n    \n    def broadcast(self, msg_type: str, data: dict):\n        for peer_id in self.peers:\n            self.send_to(peer_id, msg_type, data)\n    \n    def start_server(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        sock.bind(('0.0.0.0', self.port))\n        sock.listen(10)\n        \n        while self.running:\n            conn, addr = sock.accept()\n            data = conn.recv(4096).decode()\n            msg = json.loads(data)\n            \n            if msg['type'] in self.handlers:\n                self.handlers[msg['type']](msg)\n            conn.close()"
                        },
                        pitfalls: ["Message ordering", "Partial failures", "Network partitions"],
                        concepts: ["Node identity", "Message passing", "Failure detection"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Bully Algorithm",
                        description: "Implement the bully election algorithm.",
                        criteria: ["Higher ID wins election", "ELECTION message to higher nodes", "OK response stops election", "COORDINATOR announcement"],
                        hints: {
                            level1: "On timeout/failure: send ELECTION to higher IDs. If no OK, become leader.",
                            level2: "When receiving ELECTION from lower ID, send OK and start own election.",
                            level3: "class BullyElection(Node):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.election_in_progress = False\n        self.handlers['ELECTION'] = self.on_election\n        self.handlers['OK'] = self.on_ok\n        self.handlers['COORDINATOR'] = self.on_coordinator\n    \n    def start_election(self):\n        self.election_in_progress = True\n        higher_nodes = [nid for nid in self.peers if nid > self.node_id]\n        \n        if not higher_nodes:\n            # I'm the highest, become leader\n            self.become_leader()\n            return\n        \n        # Send ELECTION to all higher nodes\n        got_ok = False\n        for nid in higher_nodes:\n            if self.send_to(nid, 'ELECTION', {}):\n                got_ok = True\n        \n        if not got_ok:\n            # No higher node responded, become leader\n            self.become_leader()\n        else:\n            # Wait for COORDINATOR or timeout\n            threading.Timer(5.0, self.check_election_timeout).start()\n    \n    def on_election(self, msg):\n        sender = msg['from']\n        if sender < self.node_id:\n            # Send OK to stop their election\n            self.send_to(sender, 'OK', {})\n            # Start my own election\n            self.start_election()\n    \n    def on_ok(self, msg):\n        # Someone higher is alive, wait for coordinator\n        pass\n    \n    def on_coordinator(self, msg):\n        self.leader_id = msg['leader']\n        self.election_in_progress = False\n        print(f'Node {self.node_id}: New leader is {self.leader_id}')\n    \n    def become_leader(self):\n        self.leader_id = self.node_id\n        self.election_in_progress = False\n        self.broadcast('COORDINATOR', {'leader': self.node_id})\n        print(f'Node {self.node_id}: I am the leader!')"
                        },
                        pitfalls: ["Split brain", "Message loss", "Concurrent elections"],
                        concepts: ["Bully algorithm", "Leader election", "Distributed coordination"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 3,
                        name: "Ring Election",
                        description: "Implement ring-based election algorithm.",
                        criteria: ["Nodes arranged in logical ring", "Election message passed around ring", "Collect all live node IDs", "Highest ID becomes leader"],
                        hints: {
                            level1: "Pass ELECTION with list of IDs. When it returns, highest ID wins.",
                            level2: "Each node adds its ID and forwards. When message returns to initiator, broadcast result.",
                            level3: "class RingElection(Node):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ring = sorted(self.peers.keys())  # Ring order\n        self.handlers['ELECTION'] = self.on_election\n        self.handlers['ELECTED'] = self.on_elected\n    \n    def next_node(self):\n        '''Get next node in ring'''\n        idx = self.ring.index(self.node_id)\n        for i in range(1, len(self.ring)):\n            next_idx = (idx + i) % len(self.ring)\n            next_id = self.ring[next_idx]\n            if self.send_to(next_id, 'PING', {}):\n                return next_id\n        return None  # All nodes dead\n    \n    def start_election(self):\n        # Start election message with my ID\n        self.forward_election([self.node_id], self.node_id)\n    \n    def forward_election(self, participants: list, initiator: int):\n        next_id = self.next_node()\n        if next_id:\n            self.send_to(next_id, 'ELECTION', {\n                'participants': participants,\n                'initiator': initiator\n            })\n    \n    def on_election(self, msg):\n        participants = msg['participants']\n        initiator = msg['initiator']\n        \n        if self.node_id in participants:\n            # Message completed the ring\n            if initiator == self.node_id:\n                # I started this, announce winner\n                winner = max(participants)\n                self.broadcast('ELECTED', {'leader': winner})\n                self.leader_id = winner\n        else:\n            # Add myself and forward\n            participants.append(self.node_id)\n            self.forward_election(participants, initiator)\n    \n    def on_elected(self, msg):\n        self.leader_id = msg['leader']\n        print(f'Node {self.node_id}: Leader is {self.leader_id}')"
                        },
                        pitfalls: ["Ring breaks", "Multiple elections", "Node rejoins"],
                        concepts: ["Ring topology", "Token passing", "Distributed election"],
                        estimatedHours: "4-5"
                    }
                ]
            },

            // DISTRIBUTED - ADVANCED
            "2pc-impl": {
                name: "Two-Phase Commit",
                description: "Implement 2PC for distributed transactions. Learn atomic commitment and coordinator recovery.",
                difficulty: "advanced",
                estimatedHours: "15-25",
                prerequisites: ["Distributed systems", "Transaction concepts", "Failure handling"],
                languages: { recommended: ["Go", "Java", "Python"], also: ["Rust", "Erlang"] },
                resources: [
                    { name: "2PC Paper", url: "https://www.cs.princeton.edu/courses/archive/fall16/cos418/papers/bernstein-ch7.pdf", type: "paper" },
                    { name: "Distributed Transactions", url: "https://www.the-paper-trail.org/post/2014-10-16-consensus-protocols-two-phase-commit/", type: "article" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Transaction Log",
                        description: "Implement durable transaction log for recovery.",
                        criteria: ["Write-ahead logging", "Log PREPARE, COMMIT, ABORT records", "Survive crashes", "Recovery from log"],
                        hints: {
                            level1: "Log must be durable (fsync). Write before sending messages.",
                            level2: "Log format: [tx_id, state, participants]. State = PREPARE/COMMIT/ABORT.",
                            level3: "import json\nimport os\n\nclass TransactionLog:\n    def __init__(self, path: str):\n        self.path = path\n        self.file = open(path, 'a+')\n    \n    def log(self, tx_id: str, state: str, data: dict = None):\n        record = {'tx_id': tx_id, 'state': state, 'data': data or {}}\n        self.file.write(json.dumps(record) + '\\n')\n        self.file.flush()\n        os.fsync(self.file.fileno())\n    \n    def recover(self) -> dict:\n        '''Return {tx_id: last_state}'''\n        self.file.seek(0)\n        transactions = {}\n        for line in self.file:\n            if line.strip():\n                record = json.loads(line)\n                transactions[record['tx_id']] = record\n        return transactions\n\nclass Coordinator:\n    def __init__(self, log_path: str):\n        self.log = TransactionLog(log_path)\n        self.pending = {}  # tx_id -> {participants, votes}\n    \n    def recover(self):\n        '''Recover after crash'''\n        transactions = self.log.recover()\n        for tx_id, record in transactions.items():\n            if record['state'] == 'PREPARE':\n                # Was in prepare phase, need to abort or retry\n                self.abort_transaction(tx_id, record['data']['participants'])\n            elif record['state'] == 'COMMIT':\n                # Committed but may not have notified all\n                self.send_decision(tx_id, 'COMMIT', record['data']['participants'])"
                        },
                        pitfalls: ["Incomplete writes", "Recovery order", "Log truncation"],
                        concepts: ["Write-ahead logging", "Durability", "Crash recovery"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Prepare Phase",
                        description: "Implement the voting/prepare phase.",
                        criteria: ["Coordinator sends PREPARE to all participants", "Participants vote YES/NO", "Participant logs vote before responding", "Handle vote timeout"],
                        hints: {
                            level1: "PREPARE asks: can you commit? Participant checks and votes.",
                            level2: "Participant must log YES vote before responding (promise to commit if asked).",
                            level3: "class Coordinator:\n    def begin_2pc(self, tx_id: str, participants: list):\n        # Log prepare intent\n        self.log.log(tx_id, 'PREPARE', {'participants': participants})\n        \n        self.pending[tx_id] = {\n            'participants': participants,\n            'votes': {},\n            'state': 'PREPARING'\n        }\n        \n        # Send PREPARE to all participants\n        for p in participants:\n            self.send_to(p, 'PREPARE', {'tx_id': tx_id})\n        \n        # Wait for votes (with timeout)\n        threading.Timer(5.0, lambda: self.check_votes(tx_id)).start()\n    \n    def on_vote(self, participant: str, tx_id: str, vote: str):\n        if tx_id not in self.pending:\n            return\n        \n        self.pending[tx_id]['votes'][participant] = vote\n        \n        # Check if all votes received\n        if len(self.pending[tx_id]['votes']) == len(self.pending[tx_id]['participants']):\n            self.decide(tx_id)\n\nclass Participant:\n    def on_prepare(self, tx_id: str):\n        if self.can_commit(tx_id):\n            # Log YES vote (promise)\n            self.log.log(tx_id, 'VOTE_YES')\n            self.send_to_coordinator('VOTE', {'tx_id': tx_id, 'vote': 'YES'})\n        else:\n            self.log.log(tx_id, 'VOTE_NO')\n            self.send_to_coordinator('VOTE', {'tx_id': tx_id, 'vote': 'NO'})"
                        },
                        pitfalls: ["Logging before response", "Vote timeout", "Participant crash after voting"],
                        concepts: ["Voting protocol", "Prepare phase", "Participant state"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 3,
                        name: "Commit Phase",
                        description: "Implement the decision/commit phase.",
                        criteria: ["All YES -> COMMIT, any NO -> ABORT", "Log decision before sending", "Send decision to all participants", "Handle acknowledgment"],
                        hints: {
                            level1: "Decision is final once logged. Must eventually reach all participants.",
                            level2: "Retry sending decision to failed participants.",
                            level3: "class Coordinator:\n    def decide(self, tx_id: str):\n        votes = self.pending[tx_id]['votes']\n        participants = self.pending[tx_id]['participants']\n        \n        # All YES -> COMMIT, otherwise ABORT\n        all_yes = all(v == 'YES' for v in votes.values())\n        decision = 'COMMIT' if all_yes else 'ABORT'\n        \n        # Log decision BEFORE sending\n        self.log.log(tx_id, decision, {'participants': participants})\n        \n        # Send decision to all participants\n        self.send_decision(tx_id, decision, participants)\n    \n    def send_decision(self, tx_id: str, decision: str, participants: list):\n        pending_acks = set(participants)\n        \n        while pending_acks:\n            for p in list(pending_acks):\n                if self.send_to(p, decision, {'tx_id': tx_id}):\n                    # Wait for ACK\n                    pass\n            time.sleep(1)  # Retry failed participants\n    \n    def on_ack(self, participant: str, tx_id: str):\n        # Remove from pending, eventually log COMPLETE\n        pass\n\nclass Participant:\n    def on_commit(self, tx_id: str):\n        self.log.log(tx_id, 'COMMIT')\n        self.apply_transaction(tx_id)\n        self.send_ack(tx_id)\n    \n    def on_abort(self, tx_id: str):\n        self.log.log(tx_id, 'ABORT')\n        self.rollback_transaction(tx_id)\n        self.send_ack(tx_id)"
                        },
                        pitfalls: ["Decision before logging", "Lost messages", "Participant uncertainty"],
                        concepts: ["Commit protocol", "Atomic commitment", "Decision logging"],
                        estimatedHours: "4-5"
                    },
                    {
                        id: 4,
                        name: "Failure Recovery",
                        description: "Handle coordinator and participant failures.",
                        criteria: ["Coordinator crash recovery", "Participant timeout handling", "Participant crash recovery", "Blocking scenario handling"],
                        hints: {
                            level1: "On recovery: check log. If COMMIT logged, re-send. If only PREPARE, can abort.",
                            level2: "Participant in doubt (voted YES, no decision): must wait or ask coordinator.",
                            level3: "class Participant:\n    def recover(self):\n        '''Recovery after crash'''\n        transactions = self.log.recover()\n        for tx_id, record in transactions.items():\n            state = record['state']\n            \n            if state == 'VOTE_YES':\n                # I voted YES but don't know decision\n                # Must ask coordinator\n                self.query_coordinator(tx_id)\n            elif state == 'COMMIT':\n                # Was committed, ensure applied\n                if not self.is_applied(tx_id):\n                    self.apply_transaction(tx_id)\n            elif state == 'ABORT':\n                # Was aborted, ensure rolled back\n                if not self.is_rolled_back(tx_id):\n                    self.rollback_transaction(tx_id)\n    \n    def query_coordinator(self, tx_id: str):\n        '''Ask coordinator for decision (may block if coordinator down)'''\n        response = self.send_to_coordinator('QUERY', {'tx_id': tx_id})\n        if response:\n            if response['decision'] == 'COMMIT':\n                self.on_commit(tx_id)\n            else:\n                self.on_abort(tx_id)\n        else:\n            # Coordinator unreachable - must wait\n            # This is the blocking problem of 2PC\n            self.in_doubt.add(tx_id)\n\nclass Coordinator:\n    def on_query(self, tx_id: str):\n        '''Participant asking for decision'''\n        transactions = self.log.recover()\n        if tx_id in transactions:\n            return {'decision': transactions[tx_id]['state']}\n        else:\n            # Never heard of it, must have aborted\n            return {'decision': 'ABORT'}"
                        },
                        pitfalls: ["In-doubt blocking", "Coordinator single point of failure", "Network partitions"],
                        concepts: ["Crash recovery", "Blocking protocols", "Failure handling"],
                        estimatedHours: "4-6"
                    }
                ]
            },

            // SYSTEMS - INTERMEDIATE
            "process-spawner": {
                name: "Process Spawner",
                description: "Build a process manager using fork/exec. Learn Unix process lifecycle and IPC.",
                difficulty: "intermediate",
                estimatedHours: "10-15",
                prerequisites: ["Basic C programming", "Unix basics", "System calls"],
                languages: { recommended: ["C", "Rust", "Go"], also: ["Python"] },
                resources: [
                    { name: "fork(2) man page", url: "https://man7.org/linux/man-pages/man2/fork.2.html", type: "documentation" },
                    { name: "Advanced Programming in Unix", url: "https://www.apuebook.com/", type: "book" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Fork/Exec",
                        description: "Spawn a child process to run a command.",
                        criteria: ["fork() to create child process", "exec() to run command in child", "Parent waits for child", "Handle fork/exec errors"],
                        hints: {
                            level1: "fork() returns 0 in child, child PID in parent. exec() replaces process.",
                            level2: "Always check return values. Child should exec or _exit.",
                            level3: "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/wait.h>\n\nint spawn(char *program, char **args) {\n    pid_t pid = fork();\n    \n    if (pid < 0) {\n        perror(\"fork\");\n        return -1;\n    }\n    \n    if (pid == 0) {\n        // Child process\n        execvp(program, args);\n        // If exec returns, it failed\n        perror(\"exec\");\n        _exit(127);  // Use _exit, not exit\n    }\n    \n    // Parent process\n    int status;\n    if (waitpid(pid, &status, 0) < 0) {\n        perror(\"waitpid\");\n        return -1;\n    }\n    \n    if (WIFEXITED(status)) {\n        return WEXITSTATUS(status);\n    } else if (WIFSIGNALED(status)) {\n        printf(\"Killed by signal %d\\n\", WTERMSIG(status));\n        return -1;\n    }\n    \n    return -1;\n}\n\nint main() {\n    char *args[] = {\"ls\", \"-la\", NULL};\n    int result = spawn(\"ls\", args);\n    printf(\"Exit code: %d\\n\", result);\n    return 0;\n}"
                        },
                        pitfalls: ["Forgetting to exec", "Using exit() instead of _exit()", "Not handling signals"],
                        concepts: ["fork()", "exec()", "Process creation"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Pipe Communication",
                        description: "Set up pipes for parent-child communication.",
                        criteria: ["Create pipe before fork", "Redirect child stdin/stdout", "Parent reads/writes pipe", "Close unused pipe ends"],
                        hints: {
                            level1: "pipe() before fork. dup2() redirects file descriptors.",
                            level2: "Close unused ends to avoid deadlock. EOF when all writers close.",
                            level3: "#include <unistd.h>\n\nint spawn_with_pipe(char *program, char **args, int *fd_in, int *fd_out) {\n    int pipe_in[2], pipe_out[2];\n    \n    if (pipe(pipe_in) < 0 || pipe(pipe_out) < 0) {\n        perror(\"pipe\");\n        return -1;\n    }\n    \n    pid_t pid = fork();\n    if (pid < 0) {\n        perror(\"fork\");\n        return -1;\n    }\n    \n    if (pid == 0) {\n        // Child\n        // pipe_in: parent writes, child reads\n        close(pipe_in[1]);  // Close write end\n        dup2(pipe_in[0], STDIN_FILENO);\n        close(pipe_in[0]);\n        \n        // pipe_out: child writes, parent reads\n        close(pipe_out[0]);  // Close read end\n        dup2(pipe_out[1], STDOUT_FILENO);\n        close(pipe_out[1]);\n        \n        execvp(program, args);\n        _exit(127);\n    }\n    \n    // Parent\n    close(pipe_in[0]);   // Close read end\n    close(pipe_out[1]);  // Close write end\n    \n    *fd_in = pipe_in[1];   // Parent writes here\n    *fd_out = pipe_out[0]; // Parent reads here\n    \n    return pid;\n}\n\n// Usage:\nint fd_in, fd_out;\npid_t pid = spawn_with_pipe(\"cat\", args, &fd_in, &fd_out);\nwrite(fd_in, \"hello\\n\", 6);\nclose(fd_in);  // Signal EOF\nchar buf[1024];\nread(fd_out, buf, sizeof(buf));"
                        },
                        pitfalls: ["Pipe deadlock", "Forgetting to close ends", "Buffer full blocking"],
                        concepts: ["Pipes", "IPC", "File descriptor redirection"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Process Pool",
                        description: "Manage a pool of worker processes.",
                        criteria: ["Spawn N worker processes", "Distribute work to workers", "Handle worker crashes", "Clean shutdown"],
                        hints: {
                            level1: "Pre-fork workers, send work via pipes or shared memory.",
                            level2: "Parent tracks worker PIDs, respawns on SIGCHLD.",
                            level3: "#include <signal.h>\n\n#define MAX_WORKERS 4\n\ntypedef struct {\n    pid_t pid;\n    int fd_in;\n    int fd_out;\n    int busy;\n} Worker;\n\nWorker workers[MAX_WORKERS];\nint num_workers = 0;\n\nvoid sigchld_handler(int sig) {\n    int status;\n    pid_t pid;\n    while ((pid = waitpid(-1, &status, WNOHANG)) > 0) {\n        // Find and respawn crashed worker\n        for (int i = 0; i < num_workers; i++) {\n            if (workers[i].pid == pid) {\n                printf(\"Worker %d (PID %d) exited\\n\", i, pid);\n                spawn_worker(i);  // Respawn\n                break;\n            }\n        }\n    }\n}\n\nvoid spawn_worker(int idx) {\n    char *args[] = {\"./worker\", NULL};\n    workers[idx].pid = spawn_with_pipe(\"./worker\", args,\n                                        &workers[idx].fd_in,\n                                        &workers[idx].fd_out);\n    workers[idx].busy = 0;\n}\n\nvoid init_pool() {\n    signal(SIGCHLD, sigchld_handler);\n    \n    for (int i = 0; i < MAX_WORKERS; i++) {\n        spawn_worker(i);\n    }\n    num_workers = MAX_WORKERS;\n}\n\nWorker* get_available_worker() {\n    for (int i = 0; i < num_workers; i++) {\n        if (!workers[i].busy) {\n            workers[i].busy = 1;\n            return &workers[i];\n        }\n    }\n    return NULL;  // All busy\n}\n\nvoid shutdown_pool() {\n    for (int i = 0; i < num_workers; i++) {\n        close(workers[i].fd_in);\n        kill(workers[i].pid, SIGTERM);\n    }\n}"
                        },
                        pitfalls: ["Zombie processes", "Signal handling races", "Resource cleanup"],
                        concepts: ["Process pools", "SIGCHLD", "Worker management"],
                        estimatedHours: "4-5"
                    }
                ]
            },

            // SYSTEMS - INTERMEDIATE
            "signal-handler": {
                name: "Signal Handler",
                description: "Master Unix signal handling. Learn signal safety, masks, and graceful shutdown.",
                difficulty: "intermediate",
                estimatedHours: "8-12",
                prerequisites: ["C programming", "Unix basics", "Process concepts"],
                languages: { recommended: ["C", "Rust"], also: ["Go", "Python"] },
                resources: [
                    { name: "signal(7) man page", url: "https://man7.org/linux/man-pages/man7/signal.7.html", type: "documentation" },
                    { name: "Signal Safety", url: "https://man7.org/linux/man-pages/man7/signal-safety.7.html", type: "documentation" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Signal Handling",
                        description: "Install signal handlers for common signals.",
                        criteria: ["Handle SIGINT (Ctrl+C)", "Handle SIGTERM", "Use sigaction() not signal()", "Set SA_RESTART flag"],
                        hints: {
                            level1: "sigaction() is portable and reliable. signal() has race conditions.",
                            level2: "SA_RESTART makes interrupted syscalls restart automatically.",
                            level3: "#include <signal.h>\n#include <stdio.h>\n#include <unistd.h>\n\nvolatile sig_atomic_t running = 1;\n\nvoid handle_signal(int sig) {\n    // Only async-signal-safe operations here!\n    if (sig == SIGINT || sig == SIGTERM) {\n        running = 0;\n    }\n}\n\nint setup_signal_handler(int sig, void (*handler)(int)) {\n    struct sigaction sa;\n    sa.sa_handler = handler;\n    sigemptyset(&sa.sa_mask);\n    sa.sa_flags = SA_RESTART;  // Restart interrupted syscalls\n    \n    if (sigaction(sig, &sa, NULL) < 0) {\n        perror(\"sigaction\");\n        return -1;\n    }\n    return 0;\n}\n\nint main() {\n    setup_signal_handler(SIGINT, handle_signal);\n    setup_signal_handler(SIGTERM, handle_signal);\n    \n    printf(\"Running... Press Ctrl+C to stop\\n\");\n    \n    while (running) {\n        // Do work\n        sleep(1);\n        printf(\".\");\n        fflush(stdout);\n    }\n    \n    printf(\"\\nGraceful shutdown\\n\");\n    return 0;\n}"
                        },
                        pitfalls: ["Non-reentrant functions in handler", "Race conditions", "Missing SA_RESTART"],
                        concepts: ["Signal handlers", "sigaction()", "Async-signal safety"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "Signal Masking",
                        description: "Block and unblock signals for critical sections.",
                        criteria: ["Block signals during critical code", "Use sigprocmask()", "Handle pending signals", "Per-thread signal masks"],
                        hints: {
                            level1: "sigprocmask() blocks signals temporarily. Pending signals delivered when unblocked.",
                            level2: "Block signals before modifying shared data, unblock after.",
                            level3: "#include <signal.h>\n\nvoid block_signals(sigset_t *oldmask) {\n    sigset_t blockmask;\n    sigemptyset(&blockmask);\n    sigaddset(&blockmask, SIGINT);\n    sigaddset(&blockmask, SIGTERM);\n    sigaddset(&blockmask, SIGCHLD);\n    \n    if (sigprocmask(SIG_BLOCK, &blockmask, oldmask) < 0) {\n        perror(\"sigprocmask\");\n    }\n}\n\nvoid unblock_signals(sigset_t *oldmask) {\n    if (sigprocmask(SIG_SETMASK, oldmask, NULL) < 0) {\n        perror(\"sigprocmask\");\n    }\n}\n\nvoid critical_section() {\n    sigset_t oldmask;\n    \n    block_signals(&oldmask);\n    \n    // Critical code - signals blocked\n    // Modify shared data structures safely\n    update_shared_state();\n    \n    unblock_signals(&oldmask);\n    // Pending signals delivered here\n}\n\n// Check for pending signals\nvoid check_pending() {\n    sigset_t pending;\n    sigpending(&pending);\n    \n    if (sigismember(&pending, SIGINT)) {\n        printf(\"SIGINT is pending\\n\");\n    }\n}"
                        },
                        pitfalls: ["Forgetting to unblock", "Deadlock with nested blocking", "Thread safety"],
                        concepts: ["Signal masks", "Critical sections", "Pending signals"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 3,
                        name: "Self-Pipe Trick",
                        description: "Integrate signals with select/poll event loop.",
                        criteria: ["Create self-pipe", "Write to pipe in signal handler", "select/poll on pipe", "Handle signal in main loop"],
                        hints: {
                            level1: "Signal handlers can't do much safely. Write byte to pipe, handle in event loop.",
                            level2: "Make pipe non-blocking. select() returns when pipe readable.",
                            level3: "#include <fcntl.h>\n#include <sys/select.h>\n\nint signal_pipe[2];\n\nvoid signal_handler(int sig) {\n    // Async-signal-safe: write single byte\n    int saved_errno = errno;\n    char c = sig;\n    write(signal_pipe[1], &c, 1);\n    errno = saved_errno;\n}\n\nvoid setup_self_pipe() {\n    if (pipe(signal_pipe) < 0) {\n        perror(\"pipe\");\n        exit(1);\n    }\n    \n    // Make non-blocking\n    fcntl(signal_pipe[0], F_SETFL, O_NONBLOCK);\n    fcntl(signal_pipe[1], F_SETFL, O_NONBLOCK);\n    \n    setup_signal_handler(SIGINT, signal_handler);\n    setup_signal_handler(SIGTERM, signal_handler);\n    setup_signal_handler(SIGCHLD, signal_handler);\n}\n\nvoid event_loop() {\n    fd_set readfds;\n    \n    while (running) {\n        FD_ZERO(&readfds);\n        FD_SET(signal_pipe[0], &readfds);\n        FD_SET(client_socket, &readfds);  // Other FDs\n        \n        int maxfd = signal_pipe[0] > client_socket ? signal_pipe[0] : client_socket;\n        \n        int ret = select(maxfd + 1, &readfds, NULL, NULL, NULL);\n        if (ret < 0 && errno != EINTR) {\n            perror(\"select\");\n            break;\n        }\n        \n        if (FD_ISSET(signal_pipe[0], &readfds)) {\n            // Signal received - handle safely here\n            char sig;\n            while (read(signal_pipe[0], &sig, 1) > 0) {\n                handle_signal_safely(sig);\n            }\n        }\n        \n        if (FD_ISSET(client_socket, &readfds)) {\n            handle_client();\n        }\n    }\n}"
                        },
                        pitfalls: ["Pipe buffer full", "Non-blocking write", "Multiple signals coalescing"],
                        concepts: ["Self-pipe trick", "Event loops", "Signal integration"],
                        estimatedHours: "3-4"
                    }
                ]
            },

            // COMPILERS - INTERMEDIATE
            "ast-interpreter": {
                name: "AST Tree-Walking Interpreter",
                description: "Build an interpreter that directly evaluates the AST. Learn environments, scoping, and evaluation.",
                difficulty: "intermediate",
                estimatedHours: "12-20",
                prerequisites: ["AST Builder", "Recursion", "Environment/scope concepts"],
                languages: { recommended: ["Python", "JavaScript", "Java"], also: ["Go", "Rust"] },
                resources: [
                    { name: "Crafting Interpreters - Evaluating", url: "https://craftinginterpreters.com/evaluating-expressions.html", type: "book" },
                    { name: "SICP - Metacircular Evaluator", url: "https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-26.html", type: "book" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Expression Evaluation",
                        description: "Evaluate arithmetic and comparison expressions.",
                        criteria: ["Literals evaluate to themselves", "Binary operators (+, -, *, /, <, >, ==)", "Unary operators (-, !)", "Parentheses for grouping"],
                        hints: {
                            level1: "Recursive evaluate: check node type, recursively evaluate children, apply operation.",
                            level2: "Use visitor pattern or isinstance checks for different node types.",
                            level3: "class Interpreter:\n    def evaluate(self, node):\n        method_name = f'eval_{type(node).__name__}'\n        method = getattr(self, method_name, self.generic_eval)\n        return method(node)\n    \n    def generic_eval(self, node):\n        raise RuntimeError(f'No eval method for {type(node).__name__}')\n    \n    def eval_Literal(self, node):\n        return node.value\n    \n    def eval_Binary(self, node):\n        left = self.evaluate(node.left)\n        right = self.evaluate(node.right)\n        \n        ops = {\n            '+': lambda a, b: a + b,\n            '-': lambda a, b: a - b,\n            '*': lambda a, b: a * b,\n            '/': lambda a, b: a / b,\n            '<': lambda a, b: a < b,\n            '>': lambda a, b: a > b,\n            '==': lambda a, b: a == b,\n            '!=': lambda a, b: a != b,\n            'and': lambda a, b: a and b,\n            'or': lambda a, b: a or b,\n        }\n        \n        if node.operator not in ops:\n            raise RuntimeError(f'Unknown operator: {node.operator}')\n        \n        return ops[node.operator](left, right)\n    \n    def eval_Unary(self, node):\n        operand = self.evaluate(node.operand)\n        if node.operator == '-':\n            return -operand\n        elif node.operator == '!':\n            return not operand"
                        },
                        pitfalls: ["Type errors at runtime", "Division by zero", "Short-circuit evaluation"],
                        concepts: ["Tree-walking", "Evaluation", "Operator semantics"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Variables and Environment",
                        description: "Implement variable binding and lookup.",
                        criteria: ["Environment maps names to values", "Variable declaration (var/let)", "Variable assignment", "Nested scopes with parent lookup"],
                        hints: {
                            level1: "Environment = dict + parent pointer. Lookup checks local then parent.",
                            level2: "New scope = new environment with current as parent.",
                            level3: "class Environment:\n    def __init__(self, parent=None):\n        self.values = {}\n        self.parent = parent\n    \n    def define(self, name, value):\n        self.values[name] = value\n    \n    def get(self, name):\n        if name in self.values:\n            return self.values[name]\n        if self.parent:\n            return self.parent.get(name)\n        raise RuntimeError(f'Undefined variable: {name}')\n    \n    def assign(self, name, value):\n        if name in self.values:\n            self.values[name] = value\n            return\n        if self.parent:\n            self.parent.assign(name, value)\n            return\n        raise RuntimeError(f'Undefined variable: {name}')\n\nclass Interpreter:\n    def __init__(self):\n        self.environment = Environment()\n    \n    def eval_Identifier(self, node):\n        return self.environment.get(node.name)\n    \n    def eval_VarDecl(self, node):\n        value = None\n        if node.initializer:\n            value = self.evaluate(node.initializer)\n        self.environment.define(node.name, value)\n    \n    def eval_Assignment(self, node):\n        value = self.evaluate(node.value)\n        self.environment.assign(node.name, value)\n        return value\n    \n    def eval_Block(self, node):\n        # New scope\n        previous = self.environment\n        self.environment = Environment(parent=previous)\n        \n        try:\n            result = None\n            for stmt in node.statements:\n                result = self.evaluate(stmt)\n            return result\n        finally:\n            self.environment = previous"
                        },
                        pitfalls: ["Scope restoration", "Shadowing", "Assignment vs declaration"],
                        concepts: ["Environments", "Scoping", "Variable binding"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 3,
                        name: "Control Flow",
                        description: "Implement if/else and loops.",
                        criteria: ["if/else statements", "while loops", "for loops (optional)", "break/continue (optional)"],
                        hints: {
                            level1: "Evaluate condition, then evaluate appropriate branch.",
                            level2: "Loops: evaluate condition, if true evaluate body, repeat.",
                            level3: "def eval_If(self, node):\n    condition = self.evaluate(node.condition)\n    \n    if self.is_truthy(condition):\n        return self.evaluate(node.then_branch)\n    elif node.else_branch:\n        return self.evaluate(node.else_branch)\n    return None\n\ndef eval_While(self, node):\n    result = None\n    while self.is_truthy(self.evaluate(node.condition)):\n        try:\n            result = self.evaluate(node.body)\n        except BreakException:\n            break\n        except ContinueException:\n            continue\n    return result\n\ndef eval_For(self, node):\n    # for (init; condition; increment) body\n    # Desugar to: { init; while (condition) { body; increment; } }\n    previous = self.environment\n    self.environment = Environment(parent=previous)\n    \n    try:\n        if node.initializer:\n            self.evaluate(node.initializer)\n        \n        while True:\n            if node.condition:\n                if not self.is_truthy(self.evaluate(node.condition)):\n                    break\n            \n            try:\n                self.evaluate(node.body)\n            except BreakException:\n                break\n            except ContinueException:\n                pass\n            \n            if node.increment:\n                self.evaluate(node.increment)\n    finally:\n        self.environment = previous\n\ndef is_truthy(self, value):\n    if value is None:\n        return False\n    if isinstance(value, bool):\n        return value\n    return True"
                        },
                        pitfalls: ["Infinite loops", "Break outside loop", "Truthiness rules"],
                        concepts: ["Control flow", "Loops", "Conditionals"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 4,
                        name: "Functions",
                        description: "Implement function definitions and calls.",
                        criteria: ["Function declarations", "Function calls with arguments", "Return statements", "Closures (capture environment)"],
                        hints: {
                            level1: "Function = (params, body, closure_env). Call = new env with args bound.",
                            level2: "Closure captures defining environment, not calling environment.",
                            level3: "class LoxFunction:\n    def __init__(self, declaration, closure):\n        self.declaration = declaration\n        self.closure = closure  # Environment when defined\n    \n    def call(self, interpreter, arguments):\n        # Create new environment for this call\n        environment = Environment(parent=self.closure)\n        \n        # Bind parameters to arguments\n        for i, param in enumerate(self.declaration.params):\n            environment.define(param, arguments[i])\n        \n        try:\n            interpreter.execute_block(self.declaration.body, environment)\n        except ReturnException as ret:\n            return ret.value\n        \n        return None\n    \n    def arity(self):\n        return len(self.declaration.params)\n\nclass Interpreter:\n    def eval_FunctionDecl(self, node):\n        function = LoxFunction(node, self.environment)\n        self.environment.define(node.name, function)\n    \n    def eval_Call(self, node):\n        callee = self.evaluate(node.callee)\n        arguments = [self.evaluate(arg) for arg in node.arguments]\n        \n        if not hasattr(callee, 'call'):\n            raise RuntimeError('Can only call functions')\n        \n        if len(arguments) != callee.arity():\n            raise RuntimeError(f'Expected {callee.arity()} arguments but got {len(arguments)}')\n        \n        return callee.call(self, arguments)\n    \n    def eval_Return(self, node):\n        value = None\n        if node.value:\n            value = self.evaluate(node.value)\n        raise ReturnException(value)"
                        },
                        pitfalls: ["Closure capture", "Return from nested function", "Argument count"],
                        concepts: ["Functions", "Closures", "Call stack"],
                        estimatedHours: "4-5"
                    }
                ]
            },

            // DATA STORAGE - ADVANCED
            "query-optimizer": {
                name: "Query Optimizer",
                description: "Build a basic query optimizer. Learn cost estimation, join ordering, and query plans.",
                difficulty: "advanced",
                estimatedHours: "20-35",
                prerequisites: ["SQL Parser", "Database fundamentals", "Algorithm complexity"],
                languages: { recommended: ["Python", "Java", "Go"], also: ["Rust", "C++"] },
                resources: [
                    { name: "CMU Database Course", url: "https://15445.courses.cs.cmu.edu/", type: "course" },
                    { name: "Query Optimization Survey", url: "https://www.vldb.org/pvldb/vol14/p3025-yang.pdf", type: "paper" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Query Plan Representation",
                        description: "Define query plan tree structure.",
                        criteria: ["Plan nodes (Scan, Filter, Join, Project)", "Tree structure for plans", "Physical vs logical operators", "Plan pretty-printing"],
                        hints: {
                            level1: "Plan = tree of operators. Each node has children and produces rows.",
                            level2: "Logical: what to compute. Physical: how to compute (e.g., HashJoin vs NestedLoopJoin).",
                            level3: "from dataclasses import dataclass\nfrom typing import List, Optional\nfrom abc import ABC, abstractmethod\n\nclass PlanNode(ABC):\n    @abstractmethod\n    def children(self) -> List['PlanNode']:\n        pass\n    \n    @abstractmethod\n    def __str__(self) -> str:\n        pass\n\n@dataclass\nclass SeqScan(PlanNode):\n    table: str\n    alias: str = None\n    \n    def children(self): return []\n    def __str__(self): return f'SeqScan({self.table})'\n\n@dataclass\nclass IndexScan(PlanNode):\n    table: str\n    index: str\n    predicate: 'Expr'\n    \n    def children(self): return []\n    def __str__(self): return f'IndexScan({self.table}, {self.index})'\n\n@dataclass\nclass Filter(PlanNode):\n    predicate: 'Expr'\n    child: PlanNode\n    \n    def children(self): return [self.child]\n    def __str__(self): return f'Filter({self.predicate})'\n\n@dataclass\nclass HashJoin(PlanNode):\n    left: PlanNode\n    right: PlanNode\n    condition: 'Expr'\n    \n    def children(self): return [self.left, self.right]\n    def __str__(self): return f'HashJoin({self.condition})'\n\n@dataclass\nclass NestedLoopJoin(PlanNode):\n    left: PlanNode\n    right: PlanNode\n    condition: 'Expr'\n    \n    def children(self): return [self.left, self.right]\n    def __str__(self): return f'NLJoin({self.condition})'\n\n@dataclass\nclass Project(PlanNode):\n    columns: List[str]\n    child: PlanNode\n    \n    def children(self): return [self.child]\n    def __str__(self): return f'Project({self.columns})'\n\ndef print_plan(node: PlanNode, indent=0):\n    print(' ' * indent + str(node))\n    for child in node.children():\n        print_plan(child, indent + 2)"
                        },
                        pitfalls: ["Missing operators", "Tree vs DAG", "Operator semantics"],
                        concepts: ["Query plans", "Operators", "Plan trees"],
                        estimatedHours: "3-4"
                    },
                    {
                        id: 2,
                        name: "Cost Estimation",
                        description: "Estimate the cost of query plans.",
                        criteria: ["Table statistics (row count, distinct values)", "Selectivity estimation", "Join cardinality estimation", "I/O and CPU cost model"],
                        hints: {
                            level1: "Cost = f(cardinality, I/O, CPU). Need statistics about data.",
                            level2: "Selectivity: fraction of rows that pass a filter. Default = 0.1 for unknown.",
                            level3: "@dataclass\nclass TableStats:\n    row_count: int\n    distinct_values: dict  # column -> count\n    min_values: dict       # column -> min\n    max_values: dict       # column -> max\n\nclass CostEstimator:\n    def __init__(self, stats: dict):\n        self.stats = stats  # table_name -> TableStats\n    \n    def estimate_cardinality(self, node: PlanNode) -> float:\n        if isinstance(node, SeqScan):\n            return self.stats[node.table].row_count\n        \n        elif isinstance(node, Filter):\n            child_card = self.estimate_cardinality(node.child)\n            selectivity = self.estimate_selectivity(node.predicate, node.child)\n            return child_card * selectivity\n        \n        elif isinstance(node, (HashJoin, NestedLoopJoin)):\n            left_card = self.estimate_cardinality(node.left)\n            right_card = self.estimate_cardinality(node.right)\n            selectivity = self.estimate_join_selectivity(node.condition)\n            return left_card * right_card * selectivity\n        \n        return 1000  # Default\n    \n    def estimate_selectivity(self, predicate, source) -> float:\n        # col = constant\n        if isinstance(predicate, Comparison) and predicate.op == '=':\n            if isinstance(predicate.left, Identifier):\n                col = predicate.left.name\n                table = self.get_table(source)\n                if col in self.stats[table].distinct_values:\n                    return 1.0 / self.stats[table].distinct_values[col]\n        \n        # col < constant (range query)\n        if isinstance(predicate, Comparison) and predicate.op in ('<', '>'):\n            return 0.33  # Default for range\n        \n        return 0.1  # Default selectivity\n    \n    def estimate_cost(self, node: PlanNode) -> float:\n        cardinality = self.estimate_cardinality(node)\n        \n        if isinstance(node, SeqScan):\n            # I/O cost = pages to read\n            pages = cardinality / 100  # Assume 100 rows per page\n            return pages\n        \n        elif isinstance(node, HashJoin):\n            left_cost = self.estimate_cost(node.left)\n            right_cost = self.estimate_cost(node.right)\n            # Build hash table + probe\n            return left_cost + right_cost + cardinality * 0.01\n        \n        elif isinstance(node, NestedLoopJoin):\n            left_cost = self.estimate_cost(node.left)\n            right_cost = self.estimate_cost(node.right)\n            left_card = self.estimate_cardinality(node.left)\n            # Nested loop: scan right for each left row\n            return left_cost + left_card * right_cost\n        \n        # Sum children costs\n        return sum(self.estimate_cost(c) for c in node.children())"
                        },
                        pitfalls: ["Statistics staleness", "Correlation assumptions", "Estimation errors"],
                        concepts: ["Cost models", "Selectivity", "Cardinality estimation"],
                        estimatedHours: "5-7"
                    },
                    {
                        id: 3,
                        name: "Join Ordering",
                        description: "Find optimal join order for multi-table queries.",
                        criteria: ["Enumerate join orders", "Dynamic programming for optimal order", "Left-deep vs bushy trees", "Prune bad plans early"],
                        hints: {
                            level1: "Join order matters! n tables = n! possible orders.",
                            level2: "DP: build optimal plans for subsets, combine for larger sets.",
                            level3: "from itertools import combinations\n\nclass JoinOrderOptimizer:\n    def __init__(self, estimator: CostEstimator):\n        self.estimator = estimator\n        self.memo = {}  # frozenset(tables) -> best_plan\n    \n    def optimize_joins(self, tables: List[str], predicates: List['Expr']) -> PlanNode:\n        '''Find optimal join order using dynamic programming'''\n        # Base case: single tables\n        for table in tables:\n            key = frozenset([table])\n            self.memo[key] = SeqScan(table)\n        \n        # Build up from 2 tables to all tables\n        for size in range(2, len(tables) + 1):\n            for subset in combinations(tables, size):\n                subset_key = frozenset(subset)\n                self.find_best_join(subset_key, predicates)\n        \n        return self.memo[frozenset(tables)]\n    \n    def find_best_join(self, tables: frozenset, predicates: List['Expr']):\n        best_plan = None\n        best_cost = float('inf')\n        \n        # Try all ways to split tables into two non-empty subsets\n        for i in range(1, len(tables)):\n            for left_tables in combinations(tables, i):\n                left_key = frozenset(left_tables)\n                right_key = tables - left_key\n                \n                if left_key not in self.memo or right_key not in self.memo:\n                    continue\n                \n                left_plan = self.memo[left_key]\n                right_plan = self.memo[right_key]\n                \n                # Find applicable join predicate\n                join_pred = self.find_join_predicate(left_key, right_key, predicates)\n                \n                # Try different join algorithms\n                for join_type in [HashJoin, NestedLoopJoin]:\n                    plan = join_type(left_plan, right_plan, join_pred)\n                    cost = self.estimator.estimate_cost(plan)\n                    \n                    if cost < best_cost:\n                        best_cost = cost\n                        best_plan = plan\n        \n        self.memo[tables] = best_plan"
                        },
                        pitfalls: ["Exponential complexity", "Cross joins", "Missing predicates"],
                        concepts: ["Join ordering", "Dynamic programming", "Plan enumeration"],
                        estimatedHours: "6-8"
                    },
                    {
                        id: 4,
                        name: "Plan Selection",
                        description: "Choose between physical operators and access methods.",
                        criteria: ["Index selection", "Join algorithm selection", "Predicate pushdown", "Generate final plan"],
                        hints: {
                            level1: "Use index if selective enough. Push filters down before joins.",
                            level2: "HashJoin good for large joins. NestedLoop good with index on inner.",
                            level3: "class QueryOptimizer:\n    def __init__(self, stats: dict, indexes: dict):\n        self.estimator = CostEstimator(stats)\n        self.indexes = indexes  # table -> [index_info]\n    \n    def optimize(self, query: SelectStmt) -> PlanNode:\n        # 1. Generate initial logical plan\n        plan = self.logical_plan(query)\n        \n        # 2. Predicate pushdown\n        plan = self.pushdown_predicates(plan)\n        \n        # 3. Join ordering\n        if self.has_joins(plan):\n            tables = self.extract_tables(plan)\n            predicates = self.extract_join_predicates(plan)\n            plan = JoinOrderOptimizer(self.estimator).optimize_joins(tables, predicates)\n        \n        # 4. Physical operator selection\n        plan = self.select_physical_operators(plan)\n        \n        return plan\n    \n    def select_physical_operators(self, node: PlanNode) -> PlanNode:\n        if isinstance(node, SeqScan):\n            # Check if index scan is better\n            return self.select_access_method(node)\n        \n        elif isinstance(node, Filter):\n            child = self.select_physical_operators(node.child)\n            return Filter(node.predicate, child)\n        \n        elif isinstance(node, (HashJoin, NestedLoopJoin)):\n            left = self.select_physical_operators(node.left)\n            right = self.select_physical_operators(node.right)\n            return self.select_join_method(left, right, node.condition)\n        \n        return node\n    \n    def select_access_method(self, scan: SeqScan) -> PlanNode:\n        # Check for applicable indexes\n        if scan.table in self.indexes:\n            for idx in self.indexes[scan.table]:\n                # If there's a selective predicate on indexed column\n                # return IndexScan instead\n                pass\n        return scan\n    \n    def select_join_method(self, left, right, condition) -> PlanNode:\n        # Hash join for equi-joins on large tables\n        # Nested loop for small tables or with index\n        left_card = self.estimator.estimate_cardinality(left)\n        right_card = self.estimator.estimate_cardinality(right)\n        \n        if left_card > 1000 and right_card > 1000:\n            return HashJoin(left, right, condition)\n        else:\n            return NestedLoopJoin(left, right, condition)"
                        },
                        pitfalls: ["Over-optimization", "Plan caching", "Statistics accuracy"],
                        concepts: ["Physical planning", "Index selection", "Predicate pushdown"],
                        estimatedHours: "5-7"
                    }
                ]
            },

            // SYSTEMS - ADVANCED
            "mini-shell": {
                name: "Mini Shell",
                description: "Build a feature-rich shell with job control. Learn process groups, signals, and terminal handling.",
                difficulty: "advanced",
                estimatedHours: "25-40",
                prerequisites: ["Process spawner", "Signal handling", "Unix process model"],
                languages: {
                    recommended: ["C", "Rust"],
                    also: ["Go", "Zig"]
                },
                resources: [
                    { type: "article", name: "Writing Your Own Shell", url: "https://brennan.io/2015/01/16/write-a-shell-in-c/" },
                    { type: "book", name: "Advanced Programming in Unix Environment - Ch 9", url: "https://www.apuebook.com/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Command Execution",
                        description: "Parse and execute simple commands with arguments.",
                        criteria: ["Parse command line into tokens", "Handle quoted strings", "Execute external commands", "Basic builtins (cd, exit, pwd)"],
                        hints: {
                            level1: "Use strtok or write custom tokenizer. Quote handling needs state machine.",
                            level2: "Builtins must run in shell process, not forked. cd changes shell's cwd.",
                            level3: "char **parse_line(char *line) {\n    // Simple tokenizer (no quote handling)\n    char **tokens = malloc(64 * sizeof(char*));\n    int pos = 0;\n    char *token = strtok(line, \" \\t\\n\");\n    while (token) {\n        tokens[pos++] = token;\n        token = strtok(NULL, \" \\t\\n\");\n    }\n    tokens[pos] = NULL;\n    return tokens;\n}\n\nint execute_command(char **args) {\n    // Handle builtins\n    if (strcmp(args[0], \"cd\") == 0) {\n        return chdir(args[1] ? args[1] : getenv(\"HOME\"));\n    }\n    if (strcmp(args[0], \"exit\") == 0) exit(0);\n    \n    // External command\n    pid_t pid = fork();\n    if (pid == 0) {\n        execvp(args[0], args);\n        perror(\"execvp\");\n        exit(1);\n    }\n    int status;\n    waitpid(pid, &status, 0);\n    return WEXITSTATUS(status);\n}"
                        },
                        pitfalls: ["Forgetting to null-terminate args", "Not handling empty input", "Memory leaks in tokenizer"],
                        concepts: ["Lexical analysis", "fork/exec pattern", "Process creation"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 2,
                        name: "Pipes and Redirection",
                        description: "Implement pipelines and I/O redirection.",
                        criteria: ["Input/output redirection (<, >)", "Append redirection (>>)", "Pipeline (cmd1 | cmd2 | cmd3)", "Handle file creation modes"],
                        hints: {
                            level1: "Parse redirection before executing. Use dup2 to redirect fds.",
                            level2: "Pipes need careful fd management - close unused ends. Create all pipes before forking.",
                            level3: "void setup_pipeline(Command *cmds, int n) {\n    int pipes[n-1][2];  // n-1 pipes for n commands\n    \n    // Create all pipes first\n    for (int i = 0; i < n-1; i++) {\n        pipe(pipes[i]);\n    }\n    \n    for (int i = 0; i < n; i++) {\n        pid_t pid = fork();\n        if (pid == 0) {\n            // Connect to previous pipe (read end)\n            if (i > 0) {\n                dup2(pipes[i-1][0], STDIN_FILENO);\n            }\n            // Connect to next pipe (write end)\n            if (i < n-1) {\n                dup2(pipes[i][1], STDOUT_FILENO);\n            }\n            \n            // Close all pipe fds in child\n            for (int j = 0; j < n-1; j++) {\n                close(pipes[j][0]);\n                close(pipes[j][1]);\n            }\n            \n            execvp(cmds[i].args[0], cmds[i].args);\n            exit(1);\n        }\n    }\n    \n    // Parent closes all pipes\n    for (int i = 0; i < n-1; i++) {\n        close(pipes[i][0]);\n        close(pipes[i][1]);\n    }\n    \n    // Wait for all children\n    for (int i = 0; i < n; i++) {\n        wait(NULL);\n    }\n}"
                        },
                        pitfalls: ["Fd leaks causing hangs", "Not closing pipe ends", "Wrong redirection precedence"],
                        concepts: ["File descriptors", "dup2", "Pipe communication"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 3,
                        name: "Job Control",
                        description: "Implement background jobs and job management.",
                        criteria: ["Background execution (&)", "Job listing (jobs)", "Foreground/background (fg, bg)", "Process group management"],
                        hints: {
                            level1: "Each job needs its own process group. Use setpgid in child and parent.",
                            level2: "Shell must give terminal to foreground job. Use tcsetpgrp.",
                            level3: "typedef struct Job {\n    int id;\n    pid_t pgid;\n    char *command;\n    int status;  // RUNNING, STOPPED, DONE\n    struct Job *next;\n} Job;\n\nJob *jobs = NULL;\nint next_job_id = 1;\n\nvoid launch_job(char **args, int background) {\n    pid_t pid = fork();\n    if (pid == 0) {\n        // Child: create new process group\n        setpgid(0, 0);\n        \n        // If foreground, take terminal\n        if (!background) {\n            tcsetpgrp(STDIN_FILENO, getpid());\n        }\n        \n        // Reset signal handlers\n        signal(SIGINT, SIG_DFL);\n        signal(SIGTSTP, SIG_DFL);\n        \n        execvp(args[0], args);\n        exit(1);\n    }\n    \n    // Parent\n    setpgid(pid, pid);  // Avoid race condition\n    \n    Job *job = add_job(pid, args);\n    \n    if (background) {\n        printf(\"[%d] %d\\n\", job->id, pid);\n    } else {\n        // Wait for foreground job\n        wait_for_job(job);\n        // Take back terminal\n        tcsetpgrp(STDIN_FILENO, getpgrp());\n    }\n}"
                        },
                        pitfalls: ["Race conditions with setpgid", "Terminal control issues", "Zombie processes"],
                        concepts: ["Process groups", "Session management", "Terminal control"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 4,
                        name: "Signal Handling",
                        description: "Handle Ctrl+C, Ctrl+Z and job notifications properly.",
                        criteria: ["SIGINT goes to foreground job only", "SIGTSTP suspends foreground job", "SIGCHLD for background job completion", "Proper signal masking"],
                        hints: {
                            level1: "Shell ignores SIGINT/SIGTSTP, children restore defaults.",
                            level2: "Use SIGCHLD handler to reap background jobs. Mask signals during critical sections.",
                            level3: "volatile sig_atomic_t sigchld_received = 0;\n\nvoid sigchld_handler(int sig) {\n    sigchld_received = 1;\n}\n\nvoid reap_children() {\n    int status;\n    pid_t pid;\n    \n    while ((pid = waitpid(-1, &status, WNOHANG | WUNTRACED | WCONTINUED)) > 0) {\n        Job *job = find_job_by_pid(pid);\n        if (!job) continue;\n        \n        if (WIFEXITED(status) || WIFSIGNALED(status)) {\n            printf(\"\\n[%d] Done: %s\\n\", job->id, job->command);\n            remove_job(job);\n        } else if (WIFSTOPPED(status)) {\n            job->status = STOPPED;\n            printf(\"\\n[%d] Stopped: %s\\n\", job->id, job->command);\n        } else if (WIFCONTINUED(status)) {\n            job->status = RUNNING;\n        }\n    }\n}\n\nvoid setup_shell_signals() {\n    // Shell ignores job-control signals\n    signal(SIGINT, SIG_IGN);\n    signal(SIGTSTP, SIG_IGN);\n    signal(SIGTTIN, SIG_IGN);\n    signal(SIGTTOU, SIG_IGN);\n    \n    // But catches SIGCHLD\n    struct sigaction sa;\n    sa.sa_handler = sigchld_handler;\n    sigemptyset(&sa.sa_mask);\n    sa.sa_flags = SA_RESTART;\n    sigaction(SIGCHLD, &sa, NULL);\n}"
                        },
                        pitfalls: ["Signal handler races", "Async-signal-safe functions", "Interrupted system calls"],
                        concepts: ["Signal masking", "Async-signal safety", "Job notification"],
                        estimatedHours: "8-14"
                    }
                ]
            },

            // COMPILERS - ADVANCED
            "bytecode-compiler": {
                name: "Bytecode Compiler",
                description: "Compile AST to bytecode for a stack-based VM. Learn code generation, instruction encoding, and optimization.",
                difficulty: "advanced",
                estimatedHours: "25-40",
                prerequisites: ["AST builder", "Bytecode VM", "Stack-based execution"],
                languages: {
                    recommended: ["Python", "Java", "Rust"],
                    also: ["C", "Go", "TypeScript"]
                },
                resources: [
                    { type: "book", name: "Crafting Interpreters - Compiling Expressions", url: "https://craftinginterpreters.com/compiling-expressions.html" },
                    { type: "article", name: "A Python Interpreter Written in Python", url: "https://aosabook.org/en/500L/a-python-interpreter-written-in-python.html" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Expression Compilation",
                        description: "Compile arithmetic and boolean expressions to bytecode.",
                        criteria: ["Emit opcodes for literals", "Binary operations", "Unary operations", "Handle operator precedence via AST"],
                        hints: {
                            level1: "Post-order traversal: compile children first, then emit operator.",
                            level2: "Literals push to stack. Binary ops pop 2, push 1 result.",
                            level3: "class Compiler:\n    def __init__(self):\n        self.code = []  # bytecode\n        self.constants = []  # constant pool\n    \n    def compile(self, node):\n        method = f'compile_{type(node).__name__}'\n        return getattr(self, method)(node)\n    \n    def compile_Number(self, node):\n        idx = self.add_constant(node.value)\n        self.emit(OpCode.CONST, idx)\n    \n    def compile_BinaryOp(self, node):\n        self.compile(node.left)\n        self.compile(node.right)\n        \n        ops = {\n            '+': OpCode.ADD,\n            '-': OpCode.SUB,\n            '*': OpCode.MUL,\n            '/': OpCode.DIV,\n            '<': OpCode.LT,\n            '>': OpCode.GT,\n            '==': OpCode.EQ,\n        }\n        self.emit(ops[node.op])\n    \n    def compile_UnaryOp(self, node):\n        self.compile(node.operand)\n        if node.op == '-':\n            self.emit(OpCode.NEG)\n        elif node.op == '!':\n            self.emit(OpCode.NOT)\n    \n    def emit(self, opcode, operand=None):\n        self.code.append(opcode)\n        if operand is not None:\n            self.code.append(operand)\n    \n    def add_constant(self, value):\n        self.constants.append(value)\n        return len(self.constants) - 1"
                        },
                        pitfalls: ["Wrong operand order for non-commutative ops", "Constant pool indexing", "Forgetting to handle all operators"],
                        concepts: ["Tree traversal", "Stack-based code generation", "Constant pools"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 2,
                        name: "Variables and Assignment",
                        description: "Compile variable declarations, assignments, and references.",
                        criteria: ["Local variable slots", "Variable resolution", "Assignment expressions", "Scoped variable shadowing"],
                        hints: {
                            level1: "Track variables in a symbol table mapping name to slot index.",
                            level2: "Use LOAD_LOCAL/STORE_LOCAL with slot index. Handle nested scopes.",
                            level3: "class Compiler:\n    def __init__(self):\n        self.code = []\n        self.locals = []  # Stack of scope dicts\n        self.push_scope()\n    \n    def push_scope(self):\n        self.locals.append({})\n    \n    def pop_scope(self):\n        scope = self.locals.pop()\n        # Emit pops for local variables\n        for _ in scope:\n            self.emit(OpCode.POP)\n    \n    def define_local(self, name):\n        scope = self.locals[-1]\n        slot = sum(len(s) for s in self.locals) - 1 + len(scope)\n        scope[name] = slot\n        return slot\n    \n    def resolve_local(self, name):\n        for scope in reversed(self.locals):\n            if name in scope:\n                return scope[name]\n        return None  # Global or undefined\n    \n    def compile_VarDecl(self, node):\n        if node.initializer:\n            self.compile(node.initializer)\n        else:\n            self.emit(OpCode.NIL)\n        self.define_local(node.name)\n    \n    def compile_Variable(self, node):\n        slot = self.resolve_local(node.name)\n        if slot is not None:\n            self.emit(OpCode.LOAD_LOCAL, slot)\n        else:\n            idx = self.add_constant(node.name)\n            self.emit(OpCode.LOAD_GLOBAL, idx)\n    \n    def compile_Assignment(self, node):\n        self.compile(node.value)\n        slot = self.resolve_local(node.name)\n        if slot is not None:\n            self.emit(OpCode.STORE_LOCAL, slot)\n        else:\n            idx = self.add_constant(node.name)\n            self.emit(OpCode.STORE_GLOBAL, idx)"
                        },
                        pitfalls: ["Scope lifetime management", "Variable shadowing bugs", "Uninitialized variables"],
                        concepts: ["Symbol tables", "Variable resolution", "Scope management"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 3,
                        name: "Control Flow",
                        description: "Compile if statements, while loops, and logical operators.",
                        criteria: ["Conditional jumps", "Jump targets/patching", "Short-circuit evaluation", "Loop compilation"],
                        hints: {
                            level1: "Emit jump with placeholder, then patch address after compiling body.",
                            level2: "Short-circuit: 'and' jumps to false if left is false, 'or' jumps to true if left is true.",
                            level3: "def compile_If(self, node):\n    self.compile(node.condition)\n    \n    # Jump to else if false\n    jump_to_else = self.emit_jump(OpCode.JUMP_IF_FALSE)\n    self.emit(OpCode.POP)  # Pop condition\n    \n    self.compile(node.then_branch)\n    \n    # Jump over else\n    jump_to_end = self.emit_jump(OpCode.JUMP)\n    \n    self.patch_jump(jump_to_else)\n    self.emit(OpCode.POP)  # Pop condition\n    \n    if node.else_branch:\n        self.compile(node.else_branch)\n    \n    self.patch_jump(jump_to_end)\n\ndef compile_While(self, node):\n    loop_start = len(self.code)\n    \n    self.compile(node.condition)\n    exit_jump = self.emit_jump(OpCode.JUMP_IF_FALSE)\n    self.emit(OpCode.POP)\n    \n    self.compile(node.body)\n    self.emit_loop(loop_start)\n    \n    self.patch_jump(exit_jump)\n    self.emit(OpCode.POP)\n\ndef emit_jump(self, opcode):\n    self.emit(opcode)\n    self.emit(0xFF)  # Placeholder\n    self.emit(0xFF)\n    return len(self.code) - 2\n\ndef patch_jump(self, offset):\n    jump = len(self.code) - offset - 2\n    self.code[offset] = (jump >> 8) & 0xFF\n    self.code[offset + 1] = jump & 0xFF\n\ndef emit_loop(self, loop_start):\n    self.emit(OpCode.LOOP)\n    offset = len(self.code) - loop_start + 2\n    self.emit((offset >> 8) & 0xFF)\n    self.emit(offset & 0xFF)"
                        },
                        pitfalls: ["Jump offset calculation", "Forgetting to pop condition", "Break/continue in nested loops"],
                        concepts: ["Jump patching", "Backpatching", "Short-circuit evaluation"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 4,
                        name: "Functions",
                        description: "Compile function definitions and calls.",
                        criteria: ["Function objects with bytecode", "Parameter passing", "Return statements", "Closures (optional)"],
                        hints: {
                            level1: "Each function has its own bytecode chunk. Compile separately.",
                            level2: "Parameters are just the first N local slots. CALL pushes frame.",
                            level3: "class FunctionObject:\n    def __init__(self, name, arity):\n        self.name = name\n        self.arity = arity\n        self.code = []\n        self.constants = []\n\ndef compile_FunctionDecl(self, node):\n    func = FunctionObject(node.name, len(node.params))\n    \n    # Save current compiler state\n    enclosing_code = self.code\n    enclosing_constants = self.constants\n    enclosing_locals = self.locals\n    \n    # Setup for function\n    self.code = func.code\n    self.constants = func.constants\n    self.locals = [{}]\n    \n    # Parameters become first locals\n    for param in node.params:\n        self.define_local(param)\n    \n    # Compile body\n    self.compile(node.body)\n    \n    # Implicit return nil\n    self.emit(OpCode.NIL)\n    self.emit(OpCode.RETURN)\n    \n    # Restore state\n    self.code = enclosing_code\n    self.constants = enclosing_constants\n    self.locals = enclosing_locals\n    \n    # Emit function as constant\n    idx = self.add_constant(func)\n    self.emit(OpCode.CONST, idx)\n    \n    # Define in current scope\n    self.define_local(node.name)\n\ndef compile_Call(self, node):\n    self.compile(node.callee)  # Push function\n    \n    for arg in node.arguments:\n        self.compile(arg)  # Push args\n    \n    self.emit(OpCode.CALL, len(node.arguments))\n\ndef compile_Return(self, node):\n    if node.value:\n        self.compile(node.value)\n    else:\n        self.emit(OpCode.NIL)\n    self.emit(OpCode.RETURN)"
                        },
                        pitfalls: ["Frame pointer management", "Argument count validation", "Stack cleanup on return"],
                        concepts: ["Call frames", "Parameter binding", "Return addresses"],
                        estimatedHours: "10-16"
                    }
                ]
            },

            // SYSTEMS - ADVANCED
            "http2-server": {
                name: "HTTP/2 Server",
                description: "Build an HTTP/2 server with multiplexing. Learn binary framing, HPACK compression, and stream management.",
                difficulty: "advanced",
                estimatedHours: "30-50",
                prerequisites: ["HTTP/1.1 server", "TLS", "Binary protocols"],
                languages: {
                    recommended: ["Go", "Rust", "C"],
                    also: ["Java", "Python"]
                },
                resources: [
                    { type: "spec", name: "RFC 7540 - HTTP/2", url: "https://httpwg.org/specs/rfc7540.html" },
                    { type: "spec", name: "RFC 7541 - HPACK", url: "https://httpwg.org/specs/rfc7541.html" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Binary Framing",
                        description: "Parse and emit HTTP/2 frames.",
                        criteria: ["Frame header parsing (9 bytes)", "Frame type handling", "Frame flags", "Payload extraction"],
                        hints: {
                            level1: "All frames start with 9-byte header: length(3), type(1), flags(1), stream_id(4).",
                            level2: "Length is 24-bit big-endian. Stream ID has reserved high bit.",
                            level3: "class Frame:\n    def __init__(self, type, flags, stream_id, payload=b''):\n        self.type = type\n        self.flags = flags\n        self.stream_id = stream_id\n        self.payload = payload\n\ndef parse_frame(data):\n    if len(data) < 9:\n        return None, data  # Need more data\n    \n    length = int.from_bytes(data[0:3], 'big')\n    type_ = data[3]\n    flags = data[4]\n    stream_id = int.from_bytes(data[5:9], 'big') & 0x7FFFFFFF\n    \n    if len(data) < 9 + length:\n        return None, data  # Need more data\n    \n    payload = data[9:9+length]\n    remaining = data[9+length:]\n    \n    return Frame(type_, flags, stream_id, payload), remaining\n\ndef encode_frame(frame):\n    header = (\n        frame.length.to_bytes(3, 'big') +\n        bytes([frame.type, frame.flags]) +\n        (frame.stream_id & 0x7FFFFFFF).to_bytes(4, 'big')\n    )\n    return header + frame.payload\n\n# Frame types\nDATA = 0x0\nHEADERS = 0x1\nPRIORITY = 0x2\nRST_STREAM = 0x3\nSETTINGS = 0x4\nPUSH_PROMISE = 0x5\nPING = 0x6\nGOAWAY = 0x7\nWINDOW_UPDATE = 0x8\nCONTINUATION = 0x9"
                        },
                        pitfalls: ["Endianness issues", "Frame length limits", "Reserved bit handling"],
                        concepts: ["Binary protocols", "Frame-based messaging", "Protocol parsing"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 2,
                        name: "HPACK Compression",
                        description: "Implement header compression with static/dynamic tables.",
                        criteria: ["Static table lookup", "Dynamic table management", "Huffman decoding", "Integer encoding"],
                        hints: {
                            level1: "HPACK uses indexed references + Huffman. Static table has 61 entries.",
                            level2: "Dynamic table is FIFO with size limit. Integers use 5/6/7-bit prefixes.",
                            level3: "STATIC_TABLE = [\n    (None, None),  # Index 0 unused\n    (':authority', ''),\n    (':method', 'GET'),\n    (':method', 'POST'),\n    (':path', '/'),\n    (':path', '/index.html'),\n    (':scheme', 'http'),\n    (':scheme', 'https'),\n    (':status', '200'),\n    # ... 61 entries total\n]\n\nclass HPACKDecoder:\n    def __init__(self, max_size=4096):\n        self.dynamic_table = []\n        self.max_size = max_size\n        self.size = 0\n    \n    def decode(self, data):\n        headers = []\n        i = 0\n        \n        while i < len(data):\n            byte = data[i]\n            \n            if byte & 0x80:  # Indexed header (7-bit index)\n                idx, i = self.decode_int(data, i, 7)\n                name, value = self.get_indexed(idx)\n                headers.append((name, value))\n            \n            elif byte & 0x40:  # Literal with indexing\n                idx, i = self.decode_int(data, i, 6)\n                if idx > 0:\n                    name, _ = self.get_indexed(idx)\n                else:\n                    name, i = self.decode_string(data, i)\n                value, i = self.decode_string(data, i)\n                self.add_to_dynamic(name, value)\n                headers.append((name, value))\n            \n            # ... handle other cases\n        \n        return headers\n    \n    def get_indexed(self, idx):\n        if idx <= 61:\n            return STATIC_TABLE[idx]\n        return self.dynamic_table[idx - 62]"
                        },
                        pitfalls: ["Table size eviction", "Index off-by-one errors", "Huffman boundary handling"],
                        concepts: ["Header compression", "Table-based encoding", "Huffman coding"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 3,
                        name: "Stream Management",
                        description: "Handle multiplexed streams with proper state machines.",
                        criteria: ["Stream states (idle, open, closed, etc.)", "Stream ID allocation", "Priority handling", "Concurrent stream limits"],
                        hints: {
                            level1: "Client streams are odd, server (push) are even. Each has 5 states.",
                            level2: "HEADERS opens stream, END_STREAM half-closes. Track dependencies for priority.",
                            level3: "from enum import Enum\n\nclass StreamState(Enum):\n    IDLE = 'idle'\n    RESERVED_LOCAL = 'reserved_local'\n    RESERVED_REMOTE = 'reserved_remote'\n    OPEN = 'open'\n    HALF_CLOSED_LOCAL = 'half_closed_local'\n    HALF_CLOSED_REMOTE = 'half_closed_remote'\n    CLOSED = 'closed'\n\nclass Stream:\n    def __init__(self, stream_id):\n        self.id = stream_id\n        self.state = StreamState.IDLE\n        self.window = 65535  # Initial flow control window\n        self.request_headers = None\n        self.request_body = b''\n        self.response_headers = None\n        self.response_body = b''\n    \n    def recv_headers(self, end_stream):\n        if self.state == StreamState.IDLE:\n            self.state = StreamState.OPEN\n        \n        if end_stream:\n            if self.state == StreamState.OPEN:\n                self.state = StreamState.HALF_CLOSED_REMOTE\n    \n    def send_headers(self, end_stream):\n        if end_stream:\n            if self.state == StreamState.OPEN:\n                self.state = StreamState.HALF_CLOSED_LOCAL\n            elif self.state == StreamState.HALF_CLOSED_REMOTE:\n                self.state = StreamState.CLOSED\n\nclass Connection:\n    def __init__(self):\n        self.streams = {}\n        self.next_server_stream_id = 2\n        self.settings = {...}\n    \n    def get_or_create_stream(self, stream_id):\n        if stream_id not in self.streams:\n            self.streams[stream_id] = Stream(stream_id)\n        return self.streams[stream_id]"
                        },
                        pitfalls: ["State machine violations", "Stream ID exhaustion", "Dependency cycles"],
                        concepts: ["Multiplexing", "State machines", "Concurrency"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 4,
                        name: "Flow Control",
                        description: "Implement connection and stream-level flow control.",
                        criteria: ["Window sizes", "WINDOW_UPDATE frames", "Connection-level windows", "Backpressure handling"],
                        hints: {
                            level1: "Each stream AND connection has a window. Both must have space to send.",
                            level2: "WINDOW_UPDATE increments window. Sender blocks when window hits 0.",
                            level3: "class FlowController:\n    def __init__(self, initial_window=65535):\n        self.connection_window = initial_window\n        self.stream_windows = {}  # stream_id -> window\n        self.pending_data = {}  # stream_id -> [(data, callback)]\n    \n    def can_send(self, stream_id, size):\n        stream_window = self.stream_windows.get(stream_id, 65535)\n        return self.connection_window >= size and stream_window >= size\n    \n    def send_data(self, stream_id, data):\n        size = len(data)\n        if not self.can_send(stream_id, size):\n            # Queue for later\n            if stream_id not in self.pending_data:\n                self.pending_data[stream_id] = []\n            self.pending_data[stream_id].append(data)\n            return False\n        \n        # Decrement windows\n        self.connection_window -= size\n        self.stream_windows[stream_id] -= size\n        return True\n    \n    def recv_window_update(self, stream_id, increment):\n        if stream_id == 0:\n            self.connection_window += increment\n        else:\n            self.stream_windows[stream_id] = \\\n                self.stream_windows.get(stream_id, 65535) + increment\n        \n        # Check if we can now send pending data\n        self.flush_pending()\n    \n    def send_window_update(self, stream_id, increment):\n        frame = Frame(\n            type=WINDOW_UPDATE,\n            flags=0,\n            stream_id=stream_id,\n            payload=increment.to_bytes(4, 'big')\n        )\n        return encode_frame(frame)"
                        },
                        pitfalls: ["Window underflow/overflow", "Deadlocks from exhausted windows", "Forgetting connection window"],
                        concepts: ["Flow control", "Backpressure", "Resource management"],
                        estimatedHours: "9-18"
                    }
                ]
            },

            // APP-DEV - BEGINNER
            "ecommerce-basic": {
                name: "E-commerce Store (Basic)",
                description: "Build a basic online store with product catalog, cart, and checkout. Learn full-stack development patterns.",
                difficulty: "beginner",
                estimatedHours: "20-35",
                prerequisites: ["HTML/CSS/JS", "REST API basics", "Database basics"],
                languages: {
                    recommended: ["JavaScript/Node.js", "Python/Flask", "Ruby/Rails"],
                    also: ["PHP", "Go", "Java/Spring"]
                },
                resources: [
                    { type: "tutorial", name: "Full Stack E-commerce", url: "https://www.freecodecamp.org/news/how-to-build-an-e-commerce-app/" },
                    { type: "video", name: "Build an Online Store", url: "https://www.youtube.com/results?search_query=build+ecommerce+site+tutorial" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Product Catalog",
                        description: "Display products with images, prices, and descriptions.",
                        criteria: ["Product listing page", "Product detail page", "Category filtering", "Search functionality"],
                        hints: {
                            level1: "Start with static JSON data. Add database later.",
                            level2: "Use grid layout for products. Store images as URLs initially.",
                            level3: "// Product schema\nconst productSchema = {\n    id: 'string',\n    name: 'string',\n    description: 'string',\n    price: 'number',  // in cents\n    imageUrl: 'string',\n    category: 'string',\n    stock: 'number'\n};\n\n// API endpoints\napp.get('/api/products', (req, res) => {\n    let products = [...allProducts];\n    \n    if (req.query.category) {\n        products = products.filter(p => \n            p.category === req.query.category\n        );\n    }\n    \n    if (req.query.search) {\n        const term = req.query.search.toLowerCase();\n        products = products.filter(p =>\n            p.name.toLowerCase().includes(term) ||\n            p.description.toLowerCase().includes(term)\n        );\n    }\n    \n    res.json(products);\n});\n\napp.get('/api/products/:id', (req, res) => {\n    const product = allProducts.find(p => p.id === req.params.id);\n    if (!product) return res.status(404).json({error: 'Not found'});\n    res.json(product);\n});"
                        },
                        pitfalls: ["Price precision (use cents)", "Large image handling", "N+1 queries"],
                        concepts: ["CRUD operations", "Filtering/search", "Data modeling"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 2,
                        name: "Shopping Cart",
                        description: "Implement add to cart, update quantities, and remove items.",
                        criteria: ["Add/remove items", "Update quantities", "Cart persistence", "Price calculations"],
                        hints: {
                            level1: "Store cart in session/localStorage for guests, database for logged-in users.",
                            level2: "Cart items reference product IDs. Calculate totals server-side.",
                            level3: "// Cart in localStorage (guest)\nconst Cart = {\n    items: [],\n    \n    load() {\n        const saved = localStorage.getItem('cart');\n        this.items = saved ? JSON.parse(saved) : [];\n    },\n    \n    save() {\n        localStorage.setItem('cart', JSON.stringify(this.items));\n    },\n    \n    add(productId, quantity = 1) {\n        const existing = this.items.find(i => i.productId === productId);\n        if (existing) {\n            existing.quantity += quantity;\n        } else {\n            this.items.push({ productId, quantity });\n        }\n        this.save();\n    },\n    \n    update(productId, quantity) {\n        const item = this.items.find(i => i.productId === productId);\n        if (item) {\n            if (quantity <= 0) {\n                this.remove(productId);\n            } else {\n                item.quantity = quantity;\n            }\n        }\n        this.save();\n    },\n    \n    remove(productId) {\n        this.items = this.items.filter(i => i.productId !== productId);\n        this.save();\n    },\n    \n    async getTotal() {\n        // Fetch current prices from server\n        const response = await fetch('/api/cart/calculate', {\n            method: 'POST',\n            body: JSON.stringify({ items: this.items })\n        });\n        return response.json();\n    }\n};"
                        },
                        pitfalls: ["Price changes between add and checkout", "Stock validation", "Session expiry"],
                        concepts: ["State management", "Session storage", "Data synchronization"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 3,
                        name: "User Authentication",
                        description: "Implement user registration, login, and profile management.",
                        criteria: ["Registration with validation", "Login/logout", "Password hashing", "Session management"],
                        hints: {
                            level1: "Use bcrypt for passwords. Never store plaintext.",
                            level2: "JWT for API auth, session cookies for web. Add email validation.",
                            level3: "// Password hashing\nconst bcrypt = require('bcrypt');\n\nasync function register(email, password) {\n    // Validate\n    if (!email || !password) throw new Error('Required');\n    if (password.length < 8) throw new Error('Password too short');\n    \n    // Check existing\n    const existing = await db.users.findOne({ email });\n    if (existing) throw new Error('Email exists');\n    \n    // Hash password\n    const hash = await bcrypt.hash(password, 12);\n    \n    // Create user\n    const user = await db.users.create({\n        email,\n        passwordHash: hash,\n        createdAt: new Date()\n    });\n    \n    return { id: user.id, email: user.email };\n}\n\nasync function login(email, password) {\n    const user = await db.users.findOne({ email });\n    if (!user) throw new Error('Invalid credentials');\n    \n    const valid = await bcrypt.compare(password, user.passwordHash);\n    if (!valid) throw new Error('Invalid credentials');\n    \n    // Create session/token\n    const token = jwt.sign(\n        { userId: user.id },\n        process.env.JWT_SECRET,\n        { expiresIn: '7d' }\n    );\n    \n    return { token, user: { id: user.id, email: user.email } };\n}"
                        },
                        pitfalls: ["Timing attacks on login", "Password requirements", "Token invalidation"],
                        concepts: ["Authentication", "Password security", "Session management"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 4,
                        name: "Checkout Process",
                        description: "Implement checkout flow with order creation.",
                        criteria: ["Address collection", "Order summary", "Order creation", "Inventory updates"],
                        hints: {
                            level1: "Validate stock before creating order. Use transactions for consistency.",
                            level2: "Create order in 'pending' state. Update to 'confirmed' after payment.",
                            level3: "async function createOrder(userId, cartItems, shippingAddress) {\n    // Start transaction\n    const session = await db.startTransaction();\n    \n    try {\n        // Fetch and validate products\n        const products = await db.products.find({\n            id: { $in: cartItems.map(i => i.productId) }\n        });\n        \n        // Check stock and calculate total\n        let total = 0;\n        const orderItems = [];\n        \n        for (const cartItem of cartItems) {\n            const product = products.find(p => p.id === cartItem.productId);\n            if (!product) throw new Error(`Product ${cartItem.productId} not found`);\n            if (product.stock < cartItem.quantity) {\n                throw new Error(`Insufficient stock for ${product.name}`);\n            }\n            \n            orderItems.push({\n                productId: product.id,\n                name: product.name,\n                price: product.price,\n                quantity: cartItem.quantity\n            });\n            \n            total += product.price * cartItem.quantity;\n            \n            // Decrement stock\n            await db.products.updateOne(\n                { id: product.id },\n                { $inc: { stock: -cartItem.quantity } }\n            );\n        }\n        \n        // Create order\n        const order = await db.orders.create({\n            userId,\n            items: orderItems,\n            total,\n            shippingAddress,\n            status: 'pending',\n            createdAt: new Date()\n        });\n        \n        await session.commit();\n        return order;\n    } catch (error) {\n        await session.rollback();\n        throw error;\n    }\n}"
                        },
                        pitfalls: ["Race conditions on stock", "Abandoned checkouts", "Partial order failures"],
                        concepts: ["Transactions", "Inventory management", "Order lifecycle"],
                        estimatedHours: "7-15"
                    }
                ]
            },

            // APP-DEV - INTERMEDIATE
            "video-streaming": {
                name: "Video Streaming Platform",
                description: "Build a video streaming service with upload, transcoding, and adaptive playback.",
                difficulty: "intermediate",
                estimatedHours: "30-50",
                prerequisites: ["HTTP server", "File handling", "Basic frontend"],
                languages: {
                    recommended: ["Node.js", "Python", "Go"],
                    also: ["Java", "Rust"]
                },
                resources: [
                    { type: "article", name: "HLS Streaming Explained", url: "https://www.cloudflare.com/learning/video/what-is-hls-streaming/" },
                    { type: "tool", name: "FFmpeg Documentation", url: "https://ffmpeg.org/documentation.html" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Video Upload",
                        description: "Handle large video file uploads with progress tracking.",
                        criteria: ["Chunked upload support", "Progress tracking", "File validation", "Storage management"],
                        hints: {
                            level1: "Use multipart upload for large files. Validate MIME types.",
                            level2: "Resumable uploads with chunk tracking. Store metadata separately.",
                            level3: "// Chunked upload endpoint\nconst uploads = new Map();  // uploadId -> chunks\n\napp.post('/api/upload/init', (req, res) => {\n    const { filename, fileSize, mimeType } = req.body;\n    \n    // Validate\n    if (!mimeType.startsWith('video/')) {\n        return res.status(400).json({ error: 'Must be video' });\n    }\n    \n    const uploadId = crypto.randomUUID();\n    const chunkSize = 5 * 1024 * 1024;  // 5MB chunks\n    const totalChunks = Math.ceil(fileSize / chunkSize);\n    \n    uploads.set(uploadId, {\n        filename,\n        fileSize,\n        chunkSize,\n        totalChunks,\n        receivedChunks: new Set(),\n        tempPath: `/tmp/uploads/${uploadId}`\n    });\n    \n    fs.mkdirSync(`/tmp/uploads/${uploadId}`, { recursive: true });\n    \n    res.json({ uploadId, chunkSize, totalChunks });\n});\n\napp.post('/api/upload/:uploadId/chunk/:chunkIndex', async (req, res) => {\n    const { uploadId, chunkIndex } = req.params;\n    const upload = uploads.get(uploadId);\n    \n    if (!upload) return res.status(404).json({ error: 'Upload not found' });\n    \n    // Save chunk\n    const chunkPath = `${upload.tempPath}/chunk_${chunkIndex}`;\n    await pipeline(req, fs.createWriteStream(chunkPath));\n    \n    upload.receivedChunks.add(parseInt(chunkIndex));\n    \n    // Check if complete\n    if (upload.receivedChunks.size === upload.totalChunks) {\n        // Trigger assembly\n        assembleChunks(uploadId);\n    }\n    \n    res.json({\n        received: upload.receivedChunks.size,\n        total: upload.totalChunks\n    });\n});"
                        },
                        pitfalls: ["Memory issues with large files", "Incomplete uploads", "Storage cleanup"],
                        concepts: ["Chunked transfer", "Resumable uploads", "File validation"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 2,
                        name: "Video Transcoding",
                        description: "Convert videos to streaming-friendly formats using FFmpeg.",
                        criteria: ["FFmpeg integration", "Multiple quality levels", "Progress monitoring", "Background processing"],
                        hints: {
                            level1: "Use FFmpeg to create HLS segments. Start with 720p, add more qualities.",
                            level2: "Run transcoding in background job queue. Track progress via FFmpeg output.",
                            level3: "const { spawn } = require('child_process');\n\nasync function transcodeToHLS(inputPath, outputDir, qualities) {\n    // qualities: [{name: '720p', width: 1280, height: 720, bitrate: '3000k'}]\n    \n    for (const q of qualities) {\n        const outputPath = `${outputDir}/${q.name}`;\n        fs.mkdirSync(outputPath, { recursive: true });\n        \n        await new Promise((resolve, reject) => {\n            const ffmpeg = spawn('ffmpeg', [\n                '-i', inputPath,\n                '-vf', `scale=${q.width}:${q.height}`,\n                '-c:v', 'libx264',\n                '-b:v', q.bitrate,\n                '-c:a', 'aac',\n                '-b:a', '128k',\n                '-hls_time', '10',\n                '-hls_list_size', '0',\n                '-hls_segment_filename', `${outputPath}/segment_%03d.ts`,\n                `${outputPath}/playlist.m3u8`\n            ]);\n            \n            ffmpeg.stderr.on('data', (data) => {\n                // Parse progress from FFmpeg output\n                const match = data.toString().match(/time=(\\d+:\\d+:\\d+)/);\n                if (match) {\n                    updateProgress(inputPath, q.name, match[1]);\n                }\n            });\n            \n            ffmpeg.on('close', (code) => {\n                code === 0 ? resolve() : reject(new Error(`FFmpeg exit ${code}`));\n            });\n        });\n    }\n    \n    // Create master playlist\n    createMasterPlaylist(outputDir, qualities);\n}\n\nfunction createMasterPlaylist(outputDir, qualities) {\n    let content = '#EXTM3U\\n';\n    \n    for (const q of qualities) {\n        content += `#EXT-X-STREAM-INF:BANDWIDTH=${parseInt(q.bitrate)*1000},RESOLUTION=${q.width}x${q.height}\\n`;\n        content += `${q.name}/playlist.m3u8\\n`;\n    }\n    \n    fs.writeFileSync(`${outputDir}/master.m3u8`, content);\n}"
                        },
                        pitfalls: ["FFmpeg memory usage", "Codec compatibility", "Interrupted transcoding"],
                        concepts: ["Video codecs", "HLS format", "Background jobs"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 3,
                        name: "Adaptive Streaming",
                        description: "Serve HLS streams with quality adaptation.",
                        criteria: ["HLS manifest serving", "Segment serving", "Byte-range requests", "CDN-friendly headers"],
                        hints: {
                            level1: "Serve .m3u8 as application/vnd.apple.mpegurl, .ts as video/mp2t.",
                            level2: "Support Range requests for seeking. Set proper cache headers.",
                            level3: "// Serve HLS content\napp.get('/api/videos/:videoId/stream/*', async (req, res) => {\n    const { videoId } = req.params;\n    const filePath = req.params[0];  // e.g., 'master.m3u8' or '720p/segment_001.ts'\n    \n    const fullPath = path.join(VIDEO_DIR, videoId, filePath);\n    \n    if (!fs.existsSync(fullPath)) {\n        return res.status(404).send('Not found');\n    }\n    \n    // Set content type\n    const ext = path.extname(filePath);\n    const contentTypes = {\n        '.m3u8': 'application/vnd.apple.mpegurl',\n        '.ts': 'video/mp2t'\n    };\n    res.setHeader('Content-Type', contentTypes[ext] || 'application/octet-stream');\n    \n    // Cache headers\n    if (ext === '.m3u8') {\n        res.setHeader('Cache-Control', 'no-cache');  // Playlist can change\n    } else {\n        res.setHeader('Cache-Control', 'public, max-age=31536000');  // Segments immutable\n    }\n    \n    // Handle Range requests for segments\n    const stat = fs.statSync(fullPath);\n    const range = req.headers.range;\n    \n    if (range && ext === '.ts') {\n        const parts = range.replace(/bytes=/, '').split('-');\n        const start = parseInt(parts[0], 10);\n        const end = parts[1] ? parseInt(parts[1], 10) : stat.size - 1;\n        \n        res.status(206);\n        res.setHeader('Content-Range', `bytes ${start}-${end}/${stat.size}`);\n        res.setHeader('Content-Length', end - start + 1);\n        \n        fs.createReadStream(fullPath, { start, end }).pipe(res);\n    } else {\n        res.setHeader('Content-Length', stat.size);\n        fs.createReadStream(fullPath).pipe(res);\n    }\n});"
                        },
                        pitfalls: ["CORS for cross-origin players", "Playlist caching issues", "Seeking accuracy"],
                        concepts: ["HLS protocol", "HTTP range requests", "Caching strategies"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 4,
                        name: "Video Player Integration",
                        description: "Build frontend player with quality switching and progress tracking.",
                        criteria: ["HLS.js integration", "Quality selector", "Progress bar/seeking", "Playback analytics"],
                        hints: {
                            level1: "Use HLS.js library for non-Safari browsers. Safari has native HLS.",
                            level2: "Track watch progress for resume. Send analytics events.",
                            level3: "// Video player component\nclass VideoPlayer {\n    constructor(container, videoId) {\n        this.video = document.createElement('video');\n        this.video.controls = true;\n        container.appendChild(this.video);\n        \n        this.videoId = videoId;\n        this.hls = null;\n        this.qualities = [];\n        \n        this.setupHLS();\n        this.setupAnalytics();\n    }\n    \n    setupHLS() {\n        const src = `/api/videos/${this.videoId}/stream/master.m3u8`;\n        \n        if (this.video.canPlayType('application/vnd.apple.mpegurl')) {\n            // Safari native HLS\n            this.video.src = src;\n        } else if (Hls.isSupported()) {\n            this.hls = new Hls({\n                enableWorker: true,\n                lowLatencyMode: false\n            });\n            \n            this.hls.loadSource(src);\n            this.hls.attachMedia(this.video);\n            \n            this.hls.on(Hls.Events.MANIFEST_PARSED, (event, data) => {\n                this.qualities = data.levels.map((level, i) => ({\n                    index: i,\n                    height: level.height,\n                    bitrate: level.bitrate\n                }));\n                this.renderQualitySelector();\n            });\n        }\n    }\n    \n    setQuality(levelIndex) {\n        if (this.hls) {\n            this.hls.currentLevel = levelIndex;  // -1 for auto\n        }\n    }\n    \n    setupAnalytics() {\n        let lastReport = 0;\n        \n        this.video.addEventListener('timeupdate', () => {\n            const now = Date.now();\n            if (now - lastReport > 10000) {  // Every 10 seconds\n                this.reportProgress(this.video.currentTime);\n                lastReport = now;\n            }\n        });\n        \n        this.video.addEventListener('ended', () => {\n            this.reportComplete();\n        });\n    }\n    \n    async reportProgress(time) {\n        await fetch(`/api/videos/${this.videoId}/progress`, {\n            method: 'POST',\n            body: JSON.stringify({ time })\n        });\n    }\n}"
                        },
                        pitfalls: ["Browser compatibility", "Memory leaks on unmount", "Bandwidth estimation"],
                        concepts: ["Adaptive bitrate", "Media APIs", "Analytics"],
                        estimatedHours: "11-20"
                    }
                ]
            },

            // DISTRIBUTED - INTERMEDIATE
            "replicated-log": {
                name: "Replicated Log",
                description: "Build a replicated append-only log. Learn distributed replication, consistency, and failure handling.",
                difficulty: "intermediate",
                estimatedHours: "15-25",
                prerequisites: ["RPC basics", "Networking", "Log-based storage"],
                languages: {
                    recommended: ["Go", "Rust", "Java"],
                    also: ["Python", "C"]
                },
                resources: [
                    { type: "paper", name: "Viewstamped Replication", url: "https://pmg.csail.mit.edu/papers/vr-revisited.pdf" },
                    { type: "blog", name: "Distributed Log 101", url: "https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Log Storage",
                        description: "Implement an append-only log with indexing.",
                        criteria: ["Append entries with sequence numbers", "Read entries by index", "Persist to disk", "Handle log compaction"],
                        hints: {
                            level1: "Each entry has monotonic sequence number. Store entries sequentially in file.",
                            level2: "Use index file for O(1) lookups. mmap for efficient reads.",
                            level3: "import struct\nimport os\n\nclass Log:\n    HEADER_SIZE = 12  # seq(8) + len(4)\n    \n    def __init__(self, path):\n        self.path = path\n        self.data_file = open(f'{path}/data', 'ab+')\n        self.index = {}  # seq -> file_offset\n        self.next_seq = 0\n        self._rebuild_index()\n    \n    def _rebuild_index(self):\n        self.data_file.seek(0)\n        offset = 0\n        while True:\n            header = self.data_file.read(self.HEADER_SIZE)\n            if len(header) < self.HEADER_SIZE:\n                break\n            seq, length = struct.unpack('>QI', header)\n            self.index[seq] = offset\n            self.next_seq = seq + 1\n            offset += self.HEADER_SIZE + length\n            self.data_file.seek(offset)\n    \n    def append(self, data: bytes) -> int:\n        seq = self.next_seq\n        self.next_seq += 1\n        \n        offset = self.data_file.seek(0, 2)  # EOF\n        header = struct.pack('>QI', seq, len(data))\n        self.data_file.write(header + data)\n        self.data_file.flush()\n        os.fsync(self.data_file.fileno())\n        \n        self.index[seq] = offset\n        return seq\n    \n    def read(self, seq: int) -> bytes:\n        if seq not in self.index:\n            return None\n        offset = self.index[seq]\n        self.data_file.seek(offset)\n        header = self.data_file.read(self.HEADER_SIZE)\n        _, length = struct.unpack('>QI', header)\n        return self.data_file.read(length)"
                        },
                        pitfalls: ["Partial writes on crash", "Index corruption", "File handle limits"],
                        concepts: ["Append-only logs", "Sequence numbers", "Durability"],
                        estimatedHours: "3-5"
                    },
                    {
                        id: 2,
                        name: "Replication Protocol",
                        description: "Implement primary-backup replication with follower sync.",
                        criteria: ["Primary accepts writes", "Replicate to followers", "Acknowledge after quorum", "Handle follower lag"],
                        hints: {
                            level1: "Primary assigns sequence, sends to followers. Wait for majority before ack.",
                            level2: "Followers send their last seq. Primary sends missing entries.",
                            level3: "class Primary:\n    def __init__(self, log, followers):\n        self.log = log\n        self.followers = followers  # List of follower RPC clients\n        self.follower_progress = {f.id: 0 for f in followers}\n    \n    async def append(self, data: bytes) -> int:\n        # Append locally\n        seq = self.log.append(data)\n        \n        # Replicate to followers\n        acks = 1  # Self\n        tasks = []\n        for follower in self.followers:\n            tasks.append(self._replicate_to(follower, seq, data))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        acks += sum(1 for r in results if r is True)\n        \n        # Check quorum (majority)\n        quorum = (len(self.followers) + 1) // 2 + 1\n        if acks >= quorum:\n            return seq\n        raise ReplicationError('Failed to reach quorum')\n    \n    async def _replicate_to(self, follower, seq, data):\n        try:\n            # Check if follower is behind\n            follower_seq = self.follower_progress[follower.id]\n            if follower_seq < seq - 1:\n                # Send missing entries\n                for s in range(follower_seq + 1, seq):\n                    entry = self.log.read(s)\n                    await follower.append(s, entry)\n            \n            # Send current entry\n            await follower.append(seq, data)\n            self.follower_progress[follower.id] = seq\n            return True\n        except Exception as e:\n            return False\n\nclass Follower:\n    def __init__(self, log):\n        self.log = log\n    \n    def append(self, seq, data):\n        expected = self.log.next_seq\n        if seq != expected:\n            raise OutOfOrderError(f'Expected {expected}, got {seq}')\n        self.log.append(data)\n        return True"
                        },
                        pitfalls: ["Split brain without leader election", "Replication lag", "Network partitions"],
                        concepts: ["Primary-backup", "Quorum", "Replication lag"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 3,
                        name: "Failure Detection",
                        description: "Detect node failures and handle recovery.",
                        criteria: ["Heartbeat mechanism", "Timeout-based detection", "Follower catch-up on recovery", "Handle primary failure notification"],
                        hints: {
                            level1: "Followers send periodic heartbeats. Primary tracks last seen time.",
                            level2: "On recovery, follower requests entries from last known seq.",
                            level3: "class FailureDetector:\n    def __init__(self, timeout_ms=5000):\n        self.timeout = timeout_ms / 1000\n        self.last_heartbeat = {}  # node_id -> timestamp\n        self.suspected = set()\n    \n    def heartbeat_received(self, node_id):\n        self.last_heartbeat[node_id] = time.time()\n        if node_id in self.suspected:\n            self.suspected.remove(node_id)\n            self.on_node_recovered(node_id)\n    \n    def check_timeouts(self):\n        now = time.time()\n        for node_id, last in self.last_heartbeat.items():\n            if now - last > self.timeout and node_id not in self.suspected:\n                self.suspected.add(node_id)\n                self.on_node_suspected(node_id)\n\nclass RecoveringFollower:\n    def __init__(self, log, primary_client):\n        self.log = log\n        self.primary = primary_client\n    \n    async def catch_up(self):\n        # Get our last sequence\n        my_last = self.log.next_seq - 1 if self.log.next_seq > 0 else -1\n        \n        # Request entries from primary\n        entries = await self.primary.get_entries_since(my_last + 1)\n        \n        for seq, data in entries:\n            if seq != self.log.next_seq:\n                raise ConsistencyError('Gap in log')\n            self.log.append(data)\n        \n        print(f'Caught up to seq {self.log.next_seq - 1}')"
                        },
                        pitfalls: ["False positives in detection", "Thundering herd on recovery", "Consistency after catch-up"],
                        concepts: ["Failure detection", "Heartbeats", "Recovery protocols"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 4,
                        name: "Client Interface",
                        description: "Implement a client that handles primary discovery and failover.",
                        criteria: ["Find current primary", "Retry on failure", "Read from followers (optional)", "Consistent reads"],
                        hints: {
                            level1: "Client caches primary address. On error, re-discover.",
                            level2: "For reads, can go to any replica if staleness is acceptable.",
                            level3: "class LogClient:\n    def __init__(self, cluster_nodes):\n        self.nodes = cluster_nodes\n        self.primary = None\n        self.primary_addr = None\n    \n    async def discover_primary(self):\n        for addr in self.nodes:\n            try:\n                client = await connect(addr)\n                info = await client.get_info()\n                if info['role'] == 'primary':\n                    self.primary = client\n                    self.primary_addr = addr\n                    return\n                elif info.get('primary_addr'):\n                    # Follower knows who primary is\n                    self.primary = await connect(info['primary_addr'])\n                    self.primary_addr = info['primary_addr']\n                    return\n            except Exception:\n                continue\n        raise NoPrimaryError('Cannot find primary')\n    \n    async def append(self, data: bytes, retries=3) -> int:\n        for attempt in range(retries):\n            try:\n                if not self.primary:\n                    await self.discover_primary()\n                return await self.primary.append(data)\n            except (ConnectionError, PrimaryChangedError):\n                self.primary = None\n                if attempt == retries - 1:\n                    raise\n    \n    async def read(self, seq: int, allow_stale=False) -> bytes:\n        if allow_stale:\n            # Read from any node\n            for addr in self.nodes:\n                try:\n                    client = await connect(addr)\n                    return await client.read(seq)\n                except Exception:\n                    continue\n        \n        # Read from primary for consistency\n        if not self.primary:\n            await self.discover_primary()\n        return await self.primary.read(seq)"
                        },
                        pitfalls: ["Stale primary cache", "Read-your-writes consistency", "Infinite retry loops"],
                        concepts: ["Service discovery", "Client-side failover", "Consistency levels"],
                        estimatedHours: "3-6"
                    }
                ]
            },

            // SYSTEMS - ADVANCED
            "virtual-memory-sim": {
                name: "Virtual Memory Simulator",
                description: "Simulate virtual memory with page tables and TLB. Learn memory management and address translation.",
                difficulty: "advanced",
                estimatedHours: "20-30",
                prerequisites: ["Memory concepts", "Binary/hex", "Data structures"],
                languages: {
                    recommended: ["C", "Rust", "Python"],
                    also: ["Go", "Java"]
                },
                resources: [
                    { type: "book", name: "OSTEP - Address Translation", url: "https://pages.cs.wisc.edu/~remzi/OSTEP/vm-mechanism.pdf" },
                    { type: "book", name: "OSTEP - Paging", url: "https://pages.cs.wisc.edu/~remzi/OSTEP/vm-paging.pdf" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Page Table",
                        description: "Implement single-level page table with address translation.",
                        criteria: ["Virtual to physical translation", "Page table entries with flags", "Handle page faults", "Valid/invalid bits"],
                        hints: {
                            level1: "Split virtual address into page number and offset. Look up frame in table.",
                            level2: "PTE has frame number + flags (valid, dirty, accessed, protection).",
                            level3: "class PageTableEntry:\n    def __init__(self):\n        self.valid = False\n        self.frame_number = 0\n        self.dirty = False\n        self.accessed = False\n        self.protection = 0b111  # RWX\n\nclass PageTable:\n    def __init__(self, page_bits=12, addr_bits=32):\n        self.page_size = 1 << page_bits\n        self.offset_mask = self.page_size - 1\n        self.page_bits = page_bits\n        self.num_pages = 1 << (addr_bits - page_bits)\n        self.entries = [PageTableEntry() for _ in range(self.num_pages)]\n    \n    def translate(self, virtual_addr, write=False):\n        page_num = virtual_addr >> self.page_bits\n        offset = virtual_addr & self.offset_mask\n        \n        if page_num >= self.num_pages:\n            raise SegmentationFault(f'Invalid page {page_num}')\n        \n        pte = self.entries[page_num]\n        \n        if not pte.valid:\n            raise PageFault(page_num)\n        \n        # Check permissions\n        if write and not (pte.protection & 0b010):\n            raise ProtectionFault('Write not allowed')\n        \n        # Update accessed/dirty bits\n        pte.accessed = True\n        if write:\n            pte.dirty = True\n        \n        physical_addr = (pte.frame_number << self.page_bits) | offset\n        return physical_addr\n    \n    def map_page(self, page_num, frame_num, protection=0b111):\n        pte = self.entries[page_num]\n        pte.valid = True\n        pte.frame_number = frame_num\n        pte.protection = protection\n        pte.dirty = False\n        pte.accessed = False"
                        },
                        pitfalls: ["Off-by-one in bit shifting", "Forgetting offset", "Protection check order"],
                        concepts: ["Address translation", "Page tables", "Memory protection"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 2,
                        name: "TLB",
                        description: "Add Translation Lookaside Buffer for faster translations.",
                        criteria: ["TLB lookup before page table", "TLB miss handling", "TLB eviction (LRU/random)", "TLB flush on context switch"],
                        hints: {
                            level1: "TLB is small cache of recent translations. Check TLB first.",
                            level2: "On miss, walk page table and add to TLB. Track LRU for eviction.",
                            level3: "from collections import OrderedDict\n\nclass TLB:\n    def __init__(self, size=64):\n        self.size = size\n        self.entries = OrderedDict()  # page_num -> (frame_num, protection)\n        self.hits = 0\n        self.misses = 0\n    \n    def lookup(self, page_num):\n        if page_num in self.entries:\n            self.hits += 1\n            # Move to end (most recently used)\n            self.entries.move_to_end(page_num)\n            return self.entries[page_num]\n        self.misses += 1\n        return None\n    \n    def insert(self, page_num, frame_num, protection):\n        if page_num in self.entries:\n            del self.entries[page_num]\n        elif len(self.entries) >= self.size:\n            # Evict LRU (first item)\n            self.entries.popitem(last=False)\n        self.entries[page_num] = (frame_num, protection)\n    \n    def invalidate(self, page_num):\n        if page_num in self.entries:\n            del self.entries[page_num]\n    \n    def flush(self):\n        self.entries.clear()\n    \n    def hit_rate(self):\n        total = self.hits + self.misses\n        return self.hits / total if total > 0 else 0\n\nclass MMU:\n    def __init__(self, page_table):\n        self.page_table = page_table\n        self.tlb = TLB()\n    \n    def translate(self, virtual_addr, write=False):\n        page_num = virtual_addr >> self.page_table.page_bits\n        offset = virtual_addr & self.page_table.offset_mask\n        \n        # TLB lookup\n        tlb_entry = self.tlb.lookup(page_num)\n        if tlb_entry:\n            frame_num, protection = tlb_entry\n            if write and not (protection & 0b010):\n                raise ProtectionFault('Write not allowed')\n            return (frame_num << self.page_table.page_bits) | offset\n        \n        # TLB miss - walk page table\n        physical = self.page_table.translate(virtual_addr, write)\n        \n        # Add to TLB\n        pte = self.page_table.entries[page_num]\n        self.tlb.insert(page_num, pte.frame_number, pte.protection)\n        \n        return physical"
                        },
                        pitfalls: ["TLB coherency with page table", "Context switch handling", "ASID management"],
                        concepts: ["Caching", "Locality", "TLB shootdown"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 3,
                        name: "Multi-level Page Tables",
                        description: "Implement hierarchical page tables to save memory.",
                        criteria: ["Two or three-level tables", "Sparse address space handling", "On-demand table allocation", "Page table walks"],
                        hints: {
                            level1: "Split page number into multiple indices. Each level points to next.",
                            level2: "Only allocate tables for used regions. Check valid bit at each level.",
                            level3: "class MultiLevelPageTable:\n    def __init__(self, levels=2, bits_per_level=10, offset_bits=12):\n        self.levels = levels\n        self.bits_per_level = bits_per_level\n        self.offset_bits = offset_bits\n        self.entries_per_table = 1 << bits_per_level\n        self.offset_mask = (1 << offset_bits) - 1\n        \n        # Root table (always allocated)\n        self.root = self._new_table()\n        self.tables_allocated = 1\n    \n    def _new_table(self):\n        return [None] * self.entries_per_table\n    \n    def _extract_indices(self, virtual_addr):\n        indices = []\n        addr = virtual_addr >> self.offset_bits\n        for _ in range(self.levels):\n            indices.append(addr & (self.entries_per_table - 1))\n            addr >>= self.bits_per_level\n        return list(reversed(indices))\n    \n    def translate(self, virtual_addr):\n        indices = self._extract_indices(virtual_addr)\n        offset = virtual_addr & self.offset_mask\n        \n        table = self.root\n        for i, idx in enumerate(indices[:-1]):\n            entry = table[idx]\n            if entry is None:\n                raise PageFault(virtual_addr)\n            table = entry  # Next level table\n        \n        # Last level is the PTE\n        pte = table[indices[-1]]\n        if pte is None or not pte.valid:\n            raise PageFault(virtual_addr)\n        \n        return (pte.frame_number << self.offset_bits) | offset\n    \n    def map_page(self, virtual_addr, frame_num):\n        indices = self._extract_indices(virtual_addr)\n        \n        table = self.root\n        for i, idx in enumerate(indices[:-1]):\n            if table[idx] is None:\n                table[idx] = self._new_table()\n                self.tables_allocated += 1\n            table = table[idx]\n        \n        # Create PTE at leaf\n        pte = PageTableEntry()\n        pte.valid = True\n        pte.frame_number = frame_num\n        table[indices[-1]] = pte"
                        },
                        pitfalls: ["Index extraction order", "Table pointer vs PTE confusion", "Memory overhead calculation"],
                        concepts: ["Hierarchical structures", "Sparse data", "Memory efficiency"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 4,
                        name: "Page Replacement",
                        description: "Implement page replacement algorithms when memory is full.",
                        criteria: ["FIFO replacement", "LRU replacement", "Clock algorithm", "Track working set"],
                        hints: {
                            level1: "FIFO: queue of page numbers, evict oldest. LRU: track last access time.",
                            level2: "Clock: circular list with reference bits. Give second chance.",
                            level3: "class ClockPageReplacement:\n    def __init__(self, num_frames):\n        self.num_frames = num_frames\n        self.frames = [None] * num_frames  # page_num in each frame\n        self.reference_bits = [False] * num_frames\n        self.clock_hand = 0\n        self.page_to_frame = {}  # page_num -> frame_num\n    \n    def access(self, page_num):\n        if page_num in self.page_to_frame:\n            # Page hit - set reference bit\n            frame = self.page_to_frame[page_num]\n            self.reference_bits[frame] = True\n            return frame, None  # No eviction\n        \n        # Page fault - find frame to use\n        frame, evicted = self._find_victim()\n        \n        # Update mappings\n        if evicted is not None:\n            del self.page_to_frame[evicted]\n        \n        self.frames[frame] = page_num\n        self.reference_bits[frame] = True\n        self.page_to_frame[page_num] = frame\n        \n        return frame, evicted\n    \n    def _find_victim(self):\n        # Check for empty frame first\n        for i in range(self.num_frames):\n            if self.frames[i] is None:\n                return i, None\n        \n        # Clock algorithm - find victim\n        while True:\n            if not self.reference_bits[self.clock_hand]:\n                # Found victim\n                evicted = self.frames[self.clock_hand]\n                frame = self.clock_hand\n                self.clock_hand = (self.clock_hand + 1) % self.num_frames\n                return frame, evicted\n            \n            # Give second chance\n            self.reference_bits[self.clock_hand] = False\n            self.clock_hand = (self.clock_hand + 1) % self.num_frames"
                        },
                        pitfalls: ["Belady's anomaly with FIFO", "Dirty page handling", "Thrashing"],
                        concepts: ["Page replacement", "Working set", "Thrashing"],
                        estimatedHours: "7-10"
                    }
                ]
            },

            // COMPILERS - ADVANCED
            "wasm-emitter": {
                name: "WebAssembly Emitter",
                description: "Compile a simple language to WebAssembly binary. Learn WASM format and code generation.",
                difficulty: "advanced",
                estimatedHours: "25-40",
                prerequisites: ["AST building", "Binary formats", "Stack machines"],
                languages: {
                    recommended: ["Rust", "Go", "TypeScript"],
                    also: ["Python", "C"]
                },
                resources: [
                    { type: "spec", name: "WebAssembly Specification", url: "https://webassembly.github.io/spec/core/" },
                    { type: "tool", name: "WebAssembly Binary Toolkit", url: "https://github.com/WebAssembly/wabt" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "WASM Binary Format",
                        description: "Understand and emit valid WASM module structure.",
                        criteria: ["Magic number and version", "Section encoding", "LEB128 integers", "Type section"],
                        hints: {
                            level1: "WASM starts with magic (0x00 0x61 0x73 0x6d) + version (0x01 0x00 0x00 0x00).",
                            level2: "Sections have ID, size (LEB128), content. Type section defines function signatures.",
                            level3: "def leb128_unsigned(n):\n    result = []\n    while True:\n        byte = n & 0x7f\n        n >>= 7\n        if n != 0:\n            byte |= 0x80\n        result.append(byte)\n        if n == 0:\n            break\n    return bytes(result)\n\ndef leb128_signed(n):\n    result = []\n    while True:\n        byte = n & 0x7f\n        n >>= 7\n        if (n == 0 and byte & 0x40 == 0) or (n == -1 and byte & 0x40):\n            result.append(byte)\n            break\n        result.append(byte | 0x80)\n    return bytes(result)\n\nclass WasmModule:\n    MAGIC = b'\\x00asm'\n    VERSION = b'\\x01\\x00\\x00\\x00'\n    \n    # Section IDs\n    TYPE_SECTION = 1\n    FUNC_SECTION = 3\n    EXPORT_SECTION = 7\n    CODE_SECTION = 10\n    \n    def __init__(self):\n        self.types = []      # Function signatures\n        self.functions = []  # Function type indices\n        self.exports = []    # Exported items\n        self.code = []       # Function bodies\n    \n    def add_function_type(self, params, results):\n        # params/results are lists of value types (0x7f=i32, 0x7e=i64, etc.)\n        sig = bytes([0x60, len(params)] + params + [len(results)] + results)\n        if sig not in self.types:\n            self.types.append(sig)\n        return self.types.index(sig)\n    \n    def encode(self):\n        output = self.MAGIC + self.VERSION\n        \n        # Type section\n        if self.types:\n            content = bytes([len(self.types)]) + b''.join(self.types)\n            output += self._section(self.TYPE_SECTION, content)\n        \n        # ... other sections\n        return output\n    \n    def _section(self, id, content):\n        return bytes([id]) + leb128_unsigned(len(content)) + content"
                        },
                        pitfalls: ["LEB128 edge cases", "Section ordering", "Size calculation"],
                        concepts: ["Binary formats", "Variable-length encoding", "Module structure"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 2,
                        name: "Expression Compilation",
                        description: "Compile arithmetic expressions to WASM instructions.",
                        criteria: ["i32 operations", "Stack-based code generation", "Local variables", "Constants"],
                        hints: {
                            level1: "WASM is stack-based. i32.const pushes, i32.add pops 2 pushes 1.",
                            level2: "local.get/set for variables. Locals declared at function start.",
                            level3: "# WASM opcodes\nI32_CONST = 0x41\nI32_ADD = 0x6a\nI32_SUB = 0x6b\nI32_MUL = 0x6c\nI32_DIV_S = 0x6d\nLOCAL_GET = 0x20\nLOCAL_SET = 0x21\nLOCAL_TEE = 0x22  # Set and keep on stack\n\nclass CodeGen:\n    def __init__(self):\n        self.code = []\n        self.locals = {}  # name -> index\n        self.local_types = []  # type of each local\n    \n    def compile_expr(self, node):\n        if isinstance(node, NumberLit):\n            self.code.append(I32_CONST)\n            self.code.extend(leb128_signed(node.value))\n        \n        elif isinstance(node, BinaryOp):\n            self.compile_expr(node.left)\n            self.compile_expr(node.right)\n            ops = {'+': I32_ADD, '-': I32_SUB, '*': I32_MUL, '/': I32_DIV_S}\n            self.code.append(ops[node.op])\n        \n        elif isinstance(node, Variable):\n            idx = self.locals[node.name]\n            self.code.append(LOCAL_GET)\n            self.code.extend(leb128_unsigned(idx))\n        \n        elif isinstance(node, Assignment):\n            self.compile_expr(node.value)\n            idx = self.locals[node.name]\n            self.code.append(LOCAL_TEE)  # Keep value on stack\n            self.code.extend(leb128_unsigned(idx))\n    \n    def declare_local(self, name, type=0x7f):  # 0x7f = i32\n        idx = len(self.locals)\n        self.locals[name] = idx\n        self.local_types.append(type)\n        return idx"
                        },
                        pitfalls: ["Operand order for non-commutative ops", "Signed vs unsigned", "Stack imbalance"],
                        concepts: ["Stack machines", "Instruction encoding", "Local variables"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 3,
                        name: "Control Flow",
                        description: "Compile if/else and loops to WASM structured control flow.",
                        criteria: ["Block/end structure", "If/else/end", "Loop/br_if", "Break to labels"],
                        hints: {
                            level1: "WASM has structured control: block/loop/if all need end. br jumps to enclosing block.",
                            level2: "br 0 exits innermost block. loop: br goes back to start. block: br goes to end.",
                            level3: "BLOCK = 0x02\nLOOP = 0x03\nIF = 0x04\nELSE = 0x05\nEND = 0x0b\nBR = 0x0c\nBR_IF = 0x0d\nRETURN = 0x0f\nVOID = 0x40  # Empty block type\n\nclass CodeGen:\n    def __init__(self):\n        self.code = []\n        self.block_depth = 0\n    \n    def compile_if(self, node):\n        # Condition leaves i32 on stack\n        self.compile_expr(node.condition)\n        \n        self.code.append(IF)\n        self.code.append(VOID)  # No result type\n        self.block_depth += 1\n        \n        self.compile_stmt(node.then_branch)\n        \n        if node.else_branch:\n            self.code.append(ELSE)\n            self.compile_stmt(node.else_branch)\n        \n        self.code.append(END)\n        self.block_depth -= 1\n    \n    def compile_while(self, node):\n        # block { loop { if !cond br 1; body; br 0 } }\n        self.code.append(BLOCK)\n        self.code.append(VOID)\n        self.block_depth += 1\n        \n        self.code.append(LOOP)\n        self.code.append(VOID)\n        self.block_depth += 1\n        \n        # Check condition, break if false\n        self.compile_expr(node.condition)\n        self.code.append(I32_EQZ)  # Invert\n        self.code.append(BR_IF)\n        self.code.extend(leb128_unsigned(1))  # Break to outer block\n        \n        # Body\n        self.compile_stmt(node.body)\n        \n        # Loop back\n        self.code.append(BR)\n        self.code.extend(leb128_unsigned(0))  # Back to loop start\n        \n        self.code.append(END)  # End loop\n        self.block_depth -= 1\n        self.code.append(END)  # End block\n        self.block_depth -= 1"
                        },
                        pitfalls: ["Label depth calculation", "Block type annotations", "Unreachable code after br"],
                        concepts: ["Structured control flow", "Label indices", "Block nesting"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 4,
                        name: "Functions and Exports",
                        description: "Compile function definitions and export them.",
                        criteria: ["Function section", "Code section format", "Export section", "Function calls"],
                        hints: {
                            level1: "Separate sections: Type (signatures), Function (type refs), Code (bodies), Export.",
                            level2: "Code section encodes local counts then instructions. Functions called by index.",
                            level3: "CALL = 0x10\n\nclass WasmCompiler:\n    def __init__(self):\n        self.module = WasmModule()\n        self.function_indices = {}  # name -> index\n    \n    def compile_function(self, func):\n        # Get/create type signature\n        param_types = [0x7f] * len(func.params)  # All i32\n        result_types = [0x7f] if func.returns else []\n        type_idx = self.module.add_function_type(param_types, result_types)\n        \n        # Register function\n        func_idx = len(self.module.functions)\n        self.module.functions.append(type_idx)\n        self.function_indices[func.name] = func_idx\n        \n        # Compile body\n        codegen = CodeGen()\n        for i, param in enumerate(func.params):\n            codegen.locals[param] = i\n        \n        for stmt in func.body:\n            codegen.compile_stmt(stmt)\n        \n        codegen.code.append(END)\n        \n        # Encode function body\n        # Local declarations: count of (count, type) pairs\n        if codegen.local_types:\n            # Group consecutive same types\n            local_decls = self._group_locals(codegen.local_types[len(func.params):])\n        else:\n            local_decls = []\n        \n        body = bytes([len(local_decls)])\n        for count, type in local_decls:\n            body += leb128_unsigned(count) + bytes([type])\n        body += bytes(codegen.code)\n        \n        # Wrap with size\n        self.module.code.append(leb128_unsigned(len(body)) + body)\n        \n        return func_idx\n    \n    def export_function(self, name, func_name):\n        idx = self.function_indices[func_name]\n        # name as UTF-8 string + kind (0=func) + index\n        name_bytes = name.encode('utf-8')\n        self.module.exports.append(\n            leb128_unsigned(len(name_bytes)) + name_bytes + bytes([0]) + leb128_unsigned(idx)\n        )"
                        },
                        pitfalls: ["Parameter vs local indices", "Body size encoding", "Export name encoding"],
                        concepts: ["Module linking", "Function ABI", "Export mechanisms"],
                        estimatedHours: "9-14"
                    }
                ]
            },

            // CS-FUNDAMENTALS - EXPERT
            "build-btree": {
                name: "Build Your Own B-tree",
                description: "Implement a disk-friendly B-tree for database indexing. Learn balanced trees and disk I/O optimization.",
                difficulty: "expert",
                estimatedHours: "30-50",
                prerequisites: ["Binary search trees", "File I/O", "Database concepts"],
                languages: {
                    recommended: ["C", "Rust", "Go"],
                    also: ["Java", "Python"]
                },
                resources: [
                    { type: "paper", name: "The Ubiquitous B-Tree", url: "https://dl.acm.org/doi/10.1145/356770.356776" },
                    { type: "book", name: "Database Internals - Ch 2-4", url: "https://www.databass.dev/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Node Structure",
                        description: "Design and implement B-tree node layout for disk storage.",
                        criteria: ["Fixed-size pages", "Key and child pointer layout", "Leaf vs internal nodes", "Serialization/deserialization"],
                        hints: {
                            level1: "Each node is one disk page (e.g., 4KB). Store keys, values (leaf), child pointers (internal).",
                            level2: "Use fixed-size slots. Track number of keys. Leaf nodes have no children.",
                            level3: "import struct\n\nPAGE_SIZE = 4096\nMAX_KEY_SIZE = 256\nMAX_VALUE_SIZE = 256\n\nclass BTreeNode:\n    # Header: is_leaf(1) + num_keys(2) + parent_page(4) = 7 bytes\n    HEADER_SIZE = 7\n    \n    def __init__(self, page_id, is_leaf=True):\n        self.page_id = page_id\n        self.is_leaf = is_leaf\n        self.keys = []\n        self.values = []     # For leaf nodes\n        self.children = []   # For internal nodes (page IDs)\n        self.parent = None\n    \n    @classmethod\n    def max_keys(cls, is_leaf):\n        if is_leaf:\n            # Each entry: key_len(2) + key + val_len(2) + val\n            entry_size = 2 + MAX_KEY_SIZE + 2 + MAX_VALUE_SIZE\n        else:\n            # Each entry: key_len(2) + key + child_ptr(4)\n            entry_size = 2 + MAX_KEY_SIZE + 4\n        return (PAGE_SIZE - cls.HEADER_SIZE) // entry_size\n    \n    def serialize(self):\n        data = struct.pack('>BHI', \n            1 if self.is_leaf else 0,\n            len(self.keys),\n            self.parent or 0\n        )\n        \n        for i, key in enumerate(self.keys):\n            key_bytes = key.encode('utf-8')[:MAX_KEY_SIZE]\n            data += struct.pack('>H', len(key_bytes)) + key_bytes\n            \n            if self.is_leaf:\n                val_bytes = self.values[i].encode('utf-8')[:MAX_VALUE_SIZE]\n                data += struct.pack('>H', len(val_bytes)) + val_bytes\n            else:\n                data += struct.pack('>I', self.children[i])\n        \n        # Last child pointer for internal nodes\n        if not self.is_leaf and self.children:\n            data += struct.pack('>I', self.children[-1])\n        \n        # Pad to page size\n        return data.ljust(PAGE_SIZE, b'\\x00')\n    \n    @classmethod\n    def deserialize(cls, page_id, data):\n        is_leaf, num_keys, parent = struct.unpack('>BHI', data[:7])\n        node = cls(page_id, bool(is_leaf))\n        node.parent = parent if parent else None\n        # ... parse keys and values/children\n        return node"
                        },
                        pitfalls: ["Page boundary alignment", "Variable-length key handling", "Endianness"],
                        concepts: ["Disk pages", "Serialization", "Node layout"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 2,
                        name: "Search",
                        description: "Implement efficient key lookup traversing the tree.",
                        criteria: ["Binary search within nodes", "Tree traversal", "Leaf node lookup", "Range queries"],
                        hints: {
                            level1: "Binary search in node to find key or child to follow. Recurse until leaf.",
                            level2: "Cache nodes for performance. Track path for later operations.",
                            level3: "import bisect\n\nclass BTree:\n    def __init__(self, pager, order):\n        self.pager = pager  # Handles disk I/O\n        self.order = order  # Max children per node\n        self.root_page = None\n    \n    def search(self, key):\n        if self.root_page is None:\n            return None\n        \n        node = self.pager.get_page(self.root_page)\n        \n        while True:\n            # Binary search for key position\n            idx = bisect.bisect_left(node.keys, key)\n            \n            if node.is_leaf:\n                # Check if key exists\n                if idx < len(node.keys) and node.keys[idx] == key:\n                    return node.values[idx]\n                return None\n            else:\n                # Follow child pointer\n                child_page = node.children[idx]\n                node = self.pager.get_page(child_page)\n    \n    def range_search(self, start_key, end_key):\n        \"\"\"Return all key-value pairs in range [start, end]\"\"\"\n        results = []\n        \n        # Find starting leaf\n        node, idx = self._find_leaf_position(start_key)\n        if node is None:\n            return results\n        \n        # Scan leaves using sibling pointers\n        while node:\n            while idx < len(node.keys):\n                if node.keys[idx] > end_key:\n                    return results\n                results.append((node.keys[idx], node.values[idx]))\n                idx += 1\n            \n            # Move to next leaf (requires sibling pointer)\n            node = self._next_leaf(node)\n            idx = 0\n        \n        return results"
                        },
                        pitfalls: ["Off-by-one in binary search", "Empty tree handling", "Key not found cases"],
                        concepts: ["Binary search", "Tree traversal", "I/O efficiency"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 3,
                        name: "Insertion with Splitting",
                        description: "Insert keys and handle node splits when full.",
                        criteria: ["Insert into correct leaf", "Split full nodes", "Propagate splits up", "Handle root splits"],
                        hints: {
                            level1: "Insert in leaf. If full, split at middle. Push middle key to parent.",
                            level2: "Parent might also split. New root if root splits.",
                            level3: "def insert(self, key, value):\n    if self.root_page is None:\n        # Create root\n        root = BTreeNode(self.pager.allocate_page(), is_leaf=True)\n        root.keys.append(key)\n        root.values.append(value)\n        self.pager.write_page(root)\n        self.root_page = root.page_id\n        return\n    \n    # Find leaf\n    path = []  # Stack of (node, index) for path from root\n    node = self.pager.get_page(self.root_page)\n    \n    while not node.is_leaf:\n        idx = bisect.bisect_left(node.keys, key)\n        path.append((node, idx))\n        node = self.pager.get_page(node.children[idx])\n    \n    # Insert in leaf\n    idx = bisect.bisect_left(node.keys, key)\n    node.keys.insert(idx, key)\n    node.values.insert(idx, value)\n    \n    # Split if necessary\n    if len(node.keys) > self.order - 1:\n        self._split_leaf(node, path)\n    else:\n        self.pager.write_page(node)\n\ndef _split_leaf(self, node, path):\n    mid = len(node.keys) // 2\n    \n    # Create new right node\n    right = BTreeNode(self.pager.allocate_page(), is_leaf=True)\n    right.keys = node.keys[mid:]\n    right.values = node.values[mid:]\n    \n    # Update left node\n    node.keys = node.keys[:mid]\n    node.values = node.values[:mid]\n    \n    # Push up to parent\n    push_key = right.keys[0]\n    \n    if not path:\n        # Split root - create new root\n        new_root = BTreeNode(self.pager.allocate_page(), is_leaf=False)\n        new_root.keys = [push_key]\n        new_root.children = [node.page_id, right.page_id]\n        self.root_page = new_root.page_id\n        self.pager.write_page(new_root)\n    else:\n        parent, parent_idx = path.pop()\n        parent.keys.insert(parent_idx, push_key)\n        parent.children.insert(parent_idx + 1, right.page_id)\n        \n        if len(parent.keys) > self.order - 1:\n            self._split_internal(parent, path)\n        else:\n            self.pager.write_page(parent)\n    \n    self.pager.write_page(node)\n    self.pager.write_page(right)"
                        },
                        pitfalls: ["Split index calculation", "Parent key vs child key", "Root split handling"],
                        concepts: ["Node splitting", "Tree balancing", "Propagation"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 4,
                        name: "Deletion with Merging",
                        description: "Delete keys and handle underflow by borrowing or merging.",
                        criteria: ["Delete from leaf", "Borrow from siblings", "Merge underfull nodes", "Handle root underflow"],
                        hints: {
                            level1: "Delete from leaf. If too few keys, try borrowing from sibling, else merge.",
                            level2: "Borrowing: rotate through parent. Merging: combine nodes, delete parent key.",
                            level3: "def delete(self, key):\n    path = []\n    node = self.pager.get_page(self.root_page)\n    \n    # Find key\n    while not node.is_leaf:\n        idx = bisect.bisect_left(node.keys, key)\n        path.append((node, idx))\n        node = self.pager.get_page(node.children[idx])\n    \n    # Delete from leaf\n    try:\n        idx = node.keys.index(key)\n    except ValueError:\n        return False  # Key not found\n    \n    node.keys.pop(idx)\n    node.values.pop(idx)\n    \n    # Check underflow\n    min_keys = (self.order - 1) // 2\n    if len(node.keys) >= min_keys or not path:\n        self.pager.write_page(node)\n        return True\n    \n    self._handle_underflow(node, path)\n    return True\n\ndef _handle_underflow(self, node, path):\n    parent, parent_idx = path[-1]\n    min_keys = (self.order - 1) // 2\n    \n    # Try borrowing from left sibling\n    if parent_idx > 0:\n        left = self.pager.get_page(parent.children[parent_idx - 1])\n        if len(left.keys) > min_keys:\n            self._borrow_from_left(node, left, parent, parent_idx)\n            return\n    \n    # Try borrowing from right sibling\n    if parent_idx < len(parent.children) - 1:\n        right = self.pager.get_page(parent.children[parent_idx + 1])\n        if len(right.keys) > min_keys:\n            self._borrow_from_right(node, right, parent, parent_idx)\n            return\n    \n    # Must merge\n    if parent_idx > 0:\n        left = self.pager.get_page(parent.children[parent_idx - 1])\n        self._merge_nodes(left, node, parent, parent_idx - 1, path)\n    else:\n        right = self.pager.get_page(parent.children[parent_idx + 1])\n        self._merge_nodes(node, right, parent, parent_idx, path)\n\ndef _borrow_from_left(self, node, left, parent, idx):\n    if node.is_leaf:\n        # Move last key/value from left to node\n        node.keys.insert(0, left.keys.pop())\n        node.values.insert(0, left.values.pop())\n        parent.keys[idx - 1] = node.keys[0]\n    else:\n        # Rotate: left.last -> parent -> node.first\n        node.keys.insert(0, parent.keys[idx - 1])\n        parent.keys[idx - 1] = left.keys.pop()\n        node.children.insert(0, left.children.pop())\n    \n    self.pager.write_page(node)\n    self.pager.write_page(left)\n    self.pager.write_page(parent)"
                        },
                        pitfalls: ["Sibling selection", "Key rotation through parent", "Empty root handling"],
                        concepts: ["Node merging", "Key redistribution", "Tree rebalancing"],
                        estimatedHours: "13-24"
                    }
                ]
            },

            // SECURITY - EXPERT
            "build-tls": {
                name: "Build Your Own TLS",
                description: "Implement TLS 1.3 handshake and record layer. Learn modern cryptographic protocols.",
                difficulty: "expert",
                estimatedHours: "50-80",
                prerequisites: ["HTTPS client", "AES implementation", "Elliptic curve basics"],
                languages: {
                    recommended: ["Rust", "Go", "C"],
                    also: ["Python", "Java"]
                },
                resources: [
                    { type: "spec", name: "RFC 8446 - TLS 1.3", url: "https://datatracker.ietf.org/doc/html/rfc8446" },
                    { type: "book", name: "Illustrated TLS 1.3", url: "https://tls13.xargs.org/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Record Layer",
                        description: "Implement TLS record protocol for fragmenting and encrypting data.",
                        criteria: ["Record header parsing", "Fragmentation handling", "Content type handling", "Record size limits"],
                        hints: {
                            level1: "Records have 5-byte header: type(1), version(2), length(2). Max payload 16KB.",
                            level2: "In TLS 1.3, actual content type is encrypted. Outer shows 'application_data'.",
                            level3: "class TLSRecord:\n    MAX_PAYLOAD = 16384  # 2^14\n    \n    # Content types\n    CHANGE_CIPHER_SPEC = 20\n    ALERT = 21\n    HANDSHAKE = 22\n    APPLICATION_DATA = 23\n    \n    def __init__(self, content_type, payload):\n        self.content_type = content_type\n        self.payload = payload\n    \n    def encode(self):\n        # TLS 1.3 uses legacy version 0x0303 (TLS 1.2)\n        return bytes([\n            self.content_type,\n            0x03, 0x03,  # Legacy version\n            len(self.payload) >> 8,\n            len(self.payload) & 0xFF\n        ]) + self.payload\n    \n    @classmethod\n    def parse(cls, data):\n        if len(data) < 5:\n            return None, data\n        \n        content_type = data[0]\n        version = (data[1] << 8) | data[2]\n        length = (data[3] << 8) | data[4]\n        \n        if len(data) < 5 + length:\n            return None, data\n        \n        payload = data[5:5+length]\n        return cls(content_type, payload), data[5+length:]\n\nclass RecordLayer:\n    def __init__(self):\n        self.read_cipher = None\n        self.write_cipher = None\n        self.read_seq = 0\n        self.write_seq = 0\n    \n    def encrypt_record(self, content_type, plaintext):\n        if self.write_cipher is None:\n            return TLSRecord(content_type, plaintext)\n        \n        # TLS 1.3: append real content type to plaintext\n        inner_plaintext = plaintext + bytes([content_type])\n        \n        # Encrypt with AEAD\n        nonce = self._build_nonce(self.write_seq, self.write_iv)\n        ciphertext = self.write_cipher.encrypt(nonce, inner_plaintext, b'')\n        \n        self.write_seq += 1\n        \n        # Outer record always shows APPLICATION_DATA\n        return TLSRecord(TLSRecord.APPLICATION_DATA, ciphertext)"
                        },
                        pitfalls: ["Version field confusion", "Padding in TLS 1.3", "Sequence number overflow"],
                        concepts: ["Record protocol", "AEAD encryption", "Protocol layering"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 2,
                        name: "Key Exchange",
                        description: "Implement ECDHE key exchange with X25519.",
                        criteria: ["X25519 scalar multiplication", "Key share generation", "Shared secret derivation", "Key schedule"],
                        hints: {
                            level1: "X25519: generate 32-byte private key, compute public key. Shared secret from ECDH.",
                            level2: "HKDF-Extract then HKDF-Expand for key derivation. Different keys for client/server.",
                            level3: "from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PrivateKey\nfrom cryptography.hazmat.primitives.kdf.hkdf import HKDFExpand, HKDF\nfrom cryptography.hazmat.primitives import hashes\n\nclass KeyExchange:\n    def __init__(self):\n        self.private_key = X25519PrivateKey.generate()\n        self.public_key = self.private_key.public_key()\n    \n    def get_public_bytes(self):\n        return self.public_key.public_bytes_raw()\n    \n    def compute_shared_secret(self, peer_public_bytes):\n        from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PublicKey\n        peer_key = X25519PublicKey.from_public_bytes(peer_public_bytes)\n        return self.private_key.exchange(peer_key)\n\nclass KeySchedule:\n    def __init__(self, shared_secret, transcript_hash):\n        # Early secret (no PSK)\n        self.early_secret = self._hkdf_extract(b'\\x00' * 32, b'')\n        \n        # Handshake secret\n        derived = self._derive_secret(self.early_secret, b'derived', b'')\n        self.handshake_secret = self._hkdf_extract(derived, shared_secret)\n        \n        # Traffic secrets\n        self.client_handshake_secret = self._derive_secret(\n            self.handshake_secret, b'c hs traffic', transcript_hash)\n        self.server_handshake_secret = self._derive_secret(\n            self.handshake_secret, b's hs traffic', transcript_hash)\n    \n    def _hkdf_extract(self, salt, ikm):\n        import hmac\n        return hmac.new(salt, ikm, 'sha256').digest()\n    \n    def _derive_secret(self, secret, label, context):\n        return self._hkdf_expand_label(secret, label, context, 32)\n    \n    def _hkdf_expand_label(self, secret, label, context, length):\n        # TLS 1.3 specific label format\n        full_label = b'tls13 ' + label\n        hkdf_label = (\n            length.to_bytes(2, 'big') +\n            bytes([len(full_label)]) + full_label +\n            bytes([len(context)]) + context\n        )\n        hkdf = HKDFExpand(hashes.SHA256(), length, hkdf_label)\n        return hkdf.derive(secret)"
                        },
                        pitfalls: ["Endianness in key encoding", "Transcript hash timing", "Label format details"],
                        concepts: ["ECDH", "Key derivation", "Forward secrecy"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 3,
                        name: "Handshake Protocol",
                        description: "Implement TLS 1.3 full handshake flow.",
                        criteria: ["ClientHello construction", "ServerHello parsing", "EncryptedExtensions", "Finished message verification"],
                        hints: {
                            level1: "ClientHello: version, random, cipher suites, extensions (key_share, supported_versions).",
                            level2: "After ServerHello, switch to encrypted handshake. Verify Finished with HMAC.",
                            level3: "class Handshake:\n    # Handshake types\n    CLIENT_HELLO = 1\n    SERVER_HELLO = 2\n    ENCRYPTED_EXTENSIONS = 8\n    CERTIFICATE = 11\n    CERTIFICATE_VERIFY = 15\n    FINISHED = 20\n    \n    def __init__(self):\n        self.transcript = b''  # All handshake messages\n        self.key_exchange = KeyExchange()\n    \n    def create_client_hello(self):\n        # Random\n        client_random = os.urandom(32)\n        \n        # Cipher suites (TLS_AES_128_GCM_SHA256)\n        cipher_suites = bytes([0x00, 0x02, 0x13, 0x01])\n        \n        # Extensions\n        extensions = b''\n        \n        # supported_versions\n        extensions += self._extension(43, bytes([0x02, 0x03, 0x04]))  # TLS 1.3\n        \n        # key_share (X25519)\n        key_share_data = (\n            bytes([0x00, 0x1d]) +  # X25519\n            bytes([0x00, 0x20]) +  # Key length\n            self.key_exchange.get_public_bytes()\n        )\n        extensions += self._extension(51, bytes([0x00, len(key_share_data)]) + key_share_data)\n        \n        # signature_algorithms\n        extensions += self._extension(13, bytes([0x00, 0x04, 0x04, 0x03, 0x08, 0x04]))  # ECDSA, RSA-PSS\n        \n        # Build ClientHello\n        body = (\n            bytes([0x03, 0x03]) +  # Legacy version (TLS 1.2)\n            client_random +\n            bytes([0x00]) +  # Session ID length\n            cipher_suites +\n            bytes([0x01, 0x00]) +  # Compression (null)\n            len(extensions).to_bytes(2, 'big') + extensions\n        )\n        \n        msg = bytes([self.CLIENT_HELLO]) + len(body).to_bytes(3, 'big') + body\n        self.transcript += msg\n        return msg\n    \n    def process_server_hello(self, data):\n        # Parse and extract key_share\n        # ... update transcript\n        # Derive handshake keys\n        pass\n    \n    def verify_finished(self, received_verify_data):\n        # Compute expected verify_data\n        transcript_hash = hashlib.sha256(self.transcript).digest()\n        finished_key = self.key_schedule._hkdf_expand_label(\n            self.key_schedule.server_handshake_secret,\n            b'finished', b'', 32\n        )\n        expected = hmac.new(finished_key, transcript_hash, 'sha256').digest()\n        return hmac.compare_digest(received_verify_data, expected)"
                        },
                        pitfalls: ["Extension ordering", "Transcript hash calculation", "Cipher suite negotiation"],
                        concepts: ["Protocol negotiation", "Message authentication", "State machine"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 4,
                        name: "Certificate Verification",
                        description: "Verify server certificate chain and signature.",
                        criteria: ["X.509 parsing", "Chain building", "Signature verification", "Certificate Verify message"],
                        hints: {
                            level1: "Parse certificates from DER. Verify chain up to trusted root.",
                            level2: "CertificateVerify signs transcript hash. Check signature algorithm.",
                            level3: "from cryptography import x509\nfrom cryptography.hazmat.primitives.asymmetric import padding, ec\n\nclass CertificateVerifier:\n    def __init__(self, trusted_roots):\n        self.trusted_roots = trusted_roots  # List of CA certs\n    \n    def verify_chain(self, cert_chain, hostname):\n        if not cert_chain:\n            raise TLSError('Empty certificate chain')\n        \n        # Parse certificates\n        certs = [x509.load_der_x509_certificate(c) for c in cert_chain]\n        \n        # Verify hostname\n        leaf = certs[0]\n        self._verify_hostname(leaf, hostname)\n        \n        # Build and verify chain\n        for i in range(len(certs) - 1):\n            issuer = certs[i + 1]\n            self._verify_signature(certs[i], issuer)\n        \n        # Check root is trusted\n        root = certs[-1]\n        if not self._is_trusted(root):\n            raise TLSError('Untrusted root certificate')\n        \n        return leaf.public_key()\n    \n    def _verify_hostname(self, cert, hostname):\n        # Check SAN extension\n        try:\n            san = cert.extensions.get_extension_for_class(x509.SubjectAlternativeName)\n            names = san.value.get_values_for_type(x509.DNSName)\n            if hostname not in names and not self._matches_wildcard(hostname, names):\n                raise TLSError(f'Hostname {hostname} not in certificate')\n        except x509.ExtensionNotFound:\n            # Fall back to CN\n            cn = cert.subject.get_attributes_for_oid(x509.oid.NameOID.COMMON_NAME)\n            if not cn or cn[0].value != hostname:\n                raise TLSError('Hostname mismatch')\n    \n    def verify_certificate_verify(self, signature, algorithm, transcript_hash, public_key):\n        # TLS 1.3 signature context\n        context = b' ' * 64 + b'TLS 1.3, server CertificateVerify' + b'\\x00' + transcript_hash\n        \n        if isinstance(public_key, ec.EllipticCurvePublicKey):\n            public_key.verify(signature, context, ec.ECDSA(hashes.SHA256()))\n        else:\n            public_key.verify(signature, context, padding.PSS(...))"
                        },
                        pitfalls: ["Chain validation order", "Wildcard matching", "Signature algorithm selection"],
                        concepts: ["PKI", "X.509", "Certificate validation"],
                        estimatedHours: "15-25"
                    }
                ]
            },

            // DISTRIBUTED - EXPERT
            "build-raft": {
                name: "Build Your Own Raft",
                description: "Implement the Raft consensus algorithm for distributed agreement. Learn leader election, log replication, and safety.",
                difficulty: "expert",
                estimatedHours: "60-100",
                prerequisites: ["Replicated log", "Leader election", "Distributed systems basics"],
                languages: {
                    recommended: ["Go", "Rust", "Java"],
                    also: ["Python", "C++"]
                },
                resources: [
                    { type: "paper", name: "In Search of an Understandable Consensus Algorithm", url: "https://raft.github.io/raft.pdf" },
                    { type: "visualization", name: "Raft Visualization", url: "https://raft.github.io/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Leader Election",
                        description: "Implement Raft leader election with terms and voting.",
                        criteria: ["Term management", "Election timeout randomization", "RequestVote RPC", "Vote granting rules"],
                        hints: {
                            level1: "Follower times out -> becomes candidate -> requests votes. Majority wins.",
                            level2: "Random timeout (150-300ms) prevents split votes. Only vote once per term.",
                            level3: "import random\nimport asyncio\nfrom enum import Enum\n\nclass State(Enum):\n    FOLLOWER = 'follower'\n    CANDIDATE = 'candidate'\n    LEADER = 'leader'\n\nclass RaftNode:\n    def __init__(self, node_id, peers):\n        self.id = node_id\n        self.peers = peers\n        \n        # Persistent state\n        self.current_term = 0\n        self.voted_for = None\n        self.log = []  # List of (term, command)\n        \n        # Volatile state\n        self.state = State.FOLLOWER\n        self.commit_index = 0\n        self.last_applied = 0\n        \n        # Leader state\n        self.next_index = {}   # peer -> next log index to send\n        self.match_index = {}  # peer -> highest replicated index\n        \n        self.election_timeout = self._random_timeout()\n        self.last_heartbeat = time.time()\n    \n    def _random_timeout(self):\n        return random.uniform(0.15, 0.3)  # 150-300ms\n    \n    async def election_timer(self):\n        while True:\n            await asyncio.sleep(0.05)  # Check every 50ms\n            \n            if self.state == State.LEADER:\n                continue\n            \n            if time.time() - self.last_heartbeat > self.election_timeout:\n                await self.start_election()\n    \n    async def start_election(self):\n        self.state = State.CANDIDATE\n        self.current_term += 1\n        self.voted_for = self.id\n        self.election_timeout = self._random_timeout()\n        self.last_heartbeat = time.time()\n        \n        votes = 1  # Vote for self\n        last_log_index = len(self.log) - 1\n        last_log_term = self.log[-1][0] if self.log else 0\n        \n        # Request votes from all peers\n        tasks = []\n        for peer in self.peers:\n            tasks.append(self.send_request_vote(peer, last_log_index, last_log_term))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        votes += sum(1 for r in results if r is True)\n        \n        # Check if won\n        if votes > (len(self.peers) + 1) // 2:\n            self.become_leader()\n    \n    def handle_request_vote(self, term, candidate_id, last_log_index, last_log_term):\n        if term < self.current_term:\n            return False, self.current_term\n        \n        if term > self.current_term:\n            self.current_term = term\n            self.voted_for = None\n            self.state = State.FOLLOWER\n        \n        # Check if log is up-to-date\n        my_last_term = self.log[-1][0] if self.log else 0\n        my_last_index = len(self.log) - 1\n        \n        log_ok = (last_log_term > my_last_term or \n                  (last_log_term == my_last_term and last_log_index >= my_last_index))\n        \n        if (self.voted_for is None or self.voted_for == candidate_id) and log_ok:\n            self.voted_for = candidate_id\n            self.last_heartbeat = time.time()\n            return True, self.current_term\n        \n        return False, self.current_term"
                        },
                        pitfalls: ["Split vote scenarios", "Term number handling", "Log up-to-date check"],
                        concepts: ["Leader election", "Distributed voting", "Failure detection"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 2,
                        name: "Log Replication",
                        description: "Implement log replication from leader to followers.",
                        criteria: ["AppendEntries RPC", "Log consistency check", "Follower log repair", "Commit index advancement"],
                        hints: {
                            level1: "Leader sends heartbeats and log entries. Include prev log index/term for consistency.",
                            level2: "On conflict, follower rejects. Leader decrements nextIndex and retries.",
                            level3: "async def send_append_entries(self, peer):\n    prev_log_index = self.next_index[peer] - 1\n    prev_log_term = self.log[prev_log_index][0] if prev_log_index >= 0 else 0\n    \n    entries = self.log[self.next_index[peer]:]\n    \n    success, term = await peer.append_entries(\n        term=self.current_term,\n        leader_id=self.id,\n        prev_log_index=prev_log_index,\n        prev_log_term=prev_log_term,\n        entries=entries,\n        leader_commit=self.commit_index\n    )\n    \n    if term > self.current_term:\n        self.current_term = term\n        self.state = State.FOLLOWER\n        return\n    \n    if success:\n        self.next_index[peer] = len(self.log)\n        self.match_index[peer] = len(self.log) - 1\n        self.advance_commit_index()\n    else:\n        # Decrement and retry\n        self.next_index[peer] = max(0, self.next_index[peer] - 1)\n\ndef handle_append_entries(self, term, leader_id, prev_log_index, prev_log_term, entries, leader_commit):\n    self.last_heartbeat = time.time()\n    \n    if term < self.current_term:\n        return False, self.current_term\n    \n    if term > self.current_term:\n        self.current_term = term\n        self.voted_for = None\n    \n    self.state = State.FOLLOWER\n    \n    # Check log consistency\n    if prev_log_index >= 0:\n        if prev_log_index >= len(self.log):\n            return False, self.current_term\n        if self.log[prev_log_index][0] != prev_log_term:\n            # Delete conflicting entries\n            self.log = self.log[:prev_log_index]\n            return False, self.current_term\n    \n    # Append new entries\n    for i, entry in enumerate(entries):\n        idx = prev_log_index + 1 + i\n        if idx < len(self.log):\n            if self.log[idx][0] != entry[0]:\n                self.log = self.log[:idx]\n                self.log.append(entry)\n        else:\n            self.log.append(entry)\n    \n    # Update commit index\n    if leader_commit > self.commit_index:\n        self.commit_index = min(leader_commit, len(self.log) - 1)\n    \n    return True, self.current_term\n\ndef advance_commit_index(self):\n    # Find N such that majority have match_index >= N\n    for n in range(len(self.log) - 1, self.commit_index, -1):\n        if self.log[n][0] != self.current_term:\n            continue\n        \n        count = 1  # Self\n        for peer in self.peers:\n            if self.match_index.get(peer, 0) >= n:\n                count += 1\n        \n        if count > (len(self.peers) + 1) // 2:\n            self.commit_index = n\n            break"
                        },
                        pitfalls: ["Log index off-by-one", "Commit index from previous term", "Handling gaps"],
                        concepts: ["Log replication", "Consistency", "Quorum"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 3,
                        name: "Safety Properties",
                        description: "Ensure Raft safety guarantees are maintained.",
                        criteria: ["Election safety", "Leader append-only", "Log matching", "Leader completeness"],
                        hints: {
                            level1: "Never commit entries from previous terms directly. Only commit by replicating current term entry.",
                            level2: "Leader never overwrites its log. Followers may truncate conflicting entries.",
                            level3: "# Safety invariant checks\n\nclass SafetyChecker:\n    \"\"\"Debug helper to verify Raft safety properties\"\"\"\n    \n    @staticmethod\n    def check_election_safety(nodes):\n        \"\"\"At most one leader per term\"\"\"\n        leaders_by_term = {}\n        for node in nodes:\n            if node.state == State.LEADER:\n                term = node.current_term\n                if term in leaders_by_term:\n                    raise SafetyViolation(\n                        f'Multiple leaders in term {term}: '\n                        f'{leaders_by_term[term]} and {node.id}'\n                    )\n                leaders_by_term[term] = node.id\n    \n    @staticmethod\n    def check_log_matching(nodes):\n        \"\"\"If two logs contain entry with same index and term,\n           logs are identical up to that index\"\"\"\n        for i, n1 in enumerate(nodes):\n            for n2 in nodes[i+1:]:\n                for idx in range(min(len(n1.log), len(n2.log))):\n                    if n1.log[idx][0] == n2.log[idx][0]:  # Same term\n                        # All previous entries must match\n                        for j in range(idx):\n                            if n1.log[j] != n2.log[j]:\n                                raise SafetyViolation(\n                                    f'Log mismatch at {j} between {n1.id} and {n2.id}'\n                                )\n    \n    @staticmethod\n    def check_leader_completeness(committed_entries, leader):\n        \"\"\"Any committed entry must be in leader's log\"\"\"\n        for entry in committed_entries:\n            if entry not in leader.log:\n                raise SafetyViolation(\n                    f'Committed entry {entry} not in leader log'\n                )\n\n# In RaftNode, ensure we only commit from current term\ndef advance_commit_index(self):\n    for n in range(len(self.log) - 1, self.commit_index, -1):\n        # CRITICAL: Only commit entries from current term\n        # This ensures Leader Completeness property\n        if self.log[n][0] != self.current_term:\n            continue  # Can't directly commit old term entries\n        \n        count = 1\n        for peer in self.peers:\n            if self.match_index.get(peer, 0) >= n:\n                count += 1\n        \n        if count > (len(self.peers) + 1) // 2:\n            self.commit_index = n\n            break"
                        },
                        pitfalls: ["Committing old term entries", "Figure 8 scenario", "Network partition handling"],
                        concepts: ["Safety properties", "Formal verification", "Invariants"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 4,
                        name: "Cluster Membership Changes",
                        description: "Implement dynamic cluster membership changes.",
                        criteria: ["Joint consensus", "Single-server changes", "Configuration log entries", "Leader transfer"],
                        hints: {
                            level1: "Single-server changes are simpler: add/remove one at a time.",
                            level2: "New server must catch up before counting for quorum. Leader steps down if removed.",
                            level3: "class RaftNode:\n    def __init__(self, ...):\n        # ...\n        self.config = set(peers) | {self.id}\n        self.pending_config = None\n    \n    async def add_server(self, new_server):\n        if self.state != State.LEADER:\n            raise NotLeaderError()\n        \n        if self.pending_config:\n            raise ConfigChangeInProgress()\n        \n        # First, catch up new server\n        await self.catch_up_server(new_server)\n        \n        # Append config change to log\n        new_config = self.config | {new_server}\n        entry = (self.current_term, ConfigChange(new_config))\n        self.log.append(entry)\n        \n        self.pending_config = new_config\n        self.peers.add(new_server)\n        self.next_index[new_server] = len(self.log)\n        self.match_index[new_server] = 0\n        \n        # Replicate and wait for commit\n        await self.replicate_and_commit()\n        \n        self.config = new_config\n        self.pending_config = None\n    \n    async def remove_server(self, server):\n        if self.state != State.LEADER:\n            raise NotLeaderError()\n        \n        if server not in self.config:\n            raise ServerNotInConfig()\n        \n        new_config = self.config - {server}\n        entry = (self.current_term, ConfigChange(new_config))\n        self.log.append(entry)\n        \n        self.pending_config = new_config\n        \n        await self.replicate_and_commit()\n        \n        self.config = new_config\n        self.pending_config = None\n        \n        if server == self.id:\n            # Leader is being removed\n            self.state = State.FOLLOWER\n            # Transfer leadership\n            await self.transfer_leadership()\n    \n    async def catch_up_server(self, server, timeout=30):\n        \"\"\"Replicate log to new server until caught up\"\"\"\n        start = time.time()\n        while time.time() - start < timeout:\n            await self.send_append_entries(server)\n            if self.match_index.get(server, 0) >= len(self.log) - 1:\n                return\n            await asyncio.sleep(0.1)\n        raise CatchUpTimeout()"
                        },
                        pitfalls: ["Disjoint majorities", "Leader in transition", "Stuck configurations"],
                        concepts: ["Membership changes", "Joint consensus", "Availability during changes"],
                        estimatedHours: "18-32"
                    }
                ]
            },

            // DISTRIBUTED - EXPERT
            "build-kafka": {
                name: "Build Your Own Kafka",
                description: "Build a distributed message queue with partitions and consumer groups. Learn pub/sub, ordering, and scalability.",
                difficulty: "expert",
                estimatedHours: "60-100",
                prerequisites: ["Replicated log", "Distributed systems", "Binary protocols"],
                languages: {
                    recommended: ["Go", "Java", "Rust"],
                    also: ["Scala", "C++"]
                },
                resources: [
                    { type: "book", name: "Kafka: The Definitive Guide", url: "https://www.confluent.io/resources/kafka-the-definitive-guide/" },
                    { type: "paper", name: "Kafka: a Distributed Messaging System", url: "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Topic and Partitions",
                        description: "Implement topic management with multiple partitions.",
                        criteria: ["Topic creation with partition count", "Partition as append-only log", "Message key hashing for partition assignment", "Offset tracking"],
                        hints: {
                            level1: "Topic = name + list of partitions. Each partition is an independent log.",
                            level2: "Hash message key to determine partition. Null key = round-robin.",
                            level3: "import hashlib\nfrom dataclasses import dataclass\nfrom typing import Optional, List\n\n@dataclass\nclass Message:\n    key: Optional[bytes]\n    value: bytes\n    timestamp: int\n    offset: int = -1  # Set by partition\n\nclass Partition:\n    def __init__(self, topic: str, partition_id: int, log_dir: str):\n        self.topic = topic\n        self.id = partition_id\n        self.log_path = f'{log_dir}/{topic}-{partition_id}'\n        self.messages = []  # In production, this is file-backed\n        self.next_offset = 0\n        self._load_log()\n    \n    def append(self, msg: Message) -> int:\n        msg.offset = self.next_offset\n        self.next_offset += 1\n        self.messages.append(msg)\n        self._persist(msg)\n        return msg.offset\n    \n    def read(self, start_offset: int, max_messages: int) -> List[Message]:\n        if start_offset >= self.next_offset:\n            return []\n        end = min(start_offset + max_messages, self.next_offset)\n        return self.messages[start_offset:end]\n\nclass Topic:\n    def __init__(self, name: str, num_partitions: int, log_dir: str):\n        self.name = name\n        self.partitions = [\n            Partition(name, i, log_dir)\n            for i in range(num_partitions)\n        ]\n    \n    def get_partition(self, key: Optional[bytes]) -> Partition:\n        if key is None:\n            # Round-robin for null keys\n            return self.partitions[self._rr_counter() % len(self.partitions)]\n        \n        # Hash-based assignment\n        hash_val = int(hashlib.md5(key).hexdigest(), 16)\n        return self.partitions[hash_val % len(self.partitions)]\n    \n    def produce(self, key: Optional[bytes], value: bytes) -> tuple:\n        partition = self.get_partition(key)\n        msg = Message(key=key, value=value, timestamp=int(time.time() * 1000))\n        offset = partition.append(msg)\n        return partition.id, offset"
                        },
                        pitfalls: ["Partition assignment consistency", "Offset gaps", "Key null handling"],
                        concepts: ["Partitioning", "Ordered logs", "Horizontal scaling"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "Producer",
                        description: "Implement producer with batching and acknowledgments.",
                        criteria: ["Batch messages for efficiency", "Configurable acks (0, 1, all)", "Retry logic", "Idempotent producer (optional)"],
                        hints: {
                            level1: "Batch messages by partition. Send when batch full or timeout.",
                            level2: "acks=0: fire and forget. acks=1: leader ack. acks=all: all replicas.",
                            level3: "from collections import defaultdict\nimport asyncio\n\nclass Producer:\n    def __init__(self, bootstrap_servers, acks='1', batch_size=16384, linger_ms=5):\n        self.brokers = bootstrap_servers\n        self.acks = acks\n        self.batch_size = batch_size\n        self.linger_ms = linger_ms\n        \n        self.batches = defaultdict(list)  # (topic, partition) -> [messages]\n        self.batch_sizes = defaultdict(int)\n        self.pending = {}  # Future results\n        \n        self._sender_task = asyncio.create_task(self._sender_loop())\n    \n    async def send(self, topic: str, key: bytes, value: bytes) -> asyncio.Future:\n        partition = self._partition_for(topic, key)\n        \n        future = asyncio.Future()\n        msg = ProducerRecord(topic, partition, key, value, future)\n        \n        batch_key = (topic, partition)\n        self.batches[batch_key].append(msg)\n        self.batch_sizes[batch_key] += len(value)\n        \n        # Check if batch is full\n        if self.batch_sizes[batch_key] >= self.batch_size:\n            await self._send_batch(batch_key)\n        \n        return future\n    \n    async def _sender_loop(self):\n        while True:\n            await asyncio.sleep(self.linger_ms / 1000)\n            \n            for batch_key in list(self.batches.keys()):\n                if self.batches[batch_key]:\n                    await self._send_batch(batch_key)\n    \n    async def _send_batch(self, batch_key):\n        topic, partition = batch_key\n        messages = self.batches.pop(batch_key, [])\n        self.batch_sizes.pop(batch_key, 0)\n        \n        if not messages:\n            return\n        \n        try:\n            broker = await self._get_leader(topic, partition)\n            response = await broker.produce(\n                topic=topic,\n                partition=partition,\n                messages=[(m.key, m.value) for m in messages],\n                acks=self.acks\n            )\n            \n            # Resolve futures\n            for i, msg in enumerate(messages):\n                msg.future.set_result(RecordMetadata(\n                    topic=topic,\n                    partition=partition,\n                    offset=response.base_offset + i\n                ))\n        except Exception as e:\n            for msg in messages:\n                msg.future.set_exception(e)"
                        },
                        pitfalls: ["Batch timeout handling", "Leader failover during send", "Duplicate messages"],
                        concepts: ["Batching", "Acknowledgments", "At-least-once delivery"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 3,
                        name: "Consumer Groups",
                        description: "Implement consumer groups with partition assignment and rebalancing.",
                        criteria: ["Group membership protocol", "Partition assignment strategies", "Offset commits", "Rebalancing on join/leave"],
                        hints: {
                            level1: "Each partition assigned to exactly one consumer in group. Rebalance on membership change.",
                            level2: "Coordinator manages group. Consumers send heartbeats. Leader assigns partitions.",
                            level3: "class ConsumerGroup:\n    def __init__(self, group_id: str, coordinator):\n        self.group_id = group_id\n        self.coordinator = coordinator\n        self.members = {}  # member_id -> ConsumerMember\n        self.generation = 0\n        self.leader = None\n        self.assignment = {}  # member_id -> [partitions]\n    \n    def join(self, member_id: str, subscriptions: List[str]) -> JoinGroupResponse:\n        self.members[member_id] = ConsumerMember(member_id, subscriptions)\n        \n        # Trigger rebalance\n        self.generation += 1\n        \n        # Elect leader (first member)\n        if self.leader is None or self.leader not in self.members:\n            self.leader = member_id\n        \n        return JoinGroupResponse(\n            generation=self.generation,\n            leader=self.leader,\n            member_id=member_id,\n            members=list(self.members.keys()) if member_id == self.leader else []\n        )\n    \n    def sync(self, member_id: str, generation: int, assignment: dict) -> SyncGroupResponse:\n        if generation != self.generation:\n            raise RebalanceInProgress()\n        \n        if member_id == self.leader:\n            self.assignment = assignment\n        \n        return SyncGroupResponse(\n            assignment=self.assignment.get(member_id, [])\n        )\n\nclass Consumer:\n    def __init__(self, group_id: str, bootstrap_servers):\n        self.group_id = group_id\n        self.member_id = None\n        self.generation = None\n        self.assignment = []\n        self.offsets = {}  # (topic, partition) -> offset\n    \n    async def subscribe(self, topics: List[str]):\n        self.subscriptions = topics\n        await self._join_group()\n    \n    async def poll(self, timeout_ms: int) -> List[ConsumerRecord]:\n        records = []\n        for topic, partition in self.assignment:\n            offset = self.offsets.get((topic, partition), 0)\n            batch = await self._fetch(topic, partition, offset)\n            records.extend(batch)\n            if batch:\n                self.offsets[(topic, partition)] = batch[-1].offset + 1\n        return records\n    \n    async def commit(self):\n        await self.coordinator.commit_offsets(\n            self.group_id,\n            self.member_id,\n            self.generation,\n            self.offsets\n        )\n\n# Partition assignment (Range strategy)\ndef range_assignment(members: List[str], partitions: List[tuple]) -> dict:\n    assignment = {m: [] for m in members}\n    \n    # Group partitions by topic\n    by_topic = defaultdict(list)\n    for topic, partition in partitions:\n        by_topic[topic].append(partition)\n    \n    for topic, parts in by_topic.items():\n        parts.sort()\n        per_consumer = len(parts) // len(members)\n        extra = len(parts) % len(members)\n        \n        idx = 0\n        for i, member in enumerate(sorted(members)):\n            count = per_consumer + (1 if i < extra else 0)\n            for p in parts[idx:idx+count]:\n                assignment[member].append((topic, p))\n            idx += count\n    \n    return assignment"
                        },
                        pitfalls: ["Rebalance storms", "Stuck rebalances", "Duplicate processing during rebalance"],
                        concepts: ["Consumer groups", "Partition assignment", "Rebalancing"],
                        estimatedHours: "18-30"
                    },
                    {
                        id: 4,
                        name: "Replication",
                        description: "Implement partition replication for fault tolerance.",
                        criteria: ["Leader/follower replication", "ISR (In-Sync Replicas)", "Leader election on failure", "High watermark"],
                        hints: {
                            level1: "Each partition has leader and followers. Leader handles reads/writes. Followers fetch from leader.",
                            level2: "ISR = replicas within lag threshold. Only commit when all ISR have message.",
                            level3: "class ReplicatedPartition:\n    def __init__(self, topic, partition_id, replicas, leader_id):\n        self.topic = topic\n        self.id = partition_id\n        self.replicas = replicas  # [broker_id, ...]\n        self.leader = leader_id\n        self.isr = set(replicas)  # In-sync replicas\n        \n        self.log = []  # Local log\n        self.high_watermark = -1  # Last committed offset\n        self.leo = -1  # Log end offset\n        \n        # Leader state\n        self.replica_leo = {}  # replica_id -> their LEO\n    \n    def append_as_leader(self, messages: List) -> int:\n        base_offset = self.leo + 1\n        \n        for i, msg in enumerate(messages):\n            msg.offset = base_offset + i\n            self.log.append(msg)\n        \n        self.leo = self.log[-1].offset\n        self._update_high_watermark()\n        return base_offset\n    \n    def fetch_as_follower(self, fetch_offset: int, max_bytes: int) -> FetchResponse:\n        messages = []\n        size = 0\n        \n        for msg in self.log[fetch_offset:]:\n            if size + len(msg.value) > max_bytes:\n                break\n            messages.append(msg)\n            size += len(msg.value)\n        \n        return FetchResponse(\n            messages=messages,\n            high_watermark=self.high_watermark\n        )\n    \n    def update_follower_state(self, follower_id: int, follower_leo: int):\n        self.replica_leo[follower_id] = follower_leo\n        \n        # Check if follower is in sync\n        if follower_leo >= self.leo - self.max_lag:\n            self.isr.add(follower_id)\n        else:\n            self.isr.discard(follower_id)\n        \n        self._update_high_watermark()\n    \n    def _update_high_watermark(self):\n        # HW = min LEO of all ISR\n        if not self.isr:\n            return\n        \n        isr_leos = [self.replica_leo.get(r, self.leo) for r in self.isr]\n        new_hw = min(isr_leos)\n        \n        if new_hw > self.high_watermark:\n            self.high_watermark = new_hw\n\nclass ReplicaFetcher:\n    \"\"\"Follower thread that fetches from leader\"\"\"\n    \n    def __init__(self, partition, leader_broker):\n        self.partition = partition\n        self.leader = leader_broker\n    \n    async def run(self):\n        while True:\n            try:\n                response = await self.leader.fetch(\n                    self.partition.topic,\n                    self.partition.id,\n                    fetch_offset=self.partition.leo + 1\n                )\n                \n                for msg in response.messages:\n                    self.partition.log.append(msg)\n                    self.partition.leo = msg.offset\n                \n                self.partition.high_watermark = min(\n                    self.partition.high_watermark,\n                    response.high_watermark\n                )\n            except LeaderNotAvailable:\n                await asyncio.sleep(1)\n                await self._find_new_leader()"
                        },
                        pitfalls: ["ISR shrinking to empty", "Unclean leader election", "Data loss on failover"],
                        concepts: ["Replication", "ISR", "High watermark", "Exactly-once semantics"],
                        estimatedHours: "20-37"
                    }
                ]
            },

            // DISTRIBUTED - EXPERT
            "build-distributed-kv": {
                name: "Build Your Own Distributed KV Store",
                description: "Build a distributed key-value store with partitioning and replication. Learn consistent hashing, shard management, and distributed transactions.",
                difficulty: "expert",
                estimatedHours: "70-120",
                prerequisites: ["Build Raft or 2PC", "Consistent hashing", "RPC framework"],
                languages: {
                    recommended: ["Go", "Rust", "Java"],
                    also: ["C++", "Scala"]
                },
                resources: [
                    { type: "paper", name: "Dynamo: Amazon's Key-Value Store", url: "https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" },
                    { type: "course", name: "MIT 6.824 Distributed Systems", url: "https://pdos.csail.mit.edu/6.824/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Consistent Hashing",
                        description: "Implement consistent hashing for key distribution across nodes.",
                        criteria: ["Hash ring with virtual nodes", "Key lookup", "Node addition/removal", "Minimal key redistribution"],
                        hints: {
                            level1: "Hash both keys and nodes onto ring. Key belongs to first node clockwise.",
                            level2: "Virtual nodes improve balance. Each physical node has multiple positions.",
                            level3: "import hashlib\nfrom bisect import bisect_right\nfrom typing import List, Dict, Any\n\nclass ConsistentHash:\n    def __init__(self, virtual_nodes: int = 150):\n        self.virtual_nodes = virtual_nodes\n        self.ring = []  # Sorted list of (hash, node_id)\n        self.nodes = {}  # node_id -> node_info\n    \n    def _hash(self, key: str) -> int:\n        return int(hashlib.sha256(key.encode()).hexdigest(), 16)\n    \n    def add_node(self, node_id: str, node_info: Any):\n        self.nodes[node_id] = node_info\n        \n        for i in range(self.virtual_nodes):\n            virtual_key = f'{node_id}:{i}'\n            hash_val = self._hash(virtual_key)\n            # Insert maintaining sorted order\n            idx = bisect_right([h for h, _ in self.ring], hash_val)\n            self.ring.insert(idx, (hash_val, node_id))\n    \n    def remove_node(self, node_id: str):\n        if node_id not in self.nodes:\n            return\n        \n        del self.nodes[node_id]\n        self.ring = [(h, n) for h, n in self.ring if n != node_id]\n    \n    def get_node(self, key: str) -> str:\n        if not self.ring:\n            raise NoNodesError()\n        \n        hash_val = self._hash(key)\n        idx = bisect_right([h for h, _ in self.ring], hash_val)\n        \n        # Wrap around to first node if past end\n        if idx >= len(self.ring):\n            idx = 0\n        \n        return self.ring[idx][1]\n    \n    def get_nodes(self, key: str, n: int) -> List[str]:\n        \"\"\"Get n distinct nodes for replication\"\"\"\n        if len(self.nodes) < n:\n            raise InsufficientNodes()\n        \n        hash_val = self._hash(key)\n        idx = bisect_right([h for h, _ in self.ring], hash_val)\n        \n        result = []\n        seen = set()\n        \n        for i in range(len(self.ring)):\n            _, node_id = self.ring[(idx + i) % len(self.ring)]\n            if node_id not in seen:\n                result.append(node_id)\n                seen.add(node_id)\n                if len(result) == n:\n                    break\n        \n        return result"
                        },
                        pitfalls: ["Hot spots with few nodes", "Virtual node count tuning", "Rebalancing overhead"],
                        concepts: ["Consistent hashing", "Load balancing", "Data partitioning"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "Replication",
                        description: "Implement replication with configurable consistency levels.",
                        criteria: ["N replicas per key", "Read/write quorums (R + W > N)", "Sloppy quorum and hinted handoff", "Conflict resolution"],
                        hints: {
                            level1: "For N=3, R=2, W=2 gives strong consistency. R=1, W=1 for availability.",
                            level2: "Hinted handoff: if replica down, store hint at another node for later delivery.",
                            level3: "from dataclasses import dataclass\nfrom typing import Optional\nimport time\n\n@dataclass\nclass VersionedValue:\n    value: bytes\n    version: int  # Vector clock or timestamp\n    timestamp: float\n\nclass ReplicatedStore:\n    def __init__(self, ring: ConsistentHash, n: int = 3, r: int = 2, w: int = 2):\n        self.ring = ring\n        self.n = n  # Replication factor\n        self.r = r  # Read quorum\n        self.w = w  # Write quorum\n        self.hints = {}  # target_node -> [(key, value)]\n    \n    async def put(self, key: str, value: bytes) -> bool:\n        nodes = self.ring.get_nodes(key, self.n)\n        version = int(time.time() * 1000000)\n        \n        versioned = VersionedValue(value, version, time.time())\n        \n        # Write to replicas\n        successes = 0\n        for node in nodes:\n            try:\n                await self._write_to_node(node, key, versioned)\n                successes += 1\n            except NodeUnavailable:\n                # Hinted handoff\n                hint_node = self._find_hint_node(nodes)\n                if hint_node:\n                    self._store_hint(hint_node, node, key, versioned)\n        \n        return successes >= self.w\n    \n    async def get(self, key: str) -> Optional[bytes]:\n        nodes = self.ring.get_nodes(key, self.n)\n        \n        # Read from replicas\n        responses = []\n        for node in nodes:\n            try:\n                val = await self._read_from_node(node, key)\n                if val:\n                    responses.append(val)\n                if len(responses) >= self.r:\n                    break\n            except NodeUnavailable:\n                continue\n        \n        if len(responses) < self.r:\n            raise ReadQuorumNotMet()\n        \n        # Return most recent version\n        latest = max(responses, key=lambda v: v.version)\n        \n        # Read repair: update stale replicas\n        asyncio.create_task(self._read_repair(key, latest, nodes))\n        \n        return latest.value\n    \n    async def _read_repair(self, key, latest, nodes):\n        for node in nodes:\n            try:\n                current = await self._read_from_node(node, key)\n                if not current or current.version < latest.version:\n                    await self._write_to_node(node, key, latest)\n            except NodeUnavailable:\n                pass"
                        },
                        pitfalls: ["Quorum calculation", "Read repair races", "Hint delivery ordering"],
                        concepts: ["Quorum consensus", "Eventual consistency", "Hinted handoff"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 3,
                        name: "Cluster Management",
                        description: "Implement cluster membership and failure detection.",
                        criteria: ["Gossip protocol for membership", "Failure detection", "Automatic rebalancing", "Cluster state convergence"],
                        hints: {
                            level1: "Gossip: periodically exchange state with random peers. State converges.",
                            level2: "Phi accrual failure detector adapts to network conditions.",
                            level3: "import random\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Set\n\n@dataclass\nclass NodeState:\n    status: str  # 'alive', 'suspect', 'dead'\n    heartbeat: int\n    last_seen: float = field(default_factory=time.time)\n\nclass GossipProtocol:\n    def __init__(self, node_id: str, seeds: List[str]):\n        self.node_id = node_id\n        self.seeds = seeds\n        self.members: Dict[str, NodeState] = {}\n        self.heartbeat = 0\n        self.suspect_timeout = 5.0\n        self.dead_timeout = 30.0\n    \n    async def run(self, interval: float = 1.0):\n        while True:\n            await asyncio.sleep(interval)\n            self.heartbeat += 1\n            \n            # Update own state\n            self.members[self.node_id] = NodeState('alive', self.heartbeat)\n            \n            # Gossip to random peer\n            peers = [n for n in self.members if n != self.node_id]\n            if not peers:\n                peers = self.seeds\n            \n            target = random.choice(peers)\n            await self._gossip_to(target)\n            \n            # Check for failures\n            self._detect_failures()\n    \n    async def _gossip_to(self, target: str):\n        try:\n            # Send our view, receive theirs\n            their_state = await rpc_call(target, 'gossip', self.members)\n            self._merge_state(their_state)\n        except Exception:\n            pass  # Target unreachable\n    \n    def _merge_state(self, remote: Dict[str, NodeState]):\n        for node_id, state in remote.items():\n            if node_id not in self.members:\n                self.members[node_id] = state\n            elif state.heartbeat > self.members[node_id].heartbeat:\n                self.members[node_id] = state\n                self.members[node_id].last_seen = time.time()\n    \n    def _detect_failures(self):\n        now = time.time()\n        for node_id, state in self.members.items():\n            if node_id == self.node_id:\n                continue\n            \n            age = now - state.last_seen\n            \n            if state.status == 'alive' and age > self.suspect_timeout:\n                state.status = 'suspect'\n                self._on_node_suspect(node_id)\n            elif state.status == 'suspect' and age > self.dead_timeout:\n                state.status = 'dead'\n                self._on_node_dead(node_id)\n    \n    def _on_node_dead(self, node_id: str):\n        # Trigger rebalancing\n        self.ring.remove_node(node_id)\n        # Move data from failed node's range\n        asyncio.create_task(self._rebalance_for_failure(node_id))"
                        },
                        pitfalls: ["Gossip message size", "Split brain", "Cascade failures"],
                        concepts: ["Gossip protocols", "Failure detection", "Cluster membership"],
                        estimatedHours: "18-30"
                    },
                    {
                        id: 4,
                        name: "Transactions",
                        description: "Implement distributed transactions across shards.",
                        criteria: ["Single-key linearizability", "Multi-key transactions (2PC)", "Optimistic concurrency control", "Deadlock detection"],
                        hints: {
                            level1: "Single-key: use Raft per shard. Multi-key: coordinate with 2PC.",
                            level2: "Optimistic: validate reads at commit time. Abort on conflict.",
                            level3: "from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Set\nimport uuid\n\nclass TxnState(Enum):\n    ACTIVE = 'active'\n    PREPARED = 'prepared'\n    COMMITTED = 'committed'\n    ABORTED = 'aborted'\n\n@dataclass\nclass Transaction:\n    id: str\n    read_set: Dict[str, int]  # key -> version read\n    write_set: Dict[str, bytes]  # key -> new value\n    state: TxnState = TxnState.ACTIVE\n\nclass TransactionCoordinator:\n    def __init__(self, store: ReplicatedStore):\n        self.store = store\n        self.transactions: Dict[str, Transaction] = {}\n    \n    def begin(self) -> str:\n        txn_id = str(uuid.uuid4())\n        self.transactions[txn_id] = Transaction(id=txn_id, read_set={}, write_set={})\n        return txn_id\n    \n    async def read(self, txn_id: str, key: str) -> bytes:\n        txn = self.transactions[txn_id]\n        \n        # Check write set first (read your writes)\n        if key in txn.write_set:\n            return txn.write_set[key]\n        \n        # Read from store\n        value, version = await self.store.get_versioned(key)\n        txn.read_set[key] = version\n        return value\n    \n    def write(self, txn_id: str, key: str, value: bytes):\n        txn = self.transactions[txn_id]\n        txn.write_set[key] = value\n    \n    async def commit(self, txn_id: str) -> bool:\n        txn = self.transactions[txn_id]\n        \n        # Group keys by shard\n        shards = self._group_by_shard(set(txn.read_set) | set(txn.write_set))\n        \n        # Phase 1: Prepare\n        prepared = []\n        for shard, keys in shards.items():\n            try:\n                # Lock keys, validate read set\n                success = await self._prepare_shard(shard, txn, keys)\n                if not success:\n                    await self._abort_prepared(prepared, txn)\n                    return False\n                prepared.append(shard)\n            except Exception:\n                await self._abort_prepared(prepared, txn)\n                return False\n        \n        # Phase 2: Commit\n        for shard in prepared:\n            await self._commit_shard(shard, txn)\n        \n        txn.state = TxnState.COMMITTED\n        return True\n    \n    async def _prepare_shard(self, shard, txn, keys) -> bool:\n        # Validate read versions haven't changed\n        for key in keys:\n            if key in txn.read_set:\n                current_version = await self.store.get_version(key)\n                if current_version != txn.read_set[key]:\n                    return False  # Conflict\n        \n        # Acquire locks\n        locked = await self.store.try_lock_keys(shard, keys, txn.id)\n        return locked"
                        },
                        pitfalls: ["Coordinator failure during 2PC", "Lock timeout tuning", "Phantom reads"],
                        concepts: ["Distributed transactions", "2PC", "Serializability"],
                        estimatedHours: "27-50"
                    }
                ]
            },

            // SPECIALIZED - EXPERT
            "build-debugger": {
                name: "Build Your Own Debugger",
                description: "Build a debugger like GDB. Learn ptrace, breakpoints, and symbol tables.",
                difficulty: "expert",
                estimatedHours: "50-80",
                prerequisites: ["Unix processes", "Assembly basics", "ELF format"],
                languages: {
                    recommended: ["C", "Rust", "C++"],
                    also: ["Go"]
                },
                resources: [
                    { type: "blog", name: "Writing a Linux Debugger", url: "https://blog.tartanllama.xyz/writing-a-linux-debugger-setup/" },
                    { type: "book", name: "The Linux Programming Interface - Ch 26", url: "https://man7.org/tlpi/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Process Control",
                        description: "Use ptrace to control debugee execution.",
                        criteria: ["Fork and trace child", "Single-step execution", "Continue execution", "Wait for signals"],
                        hints: {
                            level1: "ptrace(PTRACE_TRACEME) in child, parent uses PTRACE_CONT, PTRACE_SINGLESTEP.",
                            level2: "Child stops on exec. Use waitpid to detect stops. Check WIFSTOPPED.",
                            level3: "#include <sys/ptrace.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <stdio.h>\n\nstruct debugger {\n    pid_t pid;\n    int running;\n};\n\nvoid run_debugee(const char* prog) {\n    ptrace(PTRACE_TRACEME, 0, NULL, NULL);\n    execl(prog, prog, NULL);\n}\n\nstruct debugger* create_debugger(const char* prog) {\n    struct debugger* dbg = malloc(sizeof(struct debugger));\n    \n    pid_t pid = fork();\n    if (pid == 0) {\n        // Child\n        run_debugee(prog);\n    }\n    \n    // Parent\n    dbg->pid = pid;\n    dbg->running = 0;\n    \n    // Wait for child to stop at exec\n    int status;\n    waitpid(pid, &status, 0);\n    \n    // Set options\n    ptrace(PTRACE_SETOPTIONS, pid, NULL, PTRACE_O_EXITKILL);\n    \n    return dbg;\n}\n\nvoid continue_execution(struct debugger* dbg) {\n    ptrace(PTRACE_CONT, dbg->pid, NULL, NULL);\n    dbg->running = 1;\n    \n    int status;\n    waitpid(dbg->pid, &status, 0);\n    dbg->running = 0;\n    \n    if (WIFSTOPPED(status)) {\n        printf(\"Stopped with signal %d\\n\", WSTOPSIG(status));\n    } else if (WIFEXITED(status)) {\n        printf(\"Exited with code %d\\n\", WEXITSTATUS(status));\n    }\n}\n\nvoid single_step(struct debugger* dbg) {\n    ptrace(PTRACE_SINGLESTEP, dbg->pid, NULL, NULL);\n    int status;\n    waitpid(dbg->pid, &status, 0);\n}"
                        },
                        pitfalls: ["Signal handling in debugger", "Race conditions", "Zombie processes"],
                        concepts: ["ptrace", "Process control", "Unix signals"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 2,
                        name: "Breakpoints",
                        description: "Implement software breakpoints using int3.",
                        criteria: ["Set breakpoint at address", "Replace instruction with int3 (0xCC)", "Restore on hit", "Handle breakpoint hit"],
                        hints: {
                            level1: "Read original byte with PTRACE_PEEKTEXT, write 0xCC. On hit, restore and rewind PC.",
                            level2: "Breakpoint hit causes SIGTRAP. PC points past int3, so decrement by 1.",
                            level3: "#include <stdint.h>\n#include <sys/ptrace.h>\n\nstruct breakpoint {\n    uint64_t addr;\n    uint8_t saved_byte;\n    int enabled;\n};\n\nstruct breakpoint* set_breakpoint(struct debugger* dbg, uint64_t addr) {\n    struct breakpoint* bp = malloc(sizeof(struct breakpoint));\n    bp->addr = addr;\n    bp->enabled = 0;\n    \n    // Read original instruction\n    long data = ptrace(PTRACE_PEEKTEXT, dbg->pid, addr, NULL);\n    bp->saved_byte = (uint8_t)(data & 0xFF);\n    \n    // Replace with int3 (0xCC)\n    long modified = (data & ~0xFF) | 0xCC;\n    ptrace(PTRACE_POKETEXT, dbg->pid, addr, modified);\n    \n    bp->enabled = 1;\n    return bp;\n}\n\nvoid disable_breakpoint(struct debugger* dbg, struct breakpoint* bp) {\n    if (!bp->enabled) return;\n    \n    long data = ptrace(PTRACE_PEEKTEXT, dbg->pid, bp->addr, NULL);\n    long restored = (data & ~0xFF) | bp->saved_byte;\n    ptrace(PTRACE_POKETEXT, dbg->pid, bp->addr, restored);\n    \n    bp->enabled = 0;\n}\n\nvoid handle_breakpoint_hit(struct debugger* dbg, struct breakpoint* bp) {\n    // Get registers\n    struct user_regs_struct regs;\n    ptrace(PTRACE_GETREGS, dbg->pid, NULL, &regs);\n    \n    // PC is now past int3, rewind\n    regs.rip = bp->addr;\n    ptrace(PTRACE_SETREGS, dbg->pid, NULL, &regs);\n    \n    // Disable breakpoint, single step, re-enable\n    disable_breakpoint(dbg, bp);\n    single_step(dbg);\n    enable_breakpoint(dbg, bp);\n}"
                        },
                        pitfalls: ["Multi-byte instruction boundaries", "Breakpoint in loop", "Thread safety"],
                        concepts: ["Software breakpoints", "Instruction patching", "Program counter"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 3,
                        name: "Symbol Tables",
                        description: "Parse DWARF debug info for source-level debugging.",
                        criteria: ["Parse ELF sections", "Read DWARF DIEs", "Map addresses to lines", "Map names to addresses"],
                        hints: {
                            level1: "ELF has .debug_info, .debug_line sections. Use libdwarf or parse manually.",
                            level2: "DWARF uses DIEs (Debug Info Entries) in tree structure. Line table maps PC to source.",
                            level3: "// Using libdwarf\n#include <libdwarf/libdwarf.h>\n#include <libdwarf/dwarf.h>\n\nstruct line_info {\n    uint64_t addr;\n    unsigned int line;\n    const char* file;\n};\n\nstruct symbol_table {\n    Dwarf_Debug dbg;\n    struct line_info* lines;\n    size_t num_lines;\n    // function name -> address map\n};\n\nstruct symbol_table* load_symbols(const char* binary) {\n    struct symbol_table* st = calloc(1, sizeof(struct symbol_table));\n    \n    int fd = open(binary, O_RDONLY);\n    Dwarf_Error err;\n    \n    if (dwarf_init(fd, DW_DLC_READ, NULL, NULL, &st->dbg, &err) != DW_DLV_OK) {\n        return NULL;\n    }\n    \n    // Iterate compilation units\n    Dwarf_Unsigned cu_header_length;\n    while (dwarf_next_cu_header(st->dbg, &cu_header_length, NULL, NULL, NULL, NULL, &err) == DW_DLV_OK) {\n        Dwarf_Die cu_die;\n        dwarf_siblingof(st->dbg, NULL, &cu_die, &err);\n        \n        // Get line table\n        Dwarf_Line* lines;\n        Dwarf_Signed num_lines;\n        dwarf_srclines(cu_die, &lines, &num_lines, &err);\n        \n        for (int i = 0; i < num_lines; i++) {\n            Dwarf_Addr addr;\n            Dwarf_Unsigned lineno;\n            char* file;\n            \n            dwarf_lineaddr(lines[i], &addr, &err);\n            dwarf_lineno(lines[i], &lineno, &err);\n            dwarf_linesrc(lines[i], &file, &err);\n            \n            // Store mapping\n            add_line_info(st, addr, lineno, file);\n        }\n    }\n    \n    return st;\n}\n\nuint64_t symbol_to_addr(struct symbol_table* st, const char* name) {\n    // Search DIEs for function with matching name\n    // Return low_pc attribute\n}\n\nstruct line_info* addr_to_line(struct symbol_table* st, uint64_t addr) {\n    // Binary search in sorted line table\n    // Return entry with largest addr <= target\n}"
                        },
                        pitfalls: ["DWARF version differences", "Inlined functions", "Optimized code mapping"],
                        concepts: ["Debug information", "Symbol tables", "ELF/DWARF"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 4,
                        name: "Variable Inspection",
                        description: "Read and display variable values using debug info.",
                        criteria: ["Get variable location from DWARF", "Read register/memory values", "Handle different types", "Display struct members"],
                        hints: {
                            level1: "DWARF location expressions describe where variable lives (register, stack, etc.).",
                            level2: "Use DW_AT_type to get variable type. Recursively handle pointers, arrays, structs.",
                            level3: "// Variable location from DWARF\nenum loc_type { LOC_REG, LOC_ADDR, LOC_EXPR };\n\nstruct var_location {\n    enum loc_type type;\n    union {\n        int reg;           // Register number\n        uint64_t addr;     // Memory address\n        Dwarf_Loc* expr;   // Location expression\n    };\n};\n\nstruct var_location get_var_location(struct symbol_table* st, const char* var_name, uint64_t pc) {\n    // Find variable DIE\n    Dwarf_Die var_die = find_variable_die(st, var_name, pc);\n    \n    // Get location attribute\n    Dwarf_Attribute loc_attr;\n    dwarf_attr(var_die, DW_AT_location, &loc_attr, NULL);\n    \n    Dwarf_Loc* locs;\n    Dwarf_Signed num_locs;\n    dwarf_loclist(loc_attr, &locs, &num_locs, NULL);\n    \n    // Interpret first location atom\n    struct var_location result;\n    \n    switch (locs[0].lr_atom) {\n        case DW_OP_reg0 ... DW_OP_reg31:\n            result.type = LOC_REG;\n            result.reg = locs[0].lr_atom - DW_OP_reg0;\n            break;\n        case DW_OP_fbreg:  // Offset from frame base\n            result.type = LOC_ADDR;\n            result.addr = get_frame_base(dbg) + locs[0].lr_number;\n            break;\n        // ... handle other cases\n    }\n    \n    return result;\n}\n\nchar* read_variable(struct debugger* dbg, struct symbol_table* st, const char* name) {\n    uint64_t pc = get_pc(dbg);\n    struct var_location loc = get_var_location(st, name, pc);\n    Dwarf_Die type_die = get_var_type(st, name, pc);\n    \n    uint64_t value;\n    if (loc.type == LOC_REG) {\n        value = get_register_value(dbg, loc.reg);\n    } else {\n        value = ptrace(PTRACE_PEEKDATA, dbg->pid, loc.addr, NULL);\n    }\n    \n    // Format based on type\n    return format_value(value, type_die);\n}"
                        },
                        pitfalls: ["Optimized-out variables", "Complex location expressions", "Type alignment"],
                        concepts: ["Variable location", "Type information", "Register access"],
                        estimatedHours: "17-28"
                    }
                ]
            },

            // SPECIALIZED - EXPERT
            "build-emulator": {
                name: "Build Your Own Emulator",
                description: "Build a NES, GameBoy, or CHIP-8 emulator. Learn CPU emulation, memory mapping, and timing.",
                difficulty: "expert",
                estimatedHours: "60-120",
                prerequisites: ["Binary/hex", "Assembly basics", "Graphics basics"],
                languages: {
                    recommended: ["C", "Rust", "C++"],
                    also: ["Go", "TypeScript"]
                },
                resources: [
                    { type: "guide", name: "Writing a CHIP-8 Emulator", url: "https://tobiasvl.github.io/blog/write-a-chip-8-emulator/" },
                    { type: "docs", name: "Pan Docs (Game Boy)", url: "https://gbdev.io/pandocs/" },
                    { type: "wiki", name: "NesDev Wiki", url: "https://www.nesdev.org/wiki/Nesdev_Wiki" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "CPU Emulation",
                        description: "Implement the instruction set and registers.",
                        criteria: ["All CPU registers", "Fetch-decode-execute cycle", "All opcodes implemented", "Correct flags behavior"],
                        hints: {
                            level1: "CHIP-8 has 35 opcodes. Game Boy has ~500. Start simple.",
                            level2: "Use a big switch statement or function pointer table for opcodes.",
                            level3: "// CHIP-8 CPU example\nstruct CPU {\n    uint8_t V[16];      // General registers V0-VF\n    uint16_t I;         // Index register\n    uint16_t PC;        // Program counter\n    uint8_t SP;         // Stack pointer\n    uint16_t stack[16]; // Call stack\n    uint8_t delay_timer;\n    uint8_t sound_timer;\n};\n\nvoid cpu_init(struct CPU* cpu) {\n    memset(cpu, 0, sizeof(struct CPU));\n    cpu->PC = 0x200;  // Programs start at 0x200\n}\n\nvoid cpu_cycle(struct CPU* cpu, uint8_t* memory) {\n    // Fetch: 2-byte opcode\n    uint16_t opcode = (memory[cpu->PC] << 8) | memory[cpu->PC + 1];\n    cpu->PC += 2;\n    \n    // Decode and execute\n    uint8_t x = (opcode >> 8) & 0x0F;  // Second nibble\n    uint8_t y = (opcode >> 4) & 0x0F;  // Third nibble\n    uint8_t n = opcode & 0x0F;         // Fourth nibble\n    uint8_t nn = opcode & 0xFF;        // Second byte\n    uint16_t nnn = opcode & 0x0FFF;    // Last 12 bits\n    \n    switch (opcode & 0xF000) {\n        case 0x0000:\n            switch (opcode) {\n                case 0x00E0: // CLS - Clear screen\n                    clear_display();\n                    break;\n                case 0x00EE: // RET - Return\n                    cpu->SP--;\n                    cpu->PC = cpu->stack[cpu->SP];\n                    break;\n            }\n            break;\n        \n        case 0x1000: // JP addr - Jump\n            cpu->PC = nnn;\n            break;\n        \n        case 0x2000: // CALL addr\n            cpu->stack[cpu->SP] = cpu->PC;\n            cpu->SP++;\n            cpu->PC = nnn;\n            break;\n        \n        case 0x3000: // SE Vx, byte - Skip if equal\n            if (cpu->V[x] == nn) cpu->PC += 2;\n            break;\n        \n        case 0x6000: // LD Vx, byte\n            cpu->V[x] = nn;\n            break;\n        \n        case 0x7000: // ADD Vx, byte\n            cpu->V[x] += nn;\n            break;\n        \n        case 0x8000: // Arithmetic\n            switch (n) {\n                case 0x0: cpu->V[x] = cpu->V[y]; break;  // LD\n                case 0x1: cpu->V[x] |= cpu->V[y]; break; // OR\n                case 0x2: cpu->V[x] &= cpu->V[y]; break; // AND\n                case 0x4: // ADD with carry\n                    cpu->V[0xF] = (cpu->V[x] + cpu->V[y]) > 255 ? 1 : 0;\n                    cpu->V[x] += cpu->V[y];\n                    break;\n                // ... more arithmetic ops\n            }\n            break;\n        \n        case 0xD000: // DRW - Draw sprite\n            draw_sprite(cpu, memory, x, y, n);\n            break;\n        // ... remaining opcodes\n    }\n}"
                        },
                        pitfalls: ["Endianness", "Flag edge cases", "Undocumented opcodes"],
                        concepts: ["CPU architecture", "Instruction sets", "Machine code"],
                        estimatedHours: "15-30"
                    },
                    {
                        id: 2,
                        name: "Memory System",
                        description: "Implement memory mapping and bank switching.",
                        criteria: ["Memory map implementation", "ROM loading", "RAM regions", "Memory-mapped I/O"],
                        hints: {
                            level1: "CHIP-8 is simple: 4KB flat memory. Game Boy has ROM banking.",
                            level2: "Memory-mapped I/O: writes to certain addresses control hardware.",
                            level3: "// Game Boy memory map\nstruct MMU {\n    uint8_t* rom;           // Cartridge ROM\n    uint8_t* rom_banks;     // Switchable ROM banks\n    uint8_t wram[8192];     // Work RAM\n    uint8_t vram[8192];     // Video RAM\n    uint8_t oam[160];       // Sprite attribute memory\n    uint8_t io[128];        // I/O registers\n    uint8_t hram[127];      // High RAM\n    uint8_t ie;             // Interrupt enable\n    \n    int rom_bank;           // Current ROM bank number\n    int mbc_type;           // Memory Bank Controller type\n};\n\nuint8_t mmu_read(struct MMU* mmu, uint16_t addr) {\n    switch (addr & 0xF000) {\n        case 0x0000:\n        case 0x1000:\n        case 0x2000:\n        case 0x3000:\n            // ROM bank 0 (fixed)\n            return mmu->rom[addr];\n        \n        case 0x4000:\n        case 0x5000:\n        case 0x6000:\n        case 0x7000:\n            // Switchable ROM bank\n            return mmu->rom_banks[(mmu->rom_bank - 1) * 0x4000 + (addr - 0x4000)];\n        \n        case 0x8000:\n        case 0x9000:\n            // Video RAM\n            return mmu->vram[addr - 0x8000];\n        \n        case 0xC000:\n        case 0xD000:\n            // Work RAM\n            return mmu->wram[addr - 0xC000];\n        \n        case 0xF000:\n            if (addr >= 0xFF00 && addr <= 0xFF7F) {\n                // I/O registers\n                return read_io(mmu, addr);\n            }\n            if (addr >= 0xFF80 && addr <= 0xFFFE) {\n                return mmu->hram[addr - 0xFF80];\n            }\n            if (addr == 0xFFFF) {\n                return mmu->ie;\n            }\n            break;\n    }\n    return 0xFF;  // Unmapped\n}\n\nvoid mmu_write(struct MMU* mmu, uint16_t addr, uint8_t value) {\n    // Handle MBC bank switching\n    if (addr >= 0x2000 && addr <= 0x3FFF) {\n        // ROM bank select\n        mmu->rom_bank = value & 0x1F;\n        if (mmu->rom_bank == 0) mmu->rom_bank = 1;\n        return;\n    }\n    // ... handle other regions\n}"
                        },
                        pitfalls: ["Bank 0 special cases", "Echo RAM", "Write-only registers"],
                        concepts: ["Memory mapping", "Bank switching", "MMIO"],
                        estimatedHours: "12-20"
                    },
                    {
                        id: 3,
                        name: "Graphics",
                        description: "Implement the display/PPU (Picture Processing Unit).",
                        criteria: ["Screen rendering", "Sprites", "Background tiles", "Correct timing"],
                        hints: {
                            level1: "CHIP-8: 64x32 monochrome, XOR drawing. Game Boy: tile-based with layers.",
                            level2: "Render line by line (scanline rendering). Check sprite priority.",
                            level3: "// Game Boy PPU (simplified)\nstruct PPU {\n    uint8_t lcdc;           // LCD Control\n    uint8_t stat;           // LCD Status\n    uint8_t scy, scx;       // Scroll Y/X\n    uint8_t ly;             // Current scanline\n    uint8_t lyc;            // LY Compare\n    uint8_t bgp;            // Background palette\n    uint8_t obp0, obp1;     // Sprite palettes\n    \n    uint8_t* vram;\n    uint8_t* oam;\n    uint32_t framebuffer[160 * 144];\n    \n    int mode;               // 0=HBlank, 1=VBlank, 2=OAM, 3=Transfer\n    int cycles;\n};\n\nvoid ppu_step(struct PPU* ppu, int cpu_cycles) {\n    ppu->cycles += cpu_cycles;\n    \n    switch (ppu->mode) {\n        case 2:  // OAM Scan (80 cycles)\n            if (ppu->cycles >= 80) {\n                ppu->cycles -= 80;\n                ppu->mode = 3;\n            }\n            break;\n        \n        case 3:  // Pixel Transfer (172 cycles avg)\n            if (ppu->cycles >= 172) {\n                ppu->cycles -= 172;\n                render_scanline(ppu);\n                ppu->mode = 0;\n            }\n            break;\n        \n        case 0:  // HBlank (204 cycles)\n            if (ppu->cycles >= 204) {\n                ppu->cycles -= 204;\n                ppu->ly++;\n                \n                if (ppu->ly == 144) {\n                    ppu->mode = 1;  // VBlank\n                    request_interrupt(INT_VBLANK);\n                } else {\n                    ppu->mode = 2;\n                }\n            }\n            break;\n        \n        case 1:  // VBlank (10 lines)\n            if (ppu->cycles >= 456) {\n                ppu->cycles -= 456;\n                ppu->ly++;\n                \n                if (ppu->ly > 153) {\n                    ppu->ly = 0;\n                    ppu->mode = 2;\n                }\n            }\n            break;\n    }\n}\n\nvoid render_scanline(struct PPU* ppu) {\n    if (!(ppu->lcdc & 0x80)) return;  // LCD off\n    \n    int y = ppu->ly;\n    \n    // Render background\n    if (ppu->lcdc & 0x01) {\n        for (int x = 0; x < 160; x++) {\n            int scrolled_x = (x + ppu->scx) & 255;\n            int scrolled_y = (y + ppu->scy) & 255;\n            \n            // Get tile from tilemap\n            int tile_x = scrolled_x / 8;\n            int tile_y = scrolled_y / 8;\n            int tile_idx = get_bg_tile(ppu, tile_x, tile_y);\n            \n            // Get pixel from tile\n            int pixel_x = scrolled_x % 8;\n            int pixel_y = scrolled_y % 8;\n            int color = get_tile_pixel(ppu, tile_idx, pixel_x, pixel_y);\n            \n            ppu->framebuffer[y * 160 + x] = apply_palette(color, ppu->bgp);\n        }\n    }\n    \n    // Render sprites on top\n    if (ppu->lcdc & 0x02) {\n        render_sprites(ppu, y);\n    }\n}"
                        },
                        pitfalls: ["Mid-frame register changes", "Sprite limit per line", "Priority rules"],
                        concepts: ["Tile graphics", "Scanline rendering", "Sprite systems"],
                        estimatedHours: "18-35"
                    },
                    {
                        id: 4,
                        name: "Timing and Input",
                        description: "Implement accurate timing and controller input.",
                        criteria: ["Cycle-accurate timing", "Timer interrupts", "Joypad input", "Audio (optional)"],
                        hints: {
                            level1: "Each instruction takes specific cycles. Run PPU/timers in sync.",
                            level2: "Game Boy: 4.19 MHz CPU, timer can fire at different rates.",
                            level3: "// Main emulation loop with timing\nstruct Emulator {\n    struct CPU cpu;\n    struct MMU mmu;\n    struct PPU ppu;\n    struct Timer timer;\n    uint8_t joypad;\n    \n    uint64_t total_cycles;\n};\n\nvoid emulator_frame(struct Emulator* emu) {\n    // One frame = 70224 cycles (at 59.7 Hz)\n    const int CYCLES_PER_FRAME = 70224;\n    int cycles_this_frame = 0;\n    \n    while (cycles_this_frame < CYCLES_PER_FRAME) {\n        // Handle interrupts\n        int interrupt_cycles = handle_interrupts(&emu->cpu, &emu->mmu);\n        \n        // Execute one instruction\n        int cpu_cycles = cpu_step(&emu->cpu, &emu->mmu);\n        \n        int total = interrupt_cycles + cpu_cycles;\n        \n        // Step other components\n        timer_step(&emu->timer, total);\n        ppu_step(&emu->ppu, total);\n        \n        cycles_this_frame += total;\n        emu->total_cycles += total;\n    }\n}\n\n// Timer implementation\nstruct Timer {\n    uint8_t div;    // Divider (increments at 16384 Hz)\n    uint8_t tima;   // Timer counter\n    uint8_t tma;    // Timer modulo (reload value)\n    uint8_t tac;    // Timer control\n    \n    int div_counter;\n    int timer_counter;\n};\n\nvoid timer_step(struct Timer* t, int cycles) {\n    // DIV increments every 256 cycles\n    t->div_counter += cycles;\n    while (t->div_counter >= 256) {\n        t->div_counter -= 256;\n        t->div++;\n    }\n    \n    // TIMA if enabled\n    if (t->tac & 0x04) {\n        t->timer_counter += cycles;\n        \n        int threshold;\n        switch (t->tac & 0x03) {\n            case 0: threshold = 1024; break;  // 4096 Hz\n            case 1: threshold = 16; break;    // 262144 Hz\n            case 2: threshold = 64; break;    // 65536 Hz\n            case 3: threshold = 256; break;   // 16384 Hz\n        }\n        \n        while (t->timer_counter >= threshold) {\n            t->timer_counter -= threshold;\n            t->tima++;\n            \n            if (t->tima == 0) {  // Overflow\n                t->tima = t->tma;  // Reload\n                request_interrupt(INT_TIMER);\n            }\n        }\n    }\n}\n\n// Joypad\nuint8_t read_joypad(struct Emulator* emu) {\n    uint8_t select = emu->mmu.io[0x00];  // P1 register\n    uint8_t result = select | 0x0F;  // Upper bits from select, lower bits high\n    \n    if (!(select & 0x10)) {  // Direction keys selected\n        if (emu->joypad & KEY_RIGHT) result &= ~0x01;\n        if (emu->joypad & KEY_LEFT)  result &= ~0x02;\n        if (emu->joypad & KEY_UP)    result &= ~0x04;\n        if (emu->joypad & KEY_DOWN)  result &= ~0x08;\n    }\n    if (!(select & 0x20)) {  // Button keys selected\n        if (emu->joypad & KEY_A)      result &= ~0x01;\n        if (emu->joypad & KEY_B)      result &= ~0x02;\n        if (emu->joypad & KEY_SELECT) result &= ~0x04;\n        if (emu->joypad & KEY_START)  result &= ~0x08;\n    }\n    \n    return result;\n}"
                        },
                        pitfalls: ["Cycle counting accuracy", "Timer edge cases", "VBlank timing"],
                        concepts: ["Cycle accuracy", "Hardware timers", "Input handling"],
                        estimatedHours: "15-35"
                    }
                ]
            },

            // SPECIALIZED - EXPERT
            "build-browser": {
                name: "Build Your Own Browser",
                description: "Build a simple web browser engine. Learn HTML/CSS parsing, layout, and rendering.",
                difficulty: "expert",
                estimatedHours: "80-150",
                prerequisites: ["HTML/CSS parsing", "Tree data structures", "Graphics basics"],
                languages: {
                    recommended: ["Rust", "C++", "Go"],
                    also: ["Python", "TypeScript"]
                },
                resources: [
                    { type: "book", name: "Web Browser Engineering", url: "https://browser.engineering/" },
                    { type: "tutorial", name: "Let's build a browser engine", url: "https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "HTML Parser",
                        description: "Parse HTML into a DOM tree.",
                        criteria: ["Tokenize HTML", "Build DOM tree", "Handle nested elements", "Self-closing tags"],
                        hints: {
                            level1: "State machine tokenizer: data state, tag open, tag name, attribute states.",
                            level2: "Build tree with stack. Push on open tag, pop on close. Handle implicit closes.",
                            level3: "from dataclasses import dataclass, field\nfrom typing import List, Dict, Optional\n\n@dataclass\nclass Node:\n    pass\n\n@dataclass\nclass Element(Node):\n    tag_name: str\n    attributes: Dict[str, str] = field(default_factory=dict)\n    children: List[Node] = field(default_factory=list)\n\n@dataclass\nclass Text(Node):\n    data: str\n\nclass HTMLParser:\n    SELF_CLOSING = {'area', 'base', 'br', 'col', 'embed', 'hr', 'img', 'input', 'link', 'meta', 'param', 'source', 'track', 'wbr'}\n    \n    def __init__(self, html: str):\n        self.html = html\n        self.pos = 0\n    \n    def parse(self) -> Element:\n        # Create implicit root\n        root = Element('html')\n        self._parse_nodes(root)\n        return root\n    \n    def _parse_nodes(self, parent: Element):\n        while self.pos < len(self.html):\n            if self._peek() == '<':\n                if self._peek(1) == '/':\n                    return  # Close tag - handled by caller\n                elif self._peek(1) == '!':\n                    self._skip_comment()\n                else:\n                    elem = self._parse_element()\n                    if elem:\n                        parent.children.append(elem)\n            else:\n                text = self._parse_text()\n                if text.strip():\n                    parent.children.append(Text(text))\n    \n    def _parse_element(self) -> Optional[Element]:\n        self._consume('<')\n        tag_name = self._parse_tag_name()\n        attrs = self._parse_attributes()\n        \n        self._skip_whitespace()\n        \n        # Self-closing syntax or void element\n        if self._peek() == '/' or tag_name.lower() in self.SELF_CLOSING:\n            if self._peek() == '/':\n                self._consume('/')\n            self._consume('>')\n            return Element(tag_name.lower(), attrs)\n        \n        self._consume('>')\n        \n        elem = Element(tag_name.lower(), attrs)\n        self._parse_nodes(elem)\n        \n        # Consume close tag\n        if self._peek() == '<' and self._peek(1) == '/':\n            self._consume('</')\n            self._parse_tag_name()  # Should match\n            self._consume('>')\n        \n        return elem\n    \n    def _parse_attributes(self) -> Dict[str, str]:\n        attrs = {}\n        while True:\n            self._skip_whitespace()\n            if self._peek() in ('>', '/', ''):\n                break\n            \n            name = self._parse_attr_name()\n            self._skip_whitespace()\n            \n            if self._peek() == '=':\n                self._consume('=')\n                value = self._parse_attr_value()\n            else:\n                value = ''\n            \n            attrs[name.lower()] = value\n        return attrs"
                        },
                        pitfalls: ["Malformed HTML handling", "Entity decoding", "Case sensitivity"],
                        concepts: ["DOM", "Tokenization", "Tree construction"],
                        estimatedHours: "12-20"
                    },
                    {
                        id: 2,
                        name: "CSS Parser",
                        description: "Parse CSS into a stylesheet.",
                        criteria: ["Selector parsing", "Property parsing", "Specificity calculation", "Cascade"],
                        hints: {
                            level1: "Selectors: element, class, id, combinators. Properties are key: value pairs.",
                            level2: "Specificity: (inline, id, class, element). Higher wins. Later wins on tie.",
                            level3: "@dataclass\nclass Selector:\n    tag: Optional[str] = None\n    id: Optional[str] = None\n    classes: List[str] = field(default_factory=list)\n    \n    def specificity(self) -> tuple:\n        # (inline, ids, classes, elements)\n        return (\n            0,\n            1 if self.id else 0,\n            len(self.classes),\n            1 if self.tag else 0\n        )\n    \n    def matches(self, elem: Element) -> bool:\n        if self.tag and self.tag != elem.tag_name:\n            return False\n        if self.id and self.id != elem.attributes.get('id'):\n            return False\n        elem_classes = set(elem.attributes.get('class', '').split())\n        if not all(c in elem_classes for c in self.classes):\n            return False\n        return True\n\n@dataclass\nclass Rule:\n    selectors: List[Selector]\n    declarations: Dict[str, str]\n\nclass CSSParser:\n    def __init__(self, css: str):\n        self.css = css\n        self.pos = 0\n    \n    def parse(self) -> List[Rule]:\n        rules = []\n        while self.pos < len(self.css):\n            self._skip_whitespace_and_comments()\n            if self.pos >= len(self.css):\n                break\n            rules.append(self._parse_rule())\n        return rules\n    \n    def _parse_selector(self) -> Selector:\n        sel = Selector()\n        while self._peek() not in (',', '{', ''):\n            if self._peek() == '#':\n                self._consume('#')\n                sel.id = self._parse_identifier()\n            elif self._peek() == '.':\n                self._consume('.')\n                sel.classes.append(self._parse_identifier())\n            elif self._peek().isalpha():\n                sel.tag = self._parse_identifier()\n            else:\n                break\n        return sel\n    \n    def _parse_declarations(self) -> Dict[str, str]:\n        decls = {}\n        self._consume('{')\n        while self._peek() != '}':\n            self._skip_whitespace()\n            name = self._parse_identifier()\n            self._skip_whitespace()\n            self._consume(':')\n            self._skip_whitespace()\n            value = self._parse_value()\n            decls[name] = value\n            self._skip_whitespace()\n            if self._peek() == ';':\n                self._consume(';')\n        self._consume('}')\n        return decls\n\n# Style computation\ndef compute_style(elem: Element, stylesheet: List[Rule]) -> Dict[str, str]:\n    # Collect matching rules with specificity\n    matched = []\n    for rule in stylesheet:\n        for selector in rule.selectors:\n            if selector.matches(elem):\n                matched.append((selector.specificity(), rule.declarations))\n    \n    # Sort by specificity\n    matched.sort(key=lambda x: x[0])\n    \n    # Apply in order (later/higher specificity wins)\n    style = {}\n    for _, decls in matched:\n        style.update(decls)\n    \n    return style"
                        },
                        pitfalls: ["Selector combinators", "Shorthand properties", "!important"],
                        concepts: ["CSS parsing", "Specificity", "Cascade"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 3,
                        name: "Layout",
                        description: "Calculate box positions and sizes.",
                        criteria: ["Box model (margin, border, padding)", "Block layout", "Inline layout", "Width/height calculation"],
                        hints: {
                            level1: "Layout tree mirrors DOM but has computed styles. Block = vertical, Inline = horizontal.",
                            level2: "Width flows down (parent constrains child). Height flows up (content determines parent).",
                            level3: "@dataclass\nclass Dimensions:\n    content: Rect = field(default_factory=Rect)\n    padding: EdgeSizes = field(default_factory=EdgeSizes)\n    border: EdgeSizes = field(default_factory=EdgeSizes)\n    margin: EdgeSizes = field(default_factory=EdgeSizes)\n    \n    def padding_box(self) -> Rect:\n        return self.content.expanded_by(self.padding)\n    \n    def border_box(self) -> Rect:\n        return self.padding_box().expanded_by(self.border)\n    \n    def margin_box(self) -> Rect:\n        return self.border_box().expanded_by(self.margin)\n\n@dataclass\nclass LayoutBox:\n    box_type: str  # 'block', 'inline', 'anonymous'\n    dimensions: Dimensions = field(default_factory=Dimensions)\n    children: List['LayoutBox'] = field(default_factory=list)\n    style: Dict[str, str] = field(default_factory=dict)\n    node: Optional[Element] = None\n\ndef layout_block(box: LayoutBox, containing_block: Dimensions):\n    # Calculate width based on containing block\n    calculate_block_width(box, containing_block)\n    \n    # Position box below previous siblings\n    calculate_block_position(box, containing_block)\n    \n    # Layout children\n    layout_block_children(box)\n    \n    # Height is determined by children\n    calculate_block_height(box)\n\ndef calculate_block_width(box: LayoutBox, container: Dimensions):\n    style = box.style\n    \n    # Default to auto\n    width = style.get('width', 'auto')\n    \n    # Get margin, padding, border values\n    margin_left = to_px(style.get('margin-left', '0'))\n    margin_right = to_px(style.get('margin-right', '0'))\n    padding_left = to_px(style.get('padding-left', '0'))\n    padding_right = to_px(style.get('padding-right', '0'))\n    border_left = to_px(style.get('border-left-width', '0'))\n    border_right = to_px(style.get('border-right-width', '0'))\n    \n    total = margin_left + border_left + padding_left + padding_right + border_right + margin_right\n    \n    if width != 'auto':\n        total += to_px(width)\n    \n    # Adjust for containing block\n    underflow = container.content.width - total\n    \n    if width == 'auto':\n        # Expand to fill\n        box.dimensions.content.width = underflow\n    else:\n        box.dimensions.content.width = to_px(width)\n        # Adjust auto margins\n        if style.get('margin-left') == 'auto' and style.get('margin-right') == 'auto':\n            margin_left = underflow / 2\n            margin_right = underflow / 2\n    \n    box.dimensions.padding.left = padding_left\n    box.dimensions.padding.right = padding_right\n    box.dimensions.margin.left = margin_left\n    box.dimensions.margin.right = margin_right"
                        },
                        pitfalls: ["Auto margins", "Percentage units", "Collapsing margins"],
                        concepts: ["Box model", "Block formatting", "Layout algorithms"],
                        estimatedHours: "20-35"
                    },
                    {
                        id: 4,
                        name: "Rendering",
                        description: "Paint the layout tree to a canvas.",
                        criteria: ["Background colors", "Borders", "Text rendering", "Z-ordering"],
                        hints: {
                            level1: "Walk layout tree. Draw backgrounds, then borders, then content (painter's algorithm).",
                            level2: "Text needs font metrics. Use a graphics library (SDL, Cairo, etc.).",
                            level3: "@dataclass\nclass DisplayCommand:\n    pass\n\n@dataclass\nclass SolidColor(DisplayCommand):\n    color: Color\n    rect: Rect\n\n@dataclass\nclass DrawText(DisplayCommand):\n    text: str\n    x: float\n    y: float\n    color: Color\n    font_size: float\n\ndef build_display_list(layout_root: LayoutBox) -> List[DisplayCommand]:\n    commands = []\n    render_layout_box(commands, layout_root)\n    return commands\n\ndef render_layout_box(commands: List, box: LayoutBox):\n    render_background(commands, box)\n    render_borders(commands, box)\n    \n    if box.box_type == 'block':\n        for child in box.children:\n            render_layout_box(commands, child)\n    elif box.box_type == 'inline':\n        render_text(commands, box)\n\ndef render_background(commands: List, box: LayoutBox):\n    color = box.style.get('background-color')\n    if color and color != 'transparent':\n        commands.append(SolidColor(\n            color=parse_color(color),\n            rect=box.dimensions.border_box()\n        ))\n\ndef render_borders(commands: List, box: LayoutBox):\n    color = box.style.get('border-color')\n    if not color:\n        return\n    \n    d = box.dimensions\n    border_box = d.border_box()\n    \n    # Top border\n    if d.border.top > 0:\n        commands.append(SolidColor(\n            color=parse_color(color),\n            rect=Rect(\n                x=border_box.x,\n                y=border_box.y,\n                width=border_box.width,\n                height=d.border.top\n            )\n        ))\n    \n    # ... similar for other sides\n\ndef render_text(commands: List, box: LayoutBox):\n    if box.node and isinstance(box.node, Text):\n        color = box.style.get('color', 'black')\n        font_size = to_px(box.style.get('font-size', '16px'))\n        \n        commands.append(DrawText(\n            text=box.node.data,\n            x=box.dimensions.content.x,\n            y=box.dimensions.content.y + font_size,  # Baseline\n            color=parse_color(color),\n            font_size=font_size\n        ))\n\n# Execute display list\ndef paint(commands: List[DisplayCommand], canvas):\n    for cmd in commands:\n        if isinstance(cmd, SolidColor):\n            canvas.fill_rect(cmd.rect, cmd.color)\n        elif isinstance(cmd, DrawText):\n            canvas.draw_text(cmd.text, cmd.x, cmd.y, cmd.color, cmd.font_size)"
                        },
                        pitfalls: ["Subpixel rendering", "Font fallbacks", "Clipping"],
                        concepts: ["Rendering pipeline", "Display lists", "Graphics"],
                        estimatedHours: "33-70"
                    }
                ]
            },

            // APP-DEV - EXPERT
            "build-react": {
                name: "Build Your Own React",
                description: "Build a React-like library with Virtual DOM, reconciliation, hooks, and fiber architecture.",
                difficulty: "expert",
                estimatedHours: "60-100",
                prerequisites: ["DOM manipulation", "JavaScript advanced concepts", "Tree data structures"],
                languages: {
                    recommended: ["JavaScript", "TypeScript"],
                    also: []
                },
                resources: [
                    { type: "article", name: "Build your own React", url: "https://pomb.us/build-your-own-react/" },
                    { type: "talk", name: "React Fiber Architecture", url: "https://github.com/acdlite/react-fiber-architecture" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Virtual DOM",
                        description: "Create virtual DOM representation and rendering.",
                        criteria: ["createElement function", "Virtual node structure", "Render to real DOM", "Handle text nodes"],
                        hints: {
                            level1: "VNode is just an object: { type, props, children }. Recursively create DOM elements.",
                            level2: "Handle primitives (string/number) as text nodes. props.children can be array or single element.",
                            level3: "// Virtual DOM element\nfunction createElement(type, props, ...children) {\n    return {\n        type,\n        props: {\n            ...props,\n            children: children.flat().map(child =>\n                typeof child === 'object' ? child : createTextElement(child)\n            )\n        }\n    };\n}\n\nfunction createTextElement(text) {\n    return {\n        type: 'TEXT_ELEMENT',\n        props: {\n            nodeValue: text,\n            children: []\n        }\n    };\n}\n\n// Render vnode to DOM\nfunction render(element, container) {\n    const dom = element.type === 'TEXT_ELEMENT'\n        ? document.createTextNode('')\n        : document.createElement(element.type);\n    \n    // Set properties\n    Object.keys(element.props)\n        .filter(key => key !== 'children')\n        .forEach(name => {\n            if (name.startsWith('on')) {\n                const eventType = name.toLowerCase().substring(2);\n                dom.addEventListener(eventType, element.props[name]);\n            } else {\n                dom[name] = element.props[name];\n            }\n        });\n    \n    // Render children\n    element.props.children.forEach(child => render(child, dom));\n    \n    container.appendChild(dom);\n}"
                        },
                        pitfalls: ["Handling null/undefined children", "Event listener naming", "SVG namespace"],
                        concepts: ["Virtual DOM", "Declarative UI", "Tree representation"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 2,
                        name: "Reconciliation (Diffing)",
                        description: "Implement efficient DOM updates through reconciliation.",
                        criteria: ["Diff old and new trees", "Update only changed nodes", "Handle additions/deletions", "Key-based reconciliation"],
                        hints: {
                            level1: "Compare type first. Same type = update props. Different = replace entire subtree.",
                            level2: "For lists, use keys to match elements. Reorder instead of recreate.",
                            level3: "function reconcile(dom, oldVNode, newVNode) {\n    // Addition\n    if (!oldVNode) {\n        return render(newVNode, dom);\n    }\n    \n    // Deletion\n    if (!newVNode) {\n        dom.remove();\n        return null;\n    }\n    \n    // Replace entire subtree if type changed\n    if (oldVNode.type !== newVNode.type) {\n        const newDom = createDom(newVNode);\n        dom.replaceWith(newDom);\n        return newDom;\n    }\n    \n    // Same type - update props\n    updateProps(dom, oldVNode.props, newVNode.props);\n    \n    // Reconcile children\n    reconcileChildren(dom, oldVNode.props.children, newVNode.props.children);\n    \n    return dom;\n}\n\nfunction reconcileChildren(dom, oldChildren, newChildren) {\n    const oldKeyed = new Map();\n    const newKeyed = new Map();\n    \n    // Index by key\n    oldChildren.forEach((child, i) => {\n        const key = child.props?.key ?? i;\n        oldKeyed.set(key, { vnode: child, dom: dom.childNodes[i] });\n    });\n    \n    newChildren.forEach((child, i) => {\n        const key = child.props?.key ?? i;\n        newKeyed.set(key, child);\n    });\n    \n    // Remove deleted\n    for (const [key, { dom: childDom }] of oldKeyed) {\n        if (!newKeyed.has(key)) {\n            childDom.remove();\n        }\n    }\n    \n    // Update or add\n    newChildren.forEach((newChild, i) => {\n        const key = newChild.props?.key ?? i;\n        const old = oldKeyed.get(key);\n        \n        if (old) {\n            reconcile(old.dom, old.vnode, newChild);\n        } else {\n            render(newChild, dom);\n        }\n    });\n}"
                        },
                        pitfalls: ["Key stability", "Index as key problems", "DOM node reference tracking"],
                        concepts: ["Tree diffing", "Minimal updates", "Keyed reconciliation"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 3,
                        name: "Fiber Architecture",
                        description: "Implement interruptible rendering with fiber nodes.",
                        criteria: ["Fiber node structure", "Work loop with requestIdleCallback", "Commit phase separation", "Prioritization"],
                        hints: {
                            level1: "Fiber = work unit with parent/child/sibling pointers. Process one fiber, then yield.",
                            level2: "Separate render (can interrupt) from commit (must complete). Use double buffering.",
                            level3: "// Fiber node\nlet nextUnitOfWork = null;\nlet wipRoot = null;  // Work in progress root\nlet currentRoot = null;  // Committed root\nlet deletions = [];\n\nfunction createFiber(element, parent) {\n    return {\n        type: element.type,\n        props: element.props,\n        parent,\n        dom: null,\n        child: null,\n        sibling: null,\n        alternate: null,  // Previous fiber\n        effectTag: null   // PLACEMENT, UPDATE, DELETION\n    };\n}\n\nfunction workLoop(deadline) {\n    let shouldYield = false;\n    \n    while (nextUnitOfWork && !shouldYield) {\n        nextUnitOfWork = performUnitOfWork(nextUnitOfWork);\n        shouldYield = deadline.timeRemaining() < 1;\n    }\n    \n    // Commit when all work is done\n    if (!nextUnitOfWork && wipRoot) {\n        commitRoot();\n    }\n    \n    requestIdleCallback(workLoop);\n}\n\nrequestIdleCallback(workLoop);\n\nfunction performUnitOfWork(fiber) {\n    // Create DOM node\n    if (!fiber.dom) {\n        fiber.dom = createDom(fiber);\n    }\n    \n    // Create fibers for children\n    reconcileChildren(fiber, fiber.props.children);\n    \n    // Return next unit of work (depth-first)\n    if (fiber.child) return fiber.child;\n    \n    let nextFiber = fiber;\n    while (nextFiber) {\n        if (nextFiber.sibling) return nextFiber.sibling;\n        nextFiber = nextFiber.parent;\n    }\n    return null;\n}\n\nfunction commitRoot() {\n    deletions.forEach(commitWork);\n    commitWork(wipRoot.child);\n    currentRoot = wipRoot;\n    wipRoot = null;\n}\n\nfunction commitWork(fiber) {\n    if (!fiber) return;\n    \n    const parentDom = fiber.parent.dom;\n    \n    if (fiber.effectTag === 'PLACEMENT' && fiber.dom) {\n        parentDom.appendChild(fiber.dom);\n    } else if (fiber.effectTag === 'DELETION') {\n        parentDom.removeChild(fiber.dom);\n    } else if (fiber.effectTag === 'UPDATE' && fiber.dom) {\n        updateDom(fiber.dom, fiber.alternate.props, fiber.props);\n    }\n    \n    commitWork(fiber.child);\n    commitWork(fiber.sibling);\n}"
                        },
                        pitfalls: ["Effect ordering", "Interrupted state", "Memory leaks in alternate"],
                        concepts: ["Cooperative scheduling", "Incremental rendering", "Work units"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 4,
                        name: "Hooks",
                        description: "Implement useState and useEffect hooks.",
                        criteria: ["useState for local state", "useEffect for side effects", "Hook ordering rules", "Cleanup functions"],
                        hints: {
                            level1: "Track hooks in array per fiber. Index determines which hook. Reset index each render.",
                            level2: "useEffect runs after commit. Compare deps to decide if effect runs.",
                            level3: "let wipFiber = null;\nlet hookIndex = null;\n\nfunction useState(initial) {\n    const oldHook = wipFiber.alternate?.hooks?.[hookIndex];\n    \n    const hook = {\n        state: oldHook ? oldHook.state : initial,\n        queue: []\n    };\n    \n    // Process queued setState calls\n    const actions = oldHook ? oldHook.queue : [];\n    actions.forEach(action => {\n        hook.state = typeof action === 'function' ? action(hook.state) : action;\n    });\n    \n    const setState = action => {\n        hook.queue.push(action);\n        // Trigger re-render\n        wipRoot = {\n            dom: currentRoot.dom,\n            props: currentRoot.props,\n            alternate: currentRoot\n        };\n        nextUnitOfWork = wipRoot;\n        deletions = [];\n    };\n    \n    wipFiber.hooks.push(hook);\n    hookIndex++;\n    return [hook.state, setState];\n}\n\nfunction useEffect(callback, deps) {\n    const oldHook = wipFiber.alternate?.hooks?.[hookIndex];\n    \n    const hasChanged = !oldHook || \n        !deps ||\n        deps.some((dep, i) => dep !== oldHook.deps[i]);\n    \n    const hook = {\n        deps,\n        cleanup: oldHook?.cleanup,\n        effect: hasChanged ? callback : null\n    };\n    \n    wipFiber.hooks.push(hook);\n    hookIndex++;\n}\n\n// In commit phase\nfunction commitEffects(fiber) {\n    if (!fiber) return;\n    \n    fiber.hooks?.forEach(hook => {\n        if (hook.cleanup) hook.cleanup();\n        if (hook.effect) {\n            hook.cleanup = hook.effect();\n        }\n    });\n    \n    commitEffects(fiber.child);\n    commitEffects(fiber.sibling);\n}\n\n// Function component handling\nfunction updateFunctionComponent(fiber) {\n    wipFiber = fiber;\n    hookIndex = 0;\n    wipFiber.hooks = [];\n    \n    const children = [fiber.type(fiber.props)];\n    reconcileChildren(fiber, children);\n}"
                        },
                        pitfalls: ["Conditional hooks", "Stale closures", "Effect cleanup timing"],
                        concepts: ["Hooks pattern", "State management", "Side effects"],
                        estimatedHours: "25-45"
                    }
                ]
            },

            // APP-DEV - EXPERT
            "build-bundler": {
                name: "Build Your Own Bundler",
                description: "Build a JavaScript bundler like webpack/rollup with module resolution, tree shaking, and code splitting.",
                difficulty: "expert",
                estimatedHours: "50-80",
                prerequisites: ["JavaScript AST", "Module systems (CommonJS, ESM)", "File system operations"],
                languages: {
                    recommended: ["JavaScript", "TypeScript"],
                    also: ["Go", "Rust"]
                },
                resources: [
                    { type: "article", name: "Minipack - Simple bundler", url: "https://github.com/ronami/minipack" },
                    { type: "docs", name: "Rollup Plugin Development", url: "https://rollupjs.org/plugin-development/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Module Parsing",
                        description: "Parse JavaScript files and extract dependencies.",
                        criteria: ["Parse ES modules (import/export)", "Extract dependency list", "Handle relative/absolute paths", "Build module graph"],
                        hints: {
                            level1: "Use a parser like @babel/parser to get AST. Walk AST to find ImportDeclaration nodes.",
                            level2: "Track both static imports and dynamic imports. Resolve paths relative to current file.",
                            level3: "const parser = require('@babel/parser');\nconst traverse = require('@babel/traverse').default;\nconst path = require('path');\nconst fs = require('fs');\n\nfunction parseModule(filePath) {\n    const content = fs.readFileSync(filePath, 'utf-8');\n    \n    const ast = parser.parse(content, {\n        sourceType: 'module',\n        plugins: ['jsx']\n    });\n    \n    const dependencies = [];\n    \n    traverse(ast, {\n        ImportDeclaration({ node }) {\n            dependencies.push({\n                source: node.source.value,\n                specifiers: node.specifiers.map(s => ({\n                    type: s.type,\n                    imported: s.imported?.name,\n                    local: s.local.name\n                }))\n            });\n        },\n        \n        ExportNamedDeclaration({ node }) {\n            if (node.source) {\n                dependencies.push({\n                    source: node.source.value,\n                    isReExport: true\n                });\n            }\n        },\n        \n        CallExpression({ node }) {\n            if (node.callee.type === 'Import') {\n                // Dynamic import\n                if (node.arguments[0].type === 'StringLiteral') {\n                    dependencies.push({\n                        source: node.arguments[0].value,\n                        isDynamic: true\n                    });\n                }\n            }\n        }\n    });\n    \n    return {\n        filePath,\n        ast,\n        content,\n        dependencies\n    };\n}\n\nfunction buildModuleGraph(entryPath) {\n    const modules = new Map();\n    const queue = [path.resolve(entryPath)];\n    \n    while (queue.length > 0) {\n        const filePath = queue.shift();\n        \n        if (modules.has(filePath)) continue;\n        \n        const module = parseModule(filePath);\n        modules.set(filePath, module);\n        \n        for (const dep of module.dependencies) {\n            const resolvedPath = resolvePath(dep.source, filePath);\n            module.dependencies.find(d => d.source === dep.source).resolved = resolvedPath;\n            queue.push(resolvedPath);\n        }\n    }\n    \n    return modules;\n}"
                        },
                        pitfalls: ["Circular dependencies", "Node modules resolution", "File extensions"],
                        concepts: ["AST parsing", "Dependency analysis", "Module graph"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "Module Resolution",
                        description: "Implement Node.js-style module resolution.",
                        criteria: ["Relative path resolution", "node_modules lookup", "Package.json main/exports", "Index file fallback"],
                        hints: {
                            level1: "Relative: resolve against current file. Bare specifiers: walk up node_modules.",
                            level2: "Check package.json for main field. Try .js, .json, /index.js extensions.",
                            level3: "function resolvePath(specifier, fromPath) {\n    const fromDir = path.dirname(fromPath);\n    \n    // Relative or absolute path\n    if (specifier.startsWith('.') || specifier.startsWith('/')) {\n        return resolveFile(path.resolve(fromDir, specifier));\n    }\n    \n    // Bare specifier - node_modules\n    return resolveNodeModule(specifier, fromDir);\n}\n\nfunction resolveFile(filePath) {\n    // Try exact path\n    if (fs.existsSync(filePath) && fs.statSync(filePath).isFile()) {\n        return filePath;\n    }\n    \n    // Try extensions\n    const extensions = ['.js', '.jsx', '.ts', '.tsx', '.json'];\n    for (const ext of extensions) {\n        if (fs.existsSync(filePath + ext)) {\n            return filePath + ext;\n        }\n    }\n    \n    // Try as directory\n    if (fs.existsSync(filePath) && fs.statSync(filePath).isDirectory()) {\n        // Check package.json\n        const pkgPath = path.join(filePath, 'package.json');\n        if (fs.existsSync(pkgPath)) {\n            const pkg = JSON.parse(fs.readFileSync(pkgPath, 'utf-8'));\n            if (pkg.main) {\n                return resolveFile(path.join(filePath, pkg.main));\n            }\n        }\n        \n        // Try index\n        return resolveFile(path.join(filePath, 'index'));\n    }\n    \n    throw new Error(`Cannot resolve: ${filePath}`);\n}\n\nfunction resolveNodeModule(specifier, fromDir) {\n    // Handle scoped packages\n    const parts = specifier.split('/');\n    const packageName = specifier.startsWith('@') \n        ? parts.slice(0, 2).join('/') \n        : parts[0];\n    const subpath = specifier.startsWith('@')\n        ? parts.slice(2).join('/')\n        : parts.slice(1).join('/');\n    \n    // Walk up directory tree\n    let dir = fromDir;\n    while (dir !== path.dirname(dir)) {\n        const nodeModules = path.join(dir, 'node_modules', packageName);\n        if (fs.existsSync(nodeModules)) {\n            if (subpath) {\n                return resolveFile(path.join(nodeModules, subpath));\n            }\n            return resolveFile(nodeModules);\n        }\n        dir = path.dirname(dir);\n    }\n    \n    throw new Error(`Cannot find module: ${specifier}`);\n}"
                        },
                        pitfalls: ["Symlinks", "Package exports field", "Conditional exports"],
                        concepts: ["Module resolution", "Package management", "Path resolution"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 3,
                        name: "Bundle Generation",
                        description: "Generate a single JavaScript bundle from module graph.",
                        criteria: ["Wrap modules in functions", "Module registry", "Handle imports/exports", "Source maps"],
                        hints: {
                            level1: "Each module becomes a function. Registry maps IDs to modules. Require loads and caches.",
                            level2: "Transform import/export to require/exports. Assign numeric IDs to modules.",
                            level3: "const generate = require('@babel/generator').default;\nconst t = require('@babel/types');\n\nfunction generateBundle(moduleGraph, entryPath) {\n    const modules = [];\n    const moduleIds = new Map();\n    let nextId = 0;\n    \n    // Assign IDs\n    for (const [filePath] of moduleGraph) {\n        moduleIds.set(filePath, nextId++);\n    }\n    \n    // Transform each module\n    for (const [filePath, module] of moduleGraph) {\n        const id = moduleIds.get(filePath);\n        const transformedCode = transformModule(module, moduleIds);\n        \n        modules.push(`\n            ${id}: function(module, exports, require) {\n                ${transformedCode}\n            }`);\n    }\n    \n    const entryId = moduleIds.get(path.resolve(entryPath));\n    \n    return `\n        (function(modules) {\n            const installedModules = {};\n            \n            function require(moduleId) {\n                if (installedModules[moduleId]) {\n                    return installedModules[moduleId].exports;\n                }\n                \n                const module = installedModules[moduleId] = {\n                    exports: {}\n                };\n                \n                modules[moduleId](module, module.exports, require);\n                \n                return module.exports;\n            }\n            \n            return require(${entryId});\n        })({${modules.join(',')}});\n    `;\n}\n\nfunction transformModule(module, moduleIds) {\n    traverse(module.ast, {\n        ImportDeclaration(path) {\n            const depId = moduleIds.get(path.node._resolved);\n            // Transform to require\n            // import { foo } from './bar' -> const { foo } = require(1)\n            const requireCall = t.callExpression(\n                t.identifier('require'),\n                [t.numericLiteral(depId)]\n            );\n            // ... handle different import types\n            path.replaceWith(/* ... */);\n        },\n        \n        ExportNamedDeclaration(path) {\n            // exports.foo = foo;\n        },\n        \n        ExportDefaultDeclaration(path) {\n            // exports.default = ...;\n        }\n    });\n    \n    return generate(module.ast).code;\n}"
                        },
                        pitfalls: ["Circular dependencies", "Live bindings", "Default export handling"],
                        concepts: ["Code generation", "Module wrapping", "Runtime"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 4,
                        name: "Tree Shaking",
                        description: "Eliminate unused code from the bundle.",
                        criteria: ["Track used exports", "Remove unused code", "Handle side effects", "Preserve necessary code"],
                        hints: {
                            level1: "Build usage graph from imports. Mark used exports. Remove unused declarations.",
                            level2: "Side effects (top-level code) must be preserved unless marked pure.",
                            level3: "function treeShake(moduleGraph, entryPath) {\n    // Track what's used\n    const usedExports = new Map();  // filePath -> Set of export names\n    const usedModules = new Set();\n    \n    // Start from entry\n    function markUsed(filePath, importedNames) {\n        usedModules.add(filePath);\n        \n        const module = moduleGraph.get(filePath);\n        if (!usedExports.has(filePath)) {\n            usedExports.set(filePath, new Set());\n        }\n        \n        const used = usedExports.get(filePath);\n        \n        if (importedNames === '*') {\n            // Namespace import - mark all\n            module.exports.forEach(exp => used.add(exp));\n        } else {\n            importedNames.forEach(name => used.add(name));\n        }\n        \n        // Follow dependencies\n        for (const dep of module.dependencies) {\n            const names = dep.specifiers?.map(s => s.imported || 'default') || ['*'];\n            markUsed(dep.resolved, names);\n        }\n    }\n    \n    markUsed(path.resolve(entryPath), ['*']);\n    \n    // Remove unused\n    for (const [filePath, module] of moduleGraph) {\n        if (!usedModules.has(filePath)) {\n            moduleGraph.delete(filePath);\n            continue;\n        }\n        \n        const used = usedExports.get(filePath);\n        \n        traverse(module.ast, {\n            ExportNamedDeclaration(path) {\n                if (path.node.declaration) {\n                    const name = path.node.declaration.id?.name;\n                    if (name && !used.has(name)) {\n                        // Check for side effects\n                        if (!hasSideEffects(path.node.declaration)) {\n                            path.remove();\n                        }\n                    }\n                }\n            },\n            \n            // Remove unused top-level declarations\n            FunctionDeclaration(path) {\n                if (path.parent.type === 'Program') {\n                    const name = path.node.id.name;\n                    if (!isUsed(name, module, used)) {\n                        path.remove();\n                    }\n                }\n            }\n        });\n    }\n    \n    return moduleGraph;\n}\n\nfunction hasSideEffects(node) {\n    // Conservative: assume functions are pure\n    // But check for top-level calls, assignments to globals, etc.\n    let found = false;\n    traverse(node, {\n        CallExpression() { found = true; },\n        AssignmentExpression({ node }) {\n            if (node.left.type === 'MemberExpression') found = true;\n        }\n    });\n    return found;\n}"
                        },
                        pitfalls: ["Side effect detection", "Re-exports", "Dynamic access patterns"],
                        concepts: ["Dead code elimination", "Static analysis", "Purity"],
                        estimatedHours: "17-28"
                    }
                ]
            },

            // APP-DEV - EXPERT
            "build-web-framework": {
                name: "Build Your Own Web Framework",
                description: "Build a web framework like Express or Django with routing, middleware, and templating.",
                difficulty: "expert",
                estimatedHours: "40-70",
                prerequisites: ["HTTP server", "Request/response handling", "Basic routing"],
                languages: {
                    recommended: ["JavaScript/Node.js", "Python", "Go"],
                    also: ["Ruby", "Rust"]
                },
                resources: [
                    { type: "code", name: "Express.js source", url: "https://github.com/expressjs/express" },
                    { type: "article", name: "Build Express from Scratch", url: "https://www.freecodecamp.org/news/express-explained-with-examples-installation-routing-middleware-and-more/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Routing",
                        description: "Implement URL routing with parameters and methods.",
                        criteria: ["Method-based routing (GET, POST, etc.)", "Path parameters (/users/:id)", "Query string parsing", "Route matching"],
                        hints: {
                            level1: "Store routes as array of { method, pattern, handler }. Match in order.",
                            level2: "Convert path patterns to regex. Capture groups for parameters.",
                            level3: "class Router {\n    constructor() {\n        this.routes = [];\n    }\n    \n    addRoute(method, path, handler) {\n        const { pattern, paramNames } = this.compilePath(path);\n        this.routes.push({ method, pattern, paramNames, handler });\n    }\n    \n    compilePath(path) {\n        const paramNames = [];\n        \n        // Convert /users/:id/posts/:postId to regex\n        const pattern = path.replace(/:([^/]+)/g, (_, name) => {\n            paramNames.push(name);\n            return '([^/]+)';\n        });\n        \n        return {\n            pattern: new RegExp(`^${pattern}$`),\n            paramNames\n        };\n    }\n    \n    match(method, url) {\n        const [path, queryString] = url.split('?');\n        \n        for (const route of this.routes) {\n            if (route.method !== method && route.method !== 'ALL') continue;\n            \n            const match = path.match(route.pattern);\n            if (match) {\n                // Extract params\n                const params = {};\n                route.paramNames.forEach((name, i) => {\n                    params[name] = decodeURIComponent(match[i + 1]);\n                });\n                \n                // Parse query string\n                const query = this.parseQuery(queryString);\n                \n                return { handler: route.handler, params, query };\n            }\n        }\n        \n        return null;\n    }\n    \n    parseQuery(queryString) {\n        if (!queryString) return {};\n        \n        return queryString.split('&').reduce((acc, pair) => {\n            const [key, value] = pair.split('=').map(decodeURIComponent);\n            acc[key] = value;\n            return acc;\n        }, {});\n    }\n    \n    // Convenience methods\n    get(path, handler) { this.addRoute('GET', path, handler); }\n    post(path, handler) { this.addRoute('POST', path, handler); }\n    put(path, handler) { this.addRoute('PUT', path, handler); }\n    delete(path, handler) { this.addRoute('DELETE', path, handler); }\n}"
                        },
                        pitfalls: ["Route ordering matters", "URL encoding", "Trailing slashes"],
                        concepts: ["URL routing", "Pattern matching", "HTTP methods"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 2,
                        name: "Middleware",
                        description: "Implement middleware pipeline for request processing.",
                        criteria: ["Middleware chain", "next() function", "Error handling middleware", "Async middleware support"],
                        hints: {
                            level1: "Middleware is function(req, res, next). Call next() to continue chain.",
                            level2: "Error middleware has 4 params: (err, req, res, next). Skip to error handlers on throw.",
                            level3: "class Application {\n    constructor() {\n        this.middleware = [];\n        this.router = new Router();\n    }\n    \n    use(pathOrFn, fn) {\n        if (typeof pathOrFn === 'function') {\n            this.middleware.push({ path: '/', handler: pathOrFn });\n        } else {\n            this.middleware.push({ path: pathOrFn, handler: fn });\n        }\n    }\n    \n    async handleRequest(req, res) {\n        // Enhance request/response\n        req.params = {};\n        req.query = {};\n        res.json = (data) => {\n            res.setHeader('Content-Type', 'application/json');\n            res.end(JSON.stringify(data));\n        };\n        res.status = (code) => { res.statusCode = code; return res; };\n        \n        // Collect applicable middleware\n        const stack = [\n            ...this.middleware.filter(m => req.url.startsWith(m.path)),\n            { handler: this.routeHandler.bind(this) }\n        ];\n        \n        let index = 0;\n        let error = null;\n        \n        const next = async (err) => {\n            if (err) error = err;\n            \n            while (index < stack.length) {\n                const layer = stack[index++];\n                const handler = layer.handler;\n                \n                try {\n                    if (error) {\n                        // Error handler has 4 params\n                        if (handler.length === 4) {\n                            await handler(error, req, res, next);\n                            error = null;\n                            return;\n                        }\n                    } else {\n                        if (handler.length < 4) {\n                            await handler(req, res, next);\n                            return;\n                        }\n                    }\n                } catch (e) {\n                    error = e;\n                }\n            }\n            \n            // Unhandled error\n            if (error) {\n                res.statusCode = 500;\n                res.end('Internal Server Error');\n            }\n        };\n        \n        await next();\n    }\n    \n    routeHandler(req, res, next) {\n        const match = this.router.match(req.method, req.url);\n        if (match) {\n            req.params = match.params;\n            req.query = match.query;\n            match.handler(req, res, next);\n        } else {\n            res.statusCode = 404;\n            res.end('Not Found');\n        }\n    }\n}"
                        },
                        pitfalls: ["Calling next multiple times", "Async error handling", "Middleware ordering"],
                        concepts: ["Middleware pattern", "Pipeline", "Error propagation"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 3,
                        name: "Request/Response Enhancement",
                        description: "Add convenience methods and body parsing.",
                        criteria: ["Body parsing (JSON, form)", "Response helpers (json, redirect, send)", "Cookie handling", "Headers management"],
                        hints: {
                            level1: "Body comes as stream. Buffer chunks, parse based on Content-Type.",
                            level2: "Add methods to res object for common patterns. Handle content negotiation.",
                            level3: "// Body parser middleware\nfunction bodyParser() {\n    return async (req, res, next) => {\n        const contentType = req.headers['content-type'] || '';\n        \n        // Collect body\n        const chunks = [];\n        for await (const chunk of req) {\n            chunks.push(chunk);\n        }\n        const body = Buffer.concat(chunks).toString();\n        \n        if (contentType.includes('application/json')) {\n            try {\n                req.body = JSON.parse(body);\n            } catch (e) {\n                req.body = {};\n            }\n        } else if (contentType.includes('application/x-www-form-urlencoded')) {\n            req.body = Object.fromEntries(new URLSearchParams(body));\n        } else {\n            req.body = body;\n        }\n        \n        next();\n    };\n}\n\n// Response enhancements\nfunction enhanceResponse(res) {\n    res.json = function(data) {\n        this.setHeader('Content-Type', 'application/json');\n        this.end(JSON.stringify(data));\n        return this;\n    };\n    \n    res.send = function(data) {\n        if (typeof data === 'object') {\n            return this.json(data);\n        }\n        this.setHeader('Content-Type', 'text/html');\n        this.end(String(data));\n        return this;\n    };\n    \n    res.redirect = function(url, status = 302) {\n        this.statusCode = status;\n        this.setHeader('Location', url);\n        this.end();\n        return this;\n    };\n    \n    res.cookie = function(name, value, options = {}) {\n        let cookie = `${name}=${encodeURIComponent(value)}`;\n        if (options.maxAge) cookie += `; Max-Age=${options.maxAge}`;\n        if (options.httpOnly) cookie += '; HttpOnly';\n        if (options.secure) cookie += '; Secure';\n        if (options.path) cookie += `; Path=${options.path}`;\n        this.setHeader('Set-Cookie', cookie);\n        return this;\n    };\n}\n\n// Cookie parser\nfunction cookieParser() {\n    return (req, res, next) => {\n        req.cookies = {};\n        const cookieHeader = req.headers.cookie;\n        if (cookieHeader) {\n            cookieHeader.split(';').forEach(cookie => {\n                const [name, value] = cookie.trim().split('=');\n                req.cookies[name] = decodeURIComponent(value);\n            });\n        }\n        next();\n    };\n}"
                        },
                        pitfalls: ["Large body handling", "Content-Type edge cases", "Cookie security"],
                        concepts: ["Request parsing", "Response helpers", "HTTP cookies"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 4,
                        name: "Template Engine",
                        description: "Implement a simple template engine for HTML rendering.",
                        criteria: ["Variable interpolation", "Control flow (if, for)", "Includes/partials", "Escaping"],
                        hints: {
                            level1: "Replace {{ variable }} with values. Compile template to function for performance.",
                            level2: "Parse control structures into AST. Generate JavaScript code to build string.",
                            level3: "class TemplateEngine {\n    constructor(options = {}) {\n        this.cache = new Map();\n        this.viewsDir = options.views || './views';\n    }\n    \n    compile(template) {\n        // Convert template to function\n        let code = 'let __output = [];\\n';\n        let cursor = 0;\n        \n        // Match {{ }}, {% %}, {# #}\n        const regex = /\\{\\{([\\s\\S]+?)\\}\\}|\\{%([\\s\\S]+?)%\\}|\\{#[\\s\\S]+?#\\}/g;\n        let match;\n        \n        while ((match = regex.exec(template)) !== null) {\n            // Add literal text before match\n            if (match.index > cursor) {\n                const text = template.slice(cursor, match.index);\n                code += `__output.push(${JSON.stringify(text)});\\n`;\n            }\n            \n            if (match[1]) {\n                // {{ expression }} - output with escaping\n                const expr = match[1].trim();\n                code += `__output.push(__escape(${expr}));\\n`;\n            } else if (match[2]) {\n                // {% statement %}\n                const stmt = match[2].trim();\n                \n                if (stmt.startsWith('if ')) {\n                    code += `if (${stmt.slice(3)}) {\\n`;\n                } else if (stmt === 'endif') {\n                    code += '}\\n';\n                } else if (stmt.startsWith('for ')) {\n                    // {% for item in items %}\n                    const forMatch = stmt.match(/for (\\w+) in (\\w+)/);\n                    if (forMatch) {\n                        code += `for (const ${forMatch[1]} of ${forMatch[2]}) {\\n`;\n                    }\n                } else if (stmt === 'endfor') {\n                    code += '}\\n';\n                } else if (stmt.startsWith('include ')) {\n                    const includePath = stmt.slice(8).trim().replace(/['\"`]/g, '');\n                    code += `__output.push(__include('${includePath}', __context));\\n`;\n                }\n            }\n            // {# comments #} are ignored\n            \n            cursor = match.index + match[0].length;\n        }\n        \n        // Add remaining text\n        if (cursor < template.length) {\n            code += `__output.push(${JSON.stringify(template.slice(cursor))});\\n`;\n        }\n        \n        code += 'return __output.join(\"\");';\n        \n        // Create function\n        return new Function('__context', '__escape', '__include', `\n            with (__context) {\n                ${code}\n            }\n        `);\n    }\n    \n    render(templatePath, context = {}) {\n        if (!this.cache.has(templatePath)) {\n            const fullPath = path.join(this.viewsDir, templatePath);\n            const template = fs.readFileSync(fullPath, 'utf-8');\n            this.cache.set(templatePath, this.compile(template));\n        }\n        \n        const fn = this.cache.get(templatePath);\n        \n        const escape = (str) => String(str)\n            .replace(/&/g, '&amp;')\n            .replace(/</g, '&lt;')\n            .replace(/>/g, '&gt;')\n            .replace(/\"/g, '&quot;');\n        \n        const include = (path, ctx) => this.render(path, ctx);\n        \n        return fn(context, escape, include);\n    }\n}"
                        },
                        pitfalls: ["XSS prevention", "Template injection", "Performance with large templates"],
                        concepts: ["Template compilation", "Code generation", "HTML escaping"],
                        estimatedHours: "16-33"
                    }
                ]
            },

            // SOFTWARE-ENGINEERING - BEGINNER
            "documentation-project": {
                name: "Documentation Project",
                description: "Learn to write effective documentation including README files, API docs, and code comments.",
                difficulty: "beginner",
                estimatedHours: "8-15",
                prerequisites: ["Basic programming", "Markdown"],
                languages: {
                    recommended: ["Markdown", "Any programming language"],
                    also: []
                },
                resources: [
                    { type: "guide", name: "Make a README", url: "https://www.makeareadme.com/" },
                    { type: "article", name: "How to Write Good Documentation", url: "https://documentation.divio.com/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "README Essentials",
                        description: "Create a comprehensive README for a project.",
                        criteria: ["Project title and description", "Installation instructions", "Usage examples", "License and contribution guidelines"],
                        hints: {
                            level1: "Start with what the project does, then how to use it. Add badges for build status.",
                            level2: "Include screenshots for visual projects. Add table of contents for long READMEs.",
                            level3: "# Project Name\n\n![Build Status](https://img.shields.io/badge/build-passing-green)\n![License](https://img.shields.io/badge/license-MIT-blue)\n\nOne-paragraph description of what this project does and why it exists.\n\n## Features\n\n- Feature 1: Brief description\n- Feature 2: Brief description\n- Feature 3: Brief description\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/user/project.git\ncd project\n\n# Install dependencies\nnpm install\n\n# Set up environment\ncp .env.example .env\n```\n\n## Quick Start\n\n```javascript\nconst project = require('project');\n\n// Basic usage example\nconst result = project.doSomething('input');\nconsole.log(result);\n```\n\n## Configuration\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `timeout` | number | 5000 | Request timeout in ms |\n| `retries` | number | 3 | Number of retry attempts |\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## License\n\nMIT - see [LICENSE](LICENSE)"
                        },
                        pitfalls: ["Outdated installation steps", "Missing prerequisites", "Assumed knowledge"],
                        concepts: ["Technical writing", "User empathy", "Information architecture"],
                        estimatedHours: "2-3"
                    },
                    {
                        id: 2,
                        name: "API Documentation",
                        description: "Document an API with endpoints, parameters, and examples.",
                        criteria: ["Endpoint descriptions", "Request/response formats", "Authentication details", "Error codes"],
                        hints: {
                            level1: "Use consistent format for each endpoint. Include curl examples.",
                            level2: "Document all possible error responses. Show request/response bodies.",
                            level3: "# API Documentation\n\n## Authentication\n\nAll API requests require a Bearer token:\n\n```\nAuthorization: Bearer <your-token>\n```\n\n## Endpoints\n\n### Get User\n\n```\nGET /api/users/:id\n```\n\nRetrieve a user by their ID.\n\n**Parameters**\n\n| Name | Type | In | Description |\n|------|------|-----|-------------|\n| id | string | path | User ID (required) |\n| include | string | query | Comma-separated relations to include |\n\n**Response**\n\n```json\n{\n  \"id\": \"123\",\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\",\n  \"createdAt\": \"2024-01-15T10:30:00Z\"\n}\n```\n\n**Errors**\n\n| Status | Code | Description |\n|--------|------|-------------|\n| 404 | USER_NOT_FOUND | User does not exist |\n| 401 | UNAUTHORIZED | Missing or invalid token |\n\n**Example**\n\n```bash\ncurl -X GET 'https://api.example.com/api/users/123' \\\n  -H 'Authorization: Bearer token123'\n```"
                        },
                        pitfalls: ["Inconsistent formatting", "Missing error cases", "Stale examples"],
                        concepts: ["API design", "REST conventions", "Developer experience"],
                        estimatedHours: "2-4"
                    },
                    {
                        id: 3,
                        name: "Code Comments",
                        description: "Write effective code comments and docstrings.",
                        criteria: ["Function/method documentation", "Complex logic explanation", "TODO/FIXME usage", "Avoiding obvious comments"],
                        hints: {
                            level1: "Comment WHY, not WHAT. The code shows what, comments explain reasoning.",
                            level2: "Use docstrings for public APIs. Keep comments updated with code.",
                            level3: "// BAD: Obvious comment\n// Increment counter by 1\ncounter++;\n\n// GOOD: Explains why\n// Use post-increment to return old value before updating\nreturn counter++;\n\n// BAD: Just restates the code\n// Check if user is admin\nif (user.role === 'admin') { ... }\n\n// GOOD: Explains business logic\n// Admins can bypass rate limiting to handle support escalations\nif (user.role === 'admin') { ... }\n\n/**\n * Calculates the optimal batch size for database operations.\n * \n * Uses an adaptive algorithm that considers:\n * - Current memory pressure\n * - Network latency to database\n * - Historical query performance\n * \n * @param {Object} metrics - Current system metrics\n * @param {number} metrics.memoryUsage - Memory usage percentage (0-100)\n * @param {number} metrics.avgLatency - Average DB latency in ms\n * @returns {number} Optimal batch size between 100 and 10000\n * \n * @example\n * const size = calculateBatchSize({ memoryUsage: 70, avgLatency: 50 });\n * // Returns ~2500 for moderate load\n */\nfunction calculateBatchSize(metrics) {\n    // Start with base size, adjust based on conditions\n    let size = 1000;\n    \n    // Reduce batch size under memory pressure to prevent OOM\n    // Threshold of 80% is based on production incidents in Q3 2023\n    if (metrics.memoryUsage > 80) {\n        size = Math.floor(size * 0.5);\n    }\n    \n    // TODO(perf): Consider adding connection pool size to calculation\n    // FIXME: This doesn't account for concurrent batch operations\n    \n    return Math.max(100, Math.min(size, 10000));\n}"
                        },
                        pitfalls: ["Comment rot", "Over-commenting", "Misleading comments"],
                        concepts: ["Self-documenting code", "Documentation debt", "Maintainability"],
                        estimatedHours: "2-4"
                    },
                    {
                        id: 4,
                        name: "Architecture Documentation",
                        description: "Document system architecture and design decisions.",
                        criteria: ["System overview diagram", "Component descriptions", "Data flow documentation", "ADRs (Architecture Decision Records)"],
                        hints: {
                            level1: "Start with high-level diagram. Then detail each component.",
                            level2: "Use ADRs to record why decisions were made. Include alternatives considered.",
                            level3: "# Architecture Overview\n\n## System Diagram\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Client    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   API GW    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Services   ‚îÇ\n‚îÇ  (React)    ‚îÇ     ‚îÇ  (Kong)     ‚îÇ     ‚îÇ  (Node.js)  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                                ‚îÇ\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                    ‚îÇ    Redis    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  PostgreSQL ‚îÇ\n                    ‚îÇ   (Cache)   ‚îÇ     ‚îÇ    (DB)     ‚îÇ\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Components\n\n### API Gateway\n- **Technology**: Kong\n- **Responsibility**: Rate limiting, authentication, routing\n- **Scaling**: Horizontal, 2-4 instances\n\n### Service Layer\n- **Technology**: Node.js with Express\n- **Responsibility**: Business logic, data validation\n- **Communication**: REST between services\n\n---\n\n# ADR-001: Use PostgreSQL for Primary Database\n\n## Status\nAccepted\n\n## Context\nWe need a primary database for user data, transactions, and product catalog.\n\n## Decision\nWe will use PostgreSQL.\n\n## Alternatives Considered\n1. **MySQL**: Familiar, but JSON support is weaker\n2. **MongoDB**: Flexible schema, but we need ACID transactions\n\n## Consequences\n- Good: Strong consistency, excellent JSON support with JSONB\n- Good: Mature ecosystem, easy to find developers\n- Bad: Schema migrations require more planning\n- Bad: Horizontal scaling requires additional tools (Citus)"
                        },
                        pitfalls: ["Diagrams becoming outdated", "Too much detail", "Missing rationale"],
                        concepts: ["System design", "Technical communication", "Decision documentation"],
                        estimatedHours: "2-4"
                    }
                ]
            },

            // SOFTWARE-ENGINEERING - INTERMEDIATE
            "code-review-practice": {
                name: "Code Review Practice",
                description: "Learn effective code review techniques by reviewing real pull requests.",
                difficulty: "intermediate",
                estimatedHours: "10-20",
                prerequisites: ["Programming experience", "Version control (Git)"],
                languages: {
                    recommended: ["Any"],
                    also: []
                },
                resources: [
                    { type: "article", name: "Google's Code Review Guidelines", url: "https://google.github.io/eng-practices/review/" },
                    { type: "book", name: "The Art of Readable Code", url: "https://www.oreilly.com/library/view/the-art-of/9781449318482/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Review Fundamentals",
                        description: "Learn what to look for in code reviews.",
                        criteria: ["Correctness checks", "Style consistency", "Performance considerations", "Security review"],
                        hints: {
                            level1: "Start with: Does it work? Then: Is it maintainable? Finally: Is it optimal?",
                            level2: "Check edge cases, error handling, and test coverage. Look for security issues.",
                            level3: "## Code Review Checklist\n\n### Correctness\n- [ ] Does the code do what the PR description says?\n- [ ] Are edge cases handled?\n- [ ] Are errors handled appropriately?\n- [ ] Do the tests actually test the functionality?\n\n### Design\n- [ ] Is the code in the right place (right file, right layer)?\n- [ ] Does it follow existing patterns in the codebase?\n- [ ] Is it over-engineered or under-engineered?\n- [ ] Are there any obvious performance issues?\n\n### Readability\n- [ ] Are variable/function names clear and descriptive?\n- [ ] Is the code self-documenting or properly commented?\n- [ ] Is the logic easy to follow?\n- [ ] Are there magic numbers that should be constants?\n\n### Security (especially for web apps)\n- [ ] Is user input validated and sanitized?\n- [ ] Are there any SQL injection vulnerabilities?\n- [ ] Is authentication/authorization checked?\n- [ ] Are secrets hardcoded?\n\n### Testing\n- [ ] Are there tests for new functionality?\n- [ ] Do tests cover edge cases?\n- [ ] Are tests readable and maintainable?\n- [ ] Is test coverage adequate?"
                        },
                        pitfalls: ["Nitpicking style over substance", "Missing the forest for trees", "Being too harsh"],
                        concepts: ["Code quality", "Defensive programming", "Best practices"],
                        estimatedHours: "2-4"
                    },
                    {
                        id: 2,
                        name: "Giving Feedback",
                        description: "Practice giving constructive, actionable feedback.",
                        criteria: ["Constructive tone", "Specific suggestions", "Distinguishing blockers from suggestions", "Asking questions vs making demands"],
                        hints: {
                            level1: "Be kind. Critique the code, not the person. Offer alternatives.",
                            level2: "Prefix with 'nit:', 'suggestion:', 'blocking:' to indicate severity.",
                            level3: "## Feedback Examples\n\n### Bad Feedback\n‚ùå \"This is wrong.\"\n‚ùå \"Why would you do it this way?\"\n‚ùå \"This code is confusing.\"\n\n### Good Feedback\n‚úÖ \"This will throw if `user` is null. Consider adding a null check:\n   ```javascript\n   if (!user) return null;\n   ```\"\n\n‚úÖ \"nit: This could be simplified using optional chaining:\n   ```javascript\n   // Instead of\n   user && user.profile && user.profile.name\n   // Consider\n   user?.profile?.name\n   ```\"\n\n‚úÖ \"question: I'm not familiar with this pattern - could you explain \n   why we're caching here instead of at the service layer?\"\n\n‚úÖ \"suggestion: This loop runs O(n¬≤). For large lists, we might want\n   to use a Set for O(n) lookup. Not blocking since our current\n   data is small, but worth considering for the future.\"\n\n### Severity Prefixes\n- **blocking**: Must fix before merge\n- **suggestion**: Recommended but optional\n- **nit**: Minor style/preference (totally optional)\n- **question**: Seeking clarification\n- **praise**: Highlighting good work! üéâ"
                        },
                        pitfalls: ["Being overly negative", "Vague feedback", "Not explaining why"],
                        concepts: ["Communication", "Empathy", "Teaching"],
                        estimatedHours: "3-5"
                    },
                    {
                        id: 3,
                        name: "Review Exercises",
                        description: "Practice reviewing real code with common issues.",
                        criteria: ["Find bugs in provided code", "Suggest improvements", "Identify security issues", "Evaluate test quality"],
                        hints: {
                            level1: "Look at open source projects' PRs. Many have good review discussions.",
                            level2: "Practice with intentionally buggy code. Time yourself to build speed.",
                            level3: "## Exercise: Review This Code\n\n```javascript\n// User authentication endpoint\napp.post('/login', async (req, res) => {\n  const { email, password } = req.body;\n  \n  const user = await db.query(\n    `SELECT * FROM users WHERE email = '${email}'`\n  );\n  \n  if (user && password === user.password) {\n    const token = jwt.sign({ id: user.id }, 'secret123');\n    res.json({ token });\n  } else {\n    res.status(401).json({ error: 'Invalid credentials' });\n  }\n});\n```\n\n### Issues to Find:\n1. **SQL Injection** (Critical): Using string interpolation in query\n   - Fix: Use parameterized queries\n   \n2. **Plaintext passwords** (Critical): Comparing passwords directly\n   - Fix: Use bcrypt.compare()\n   \n3. **Hardcoded secret** (High): JWT secret in code\n   - Fix: Use environment variable\n   \n4. **Timing attack** (Medium): Different response times for user-exists vs wrong-password\n   - Fix: Always do password comparison\n   \n5. **No input validation** (Medium): email/password not validated\n   - Fix: Validate format and length\n   \n6. **Missing rate limiting** (Medium): Allows brute force\n   - Fix: Add rate limiter middleware"
                        },
                        pitfalls: ["Missing subtle bugs", "Focusing only on obvious issues", "Not considering context"],
                        concepts: ["Bug patterns", "Security awareness", "Code smells"],
                        estimatedHours: "3-6"
                    },
                    {
                        id: 4,
                        name: "Receiving Feedback",
                        description: "Learn to respond professionally to code review feedback.",
                        criteria: ["Accepting valid criticism", "Discussing disagreements constructively", "Learning from reviews", "Knowing when to push back"],
                        hints: {
                            level1: "Assume good intent. Reviewers want to help. Ask for clarification if needed.",
                            level2: "It's okay to disagree, but explain your reasoning. Be open to being wrong.",
                            level3: "## Responding to Reviews\n\n### Good Responses\n\n**To valid feedback:**\n> \"Good catch! Fixed in abc123.\"\n\n**To suggestions you disagree with:**\n> \"I considered that approach, but went with X because [reason].\n> Happy to change if you feel strongly - what do you think?\"\n\n**To feedback you don't understand:**\n> \"Could you elaborate on this? I'm not sure I understand\n> the concern with the current approach.\"\n\n**To nitpicks you'll skip:**\n> \"Noted for future PRs! Keeping as-is for now to limit\n> scope of this change.\"\n\n### Handling Disagreements\n\n1. **Assume good intent** - They're trying to help\n2. **Seek to understand** - Ask questions\n3. **Explain your reasoning** - Share context they might lack\n4. **Find common ground** - What do you both agree on?\n5. **Escalate gracefully** - Get a third opinion if stuck\n\n### When to Push Back\n\n- Reviewer is enforcing personal preference, not team standard\n- Suggested change would introduce bugs or regressions\n- Change is out of scope for this PR\n- You have context the reviewer doesn't\n\nAlways push back respectfully with reasoning!"
                        },
                        pitfalls: ["Taking feedback personally", "Being defensive", "Blindly accepting all feedback"],
                        concepts: ["Professional growth", "Collaboration", "Ego management"],
                        estimatedHours: "2-5"
                    }
                ]
            },

            // SOFTWARE-ENGINEERING - ADVANCED
            "cd-deployment": {
                name: "CD with Blue-Green Deployment",
                description: "Implement continuous deployment with zero-downtime blue-green deployment strategy.",
                difficulty: "advanced",
                estimatedHours: "20-35",
                prerequisites: ["CI pipeline basics", "Docker/containers", "Load balancer concepts", "Shell scripting"],
                languages: {
                    recommended: ["Bash", "Python", "Go"],
                    also: ["JavaScript", "Ruby"]
                },
                resources: [
                    { type: "article", name: "Blue-Green Deployments on AWS", url: "https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/welcome.html" },
                    { type: "documentation", name: "Kubernetes Rolling Updates", url: "https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/" },
                    { type: "article", name: "Martin Fowler on Blue-Green", url: "https://martinfowler.com/bliki/BlueGreenDeployment.html" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Dual Environment Setup",
                        description: "Set up blue and green environments that can run simultaneously.",
                        criteria: ["Two identical environments", "Independent deployability", "Shared database/persistence", "Health check endpoints"],
                        hints: {
                            level1: "Use Docker Compose or similar to define both environments.",
                            level2: "Each environment needs its own port/address. Use environment variables for configuration.",
                            level3: "## Dual Environment Architecture\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  blue:\n    build: .\n    environment:\n      - ENV_COLOR=blue\n      - PORT=3001\n    ports:\n      - \"3001:3000\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/health\"]\n      interval: 10s\n      timeout: 5s\n      retries: 3\n\n  green:\n    build: .\n    environment:\n      - ENV_COLOR=green\n      - PORT=3002\n    ports:\n      - \"3002:3000\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/health\"]\n      interval: 10s\n      timeout: 5s\n      retries: 3\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - blue\n      - green\n```\n\n```javascript\n// Health check endpoint\napp.get('/health', (req, res) => {\n  const health = {\n    status: 'healthy',\n    version: process.env.APP_VERSION,\n    color: process.env.ENV_COLOR,\n    uptime: process.uptime(),\n    timestamp: Date.now()\n  };\n  res.json(health);\n});\n```"
                        },
                        pitfalls: ["Forgetting database migrations", "Environment config drift", "Not testing both environments equally"],
                        concepts: ["Environment isolation", "Infrastructure as code", "Health checks"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 2,
                        name: "Load Balancer Switching",
                        description: "Implement traffic switching between blue and green environments.",
                        criteria: ["Nginx/HAProxy configuration", "Dynamic upstream switching", "No dropped connections", "Rollback capability"],
                        hints: {
                            level1: "Use Nginx upstream directive to define backends.",
                            level2: "Update nginx config and reload (not restart) for zero-downtime.",
                            level3: "## Load Balancer Configuration\n\n```nginx\n# nginx.conf\nupstream backend {\n    # Active environment (switched during deployment)\n    server blue:3000;\n    # Standby (commented out)\n    # server green:3000;\n}\n\nserver {\n    listen 80;\n    \n    location / {\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_cache_bypass $http_upgrade;\n    }\n    \n    location /health {\n        proxy_pass http://backend/health;\n    }\n}\n```\n\n```bash\n#!/bin/bash\n# switch-traffic.sh\n\nTARGET=$1  # blue or green\n\nif [ \"$TARGET\" != \"blue\" ] && [ \"$TARGET\" != \"green\" ]; then\n    echo \"Usage: $0 <blue|green>\"\n    exit 1\nfi\n\n# Generate new nginx config\ncat > /etc/nginx/conf.d/upstream.conf << EOF\nupstream backend {\n    server ${TARGET}:3000;\n}\nEOF\n\n# Test config\nnginx -t\nif [ $? -ne 0 ]; then\n    echo \"Nginx config test failed!\"\n    exit 1\nfi\n\n# Reload nginx (graceful, no dropped connections)\nnginx -s reload\n\necho \"Traffic switched to $TARGET\"\n```"
                        },
                        pitfalls: ["Using restart instead of reload", "Not testing config before applying", "Forgetting to drain connections"],
                        concepts: ["Reverse proxy", "Graceful reload", "Connection draining"],
                        estimatedHours: "4-6"
                    },
                    {
                        id: 3,
                        name: "Deployment Automation",
                        description: "Automate the deployment process with pre-deployment checks.",
                        criteria: ["Automated deployment script", "Pre-deployment health checks", "Smoke tests after switch", "Deployment logging"],
                        hints: {
                            level1: "Write a deployment script that: builds, deploys to standby, tests, switches traffic.",
                            level2: "Add timeouts and failure handling. Log everything for debugging.",
                            level3: "## Deployment Script\n\n```bash\n#!/bin/bash\n# deploy.sh - Blue-Green Deployment Script\n\nset -e  # Exit on error\n\n# Configuration\nDEPLOY_TIMEOUT=300\nHEALTH_CHECK_RETRIES=30\nHEALTH_CHECK_INTERVAL=5\n\n# Determine current and target environments\nCURRENT=$(curl -s http://localhost/health | jq -r '.color')\nif [ \"$CURRENT\" == \"blue\" ]; then\n    TARGET=\"green\"\nelse\n    TARGET=\"blue\"\nfi\n\necho \"Current: $CURRENT, Deploying to: $TARGET\"\n\n# Step 1: Build and deploy to target\necho \"Building new version...\"\ndocker-compose build $TARGET\n\necho \"Starting $TARGET environment...\"\ndocker-compose up -d $TARGET\n\n# Step 2: Wait for health check\necho \"Waiting for $TARGET to be healthy...\"\nfor i in $(seq 1 $HEALTH_CHECK_RETRIES); do\n    if curl -sf \"http://${TARGET}:3000/health\" > /dev/null; then\n        echo \"$TARGET is healthy!\"\n        break\n    fi\n    if [ $i -eq $HEALTH_CHECK_RETRIES ]; then\n        echo \"Health check failed after $HEALTH_CHECK_RETRIES attempts\"\n        exit 1\n    fi\n    echo \"Attempt $i/$HEALTH_CHECK_RETRIES - waiting...\"\n    sleep $HEALTH_CHECK_INTERVAL\ndone\n\n# Step 3: Run smoke tests\necho \"Running smoke tests on $TARGET...\"\n./smoke-tests.sh \"http://${TARGET}:3000\"\nif [ $? -ne 0 ]; then\n    echo \"Smoke tests failed!\"\n    exit 1\nfi\n\n# Step 4: Switch traffic\necho \"Switching traffic to $TARGET...\"\n./switch-traffic.sh $TARGET\n\n# Step 5: Verify production\necho \"Verifying production...\"\nsleep 5\nPROD_COLOR=$(curl -s http://localhost/health | jq -r '.color')\nif [ \"$PROD_COLOR\" != \"$TARGET\" ]; then\n    echo \"Traffic switch verification failed!\"\n    exit 1\nfi\n\necho \"Deployment successful! Now running: $TARGET\"\necho \"Previous environment ($CURRENT) is still available for rollback\"\n```"
                        },
                        pitfalls: ["No timeout handling", "Missing error handling", "Not verifying after switch"],
                        concepts: ["Deployment automation", "Smoke testing", "Idempotent deployments"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 4,
                        name: "Rollback & Database Migrations",
                        description: "Handle rollbacks and database migrations safely.",
                        criteria: ["Instant rollback capability", "Backward-compatible migrations", "Migration verification", "Database version tracking"],
                        hints: {
                            level1: "Rollback is just switching back to the previous environment.",
                            level2: "Database migrations must be backward-compatible (expand-contract pattern).",
                            level3: "## Rollback & Migrations\n\n```bash\n#!/bin/bash\n# rollback.sh - Instant rollback to previous environment\n\nCURRENT=$(curl -s http://localhost/health | jq -r '.color')\nif [ \"$CURRENT\" == \"blue\" ]; then\n    PREVIOUS=\"green\"\nelse\n    PREVIOUS=\"blue\"\nfi\n\necho \"Rolling back from $CURRENT to $PREVIOUS...\"\n\n# Verify previous environment is still running\nif ! curl -sf \"http://${PREVIOUS}:3000/health\" > /dev/null; then\n    echo \"Previous environment is not available!\"\n    exit 1\nfi\n\n# Switch traffic back\n./switch-traffic.sh $PREVIOUS\n\necho \"Rollback complete. Now running: $PREVIOUS\"\n```\n\n### Database Migration Strategy\n\n```\nExpand-Contract Pattern:\n\n1. EXPAND: Add new column (nullable or with default)\n   - Both old and new code can work\n   \n2. MIGRATE: Deploy new code that writes to both\n   - Backfill existing data\n   \n3. CONTRACT: Remove old column/code (after verification)\n   - Only when sure rollback isn't needed\n```\n\n```javascript\n// migrations/20240115_add_user_email.js\n\n// EXPAND phase - backward compatible\nexports.up = async (db) => {\n  // Add column as nullable (old code ignores it)\n  await db.query(`\n    ALTER TABLE users \n    ADD COLUMN email VARCHAR(255) NULL\n  `);\n};\n\nexports.down = async (db) => {\n  await db.query(`\n    ALTER TABLE users \n    DROP COLUMN email\n  `);\n};\n\n// CONTRACT phase (separate migration, run later)\nexports.up = async (db) => {\n  // Only run after all instances use new code\n  await db.query(`\n    ALTER TABLE users \n    ALTER COLUMN email SET NOT NULL\n  `);\n};\n```\n\n```bash\n# deployment with migrations\n./run-migrations.sh  # Run expand migrations first\n./deploy.sh          # Deploy new code\n# ... wait for stability ...\n./run-migrations.sh --contract  # Run contract migrations\n```"
                        },
                        pitfalls: ["Breaking migrations that prevent rollback", "Running contract too early", "Not testing rollback regularly"],
                        concepts: ["Expand-contract pattern", "Backward compatibility", "Database versioning"],
                        estimatedHours: "5-8"
                    }
                ]
            },

            "metrics-dashboard": {
                name: "Metrics & Alerting Dashboard",
                description: "Build a metrics collection and visualization system with alerting capabilities.",
                difficulty: "advanced",
                estimatedHours: "25-40",
                prerequisites: ["HTTP APIs", "Time-series data concepts", "Basic statistics", "Docker"],
                languages: {
                    recommended: ["Go", "Python"],
                    also: ["JavaScript", "Rust"]
                },
                resources: [
                    { type: "documentation", name: "Prometheus Documentation", url: "https://prometheus.io/docs/introduction/overview/" },
                    { type: "documentation", name: "Grafana Documentation", url: "https://grafana.com/docs/grafana/latest/" },
                    { type: "article", name: "Google SRE Book - Monitoring", url: "https://sre.google/sre-book/monitoring-distributed-systems/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Metrics Collection",
                        description: "Implement metrics collection with counters, gauges, and histograms.",
                        criteria: ["Counter metrics", "Gauge metrics", "Histogram metrics", "Labels/dimensions", "Prometheus exposition format"],
                        hints: {
                            level1: "Start with counters (always increasing) and gauges (can go up/down).",
                            level2: "Implement the Prometheus text format: metric_name{label=\"value\"} value timestamp",
                            level3: "## Metrics Types Implementation\n\n```go\npackage metrics\n\nimport (\n    \"fmt\"\n    \"sort\"\n    \"strings\"\n    \"sync\"\n    \"time\"\n)\n\n// Counter - only increases\ntype Counter struct {\n    mu     sync.RWMutex\n    values map[string]float64  // key: serialized labels\n    labels []string\n}\n\nfunc NewCounter(labels ...string) *Counter {\n    return &Counter{\n        values: make(map[string]float64),\n        labels: labels,\n    }\n}\n\nfunc (c *Counter) Inc(labelValues ...string) {\n    c.Add(1, labelValues...)\n}\n\nfunc (c *Counter) Add(v float64, labelValues ...string) {\n    key := strings.Join(labelValues, \"|\")\n    c.mu.Lock()\n    c.values[key] += v\n    c.mu.Unlock()\n}\n\n// Gauge - can increase or decrease\ntype Gauge struct {\n    mu     sync.RWMutex\n    values map[string]float64\n    labels []string\n}\n\nfunc (g *Gauge) Set(v float64, labelValues ...string) {\n    key := strings.Join(labelValues, \"|\")\n    g.mu.Lock()\n    g.values[key] = v\n    g.mu.Unlock()\n}\n\n// Histogram - distribution of values\ntype Histogram struct {\n    mu      sync.RWMutex\n    buckets []float64\n    counts  map[string][]uint64  // per-label bucket counts\n    sums    map[string]float64\n    totals  map[string]uint64\n}\n\nfunc NewHistogram(buckets []float64) *Histogram {\n    sort.Float64s(buckets)\n    return &Histogram{\n        buckets: buckets,\n        counts:  make(map[string][]uint64),\n        sums:    make(map[string]float64),\n        totals:  make(map[string]uint64),\n    }\n}\n\nfunc (h *Histogram) Observe(v float64, labelValues ...string) {\n    key := strings.Join(labelValues, \"|\")\n    h.mu.Lock()\n    defer h.mu.Unlock()\n    \n    if _, ok := h.counts[key]; !ok {\n        h.counts[key] = make([]uint64, len(h.buckets))\n    }\n    \n    for i, bucket := range h.buckets {\n        if v <= bucket {\n            h.counts[key][i]++\n        }\n    }\n    h.sums[key] += v\n    h.totals[key]++\n}\n```\n\n```\n# Prometheus Exposition Format\nhttp_requests_total{method=\"GET\",status=\"200\"} 1234\nhttp_requests_total{method=\"POST\",status=\"201\"} 567\nhttp_request_duration_seconds_bucket{le=\"0.1\"} 500\nhttp_request_duration_seconds_bucket{le=\"0.5\"} 900\nhttp_request_duration_seconds_bucket{le=\"1.0\"} 990\nhttp_request_duration_seconds_bucket{le=\"+Inf\"} 1000\nhttp_request_duration_seconds_sum 450.5\nhttp_request_duration_seconds_count 1000\n```"
                        },
                        pitfalls: ["Not using labels effectively", "Too many unique label values (cardinality explosion)", "Forgetting thread safety"],
                        concepts: ["Time-series data", "Metric types", "Label cardinality"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 2,
                        name: "Storage & Querying",
                        description: "Implement time-series storage and basic query capabilities.",
                        criteria: ["Time-series storage", "Data retention/compaction", "Range queries", "Aggregation functions"],
                        hints: {
                            level1: "Store data points as (timestamp, value) pairs grouped by metric+labels.",
                            level2: "Use a simple append-only log with periodic compaction.",
                            level3: "## Time-Series Storage\n\n```go\ntype DataPoint struct {\n    Timestamp int64\n    Value     float64\n}\n\ntype TimeSeries struct {\n    mu     sync.RWMutex\n    points []DataPoint\n    maxAge time.Duration\n}\n\nfunc (ts *TimeSeries) Append(t int64, v float64) {\n    ts.mu.Lock()\n    ts.points = append(ts.points, DataPoint{t, v})\n    ts.mu.Unlock()\n}\n\nfunc (ts *TimeSeries) Range(start, end int64) []DataPoint {\n    ts.mu.RLock()\n    defer ts.mu.RUnlock()\n    \n    // Binary search for start\n    i := sort.Search(len(ts.points), func(i int) bool {\n        return ts.points[i].Timestamp >= start\n    })\n    \n    result := []DataPoint{}\n    for ; i < len(ts.points) && ts.points[i].Timestamp <= end; i++ {\n        result = append(result, ts.points[i])\n    }\n    return result\n}\n\n// Compaction - remove old data\nfunc (ts *TimeSeries) Compact() {\n    cutoff := time.Now().Add(-ts.maxAge).UnixMilli()\n    ts.mu.Lock()\n    defer ts.mu.Unlock()\n    \n    i := sort.Search(len(ts.points), func(i int) bool {\n        return ts.points[i].Timestamp >= cutoff\n    })\n    ts.points = ts.points[i:]\n}\n```\n\n```go\n// Query Language (simplified PromQL-like)\ntype Query struct {\n    MetricName string\n    Labels     map[string]string\n    Start      int64\n    End        int64\n    Step       int64  // resolution\n    Aggregation string // sum, avg, max, min, rate\n}\n\nfunc (db *MetricsDB) Execute(q Query) []DataPoint {\n    series := db.findSeries(q.MetricName, q.Labels)\n    points := series.Range(q.Start, q.End)\n    \n    switch q.Aggregation {\n    case \"rate\":\n        return calculateRate(points, q.Step)\n    case \"avg\":\n        return downsampleAvg(points, q.Step)\n    case \"sum\":\n        return downsampleSum(points, q.Step)\n    default:\n        return points\n    }\n}\n\nfunc calculateRate(points []DataPoint, step int64) []DataPoint {\n    if len(points) < 2 {\n        return nil\n    }\n    \n    result := []DataPoint{}\n    for i := 1; i < len(points); i++ {\n        dt := float64(points[i].Timestamp - points[i-1].Timestamp) / 1000\n        dv := points[i].Value - points[i-1].Value\n        result = append(result, DataPoint{\n            Timestamp: points[i].Timestamp,\n            Value:     dv / dt,  // rate per second\n        })\n    }\n    return result\n}\n```"
                        },
                        pitfalls: ["Not implementing compaction", "Inefficient range queries", "Memory issues with large datasets"],
                        concepts: ["Time-series databases", "Data compaction", "Query optimization"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 3,
                        name: "Visualization Dashboard",
                        description: "Build a web dashboard for visualizing metrics.",
                        criteria: ["Line charts for time-series", "Dashboard configuration", "Auto-refresh", "Time range selection"],
                        hints: {
                            level1: "Use Chart.js or similar for rendering. Fetch data via API.",
                            level2: "Support multiple panels per dashboard, configurable via JSON.",
                            level3: "## Dashboard Implementation\n\n```html\n<!-- dashboard.html -->\n<div id=\"dashboard\">\n  <div class=\"controls\">\n    <select id=\"timeRange\">\n      <option value=\"5m\">Last 5 minutes</option>\n      <option value=\"1h\" selected>Last 1 hour</option>\n      <option value=\"24h\">Last 24 hours</option>\n    </select>\n    <button onclick=\"refresh()\">Refresh</button>\n    <label>\n      <input type=\"checkbox\" id=\"autoRefresh\" checked>\n      Auto-refresh (30s)\n    </label>\n  </div>\n  <div id=\"panels\" class=\"panels-grid\"></div>\n</div>\n```\n\n```javascript\n// Dashboard Configuration\nconst dashboardConfig = {\n  title: \"Application Metrics\",\n  refreshInterval: 30000,\n  panels: [\n    {\n      title: \"Request Rate\",\n      query: \"rate(http_requests_total[5m])\",\n      type: \"line\",\n      yAxis: { label: \"req/s\" }\n    },\n    {\n      title: \"Error Rate\",\n      query: \"rate(http_requests_total{status=~'5..'}[5m])\",\n      type: \"line\",\n      thresholds: [{ value: 0.01, color: \"red\" }]\n    },\n    {\n      title: \"Response Time P99\",\n      query: \"histogram_quantile(0.99, http_request_duration_bucket)\",\n      type: \"line\",\n      yAxis: { label: \"seconds\" }\n    }\n  ]\n};\n\nasync function renderPanel(panel, container) {\n  const timeRange = document.getElementById('timeRange').value;\n  const { start, end } = parseTimeRange(timeRange);\n  \n  const data = await fetch(\n    `/api/query?q=${encodeURIComponent(panel.query)}&start=${start}&end=${end}`\n  ).then(r => r.json());\n  \n  const ctx = container.querySelector('canvas').getContext('2d');\n  new Chart(ctx, {\n    type: 'line',\n    data: {\n      labels: data.map(p => new Date(p.timestamp)),\n      datasets: [{\n        label: panel.title,\n        data: data.map(p => p.value),\n        borderColor: '#3498db',\n        tension: 0.1\n      }]\n    },\n    options: {\n      responsive: true,\n      scales: {\n        x: { type: 'time' },\n        y: { title: { text: panel.yAxis?.label || '' } }\n      }\n    }\n  });\n}\n```"
                        },
                        pitfalls: ["Not handling missing data points", "Chart performance with many points", "Time zone issues"],
                        concepts: ["Data visualization", "Dashboard design", "Real-time updates"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 4,
                        name: "Alerting System",
                        description: "Implement alerting rules with notifications.",
                        criteria: ["Alert rule definitions", "Threshold-based alerts", "Alert states (pending/firing/resolved)", "Notification channels"],
                        hints: {
                            level1: "Poll metrics at regular intervals, evaluate rules, track state changes.",
                            level2: "Implement hysteresis (for duration) to avoid flapping alerts.",
                            level3: "## Alerting System\n\n```go\ntype AlertRule struct {\n    Name        string\n    Query       string\n    Condition   string        // e.g., \"> 0.95\"\n    For         time.Duration // must be true for this long\n    Labels      map[string]string\n    Annotations map[string]string\n}\n\ntype AlertState int\nconst (\n    AlertInactive AlertState = iota\n    AlertPending\n    AlertFiring\n)\n\ntype Alert struct {\n    Rule       *AlertRule\n    State      AlertState\n    ActiveAt   time.Time\n    FiredAt    time.Time\n    ResolvedAt time.Time\n    Value      float64\n}\n\ntype AlertManager struct {\n    rules    []*AlertRule\n    alerts   map[string]*Alert  // rule name -> alert\n    notifier Notifier\n}\n\nfunc (am *AlertManager) Evaluate(db *MetricsDB) {\n    for _, rule := range am.rules {\n        value := db.QuerySingle(rule.Query)\n        triggered := evaluateCondition(value, rule.Condition)\n        \n        alert, exists := am.alerts[rule.Name]\n        if !exists {\n            alert = &Alert{Rule: rule, State: AlertInactive}\n            am.alerts[rule.Name] = alert\n        }\n        \n        switch alert.State {\n        case AlertInactive:\n            if triggered {\n                alert.State = AlertPending\n                alert.ActiveAt = time.Now()\n                alert.Value = value\n            }\n            \n        case AlertPending:\n            if !triggered {\n                alert.State = AlertInactive\n            } else if time.Since(alert.ActiveAt) >= rule.For {\n                alert.State = AlertFiring\n                alert.FiredAt = time.Now()\n                am.notifier.Send(alert)\n            }\n            \n        case AlertFiring:\n            if !triggered {\n                alert.State = AlertInactive\n                alert.ResolvedAt = time.Now()\n                am.notifier.SendResolved(alert)\n            }\n        }\n    }\n}\n```\n\n```yaml\n# alert-rules.yaml\ngroups:\n  - name: application\n    rules:\n      - alert: HighErrorRate\n        query: rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m])\n        condition: \"> 0.01\"\n        for: 5m\n        annotations:\n          summary: \"High error rate detected\"\n          description: \"Error rate is {{ $value | printf \\\"%.2f\\\" }}%\"\n        \n      - alert: HighLatency\n        query: histogram_quantile(0.99, http_request_duration_bucket)\n        condition: \"> 1.0\"\n        for: 10m\n        annotations:\n          summary: \"P99 latency above 1 second\"\n```\n\n```go\n// Notification channels\ntype Notifier interface {\n    Send(alert *Alert) error\n    SendResolved(alert *Alert) error\n}\n\ntype SlackNotifier struct {\n    webhookURL string\n}\n\nfunc (s *SlackNotifier) Send(alert *Alert) error {\n    payload := map[string]interface{}{\n        \"text\": fmt.Sprintf(\":rotating_light: ALERT: %s\\n%s\\nValue: %.4f\",\n            alert.Rule.Name,\n            alert.Rule.Annotations[\"description\"],\n            alert.Value),\n    }\n    body, _ := json.Marshal(payload)\n    _, err := http.Post(s.webhookURL, \"application/json\", bytes.NewReader(body))\n    return err\n}\n```"
                        },
                        pitfalls: ["Alert fatigue from too many alerts", "Flapping alerts", "Missing alert resolution notifications"],
                        concepts: ["Alert rules", "State machines", "Notification routing"],
                        estimatedHours: "7-10"
                    }
                ]
            },

            "distributed-tracing": {
                name: "Distributed Tracing",
                description: "Implement distributed tracing to follow requests across services.",
                difficulty: "advanced",
                estimatedHours: "25-40",
                prerequisites: ["HTTP/RPC concepts", "Microservices basics", "Async programming", "JSON/Protocol Buffers"],
                languages: {
                    recommended: ["Go", "Java", "Python"],
                    also: ["JavaScript", "Rust"]
                },
                resources: [
                    { type: "documentation", name: "OpenTelemetry Documentation", url: "https://opentelemetry.io/docs/" },
                    { type: "article", name: "Dapper Paper (Google)", url: "https://research.google/pubs/pub36356/" },
                    { type: "documentation", name: "Jaeger Documentation", url: "https://www.jaegertracing.io/docs/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Trace Context & Spans",
                        description: "Implement trace context propagation and span creation.",
                        criteria: ["Trace ID generation", "Span ID generation", "Parent-child relationships", "Context propagation headers"],
                        hints: {
                            level1: "A trace has one ID, each operation is a span with its own ID.",
                            level2: "Use W3C Trace Context headers: traceparent, tracestate",
                            level3: "## Trace Context Implementation\n\n```go\npackage tracing\n\nimport (\n    \"context\"\n    \"crypto/rand\"\n    \"encoding/hex\"\n    \"fmt\"\n    \"net/http\"\n    \"strings\"\n    \"time\"\n)\n\ntype TraceContext struct {\n    TraceID  string\n    SpanID   string\n    ParentID string\n    Sampled  bool\n}\n\ntype Span struct {\n    TraceID    string\n    SpanID     string\n    ParentID   string\n    Name       string\n    Service    string\n    StartTime  time.Time\n    EndTime    time.Time\n    Status     string\n    Attributes map[string]string\n    Events     []SpanEvent\n}\n\ntype SpanEvent struct {\n    Name      string\n    Timestamp time.Time\n    Attributes map[string]string\n}\n\nfunc NewTraceID() string {\n    b := make([]byte, 16)\n    rand.Read(b)\n    return hex.EncodeToString(b)\n}\n\nfunc NewSpanID() string {\n    b := make([]byte, 8)\n    rand.Read(b)\n    return hex.EncodeToString(b)\n}\n\n// W3C Trace Context format: 00-traceid-spanid-flags\nfunc ParseTraceParent(header string) *TraceContext {\n    parts := strings.Split(header, \"-\")\n    if len(parts) != 4 {\n        return nil\n    }\n    return &TraceContext{\n        TraceID:  parts[1],\n        ParentID: parts[2],\n        Sampled:  parts[3] == \"01\",\n    }\n}\n\nfunc (tc *TraceContext) ToHeader() string {\n    flags := \"00\"\n    if tc.Sampled {\n        flags = \"01\"\n    }\n    return fmt.Sprintf(\"00-%s-%s-%s\", tc.TraceID, tc.SpanID, flags)\n}\n\n// Middleware for HTTP servers\nfunc TracingMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        var tc *TraceContext\n        \n        if parent := r.Header.Get(\"traceparent\"); parent != \"\" {\n            tc = ParseTraceParent(parent)\n            tc.SpanID = NewSpanID()\n        } else {\n            tc = &TraceContext{\n                TraceID: NewTraceID(),\n                SpanID:  NewSpanID(),\n                Sampled: true,\n            }\n        }\n        \n        ctx := context.WithValue(r.Context(), traceContextKey, tc)\n        next.ServeHTTP(w, r.WithContext(ctx))\n    })\n}\n```"
                        },
                        pitfalls: ["Not propagating context across async boundaries", "Generating predictable IDs", "Losing parent context"],
                        concepts: ["Trace context", "W3C standards", "Context propagation"],
                        estimatedHours: "5-8"
                    },
                    {
                        id: 2,
                        name: "Span Collection & Export",
                        description: "Collect spans and export them to a backend.",
                        criteria: ["Span buffering", "Batch export", "Multiple exporters", "Sampling decisions"],
                        hints: {
                            level1: "Buffer spans in memory, export in batches to reduce overhead.",
                            level2: "Implement sampling (e.g., 10% of traces) for high-traffic systems.",
                            level3: "## Span Collection\n\n```go\ntype SpanProcessor interface {\n    OnStart(span *Span)\n    OnEnd(span *Span)\n    Shutdown()\n}\n\ntype BatchSpanProcessor struct {\n    exporter   SpanExporter\n    batchSize  int\n    timeout    time.Duration\n    queue      chan *Span\n    batch      []*Span\n    mu         sync.Mutex\n    done       chan struct{}\n}\n\nfunc NewBatchProcessor(exporter SpanExporter) *BatchSpanProcessor {\n    p := &BatchSpanProcessor{\n        exporter:  exporter,\n        batchSize: 100,\n        timeout:   5 * time.Second,\n        queue:     make(chan *Span, 1000),\n        done:      make(chan struct{}),\n    }\n    go p.worker()\n    return p\n}\n\nfunc (p *BatchSpanProcessor) OnEnd(span *Span) {\n    select {\n    case p.queue <- span:\n    default:\n        // Queue full, drop span (or log)\n    }\n}\n\nfunc (p *BatchSpanProcessor) worker() {\n    ticker := time.NewTicker(p.timeout)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case span := <-p.queue:\n            p.mu.Lock()\n            p.batch = append(p.batch, span)\n            if len(p.batch) >= p.batchSize {\n                p.flush()\n            }\n            p.mu.Unlock()\n            \n        case <-ticker.C:\n            p.mu.Lock()\n            if len(p.batch) > 0 {\n                p.flush()\n            }\n            p.mu.Unlock()\n            \n        case <-p.done:\n            p.mu.Lock()\n            p.flush()\n            p.mu.Unlock()\n            return\n        }\n    }\n}\n\nfunc (p *BatchSpanProcessor) flush() {\n    if len(p.batch) == 0 {\n        return\n    }\n    p.exporter.Export(p.batch)\n    p.batch = nil\n}\n```\n\n```go\n// Sampling\ntype Sampler interface {\n    ShouldSample(traceID string) bool\n}\n\ntype RatioSampler struct {\n    ratio float64\n}\n\nfunc (s *RatioSampler) ShouldSample(traceID string) bool {\n    // Use trace ID for consistent sampling\n    h := fnv.New64a()\n    h.Write([]byte(traceID))\n    return float64(h.Sum64()) / float64(math.MaxUint64) < s.ratio\n}\n\n// 10% sampling\nsampler := &RatioSampler{ratio: 0.1}\n```"
                        },
                        pitfalls: ["Unbounded memory usage", "Blocking on export", "Inconsistent sampling"],
                        concepts: ["Batch processing", "Sampling strategies", "Backpressure"],
                        estimatedHours: "6-8"
                    },
                    {
                        id: 3,
                        name: "Storage Backend",
                        description: "Build a storage backend for traces with query capabilities.",
                        criteria: ["Trace storage", "Service dependency graph", "Trace search by attributes", "Time-based queries"],
                        hints: {
                            level1: "Store spans indexed by trace ID and time. Build service graph from span data.",
                            level2: "Use secondary indexes for searching by service, operation, status, etc.",
                            level3: "## Trace Storage\n\n```go\ntype TraceStore interface {\n    SaveSpan(span *Span) error\n    GetTrace(traceID string) ([]*Span, error)\n    SearchTraces(query TraceQuery) ([]string, error)\n    GetServices() ([]string, error)\n    GetOperations(service string) ([]string, error)\n    GetDependencies(start, end time.Time) ([]Dependency, error)\n}\n\ntype TraceQuery struct {\n    Service     string\n    Operation   string\n    Tags        map[string]string\n    MinDuration time.Duration\n    MaxDuration time.Duration\n    Start       time.Time\n    End         time.Time\n    Limit       int\n}\n\ntype Dependency struct {\n    Parent    string\n    Child     string\n    CallCount int64\n}\n\n// In-memory implementation\ntype MemoryStore struct {\n    mu     sync.RWMutex\n    spans  map[string][]*Span  // traceID -> spans\n    byTime *btree.BTree       // for time-based queries\n    byService map[string][]string // service -> traceIDs\n}\n\nfunc (s *MemoryStore) SaveSpan(span *Span) error {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n    \n    s.spans[span.TraceID] = append(s.spans[span.TraceID], span)\n    \n    // Index by time\n    s.byTime.ReplaceOrInsert(&timeIndex{\n        time:    span.StartTime,\n        traceID: span.TraceID,\n    })\n    \n    // Index by service\n    s.byService[span.Service] = append(s.byService[span.Service], span.TraceID)\n    \n    return nil\n}\n\nfunc (s *MemoryStore) GetDependencies(start, end time.Time) ([]Dependency, error) {\n    s.mu.RLock()\n    defer s.mu.RUnlock()\n    \n    deps := make(map[string]*Dependency)\n    \n    for _, spans := range s.spans {\n        spanMap := make(map[string]*Span)\n        for _, span := range spans {\n            spanMap[span.SpanID] = span\n        }\n        \n        for _, span := range spans {\n            if span.ParentID != \"\" {\n                if parent, ok := spanMap[span.ParentID]; ok {\n                    key := parent.Service + \"->\" + span.Service\n                    if _, exists := deps[key]; !exists {\n                        deps[key] = &Dependency{\n                            Parent: parent.Service,\n                            Child:  span.Service,\n                        }\n                    }\n                    deps[key].CallCount++\n                }\n            }\n        }\n    }\n    \n    result := make([]Dependency, 0, len(deps))\n    for _, d := range deps {\n        result = append(result, *d)\n    }\n    return result, nil\n}\n```"
                        },
                        pitfalls: ["Not handling missing parent spans", "Slow queries without indexes", "Memory growth without TTL"],
                        concepts: ["Indexing strategies", "Graph construction", "Time-series storage"],
                        estimatedHours: "7-12"
                    },
                    {
                        id: 4,
                        name: "Trace Visualization UI",
                        description: "Build a UI for viewing and analyzing traces.",
                        criteria: ["Trace timeline view", "Service dependency graph", "Trace search", "Span details"],
                        hints: {
                            level1: "Render spans as horizontal bars on a timeline, indented by depth.",
                            level2: "Use D3.js or similar for the dependency graph visualization.",
                            level3: "## Trace UI Components\n\n```javascript\n// Trace Timeline Component\nfunction TraceTimeline({ trace }) {\n  const spans = trace.spans.sort((a, b) => a.startTime - b.startTime);\n  const traceStart = Math.min(...spans.map(s => s.startTime));\n  const traceEnd = Math.max(...spans.map(s => s.endTime));\n  const traceDuration = traceEnd - traceStart;\n  \n  // Build span tree\n  const spanMap = new Map(spans.map(s => [s.spanId, s]));\n  const roots = spans.filter(s => !s.parentId || !spanMap.has(s.parentId));\n  \n  function getDepth(span, depth = 0) {\n    if (!span.parentId || !spanMap.has(span.parentId)) return depth;\n    return getDepth(spanMap.get(span.parentId), depth + 1);\n  }\n  \n  return (\n    <div className=\"trace-timeline\">\n      <div className=\"timeline-header\">\n        <span>Service / Operation</span>\n        <span>Duration: {(traceDuration / 1000).toFixed(2)}ms</span>\n      </div>\n      \n      {spans.map(span => {\n        const left = ((span.startTime - traceStart) / traceDuration) * 100;\n        const width = ((span.endTime - span.startTime) / traceDuration) * 100;\n        const depth = getDepth(span);\n        \n        return (\n          <div key={span.spanId} className=\"span-row\">\n            <div className=\"span-label\" style={{ paddingLeft: depth * 20 }}>\n              <span className=\"service\">{span.service}</span>\n              <span className=\"operation\">{span.name}</span>\n            </div>\n            <div className=\"span-bar-container\">\n              <div \n                className={`span-bar ${span.status === 'ERROR' ? 'error' : ''}`}\n                style={{ left: `${left}%`, width: `${Math.max(width, 0.5)}%` }}\n                title={`${(span.endTime - span.startTime) / 1000}ms`}\n              />\n            </div>\n          </div>\n        );\n      })}\n    </div>\n  );\n}\n```\n\n```javascript\n// Service Dependency Graph (D3.js)\nfunction DependencyGraph({ dependencies }) {\n  useEffect(() => {\n    const svg = d3.select('#graph-container')\n      .append('svg')\n      .attr('width', 800)\n      .attr('height', 600);\n    \n    // Build nodes and links\n    const services = new Set();\n    dependencies.forEach(d => {\n      services.add(d.parent);\n      services.add(d.child);\n    });\n    \n    const nodes = Array.from(services).map(s => ({ id: s }));\n    const links = dependencies.map(d => ({\n      source: d.parent,\n      target: d.child,\n      value: d.callCount\n    }));\n    \n    const simulation = d3.forceSimulation(nodes)\n      .force('link', d3.forceLink(links).id(d => d.id))\n      .force('charge', d3.forceManyBody().strength(-300))\n      .force('center', d3.forceCenter(400, 300));\n    \n    // Draw links\n    const link = svg.selectAll('.link')\n      .data(links)\n      .enter().append('line')\n      .attr('class', 'link')\n      .attr('stroke', '#999')\n      .attr('stroke-width', d => Math.sqrt(d.value));\n    \n    // Draw nodes\n    const node = svg.selectAll('.node')\n      .data(nodes)\n      .enter().append('g')\n      .attr('class', 'node');\n    \n    node.append('circle')\n      .attr('r', 20)\n      .attr('fill', '#69b3a2');\n    \n    node.append('text')\n      .text(d => d.id)\n      .attr('text-anchor', 'middle')\n      .attr('dy', 4);\n    \n    simulation.on('tick', () => {\n      link.attr('x1', d => d.source.x)\n          .attr('y1', d => d.source.y)\n          .attr('x2', d => d.target.x)\n          .attr('y2', d => d.target.y);\n      \n      node.attr('transform', d => `translate(${d.x},${d.y})`);\n    });\n  }, [dependencies]);\n  \n  return <div id=\"graph-container\" />;\n}\n```"
                        },
                        pitfalls: ["Performance with large traces", "Not showing orphan spans", "Confusing time alignment"],
                        concepts: ["Timeline visualization", "Graph visualization", "Interactive UI"],
                        estimatedHours: "7-12"
                    }
                ]
            },

            // SOFTWARE-ENGINEERING - EXPERT
            "build-ci-system": {
                name: "Build Your Own CI System",
                description: "Build a continuous integration system that runs pipelines based on code changes.",
                difficulty: "expert",
                estimatedHours: "50-80",
                prerequisites: ["Docker/containers", "Git hooks/webhooks", "Process management", "Queue systems"],
                languages: {
                    recommended: ["Go", "Python", "Rust"],
                    also: ["JavaScript", "Java"]
                },
                resources: [
                    { type: "article", name: "How GitHub Actions Works", url: "https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions" },
                    { type: "article", name: "Building a CI Server", url: "https://blog.boot.dev/education/build-ci-cd-server/" },
                    { type: "documentation", name: "Jenkins Architecture", url: "https://www.jenkins.io/doc/developer/architecture/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Pipeline Configuration Parser",
                        description: "Parse and validate pipeline configuration files (YAML).",
                        criteria: ["YAML pipeline parsing", "Step/stage structure", "Environment variables", "Conditional execution", "Matrix builds"],
                        hints: {
                            level1: "Define a schema for pipelines with stages, jobs, and steps.",
                            level2: "Support variables, conditionals (if), and matrix for parallel variants.",
                            level3: "## Pipeline Configuration\n\n```yaml\n# Example: .ci/pipeline.yaml\nname: Build and Test\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\nenv:\n  GO_VERSION: \"1.21\"\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        go-version: [\"1.20\", \"1.21\"]\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        \n      - name: Setup Go\n        run: |\n          wget https://go.dev/dl/go${{ matrix.go-version }}.linux-amd64.tar.gz\n          tar -xzf go${{ matrix.go-version }}.linux-amd64.tar.gz\n          \n      - name: Run Tests\n        run: go test ./...\n        \n  build:\n    needs: test\n    if: github.ref == 'refs/heads/main'\n    steps:\n      - name: Build\n        run: go build -o app ./cmd/app\n        \n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: app-binary\n          path: app\n```\n\n```go\npackage pipeline\n\nimport (\n    \"gopkg.in/yaml.v3\"\n)\n\ntype Pipeline struct {\n    Name string            `yaml:\"name\"`\n    On   Trigger           `yaml:\"on\"`\n    Env  map[string]string `yaml:\"env\"`\n    Jobs map[string]*Job   `yaml:\"jobs\"`\n}\n\ntype Trigger struct {\n    Push        *PushTrigger        `yaml:\"push,omitempty\"`\n    PullRequest *PullRequestTrigger `yaml:\"pull_request,omitempty\"`\n    Schedule    []CronTrigger       `yaml:\"schedule,omitempty\"`\n}\n\ntype Job struct {\n    RunsOn   string            `yaml:\"runs-on\"`\n    Needs    []string          `yaml:\"needs,omitempty\"`\n    If       string            `yaml:\"if,omitempty\"`\n    Strategy *Strategy         `yaml:\"strategy,omitempty\"`\n    Env      map[string]string `yaml:\"env,omitempty\"`\n    Steps    []Step            `yaml:\"steps\"`\n}\n\ntype Strategy struct {\n    Matrix    map[string][]interface{} `yaml:\"matrix\"`\n    FailFast  bool                     `yaml:\"fail-fast\"`\n    MaxParallel int                    `yaml:\"max-parallel\"`\n}\n\ntype Step struct {\n    Name    string            `yaml:\"name\"`\n    Uses    string            `yaml:\"uses,omitempty\"`\n    Run     string            `yaml:\"run,omitempty\"`\n    With    map[string]string `yaml:\"with,omitempty\"`\n    Env     map[string]string `yaml:\"env,omitempty\"`\n    If      string            `yaml:\"if,omitempty\"`\n}\n\nfunc ParsePipeline(data []byte) (*Pipeline, error) {\n    var p Pipeline\n    if err := yaml.Unmarshal(data, &p); err != nil {\n        return nil, err\n    }\n    return &p, p.Validate()\n}\n\nfunc (p *Pipeline) Validate() error {\n    // Check for circular dependencies\n    // Validate step configurations\n    // Verify matrix combinations\n    return nil\n}\n```"
                        },
                        pitfalls: ["Circular job dependencies", "Invalid matrix combinations", "Missing required fields"],
                        concepts: ["YAML parsing", "Configuration validation", "DAG construction"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 2,
                        name: "Job Execution Engine",
                        description: "Execute pipeline jobs in isolated containers.",
                        criteria: ["Container-based isolation", "Step execution", "Environment injection", "Output capture", "Artifact handling"],
                        hints: {
                            level1: "Use Docker to run each job in isolation. Mount workspace as volume.",
                            level2: "Capture stdout/stderr, handle exit codes, support timeouts.",
                            level3: "## Job Execution\n\n```go\ntype JobExecutor struct {\n    dockerClient *docker.Client\n    workspace    string\n}\n\ntype JobResult struct {\n    Status    string\n    StartTime time.Time\n    EndTime   time.Time\n    Steps     []StepResult\n    Artifacts []string\n}\n\ntype StepResult struct {\n    Name     string\n    Status   string\n    Output   string\n    Duration time.Duration\n    ExitCode int\n}\n\nfunc (e *JobExecutor) Execute(ctx context.Context, job *Job, matrix map[string]interface{}) (*JobResult, error) {\n    result := &JobResult{\n        Status:    \"running\",\n        StartTime: time.Now(),\n    }\n    \n    // Create container\n    containerConfig := &container.Config{\n        Image:      job.RunsOn,\n        WorkingDir: \"/workspace\",\n        Env:        buildEnv(job.Env, matrix),\n    }\n    \n    hostConfig := &container.HostConfig{\n        Binds: []string{\n            fmt.Sprintf(\"%s:/workspace\", e.workspace),\n        },\n    }\n    \n    resp, err := e.dockerClient.ContainerCreate(ctx, containerConfig, hostConfig, nil, nil, \"\")\n    if err != nil {\n        return nil, err\n    }\n    defer e.dockerClient.ContainerRemove(ctx, resp.ID, types.ContainerRemoveOptions{Force: true})\n    \n    if err := e.dockerClient.ContainerStart(ctx, resp.ID, types.ContainerStartOptions{}); err != nil {\n        return nil, err\n    }\n    \n    // Execute steps\n    for _, step := range job.Steps {\n        stepResult := e.executeStep(ctx, resp.ID, step, matrix)\n        result.Steps = append(result.Steps, stepResult)\n        \n        if stepResult.Status == \"failed\" {\n            result.Status = \"failed\"\n            break\n        }\n    }\n    \n    if result.Status == \"running\" {\n        result.Status = \"success\"\n    }\n    result.EndTime = time.Now()\n    \n    return result, nil\n}\n\nfunc (e *JobExecutor) executeStep(ctx context.Context, containerID string, step Step, matrix map[string]interface{}) StepResult {\n    result := StepResult{\n        Name: step.Name,\n    }\n    start := time.Now()\n    \n    // Interpolate variables\n    script := interpolateVariables(step.Run, matrix)\n    \n    // Create exec\n    execConfig := types.ExecConfig{\n        Cmd:          []string{\"/bin/sh\", \"-c\", script},\n        AttachStdout: true,\n        AttachStderr: true,\n    }\n    \n    execResp, err := e.dockerClient.ContainerExecCreate(ctx, containerID, execConfig)\n    if err != nil {\n        result.Status = \"failed\"\n        result.Output = err.Error()\n        return result\n    }\n    \n    // Attach and capture output\n    attachResp, _ := e.dockerClient.ContainerExecAttach(ctx, execResp.ID, types.ExecStartCheck{})\n    defer attachResp.Close()\n    \n    var output bytes.Buffer\n    io.Copy(&output, attachResp.Reader)\n    \n    // Get exit code\n    inspectResp, _ := e.dockerClient.ContainerExecInspect(ctx, execResp.ID)\n    \n    result.Duration = time.Since(start)\n    result.Output = output.String()\n    result.ExitCode = inspectResp.ExitCode\n    \n    if result.ExitCode == 0 {\n        result.Status = \"success\"\n    } else {\n        result.Status = \"failed\"\n    }\n    \n    return result\n}\n```"
                        },
                        pitfalls: ["Container cleanup on failure", "Timeout handling", "Large output handling"],
                        concepts: ["Container isolation", "Process execution", "Resource management"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 3,
                        name: "Webhook & Queue System",
                        description: "Handle webhooks and queue jobs for execution.",
                        criteria: ["Git webhook handling", "Job queue", "Concurrent execution", "Rate limiting", "Priority scheduling"],
                        hints: {
                            level1: "Parse webhook payloads to determine which pipelines to trigger.",
                            level2: "Use a work queue with configurable parallelism. Track job state.",
                            level3: "## Webhook & Queue\n\n```go\n// Webhook handler\nfunc (s *CIServer) handleWebhook(w http.ResponseWriter, r *http.Request) {\n    // Verify signature\n    signature := r.Header.Get(\"X-Hub-Signature-256\")\n    if !verifySignature(r.Body, signature, s.webhookSecret) {\n        http.Error(w, \"Invalid signature\", http.StatusUnauthorized)\n        return\n    }\n    \n    var payload WebhookPayload\n    if err := json.NewDecoder(r.Body).Decode(&payload); err != nil {\n        http.Error(w, err.Error(), http.StatusBadRequest)\n        return\n    }\n    \n    // Determine event type\n    eventType := r.Header.Get(\"X-GitHub-Event\")\n    \n    // Find matching pipelines\n    pipelines := s.findMatchingPipelines(payload.Repository, eventType, payload)\n    \n    // Queue jobs\n    for _, pipeline := range pipelines {\n        build := &Build{\n            ID:         uuid.New().String(),\n            Pipeline:   pipeline,\n            Commit:     payload.After,\n            Branch:     payload.Ref,\n            Author:     payload.Pusher.Name,\n            Status:     \"queued\",\n            QueuedAt:   time.Now(),\n        }\n        s.queue.Enqueue(build)\n    }\n    \n    w.WriteHeader(http.StatusAccepted)\n}\n\n// Job Queue\ntype JobQueue struct {\n    mu          sync.Mutex\n    pending     []*Build\n    running     map[string]*Build\n    maxParallel int\n    workers     chan struct{}\n}\n\nfunc NewJobQueue(maxParallel int) *JobQueue {\n    return &JobQueue{\n        pending:     make([]*Build, 0),\n        running:     make(map[string]*Build),\n        maxParallel: maxParallel,\n        workers:     make(chan struct{}, maxParallel),\n    }\n}\n\nfunc (q *JobQueue) Enqueue(build *Build) {\n    q.mu.Lock()\n    // Priority: main branch > PRs > other branches\n    insertIdx := len(q.pending)\n    for i, b := range q.pending {\n        if build.Priority() > b.Priority() {\n            insertIdx = i\n            break\n        }\n    }\n    q.pending = append(q.pending[:insertIdx], append([]*Build{build}, q.pending[insertIdx:]...)...)\n    q.mu.Unlock()\n    \n    q.tryDispatch()\n}\n\nfunc (q *JobQueue) tryDispatch() {\n    q.mu.Lock()\n    defer q.mu.Unlock()\n    \n    for len(q.pending) > 0 && len(q.running) < q.maxParallel {\n        build := q.pending[0]\n        q.pending = q.pending[1:]\n        q.running[build.ID] = build\n        \n        go func(b *Build) {\n            q.workers <- struct{}{}  // Acquire worker slot\n            defer func() { <-q.workers }()\n            \n            b.Run()\n            \n            q.mu.Lock()\n            delete(q.running, b.ID)\n            q.mu.Unlock()\n            \n            q.tryDispatch()  // Try to dispatch next job\n        }(build)\n    }\n}\n```"
                        },
                        pitfalls: ["Webhook replay attacks", "Queue starvation", "Resource exhaustion"],
                        concepts: ["Webhook security", "Work queues", "Concurrency control"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 4,
                        name: "Web Dashboard",
                        description: "Build a dashboard for viewing builds and logs.",
                        criteria: ["Build list view", "Real-time log streaming", "Build status badges", "Pipeline visualization"],
                        hints: {
                            level1: "Use WebSockets for real-time log streaming.",
                            level2: "Show pipeline as a graph with job nodes and dependency edges.",
                            level3: "## Dashboard Implementation\n\n```go\n// Log streaming via WebSocket\nfunc (s *CIServer) handleLogStream(w http.ResponseWriter, r *http.Request) {\n    buildID := r.URL.Query().Get(\"build\")\n    \n    upgrader := websocket.Upgrader{}\n    conn, err := upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        return\n    }\n    defer conn.Close()\n    \n    // Subscribe to build logs\n    logChan := s.logBroker.Subscribe(buildID)\n    defer s.logBroker.Unsubscribe(buildID, logChan)\n    \n    // Send existing logs\n    existingLogs := s.logStore.GetLogs(buildID)\n    for _, log := range existingLogs {\n        conn.WriteJSON(log)\n    }\n    \n    // Stream new logs\n    for {\n        select {\n        case log, ok := <-logChan:\n            if !ok {\n                return\n            }\n            if err := conn.WriteJSON(log); err != nil {\n                return\n            }\n        case <-r.Context().Done():\n            return\n        }\n    }\n}\n```\n\n```javascript\n// Frontend Log Viewer\nfunction LogViewer({ buildId }) {\n  const [logs, setLogs] = useState([]);\n  const logEndRef = useRef(null);\n  \n  useEffect(() => {\n    const ws = new WebSocket(`ws://localhost:8080/logs?build=${buildId}`);\n    \n    ws.onmessage = (event) => {\n      const log = JSON.parse(event.data);\n      setLogs(prev => [...prev, log]);\n    };\n    \n    return () => ws.close();\n  }, [buildId]);\n  \n  useEffect(() => {\n    logEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n  }, [logs]);\n  \n  return (\n    <div className=\"log-viewer\">\n      {logs.map((log, i) => (\n        <div key={i} className={`log-line ${log.level}`}>\n          <span className=\"timestamp\">{log.timestamp}</span>\n          <span className=\"step\">[{log.step}]</span>\n          <span className=\"message\">{log.message}</span>\n        </div>\n      ))}\n      <div ref={logEndRef} />\n    </div>\n  );\n}\n\n// Pipeline Graph\nfunction PipelineGraph({ pipeline, buildStatus }) {\n  return (\n    <div className=\"pipeline-graph\">\n      {Object.entries(pipeline.jobs).map(([name, job]) => (\n        <JobNode\n          key={name}\n          name={name}\n          job={job}\n          status={buildStatus.jobs[name]?.status || 'pending'}\n          needs={job.needs || []}\n        />\n      ))}\n    </div>\n  );\n}\n```\n\n```html\n<!-- Status Badge endpoint -->\n/api/badge/:repo/:branch\n\nReturns SVG:\n<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"100\" height=\"20\">\n  <rect width=\"100\" height=\"20\" fill=\"#555\"/>\n  <rect x=\"50\" width=\"50\" height=\"20\" fill=\"#4c1\"/>  <!-- green for passing -->\n  <text x=\"25\" y=\"14\" fill=\"#fff\" text-anchor=\"middle\">build</text>\n  <text x=\"75\" y=\"14\" fill=\"#fff\" text-anchor=\"middle\">passing</text>\n</svg>\n```"
                        },
                        pitfalls: ["WebSocket connection management", "Large log performance", "Badge caching"],
                        concepts: ["Real-time streaming", "Data visualization", "SVG generation"],
                        estimatedHours: "10-15"
                    }
                ]
            },

            "build-observability-platform": {
                name: "Build Your Own Observability Platform",
                description: "Build a unified observability platform combining logs, metrics, and traces.",
                difficulty: "expert",
                estimatedHours: "80-120",
                prerequisites: ["Distributed tracing", "Metrics systems", "Log aggregation", "Time-series databases", "Full-text search"],
                languages: {
                    recommended: ["Go", "Rust"],
                    also: ["Java", "Python"]
                },
                resources: [
                    { type: "book", name: "Observability Engineering", url: "https://www.oreilly.com/library/view/observability-engineering/9781492076438/" },
                    { type: "article", name: "Three Pillars of Observability", url: "https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html" },
                    { type: "documentation", name: "OpenTelemetry Collector", url: "https://opentelemetry.io/docs/collector/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Unified Data Model",
                        description: "Design a unified data model that correlates logs, metrics, and traces.",
                        criteria: ["Common attributes", "Trace-log correlation", "Metric-trace correlation", "Resource attribution", "OpenTelemetry compatibility"],
                        hints: {
                            level1: "All signals share: timestamp, service, resource attributes, trace context.",
                            level2: "Use trace_id/span_id to correlate logs with traces. Use exemplars for metrics.",
                            level3: "## Unified Data Model\n\n```go\n// Common resource attributes\ntype Resource struct {\n    ServiceName      string            `json:\"service.name\"`\n    ServiceVersion   string            `json:\"service.version\"`\n    ServiceInstance  string            `json:\"service.instance.id\"`\n    HostName         string            `json:\"host.name\"`\n    Environment      string            `json:\"deployment.environment\"`\n    Attributes       map[string]string `json:\"attributes\"`\n}\n\n// Trace context for correlation\ntype TraceContext struct {\n    TraceID string `json:\"trace_id,omitempty\"`\n    SpanID  string `json:\"span_id,omitempty\"`\n}\n\n// Log entry\ntype LogRecord struct {\n    Timestamp     time.Time         `json:\"timestamp\"`\n    Resource      Resource          `json:\"resource\"`\n    TraceContext  TraceContext      `json:\"trace_context,omitempty\"`\n    SeverityText  string            `json:\"severity_text\"`\n    SeverityNum   int               `json:\"severity_number\"`\n    Body          string            `json:\"body\"`\n    Attributes    map[string]string `json:\"attributes\"`\n}\n\n// Metric data point\ntype MetricDataPoint struct {\n    Timestamp   time.Time         `json:\"timestamp\"`\n    Resource    Resource          `json:\"resource\"`\n    Name        string            `json:\"name\"`\n    Type        string            `json:\"type\"` // counter, gauge, histogram\n    Value       float64           `json:\"value\"`\n    Labels      map[string]string `json:\"labels\"`\n    Exemplars   []Exemplar        `json:\"exemplars,omitempty\"`\n}\n\n// Exemplar links metric to trace\ntype Exemplar struct {\n    TraceID   string    `json:\"trace_id\"`\n    SpanID    string    `json:\"span_id\"`\n    Value     float64   `json:\"value\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\n// Span (trace segment)\ntype Span struct {\n    TraceID      string            `json:\"trace_id\"`\n    SpanID       string            `json:\"span_id\"`\n    ParentSpanID string            `json:\"parent_span_id,omitempty\"`\n    Resource     Resource          `json:\"resource\"`\n    Name         string            `json:\"name\"`\n    Kind         string            `json:\"kind\"` // client, server, internal\n    StartTime    time.Time         `json:\"start_time\"`\n    EndTime      time.Time         `json:\"end_time\"`\n    Status       string            `json:\"status\"`\n    Attributes   map[string]string `json:\"attributes\"`\n    Events       []SpanEvent       `json:\"events\"`\n    Links        []SpanLink        `json:\"links\"`\n}\n\n// Correlation helpers\nfunc (m *MetricDataPoint) GetExemplarTraces() []string {\n    traceIDs := make([]string, len(m.Exemplars))\n    for i, e := range m.Exemplars {\n        traceIDs[i] = e.TraceID\n    }\n    return traceIDs\n}\n\nfunc (db *ObservabilityDB) GetLogsForTrace(traceID string) ([]LogRecord, error) {\n    return db.logs.Query(LogQuery{TraceID: traceID})\n}\n\nfunc (db *ObservabilityDB) GetMetricsWithExemplar(traceID string) ([]MetricDataPoint, error) {\n    return db.metrics.QueryByExemplar(traceID)\n}\n```"
                        },
                        pitfalls: ["Schema drift across signals", "Missing correlation IDs", "Cardinality explosion"],
                        concepts: ["Data modeling", "Signal correlation", "OpenTelemetry semantics"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "Data Ingestion Pipeline",
                        description: "Build a high-throughput ingestion pipeline for all signal types.",
                        criteria: ["OTLP protocol support", "Batch ingestion", "Backpressure handling", "Data validation", "Routing/filtering"],
                        hints: {
                            level1: "Accept OTLP (gRPC and HTTP). Buffer incoming data before writing to storage.",
                            level2: "Implement pipelines: receivers -> processors -> exporters.",
                            level3: "## Ingestion Pipeline\n\n```go\n// Pipeline architecture\ntype Pipeline struct {\n    receivers  []Receiver\n    processors []Processor\n    exporters  []Exporter\n    bufferSize int\n    dataChan   chan Signal\n}\n\ntype Signal interface {\n    Type() string  // \"logs\", \"metrics\", \"traces\"\n    Resource() Resource\n}\n\n// OTLP Receiver\ntype OTLPReceiver struct {\n    grpcServer *grpc.Server\n    httpServer *http.Server\n    output     chan<- Signal\n}\n\nfunc (r *OTLPReceiver) ReceiveTraces(ctx context.Context, req *otlp.TracesRequest) (*otlp.TracesResponse, error) {\n    for _, resourceSpan := range req.ResourceSpans {\n        resource := convertResource(resourceSpan.Resource)\n        for _, scopeSpan := range resourceSpan.ScopeSpans {\n            for _, span := range scopeSpan.Spans {\n                r.output <- &Span{\n                    Resource:  resource,\n                    TraceID:   hex.EncodeToString(span.TraceId),\n                    SpanID:    hex.EncodeToString(span.SpanId),\n                    Name:      span.Name,\n                    StartTime: time.Unix(0, int64(span.StartTimeUnixNano)),\n                    EndTime:   time.Unix(0, int64(span.EndTimeUnixNano)),\n                    // ... convert other fields\n                }\n            }\n        }\n    }\n    return &otlp.TracesResponse{}, nil\n}\n\n// Processors\ntype FilterProcessor struct {\n    rules []FilterRule\n}\n\nfunc (p *FilterProcessor) Process(signal Signal) (Signal, bool) {\n    for _, rule := range p.rules {\n        if rule.Matches(signal) {\n            if rule.Action == \"drop\" {\n                return nil, false\n            }\n        }\n    }\n    return signal, true\n}\n\ntype AttributeProcessor struct {\n    actions []AttributeAction\n}\n\nfunc (p *AttributeProcessor) Process(signal Signal) (Signal, bool) {\n    for _, action := range p.actions {\n        switch action.Type {\n        case \"insert\":\n            signal.SetAttribute(action.Key, action.Value)\n        case \"delete\":\n            signal.DeleteAttribute(action.Key)\n        case \"hash\":\n            // Hash PII\n            val := signal.GetAttribute(action.Key)\n            signal.SetAttribute(action.Key, hash(val))\n        }\n    }\n    return signal, true\n}\n\n// Backpressure handling\nfunc (p *Pipeline) Run(ctx context.Context) {\n    // Bounded buffer with backpressure\n    buffer := make(chan Signal, p.bufferSize)\n    \n    // Receiver goroutines write to buffer\n    for _, receiver := range p.receivers {\n        go receiver.Start(ctx, buffer)\n    }\n    \n    // Worker pool processes signals\n    var wg sync.WaitGroup\n    for i := 0; i < runtime.NumCPU(); i++ {\n        wg.Add(1)\n        go func() {\n            defer wg.Done()\n            for signal := range buffer {\n                // Run through processors\n                var ok bool\n                for _, proc := range p.processors {\n                    signal, ok = proc.Process(signal)\n                    if !ok {\n                        break\n                    }\n                }\n                if !ok {\n                    continue\n                }\n                \n                // Send to exporters\n                for _, exp := range p.exporters {\n                    exp.Export(signal)\n                }\n            }\n        }()\n    }\n    \n    <-ctx.Done()\n    close(buffer)\n    wg.Wait()\n}\n```"
                        },
                        pitfalls: ["Data loss under load", "Memory exhaustion", "Head-of-line blocking"],
                        concepts: ["Data pipelines", "Backpressure", "Protocol buffers"],
                        estimatedHours: "15-20"
                    },
                    {
                        id: 3,
                        name: "Multi-Signal Storage",
                        description: "Implement storage backends optimized for each signal type.",
                        criteria: ["Log storage (full-text search)", "Metric storage (time-series)", "Trace storage (graph queries)", "Retention policies", "Compaction"],
                        hints: {
                            level1: "Use different storage strategies: inverted index for logs, TSDB for metrics, span storage for traces.",
                            level2: "Implement tiered storage: hot (recent, fast) -> warm -> cold (archived, slow).",
                            level3: "## Multi-Signal Storage\n\n```go\n// Storage interface per signal type\ntype LogStorage interface {\n    Insert(log *LogRecord) error\n    Search(query LogQuery) ([]LogRecord, error)\n    Aggregate(query AggregationQuery) (map[string]int64, error)\n}\n\ntype MetricStorage interface {\n    Insert(metric *MetricDataPoint) error\n    Query(query MetricQuery) ([]MetricDataPoint, error)\n    Downsample(resolution time.Duration) error\n}\n\ntype TraceStorage interface {\n    InsertSpan(span *Span) error\n    GetTrace(traceID string) (*Trace, error)\n    SearchTraces(query TraceQuery) ([]*TraceSummary, error)\n    GetServiceGraph(start, end time.Time) (*ServiceGraph, error)\n}\n\n// Log storage with inverted index\ntype LogStore struct {\n    index    *InvertedIndex\n    segments []*LogSegment\n    mu       sync.RWMutex\n}\n\ntype InvertedIndex struct {\n    terms map[string]*PostingList  // term -> doc IDs\n    fields map[string]*FieldIndex  // field name -> field values -> doc IDs\n}\n\nfunc (s *LogStore) Search(query LogQuery) ([]LogRecord, error) {\n    // Parse query into terms\n    terms := tokenize(query.Query)\n    \n    // Intersect posting lists\n    var docIDs *roaring.Bitmap\n    for _, term := range terms {\n        if posting := s.index.terms[term]; posting != nil {\n            if docIDs == nil {\n                docIDs = posting.bitmap.Clone()\n            } else {\n                docIDs.And(posting.bitmap)\n            }\n        }\n    }\n    \n    // Apply field filters\n    if query.Service != \"\" {\n        fieldBitmap := s.index.fields[\"service\"].Get(query.Service)\n        docIDs.And(fieldBitmap)\n    }\n    \n    // Fetch documents\n    results := make([]LogRecord, 0)\n    it := docIDs.Iterator()\n    for it.HasNext() {\n        docID := it.Next()\n        log := s.getDocument(docID)\n        results = append(results, log)\n    }\n    \n    return results, nil\n}\n\n// Metric storage with time-series optimization\ntype MetricStore struct {\n    series map[string]*TimeSeries  // metric+labels hash -> series\n    wal    *WAL\n}\n\ntype TimeSeries struct {\n    chunks []*Chunk\n    current *Chunk\n}\n\ntype Chunk struct {\n    startTime  int64\n    endTime    int64\n    timestamps []int64\n    values     []float64\n    // Use delta encoding and compression\n}\n\n// Tiered storage\ntype TieredStorage struct {\n    hot   Storage  // Recent data, fast SSD\n    warm  Storage  // Older data, cheaper storage\n    cold  Storage  // Archive, object storage\n    \n    hotRetention  time.Duration  // e.g., 24h\n    warmRetention time.Duration  // e.g., 7d\n}\n\nfunc (t *TieredStorage) Compact() {\n    now := time.Now()\n    \n    // Move hot -> warm\n    hotCutoff := now.Add(-t.hotRetention)\n    t.hot.MoveOlderThan(hotCutoff, t.warm)\n    \n    // Move warm -> cold\n    warmCutoff := now.Add(-t.warmRetention)\n    t.warm.MoveOlderThan(warmCutoff, t.cold)\n}\n```"
                        },
                        pitfalls: ["Query performance at scale", "Storage costs", "Data consistency across stores"],
                        concepts: ["Inverted indexes", "Time-series compression", "Tiered storage"],
                        estimatedHours: "20-30"
                    },
                    {
                        id: 4,
                        name: "Unified Query Interface",
                        description: "Build a query interface that can correlate across all signal types.",
                        criteria: ["Cross-signal queries", "Automatic correlation", "Query language", "Saved queries", "Query performance"],
                        hints: {
                            level1: "Allow jumping from metric anomaly -> exemplar traces -> related logs.",
                            level2: "Implement a query language that can express correlations explicitly.",
                            level3: "## Unified Query Interface\n\n```go\n// Query language supporting cross-signal correlation\ntype Query struct {\n    // Base query\n    Signal    string    // \"logs\", \"metrics\", \"traces\"\n    Filter    string    // Signal-specific filter\n    TimeRange TimeRange\n    \n    // Correlation\n    Correlate *CorrelationQuery `json:\"correlate,omitempty\"`\n}\n\ntype CorrelationQuery struct {\n    // Follow trace context\n    FollowTrace bool\n    // Get exemplars from metrics\n    GetExemplars bool\n    // Include related signals\n    Include []string  // [\"logs\", \"metrics\"]\n}\n\n// Example queries:\n// 1. \"Show me all logs for traces with errors\"\n// {\n//   \"signal\": \"traces\",\n//   \"filter\": \"status = 'ERROR'\",\n//   \"correlate\": { \"include\": [\"logs\"] }\n// }\n\n// 2. \"Show me traces for this metric spike\"\n// {\n//   \"signal\": \"metrics\",\n//   \"filter\": \"http_latency_p99 > 1s\",\n//   \"correlate\": { \"getExemplars\": true }\n// }\n\nfunc (db *ObservabilityDB) ExecuteQuery(q Query) (*QueryResult, error) {\n    result := &QueryResult{\n        Signal: q.Signal,\n    }\n    \n    // Execute base query\n    switch q.Signal {\n    case \"logs\":\n        logs, _ := db.logs.Search(parseLogFilter(q.Filter), q.TimeRange)\n        result.Logs = logs\n        \n    case \"metrics\":\n        metrics, _ := db.metrics.Query(parseMetricFilter(q.Filter), q.TimeRange)\n        result.Metrics = metrics\n        \n    case \"traces\":\n        traces, _ := db.traces.Search(parseTraceFilter(q.Filter), q.TimeRange)\n        result.Traces = traces\n    }\n    \n    // Handle correlations\n    if q.Correlate != nil {\n        result.Correlated = db.correlate(result, q.Correlate)\n    }\n    \n    return result, nil\n}\n\nfunc (db *ObservabilityDB) correlate(result *QueryResult, corr *CorrelationQuery) *CorrelatedData {\n    data := &CorrelatedData{}\n    \n    // Collect trace IDs from results\n    traceIDs := make(map[string]bool)\n    \n    if corr.GetExemplars && len(result.Metrics) > 0 {\n        for _, m := range result.Metrics {\n            for _, e := range m.Exemplars {\n                traceIDs[e.TraceID] = true\n            }\n        }\n    }\n    \n    if len(result.Traces) > 0 {\n        for _, t := range result.Traces {\n            traceIDs[t.TraceID] = true\n        }\n    }\n    \n    // Fetch correlated data\n    for _, include := range corr.Include {\n        switch include {\n        case \"logs\":\n            for traceID := range traceIDs {\n                logs, _ := db.logs.Search(LogQuery{TraceID: traceID})\n                data.Logs = append(data.Logs, logs...)\n            }\n        case \"traces\":\n            for traceID := range traceIDs {\n                trace, _ := db.traces.GetTrace(traceID)\n                data.Traces = append(data.Traces, trace)\n            }\n        }\n    }\n    \n    return data\n}\n```\n\n```javascript\n// UI for correlated exploration\nfunction ObservabilityExplorer() {\n  const [query, setQuery] = useState('');\n  const [results, setResults] = useState(null);\n  const [selectedTrace, setSelectedTrace] = useState(null);\n  \n  const explore = async (q) => {\n    const res = await fetch('/api/query', {\n      method: 'POST',\n      body: JSON.stringify(q)\n    }).then(r => r.json());\n    setResults(res);\n  };\n  \n  return (\n    <div className=\"explorer\">\n      <QueryEditor value={query} onChange={setQuery} onRun={explore} />\n      \n      {results && (\n        <>\n          <SignalTabs>\n            {results.metrics && <MetricsPanel data={results.metrics} onExemplarClick={traceId => {\n              explore({ signal: 'traces', filter: `traceId = '${traceId}'`, correlate: { include: ['logs'] }});\n            }} />}\n            {results.traces && <TracesPanel data={results.traces} onSelect={setSelectedTrace} />}\n            {results.logs && <LogsPanel data={results.logs} />}\n          </SignalTabs>\n          \n          {selectedTrace && (\n            <CorrelatedView\n              trace={selectedTrace}\n              logs={results.correlated?.logs}\n              metrics={results.correlated?.metrics}\n            />\n          )}\n        </>\n      )}\n    </div>\n  );\n}\n```"
                        },
                        pitfalls: ["Query complexity explosion", "Slow correlation queries", "Missing correlation data"],
                        concepts: ["Query planning", "Data correlation", "Unified observability"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 5,
                        name: "Alerting & Anomaly Detection",
                        description: "Implement intelligent alerting that correlates anomalies across signals.",
                        criteria: ["Multi-signal alerts", "Anomaly detection", "Alert correlation/deduplication", "Root cause suggestions"],
                        hints: {
                            level1: "Trigger alerts based on combinations: error logs + latency spike + trace errors.",
                            level2: "Use statistical methods (z-score, MAD) for anomaly detection on metrics.",
                            level3: "## Multi-Signal Alerting\n\n```go\n// Multi-signal alert rule\ntype AlertRule struct {\n    Name        string\n    Description string\n    \n    // Conditions across signals\n    Conditions []AlertCondition\n    Logic      string  // \"all\" or \"any\"\n    \n    For         time.Duration\n    Annotations map[string]string\n}\n\ntype AlertCondition struct {\n    Signal    string  // \"logs\", \"metrics\", \"traces\"\n    Query     string\n    Threshold string  // e.g., \"> 100\", \"exists\"\n}\n\n// Example: Alert when high error rate AND error logs appear\n// {\n//   \"name\": \"ServiceDegraded\",\n//   \"conditions\": [\n//     { \"signal\": \"metrics\", \"query\": \"rate(http_errors_total[5m])\", \"threshold\": \"> 0.01\" },\n//     { \"signal\": \"logs\", \"query\": \"level=error\", \"threshold\": \"count > 10\" },\n//     { \"signal\": \"traces\", \"query\": \"status=ERROR\", \"threshold\": \"ratio > 0.05\" }\n//   ],\n//   \"logic\": \"any\",\n//   \"for\": \"5m\"\n// }\n\n// Anomaly detection\ntype AnomalyDetector struct {\n    history map[string]*MetricHistory\n}\n\ntype MetricHistory struct {\n    values []float64\n    times  []time.Time\n}\n\nfunc (d *AnomalyDetector) Detect(metric string, value float64) *Anomaly {\n    hist := d.history[metric]\n    if hist == nil || len(hist.values) < 100 {\n        return nil  // Not enough history\n    }\n    \n    // Calculate statistics\n    mean := stat.Mean(hist.values, nil)\n    stddev := stat.StdDev(hist.values, nil)\n    \n    // Z-score\n    zscore := (value - mean) / stddev\n    \n    if math.Abs(zscore) > 3 {\n        return &Anomaly{\n            Metric:    metric,\n            Value:     value,\n            Expected:  mean,\n            ZScore:    zscore,\n            Timestamp: time.Now(),\n        }\n    }\n    return nil\n}\n\n// Alert correlation - group related alerts\ntype AlertCorrelator struct {\n    window   time.Duration\n    alerts   []*Alert\n    groups   map[string]*AlertGroup\n}\n\nfunc (c *AlertCorrelator) Correlate(alert *Alert) *AlertGroup {\n    // Find existing group by:\n    // 1. Same service\n    // 2. Related trace IDs\n    // 3. Close in time\n    \n    for _, group := range c.groups {\n        if c.isRelated(alert, group) {\n            group.Alerts = append(group.Alerts, alert)\n            return group\n        }\n    }\n    \n    // Create new group\n    group := &AlertGroup{\n        ID:        uuid.New().String(),\n        Alerts:    []*Alert{alert},\n        CreatedAt: time.Now(),\n    }\n    c.groups[group.ID] = group\n    return group\n}\n\n// Root cause analysis\nfunc (db *ObservabilityDB) SuggestRootCause(alertGroup *AlertGroup) []RootCauseSuggestion {\n    suggestions := []RootCauseSuggestion{}\n    \n    // Collect all trace IDs from alerts\n    traceIDs := collectTraceIDs(alertGroup)\n    \n    // Find common error spans\n    errorSpans := map[string]int{}  // operation -> count\n    for _, traceID := range traceIDs {\n        trace, _ := db.traces.GetTrace(traceID)\n        for _, span := range trace.Spans {\n            if span.Status == \"ERROR\" {\n                key := span.Service + \"/\" + span.Name\n                errorSpans[key]++\n            }\n        }\n    }\n    \n    // Sort by frequency\n    for op, count := range errorSpans {\n        if count > len(traceIDs)/2 {\n            suggestions = append(suggestions, RootCauseSuggestion{\n                Type:       \"common_error_operation\",\n                Operation:  op,\n                Confidence: float64(count) / float64(len(traceIDs)),\n                Evidence:   fmt.Sprintf(\"%d/%d traces have errors in %s\", count, len(traceIDs), op),\n            })\n        }\n    }\n    \n    return suggestions\n}\n```"
                        },
                        pitfalls: ["Alert fatigue", "False positives", "Missing root causes"],
                        concepts: ["Multi-signal correlation", "Statistical anomaly detection", "Root cause analysis"],
                        estimatedHours: "20-30"
                    }
                ]
            },

            // APP-DEV - EXPERT
            "build-spreadsheet": {
                name: "Build Your Own Spreadsheet",
                description: "Build an Excel-like spreadsheet application with formulas, cell references, and recalculation.",
                difficulty: "expert",
                estimatedHours: "50-80",
                prerequisites: ["DOM manipulation", "Graph algorithms", "Expression parsing", "Event handling"],
                languages: {
                    recommended: ["JavaScript", "TypeScript"],
                    also: ["Python", "Rust"]
                },
                resources: [
                    { type: "article", name: "Building a Spreadsheet Engine", url: "https://leanrada.com/notes/spreadsheet-engine/" },
                    { type: "video", name: "How Excel Recalculates", url: "https://www.youtube.com/watch?v=R0hhDgvWbUU" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Grid & Cell Rendering",
                        description: "Build the spreadsheet grid with editable cells.",
                        criteria: ["Virtual scrolling for large grids", "Cell selection and editing", "Column/row headers (A, B, C... and 1, 2, 3...)", "Cell formatting (width, alignment)"],
                        hints: {
                            level1: "Use a virtualized grid - only render visible cells plus buffer.",
                            level2: "Track selection state, handle keyboard navigation (arrow keys, Tab, Enter).",
                            level3: "## Grid Implementation\n\n```javascript\nclass SpreadsheetGrid {\n  constructor(container, rows = 1000, cols = 26) {\n    this.rows = rows;\n    this.cols = cols;\n    this.cellData = new Map(); // 'A1' -> { value, formula, formatted }\n    this.selection = { row: 0, col: 0 };\n    this.viewportStart = { row: 0, col: 0 };\n    \n    this.rowHeight = 24;\n    this.defaultColWidth = 100;\n    this.colWidths = new Map();\n    \n    this.setupDOM(container);\n    this.render();\n  }\n  \n  getCellId(row, col) {\n    return String.fromCharCode(65 + col) + (row + 1);\n  }\n  \n  parseCellId(id) {\n    const match = id.match(/^([A-Z]+)(\\d+)$/);\n    if (!match) return null;\n    const col = match[1].split('').reduce((acc, c) => acc * 26 + c.charCodeAt(0) - 64, 0) - 1;\n    const row = parseInt(match[2]) - 1;\n    return { row, col };\n  }\n  \n  render() {\n    const visibleRows = Math.ceil(this.container.clientHeight / this.rowHeight) + 2;\n    const visibleCols = this.getVisibleCols();\n    \n    // Only render visible cells\n    this.gridBody.innerHTML = '';\n    for (let r = this.viewportStart.row; r < this.viewportStart.row + visibleRows; r++) {\n      const rowEl = document.createElement('div');\n      rowEl.className = 'row';\n      rowEl.style.top = r * this.rowHeight + 'px';\n      \n      for (const col of visibleCols) {\n        const cell = this.createCell(r, col);\n        rowEl.appendChild(cell);\n      }\n      this.gridBody.appendChild(rowEl);\n    }\n  }\n  \n  handleKeydown(e) {\n    switch(e.key) {\n      case 'ArrowUp': this.moveSelection(-1, 0); break;\n      case 'ArrowDown': this.moveSelection(1, 0); break;\n      case 'ArrowLeft': this.moveSelection(0, -1); break;\n      case 'ArrowRight': this.moveSelection(0, 1); break;\n      case 'Tab': this.moveSelection(0, e.shiftKey ? -1 : 1); e.preventDefault(); break;\n      case 'Enter': this.startEditing(); break;\n    }\n  }\n}\n```"
                        },
                        pitfalls: ["Rendering too many cells", "Slow scrolling", "Focus management issues"],
                        concepts: ["Virtual scrolling", "DOM optimization", "Keyboard navigation"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "Formula Parser",
                        description: "Parse and evaluate spreadsheet formulas.",
                        criteria: ["Arithmetic expressions (+, -, *, /, ^)", "Cell references (A1, B2)", "Range references (A1:B5)", "Built-in functions (SUM, AVERAGE, IF, etc.)"],
                        hints: {
                            level1: "Use recursive descent parsing. Formulas start with '='.",
                            level2: "Build an AST, then evaluate. Handle operator precedence.",
                            level3: "## Formula Parser\n\n```javascript\nclass FormulaParser {\n  constructor(getCellValue) {\n    this.getCellValue = getCellValue;\n  }\n  \n  parse(formula) {\n    this.tokens = this.tokenize(formula.substring(1)); // Remove '='\n    this.pos = 0;\n    return this.parseExpression();\n  }\n  \n  tokenize(str) {\n    const tokens = [];\n    const regex = /([A-Z]+\\d+:[A-Z]+\\d+|[A-Z]+\\d+|\\d+\\.?\\d*|[+\\-*/^(),]|[A-Z]+(?=\\()|\"[^\"]*\")/gi;\n    let match;\n    while ((match = regex.exec(str)) !== null) {\n      tokens.push(match[0]);\n    }\n    return tokens;\n  }\n  \n  parseExpression() {\n    let left = this.parseTerm();\n    while (this.current() === '+' || this.current() === '-') {\n      const op = this.consume();\n      const right = this.parseTerm();\n      left = { type: 'binary', op, left, right };\n    }\n    return left;\n  }\n  \n  parseTerm() {\n    let left = this.parseFactor();\n    while (this.current() === '*' || this.current() === '/') {\n      const op = this.consume();\n      const right = this.parseFactor();\n      left = { type: 'binary', op, left, right };\n    }\n    return left;\n  }\n  \n  parseFactor() {\n    const token = this.current();\n    \n    if (token === '(') {\n      this.consume();\n      const expr = this.parseExpression();\n      this.consume(); // ')'\n      return expr;\n    }\n    \n    if (/^[A-Z]+$/i.test(token) && this.peek() === '(') {\n      return this.parseFunction();\n    }\n    \n    if (/^[A-Z]+\\d+:[A-Z]+\\d+$/i.test(token)) {\n      this.consume();\n      return { type: 'range', range: token };\n    }\n    \n    if (/^[A-Z]+\\d+$/i.test(token)) {\n      this.consume();\n      return { type: 'cell', ref: token };\n    }\n    \n    if (/^\\d/.test(token)) {\n      this.consume();\n      return { type: 'number', value: parseFloat(token) };\n    }\n    \n    throw new Error(`Unexpected token: ${token}`);\n  }\n  \n  parseFunction() {\n    const name = this.consume();\n    this.consume(); // '('\n    const args = [];\n    while (this.current() !== ')') {\n      args.push(this.parseExpression());\n      if (this.current() === ',') this.consume();\n    }\n    this.consume(); // ')'\n    return { type: 'function', name: name.toUpperCase(), args };\n  }\n}\n```"
                        },
                        pitfalls: ["Operator precedence errors", "Not handling negative numbers", "String vs number coercion"],
                        concepts: ["Lexical analysis", "Recursive descent parsing", "AST construction"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 3,
                        name: "Dependency Graph & Recalculation",
                        description: "Track cell dependencies and recalculate in correct order.",
                        criteria: ["Dependency tracking", "Topological sort for recalc order", "Circular reference detection", "Incremental recalculation"],
                        hints: {
                            level1: "When a cell is edited, find all cells that depend on it and recalculate.",
                            level2: "Build a dependency graph. Use topological sort to determine recalc order.",
                            level3: "## Dependency Graph\n\n```javascript\nclass DependencyGraph {\n  constructor() {\n    this.dependencies = new Map();  // cell -> Set of cells it depends on\n    this.dependents = new Map();    // cell -> Set of cells that depend on it\n  }\n  \n  setDependencies(cell, deps) {\n    // Remove old dependencies\n    const oldDeps = this.dependencies.get(cell) || new Set();\n    for (const dep of oldDeps) {\n      this.dependents.get(dep)?.delete(cell);\n    }\n    \n    // Set new dependencies\n    this.dependencies.set(cell, new Set(deps));\n    for (const dep of deps) {\n      if (!this.dependents.has(dep)) {\n        this.dependents.set(dep, new Set());\n      }\n      this.dependents.get(dep).add(cell);\n    }\n  }\n  \n  getCellsToRecalculate(changedCell) {\n    // Get all cells affected by this change\n    const affected = new Set();\n    const queue = [changedCell];\n    \n    while (queue.length > 0) {\n      const cell = queue.shift();\n      const deps = this.dependents.get(cell) || new Set();\n      for (const dep of deps) {\n        if (!affected.has(dep)) {\n          affected.add(dep);\n          queue.push(dep);\n        }\n      }\n    }\n    \n    // Topological sort\n    return this.topologicalSort(affected);\n  }\n  \n  topologicalSort(cells) {\n    const visited = new Set();\n    const result = [];\n    \n    const visit = (cell) => {\n      if (visited.has(cell)) return;\n      visited.add(cell);\n      \n      const deps = this.dependencies.get(cell) || new Set();\n      for (const dep of deps) {\n        if (cells.has(dep)) visit(dep);\n      }\n      result.push(cell);\n    };\n    \n    for (const cell of cells) {\n      visit(cell);\n    }\n    \n    return result;\n  }\n  \n  detectCircular(cell, newDeps) {\n    // DFS to check if adding these deps would create a cycle\n    const visited = new Set();\n    const stack = [...newDeps];\n    \n    while (stack.length > 0) {\n      const current = stack.pop();\n      if (current === cell) return true;  // Circular!\n      if (visited.has(current)) continue;\n      visited.add(current);\n      \n      const deps = this.dependencies.get(current) || new Set();\n      for (const dep of deps) {\n        stack.push(dep);\n      }\n    }\n    return false;\n  }\n}\n```"
                        },
                        pitfalls: ["Infinite loops from circular refs", "Recalculating too much", "Wrong recalc order"],
                        concepts: ["Directed graphs", "Topological sorting", "Cycle detection"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 4,
                        name: "Advanced Features",
                        description: "Add formatting, copy/paste, and undo/redo.",
                        criteria: ["Cell formatting (bold, colors, borders)", "Copy/paste with formula adjustment", "Undo/redo stack", "Import/export (CSV, JSON)"],
                        hints: {
                            level1: "For copy/paste, adjust relative cell references based on destination.",
                            level2: "Use command pattern for undo/redo. Each action is a command that can be reversed.",
                            level3: "## Copy/Paste & Undo\n\n```javascript\n// Adjust cell references when copying\nfunction adjustFormula(formula, rowDelta, colDelta) {\n  return formula.replace(/([A-Z]+)(\\d+)/g, (match, col, row) => {\n    // Check if reference is absolute ($A$1)\n    const newCol = String.fromCharCode(col.charCodeAt(0) + colDelta);\n    const newRow = parseInt(row) + rowDelta;\n    return newCol + newRow;\n  });\n}\n\n// Command pattern for undo/redo\nclass Command {\n  execute() { throw new Error('Not implemented'); }\n  undo() { throw new Error('Not implemented'); }\n}\n\nclass SetCellCommand extends Command {\n  constructor(spreadsheet, cellId, newValue) {\n    super();\n    this.spreadsheet = spreadsheet;\n    this.cellId = cellId;\n    this.newValue = newValue;\n    this.oldValue = spreadsheet.getRawValue(cellId);\n  }\n  \n  execute() {\n    this.spreadsheet.setCellValue(this.cellId, this.newValue, false);\n  }\n  \n  undo() {\n    this.spreadsheet.setCellValue(this.cellId, this.oldValue, false);\n  }\n}\n\nclass UndoManager {\n  constructor() {\n    this.undoStack = [];\n    this.redoStack = [];\n  }\n  \n  execute(command) {\n    command.execute();\n    this.undoStack.push(command);\n    this.redoStack = [];  // Clear redo stack\n  }\n  \n  undo() {\n    if (this.undoStack.length === 0) return;\n    const command = this.undoStack.pop();\n    command.undo();\n    this.redoStack.push(command);\n  }\n  \n  redo() {\n    if (this.redoStack.length === 0) return;\n    const command = this.redoStack.pop();\n    command.execute();\n    this.undoStack.push(command);\n  }\n}\n```"
                        },
                        pitfalls: ["Memory leaks in undo stack", "Not handling absolute references ($A$1)", "Paste overwriting formulas"],
                        concepts: ["Command pattern", "Reference adjustment", "State management"],
                        estimatedHours: "15-20"
                    }
                ]
            },

            // SYSTEMS - EXPERT
            "build-allocator": {
                name: "Build Your Own Memory Allocator",
                description: "Implement malloc/free with various allocation strategies.",
                difficulty: "expert",
                estimatedHours: "40-60",
                prerequisites: ["C programming", "Pointers and memory", "Data structures", "System calls (sbrk, mmap)"],
                languages: {
                    recommended: ["C", "Rust", "Zig"],
                    also: []
                },
                resources: [
                    { type: "article", name: "Implementing malloc", url: "https://moss.cs.iit.edu/cs351/slides/slides-malloc.pdf" },
                    { type: "book", name: "The C Programming Language", url: "https://en.wikipedia.org/wiki/The_C_Programming_Language" },
                    { type: "article", name: "Writing a Memory Allocator", url: "http://dmitrysoshnikov.com/compilers/writing-a-memory-allocator/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Basic Allocator with sbrk",
                        description: "Implement simple bump allocator using sbrk.",
                        criteria: ["Request memory from OS via sbrk", "Track allocated blocks", "Basic malloc/free interface", "Handle alignment"],
                        hints: {
                            level1: "Use sbrk(0) to get current break, sbrk(size) to extend heap.",
                            level2: "Store metadata (size, free flag) before each block.",
                            level3: "## Basic Allocator\n\n```c\n#include <unistd.h>\n#include <stdint.h>\n\n// Block header\ntypedef struct block_header {\n    size_t size;          // Size of block (not including header)\n    int is_free;          // 1 if free, 0 if allocated\n    struct block_header *next;  // Next block in list\n} block_header_t;\n\n#define HEADER_SIZE sizeof(block_header_t)\n#define ALIGN(size) (((size) + 7) & ~7)  // 8-byte alignment\n\nstatic block_header_t *head = NULL;\nstatic block_header_t *tail = NULL;\n\n// Request memory from OS\nstatic block_header_t *request_space(size_t size) {\n    block_header_t *block = sbrk(0);\n    void *request = sbrk(HEADER_SIZE + size);\n    \n    if (request == (void*)-1) {\n        return NULL;  // sbrk failed\n    }\n    \n    block->size = size;\n    block->is_free = 0;\n    block->next = NULL;\n    \n    if (tail) {\n        tail->next = block;\n    }\n    tail = block;\n    \n    if (!head) {\n        head = block;\n    }\n    \n    return block;\n}\n\nvoid *my_malloc(size_t size) {\n    if (size == 0) return NULL;\n    \n    size = ALIGN(size);\n    \n    // First fit: find first free block that fits\n    block_header_t *current = head;\n    while (current) {\n        if (current->is_free && current->size >= size) {\n            current->is_free = 0;\n            return (void*)(current + 1);  // Return pointer after header\n        }\n        current = current->next;\n    }\n    \n    // No free block found, request more space\n    block_header_t *block = request_space(size);\n    if (!block) return NULL;\n    \n    return (void*)(block + 1);\n}\n\nvoid my_free(void *ptr) {\n    if (!ptr) return;\n    \n    block_header_t *block = (block_header_t*)ptr - 1;\n    block->is_free = 1;\n}\n```"
                        },
                        pitfalls: ["Forgetting alignment", "Memory leaks in metadata", "Not handling sbrk failure"],
                        concepts: ["Heap management", "Memory alignment", "System calls"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 2,
                        name: "Free List Management",
                        description: "Implement efficient free block tracking.",
                        criteria: ["Explicit free list", "Block splitting", "Block coalescing", "First-fit, best-fit, worst-fit strategies"],
                        hints: {
                            level1: "Maintain a linked list of only free blocks for faster allocation.",
                            level2: "When freeing, merge adjacent free blocks to reduce fragmentation.",
                            level3: "## Free List with Coalescing\n\n```c\n// Split block if it's too large\nstatic void split_block(block_header_t *block, size_t size) {\n    if (block->size >= size + HEADER_SIZE + 16) {  // Only split if remainder is useful\n        block_header_t *new_block = (block_header_t*)((char*)(block + 1) + size);\n        new_block->size = block->size - size - HEADER_SIZE;\n        new_block->is_free = 1;\n        new_block->next = block->next;\n        \n        block->size = size;\n        block->next = new_block;\n    }\n}\n\n// Coalesce adjacent free blocks\nstatic void coalesce() {\n    block_header_t *current = head;\n    while (current && current->next) {\n        if (current->is_free && current->next->is_free) {\n            // Merge with next block\n            current->size += HEADER_SIZE + current->next->size;\n            current->next = current->next->next;\n            // Don't advance - check if we can merge more\n        } else {\n            current = current->next;\n        }\n    }\n}\n\n// Best-fit allocation\nvoid *my_malloc_bestfit(size_t size) {\n    size = ALIGN(size);\n    \n    block_header_t *best = NULL;\n    block_header_t *current = head;\n    \n    while (current) {\n        if (current->is_free && current->size >= size) {\n            if (!best || current->size < best->size) {\n                best = current;\n                if (best->size == size) break;  // Perfect fit\n            }\n        }\n        current = current->next;\n    }\n    \n    if (best) {\n        split_block(best, size);\n        best->is_free = 0;\n        return (void*)(best + 1);\n    }\n    \n    // Request new space...\n    return NULL;\n}\n\nvoid my_free_coalesce(void *ptr) {\n    if (!ptr) return;\n    \n    block_header_t *block = (block_header_t*)ptr - 1;\n    block->is_free = 1;\n    \n    coalesce();\n}\n```"
                        },
                        pitfalls: ["Fragmentation from not coalescing", "Splitting blocks too small", "Corrupting free list pointers"],
                        concepts: ["Free lists", "Fragmentation", "Allocation strategies"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 3,
                        name: "Segregated Free Lists",
                        description: "Implement size-class based allocation for better performance.",
                        criteria: ["Multiple free lists by size class", "Fast allocation for common sizes", "Reduced fragmentation", "Efficient small allocations"],
                        hints: {
                            level1: "Maintain separate free lists for different size ranges (16, 32, 64, 128... bytes).",
                            level2: "Round up allocation size to next size class. Search only appropriate list.",
                            level3: "## Segregated Free Lists\n\n```c\n#define NUM_SIZE_CLASSES 10\n// Size classes: 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, larger\n\nstatic block_header_t *free_lists[NUM_SIZE_CLASSES] = {NULL};\n\nstatic int get_size_class(size_t size) {\n    if (size <= 16) return 0;\n    if (size <= 32) return 1;\n    if (size <= 64) return 2;\n    if (size <= 128) return 3;\n    if (size <= 256) return 4;\n    if (size <= 512) return 5;\n    if (size <= 1024) return 6;\n    if (size <= 2048) return 7;\n    if (size <= 4096) return 8;\n    return 9;  // Larger allocations\n}\n\nstatic size_t get_class_size(int class) {\n    static size_t sizes[] = {16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 0};\n    return sizes[class];\n}\n\n// Remove block from its free list\nstatic void remove_from_freelist(block_header_t *block) {\n    int class = get_size_class(block->size);\n    \n    if (free_lists[class] == block) {\n        free_lists[class] = block->next;\n    } else {\n        block_header_t *prev = free_lists[class];\n        while (prev && prev->next != block) {\n            prev = prev->next;\n        }\n        if (prev) {\n            prev->next = block->next;\n        }\n    }\n}\n\n// Add block to appropriate free list\nstatic void add_to_freelist(block_header_t *block) {\n    int class = get_size_class(block->size);\n    block->next = free_lists[class];\n    free_lists[class] = block;\n}\n\nvoid *segregated_malloc(size_t size) {\n    size = ALIGN(size);\n    int class = get_size_class(size);\n    \n    // Search in this class and larger\n    for (int i = class; i < NUM_SIZE_CLASSES; i++) {\n        block_header_t *block = free_lists[i];\n        while (block) {\n            if (block->size >= size) {\n                remove_from_freelist(block);\n                block->is_free = 0;\n                // Could split if much larger\n                return (void*)(block + 1);\n            }\n            block = block->next;\n        }\n    }\n    \n    // Request new space\n    size_t alloc_size = (class < 9) ? get_class_size(class) : size;\n    return request_and_return(alloc_size);\n}\n```"
                        },
                        pitfalls: ["Internal fragmentation from size classes", "Complex bookkeeping", "Cache unfriendly access patterns"],
                        concepts: ["Size classes", "Segregated storage", "Performance optimization"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 4,
                        name: "Thread Safety & mmap",
                        description: "Add thread safety and large allocation support.",
                        criteria: ["Lock-free or mutex-based thread safety", "Use mmap for large allocations", "Per-thread caches", "Memory debugging support"],
                        hints: {
                            level1: "Use mutex to protect allocator state. For large allocs, use mmap directly.",
                            level2: "mmap allocations can be munmapped directly, avoiding fragmentation.",
                            level3: "## Thread-Safe Allocator\n\n```c\n#include <pthread.h>\n#include <sys/mman.h>\n\n#define MMAP_THRESHOLD 128 * 1024  // Use mmap for > 128KB\n\nstatic pthread_mutex_t alloc_mutex = PTHREAD_MUTEX_INITIALIZER;\n\nvoid *thread_safe_malloc(size_t size) {\n    if (size == 0) return NULL;\n    \n    // Large allocations: use mmap directly\n    if (size >= MMAP_THRESHOLD) {\n        size_t total = size + HEADER_SIZE;\n        void *ptr = mmap(NULL, total, PROT_READ | PROT_WRITE,\n                        MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n        if (ptr == MAP_FAILED) return NULL;\n        \n        block_header_t *block = ptr;\n        block->size = size;\n        block->is_free = 0;\n        block->next = NULL;  // Mark as mmap'd (could use flag)\n        \n        return (void*)(block + 1);\n    }\n    \n    // Small allocations: use segregated lists with mutex\n    pthread_mutex_lock(&alloc_mutex);\n    void *result = segregated_malloc(size);\n    pthread_mutex_unlock(&alloc_mutex);\n    \n    return result;\n}\n\nvoid thread_safe_free(void *ptr) {\n    if (!ptr) return;\n    \n    block_header_t *block = (block_header_t*)ptr - 1;\n    \n    // Check if this was mmap'd (large allocation)\n    if (block->size >= MMAP_THRESHOLD) {\n        munmap(block, block->size + HEADER_SIZE);\n        return;\n    }\n    \n    pthread_mutex_lock(&alloc_mutex);\n    block->is_free = 1;\n    add_to_freelist(block);\n    pthread_mutex_unlock(&alloc_mutex);\n}\n\n// Debug: check for memory leaks\nvoid debug_print_stats() {\n    pthread_mutex_lock(&alloc_mutex);\n    \n    size_t total_allocated = 0;\n    size_t total_free = 0;\n    int num_blocks = 0;\n    \n    block_header_t *current = head;\n    while (current) {\n        num_blocks++;\n        if (current->is_free) {\n            total_free += current->size;\n        } else {\n            total_allocated += current->size;\n        }\n        current = current->next;\n    }\n    \n    printf(\"Blocks: %d, Allocated: %zu, Free: %zu\\n\",\n           num_blocks, total_allocated, total_free);\n    \n    pthread_mutex_unlock(&alloc_mutex);\n}\n```"
                        },
                        pitfalls: ["Lock contention", "Deadlocks", "Memory leaks from lost blocks"],
                        concepts: ["Thread safety", "Virtual memory", "Memory debugging"],
                        estimatedHours: "12-18"
                    }
                ]
            },

            "build-os": {
                name: "Build Your Own OS",
                description: "Build a minimal operating system kernel from scratch.",
                difficulty: "expert",
                estimatedHours: "100-200",
                prerequisites: ["Assembly language", "C programming", "Computer architecture", "Memory management"],
                languages: {
                    recommended: ["C", "Rust", "Zig"],
                    also: ["Assembly"]
                },
                resources: [
                    { type: "book", name: "Operating Systems: Three Easy Pieces", url: "https://pages.cs.wisc.edu/~remzi/OSTEP/" },
                    { type: "tutorial", name: "Writing an OS in Rust", url: "https://os.phil-opp.com/" },
                    { type: "wiki", name: "OSDev Wiki", url: "https://wiki.osdev.org/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Bootloader & Kernel Entry",
                        description: "Boot from BIOS/UEFI and enter kernel code.",
                        criteria: ["Bootable image (ISO/USB)", "Switch to protected/long mode", "Set up GDT", "Jump to kernel main"],
                        hints: {
                            level1: "Use GRUB or write a minimal bootloader. Kernel starts in real mode on x86.",
                            level2: "Set up Global Descriptor Table (GDT) to enter 32/64-bit protected mode.",
                            level3: "## Bootloader Basics\n\n```asm\n; boot.asm - Minimal bootloader\n[BITS 16]\n[ORG 0x7C00]\n\nstart:\n    ; Set up segments\n    xor ax, ax\n    mov ds, ax\n    mov es, ax\n    mov ss, ax\n    mov sp, 0x7C00\n    \n    ; Load kernel from disk\n    mov ah, 0x02    ; BIOS read sectors\n    mov al, 10      ; Number of sectors\n    mov ch, 0       ; Cylinder\n    mov cl, 2       ; Sector (1-indexed, 1 is boot)\n    mov dh, 0       ; Head\n    mov bx, 0x1000  ; Load address\n    int 0x13\n    \n    ; Switch to protected mode\n    cli\n    lgdt [gdt_descriptor]\n    \n    mov eax, cr0\n    or eax, 1\n    mov cr0, eax\n    \n    jmp 0x08:protected_mode\n\n[BITS 32]\nprotected_mode:\n    ; Set up segment registers\n    mov ax, 0x10\n    mov ds, ax\n    mov es, ax\n    mov fs, ax\n    mov gs, ax\n    mov ss, ax\n    mov esp, 0x90000\n    \n    ; Jump to kernel\n    jmp 0x1000\n\n; GDT\ngdt_start:\n    dq 0                    ; Null descriptor\ngdt_code:\n    dw 0xFFFF, 0x0000      ; Code segment\n    db 0x00, 0x9A, 0xCF, 0x00\ngdt_data:\n    dw 0xFFFF, 0x0000      ; Data segment\n    db 0x00, 0x92, 0xCF, 0x00\ngdt_end:\n\ngdt_descriptor:\n    dw gdt_end - gdt_start - 1\n    dd gdt_start\n\ntimes 510-($-$$) db 0\ndw 0xAA55               ; Boot signature\n```"
                        },
                        pitfalls: ["Wrong memory addresses", "Forgetting to disable interrupts", "GDT misconfiguration"],
                        concepts: ["Boot process", "CPU modes", "Memory layout"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 2,
                        name: "Interrupts & Keyboard",
                        description: "Handle hardware interrupts and keyboard input.",
                        criteria: ["Interrupt Descriptor Table (IDT)", "PIC/APIC configuration", "Timer interrupt", "Keyboard driver"],
                        hints: {
                            level1: "Set up IDT with handlers for each interrupt. Remap PIC to avoid conflicts.",
                            level2: "Timer (IRQ0) drives preemptive scheduling. Keyboard (IRQ1) for input.",
                            level3: "## Interrupt Handling\n\n```c\n// idt.c\n#include <stdint.h>\n\nstruct idt_entry {\n    uint16_t base_low;\n    uint16_t selector;\n    uint8_t  zero;\n    uint8_t  flags;\n    uint16_t base_high;\n} __attribute__((packed));\n\nstruct idt_ptr {\n    uint16_t limit;\n    uint32_t base;\n} __attribute__((packed));\n\nstruct idt_entry idt[256];\nstruct idt_ptr idtp;\n\nvoid idt_set_gate(int num, uint32_t base, uint16_t sel, uint8_t flags) {\n    idt[num].base_low = base & 0xFFFF;\n    idt[num].base_high = (base >> 16) & 0xFFFF;\n    idt[num].selector = sel;\n    idt[num].zero = 0;\n    idt[num].flags = flags;\n}\n\n// Remap PIC\nvoid pic_remap() {\n    outb(0x20, 0x11);  // Init PIC1\n    outb(0xA0, 0x11);  // Init PIC2\n    outb(0x21, 0x20);  // PIC1 offset (IRQ 0-7 -> INT 32-39)\n    outb(0xA1, 0x28);  // PIC2 offset (IRQ 8-15 -> INT 40-47)\n    outb(0x21, 0x04);  // Tell PIC1 about PIC2\n    outb(0xA1, 0x02);  // Tell PIC2 its cascade identity\n    outb(0x21, 0x01);  // 8086 mode\n    outb(0xA1, 0x01);\n    outb(0x21, 0x0);   // Unmask all\n    outb(0xA1, 0x0);\n}\n\n// Keyboard handler\nvoid keyboard_handler() {\n    uint8_t scancode = inb(0x60);\n    \n    if (!(scancode & 0x80)) {  // Key press (not release)\n        char c = scancode_to_ascii(scancode);\n        if (c) {\n            terminal_putchar(c);\n        }\n    }\n    \n    // Send EOI\n    outb(0x20, 0x20);\n}\n```"
                        },
                        pitfalls: ["Not sending EOI", "Wrong PIC remapping", "Stack corruption in handlers"],
                        concepts: ["Interrupts", "Hardware I/O", "Device drivers"],
                        estimatedHours: "15-25"
                    },
                    {
                        id: 3,
                        name: "Memory Management",
                        description: "Implement physical and virtual memory management.",
                        criteria: ["Physical memory manager", "Paging setup", "Page fault handler", "Kernel heap allocator"],
                        hints: {
                            level1: "Track physical pages with a bitmap or free list. Set up page tables.",
                            level2: "Enable paging in CR0. Handle page faults to implement demand paging.",
                            level3: "## Paging\n\n```c\n// Physical memory manager (bitmap)\n#define PAGE_SIZE 4096\n#define BITMAP_SIZE (total_memory / PAGE_SIZE / 8)\n\nuint8_t *page_bitmap;\nsize_t total_pages;\n\nvoid pmm_init(size_t mem_size) {\n    total_pages = mem_size / PAGE_SIZE;\n    page_bitmap = (uint8_t*)BITMAP_ADDR;\n    memset(page_bitmap, 0xFF, BITMAP_SIZE);  // All used initially\n    \n    // Mark available memory as free\n    // (from memory map provided by bootloader)\n}\n\nvoid *pmm_alloc_page() {\n    for (size_t i = 0; i < total_pages; i++) {\n        if (!(page_bitmap[i / 8] & (1 << (i % 8)))) {\n            page_bitmap[i / 8] |= (1 << (i % 8));  // Mark used\n            return (void*)(i * PAGE_SIZE);\n        }\n    }\n    return NULL;  // Out of memory\n}\n\nvoid pmm_free_page(void *page) {\n    size_t idx = (size_t)page / PAGE_SIZE;\n    page_bitmap[idx / 8] &= ~(1 << (idx % 8));\n}\n\n// Page table setup (x86)\ntypedef uint32_t page_entry_t;\n\npage_entry_t *page_directory;\npage_entry_t *page_tables[1024];\n\nvoid paging_init() {\n    page_directory = pmm_alloc_page();\n    memset(page_directory, 0, PAGE_SIZE);\n    \n    // Identity map first 4MB\n    page_entry_t *first_table = pmm_alloc_page();\n    for (int i = 0; i < 1024; i++) {\n        first_table[i] = (i * PAGE_SIZE) | 3;  // Present, R/W\n    }\n    page_directory[0] = ((uint32_t)first_table) | 3;\n    \n    // Enable paging\n    asm volatile(\n        \"mov %0, %%cr3\\n\"\n        \"mov %%cr0, %%eax\\n\"\n        \"or $0x80000000, %%eax\\n\"\n        \"mov %%eax, %%cr0\\n\"\n        : : \"r\"(page_directory) : \"eax\"\n    );\n}\n\nvoid map_page(uint32_t virt, uint32_t phys, uint32_t flags) {\n    uint32_t pd_idx = virt >> 22;\n    uint32_t pt_idx = (virt >> 12) & 0x3FF;\n    \n    if (!(page_directory[pd_idx] & 1)) {\n        page_entry_t *new_table = pmm_alloc_page();\n        memset(new_table, 0, PAGE_SIZE);\n        page_directory[pd_idx] = ((uint32_t)new_table) | 3;\n    }\n    \n    page_entry_t *table = (page_entry_t*)(page_directory[pd_idx] & ~0xFFF);\n    table[pt_idx] = phys | flags;\n}\n```"
                        },
                        pitfalls: ["TLB not flushed", "Wrong page table structure", "Kernel unmapped after enabling paging"],
                        concepts: ["Virtual memory", "Paging", "Memory protection"],
                        estimatedHours: "25-40"
                    },
                    {
                        id: 4,
                        name: "Process Management",
                        description: "Implement processes and basic scheduling.",
                        criteria: ["Process control block (PCB)", "Context switching", "Round-robin scheduler", "System calls (fork, exec, exit)"],
                        hints: {
                            level1: "Store process state in PCB. Timer interrupt triggers scheduler.",
                            level2: "Context switch saves/restores registers. Each process has own page table.",
                            level3: "## Process Management\n\n```c\ntypedef struct {\n    uint32_t eax, ebx, ecx, edx;\n    uint32_t esi, edi, ebp, esp;\n    uint32_t eip, eflags;\n    uint32_t cr3;  // Page directory\n} cpu_state_t;\n\ntypedef struct process {\n    int pid;\n    enum { READY, RUNNING, BLOCKED, ZOMBIE } state;\n    cpu_state_t regs;\n    uint32_t *page_directory;\n    struct process *next;\n} process_t;\n\nprocess_t *current_process;\nprocess_t *ready_queue;\n\nvoid schedule() {\n    if (!ready_queue) return;\n    \n    // Save current process state\n    if (current_process && current_process->state == RUNNING) {\n        current_process->state = READY;\n        // Add to end of ready queue\n        process_t *p = ready_queue;\n        while (p->next) p = p->next;\n        p->next = current_process;\n        current_process->next = NULL;\n    }\n    \n    // Get next process\n    current_process = ready_queue;\n    ready_queue = ready_queue->next;\n    current_process->state = RUNNING;\n    \n    // Switch to process\n    switch_context(&current_process->regs);\n}\n\n// Context switch (assembly)\nextern void switch_context(cpu_state_t *state);\n\n// Timer interrupt calls scheduler\nvoid timer_handler() {\n    // ... update time ...\n    schedule();\n    outb(0x20, 0x20);  // EOI\n}\n\nint sys_fork() {\n    process_t *child = kmalloc(sizeof(process_t));\n    child->pid = next_pid++;\n    child->state = READY;\n    \n    // Copy page directory (copy-on-write would be better)\n    child->page_directory = clone_page_directory(current_process->page_directory);\n    \n    // Copy registers, set child return value to 0\n    child->regs = current_process->regs;\n    child->regs.eax = 0;\n    \n    // Add to ready queue\n    child->next = ready_queue;\n    ready_queue = child;\n    \n    return child->pid;  // Parent returns child PID\n}\n```"
                        },
                        pitfalls: ["Stack corruption during switch", "Not saving all registers", "Deadlocks in scheduling"],
                        concepts: ["Context switching", "Scheduling algorithms", "Process isolation"],
                        estimatedHours: "30-50"
                    }
                ]
            },

            "build-tcp-stack": {
                name: "Build Your Own TCP/IP Stack",
                description: "Implement a TCP/IP network stack from scratch.",
                difficulty: "expert",
                estimatedHours: "60-100",
                prerequisites: ["Networking fundamentals", "C programming", "Packet analysis", "State machines"],
                languages: {
                    recommended: ["C", "Rust", "Go"],
                    also: []
                },
                resources: [
                    { type: "book", name: "TCP/IP Illustrated Vol 1", url: "https://www.amazon.com/TCP-Illustrated-Vol-Addison-Wesley-Professional/dp/0201633469" },
                    { type: "rfc", name: "RFC 793 - TCP", url: "https://tools.ietf.org/html/rfc793" },
                    { type: "tutorial", name: "Let's code a TCP/IP stack", url: "https://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Ethernet & ARP",
                        description: "Handle Ethernet frames and ARP protocol.",
                        criteria: ["Raw socket/TAP device access", "Ethernet frame parsing", "ARP request/reply", "MAC address table"],
                        hints: {
                            level1: "Use TAP device or raw sockets to send/receive Ethernet frames.",
                            level2: "ARP resolves IP to MAC. Cache results in ARP table.",
                            level3: "## Ethernet & ARP\n\n```c\n#include <linux/if_tun.h>\n#include <net/if.h>\n\n// Ethernet header\nstruct eth_hdr {\n    uint8_t  dst[6];\n    uint8_t  src[6];\n    uint16_t ethertype;\n} __attribute__((packed));\n\n#define ETH_P_ARP  0x0806\n#define ETH_P_IP   0x0800\n\n// ARP header\nstruct arp_hdr {\n    uint16_t hwtype;\n    uint16_t protype;\n    uint8_t  hwsize;\n    uint8_t  prosize;\n    uint16_t opcode;\n    uint8_t  sender_mac[6];\n    uint32_t sender_ip;\n    uint8_t  target_mac[6];\n    uint32_t target_ip;\n} __attribute__((packed));\n\n#define ARP_REQUEST 1\n#define ARP_REPLY   2\n\n// ARP cache\nstruct arp_entry {\n    uint32_t ip;\n    uint8_t mac[6];\n    time_t expires;\n};\n\nstruct arp_entry arp_cache[256];\n\nvoid handle_arp(struct eth_hdr *eth, struct arp_hdr *arp) {\n    if (ntohs(arp->opcode) == ARP_REQUEST) {\n        if (arp->target_ip == our_ip) {\n            // Send ARP reply\n            struct arp_hdr reply = {\n                .hwtype = htons(1),\n                .protype = htons(ETH_P_IP),\n                .hwsize = 6,\n                .prosize = 4,\n                .opcode = htons(ARP_REPLY),\n                .sender_ip = our_ip,\n                .target_ip = arp->sender_ip,\n            };\n            memcpy(reply.sender_mac, our_mac, 6);\n            memcpy(reply.target_mac, arp->sender_mac, 6);\n            \n            send_ethernet(arp->sender_mac, ETH_P_ARP, &reply, sizeof(reply));\n        }\n    } else if (ntohs(arp->opcode) == ARP_REPLY) {\n        // Update ARP cache\n        arp_cache_add(arp->sender_ip, arp->sender_mac);\n    }\n}\n\nvoid arp_resolve(uint32_t ip, void (*callback)(uint8_t *mac)) {\n    // Check cache first\n    struct arp_entry *entry = arp_cache_lookup(ip);\n    if (entry) {\n        callback(entry->mac);\n        return;\n    }\n    \n    // Send ARP request\n    struct arp_hdr req = {\n        .hwtype = htons(1),\n        .protype = htons(ETH_P_IP),\n        .hwsize = 6,\n        .prosize = 4,\n        .opcode = htons(ARP_REQUEST),\n        .sender_ip = our_ip,\n        .target_ip = ip,\n    };\n    memcpy(req.sender_mac, our_mac, 6);\n    memset(req.target_mac, 0, 6);\n    \n    uint8_t broadcast[6] = {0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF};\n    send_ethernet(broadcast, ETH_P_ARP, &req, sizeof(req));\n}\n```"
                        },
                        pitfalls: ["Byte order (network vs host)", "ARP cache poisoning", "Broadcast handling"],
                        concepts: ["Layer 2 networking", "Address resolution", "Frame parsing"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "IP & ICMP",
                        description: "Implement IP routing and ICMP (ping).",
                        criteria: ["IP header parsing/construction", "Checksum calculation", "Basic routing", "ICMP echo (ping)"],
                        hints: {
                            level1: "IP header has checksum over header only. ICMP rides on IP.",
                            level2: "Route packets based on destination IP. Handle TTL expiry.",
                            level3: "## IP & ICMP\n\n```c\nstruct ip_hdr {\n    uint8_t  ihl:4, version:4;\n    uint8_t  tos;\n    uint16_t len;\n    uint16_t id;\n    uint16_t frag_offset;\n    uint8_t  ttl;\n    uint8_t  protocol;\n    uint16_t checksum;\n    uint32_t src;\n    uint32_t dst;\n} __attribute__((packed));\n\n#define IP_PROTO_ICMP 1\n#define IP_PROTO_TCP  6\n#define IP_PROTO_UDP  17\n\nstruct icmp_hdr {\n    uint8_t  type;\n    uint8_t  code;\n    uint16_t checksum;\n    uint16_t id;\n    uint16_t seq;\n} __attribute__((packed));\n\n#define ICMP_ECHO_REQUEST 8\n#define ICMP_ECHO_REPLY   0\n\nuint16_t checksum(void *data, int len) {\n    uint32_t sum = 0;\n    uint16_t *ptr = data;\n    \n    while (len > 1) {\n        sum += *ptr++;\n        len -= 2;\n    }\n    if (len) {\n        sum += *(uint8_t*)ptr;\n    }\n    \n    while (sum >> 16) {\n        sum = (sum & 0xFFFF) + (sum >> 16);\n    }\n    \n    return ~sum;\n}\n\nvoid handle_ip(struct eth_hdr *eth, struct ip_hdr *ip) {\n    // Verify checksum\n    if (checksum(ip, ip->ihl * 4) != 0) {\n        return;  // Bad checksum\n    }\n    \n    // Check if for us\n    if (ip->dst != our_ip) {\n        // Forward or drop\n        return;\n    }\n    \n    switch (ip->protocol) {\n        case IP_PROTO_ICMP:\n            handle_icmp(ip, (struct icmp_hdr*)((uint8_t*)ip + ip->ihl * 4));\n            break;\n        case IP_PROTO_TCP:\n            handle_tcp(ip, (struct tcp_hdr*)((uint8_t*)ip + ip->ihl * 4));\n            break;\n    }\n}\n\nvoid handle_icmp(struct ip_hdr *ip, struct icmp_hdr *icmp) {\n    if (icmp->type == ICMP_ECHO_REQUEST) {\n        // Send echo reply\n        send_icmp_reply(ip->src, icmp->id, icmp->seq,\n                       (uint8_t*)(icmp + 1),\n                       ntohs(ip->len) - ip->ihl * 4 - sizeof(struct icmp_hdr));\n    }\n}\n```"
                        },
                        pitfalls: ["Checksum calculation errors", "Fragmentation handling", "TTL not decremented"],
                        concepts: ["Layer 3 networking", "Routing", "Error handling"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 3,
                        name: "TCP Connection Management",
                        description: "Implement TCP 3-way handshake and state machine.",
                        criteria: ["TCP header parsing", "Connection state machine", "3-way handshake", "Connection termination"],
                        hints: {
                            level1: "TCP has many states: LISTEN, SYN_SENT, ESTABLISHED, etc.",
                            level2: "Handshake: SYN -> SYN-ACK -> ACK. Track sequence numbers.",
                            level3: "## TCP State Machine\n\n```c\nstruct tcp_hdr {\n    uint16_t src_port;\n    uint16_t dst_port;\n    uint32_t seq;\n    uint32_t ack;\n    uint8_t  reserved:4, data_offset:4;\n    uint8_t  flags;\n    uint16_t window;\n    uint16_t checksum;\n    uint16_t urgent;\n} __attribute__((packed));\n\n#define TCP_FIN 0x01\n#define TCP_SYN 0x02\n#define TCP_RST 0x04\n#define TCP_PSH 0x08\n#define TCP_ACK 0x10\n\nenum tcp_state {\n    CLOSED, LISTEN, SYN_SENT, SYN_RECEIVED,\n    ESTABLISHED, FIN_WAIT_1, FIN_WAIT_2,\n    CLOSE_WAIT, CLOSING, LAST_ACK, TIME_WAIT\n};\n\nstruct tcp_conn {\n    uint32_t local_ip, remote_ip;\n    uint16_t local_port, remote_port;\n    enum tcp_state state;\n    uint32_t snd_una;  // Send unacknowledged\n    uint32_t snd_nxt;  // Send next\n    uint32_t rcv_nxt;  // Receive next\n    uint16_t rcv_wnd;  // Receive window\n    // ... buffers, timers ...\n};\n\nvoid tcp_input(struct ip_hdr *ip, struct tcp_hdr *tcp) {\n    struct tcp_conn *conn = find_connection(ip, tcp);\n    \n    if (!conn) {\n        if (tcp->flags & TCP_SYN) {\n            // New connection attempt\n            conn = create_connection(ip, tcp);\n            conn->state = SYN_RECEIVED;\n            conn->rcv_nxt = ntohl(tcp->seq) + 1;\n            conn->snd_nxt = generate_isn();\n            \n            // Send SYN-ACK\n            send_tcp(conn, TCP_SYN | TCP_ACK, NULL, 0);\n            conn->snd_nxt++;\n        }\n        return;\n    }\n    \n    switch (conn->state) {\n        case SYN_SENT:\n            if ((tcp->flags & (TCP_SYN | TCP_ACK)) == (TCP_SYN | TCP_ACK)) {\n                conn->rcv_nxt = ntohl(tcp->seq) + 1;\n                conn->snd_una = ntohl(tcp->ack);\n                conn->state = ESTABLISHED;\n                send_tcp(conn, TCP_ACK, NULL, 0);\n            }\n            break;\n            \n        case SYN_RECEIVED:\n            if (tcp->flags & TCP_ACK) {\n                conn->snd_una = ntohl(tcp->ack);\n                conn->state = ESTABLISHED;\n            }\n            break;\n            \n        case ESTABLISHED:\n            if (tcp->flags & TCP_FIN) {\n                conn->rcv_nxt++;\n                conn->state = CLOSE_WAIT;\n                send_tcp(conn, TCP_ACK, NULL, 0);\n            } else if (tcp->flags & TCP_ACK) {\n                // Handle data\n                process_tcp_data(conn, tcp);\n            }\n            break;\n        // ... other states ...\n    }\n}\n```"
                        },
                        pitfalls: ["Wrong state transitions", "Sequence number wraparound", "Not handling RST"],
                        concepts: ["State machines", "Connection management", "Reliability"],
                        estimatedHours: "18-25"
                    },
                    {
                        id: 4,
                        name: "TCP Data Transfer & Flow Control",
                        description: "Implement reliable data transfer with flow control.",
                        criteria: ["Sliding window", "Retransmission", "Congestion control (slow start, congestion avoidance)", "Out-of-order handling"],
                        hints: {
                            level1: "Window size controls how much unacknowledged data can be in flight.",
                            level2: "Retransmit on timeout. Implement fast retransmit on 3 duplicate ACKs.",
                            level3: "## Sliding Window & Retransmission\n\n```c\nstruct tcp_conn {\n    // ... previous fields ...\n    \n    // Send buffer\n    uint8_t *snd_buf;\n    size_t snd_buf_size;\n    \n    // Receive buffer (for out-of-order)\n    uint8_t *rcv_buf;\n    uint32_t rcv_buf_start;  // seq of first byte\n    \n    // Congestion control\n    uint32_t cwnd;           // Congestion window\n    uint32_t ssthresh;       // Slow start threshold\n    \n    // RTT estimation\n    uint32_t srtt;           // Smoothed RTT\n    uint32_t rttvar;         // RTT variance\n    uint32_t rto;            // Retransmission timeout\n    \n    // Retransmission\n    struct timer *retx_timer;\n    int dup_ack_count;\n};\n\nvoid tcp_send(struct tcp_conn *conn, const void *data, size_t len) {\n    // Copy to send buffer\n    memcpy(conn->snd_buf + conn->snd_nxt - conn->snd_una, data, len);\n    \n    // Send what window allows\n    size_t can_send = MIN(conn->cwnd, conn->rcv_wnd) - (conn->snd_nxt - conn->snd_una);\n    can_send = MIN(can_send, len);\n    \n    if (can_send > 0) {\n        send_tcp(conn, TCP_ACK | TCP_PSH, data, can_send);\n        conn->snd_nxt += can_send;\n        \n        // Start retransmission timer\n        if (!timer_active(conn->retx_timer)) {\n            timer_start(conn->retx_timer, conn->rto);\n        }\n    }\n}\n\nvoid tcp_ack_received(struct tcp_conn *conn, uint32_t ack_num) {\n    if (ack_num > conn->snd_una) {\n        // New data acknowledged\n        size_t acked = ack_num - conn->snd_una;\n        conn->snd_una = ack_num;\n        \n        // Update congestion window\n        if (conn->cwnd < conn->ssthresh) {\n            // Slow start: exponential growth\n            conn->cwnd += MSS;\n        } else {\n            // Congestion avoidance: linear growth\n            conn->cwnd += MSS * MSS / conn->cwnd;\n        }\n        \n        // Reset retransmission timer\n        timer_stop(conn->retx_timer);\n        if (conn->snd_nxt > conn->snd_una) {\n            timer_start(conn->retx_timer, conn->rto);\n        }\n        \n        conn->dup_ack_count = 0;\n    } else {\n        // Duplicate ACK\n        conn->dup_ack_count++;\n        if (conn->dup_ack_count == 3) {\n            // Fast retransmit\n            conn->ssthresh = conn->cwnd / 2;\n            conn->cwnd = conn->ssthresh + 3 * MSS;\n            retransmit(conn);\n        }\n    }\n}\n\nvoid retransmit_timeout(struct tcp_conn *conn) {\n    // Timeout: retransmit and back off\n    conn->ssthresh = conn->cwnd / 2;\n    conn->cwnd = MSS;  // Reset to slow start\n    conn->rto *= 2;    // Exponential backoff\n    retransmit(conn);\n}\n```"
                        },
                        pitfalls: ["Timer management bugs", "Window calculation errors", "Sequence wraparound"],
                        concepts: ["Flow control", "Congestion control", "Reliable delivery"],
                        estimatedHours: "20-30"
                    }
                ]
            },

            // DISTRIBUTED - ADVANCED
            "distributed-cache": {
                name: "Distributed Cache",
                description: "Build a distributed cache with consistent hashing.",
                difficulty: "advanced",
                estimatedHours: "25-40",
                prerequisites: ["Hash tables", "Networking basics", "Consistent hashing concept"],
                languages: {
                    recommended: ["Go", "Java", "Python"],
                    also: ["Rust", "JavaScript"]
                },
                resources: [
                    { type: "article", name: "Consistent Hashing", url: "https://www.toptal.com/big-data/consistent-hashing" },
                    { type: "paper", name: "Dynamo Paper", url: "https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Consistent Hash Ring",
                        description: "Implement consistent hashing for key distribution.",
                        criteria: ["Hash ring implementation", "Virtual nodes", "Node addition/removal", "Key lookup"],
                        hints: {
                            level1: "Hash both nodes and keys to positions on a ring (0 to 2^32-1).",
                            level2: "Virtual nodes: each physical node gets multiple positions for better distribution.",
                            level3: "## Consistent Hash Ring\n\n```go\npackage cache\n\nimport (\n    \"hash/crc32\"\n    \"sort\"\n    \"sync\"\n)\n\ntype ConsistentHash struct {\n    mu           sync.RWMutex\n    ring         []uint32          // Sorted hash values\n    nodes        map[uint32]string // Hash -> node ID\n    virtualNodes int               // Virtual nodes per physical node\n}\n\nfunc NewConsistentHash(virtualNodes int) *ConsistentHash {\n    return &ConsistentHash{\n        nodes:        make(map[uint32]string),\n        virtualNodes: virtualNodes,\n    }\n}\n\nfunc (c *ConsistentHash) hash(key string) uint32 {\n    return crc32.ChecksumIEEE([]byte(key))\n}\n\nfunc (c *ConsistentHash) AddNode(nodeID string) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    for i := 0; i < c.virtualNodes; i++ {\n        virtualKey := fmt.Sprintf(\"%s-%d\", nodeID, i)\n        h := c.hash(virtualKey)\n        c.ring = append(c.ring, h)\n        c.nodes[h] = nodeID\n    }\n    \n    sort.Slice(c.ring, func(i, j int) bool {\n        return c.ring[i] < c.ring[j]\n    })\n}\n\nfunc (c *ConsistentHash) RemoveNode(nodeID string) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    newRing := make([]uint32, 0)\n    for _, h := range c.ring {\n        if c.nodes[h] != nodeID {\n            newRing = append(newRing, h)\n        } else {\n            delete(c.nodes, h)\n        }\n    }\n    c.ring = newRing\n}\n\nfunc (c *ConsistentHash) GetNode(key string) string {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    if len(c.ring) == 0 {\n        return \"\"\n    }\n    \n    h := c.hash(key)\n    \n    // Binary search for first node >= hash\n    idx := sort.Search(len(c.ring), func(i int) bool {\n        return c.ring[i] >= h\n    })\n    \n    if idx == len(c.ring) {\n        idx = 0  // Wrap around\n    }\n    \n    return c.nodes[c.ring[idx]]\n}\n\n// Get N nodes for replication\nfunc (c *ConsistentHash) GetNodes(key string, n int) []string {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    if len(c.ring) == 0 {\n        return nil\n    }\n    \n    h := c.hash(key)\n    idx := sort.Search(len(c.ring), func(i int) bool {\n        return c.ring[i] >= h\n    })\n    \n    seen := make(map[string]bool)\n    result := make([]string, 0, n)\n    \n    for i := 0; len(result) < n && i < len(c.ring); i++ {\n        nodeID := c.nodes[c.ring[(idx+i)%len(c.ring)]]\n        if !seen[nodeID] {\n            seen[nodeID] = true\n            result = append(result, nodeID)\n        }\n    }\n    \n    return result\n}\n```"
                        },
                        pitfalls: ["Poor hash distribution", "Not enough virtual nodes", "Race conditions"],
                        concepts: ["Consistent hashing", "Load balancing", "Ring topology"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 2,
                        name: "Cache Node Implementation",
                        description: "Implement a cache node with LRU eviction.",
                        criteria: ["Get/Set/Delete operations", "LRU eviction", "TTL support", "Memory limit"],
                        hints: {
                            level1: "Use hashmap + doubly linked list for O(1) LRU operations.",
                            level2: "Track memory usage, evict LRU items when limit exceeded.",
                            level3: "## LRU Cache Node\n\n```go\ntype CacheEntry struct {\n    key        string\n    value      []byte\n    expireAt   time.Time\n    prev, next *CacheEntry\n}\n\ntype CacheNode struct {\n    mu          sync.RWMutex\n    data        map[string]*CacheEntry\n    head, tail  *CacheEntry  // LRU list\n    maxMemory   int64\n    usedMemory  int64\n}\n\nfunc NewCacheNode(maxMemory int64) *CacheNode {\n    c := &CacheNode{\n        data:      make(map[string]*CacheEntry),\n        maxMemory: maxMemory,\n    }\n    // Initialize sentinel nodes\n    c.head = &CacheEntry{}\n    c.tail = &CacheEntry{}\n    c.head.next = c.tail\n    c.tail.prev = c.head\n    return c\n}\n\nfunc (c *CacheNode) moveToFront(entry *CacheEntry) {\n    // Remove from current position\n    entry.prev.next = entry.next\n    entry.next.prev = entry.prev\n    \n    // Add to front\n    entry.next = c.head.next\n    entry.prev = c.head\n    c.head.next.prev = entry\n    c.head.next = entry\n}\n\nfunc (c *CacheNode) Get(key string) ([]byte, bool) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    entry, ok := c.data[key]\n    if !ok {\n        return nil, false\n    }\n    \n    // Check expiration\n    if !entry.expireAt.IsZero() && time.Now().After(entry.expireAt) {\n        c.removeEntry(entry)\n        return nil, false\n    }\n    \n    c.moveToFront(entry)\n    return entry.value, true\n}\n\nfunc (c *CacheNode) Set(key string, value []byte, ttl time.Duration) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    entrySize := int64(len(key) + len(value))\n    \n    // Evict if necessary\n    for c.usedMemory + entrySize > c.maxMemory && c.tail.prev != c.head {\n        c.removeEntry(c.tail.prev)\n    }\n    \n    entry := &CacheEntry{\n        key:   key,\n        value: value,\n    }\n    if ttl > 0 {\n        entry.expireAt = time.Now().Add(ttl)\n    }\n    \n    // Remove old entry if exists\n    if old, ok := c.data[key]; ok {\n        c.removeEntry(old)\n    }\n    \n    // Add new entry\n    c.data[key] = entry\n    c.usedMemory += entrySize\n    \n    // Add to front of LRU list\n    entry.next = c.head.next\n    entry.prev = c.head\n    c.head.next.prev = entry\n    c.head.next = entry\n}\n\nfunc (c *CacheNode) removeEntry(entry *CacheEntry) {\n    entry.prev.next = entry.next\n    entry.next.prev = entry.prev\n    \n    c.usedMemory -= int64(len(entry.key) + len(entry.value))\n    delete(c.data, entry.key)\n}\n```"
                        },
                        pitfalls: ["Memory accounting errors", "TTL cleanup overhead", "Lock contention"],
                        concepts: ["LRU cache", "Memory management", "TTL expiration"],
                        estimatedHours: "6-10"
                    },
                    {
                        id: 3,
                        name: "Cluster Communication",
                        description: "Implement inter-node communication and routing.",
                        criteria: ["Node discovery", "Request routing", "Health checks", "Cluster membership"],
                        hints: {
                            level1: "Each node needs to know about others. Route requests to correct node.",
                            level2: "Use gossip or central coordinator for membership. Health check regularly.",
                            level3: "## Cluster Communication\n\n```go\ntype ClusterNode struct {\n    ID       string\n    Address  string\n    Healthy  bool\n    LastSeen time.Time\n}\n\ntype CacheCluster struct {\n    localNode   *CacheNode\n    nodeID      string\n    hashRing    *ConsistentHash\n    nodes       map[string]*ClusterNode\n    mu          sync.RWMutex\n}\n\nfunc (c *CacheCluster) Get(key string) ([]byte, error) {\n    targetNode := c.hashRing.GetNode(key)\n    \n    if targetNode == c.nodeID {\n        // Local\n        val, ok := c.localNode.Get(key)\n        if !ok {\n            return nil, ErrNotFound\n        }\n        return val, nil\n    }\n    \n    // Remote\n    return c.forwardGet(targetNode, key)\n}\n\nfunc (c *CacheCluster) forwardGet(nodeID, key string) ([]byte, error) {\n    c.mu.RLock()\n    node, ok := c.nodes[nodeID]\n    c.mu.RUnlock()\n    \n    if !ok || !node.Healthy {\n        return nil, ErrNodeUnavailable\n    }\n    \n    // HTTP request to remote node\n    resp, err := http.Get(fmt.Sprintf(\"http://%s/cache/%s\", node.Address, key))\n    if err != nil {\n        return nil, err\n    }\n    defer resp.Body.Close()\n    \n    if resp.StatusCode == http.StatusNotFound {\n        return nil, ErrNotFound\n    }\n    \n    return io.ReadAll(resp.Body)\n}\n\n// Health checker\nfunc (c *CacheCluster) healthCheck() {\n    ticker := time.NewTicker(5 * time.Second)\n    for range ticker.C {\n        c.mu.Lock()\n        for id, node := range c.nodes {\n            if id == c.nodeID {\n                continue\n            }\n            \n            resp, err := http.Get(fmt.Sprintf(\"http://%s/health\", node.Address))\n            if err != nil || resp.StatusCode != http.StatusOK {\n                node.Healthy = false\n                if time.Since(node.LastSeen) > 30*time.Second {\n                    // Remove dead node\n                    c.hashRing.RemoveNode(id)\n                    delete(c.nodes, id)\n                }\n            } else {\n                node.Healthy = true\n                node.LastSeen = time.Now()\n            }\n        }\n        c.mu.Unlock()\n    }\n}\n```"
                        },
                        pitfalls: ["Split brain scenarios", "Network partition handling", "Stale membership info"],
                        concepts: ["Distributed systems", "Health checking", "Service discovery"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 4,
                        name: "Replication & Consistency",
                        description: "Add data replication for fault tolerance.",
                        criteria: ["Configurable replication factor", "Read/write quorums", "Conflict resolution", "Anti-entropy"],
                        hints: {
                            level1: "Replicate to N nodes. Use quorums: W + R > N for consistency.",
                            level2: "Last-write-wins or vector clocks for conflict resolution.",
                            level3: "## Replication\n\n```go\ntype ReplicatedCache struct {\n    cluster         *CacheCluster\n    replicationFactor int  // N\n    writeQuorum     int  // W\n    readQuorum      int  // R\n}\n\nfunc (r *ReplicatedCache) Set(key string, value []byte, ttl time.Duration) error {\n    nodes := r.cluster.hashRing.GetNodes(key, r.replicationFactor)\n    \n    // Write to all replicas, wait for quorum\n    results := make(chan error, len(nodes))\n    for _, nodeID := range nodes {\n        go func(id string) {\n            results <- r.cluster.writeToNode(id, key, value, ttl)\n        }(nodeID)\n    }\n    \n    successCount := 0\n    var lastErr error\n    for i := 0; i < len(nodes); i++ {\n        if err := <-results; err == nil {\n            successCount++\n            if successCount >= r.writeQuorum {\n                return nil  // Quorum achieved\n            }\n        } else {\n            lastErr = err\n        }\n    }\n    \n    return fmt.Errorf(\"write quorum not achieved: %v\", lastErr)\n}\n\nfunc (r *ReplicatedCache) Get(key string) ([]byte, error) {\n    nodes := r.cluster.hashRing.GetNodes(key, r.replicationFactor)\n    \n    type readResult struct {\n        value     []byte\n        timestamp int64\n        err       error\n    }\n    results := make(chan readResult, len(nodes))\n    \n    for _, nodeID := range nodes {\n        go func(id string) {\n            val, ts, err := r.cluster.readFromNode(id, key)\n            results <- readResult{val, ts, err}\n        }(nodeID)\n    }\n    \n    var values []readResult\n    for i := 0; i < len(nodes); i++ {\n        res := <-results\n        if res.err == nil {\n            values = append(values, res)\n            if len(values) >= r.readQuorum {\n                break\n            }\n        }\n    }\n    \n    if len(values) < r.readQuorum {\n        return nil, ErrReadQuorumNotMet\n    }\n    \n    // Return most recent value (last-write-wins)\n    latest := values[0]\n    for _, v := range values[1:] {\n        if v.timestamp > latest.timestamp {\n            latest = v\n        }\n    }\n    \n    return latest.value, nil\n}\n```"
                        },
                        pitfalls: ["Stale reads", "Write conflicts", "Replica divergence"],
                        concepts: ["Quorum systems", "Eventual consistency", "Conflict resolution"],
                        estimatedHours: "8-12"
                    }
                ]
            },

            // DISTRIBUTED - EXPERT
            "build-blockchain": {
                name: "Build Your Own Blockchain",
                description: "Build a blockchain with proof of work and P2P networking.",
                difficulty: "expert",
                estimatedHours: "50-80",
                prerequisites: ["Cryptographic hashing", "P2P networking", "Consensus concepts", "Merkle trees"],
                languages: {
                    recommended: ["Python", "Go", "Rust"],
                    also: ["JavaScript", "Java"]
                },
                resources: [
                    { type: "article", name: "Bitcoin Whitepaper", url: "https://bitcoin.org/bitcoin.pdf" },
                    { type: "book", name: "Mastering Bitcoin", url: "https://github.com/bitcoinbook/bitcoinbook" },
                    { type: "tutorial", name: "Build a Blockchain in Python", url: "https://hackernoon.com/learn-blockchains-by-building-one-117428612f46" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Block Structure & Hashing",
                        description: "Implement block data structure with cryptographic linking.",
                        criteria: ["Block header (prev hash, timestamp, nonce, merkle root)", "Transaction structure", "Block hashing (SHA-256)", "Chain validation"],
                        hints: {
                            level1: "Each block contains hash of previous block, creating a chain.",
                            level2: "Merkle root summarizes all transactions efficiently.",
                            level3: "## Block Structure\n\n```python\nimport hashlib\nimport time\nimport json\n\nclass Transaction:\n    def __init__(self, sender, recipient, amount):\n        self.sender = sender\n        self.recipient = recipient\n        self.amount = amount\n        self.timestamp = time.time()\n    \n    def to_dict(self):\n        return {\n            'sender': self.sender,\n            'recipient': self.recipient,\n            'amount': self.amount,\n            'timestamp': self.timestamp\n        }\n    \n    def hash(self):\n        return hashlib.sha256(\n            json.dumps(self.to_dict(), sort_keys=True).encode()\n        ).hexdigest()\n\nclass Block:\n    def __init__(self, index, transactions, previous_hash, nonce=0):\n        self.index = index\n        self.timestamp = time.time()\n        self.transactions = transactions\n        self.previous_hash = previous_hash\n        self.nonce = nonce\n        self.merkle_root = self.calculate_merkle_root()\n        self.hash = self.calculate_hash()\n    \n    def calculate_merkle_root(self):\n        if not self.transactions:\n            return hashlib.sha256(b'').hexdigest()\n        \n        hashes = [tx.hash() for tx in self.transactions]\n        \n        while len(hashes) > 1:\n            if len(hashes) % 2 == 1:\n                hashes.append(hashes[-1])  # Duplicate last if odd\n            \n            hashes = [\n                hashlib.sha256((hashes[i] + hashes[i+1]).encode()).hexdigest()\n                for i in range(0, len(hashes), 2)\n            ]\n        \n        return hashes[0]\n    \n    def calculate_hash(self):\n        block_string = json.dumps({\n            'index': self.index,\n            'timestamp': self.timestamp,\n            'merkle_root': self.merkle_root,\n            'previous_hash': self.previous_hash,\n            'nonce': self.nonce\n        }, sort_keys=True)\n        return hashlib.sha256(block_string.encode()).hexdigest()\n\nclass Blockchain:\n    def __init__(self):\n        self.chain = [self.create_genesis_block()]\n        self.pending_transactions = []\n    \n    def create_genesis_block(self):\n        return Block(0, [], \"0\")\n    \n    def is_chain_valid(self):\n        for i in range(1, len(self.chain)):\n            current = self.chain[i]\n            previous = self.chain[i-1]\n            \n            if current.hash != current.calculate_hash():\n                return False\n            if current.previous_hash != previous.hash:\n                return False\n        return True\n```"
                        },
                        pitfalls: ["Mutable transaction data", "Hash calculation order", "Genesis block handling"],
                        concepts: ["Cryptographic hashing", "Merkle trees", "Data integrity"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 2,
                        name: "Proof of Work",
                        description: "Implement mining with difficulty adjustment.",
                        criteria: ["Hash target/difficulty", "Nonce search", "Difficulty adjustment", "Block reward"],
                        hints: {
                            level1: "Find nonce such that hash starts with N zeros (difficulty).",
                            level2: "Adjust difficulty based on time between blocks to maintain target rate.",
                            level3: "## Proof of Work\n\n```python\nclass Blockchain:\n    def __init__(self):\n        self.chain = [self.create_genesis_block()]\n        self.difficulty = 4  # Number of leading zeros required\n        self.target_block_time = 10  # seconds\n        self.adjustment_interval = 10  # blocks\n        self.block_reward = 50\n    \n    def get_difficulty_target(self):\n        # Target: hash must be less than this\n        return '0' * self.difficulty + 'f' * (64 - self.difficulty)\n    \n    def mine_block(self, miner_address):\n        # Add coinbase transaction (block reward)\n        coinbase = Transaction('COINBASE', miner_address, self.block_reward)\n        transactions = [coinbase] + self.pending_transactions\n        \n        new_block = Block(\n            len(self.chain),\n            transactions,\n            self.chain[-1].hash\n        )\n        \n        # Mine: find valid nonce\n        target = self.get_difficulty_target()\n        while new_block.hash > target:\n            new_block.nonce += 1\n            new_block.hash = new_block.calculate_hash()\n        \n        self.chain.append(new_block)\n        self.pending_transactions = []\n        \n        # Adjust difficulty\n        if len(self.chain) % self.adjustment_interval == 0:\n            self.adjust_difficulty()\n        \n        return new_block\n    \n    def adjust_difficulty(self):\n        # Calculate actual time for last N blocks\n        start_block = self.chain[-self.adjustment_interval]\n        end_block = self.chain[-1]\n        actual_time = end_block.timestamp - start_block.timestamp\n        expected_time = self.target_block_time * self.adjustment_interval\n        \n        # Adjust difficulty\n        ratio = actual_time / expected_time\n        if ratio < 0.5:\n            self.difficulty += 1  # Too fast, increase difficulty\n        elif ratio > 2.0:\n            self.difficulty = max(1, self.difficulty - 1)  # Too slow\n        \n        print(f\"Difficulty adjusted to {self.difficulty}\")\n    \n    def get_balance(self, address):\n        balance = 0\n        for block in self.chain:\n            for tx in block.transactions:\n                if tx.recipient == address:\n                    balance += tx.amount\n                if tx.sender == address:\n                    balance -= tx.amount\n        return balance\n```"
                        },
                        pitfalls: ["Difficulty too high/low", "No difficulty adjustment", "Mining centralization"],
                        concepts: ["Proof of work", "Mining", "Difficulty adjustment"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 3,
                        name: "P2P Networking",
                        description: "Implement peer-to-peer network for block propagation.",
                        criteria: ["Peer discovery", "Block broadcasting", "Transaction broadcasting", "Chain synchronization"],
                        hints: {
                            level1: "Nodes connect to peers, share new blocks and transactions.",
                            level2: "On new block, validate and broadcast to peers. Sync chain on startup.",
                            level3: "## P2P Network\n\n```python\nimport socket\nimport threading\nimport json\n\nclass P2PNode:\n    def __init__(self, host, port, blockchain):\n        self.host = host\n        self.port = port\n        self.blockchain = blockchain\n        self.peers = set()  # (host, port) tuples\n        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    \n    def start(self):\n        self.server.bind((self.host, self.port))\n        self.server.listen(10)\n        \n        threading.Thread(target=self.accept_connections).start()\n    \n    def accept_connections(self):\n        while True:\n            client, addr = self.server.accept()\n            threading.Thread(target=self.handle_client, args=(client,)).start()\n    \n    def handle_client(self, client):\n        try:\n            data = client.recv(65536).decode()\n            message = json.loads(data)\n            \n            if message['type'] == 'GET_CHAIN':\n                self.send_chain(client)\n            elif message['type'] == 'NEW_BLOCK':\n                self.receive_block(message['block'])\n            elif message['type'] == 'NEW_TX':\n                self.receive_transaction(message['transaction'])\n            elif message['type'] == 'GET_PEERS':\n                self.send_peers(client)\n        finally:\n            client.close()\n    \n    def connect_to_peer(self, host, port):\n        self.peers.add((host, port))\n        # Sync chain from peer\n        self.request_chain(host, port)\n    \n    def request_chain(self, host, port):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.connect((host, port))\n        sock.send(json.dumps({'type': 'GET_CHAIN'}).encode())\n        \n        data = sock.recv(1000000).decode()\n        chain_data = json.loads(data)\n        sock.close()\n        \n        # Replace chain if longer and valid\n        if len(chain_data) > len(self.blockchain.chain):\n            new_chain = self.deserialize_chain(chain_data)\n            if self.blockchain.is_chain_valid_full(new_chain):\n                self.blockchain.chain = new_chain\n                print(\"Chain replaced with longer valid chain\")\n    \n    def broadcast_block(self, block):\n        message = json.dumps({\n            'type': 'NEW_BLOCK',\n            'block': block.to_dict()\n        })\n        \n        for host, port in self.peers:\n            try:\n                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                sock.connect((host, port))\n                sock.send(message.encode())\n                sock.close()\n            except:\n                pass  # Peer offline\n    \n    def receive_block(self, block_data):\n        block = Block.from_dict(block_data)\n        \n        # Validate block\n        if block.previous_hash != self.blockchain.chain[-1].hash:\n            # We might be behind, request full chain\n            return\n        \n        if not self.blockchain.is_valid_block(block):\n            return\n        \n        self.blockchain.chain.append(block)\n        print(f\"Added block {block.index}\")\n```"
                        },
                        pitfalls: ["Network partitions", "Eclipse attacks", "Sybil attacks"],
                        concepts: ["P2P networking", "Gossip protocols", "Chain synchronization"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 4,
                        name: "Consensus & Fork Resolution",
                        description: "Handle chain forks and achieve consensus.",
                        criteria: ["Longest chain rule", "Fork detection", "Orphan block handling", "Double-spend prevention"],
                        hints: {
                            level1: "Accept the longest valid chain. Reorganize on longer chain.",
                            level2: "Keep orphan blocks in case parent arrives later. Wait for confirmations.",
                            level3: "## Fork Resolution\n\n```python\nclass Blockchain:\n    def __init__(self):\n        self.chain = [self.create_genesis_block()]\n        self.orphan_blocks = {}  # parent_hash -> [blocks]\n        self.confirmations_required = 6\n    \n    def add_block(self, block):\n        # Case 1: Extends main chain\n        if block.previous_hash == self.chain[-1].hash:\n            if self.is_valid_block(block):\n                self.chain.append(block)\n                self.process_orphans(block.hash)\n                return True\n        \n        # Case 2: Orphan (parent not found)\n        parent_in_chain = self.find_block_by_hash(block.previous_hash)\n        if not parent_in_chain:\n            self.orphan_blocks.setdefault(block.previous_hash, []).append(block)\n            return False\n        \n        # Case 3: Fork - extends a previous block\n        fork_chain = self.build_fork_chain(block)\n        if len(fork_chain) > len(self.chain):\n            # Reorganize to longer chain\n            self.reorganize(fork_chain)\n            return True\n        \n        return False\n    \n    def process_orphans(self, parent_hash):\n        if parent_hash in self.orphan_blocks:\n            orphans = self.orphan_blocks.pop(parent_hash)\n            for orphan in orphans:\n                self.add_block(orphan)\n    \n    def build_fork_chain(self, new_block):\n        # Build chain from genesis to new block\n        chain = [new_block]\n        current_hash = new_block.previous_hash\n        \n        while current_hash != self.chain[0].hash:\n            block = self.find_block_by_hash(current_hash)\n            if not block:\n                return []  # Can't build complete chain\n            chain.insert(0, block)\n            current_hash = block.previous_hash\n        \n        return [self.chain[0]] + chain\n    \n    def reorganize(self, new_chain):\n        # Find common ancestor\n        old_blocks = []\n        for i, (old, new) in enumerate(zip(self.chain, new_chain)):\n            if old.hash != new.hash:\n                old_blocks = self.chain[i:]\n                break\n        \n        # Return transactions from old blocks to mempool\n        for block in old_blocks:\n            for tx in block.transactions:\n                if tx.sender != 'COINBASE':\n                    self.pending_transactions.append(tx)\n        \n        self.chain = new_chain\n        print(f\"Chain reorganized, dropped {len(old_blocks)} blocks\")\n    \n    def is_confirmed(self, tx_hash):\n        for i, block in enumerate(reversed(self.chain)):\n            for tx in block.transactions:\n                if tx.hash() == tx_hash:\n                    confirmations = i + 1\n                    return confirmations >= self.confirmations_required\n        return False\n```"
                        },
                        pitfalls: ["51% attacks", "Selfish mining", "Long reorgs"],
                        concepts: ["Nakamoto consensus", "Fork resolution", "Finality"],
                        estimatedHours: "12-18"
                    }
                ]
            },

            // AI-ML - EXPERT
            "build-nn-framework": {
                name: "Build Your Own Neural Network Framework",
                description: "Build a PyTorch/TensorFlow-like deep learning framework with automatic differentiation.",
                difficulty: "expert",
                estimatedHours: "60-100",
                prerequisites: ["Linear algebra", "Calculus (chain rule)", "Python/NumPy", "Basic neural networks"],
                languages: {
                    recommended: ["Python", "Rust", "C++"],
                    also: ["Julia"]
                },
                resources: [
                    { type: "video", name: "Micrograd by Karpathy", url: "https://www.youtube.com/watch?v=VMj-3S1tku0" },
                    { type: "article", name: "Autodiff from Scratch", url: "https://sidsite.com/posts/autodiff/" },
                    { type: "code", name: "Tinygrad", url: "https://github.com/tinygrad/tinygrad" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Tensor & Operations",
                        description: "Implement tensor data structure with basic operations.",
                        criteria: ["N-dimensional array storage", "Element-wise operations", "Matrix multiplication", "Broadcasting", "GPU support (optional)"],
                        hints: {
                            level1: "Wrap NumPy arrays, track shape and data. Implement __add__, __mul__, etc.",
                            level2: "Support broadcasting rules. Matrix multiply with @ operator.",
                            level3: "## Tensor Implementation\n\n```python\nimport numpy as np\n\nclass Tensor:\n    def __init__(self, data, requires_grad=False):\n        self.data = np.array(data, dtype=np.float32)\n        self.requires_grad = requires_grad\n        self.grad = None\n        self._backward = lambda: None\n        self._prev = set()\n    \n    @property\n    def shape(self):\n        return self.data.shape\n    \n    def __repr__(self):\n        return f\"Tensor({self.data}, requires_grad={self.requires_grad})\"\n    \n    def __add__(self, other):\n        other = other if isinstance(other, Tensor) else Tensor(other)\n        out = Tensor(self.data + other.data, requires_grad=self.requires_grad or other.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n                self.grad = self.grad + out.grad if self.grad is not None else out.grad.copy()\n            if other.requires_grad:\n                other.grad = other.grad + out.grad if other.grad is not None else out.grad.copy()\n        \n        out._backward = _backward\n        out._prev = {self, other}\n        return out\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Tensor) else Tensor(other)\n        out = Tensor(self.data * other.data, requires_grad=self.requires_grad or other.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n                grad = other.data * out.grad\n                self.grad = self.grad + grad if self.grad is not None else grad\n            if other.requires_grad:\n                grad = self.data * out.grad\n                other.grad = other.grad + grad if other.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev = {self, other}\n        return out\n    \n    def __matmul__(self, other):\n        out = Tensor(self.data @ other.data, requires_grad=self.requires_grad or other.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n                grad = out.grad @ other.data.T\n                self.grad = self.grad + grad if self.grad is not None else grad\n            if other.requires_grad:\n                grad = self.data.T @ out.grad\n                other.grad = other.grad + grad if other.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev = {self, other}\n        return out\n    \n    def sum(self):\n        out = Tensor(self.data.sum(), requires_grad=self.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n                grad = np.ones_like(self.data) * out.grad\n                self.grad = self.grad + grad if self.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev = {self}\n        return out\n```"
                        },
                        pitfalls: ["Broadcasting gradient shapes", "In-place operations breaking grad", "Memory management"],
                        concepts: ["Tensor operations", "Broadcasting", "Memory layout"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 2,
                        name: "Automatic Differentiation",
                        description: "Implement reverse-mode autodiff (backpropagation).",
                        criteria: ["Computational graph", "Reverse-mode autodiff", "Gradient accumulation", "Topological sort for backward pass"],
                        hints: {
                            level1: "Build computation graph as operations happen. Backward traverses in reverse.",
                            level2: "Topological sort ensures gradients flow correctly. Accumulate, don't replace.",
                            level3: "## Backpropagation\n\n```python\nclass Tensor:\n    # ... previous code ...\n    \n    def backward(self):\n        # Build topological order\n        topo = []\n        visited = set()\n        \n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        \n        build_topo(self)\n        \n        # Initialize gradient of output\n        self.grad = np.ones_like(self.data)\n        \n        # Backpropagate\n        for node in reversed(topo):\n            node._backward()\n    \n    def relu(self):\n        out = Tensor(np.maximum(0, self.data), requires_grad=self.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n                grad = (self.data > 0) * out.grad\n                self.grad = self.grad + grad if self.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev = {self}\n        return out\n    \n    def exp(self):\n        out = Tensor(np.exp(self.data), requires_grad=self.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n                grad = out.data * out.grad\n                self.grad = self.grad + grad if self.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev = {self}\n        return out\n    \n    def log(self):\n        out = Tensor(np.log(self.data), requires_grad=self.requires_grad)\n        \n        def _backward():\n            if self.requires_grad:\n                grad = out.grad / self.data\n                self.grad = self.grad + grad if self.grad is not None else grad\n        \n        out._backward = _backward\n        out._prev = {self}\n        return out\n\ndef softmax(x):\n    exp_x = (x - Tensor(x.data.max(axis=-1, keepdims=True))).exp()\n    return exp_x * Tensor(1.0 / exp_x.data.sum(axis=-1, keepdims=True))\n\ndef cross_entropy(pred, target):\n    # pred: (batch, classes), target: (batch,) indices\n    batch_size = pred.shape[0]\n    log_probs = pred.log()\n    # Select log prob of correct class\n    loss = Tensor(0.0, requires_grad=True)\n    for i in range(batch_size):\n        loss = loss - log_probs.data[i, target[i]]\n    return loss * Tensor(1.0 / batch_size)\n```"
                        },
                        pitfalls: ["Gradient not accumulated", "Wrong topological order", "Vanishing/exploding gradients"],
                        concepts: ["Computational graphs", "Chain rule", "Backpropagation"],
                        estimatedHours: "15-20"
                    },
                    {
                        id: 3,
                        name: "Layers & Modules",
                        description: "Implement neural network layers and module system.",
                        criteria: ["Linear layer", "Activation functions", "Module base class", "Parameter management", "Forward pass"],
                        hints: {
                            level1: "Module stores parameters. Linear layer: y = Wx + b.",
                            level2: "Implement parameters() method to collect all trainable tensors.",
                            level3: "## Module System\n\n```python\nclass Module:\n    def __init__(self):\n        self._parameters = {}\n        self._modules = {}\n    \n    def __setattr__(self, name, value):\n        if isinstance(value, Tensor) and value.requires_grad:\n            self._parameters[name] = value\n        elif isinstance(value, Module):\n            self._modules[name] = value\n        super().__setattr__(name, value)\n    \n    def parameters(self):\n        params = list(self._parameters.values())\n        for module in self._modules.values():\n            params.extend(module.parameters())\n        return params\n    \n    def zero_grad(self):\n        for p in self.parameters():\n            p.grad = None\n    \n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\nclass Linear(Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        # Xavier initialization\n        k = np.sqrt(1 / in_features)\n        self.weight = Tensor(np.random.uniform(-k, k, (in_features, out_features)), requires_grad=True)\n        self.bias = Tensor(np.zeros(out_features), requires_grad=True)\n    \n    def forward(self, x):\n        return x @ self.weight + self.bias\n\nclass ReLU(Module):\n    def forward(self, x):\n        return x.relu()\n\nclass Sequential(Module):\n    def __init__(self, *layers):\n        super().__init__()\n        for i, layer in enumerate(layers):\n            setattr(self, f'layer_{i}', layer)\n        self.layers = layers\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n# Example: MLP\nclass MLP(Module):\n    def __init__(self, in_dim, hidden_dim, out_dim):\n        super().__init__()\n        self.fc1 = Linear(in_dim, hidden_dim)\n        self.fc2 = Linear(hidden_dim, out_dim)\n    \n    def forward(self, x):\n        x = self.fc1(x).relu()\n        x = self.fc2(x)\n        return x\n```"
                        },
                        pitfalls: ["Parameter not tracked", "In-place modifications", "Wrong initialization"],
                        concepts: ["Module pattern", "Parameter management", "Initialization"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 4,
                        name: "Optimizers & Training",
                        description: "Implement optimizers and training loop.",
                        criteria: ["SGD optimizer", "Adam optimizer", "Learning rate scheduling", "Mini-batch training", "Model save/load"],
                        hints: {
                            level1: "SGD: param -= lr * grad. Adam adds momentum and adaptive learning.",
                            level2: "Track running averages for Adam. Clip gradients to prevent explosion.",
                            level3: "## Optimizers\n\n```python\nclass Optimizer:\n    def __init__(self, parameters, lr=0.01):\n        self.parameters = parameters\n        self.lr = lr\n    \n    def zero_grad(self):\n        for p in self.parameters:\n            p.grad = None\n    \n    def step(self):\n        raise NotImplementedError\n\nclass SGD(Optimizer):\n    def __init__(self, parameters, lr=0.01, momentum=0.0):\n        super().__init__(parameters, lr)\n        self.momentum = momentum\n        self.velocities = [np.zeros_like(p.data) for p in parameters]\n    \n    def step(self):\n        for i, p in enumerate(self.parameters):\n            if p.grad is None:\n                continue\n            self.velocities[i] = self.momentum * self.velocities[i] - self.lr * p.grad\n            p.data += self.velocities[i]\n\nclass Adam(Optimizer):\n    def __init__(self, parameters, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n        super().__init__(parameters, lr)\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n        self.t = 0\n        self.m = [np.zeros_like(p.data) for p in parameters]\n        self.v = [np.zeros_like(p.data) for p in parameters]\n    \n    def step(self):\n        self.t += 1\n        for i, p in enumerate(self.parameters):\n            if p.grad is None:\n                continue\n            \n            # Update biased first moment estimate\n            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * p.grad\n            # Update biased second raw moment estimate\n            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (p.grad ** 2)\n            \n            # Compute bias-corrected estimates\n            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n            \n            # Update parameters\n            p.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n\n# Training loop\ndef train(model, X, y, epochs=100, batch_size=32, lr=0.01):\n    optimizer = Adam(model.parameters(), lr=lr)\n    n_samples = len(X)\n    \n    for epoch in range(epochs):\n        # Shuffle data\n        indices = np.random.permutation(n_samples)\n        total_loss = 0\n        \n        for i in range(0, n_samples, batch_size):\n            batch_idx = indices[i:i+batch_size]\n            x_batch = Tensor(X[batch_idx], requires_grad=False)\n            y_batch = y[batch_idx]\n            \n            # Forward\n            pred = model(x_batch)\n            loss = cross_entropy(softmax(pred), y_batch)\n            \n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.data\n        \n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}, Loss: {total_loss / (n_samples / batch_size):.4f}\")\n```"
                        },
                        pitfalls: ["Bias correction in Adam", "Learning rate too high/low", "Not shuffling data"],
                        concepts: ["Optimization algorithms", "Momentum", "Adaptive learning rates"],
                        estimatedHours: "15-20"
                    }
                ]
            },

            "build-transformer": {
                name: "Build Your Own Transformer",
                description: "Implement a GPT-style transformer from scratch.",
                difficulty: "expert",
                estimatedHours: "50-80",
                prerequisites: ["Neural networks", "Attention mechanism basics", "NLP fundamentals", "PyTorch/TensorFlow"],
                languages: {
                    recommended: ["Python"],
                    also: ["Julia", "Rust"]
                },
                resources: [
                    { type: "paper", name: "Attention Is All You Need", url: "https://arxiv.org/abs/1706.03762" },
                    { type: "video", name: "Let's build GPT by Karpathy", url: "https://www.youtube.com/watch?v=kCc8FmEb1nY" },
                    { type: "code", name: "minGPT", url: "https://github.com/karpathy/minGPT" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Self-Attention",
                        description: "Implement scaled dot-product attention.",
                        criteria: ["Query, Key, Value projections", "Attention scores computation", "Softmax normalization", "Causal masking for autoregressive"],
                        hints: {
                            level1: "Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V",
                            level2: "For autoregressive: mask future positions with -inf before softmax.",
                            level3: "## Self-Attention\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n    \n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.shape\n        \n        # Project to Q, K, V\n        Q = self.q_proj(x)  # (batch, seq, embed)\n        K = self.k_proj(x)\n        V = self.v_proj(x)\n        \n        # Reshape for multi-head: (batch, heads, seq, head_dim)\n        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        # Apply causal mask\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        \n        # Apply attention to values\n        context = torch.matmul(attn_weights, V)  # (batch, heads, seq, head_dim)\n        \n        # Concatenate heads\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n        \n        return self.out_proj(context)\n\ndef create_causal_mask(seq_len):\n    # Lower triangular matrix (1s where we can attend)\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq, seq)\n```"
                        },
                        pitfalls: ["Wrong attention dimension", "Forgetting to scale", "Mask applied after softmax"],
                        concepts: ["Attention mechanism", "Multi-head attention", "Masking"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "Transformer Block",
                        description: "Build a complete transformer block with FFN and normalization.",
                        criteria: ["Feed-forward network", "Layer normalization", "Residual connections", "Dropout"],
                        hints: {
                            level1: "Block: x -> LayerNorm -> Attention -> Residual -> LayerNorm -> FFN -> Residual",
                            level2: "FFN typically expands 4x then projects back. Use GELU activation.",
                            level3: "## Transformer Block\n\n```python\nclass FeedForward(nn.Module):\n    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n        super().__init__()\n        self.attention = SelfAttention(embed_dim, num_heads)\n        self.ffn = FeedForward(embed_dim, ff_hidden_dim, dropout)\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Pre-norm architecture (GPT-2 style)\n        attn_out = self.attention(self.ln1(x), mask)\n        x = x + self.dropout1(attn_out)\n        \n        ffn_out = self.ffn(self.ln2(x))\n        x = x + self.dropout2(ffn_out)\n        \n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, max_seq_len, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.max_seq_len = max_seq_len\n        \n        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, embed_dim * 4, dropout)\n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(embed_dim)\n        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n        \n        # Weight tying\n        self.lm_head.weight = self.token_embedding.weight\n    \n    def forward(self, idx):\n        batch_size, seq_len = idx.shape\n        \n        # Embeddings\n        tok_emb = self.token_embedding(idx)\n        pos = torch.arange(0, seq_len, device=idx.device)\n        pos_emb = self.position_embedding(pos)\n        x = self.dropout(tok_emb + pos_emb)\n        \n        # Causal mask\n        mask = create_causal_mask(seq_len).to(idx.device)\n        \n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x, mask)\n        \n        x = self.ln_final(x)\n        logits = self.lm_head(x)\n        \n        return logits\n```"
                        },
                        pitfalls: ["Pre-norm vs post-norm confusion", "Missing residual connections", "Wrong FFN expansion ratio"],
                        concepts: ["Transformer architecture", "Layer normalization", "Residual learning"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 3,
                        name: "Training Pipeline",
                        description: "Implement training with language modeling objective.",
                        criteria: ["Cross-entropy loss for next token", "Gradient clipping", "Learning rate warmup & decay", "Mixed precision training"],
                        hints: {
                            level1: "Shift labels by 1: predict token[i+1] from token[0:i].",
                            level2: "Use cosine learning rate schedule with warmup.",
                            level3: "## Training\n\n```python\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\n\nclass GPTTrainer:\n    def __init__(self, model, train_data, config):\n        self.model = model\n        self.train_data = train_data\n        self.config = config\n        \n        self.optimizer = optim.AdamW(\n            model.parameters(),\n            lr=config.learning_rate,\n            betas=(0.9, 0.95),\n            weight_decay=config.weight_decay\n        )\n        \n        self.scaler = GradScaler()  # For mixed precision\n    \n    def get_lr(self, step):\n        # Linear warmup then cosine decay\n        warmup_steps = self.config.warmup_steps\n        max_steps = self.config.max_steps\n        min_lr = self.config.learning_rate / 10\n        \n        if step < warmup_steps:\n            return self.config.learning_rate * step / warmup_steps\n        elif step > max_steps:\n            return min_lr\n        else:\n            decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n            return min_lr + coeff * (self.config.learning_rate - min_lr)\n    \n    def train_step(self, batch, step):\n        # Update learning rate\n        lr = self.get_lr(step)\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        \n        self.model.train()\n        x, y = batch  # x: input tokens, y: target tokens (shifted by 1)\n        \n        with autocast():  # Mixed precision\n            logits = self.model(x)\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                y.view(-1),\n                ignore_index=-100  # Padding token\n            )\n        \n        self.optimizer.zero_grad()\n        self.scaler.scale(loss).backward()\n        \n        # Gradient clipping\n        self.scaler.unscale_(self.optimizer)\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)\n        \n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n        \n        return loss.item()\n    \n    def train(self):\n        step = 0\n        for epoch in range(self.config.epochs):\n            for batch in self.train_data:\n                loss = self.train_step(batch, step)\n                step += 1\n                \n                if step % 100 == 0:\n                    print(f\"Step {step}, Loss: {loss:.4f}, LR: {self.get_lr(step):.6f}\")\n                \n                if step >= self.config.max_steps:\n                    return\n```"
                        },
                        pitfalls: ["Label shifting wrong", "No gradient clipping causing instability", "Wrong loss computation"],
                        concepts: ["Language modeling", "Learning rate scheduling", "Mixed precision"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 4,
                        name: "Text Generation",
                        description: "Implement autoregressive text generation.",
                        criteria: ["Greedy decoding", "Temperature sampling", "Top-k sampling", "Top-p (nucleus) sampling", "KV cache for efficient generation"],
                        hints: {
                            level1: "Generate one token at a time, append to input, repeat.",
                            level2: "KV cache: store key/value from previous tokens to avoid recomputation.",
                            level3: "## Text Generation\n\n```python\nclass GPT(nn.Module):\n    # ... previous code ...\n    \n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None):\n        for _ in range(max_new_tokens):\n            # Crop to max sequence length\n            idx_cond = idx if idx.size(1) <= self.max_seq_len else idx[:, -self.max_seq_len:]\n            \n            # Forward pass\n            logits = self(idx_cond)\n            logits = logits[:, -1, :] / temperature  # Last position only\n            \n            # Apply top-k\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = float('-inf')\n            \n            # Apply top-p (nucleus sampling)\n            if top_p is not None:\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n                \n                # Remove tokens with cumulative probability above threshold\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n                sorted_indices_to_remove[:, 0] = 0\n                \n                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n                logits[indices_to_remove] = float('-inf')\n            \n            # Sample\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            \n            idx = torch.cat((idx, idx_next), dim=1)\n        \n        return idx\n\n# KV Cache for efficient generation\nclass CachedSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        # ... same as SelfAttention ...\n    \n    def forward(self, x, kv_cache=None):\n        batch_size, seq_len, _ = x.shape\n        \n        Q = self.q_proj(x)\n        K = self.k_proj(x)\n        V = self.v_proj(x)\n        \n        if kv_cache is not None:\n            # Append new K, V to cache\n            cached_k, cached_v = kv_cache\n            K = torch.cat([cached_k, K], dim=1)\n            V = torch.cat([cached_v, V], dim=1)\n        \n        new_cache = (K, V)\n        \n        # Reshape for multi-head\n        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Attention (Q only attends to available K, V)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attn_weights = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn_weights, V)\n        \n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n        return self.out_proj(context), new_cache\n```"
                        },
                        pitfalls: ["Repetitive text without sampling", "KV cache dimension mismatch", "Temperature of 0 causing division"],
                        concepts: ["Autoregressive generation", "Sampling strategies", "KV caching"],
                        estimatedHours: "12-18"
                    }
                ]
            },

            // GAME-DEV - EXPERT
            "build-game-engine": {
                name: "Build Your Own Game Engine",
                description: "Build a 2D/3D game engine with rendering, physics, and entity management.",
                difficulty: "expert",
                estimatedHours: "100-200",
                prerequisites: ["Graphics programming basics", "Linear algebra", "C/C++ or Rust", "Game architecture patterns"],
                languages: {
                    recommended: ["C++", "Rust", "C"],
                    also: ["Zig"]
                },
                resources: [
                    { type: "book", name: "Game Engine Architecture", url: "https://www.gameenginebook.com/" },
                    { type: "video", name: "Handmade Hero", url: "https://handmadehero.org/" },
                    { type: "tutorial", name: "Learn OpenGL", url: "https://learnopengl.com/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Window & Rendering Foundation",
                        description: "Create window and basic rendering pipeline.",
                        criteria: ["Window creation (SDL/GLFW)", "OpenGL/Vulkan context", "Basic 2D sprite rendering", "Texture loading", "Shader system"],
                        hints: {
                            level1: "Use SDL2 or GLFW for cross-platform windowing. Initialize OpenGL context.",
                            level2: "Create vertex buffer for quad, write basic vertex/fragment shaders.",
                            level3: "## Rendering Foundation\n\n```cpp\n// Window and OpenGL setup\n#include <SDL2/SDL.h>\n#include <glad/glad.h>\n\nclass Window {\npublic:\n    SDL_Window* window;\n    SDL_GLContext glContext;\n    int width, height;\n    \n    bool init(const char* title, int w, int h) {\n        SDL_Init(SDL_INIT_VIDEO);\n        \n        SDL_GL_SetAttribute(SDL_GL_CONTEXT_MAJOR_VERSION, 4);\n        SDL_GL_SetAttribute(SDL_GL_CONTEXT_MINOR_VERSION, 1);\n        SDL_GL_SetAttribute(SDL_GL_CONTEXT_PROFILE_MASK, SDL_GL_CONTEXT_PROFILE_CORE);\n        \n        window = SDL_CreateWindow(title, SDL_WINDOWPOS_CENTERED, SDL_WINDOWPOS_CENTERED,\n                                  w, h, SDL_WINDOW_OPENGL);\n        glContext = SDL_GL_CreateContext(window);\n        gladLoadGLLoader((GLADloadproc)SDL_GL_GetProcAddress);\n        \n        width = w; height = h;\n        return true;\n    }\n    \n    void swap() { SDL_GL_SwapWindow(window); }\n};\n\n// Shader\nclass Shader {\npublic:\n    GLuint program;\n    \n    void compile(const char* vertSrc, const char* fragSrc) {\n        GLuint vert = glCreateShader(GL_VERTEX_SHADER);\n        glShaderSource(vert, 1, &vertSrc, NULL);\n        glCompileShader(vert);\n        \n        GLuint frag = glCreateShader(GL_FRAGMENT_SHADER);\n        glShaderSource(frag, 1, &fragSrc, NULL);\n        glCompileShader(frag);\n        \n        program = glCreateProgram();\n        glAttachShader(program, vert);\n        glAttachShader(program, frag);\n        glLinkProgram(program);\n        \n        glDeleteShader(vert);\n        glDeleteShader(frag);\n    }\n    \n    void use() { glUseProgram(program); }\n    void setMat4(const char* name, const float* mat) {\n        glUniformMatrix4fv(glGetUniformLocation(program, name), 1, GL_FALSE, mat);\n    }\n};\n\n// Sprite Batch for efficient 2D rendering\nclass SpriteBatch {\n    GLuint vao, vbo, ebo;\n    std::vector<Vertex> vertices;\n    Shader* shader;\n    \npublic:\n    void begin() { vertices.clear(); }\n    \n    void draw(Texture* tex, float x, float y, float w, float h) {\n        // Add 4 vertices for quad\n        vertices.push_back({{x, y}, {0, 0}});\n        vertices.push_back({{x+w, y}, {1, 0}});\n        vertices.push_back({{x+w, y+h}, {1, 1}});\n        vertices.push_back({{x, y+h}, {0, 1}});\n    }\n    \n    void end() {\n        glBindBuffer(GL_ARRAY_BUFFER, vbo);\n        glBufferData(GL_ARRAY_BUFFER, vertices.size() * sizeof(Vertex),\n                    vertices.data(), GL_DYNAMIC_DRAW);\n        \n        shader->use();\n        glBindVertexArray(vao);\n        glDrawElements(GL_TRIANGLES, (vertices.size() / 4) * 6, GL_UNSIGNED_INT, 0);\n    }\n};\n```"
                        },
                        pitfalls: ["OpenGL state leaks", "Shader compilation errors not checked", "Wrong vertex attribute setup"],
                        concepts: ["Graphics APIs", "Shader programming", "Batch rendering"],
                        estimatedHours: "20-30"
                    },
                    {
                        id: 2,
                        name: "Entity Component System",
                        description: "Implement ECS architecture for game objects.",
                        criteria: ["Entity management", "Component storage", "System execution", "Component queries", "Entity creation/destruction"],
                        hints: {
                            level1: "Entity is just an ID. Components are data. Systems process entities with specific components.",
                            level2: "Use archetypes or sparse sets for efficient component storage and iteration.",
                            level3: "## ECS Implementation\n\n```cpp\nusing Entity = uint32_t;\nusing ComponentType = uint32_t;\n\n// Component storage using sparse set\ntemplate<typename T>\nclass ComponentArray {\n    std::vector<T> dense;\n    std::vector<Entity> denseToEntity;\n    std::unordered_map<Entity, size_t> entityToIndex;\n    \npublic:\n    void insert(Entity entity, T component) {\n        size_t index = dense.size();\n        dense.push_back(component);\n        denseToEntity.push_back(entity);\n        entityToIndex[entity] = index;\n    }\n    \n    void remove(Entity entity) {\n        size_t index = entityToIndex[entity];\n        size_t lastIndex = dense.size() - 1;\n        \n        // Swap with last\n        dense[index] = dense[lastIndex];\n        denseToEntity[index] = denseToEntity[lastIndex];\n        entityToIndex[denseToEntity[index]] = index;\n        \n        dense.pop_back();\n        denseToEntity.pop_back();\n        entityToIndex.erase(entity);\n    }\n    \n    T& get(Entity entity) { return dense[entityToIndex[entity]]; }\n    bool has(Entity entity) { return entityToIndex.count(entity) > 0; }\n    \n    auto begin() { return dense.begin(); }\n    auto end() { return dense.end(); }\n};\n\nclass World {\n    Entity nextEntity = 0;\n    std::unordered_map<ComponentType, void*> componentArrays;\n    std::set<Entity> entities;\n    \npublic:\n    Entity createEntity() {\n        Entity e = nextEntity++;\n        entities.insert(e);\n        return e;\n    }\n    \n    void destroyEntity(Entity e) {\n        entities.erase(e);\n        // Remove from all component arrays...\n    }\n    \n    template<typename T>\n    void addComponent(Entity e, T component) {\n        getComponentArray<T>()->insert(e, component);\n    }\n    \n    template<typename T>\n    T& getComponent(Entity e) {\n        return getComponentArray<T>()->get(e);\n    }\n    \n    template<typename... Components>\n    void forEach(std::function<void(Entity, Components&...)> func) {\n        for (Entity e : entities) {\n            if ((getComponentArray<Components>()->has(e) && ...)) {\n                func(e, getComponent<Components>(e)...);\n            }\n        }\n    }\n};\n\n// Components\nstruct Transform { float x, y, rotation, scaleX, scaleY; };\nstruct Velocity { float vx, vy; };\nstruct Sprite { Texture* texture; float width, height; };\n\n// Systems\nvoid MovementSystem(World& world, float dt) {\n    world.forEach<Transform, Velocity>([dt](Entity e, Transform& t, Velocity& v) {\n        t.x += v.vx * dt;\n        t.y += v.vy * dt;\n    });\n}\n\nvoid RenderSystem(World& world, SpriteBatch& batch) {\n    world.forEach<Transform, Sprite>([&batch](Entity e, Transform& t, Sprite& s) {\n        batch.draw(s.texture, t.x, t.y, s.width * t.scaleX, s.height * t.scaleY);\n    });\n}\n```"
                        },
                        pitfalls: ["Component iteration invalidation", "Memory fragmentation", "System ordering dependencies"],
                        concepts: ["ECS architecture", "Data-oriented design", "Cache efficiency"],
                        estimatedHours: "20-30"
                    },
                    {
                        id: 3,
                        name: "Physics & Collision",
                        description: "Implement 2D physics and collision detection.",
                        criteria: ["Rigid body dynamics", "Collision detection (AABB, circles)", "Collision response", "Spatial partitioning", "Physics timestep"],
                        hints: {
                            level1: "Semi-implicit Euler for integration. AABB for broad phase, then narrow phase.",
                            level2: "Fixed timestep with accumulator for deterministic physics.",
                            level3: "## 2D Physics\n\n```cpp\nstruct RigidBody {\n    float mass, invMass;\n    float vx, vy;\n    float ax, ay;\n    float restitution;  // Bounciness\n};\n\nstruct Collider {\n    enum Type { AABB, Circle } type;\n    union {\n        struct { float halfW, halfH; } aabb;\n        struct { float radius; } circle;\n    };\n};\n\nclass PhysicsWorld {\n    float gravity = -9.81f;\n    float fixedDeltaTime = 1.0f / 60.0f;\n    float accumulator = 0.0f;\n    \npublic:\n    void update(World& world, float dt) {\n        accumulator += dt;\n        \n        while (accumulator >= fixedDeltaTime) {\n            fixedUpdate(world, fixedDeltaTime);\n            accumulator -= fixedDeltaTime;\n        }\n    }\n    \n    void fixedUpdate(World& world, float dt) {\n        // Apply forces\n        world.forEach<Transform, RigidBody>([this, dt](Entity e, Transform& t, RigidBody& rb) {\n            if (rb.invMass > 0) {\n                rb.ay += gravity;\n            }\n            \n            // Semi-implicit Euler\n            rb.vx += rb.ax * dt;\n            rb.vy += rb.ay * dt;\n            t.x += rb.vx * dt;\n            t.y += rb.vy * dt;\n            \n            rb.ax = rb.ay = 0;\n        });\n        \n        // Collision detection & response\n        detectAndResolveCollisions(world);\n    }\n    \n    void detectAndResolveCollisions(World& world) {\n        std::vector<std::tuple<Entity, Entity>> pairs;\n        \n        // Broad phase: find potential collisions\n        world.forEach<Transform, Collider>([&](Entity e1, Transform& t1, Collider& c1) {\n            world.forEach<Transform, Collider>([&](Entity e2, Transform& t2, Collider& c2) {\n                if (e1 >= e2) return;\n                if (broadPhaseCheck(t1, c1, t2, c2)) {\n                    pairs.emplace_back(e1, e2);\n                }\n            });\n        });\n        \n        // Narrow phase: resolve collisions\n        for (auto [e1, e2] : pairs) {\n            auto& t1 = world.getComponent<Transform>(e1);\n            auto& t2 = world.getComponent<Transform>(e2);\n            auto& c1 = world.getComponent<Collider>(e1);\n            auto& c2 = world.getComponent<Collider>(e2);\n            \n            CollisionInfo info;\n            if (narrowPhaseCheck(t1, c1, t2, c2, info)) {\n                resolveCollision(world, e1, e2, info);\n            }\n        }\n    }\n    \n    void resolveCollision(World& world, Entity e1, Entity e2, CollisionInfo& info) {\n        auto& rb1 = world.getComponent<RigidBody>(e1);\n        auto& rb2 = world.getComponent<RigidBody>(e2);\n        auto& t1 = world.getComponent<Transform>(e1);\n        auto& t2 = world.getComponent<Transform>(e2);\n        \n        // Separate objects\n        float totalInvMass = rb1.invMass + rb2.invMass;\n        t1.x -= info.normal.x * info.depth * (rb1.invMass / totalInvMass);\n        t1.y -= info.normal.y * info.depth * (rb1.invMass / totalInvMass);\n        t2.x += info.normal.x * info.depth * (rb2.invMass / totalInvMass);\n        t2.y += info.normal.y * info.depth * (rb2.invMass / totalInvMass);\n        \n        // Impulse-based response\n        float relVelN = (rb2.vx - rb1.vx) * info.normal.x + (rb2.vy - rb1.vy) * info.normal.y;\n        if (relVelN > 0) return;  // Already separating\n        \n        float e = std::min(rb1.restitution, rb2.restitution);\n        float j = -(1 + e) * relVelN / totalInvMass;\n        \n        rb1.vx -= j * rb1.invMass * info.normal.x;\n        rb1.vy -= j * rb1.invMass * info.normal.y;\n        rb2.vx += j * rb2.invMass * info.normal.x;\n        rb2.vy += j * rb2.invMass * info.normal.y;\n    }\n};\n```"
                        },
                        pitfalls: ["Variable timestep physics", "Tunneling (fast objects passing through)", "Collision jitter"],
                        concepts: ["Physics simulation", "Collision detection", "Impulse resolution"],
                        estimatedHours: "25-40"
                    },
                    {
                        id: 4,
                        name: "Resource & Scene Management",
                        description: "Implement asset loading and scene system.",
                        criteria: ["Asset loading (textures, audio, data)", "Resource caching", "Scene serialization", "Scene transitions", "Game loop"],
                        hints: {
                            level1: "Load resources once, cache by path. Scenes contain entities and their components.",
                            level2: "Use JSON/binary for scene serialization. Implement proper cleanup on scene switch.",
                            level3: "## Resource & Scene Management\n\n```cpp\nclass ResourceManager {\n    std::unordered_map<std::string, std::shared_ptr<Texture>> textures;\n    std::unordered_map<std::string, std::shared_ptr<Sound>> sounds;\n    \npublic:\n    std::shared_ptr<Texture> loadTexture(const std::string& path) {\n        if (textures.count(path)) return textures[path];\n        \n        auto tex = std::make_shared<Texture>();\n        tex->loadFromFile(path);\n        textures[path] = tex;\n        return tex;\n    }\n    \n    void unloadUnused() {\n        for (auto it = textures.begin(); it != textures.end();) {\n            if (it->second.use_count() == 1) {\n                it = textures.erase(it);\n            } else {\n                ++it;\n            }\n        }\n    }\n};\n\nclass Scene {\npublic:\n    std::string name;\n    World world;\n    \n    virtual void onEnter() = 0;\n    virtual void onExit() = 0;\n    virtual void update(float dt) = 0;\n    virtual void render() = 0;\n    \n    void saveToFile(const std::string& path) {\n        json j;\n        j[\"name\"] = name;\n        j[\"entities\"] = json::array();\n        \n        world.forEach<Transform>([&](Entity e, Transform& t) {\n            json entity;\n            entity[\"id\"] = e;\n            entity[\"transform\"] = {t.x, t.y, t.rotation, t.scaleX, t.scaleY};\n            // Serialize other components...\n            j[\"entities\"].push_back(entity);\n        });\n        \n        std::ofstream file(path);\n        file << j.dump(2);\n    }\n    \n    void loadFromFile(const std::string& path) {\n        std::ifstream file(path);\n        json j = json::parse(file);\n        \n        name = j[\"name\"];\n        for (auto& entity : j[\"entities\"]) {\n            Entity e = world.createEntity();\n            auto& t = entity[\"transform\"];\n            world.addComponent(e, Transform{t[0], t[1], t[2], t[3], t[4]});\n            // Deserialize other components...\n        }\n    }\n};\n\nclass Engine {\n    Window window;\n    ResourceManager resources;\n    std::unique_ptr<Scene> currentScene;\n    std::unique_ptr<Scene> nextScene;\n    bool running = true;\n    \npublic:\n    void run() {\n        float lastTime = SDL_GetTicks() / 1000.0f;\n        \n        while (running) {\n            float currentTime = SDL_GetTicks() / 1000.0f;\n            float dt = currentTime - lastTime;\n            lastTime = currentTime;\n            \n            processInput();\n            \n            if (nextScene) {\n                if (currentScene) currentScene->onExit();\n                currentScene = std::move(nextScene);\n                currentScene->onEnter();\n            }\n            \n            if (currentScene) {\n                currentScene->update(dt);\n                currentScene->render();\n            }\n            \n            window.swap();\n        }\n    }\n    \n    void changeScene(std::unique_ptr<Scene> scene) {\n        nextScene = std::move(scene);\n    }\n};\n```"
                        },
                        pitfalls: ["Resource leaks", "Scene state corruption during transition", "Serialization version incompatibility"],
                        concepts: ["Resource management", "Scene graphs", "Game architecture"],
                        estimatedHours: "20-30"
                    }
                ]
            },

            // COMPILERS - EXPERT
            "build-gc": {
                name: "Build Your Own Garbage Collector",
                description: "Implement a garbage collector with mark-sweep and generational collection.",
                difficulty: "expert",
                estimatedHours: "40-60",
                prerequisites: ["Memory management", "C/Rust", "Data structures", "Graph algorithms"],
                languages: {
                    recommended: ["C", "Rust", "Zig"],
                    also: []
                },
                resources: [
                    { type: "book", name: "The Garbage Collection Handbook", url: "https://gchandbook.org/" },
                    { type: "article", name: "Baby's First Garbage Collector", url: "https://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/" },
                    { type: "article", name: "Writing a Simple GC in C", url: "https://maplant.com/gc.html" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Mark-Sweep Collector",
                        description: "Implement basic mark-sweep garbage collection.",
                        criteria: ["Object allocation", "Root set identification", "Mark phase (DFS)", "Sweep phase", "Memory reclamation"],
                        hints: {
                            level1: "Mark all reachable objects from roots, then sweep unmarked ones.",
                            level2: "Roots are global variables and stack. Mark with DFS to find all reachable.",
                            level3: "## Mark-Sweep GC\n\n```c\n#include <stdlib.h>\n#include <stdbool.h>\n\ntypedef struct Object {\n    bool marked;\n    struct Object* next;  // Intrusive list of all objects\n    // Object type info for tracing\n    enum { OBJ_INT, OBJ_PAIR } type;\n    union {\n        int value;\n        struct { struct Object* head; struct Object* tail; } pair;\n    };\n} Object;\n\ntypedef struct {\n    Object* firstObject;  // All allocated objects\n    Object** stack;       // VM stack (roots)\n    int stackSize;\n    int numObjects;\n    int maxObjects;\n} VM;\n\nVM* newVM() {\n    VM* vm = malloc(sizeof(VM));\n    vm->firstObject = NULL;\n    vm->stack = malloc(sizeof(Object*) * 256);\n    vm->stackSize = 0;\n    vm->numObjects = 0;\n    vm->maxObjects = 8;  // Initial threshold\n    return vm;\n}\n\nObject* allocate(VM* vm, int type) {\n    if (vm->numObjects >= vm->maxObjects) {\n        gc(vm);\n    }\n    \n    Object* obj = malloc(sizeof(Object));\n    obj->marked = false;\n    obj->type = type;\n    obj->next = vm->firstObject;\n    vm->firstObject = obj;\n    vm->numObjects++;\n    return obj;\n}\n\nvoid mark(Object* obj) {\n    if (obj == NULL || obj->marked) return;\n    \n    obj->marked = true;\n    \n    if (obj->type == OBJ_PAIR) {\n        mark(obj->pair.head);\n        mark(obj->pair.tail);\n    }\n}\n\nvoid markAll(VM* vm) {\n    for (int i = 0; i < vm->stackSize; i++) {\n        mark(vm->stack[i]);\n    }\n}\n\nvoid sweep(VM* vm) {\n    Object** obj = &vm->firstObject;\n    while (*obj) {\n        if (!(*obj)->marked) {\n            Object* unreached = *obj;\n            *obj = unreached->next;\n            free(unreached);\n            vm->numObjects--;\n        } else {\n            (*obj)->marked = false;  // Reset for next GC\n            obj = &(*obj)->next;\n        }\n    }\n}\n\nvoid gc(VM* vm) {\n    int numObjects = vm->numObjects;\n    \n    markAll(vm);\n    sweep(vm);\n    \n    vm->maxObjects = vm->numObjects * 2;  // Grow threshold\n    \n    printf(\"Collected %d objects, %d remaining.\\n\",\n           numObjects - vm->numObjects, vm->numObjects);\n}\n```"
                        },
                        pitfalls: ["Missing roots", "Marking already marked objects (infinite loop)", "Not resetting marks"],
                        concepts: ["Reachability", "Graph traversal", "Memory reclamation"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "Tri-color Marking",
                        description: "Implement tri-color invariant for incremental GC.",
                        criteria: ["White/gray/black coloring", "Worklist-based marking", "Incremental marking", "Write barriers"],
                        hints: {
                            level1: "White=unmarked, gray=marked but children not scanned, black=fully scanned.",
                            level2: "Process gray objects incrementally. Write barrier adds to gray set.",
                            level3: "## Tri-color Marking\n\n```c\ntypedef enum { WHITE, GRAY, BLACK } Color;\n\ntypedef struct Object {\n    Color color;\n    // ... other fields ...\n} Object;\n\ntypedef struct {\n    Object** grayList;\n    int grayCount;\n    int grayCapacity;\n} GC;\n\nvoid makeGray(GC* gc, Object* obj) {\n    if (obj == NULL || obj->color != WHITE) return;\n    \n    obj->color = GRAY;\n    if (gc->grayCount >= gc->grayCapacity) {\n        gc->grayCapacity *= 2;\n        gc->grayList = realloc(gc->grayList, gc->grayCapacity * sizeof(Object*));\n    }\n    gc->grayList[gc->grayCount++] = obj;\n}\n\nvoid markRoots(VM* vm, GC* gc) {\n    for (int i = 0; i < vm->stackSize; i++) {\n        makeGray(gc, vm->stack[i]);\n    }\n}\n\n// Process some gray objects (incremental)\nbool markSome(GC* gc, int workAmount) {\n    int work = 0;\n    while (gc->grayCount > 0 && work < workAmount) {\n        Object* obj = gc->grayList[--gc->grayCount];\n        \n        // Scan children\n        if (obj->type == OBJ_PAIR) {\n            makeGray(gc, obj->pair.head);\n            makeGray(gc, obj->pair.tail);\n        }\n        \n        obj->color = BLACK;\n        work++;\n    }\n    return gc->grayCount == 0;  // Done?\n}\n\n// Write barrier: when mutator changes a pointer in a black object\nvoid writeBarrier(GC* gc, Object* parent, Object* child) {\n    if (parent->color == BLACK && child->color == WHITE) {\n        // Maintain tri-color invariant\n        makeGray(gc, child);  // or makeGray(gc, parent) for snapshot-at-the-beginning\n    }\n}\n\n// Incremental GC cycle\nvoid incrementalGC(VM* vm, GC* gc) {\n    static enum { IDLE, MARKING, SWEEPING } phase = IDLE;\n    static Object** sweepPtr = NULL;\n    \n    switch (phase) {\n        case IDLE:\n            // All objects start white\n            markRoots(vm, gc);\n            phase = MARKING;\n            break;\n            \n        case MARKING:\n            if (markSome(gc, 10)) {  // Process 10 objects per step\n                sweepPtr = &vm->firstObject;\n                phase = SWEEPING;\n            }\n            break;\n            \n        case SWEEPING:\n            // Sweep a few objects\n            for (int i = 0; i < 10 && *sweepPtr; i++) {\n                if ((*sweepPtr)->color == WHITE) {\n                    Object* unreached = *sweepPtr;\n                    *sweepPtr = unreached->next;\n                    free(unreached);\n                } else {\n                    (*sweepPtr)->color = WHITE;  // Reset\n                    sweepPtr = &(*sweepPtr)->next;\n                }\n            }\n            if (*sweepPtr == NULL) {\n                phase = IDLE;\n            }\n            break;\n    }\n}\n```"
                        },
                        pitfalls: ["Write barrier missing", "Violating tri-color invariant", "Race conditions"],
                        concepts: ["Incremental GC", "Write barriers", "Tri-color abstraction"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 3,
                        name: "Generational Collection",
                        description: "Implement generational GC with nursery and old generation.",
                        criteria: ["Young generation (nursery)", "Old generation", "Promotion policy", "Remembered set", "Minor vs major collection"],
                        hints: {
                            level1: "Most objects die young. Collect nursery frequently, old gen rarely.",
                            level2: "Remembered set tracks old->young pointers so we don't scan all old objects.",
                            level3: "## Generational GC\n\n```c\ntypedef struct {\n    // Young generation (bump allocation)\n    char* nurseryStart;\n    char* nurseryEnd;\n    char* nurseryAlloc;\n    \n    // Old generation\n    Object* oldGenList;\n    \n    // Remembered set: old objects pointing to young\n    Object** rememberedSet;\n    int rememberedCount;\n    int rememberedCapacity;\n    \n    int youngCollections;\n    int promotionAge;  // Survive this many collections to promote\n} GenerationalGC;\n\nObject* allocYoung(GenerationalGC* gc, size_t size) {\n    size = ALIGN(size);\n    \n    if (gc->nurseryAlloc + size > gc->nurseryEnd) {\n        minorCollection(gc);\n        if (gc->nurseryAlloc + size > gc->nurseryEnd) {\n            // Still no space, trigger major collection\n            majorCollection(gc);\n        }\n    }\n    \n    Object* obj = (Object*)gc->nurseryAlloc;\n    gc->nurseryAlloc += size;\n    obj->age = 0;\n    obj->isOld = false;\n    return obj;\n}\n\nvoid addToRememberedSet(GenerationalGC* gc, Object* oldObj) {\n    if (gc->rememberedCount >= gc->rememberedCapacity) {\n        gc->rememberedCapacity *= 2;\n        gc->rememberedSet = realloc(gc->rememberedSet,\n                                    gc->rememberedCapacity * sizeof(Object*));\n    }\n    gc->rememberedSet[gc->rememberedCount++] = oldObj;\n}\n\n// Write barrier for generational GC\nvoid generationalWriteBarrier(GenerationalGC* gc, Object* parent, Object* child) {\n    if (parent->isOld && !child->isOld) {\n        addToRememberedSet(gc, parent);\n    }\n}\n\nvoid minorCollection(GenerationalGC* gc) {\n    // Roots: stack + remembered set\n    // Copy live young objects to either:\n    // 1. Survivor space (if young)\n    // 2. Old generation (if survived enough)\n    \n    char* toSpace = allocateToSpace();\n    char* toPtr = toSpace;\n    \n    // Process roots\n    for (int i = 0; i < vm->stackSize; i++) {\n        if (!vm->stack[i]->isOld) {\n            vm->stack[i] = copyObject(vm->stack[i], &toPtr, gc);\n        }\n    }\n    \n    // Process remembered set\n    for (int i = 0; i < gc->rememberedCount; i++) {\n        Object* old = gc->rememberedSet[i];\n        // Scan old object's children\n        scanChildren(old, &toPtr, gc);\n    }\n    \n    // Process copied objects (Cheney's algorithm)\n    char* scanPtr = toSpace;\n    while (scanPtr < toPtr) {\n        Object* obj = (Object*)scanPtr;\n        scanChildren(obj, &toPtr, gc);\n        scanPtr += objectSize(obj);\n    }\n    \n    // Swap spaces\n    gc->nurseryAlloc = gc->nurseryStart;\n    gc->rememberedCount = 0;\n    gc->youngCollections++;\n}\n\nObject* copyObject(Object* obj, char** toPtr, GenerationalGC* gc) {\n    if (obj->forwarding) return obj->forwarding;  // Already copied\n    \n    obj->age++;\n    \n    if (obj->age >= gc->promotionAge) {\n        // Promote to old generation\n        Object* copy = allocOld(gc, objectSize(obj));\n        memcpy(copy, obj, objectSize(obj));\n        copy->isOld = true;\n        obj->forwarding = copy;\n        return copy;\n    } else {\n        // Copy to to-space\n        Object* copy = (Object*)*toPtr;\n        memcpy(copy, obj, objectSize(obj));\n        *toPtr += objectSize(obj);\n        obj->forwarding = copy;\n        return copy;\n    }\n}\n```"
                        },
                        pitfalls: ["Missing remembered set entries", "Promoting too early/late", "Write barrier overhead"],
                        concepts: ["Generational hypothesis", "Copying collection", "Inter-generational pointers"],
                        estimatedHours: "15-20"
                    },
                    {
                        id: 4,
                        name: "Concurrent Collection",
                        description: "Add concurrent marking for reduced pause times.",
                        criteria: ["Concurrent marking thread", "Handshakes/safepoints", "SATB or incremental update", "Concurrent sweep"],
                        hints: {
                            level1: "GC thread marks concurrently while mutator runs. Need synchronization.",
                            level2: "Safepoints where mutator checks for GC requests. SATB logs overwritten pointers.",
                            level3: "## Concurrent GC\n\n```c\n#include <pthread.h>\n#include <stdatomic.h>\n\ntypedef struct {\n    atomic_bool gcRequested;\n    atomic_bool gcInProgress;\n    \n    // SATB (Snapshot-At-The-Beginning) buffer\n    Object** satbBuffer;\n    atomic_int satbCount;\n    \n    pthread_t gcThread;\n    pthread_mutex_t mutex;\n    pthread_cond_t cond;\n} ConcurrentGC;\n\n// Mutator safepoint - called periodically\nvoid safepoint(ConcurrentGC* gc) {\n    if (atomic_load(&gc->gcRequested)) {\n        // Wait for GC to complete initial marking\n        pthread_mutex_lock(&gc->mutex);\n        while (atomic_load(&gc->gcInProgress)) {\n            pthread_cond_wait(&gc->cond, &gc->mutex);\n        }\n        pthread_mutex_unlock(&gc->mutex);\n    }\n}\n\n// SATB write barrier - log overwritten pointer\nvoid satbWriteBarrier(ConcurrentGC* gc, Object** slot) {\n    if (atomic_load(&gc->gcInProgress)) {\n        Object* old = *slot;\n        if (old != NULL) {\n            int idx = atomic_fetch_add(&gc->satbCount, 1);\n            gc->satbBuffer[idx] = old;  // Log old value\n        }\n    }\n}\n\n// GC thread\nvoid* gcThreadFunc(void* arg) {\n    ConcurrentGC* gc = (ConcurrentGC*)arg;\n    VM* vm = gc->vm;\n    \n    while (1) {\n        // Wait for GC request\n        pthread_mutex_lock(&gc->mutex);\n        while (!atomic_load(&gc->gcRequested)) {\n            pthread_cond_wait(&gc->cond, &gc->mutex);\n        }\n        pthread_mutex_unlock(&gc->mutex);\n        \n        atomic_store(&gc->gcInProgress, true);\n        \n        // Phase 1: Initial mark (stop-the-world, mark roots)\n        stopTheWorld(vm);\n        markRoots(vm, gc);\n        resumeTheWorld(vm);\n        \n        // Phase 2: Concurrent mark\n        while (gc->grayCount > 0) {\n            Object* obj = gc->grayList[--gc->grayCount];\n            scanAndMarkChildren(gc, obj);\n        }\n        \n        // Phase 3: Remark (stop-the-world, process SATB buffer)\n        stopTheWorld(vm);\n        processSATBBuffer(gc);\n        while (gc->grayCount > 0) {\n            Object* obj = gc->grayList[--gc->grayCount];\n            scanAndMarkChildren(gc, obj);\n        }\n        resumeTheWorld(vm);\n        \n        // Phase 4: Concurrent sweep\n        concurrentSweep(gc);\n        \n        atomic_store(&gc->gcInProgress, false);\n        atomic_store(&gc->gcRequested, false);\n        \n        pthread_mutex_lock(&gc->mutex);\n        pthread_cond_broadcast(&gc->cond);\n        pthread_mutex_unlock(&gc->mutex);\n    }\n}\n\nvoid processSATBBuffer(ConcurrentGC* gc) {\n    int count = atomic_exchange(&gc->satbCount, 0);\n    for (int i = 0; i < count; i++) {\n        makeGray(gc, gc->satbBuffer[i]);\n    }\n}\n```"
                        },
                        pitfalls: ["Data races", "Missing SATB entries", "Long pauses during STW phases"],
                        concepts: ["Concurrent algorithms", "SATB/incremental update", "Safepoints"],
                        estimatedHours: "15-20"
                    }
                ]
            },

            "build-regex": {
                name: "Build Your Own Regex Engine",
                description: "Build a regex engine using NFA/DFA and Thompson's construction.",
                difficulty: "expert",
                estimatedHours: "40-60",
                prerequisites: ["Automata theory", "Graph algorithms", "Recursion", "C/Rust/Go"],
                languages: {
                    recommended: ["C", "Rust", "Go", "Python"],
                    also: ["JavaScript"]
                },
                resources: [
                    { type: "article", name: "Regular Expression Matching", url: "https://swtch.com/~rsc/regexp/regexp1.html" },
                    { type: "book", name: "Introduction to Automata Theory", url: "https://www.amazon.com/Introduction-Automata-Theory-Languages-Computation/dp/0321455363" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Regex Parser",
                        description: "Parse regex pattern into AST.",
                        criteria: ["Literal characters", "Alternation (|)", "Concatenation", "Quantifiers (*, +, ?)", "Character classes ([a-z])", "Escape sequences"],
                        hints: {
                            level1: "Use recursive descent. Handle operator precedence: () > * > concat > |",
                            level2: "Build AST nodes for each regex construct.",
                            level3: "## Regex Parser\n\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\nclass Literal:\n    char: str\n\n@dataclass\nclass Concat:\n    left: 'Node'\n    right: 'Node'\n\n@dataclass\nclass Alternation:\n    left: 'Node'\n    right: 'Node'\n\n@dataclass\nclass Star:\n    child: 'Node'\n\n@dataclass\nclass Plus:\n    child: 'Node'\n\n@dataclass\nclass Optional:\n    child: 'Node'\n\n@dataclass\nclass CharClass:\n    chars: set\n    negated: bool = False\n\nNode = Literal | Concat | Alternation | Star | Plus | Optional | CharClass\n\nclass RegexParser:\n    def __init__(self, pattern: str):\n        self.pattern = pattern\n        self.pos = 0\n    \n    def parse(self) -> Node:\n        return self.parseAlternation()\n    \n    def parseAlternation(self) -> Node:\n        left = self.parseConcat()\n        while self.match('|'):\n            right = self.parseConcat()\n            left = Alternation(left, right)\n        return left\n    \n    def parseConcat(self) -> Node:\n        nodes = []\n        while self.pos < len(self.pattern) and self.peek() not in '|)':\n            nodes.append(self.parseQuantified())\n        \n        if not nodes:\n            return Literal('')\n        \n        result = nodes[0]\n        for node in nodes[1:]:\n            result = Concat(result, node)\n        return result\n    \n    def parseQuantified(self) -> Node:\n        base = self.parseAtom()\n        \n        if self.match('*'):\n            return Star(base)\n        elif self.match('+'):\n            return Plus(base)\n        elif self.match('?'):\n            return Optional(base)\n        \n        return base\n    \n    def parseAtom(self) -> Node:\n        if self.match('('):\n            node = self.parseAlternation()\n            self.expect(')')\n            return node\n        \n        if self.match('['):\n            return self.parseCharClass()\n        \n        if self.match('\\\\'):\n            return self.parseEscape()\n        \n        if self.match('.'):\n            return CharClass(set(), negated=True)  # Match any\n        \n        char = self.advance()\n        return Literal(char)\n    \n    def parseCharClass(self) -> Node:\n        negated = self.match('^')\n        chars = set()\n        \n        while not self.match(']'):\n            c = self.advance()\n            if self.match('-') and self.peek() != ']':\n                end = self.advance()\n                for i in range(ord(c), ord(end) + 1):\n                    chars.add(chr(i))\n            else:\n                chars.add(c)\n        \n        return CharClass(chars, negated)\n    \n    # Helper methods\n    def peek(self) -> str:\n        return self.pattern[self.pos] if self.pos < len(self.pattern) else ''\n    \n    def advance(self) -> str:\n        c = self.peek()\n        self.pos += 1\n        return c\n    \n    def match(self, c: str) -> bool:\n        if self.peek() == c:\n            self.pos += 1\n            return True\n        return False\n```"
                        },
                        pitfalls: ["Operator precedence wrong", "Escape handling", "Empty alternation branches"],
                        concepts: ["Parsing", "AST construction", "Regex syntax"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 2,
                        name: "Thompson's Construction (NFA)",
                        description: "Convert regex AST to NFA using Thompson's construction.",
                        criteria: ["NFA state representation", "Epsilon transitions", "Construction for each regex operator", "Connect sub-NFAs"],
                        hints: {
                            level1: "Each regex construct becomes a small NFA fragment with start/accept states.",
                            level2: "Concatenation: connect first's accept to second's start. Alternation: split with epsilon.",
                            level3: "## Thompson's Construction\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Set, Dict, List\n\n@dataclass\nclass NFAState:\n    id: int\n    epsilon: List['NFAState'] = field(default_factory=list)\n    transitions: Dict[str, List['NFAState']] = field(default_factory=dict)\n    is_accept: bool = False\n\nclass NFAFragment:\n    def __init__(self, start: NFAState, accept: NFAState):\n        self.start = start\n        self.accept = accept\n\nclass NFABuilder:\n    def __init__(self):\n        self.state_count = 0\n    \n    def new_state(self) -> NFAState:\n        state = NFAState(id=self.state_count)\n        self.state_count += 1\n        return state\n    \n    def build(self, node: Node) -> NFAFragment:\n        if isinstance(node, Literal):\n            start = self.new_state()\n            accept = self.new_state()\n            if node.char:\n                start.transitions[node.char] = [accept]\n            else:\n                start.epsilon.append(accept)\n            return NFAFragment(start, accept)\n        \n        elif isinstance(node, Concat):\n            left = self.build(node.left)\n            right = self.build(node.right)\n            left.accept.epsilon.append(right.start)\n            return NFAFragment(left.start, right.accept)\n        \n        elif isinstance(node, Alternation):\n            left = self.build(node.left)\n            right = self.build(node.right)\n            start = self.new_state()\n            accept = self.new_state()\n            start.epsilon.extend([left.start, right.start])\n            left.accept.epsilon.append(accept)\n            right.accept.epsilon.append(accept)\n            return NFAFragment(start, accept)\n        \n        elif isinstance(node, Star):\n            child = self.build(node.child)\n            start = self.new_state()\n            accept = self.new_state()\n            start.epsilon.extend([child.start, accept])\n            child.accept.epsilon.extend([child.start, accept])\n            return NFAFragment(start, accept)\n        \n        elif isinstance(node, Plus):\n            child = self.build(node.child)\n            start = self.new_state()\n            accept = self.new_state()\n            start.epsilon.append(child.start)\n            child.accept.epsilon.extend([child.start, accept])\n            return NFAFragment(start, accept)\n        \n        elif isinstance(node, Optional):\n            child = self.build(node.child)\n            start = self.new_state()\n            accept = self.new_state()\n            start.epsilon.extend([child.start, accept])\n            child.accept.epsilon.append(accept)\n            return NFAFragment(start, accept)\n        \n        elif isinstance(node, CharClass):\n            start = self.new_state()\n            accept = self.new_state()\n            # Add transitions for each char in class\n            for c in node.chars:\n                start.transitions.setdefault(c, []).append(accept)\n            return NFAFragment(start, accept)\n\ndef compile_regex(pattern: str) -> NFAFragment:\n    parser = RegexParser(pattern)\n    ast = parser.parse()\n    builder = NFABuilder()\n    nfa = builder.build(ast)\n    nfa.accept.is_accept = True\n    return nfa\n```"
                        },
                        pitfalls: ["Wrong epsilon connections", "Not marking accept state", "Memory leaks from circular refs"],
                        concepts: ["NFAs", "Thompson's construction", "Epsilon transitions"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 3,
                        name: "NFA Simulation",
                        description: "Match strings using NFA simulation.",
                        criteria: ["Epsilon closure computation", "Simultaneous state tracking", "Match detection", "Greedy vs non-greedy"],
                        hints: {
                            level1: "Track set of current states. For each input char, compute next states.",
                            level2: "Epsilon closure: follow all epsilon transitions from a state set.",
                            level3: "## NFA Simulation\n\n```python\ndef epsilon_closure(states: Set[NFAState]) -> Set[NFAState]:\n    \"\"\"Find all states reachable via epsilon transitions.\"\"\"\n    closure = set(states)\n    stack = list(states)\n    \n    while stack:\n        state = stack.pop()\n        for next_state in state.epsilon:\n            if next_state not in closure:\n                closure.add(next_state)\n                stack.append(next_state)\n    \n    return closure\n\ndef move(states: Set[NFAState], char: str) -> Set[NFAState]:\n    \"\"\"Find states reachable via the given character.\"\"\"\n    next_states = set()\n    for state in states:\n        if char in state.transitions:\n            next_states.update(state.transitions[char])\n    return next_states\n\ndef nfa_match(nfa: NFAFragment, text: str) -> bool:\n    \"\"\"Check if NFA matches the entire text.\"\"\"\n    current = epsilon_closure({nfa.start})\n    \n    for char in text:\n        current = epsilon_closure(move(current, char))\n        if not current:\n            return False\n    \n    return any(state.is_accept for state in current)\n\ndef nfa_search(nfa: NFAFragment, text: str) -> Optional[tuple]:\n    \"\"\"Find first match in text, return (start, end) indices.\"\"\"\n    for start in range(len(text)):\n        current = epsilon_closure({nfa.start})\n        \n        for end in range(start, len(text) + 1):\n            if any(state.is_accept for state in current):\n                return (start, end)\n            \n            if end < len(text):\n                current = epsilon_closure(move(current, text[end]))\n                if not current:\n                    break\n    \n    return None\n\ndef nfa_findall(nfa: NFAFragment, text: str) -> List[str]:\n    \"\"\"Find all non-overlapping matches.\"\"\"\n    matches = []\n    pos = 0\n    \n    while pos < len(text):\n        match = nfa_search_from(nfa, text, pos)\n        if match:\n            start, end = match\n            matches.append(text[start:end])\n            pos = end if end > pos else pos + 1\n        else:\n            pos += 1\n    \n    return matches\n```"
                        },
                        pitfalls: ["Forgetting epsilon closure", "Infinite loop on empty match", "Wrong match boundaries"],
                        concepts: ["NFA simulation", "State sets", "Pattern matching"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 4,
                        name: "DFA Conversion & Optimization",
                        description: "Convert NFA to DFA for faster matching.",
                        criteria: ["Subset construction", "DFA state minimization", "Lazy DFA construction", "Caching compiled patterns"],
                        hints: {
                            level1: "Each DFA state = set of NFA states. Build on demand (lazy).",
                            level2: "Minimize DFA by merging equivalent states (Hopcroft's algorithm).",
                            level3: "## NFA to DFA Conversion\n\n```python\nclass DFAState:\n    def __init__(self, nfa_states: frozenset):\n        self.nfa_states = nfa_states\n        self.transitions: Dict[str, 'DFAState'] = {}\n        self.is_accept = any(s.is_accept for s in nfa_states)\n\nclass DFA:\n    def __init__(self, nfa: NFAFragment):\n        self.start_nfa_states = frozenset(epsilon_closure({nfa.start}))\n        self.states: Dict[frozenset, DFAState] = {}\n        self.start = self.get_or_create_state(self.start_nfa_states)\n    \n    def get_or_create_state(self, nfa_states: frozenset) -> DFAState:\n        if nfa_states not in self.states:\n            self.states[nfa_states] = DFAState(nfa_states)\n        return self.states[nfa_states]\n    \n    def get_transition(self, state: DFAState, char: str) -> Optional[DFAState]:\n        if char in state.transitions:\n            return state.transitions[char]\n        \n        # Lazy construction\n        next_nfa_states = frozenset(epsilon_closure(move(state.nfa_states, char)))\n        if not next_nfa_states:\n            return None\n        \n        next_state = self.get_or_create_state(next_nfa_states)\n        state.transitions[char] = next_state\n        return next_state\n    \n    def match(self, text: str) -> bool:\n        current = self.start\n        for char in text:\n            current = self.get_transition(current, char)\n            if current is None:\n                return False\n        return current.is_accept\n\n# DFA Minimization (Hopcroft's algorithm)\ndef minimize_dfa(dfa: DFA) -> DFA:\n    # Partition states into accept and non-accept\n    accept = {s for s in dfa.states.values() if s.is_accept}\n    non_accept = {s for s in dfa.states.values() if not s.is_accept}\n    \n    partitions = [accept, non_accept] if non_accept else [accept]\n    worklist = list(partitions)\n    \n    alphabet = set()\n    for state in dfa.states.values():\n        alphabet.update(state.transitions.keys())\n    \n    while worklist:\n        A = worklist.pop()\n        for c in alphabet:\n            # States that transition to A on c\n            X = {s for s in dfa.states.values() \n                 if s.transitions.get(c) in A}\n            \n            new_partitions = []\n            for Y in partitions:\n                intersection = Y & X\n                difference = Y - X\n                \n                if intersection and difference:\n                    new_partitions.extend([intersection, difference])\n                    if Y in worklist:\n                        worklist.remove(Y)\n                        worklist.extend([intersection, difference])\n                    else:\n                        worklist.append(min(intersection, difference, key=len))\n                else:\n                    new_partitions.append(Y)\n            \n            partitions = new_partitions\n    \n    # Build minimized DFA from partitions\n    # ...\n```"
                        },
                        pitfalls: ["Exponential state blowup", "Not handling dead states", "Alphabet enumeration"],
                        concepts: ["Subset construction", "DFA minimization", "Lazy evaluation"],
                        estimatedHours: "12-18"
                    }
                ]
            },

            // SPECIALIZED - EXPERT
            "build-bittorrent": {
                name: "Build Your Own BitTorrent",
                description: "Build a BitTorrent client with P2P file sharing.",
                difficulty: "expert",
                estimatedHours: "50-80",
                prerequisites: ["Networking (TCP/UDP)", "Concurrency", "File I/O", "Bencode format"],
                languages: {
                    recommended: ["Go", "Rust", "Python"],
                    also: ["JavaScript", "Java"]
                },
                resources: [
                    { type: "specification", name: "BitTorrent Protocol", url: "https://www.bittorrent.org/beps/bep_0003.html" },
                    { type: "tutorial", name: "Building a BitTorrent Client", url: "https://blog.jse.li/posts/torrent/" },
                    { type: "interactive", name: "CodeCrafters BitTorrent", url: "https://app.codecrafters.io/courses/bittorrent/overview" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Torrent File Parsing",
                        description: "Parse .torrent files and extract metadata.",
                        criteria: ["Bencode decoder", "Extract announce URL", "Extract file info", "Calculate info hash"],
                        hints: {
                            level1: "Bencode: i42e=int, 4:spam=string, l...e=list, d...e=dict",
                            level2: "Info hash = SHA1 of bencoded info dict. Used to identify torrent.",
                            level3: "## Torrent Parsing\n\n```python\nimport hashlib\n\ndef decode_bencode(data: bytes, pos=0):\n    if data[pos:pos+1] == b'i':  # Integer\n        end = data.index(b'e', pos)\n        return int(data[pos+1:end]), end + 1\n    \n    elif data[pos:pos+1] == b'l':  # List\n        result = []\n        pos += 1\n        while data[pos:pos+1] != b'e':\n            item, pos = decode_bencode(data, pos)\n            result.append(item)\n        return result, pos + 1\n    \n    elif data[pos:pos+1] == b'd':  # Dict\n        result = {}\n        pos += 1\n        while data[pos:pos+1] != b'e':\n            key, pos = decode_bencode(data, pos)\n            value, pos = decode_bencode(data, pos)\n            result[key] = value\n        return result, pos + 1\n    \n    else:  # String (length:content)\n        colon = data.index(b':', pos)\n        length = int(data[pos:colon])\n        start = colon + 1\n        return data[start:start+length], start + length\n\nclass Torrent:\n    def __init__(self, filepath):\n        with open(filepath, 'rb') as f:\n            data = f.read()\n        \n        self.meta, _ = decode_bencode(data)\n        self.announce = self.meta[b'announce'].decode()\n        \n        info = self.meta[b'info']\n        self.name = info[b'name'].decode()\n        self.piece_length = info[b'piece length']\n        self.pieces = info[b'pieces']  # Concatenated SHA1 hashes\n        \n        if b'files' in info:  # Multi-file torrent\n            self.files = [\n                {'path': '/'.join(p.decode() for p in f[b'path']),\n                 'length': f[b'length']}\n                for f in info[b'files']\n            ]\n            self.total_length = sum(f['length'] for f in self.files)\n        else:  # Single file\n            self.files = [{'path': self.name, 'length': info[b'length']}]\n            self.total_length = info[b'length']\n        \n        # Calculate info hash\n        info_start = data.index(b'4:info') + 6\n        info_end = len(data) - 1  # Before final 'e'\n        self.info_hash = hashlib.sha1(data[info_start:info_end]).digest()\n    \n    @property\n    def num_pieces(self):\n        return len(self.pieces) // 20\n    \n    def piece_hash(self, index):\n        return self.pieces[index*20:(index+1)*20]\n```"
                        },
                        pitfalls: ["Bencode parsing edge cases", "Wrong info dict boundaries", "Binary vs text strings"],
                        concepts: ["Bencode format", "Hashing", "Torrent metadata"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 2,
                        name: "Tracker Communication",
                        description: "Communicate with tracker to get peer list.",
                        criteria: ["HTTP tracker protocol", "Announce request", "Parse peer list", "Periodic re-announce"],
                        hints: {
                            level1: "GET request with info_hash, peer_id, port, uploaded, downloaded, left.",
                            level2: "Response is bencoded dict with 'peers' (compact or dict format).",
                            level3: "## Tracker Client\n\n```python\nimport requests\nimport struct\nimport random\nimport string\n\nclass TrackerClient:\n    def __init__(self, torrent):\n        self.torrent = torrent\n        self.peer_id = self.generate_peer_id()\n        self.port = 6881\n        self.uploaded = 0\n        self.downloaded = 0\n    \n    def generate_peer_id(self):\n        # Format: -XX0000-xxxxxxxxxxxx\n        return f\"-PY0001-{''.join(random.choices(string.digits, k=12))}\".encode()\n    \n    def announce(self, event=None):\n        params = {\n            'info_hash': self.torrent.info_hash,\n            'peer_id': self.peer_id,\n            'port': self.port,\n            'uploaded': self.uploaded,\n            'downloaded': self.downloaded,\n            'left': self.torrent.total_length - self.downloaded,\n            'compact': 1,\n        }\n        if event:\n            params['event'] = event\n        \n        response = requests.get(self.torrent.announce, params=params)\n        return self.parse_response(response.content)\n    \n    def parse_response(self, data):\n        response, _ = decode_bencode(data)\n        \n        if b'failure reason' in response:\n            raise Exception(response[b'failure reason'].decode())\n        \n        interval = response.get(b'interval', 1800)\n        peers = self.parse_peers(response[b'peers'])\n        \n        return {'interval': interval, 'peers': peers}\n    \n    def parse_peers(self, peers_data):\n        if isinstance(peers_data, list):  # Dictionary format\n            return [\n                (p[b'ip'].decode(), p[b'port'])\n                for p in peers_data\n            ]\n        else:  # Compact format: 6 bytes per peer (4 IP + 2 port)\n            peers = []\n            for i in range(0, len(peers_data), 6):\n                ip = '.'.join(str(b) for b in peers_data[i:i+4])\n                port = struct.unpack('>H', peers_data[i+4:i+6])[0]\n                peers.append((ip, port))\n            return peers\n```"
                        },
                        pitfalls: ["URL encoding info_hash", "Compact vs dict peer format", "Handling tracker errors"],
                        concepts: ["HTTP protocol", "Tracker protocol", "Peer discovery"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 3,
                        name: "Peer Protocol",
                        description: "Implement BitTorrent peer wire protocol.",
                        criteria: ["Handshake", "Message framing", "Bitfield exchange", "Request/piece messages", "Choking/unchoking"],
                        hints: {
                            level1: "Handshake: pstrlen + pstr + reserved + info_hash + peer_id",
                            level2: "Messages: 4-byte length + 1-byte type + payload",
                            level3: "## Peer Protocol\n\n```python\nimport struct\nimport socket\n\nclass PeerConnection:\n    CHOKE = 0\n    UNCHOKE = 1\n    INTERESTED = 2\n    NOT_INTERESTED = 3\n    HAVE = 4\n    BITFIELD = 5\n    REQUEST = 6\n    PIECE = 7\n    CANCEL = 8\n    \n    def __init__(self, ip, port, info_hash, peer_id):\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.connect((ip, port))\n        self.info_hash = info_hash\n        self.peer_id = peer_id\n        self.bitfield = None\n        self.choked = True\n        self.interested = False\n    \n    def handshake(self):\n        pstr = b'BitTorrent protocol'\n        handshake = bytes([len(pstr)]) + pstr + bytes(8) + self.info_hash + self.peer_id\n        self.socket.send(handshake)\n        \n        response = self.socket.recv(68)\n        if response[28:48] != self.info_hash:\n            raise Exception(\"Info hash mismatch\")\n        \n        return response[48:68]  # Peer's peer_id\n    \n    def recv_message(self):\n        length_bytes = self.socket.recv(4)\n        if not length_bytes:\n            return None\n        \n        length = struct.unpack('>I', length_bytes)[0]\n        if length == 0:  # Keep-alive\n            return ('keep-alive', None)\n        \n        msg_id = self.socket.recv(1)[0]\n        payload = self.socket.recv(length - 1) if length > 1 else b''\n        \n        return (msg_id, payload)\n    \n    def send_interested(self):\n        self.socket.send(struct.pack('>IB', 1, self.INTERESTED))\n        self.interested = True\n    \n    def send_request(self, index, begin, length):\n        payload = struct.pack('>III', index, begin, length)\n        self.socket.send(struct.pack('>IB', 13, self.REQUEST) + payload)\n    \n    def parse_bitfield(self, payload):\n        self.bitfield = []\n        for byte in payload:\n            for i in range(8):\n                self.bitfield.append(bool(byte & (128 >> i)))\n    \n    def parse_piece(self, payload):\n        index = struct.unpack('>I', payload[:4])[0]\n        begin = struct.unpack('>I', payload[4:8])[0]\n        data = payload[8:]\n        return index, begin, data\n    \n    def download_piece(self, piece_index, piece_length):\n        BLOCK_SIZE = 16384  # 16 KB\n        data = bytearray(piece_length)\n        \n        # Send all requests\n        for offset in range(0, piece_length, BLOCK_SIZE):\n            length = min(BLOCK_SIZE, piece_length - offset)\n            self.send_request(piece_index, offset, length)\n        \n        # Receive all blocks\n        received = 0\n        while received < piece_length:\n            msg_id, payload = self.recv_message()\n            if msg_id == self.PIECE:\n                idx, begin, block = self.parse_piece(payload)\n                if idx == piece_index:\n                    data[begin:begin+len(block)] = block\n                    received += len(block)\n        \n        return bytes(data)\n```"
                        },
                        pitfalls: ["Endianness", "Partial message reads", "Blocking on choked peer"],
                        concepts: ["Wire protocol", "Message framing", "State machines"],
                        estimatedHours: "15-20"
                    },
                    {
                        id: 4,
                        name: "Piece Management & Seeding",
                        description: "Manage pieces, verify hashes, and seed to other peers.",
                        criteria: ["Piece verification", "Rarest-first selection", "File assembly", "Serving requests", "Multiple peer connections"],
                        hints: {
                            level1: "Verify each piece's SHA1 hash. Rarest-first: download pieces others have least.",
                            level2: "Run multiple peer connections concurrently. Balance downloading vs uploading.",
                            level3: "## Piece Manager\n\n```python\nimport asyncio\nimport hashlib\nfrom collections import defaultdict\n\nclass PieceManager:\n    def __init__(self, torrent):\n        self.torrent = torrent\n        self.have = [False] * torrent.num_pieces\n        self.pending = set()  # Pieces being downloaded\n        self.piece_availability = defaultdict(int)  # piece -> count of peers\n    \n    def update_availability(self, peer_bitfield):\n        for i, has in enumerate(peer_bitfield):\n            if has:\n                self.piece_availability[i] += 1\n    \n    def select_piece(self, peer_bitfield):\n        # Rarest first\n        candidates = [\n            (self.piece_availability[i], i)\n            for i in range(self.torrent.num_pieces)\n            if peer_bitfield[i] and not self.have[i] and i not in self.pending\n        ]\n        if not candidates:\n            return None\n        \n        candidates.sort()  # Sort by availability\n        piece_index = candidates[0][1]\n        self.pending.add(piece_index)\n        return piece_index\n    \n    def verify_piece(self, index, data):\n        expected_hash = self.torrent.piece_hash(index)\n        actual_hash = hashlib.sha1(data).digest()\n        return expected_hash == actual_hash\n    \n    def piece_received(self, index, data):\n        if self.verify_piece(index, data):\n            self.have[index] = True\n            self.pending.discard(index)\n            self.write_piece(index, data)\n            return True\n        else:\n            self.pending.discard(index)\n            return False\n    \n    def write_piece(self, index, data):\n        offset = index * self.torrent.piece_length\n        # Handle multi-file torrents...\n        with open(self.torrent.name, 'r+b') as f:\n            f.seek(offset)\n            f.write(data)\n\nclass BitTorrentClient:\n    def __init__(self, torrent_path):\n        self.torrent = Torrent(torrent_path)\n        self.tracker = TrackerClient(self.torrent)\n        self.piece_manager = PieceManager(self.torrent)\n        self.peers = []\n    \n    async def download(self):\n        # Get peers from tracker\n        response = self.tracker.announce('started')\n        \n        # Connect to peers\n        tasks = []\n        for ip, port in response['peers'][:50]:  # Limit connections\n            task = asyncio.create_task(self.connect_peer(ip, port))\n            tasks.append(task)\n        \n        await asyncio.gather(*tasks, return_exceptions=True)\n    \n    async def connect_peer(self, ip, port):\n        try:\n            peer = PeerConnection(ip, port, self.torrent.info_hash, \n                                  self.tracker.peer_id)\n            peer.handshake()\n            \n            # Get bitfield\n            msg_id, payload = peer.recv_message()\n            if msg_id == PeerConnection.BITFIELD:\n                peer.parse_bitfield(payload)\n                self.piece_manager.update_availability(peer.bitfield)\n            \n            peer.send_interested()\n            \n            # Wait for unchoke\n            while peer.choked:\n                msg_id, _ = peer.recv_message()\n                if msg_id == PeerConnection.UNCHOKE:\n                    peer.choked = False\n            \n            # Download pieces\n            while not all(self.piece_manager.have):\n                piece_index = self.piece_manager.select_piece(peer.bitfield)\n                if piece_index is None:\n                    break\n                \n                piece_length = self.torrent.piece_length\n                if piece_index == self.torrent.num_pieces - 1:\n                    piece_length = self.torrent.total_length % self.torrent.piece_length\n                \n                data = peer.download_piece(piece_index, piece_length)\n                self.piece_manager.piece_received(piece_index, data)\n                \n        except Exception as e:\n            print(f\"Peer {ip}:{port} error: {e}\")\n```"
                        },
                        pitfalls: ["Hash verification failures", "Race conditions in piece selection", "Connection management"],
                        concepts: ["Content verification", "Scheduling algorithms", "Concurrent downloads"],
                        estimatedHours: "15-20"
                    }
                ]
            },

            "build-dns": {
                name: "Build Your Own DNS Server",
                description: "Build a DNS server with recursive resolution.",
                difficulty: "expert",
                estimatedHours: "40-60",
                prerequisites: ["UDP networking", "DNS protocol basics", "Caching strategies"],
                languages: {
                    recommended: ["Go", "Rust", "C"],
                    also: ["Python", "JavaScript"]
                },
                resources: [
                    { type: "rfc", name: "RFC 1035 - DNS", url: "https://tools.ietf.org/html/rfc1035" },
                    { type: "interactive", name: "CodeCrafters DNS", url: "https://app.codecrafters.io/courses/dns-server/overview" },
                    { type: "article", name: "How DNS Works", url: "https://howdns.works/" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "DNS Message Parsing",
                        description: "Parse and construct DNS messages.",
                        criteria: ["Header parsing", "Question section", "Answer section", "Name compression", "Message construction"],
                        hints: {
                            level1: "DNS message: header (12 bytes) + question + answer + authority + additional",
                            level2: "Name compression uses pointers (0xC0 prefix) to reduce message size.",
                            level3: "## DNS Message Parsing\n\n```go\npackage dns\n\nimport (\n    \"bytes\"\n    \"encoding/binary\"\n)\n\ntype Header struct {\n    ID      uint16\n    Flags   uint16\n    QDCount uint16  // Question count\n    ANCount uint16  // Answer count\n    NSCount uint16  // Authority count\n    ARCount uint16  // Additional count\n}\n\ntype Question struct {\n    Name  string\n    Type  uint16\n    Class uint16\n}\n\ntype ResourceRecord struct {\n    Name     string\n    Type     uint16\n    Class    uint16\n    TTL      uint32\n    RDLength uint16\n    RData    []byte\n}\n\ntype Message struct {\n    Header    Header\n    Questions []Question\n    Answers   []ResourceRecord\n    Authority []ResourceRecord\n    Additional []ResourceRecord\n}\n\nfunc ParseMessage(data []byte) (*Message, error) {\n    reader := bytes.NewReader(data)\n    msg := &Message{}\n    \n    // Parse header\n    binary.Read(reader, binary.BigEndian, &msg.Header)\n    \n    // Parse questions\n    for i := 0; i < int(msg.Header.QDCount); i++ {\n        q := Question{}\n        q.Name = parseName(data, reader)\n        binary.Read(reader, binary.BigEndian, &q.Type)\n        binary.Read(reader, binary.BigEndian, &q.Class)\n        msg.Questions = append(msg.Questions, q)\n    }\n    \n    // Parse answers\n    for i := 0; i < int(msg.Header.ANCount); i++ {\n        rr := parseRR(data, reader)\n        msg.Answers = append(msg.Answers, rr)\n    }\n    \n    return msg, nil\n}\n\nfunc parseName(data []byte, reader *bytes.Reader) string {\n    var name []string\n    for {\n        length, _ := reader.ReadByte()\n        \n        if length == 0 {\n            break\n        }\n        \n        // Check for compression pointer\n        if length&0xC0 == 0xC0 {\n            nextByte, _ := reader.ReadByte()\n            offset := int(length&0x3F)<<8 | int(nextByte)\n            // Follow pointer\n            ptrReader := bytes.NewReader(data[offset:])\n            name = append(name, parseName(data, ptrReader))\n            break\n        }\n        \n        label := make([]byte, length)\n        reader.Read(label)\n        name = append(name, string(label))\n    }\n    return strings.Join(name, \".\")\n}\n\nfunc (m *Message) Serialize() []byte {\n    buf := new(bytes.Buffer)\n    \n    // Write header\n    binary.Write(buf, binary.BigEndian, m.Header)\n    \n    // Write questions\n    for _, q := range m.Questions {\n        writeName(buf, q.Name)\n        binary.Write(buf, binary.BigEndian, q.Type)\n        binary.Write(buf, binary.BigEndian, q.Class)\n    }\n    \n    // Write answers\n    for _, rr := range m.Answers {\n        writeRR(buf, rr)\n    }\n    \n    return buf.Bytes()\n}\n\nfunc writeName(buf *bytes.Buffer, name string) {\n    parts := strings.Split(name, \".\")\n    for _, part := range parts {\n        buf.WriteByte(byte(len(part)))\n        buf.WriteString(part)\n    }\n    buf.WriteByte(0)  // Null terminator\n}\n```"
                        },
                        pitfalls: ["Compression pointer loops", "Wrong byte order", "Name not null-terminated"],
                        concepts: ["Binary protocols", "Name compression", "Message format"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "Authoritative Server",
                        description: "Respond to queries from local zone data.",
                        criteria: ["Zone file parsing", "Query matching", "SOA records", "NS records", "A/AAAA records"],
                        hints: {
                            level1: "Load zone data from file. Match queries against records.",
                            level2: "Return NXDOMAIN for non-existent names. Set authority flag.",
                            level3: "## Authoritative Server\n\n```go\ntype Zone struct {\n    Origin  string\n    TTL     uint32\n    Records map[string][]ResourceRecord\n}\n\nfunc LoadZone(filename string) (*Zone, error) {\n    zone := &Zone{\n        Records: make(map[string][]ResourceRecord),\n    }\n    \n    // Parse zone file (simplified)\n    file, _ := os.Open(filename)\n    scanner := bufio.NewScanner(file)\n    \n    for scanner.Scan() {\n        line := scanner.Text()\n        if strings.HasPrefix(line, \";\") || line == \"\" {\n            continue\n        }\n        \n        parts := strings.Fields(line)\n        name := parts[0]\n        if name == \"@\" {\n            name = zone.Origin\n        }\n        \n        rr := ResourceRecord{\n            Name:  name,\n            Class: 1,  // IN\n        }\n        \n        // Parse type and data\n        switch parts[len(parts)-2] {\n        case \"A\":\n            rr.Type = 1\n            rr.RData = net.ParseIP(parts[len(parts)-1]).To4()\n        case \"AAAA\":\n            rr.Type = 28\n            rr.RData = net.ParseIP(parts[len(parts)-1]).To16()\n        case \"NS\":\n            rr.Type = 2\n            // Encode name\n        case \"MX\":\n            rr.Type = 15\n            // Encode preference + exchange\n        }\n        \n        zone.Records[name] = append(zone.Records[name], rr)\n    }\n    \n    return zone, nil\n}\n\ntype AuthServer struct {\n    zones map[string]*Zone\n}\n\nfunc (s *AuthServer) HandleQuery(query *Message) *Message {\n    response := &Message{\n        Header: Header{\n            ID:      query.Header.ID,\n            Flags:   0x8400,  // Response + Authoritative\n            QDCount: query.Header.QDCount,\n        },\n        Questions: query.Questions,\n    }\n    \n    for _, q := range query.Questions {\n        // Find matching zone\n        zone := s.findZone(q.Name)\n        if zone == nil {\n            response.Header.Flags |= 0x0003  // NXDOMAIN\n            continue\n        }\n        \n        // Find matching records\n        records := zone.Records[q.Name]\n        for _, rr := range records {\n            if rr.Type == q.Type || q.Type == 255 {  // 255 = ANY\n                response.Answers = append(response.Answers, rr)\n            }\n        }\n    }\n    \n    response.Header.ANCount = uint16(len(response.Answers))\n    return response\n}\n```"
                        },
                        pitfalls: ["Wildcard matching", "CNAME chasing", "Case insensitivity"],
                        concepts: ["Zone files", "Record types", "Authoritative responses"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 3,
                        name: "Recursive Resolver",
                        description: "Implement recursive resolution from root servers.",
                        criteria: ["Iterative queries to authoritative servers", "Following referrals", "Root hints", "Glue records"],
                        hints: {
                            level1: "Start from root servers, follow NS referrals until authoritative answer.",
                            level2: "Glue records provide IP addresses for nameservers in referrals.",
                            level3: "## Recursive Resolver\n\n```go\nvar RootServers = []string{\n    \"198.41.0.4\",   // a.root-servers.net\n    \"199.9.14.201\", // b.root-servers.net\n    // ... more root servers\n}\n\ntype Resolver struct {\n    cache *Cache\n}\n\nfunc (r *Resolver) Resolve(name string, qtype uint16) ([]ResourceRecord, error) {\n    // Check cache first\n    if cached := r.cache.Get(name, qtype); cached != nil {\n        return cached, nil\n    }\n    \n    // Start from root\n    nameservers := RootServers\n    \n    for {\n        // Query one of the nameservers\n        response, err := r.queryNS(nameservers[0], name, qtype)\n        if err != nil {\n            // Try next nameserver\n            nameservers = nameservers[1:]\n            continue\n        }\n        \n        // Got authoritative answer\n        if response.Header.Flags&0x0400 != 0 {  // AA flag\n            r.cache.Set(name, qtype, response.Answers)\n            return response.Answers, nil\n        }\n        \n        // Got referral - follow it\n        if len(response.Authority) > 0 {\n            nameservers = r.extractNS(response)\n            continue\n        }\n        \n        // No answer, no referral\n        return nil, fmt.Errorf(\"resolution failed\")\n    }\n}\n\nfunc (r *Resolver) queryNS(server, name string, qtype uint16) (*Message, error) {\n    query := &Message{\n        Header: Header{\n            ID:      uint16(rand.Int()),\n            Flags:   0x0100,  // RD (Recursion Desired) = 0 for iterative\n            QDCount: 1,\n        },\n        Questions: []Question{{\n            Name:  name,\n            Type:  qtype,\n            Class: 1,\n        }},\n    }\n    \n    conn, _ := net.Dial(\"udp\", server+\":53\")\n    defer conn.Close()\n    \n    conn.Write(query.Serialize())\n    \n    buf := make([]byte, 512)\n    n, _ := conn.Read(buf)\n    \n    return ParseMessage(buf[:n])\n}\n\nfunc (r *Resolver) extractNS(response *Message) []string {\n    var servers []string\n    \n    // Get NS names from authority section\n    nsNames := make(map[string]bool)\n    for _, rr := range response.Authority {\n        if rr.Type == 2 {  // NS\n            nsNames[string(rr.RData)] = true\n        }\n    }\n    \n    // Look for glue records (A records for NS)\n    for _, rr := range response.Additional {\n        if rr.Type == 1 {  // A record\n            if nsNames[rr.Name] {\n                ip := net.IP(rr.RData)\n                servers = append(servers, ip.String())\n            }\n        }\n    }\n    \n    // If no glue, need to resolve NS names separately\n    if len(servers) == 0 {\n        for name := range nsNames {\n            addrs, _ := r.Resolve(name, 1)  // Resolve A record\n            for _, rr := range addrs {\n                servers = append(servers, net.IP(rr.RData).String())\n            }\n        }\n    }\n    \n    return servers\n}\n```"
                        },
                        pitfalls: ["Infinite referral loops", "Missing glue records", "CNAME handling"],
                        concepts: ["Iterative resolution", "Referrals", "DNS hierarchy"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 4,
                        name: "Caching & Performance",
                        description: "Implement caching with TTL and negative caching.",
                        criteria: ["TTL-based expiration", "Negative caching", "Cache poisoning prevention", "UDP server with concurrency"],
                        hints: {
                            level1: "Cache answers with TTL countdown. Negative cache NXDOMAIN responses.",
                            level2: "Validate response matches query. Use random query IDs.",
                            level3: "## DNS Cache\n\n```go\ntype CacheEntry struct {\n    Records    []ResourceRecord\n    Expiration time.Time\n    Negative   bool  // NXDOMAIN cache\n}\n\ntype Cache struct {\n    mu      sync.RWMutex\n    entries map[string]*CacheEntry  // key: name+type\n}\n\nfunc (c *Cache) makeKey(name string, qtype uint16) string {\n    return fmt.Sprintf(\"%s:%d\", strings.ToLower(name), qtype)\n}\n\nfunc (c *Cache) Get(name string, qtype uint16) []ResourceRecord {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    entry, ok := c.entries[c.makeKey(name, qtype)]\n    if !ok {\n        return nil\n    }\n    \n    if time.Now().After(entry.Expiration) {\n        return nil  // Expired\n    }\n    \n    if entry.Negative {\n        return []ResourceRecord{}  // Negative cache hit\n    }\n    \n    // Adjust TTLs in returned records\n    remaining := uint32(entry.Expiration.Sub(time.Now()).Seconds())\n    records := make([]ResourceRecord, len(entry.Records))\n    for i, rr := range entry.Records {\n        records[i] = rr\n        records[i].TTL = remaining\n    }\n    \n    return records\n}\n\nfunc (c *Cache) Set(name string, qtype uint16, records []ResourceRecord) {\n    if len(records) == 0 {\n        return\n    }\n    \n    // Use minimum TTL\n    minTTL := records[0].TTL\n    for _, rr := range records[1:] {\n        if rr.TTL < minTTL {\n            minTTL = rr.TTL\n        }\n    }\n    \n    c.mu.Lock()\n    c.entries[c.makeKey(name, qtype)] = &CacheEntry{\n        Records:    records,\n        Expiration: time.Now().Add(time.Duration(minTTL) * time.Second),\n    }\n    c.mu.Unlock()\n}\n\nfunc (c *Cache) SetNegative(name string, qtype uint16, ttl uint32) {\n    c.mu.Lock()\n    c.entries[c.makeKey(name, qtype)] = &CacheEntry{\n        Negative:   true,\n        Expiration: time.Now().Add(time.Duration(ttl) * time.Second),\n    }\n    c.mu.Unlock()\n}\n\n// Server with validation\nfunc (s *Server) handleQuery(addr *net.UDPAddr, query *Message) {\n    response := s.resolver.Resolve(query)\n    \n    // Validate response\n    if response.Header.ID != query.Header.ID {\n        return  // Ignore mismatched response\n    }\n    \n    s.conn.WriteToUDP(response.Serialize(), addr)\n}\n```"
                        },
                        pitfalls: ["TTL underflow", "Cache poisoning", "Memory exhaustion"],
                        concepts: ["Caching strategies", "Security", "Concurrency"],
                        estimatedHours: "10-15"
                    }
                ]
            },

            "build-lsp": {
                name: "Build Your Own LSP Server",
                description: "Build a Language Server Protocol server for IDE features.",
                difficulty: "expert",
                estimatedHours: "50-80",
                prerequisites: ["JSON-RPC", "AST/parsing", "IDE concepts", "Concurrency"],
                languages: {
                    recommended: ["TypeScript", "Rust", "Go"],
                    also: ["Python", "C#"]
                },
                resources: [
                    { type: "specification", name: "LSP Specification", url: "https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/" },
                    { type: "article", name: "LSP Tutorial", url: "https://code.visualstudio.com/api/language-extensions/language-server-extension-guide" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "JSON-RPC & Initialization",
                        description: "Implement JSON-RPC transport and LSP initialization.",
                        criteria: ["JSON-RPC message handling", "Initialize/initialized handshake", "Capability negotiation", "Shutdown/exit"],
                        hints: {
                            level1: "LSP uses JSON-RPC 2.0 over stdio. Messages have Content-Length header.",
                            level2: "Server advertises capabilities in initialize response.",
                            level3: "## LSP Transport\n\n```typescript\nimport * as readline from 'readline';\n\ninterface Message {\n  jsonrpc: '2.0';\n  id?: number | string;\n  method?: string;\n  params?: any;\n  result?: any;\n  error?: { code: number; message: string };\n}\n\nclass LSPServer {\n  private pendingData = '';\n  private contentLength = -1;\n  \n  constructor() {\n    process.stdin.on('data', (data) => this.handleData(data.toString()));\n  }\n  \n  private handleData(data: string) {\n    this.pendingData += data;\n    \n    while (true) {\n      if (this.contentLength === -1) {\n        const headerEnd = this.pendingData.indexOf('\\r\\n\\r\\n');\n        if (headerEnd === -1) return;\n        \n        const header = this.pendingData.slice(0, headerEnd);\n        const match = header.match(/Content-Length: (\\d+)/);\n        if (!match) throw new Error('Invalid header');\n        \n        this.contentLength = parseInt(match[1]);\n        this.pendingData = this.pendingData.slice(headerEnd + 4);\n      }\n      \n      if (this.pendingData.length < this.contentLength) return;\n      \n      const content = this.pendingData.slice(0, this.contentLength);\n      this.pendingData = this.pendingData.slice(this.contentLength);\n      this.contentLength = -1;\n      \n      this.handleMessage(JSON.parse(content));\n    }\n  }\n  \n  private handleMessage(msg: Message) {\n    if (msg.method) {\n      this.handleRequest(msg);\n    } else if (msg.id !== undefined) {\n      // Response to our request (not common for servers)\n    }\n  }\n  \n  private handleRequest(msg: Message) {\n    switch (msg.method) {\n      case 'initialize':\n        this.sendResponse(msg.id, {\n          capabilities: {\n            textDocumentSync: 1,  // Full sync\n            completionProvider: { triggerCharacters: ['.'] },\n            hoverProvider: true,\n            definitionProvider: true,\n          }\n        });\n        break;\n        \n      case 'initialized':\n        // Client is ready\n        break;\n        \n      case 'shutdown':\n        this.sendResponse(msg.id, null);\n        break;\n        \n      case 'exit':\n        process.exit(0);\n    }\n  }\n  \n  private sendResponse(id: number | string, result: any) {\n    this.send({ jsonrpc: '2.0', id, result });\n  }\n  \n  private send(msg: Message) {\n    const content = JSON.stringify(msg);\n    const header = `Content-Length: ${Buffer.byteLength(content)}\\r\\n\\r\\n`;\n    process.stdout.write(header + content);\n  }\n}\n```"
                        },
                        pitfalls: ["Content-Length calculation", "Partial message handling", "Encoding issues"],
                        concepts: ["JSON-RPC", "Protocol negotiation", "Streaming"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 2,
                        name: "Document Synchronization",
                        description: "Track document changes from the editor.",
                        criteria: ["textDocument/didOpen", "textDocument/didChange", "textDocument/didClose", "Incremental sync"],
                        hints: {
                            level1: "Track open documents in a map. Update on didChange.",
                            level2: "Incremental sync: apply text edits to stored content.",
                            level3: "## Document Sync\n\n```typescript\ninterface TextDocument {\n  uri: string;\n  languageId: string;\n  version: number;\n  content: string;\n}\n\nclass DocumentManager {\n  private documents = new Map<string, TextDocument>();\n  \n  open(uri: string, languageId: string, version: number, content: string) {\n    this.documents.set(uri, { uri, languageId, version, content });\n  }\n  \n  close(uri: string) {\n    this.documents.delete(uri);\n  }\n  \n  get(uri: string): TextDocument | undefined {\n    return this.documents.get(uri);\n  }\n  \n  update(uri: string, version: number, changes: TextDocumentContentChangeEvent[]) {\n    const doc = this.documents.get(uri);\n    if (!doc) return;\n    \n    for (const change of changes) {\n      if ('range' in change) {\n        // Incremental change\n        const start = this.offsetAt(doc.content, change.range.start);\n        const end = this.offsetAt(doc.content, change.range.end);\n        doc.content = doc.content.slice(0, start) + change.text + doc.content.slice(end);\n      } else {\n        // Full content\n        doc.content = change.text;\n      }\n    }\n    doc.version = version;\n  }\n  \n  private offsetAt(content: string, position: Position): number {\n    const lines = content.split('\\n');\n    let offset = 0;\n    for (let i = 0; i < position.line; i++) {\n      offset += lines[i].length + 1;  // +1 for newline\n    }\n    return offset + position.character;\n  }\n  \n  positionAt(content: string, offset: number): Position {\n    const lines = content.slice(0, offset).split('\\n');\n    return {\n      line: lines.length - 1,\n      character: lines[lines.length - 1].length\n    };\n  }\n}\n\n// In LSPServer\ncase 'textDocument/didOpen':\n  this.documents.open(\n    msg.params.textDocument.uri,\n    msg.params.textDocument.languageId,\n    msg.params.textDocument.version,\n    msg.params.textDocument.text\n  );\n  this.validate(msg.params.textDocument.uri);\n  break;\n\ncase 'textDocument/didChange':\n  this.documents.update(\n    msg.params.textDocument.uri,\n    msg.params.textDocument.version,\n    msg.params.contentChanges\n  );\n  this.validate(msg.params.textDocument.uri);\n  break;\n```"
                        },
                        pitfalls: ["Version mismatch", "Offset calculation errors", "Unicode handling"],
                        concepts: ["Document tracking", "Incremental updates", "Positions"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 3,
                        name: "Language Features",
                        description: "Implement completion, hover, and go-to-definition.",
                        criteria: ["textDocument/completion", "textDocument/hover", "textDocument/definition", "Symbol resolution"],
                        hints: {
                            level1: "Parse document to AST. Find symbol at cursor position.",
                            level2: "Build symbol table for quick lookups. Handle imports/includes.",
                            level3: "## Language Features\n\n```typescript\ninterface Symbol {\n  name: string;\n  kind: SymbolKind;\n  location: Location;\n  type?: string;\n  documentation?: string;\n}\n\nclass LanguageService {\n  private symbolTable = new Map<string, Symbol[]>();  // uri -> symbols\n  \n  analyze(uri: string, content: string) {\n    const ast = this.parse(content);\n    const symbols: Symbol[] = [];\n    \n    // Walk AST to collect symbols\n    this.walk(ast, (node) => {\n      if (node.type === 'FunctionDeclaration') {\n        symbols.push({\n          name: node.name,\n          kind: SymbolKind.Function,\n          location: { uri, range: node.range },\n          type: this.formatFunctionType(node),\n          documentation: node.docComment\n        });\n      } else if (node.type === 'VariableDeclaration') {\n        symbols.push({\n          name: node.name,\n          kind: SymbolKind.Variable,\n          location: { uri, range: node.range },\n          type: node.declaredType\n        });\n      }\n    });\n    \n    this.symbolTable.set(uri, symbols);\n  }\n  \n  getCompletions(uri: string, position: Position): CompletionItem[] {\n    const doc = this.documents.get(uri);\n    const symbols = this.symbolTable.get(uri) || [];\n    const context = this.getContextAtPosition(doc.content, position);\n    \n    // Filter symbols based on context\n    return symbols\n      .filter(s => this.isVisible(s, position))\n      .map(s => ({\n        label: s.name,\n        kind: s.kind,\n        detail: s.type,\n        documentation: s.documentation\n      }));\n  }\n  \n  getHover(uri: string, position: Position): Hover | null {\n    const symbol = this.findSymbolAtPosition(uri, position);\n    if (!symbol) return null;\n    \n    return {\n      contents: {\n        kind: 'markdown',\n        value: `**${symbol.name}**: ${symbol.type || 'unknown'}\\n\\n${symbol.documentation || ''}`\n      }\n    };\n  }\n  \n  getDefinition(uri: string, position: Position): Location | null {\n    const word = this.getWordAtPosition(uri, position);\n    \n    // Search all documents for definition\n    for (const [docUri, symbols] of this.symbolTable) {\n      const symbol = symbols.find(s => s.name === word);\n      if (symbol) {\n        return symbol.location;\n      }\n    }\n    \n    return null;\n  }\n  \n  private findSymbolAtPosition(uri: string, position: Position): Symbol | null {\n    const word = this.getWordAtPosition(uri, position);\n    const symbols = this.symbolTable.get(uri) || [];\n    return symbols.find(s => s.name === word) || null;\n  }\n}\n```"
                        },
                        pitfalls: ["Stale symbol table", "Scope visibility", "Performance on large files"],
                        concepts: ["Symbol resolution", "AST analysis", "IDE features"],
                        estimatedHours: "15-20"
                    },
                    {
                        id: 4,
                        name: "Diagnostics & Code Actions",
                        description: "Report errors and suggest fixes.",
                        criteria: ["textDocument/publishDiagnostics", "textDocument/codeAction", "Diagnostic severity", "Quick fixes"],
                        hints: {
                            level1: "Push diagnostics to client on document change.",
                            level2: "Code actions provide fixes for diagnostics at a location.",
                            level3: "## Diagnostics & Code Actions\n\n```typescript\ninterface Diagnostic {\n  range: Range;\n  severity: DiagnosticSeverity;\n  code?: string | number;\n  source?: string;\n  message: string;\n  relatedInformation?: DiagnosticRelatedInformation[];\n}\n\nclass DiagnosticProvider {\n  validate(uri: string, content: string): Diagnostic[] {\n    const diagnostics: Diagnostic[] = [];\n    const ast = this.parse(content);\n    \n    // Check for errors\n    this.walk(ast, (node) => {\n      // Undefined variable\n      if (node.type === 'Identifier' && !this.isDefined(node.name)) {\n        diagnostics.push({\n          range: node.range,\n          severity: DiagnosticSeverity.Error,\n          code: 'undefined-variable',\n          source: 'my-lsp',\n          message: `'${node.name}' is not defined`\n        });\n      }\n      \n      // Type mismatch\n      if (node.type === 'Assignment') {\n        const leftType = this.getType(node.left);\n        const rightType = this.getType(node.right);\n        if (leftType && rightType && !this.isAssignable(leftType, rightType)) {\n          diagnostics.push({\n            range: node.range,\n            severity: DiagnosticSeverity.Error,\n            code: 'type-mismatch',\n            source: 'my-lsp',\n            message: `Type '${rightType}' is not assignable to type '${leftType}'`\n          });\n        }\n      }\n      \n      // Unused variable (warning)\n      if (node.type === 'VariableDeclaration' && !this.isUsed(node.name)) {\n        diagnostics.push({\n          range: node.range,\n          severity: DiagnosticSeverity.Warning,\n          code: 'unused-variable',\n          source: 'my-lsp',\n          message: `'${node.name}' is declared but never used`\n        });\n      }\n    });\n    \n    return diagnostics;\n  }\n}\n\nclass CodeActionProvider {\n  getCodeActions(uri: string, range: Range, diagnostics: Diagnostic[]): CodeAction[] {\n    const actions: CodeAction[] = [];\n    \n    for (const diag of diagnostics) {\n      switch (diag.code) {\n        case 'undefined-variable':\n          // Suggest declaring the variable\n          const varName = this.extractVarName(diag.message);\n          actions.push({\n            title: `Declare variable '${varName}'`,\n            kind: CodeActionKind.QuickFix,\n            diagnostics: [diag],\n            edit: {\n              changes: {\n                [uri]: [{\n                  range: { start: { line: range.start.line, character: 0 }, end: { line: range.start.line, character: 0 } },\n                  newText: `let ${varName};\\n`\n                }]\n              }\n            }\n          });\n          break;\n          \n        case 'unused-variable':\n          // Suggest removing or prefixing with _\n          actions.push({\n            title: 'Remove unused variable',\n            kind: CodeActionKind.QuickFix,\n            diagnostics: [diag],\n            edit: {\n              changes: {\n                [uri]: [{ range: diag.range, newText: '' }]\n              }\n            }\n          });\n          break;\n      }\n    }\n    \n    return actions;\n  }\n}\n\n// Publish diagnostics\nprivate validate(uri: string) {\n  const doc = this.documents.get(uri);\n  if (!doc) return;\n  \n  const diagnostics = this.diagnosticProvider.validate(uri, doc.content);\n  \n  this.sendNotification('textDocument/publishDiagnostics', {\n    uri,\n    diagnostics\n  });\n}\n```"
                        },
                        pitfalls: ["Diagnostic spam", "Outdated diagnostics", "Large workspace performance"],
                        concepts: ["Static analysis", "Quick fixes", "Editor integration"],
                        estimatedHours: "15-20"
                    }
                ]
            },

            // SOFTWARE-ENGINEERING - EXPERT
            "build-test-framework": {
                name: "Build Your Own Test Framework",
                description: "Build a test framework like pytest or jest.",
                difficulty: "expert",
                estimatedHours: "40-60",
                prerequisites: ["Reflection/metaprogramming", "Assertion libraries", "CLI development"],
                languages: {
                    recommended: ["Python", "JavaScript", "Go"],
                    also: ["Rust", "Java"]
                },
                resources: [
                    { type: "code", name: "pytest source", url: "https://github.com/pytest-dev/pytest" },
                    { type: "article", name: "Building a Test Framework", url: "https://www.destroyallsoftware.com/screencasts/catalog/building-a-test-framework" }
                ],
                milestones: [
                    {
                        id: 1,
                        name: "Test Discovery & Execution",
                        description: "Discover and run test functions.",
                        criteria: ["Find test files (test_*.py)", "Find test functions (test_*)", "Run tests with isolation", "Collect results"],
                        hints: {
                            level1: "Walk directory tree, import test modules, find functions starting with 'test_'.",
                            level2: "Run each test in isolation (fresh module state). Catch exceptions.",
                            level3: "## Test Discovery\n\n```python\nimport importlib.util\nimport sys\nimport os\nimport traceback\nfrom dataclasses import dataclass\nfrom typing import Callable, List\nfrom enum import Enum\n\nclass TestResult(Enum):\n    PASSED = 'passed'\n    FAILED = 'failed'\n    ERROR = 'error'\n    SKIPPED = 'skipped'\n\n@dataclass\nclass TestCase:\n    name: str\n    module: str\n    func: Callable\n\n@dataclass\nclass TestOutcome:\n    test: TestCase\n    result: TestResult\n    duration: float\n    error: Exception = None\n    traceback: str = None\n\nclass TestCollector:\n    def __init__(self, path='.'):\n        self.path = path\n        self.tests: List[TestCase] = []\n    \n    def collect(self):\n        for root, dirs, files in os.walk(self.path):\n            # Skip hidden and __pycache__ directories\n            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n            \n            for file in files:\n                if file.startswith('test_') and file.endswith('.py'):\n                    filepath = os.path.join(root, file)\n                    self.collect_from_file(filepath)\n        \n        return self.tests\n    \n    def collect_from_file(self, filepath):\n        module_name = os.path.splitext(os.path.basename(filepath))[0]\n        \n        spec = importlib.util.spec_from_file_location(module_name, filepath)\n        module = importlib.util.module_from_spec(spec)\n        sys.modules[module_name] = module\n        \n        try:\n            spec.loader.exec_module(module)\n        except Exception as e:\n            print(f\"Error loading {filepath}: {e}\")\n            return\n        \n        for name in dir(module):\n            if name.startswith('test_'):\n                obj = getattr(module, name)\n                if callable(obj):\n                    self.tests.append(TestCase(\n                        name=name,\n                        module=module_name,\n                        func=obj\n                    ))\n\nclass TestRunner:\n    def __init__(self):\n        self.outcomes: List[TestOutcome] = []\n    \n    def run(self, tests: List[TestCase]):\n        for test in tests:\n            outcome = self.run_one(test)\n            self.outcomes.append(outcome)\n            self.report_one(outcome)\n        \n        return self.outcomes\n    \n    def run_one(self, test: TestCase) -> TestOutcome:\n        import time\n        start = time.time()\n        \n        try:\n            test.func()\n            result = TestResult.PASSED\n            error = None\n            tb = None\n        except AssertionError as e:\n            result = TestResult.FAILED\n            error = e\n            tb = traceback.format_exc()\n        except Exception as e:\n            result = TestResult.ERROR\n            error = e\n            tb = traceback.format_exc()\n        \n        duration = time.time() - start\n        return TestOutcome(test, result, duration, error, tb)\n    \n    def report_one(self, outcome: TestOutcome):\n        symbol = {\n            TestResult.PASSED: '.',\n            TestResult.FAILED: 'F',\n            TestResult.ERROR: 'E',\n            TestResult.SKIPPED: 's'\n        }[outcome.result]\n        print(symbol, end='', flush=True)\n```"
                        },
                        pitfalls: ["Module import side effects", "Test isolation", "Path handling"],
                        concepts: ["Reflection", "Module loading", "Test isolation"],
                        estimatedHours: "10-15"
                    },
                    {
                        id: 2,
                        name: "Assertions & Matchers",
                        description: "Implement rich assertion library.",
                        criteria: ["Basic assertions (assertEqual, assertTrue)", "Collection assertions", "Exception assertions", "Diff output for failures"],
                        hints: {
                            level1: "Assertions raise AssertionError with helpful messages.",
                            level2: "Show diff for string/collection comparisons. Pretty-print values.",
                            level3: "## Assertions\n\n```python\nimport difflib\nfrom typing import Any, Type, Callable\n\nclass AssertionHelper:\n    @staticmethod\n    def assertEqual(actual: Any, expected: Any, msg: str = None):\n        if actual != expected:\n            diff = AssertionHelper._format_diff(actual, expected)\n            message = msg or f\"\\nExpected: {expected!r}\\nActual:   {actual!r}\\n{diff}\"\n            raise AssertionError(message)\n    \n    @staticmethod\n    def assertTrue(value: Any, msg: str = None):\n        if not value:\n            raise AssertionError(msg or f\"Expected truthy value, got {value!r}\")\n    \n    @staticmethod\n    def assertFalse(value: Any, msg: str = None):\n        if value:\n            raise AssertionError(msg or f\"Expected falsy value, got {value!r}\")\n    \n    @staticmethod\n    def assertIn(item: Any, container: Any, msg: str = None):\n        if item not in container:\n            raise AssertionError(msg or f\"{item!r} not found in {container!r}\")\n    \n    @staticmethod\n    def assertRaises(exc_type: Type[Exception]):\n        return _ExceptionContext(exc_type)\n    \n    @staticmethod\n    def assertAlmostEqual(actual: float, expected: float, places: int = 7):\n        if round(abs(actual - expected), places) != 0:\n            raise AssertionError(f\"{actual} != {expected} within {places} places\")\n    \n    @staticmethod\n    def _format_diff(actual: Any, expected: Any) -> str:\n        if isinstance(actual, str) and isinstance(expected, str):\n            diff = difflib.unified_diff(\n                expected.splitlines(keepends=True),\n                actual.splitlines(keepends=True),\n                fromfile='expected',\n                tofile='actual'\n            )\n            return ''.join(diff)\n        \n        if isinstance(actual, (list, tuple)) and isinstance(expected, (list, tuple)):\n            diff_lines = []\n            for i, (a, e) in enumerate(zip(actual, expected)):\n                if a != e:\n                    diff_lines.append(f\"  Index {i}: expected {e!r}, got {a!r}\")\n            if len(actual) != len(expected):\n                diff_lines.append(f\"  Length: expected {len(expected)}, got {len(actual)}\")\n            return '\\n'.join(diff_lines)\n        \n        return ''\n\nclass _ExceptionContext:\n    def __init__(self, exc_type: Type[Exception]):\n        self.exc_type = exc_type\n        self.exception = None\n    \n    def __enter__(self):\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is None:\n            raise AssertionError(f\"Expected {self.exc_type.__name__} to be raised\")\n        \n        if not issubclass(exc_type, self.exc_type):\n            raise AssertionError(\n                f\"Expected {self.exc_type.__name__}, got {exc_type.__name__}\"\n            )\n        \n        self.exception = exc_val\n        return True  # Suppress the exception\n\n# Usage convenience\nassert_equal = AssertionHelper.assertEqual\nassert_true = AssertionHelper.assertTrue\nassert_raises = AssertionHelper.assertRaises\n```"
                        },
                        pitfalls: ["Unhelpful error messages", "Float comparison", "Exception context"],
                        concepts: ["Assertions", "Diff algorithms", "Context managers"],
                        estimatedHours: "8-12"
                    },
                    {
                        id: 3,
                        name: "Fixtures & Setup/Teardown",
                        description: "Implement test fixtures for setup and cleanup.",
                        criteria: ["Function-level setup/teardown", "Module-level setup/teardown", "Fixture dependencies", "Fixture scope (function, module, session)"],
                        hints: {
                            level1: "Fixtures are functions that provide test dependencies.",
                            level2: "Use generators for setup/teardown. Scope controls lifetime.",
                            level3: "## Fixtures\n\n```python\nfrom typing import Dict, Any, Generator\nfrom functools import wraps\nimport inspect\n\n_fixtures: Dict[str, 'Fixture'] = {}\n\nclass Fixture:\n    def __init__(self, func: Callable, scope: str = 'function'):\n        self.func = func\n        self.scope = scope  # 'function', 'module', 'session'\n        self.name = func.__name__\n        self._cached_value = None\n        self._finalized = True\n    \n    def get_value(self, context: 'FixtureContext'):\n        if self.scope != 'function' and not self._finalized:\n            return self._cached_value\n        \n        # Resolve fixture dependencies\n        sig = inspect.signature(self.func)\n        kwargs = {}\n        for param in sig.parameters:\n            if param in _fixtures:\n                kwargs[param] = _fixtures[param].get_value(context)\n        \n        result = self.func(**kwargs)\n        \n        if inspect.isgenerator(result):\n            # Setup/teardown fixture\n            value = next(result)\n            context.add_finalizer(lambda: self._finalize(result))\n        else:\n            value = result\n        \n        self._cached_value = value\n        self._finalized = False\n        return value\n    \n    def _finalize(self, gen: Generator):\n        try:\n            next(gen)\n        except StopIteration:\n            pass\n        self._finalized = True\n\ndef fixture(scope: str = 'function'):\n    def decorator(func: Callable):\n        fix = Fixture(func, scope)\n        _fixtures[func.__name__] = fix\n        return func\n    return decorator\n\nclass FixtureContext:\n    def __init__(self):\n        self.finalizers = []\n    \n    def add_finalizer(self, func: Callable):\n        self.finalizers.append(func)\n    \n    def teardown(self):\n        for finalizer in reversed(self.finalizers):\n            try:\n                finalizer()\n            except Exception as e:\n                print(f\"Finalizer error: {e}\")\n        self.finalizers.clear()\n\n# Updated TestRunner\nclass TestRunner:\n    def run_one(self, test: TestCase) -> TestOutcome:\n        context = FixtureContext()\n        \n        try:\n            # Inject fixtures into test function\n            sig = inspect.signature(test.func)\n            kwargs = {}\n            for param in sig.parameters:\n                if param in _fixtures:\n                    kwargs[param] = _fixtures[param].get_value(context)\n            \n            test.func(**kwargs)\n            result = TestResult.PASSED\n            error = None\n        except AssertionError as e:\n            result = TestResult.FAILED\n            error = e\n        finally:\n            context.teardown()\n        \n        return TestOutcome(test, result, 0, error)\n\n# Example fixtures\n@fixture(scope='function')\ndef temp_dir():\n    import tempfile\n    import shutil\n    d = tempfile.mkdtemp()\n    yield d\n    shutil.rmtree(d)\n\n@fixture(scope='session')\ndef database():\n    db = connect_to_test_db()\n    yield db\n    db.close()\n```"
                        },
                        pitfalls: ["Fixture cleanup on error", "Circular dependencies", "Scope leaks"],
                        concepts: ["Dependency injection", "Resource management", "Generators"],
                        estimatedHours: "12-18"
                    },
                    {
                        id: 4,
                        name: "Reporting & CLI",
                        description: "Generate test reports and provide CLI interface.",
                        criteria: ["Console output with colors", "JUnit XML output", "Code coverage integration", "Parallel execution", "Watch mode"],
                        hints: {
                            level1: "Color-code pass/fail. Show failure details at end.",
                            level2: "JUnit XML is standard format for CI integration.",
                            level3: "## Reporting & CLI\n\n```python\nimport argparse\nimport xml.etree.ElementTree as ET\nfrom typing import List\nimport sys\n\nclass ConsoleReporter:\n    def __init__(self, verbose: bool = False):\n        self.verbose = verbose\n        self.colors = sys.stdout.isatty()\n    \n    def _color(self, text: str, color: str) -> str:\n        if not self.colors:\n            return text\n        colors = {'green': '\\033[92m', 'red': '\\033[91m', 'yellow': '\\033[93m', 'reset': '\\033[0m'}\n        return f\"{colors.get(color, '')}{text}{colors['reset']}\"\n    \n    def report_start(self, tests: List[TestCase]):\n        print(f\"\\nCollected {len(tests)} tests\\n\")\n    \n    def report_test(self, outcome: TestOutcome):\n        if self.verbose:\n            status = {\n                TestResult.PASSED: self._color('PASSED', 'green'),\n                TestResult.FAILED: self._color('FAILED', 'red'),\n                TestResult.ERROR: self._color('ERROR', 'red'),\n                TestResult.SKIPPED: self._color('SKIPPED', 'yellow')\n            }[outcome.result]\n            print(f\"{outcome.test.module}::{outcome.test.name} {status}\")\n        else:\n            symbol = {\n                TestResult.PASSED: self._color('.', 'green'),\n                TestResult.FAILED: self._color('F', 'red'),\n                TestResult.ERROR: self._color('E', 'red'),\n                TestResult.SKIPPED: self._color('s', 'yellow')\n            }[outcome.result]\n            print(symbol, end='', flush=True)\n    \n    def report_summary(self, outcomes: List[TestOutcome]):\n        passed = sum(1 for o in outcomes if o.result == TestResult.PASSED)\n        failed = sum(1 for o in outcomes if o.result == TestResult.FAILED)\n        errors = sum(1 for o in outcomes if o.result == TestResult.ERROR)\n        total_time = sum(o.duration for o in outcomes)\n        \n        print(f\"\\n\\n{'='*60}\")\n        \n        # Show failures\n        for outcome in outcomes:\n            if outcome.result in (TestResult.FAILED, TestResult.ERROR):\n                print(f\"\\n{self._color('FAILED', 'red')} {outcome.test.module}::{outcome.test.name}\")\n                print(outcome.traceback)\n        \n        print(f\"\\n{passed} passed, {failed} failed, {errors} errors in {total_time:.2f}s\")\n\nclass JUnitReporter:\n    def generate(self, outcomes: List[TestOutcome], output_file: str):\n        testsuite = ET.Element('testsuite')\n        testsuite.set('tests', str(len(outcomes)))\n        testsuite.set('failures', str(sum(1 for o in outcomes if o.result == TestResult.FAILED)))\n        testsuite.set('errors', str(sum(1 for o in outcomes if o.result == TestResult.ERROR)))\n        \n        for outcome in outcomes:\n            testcase = ET.SubElement(testsuite, 'testcase')\n            testcase.set('classname', outcome.test.module)\n            testcase.set('name', outcome.test.name)\n            testcase.set('time', str(outcome.duration))\n            \n            if outcome.result == TestResult.FAILED:\n                failure = ET.SubElement(testcase, 'failure')\n                failure.set('message', str(outcome.error))\n                failure.text = outcome.traceback\n            elif outcome.result == TestResult.ERROR:\n                error = ET.SubElement(testcase, 'error')\n                error.set('message', str(outcome.error))\n                error.text = outcome.traceback\n        \n        tree = ET.ElementTree(testsuite)\n        tree.write(output_file, encoding='unicode', xml_declaration=True)\n\n# CLI\ndef main():\n    parser = argparse.ArgumentParser(description='Test Framework')\n    parser.add_argument('path', nargs='?', default='.', help='Test directory')\n    parser.add_argument('-v', '--verbose', action='store_true')\n    parser.add_argument('--junit-xml', help='Output JUnit XML file')\n    parser.add_argument('-k', '--filter', help='Filter tests by name')\n    args = parser.parse_args()\n    \n    collector = TestCollector(args.path)\n    tests = collector.collect()\n    \n    if args.filter:\n        tests = [t for t in tests if args.filter in t.name]\n    \n    reporter = ConsoleReporter(verbose=args.verbose)\n    reporter.report_start(tests)\n    \n    runner = TestRunner()\n    outcomes = runner.run(tests)\n    \n    reporter.report_summary(outcomes)\n    \n    if args.junit_xml:\n        JUnitReporter().generate(outcomes, args.junit_xml)\n    \n    sys.exit(0 if all(o.result == TestResult.PASSED for o in outcomes) else 1)\n\nif __name__ == '__main__':\n    main()\n```"
                        },
                        pitfalls: ["Exit codes", "Terminal detection", "XML escaping"],
                        concepts: ["CLI design", "Reporting formats", "CI integration"],
                        estimatedHours: "10-15"
                    }
                ]
            }
        }
    };

    // State
    let currentFilter = 'all';
    let searchQuery = '';
    let currentView = 'domains'; // 'domains', 'domain-detail', 'project-detail'
    let selectedDomain = null;
    let selectedProject = null;

    // DOM Elements
    const domainsGrid = document.getElementById('domainsGrid');
    const modalOverlay = document.getElementById('modalOverlay');
    const modal = document.getElementById('modal');
    const modalTitle = document.getElementById('modalTitle');
    const modalContent = document.getElementById('modalContent');
    const closeModal = document.getElementById('closeModal');
    const searchInput = document.getElementById('searchInput');
    const filterPills = document.querySelectorAll('.filter-pill');

    // Calculate stats
    function calculateStats() {
        let totalProjects = 0;
        let detailedProjects = 0;
        let totalMilestones = 0;

        projectsData.domains.forEach(domain => {
            ['beginner', 'intermediate', 'advanced', 'expert'].forEach(level => {
                const projects = domain.projects[level] || [];
                totalProjects += projects.length;
                projects.forEach(p => {
                    if (p.detailed) detailedProjects++;
                });
            });
        });

        Object.values(projectsData.expertProjects).forEach(project => {
            totalMilestones += project.milestones.length;
        });

        document.getElementById('totalProjects').textContent = totalProjects;
        document.getElementById('detailedProjects').textContent = detailedProjects;
        document.getElementById('totalMilestones').textContent = totalMilestones;
    }

    // Render domain cards
    function renderDomains() {
        const filteredDomains = projectsData.domains.filter(domain => {
            if (searchQuery) {
                const query = searchQuery.toLowerCase();
                const nameMatch = domain.name.toLowerCase().includes(query);
                const subdomainMatch = domain.subdomains.some(s => s.toLowerCase().includes(query));
                const projectMatch = ['beginner', 'intermediate', 'advanced', 'expert'].some(level =>
                    (domain.projects[level] || []).some(p =>
                        p.name.toLowerCase().includes(query) ||
                        p.description.toLowerCase().includes(query)
                    )
                );
                return nameMatch || subdomainMatch || projectMatch;
            }
            return true;
        });

        domainsGrid.innerHTML = filteredDomains.map(domain => {
            const counts = {
                beginner: (domain.projects.beginner || []).length,
                intermediate: (domain.projects.intermediate || []).length,
                advanced: (domain.projects.advanced || []).length,
                expert: (domain.projects.expert || []).length
            };

            const total = counts.beginner + counts.intermediate + counts.advanced + counts.expert;
            if (total === 0 && currentFilter !== 'all') return '';

            const filteredCounts = currentFilter === 'all' ? counts : { [currentFilter]: counts[currentFilter] };
            const filteredTotal = currentFilter === 'all' ? total : counts[currentFilter];

            if (filteredTotal === 0) return '';

            return `
                <div class="domain-card" data-domain-id="${domain.id}">
                    <div class="domain-icon">${domain.icon}</div>
                    <h3 class="domain-name">${domain.name}</h3>
                    <p class="domain-subdomains">${domain.subdomains.slice(0, 3).join(' ‚Ä¢ ')}${domain.subdomains.length > 3 ? ' ...' : ''}</p>
                    <div class="project-counts">
                        ${currentFilter === 'all' ? `
                            ${counts.beginner > 0 ? `<span class="count-badge beginner">${counts.beginner} Beginner</span>` : ''}
                            ${counts.intermediate > 0 ? `<span class="count-badge intermediate">${counts.intermediate} Intermediate</span>` : ''}
                            ${counts.advanced > 0 ? `<span class="count-badge advanced">${counts.advanced} Advanced</span>` : ''}
                            ${counts.expert > 0 ? `<span class="count-badge expert">${counts.expert} Expert</span>` : ''}
                        ` : `
                            <span class="count-badge ${currentFilter}">${filteredTotal} ${currentFilter.charAt(0).toUpperCase() + currentFilter.slice(1)}</span>
                        `}
                    </div>
                </div>
            `;
        }).join('');

        // Add click handlers
        document.querySelectorAll('.domain-card').forEach(card => {
            card.addEventListener('click', () => {
                const domainId = card.dataset.domainId;
                selectedDomain = projectsData.domains.find(d => d.id === domainId);
                openDomainModal(selectedDomain);
            });
        });
    }

    // Open domain modal
    function openDomainModal(domain) {
        modalTitle.innerHTML = `${domain.icon} ${domain.name}`;

        const levels = currentFilter === 'all'
            ? ['beginner', 'intermediate', 'advanced', 'expert']
            : [currentFilter];

        let content = '';

        levels.forEach(level => {
            const projects = domain.projects[level] || [];
            const filteredProjects = searchQuery
                ? projects.filter(p =>
                    p.name.toLowerCase().includes(searchQuery.toLowerCase()) ||
                    p.description.toLowerCase().includes(searchQuery.toLowerCase())
                  )
                : projects;

            if (filteredProjects.length > 0) {
                content += `
                    <div class="level-section">
                        <div class="level-header">
                            <span class="level-indicator ${level}">${level}</span>
                            <span style="color: var(--text-muted); font-size: 14px;">${filteredProjects.length} project${filteredProjects.length !== 1 ? 's' : ''}</span>
                        </div>
                        <div class="projects-list">
                            ${filteredProjects.map(project => `
                                <div class="project-item ${project.detailed ? 'detailed' : ''}" data-project-id="${project.id}">
                                    <div class="project-info">
                                        <h4>${project.name}</h4>
                                        <p>${project.description}</p>
                                        ${project.languages ? `<div class="tags">${project.languages.map(l => `<span class="tag">${l}</span>`).join('')}</div>` : ''}
                                    </div>
                                    <span class="project-arrow">${project.detailed ? '‚Üí' : ''}</span>
                                </div>
                            `).join('')}
                        </div>
                    </div>
                `;
            }
        });

        if (!content) {
            content = `
                <div class="empty-state">
                    <div class="icon">üì≠</div>
                    <p>No projects match your current filter.</p>
                </div>
            `;
        }

        modalContent.innerHTML = content;

        // Add click handlers for detailed projects
        document.querySelectorAll('.project-item.detailed').forEach(item => {
            item.addEventListener('click', () => {
                const projectId = item.dataset.projectId;
                const projectData = projectsData.expertProjects[projectId];
                if (projectData) {
                    openProjectDetail(projectData);
                }
            });
        });

        showModal();
    }

    // Open project detail view
    function openProjectDetail(project) {
        modalTitle.innerHTML = `üî¨ ${project.name}`;

        let content = `
            <div class="project-detail">
                <button class="back-btn" id="backToList">‚Üê Back to list</button>

                <div class="detail-header">
                    <h3>${project.name}</h3>
                    <p style="color: var(--text-secondary); line-height: 1.6;">${project.description}</p>
                    <div class="detail-meta">
                        <span class="meta-item">‚è±Ô∏è ${project.estimatedHours} hours</span>
                        <span class="meta-item">üìä ${project.milestones.length} milestones</span>
                    </div>
                    ${project.languages ? `
                        <div class="tags" style="margin-top: 16px;">
                            ${project.languages.recommended.map(l => `<span class="tag">${l}</span>`).join('')}
                            ${project.languages.also ? project.languages.also.map(l => `<span class="tag" style="opacity: 0.6">${l}</span>`).join('') : ''}
                        </div>
                    ` : ''}
                </div>

                ${project.prerequisites ? `
                    <div class="section">
                        <h4 class="section-title">Prerequisites</h4>
                        <ul class="concepts-list">
                            ${project.prerequisites.map(p => `<li>${p}</li>`).join('')}
                        </ul>
                    </div>
                ` : ''}

                ${project.resources && project.resources.length > 0 ? `
                    <div class="section">
                        <h4 class="section-title">Resources</h4>
                        <div class="resources-grid">
                            ${project.resources.map(r => `
                                <a href="${r.url}" target="_blank" class="resource-link">
                                    <div class="type">${r.type}</div>
                                    <div class="name">${r.name}</div>
                                </a>
                            `).join('')}
                        </div>
                    </div>
                ` : ''}

                <div class="section">
                    <h4 class="section-title">Milestones</h4>
                    ${project.milestones.map((m, idx) => `
                        <div class="milestone" data-milestone-id="${idx}">
                            <div class="milestone-header">
                                <span class="milestone-number">${m.id || idx + 1}</span>
                                <div>
                                    <h4 class="milestone-title">${m.name}</h4>
                                    ${m.estimatedHours ? `<span style="font-size: 12px; color: var(--text-muted);">‚è±Ô∏è ${m.estimatedHours} hours</span>` : ''}
                                </div>
                            </div>
                            <p class="milestone-desc">${m.description}</p>
                            <button class="expand-btn">Show details ‚ñº</button>

                            <div class="milestone-details">
                                ${m.criteria && m.criteria.length > 0 ? `
                                    <h5 style="margin-top: 16px; font-size: 13px; color: var(--text-muted); text-transform: uppercase; letter-spacing: 0.5px;">Acceptance Criteria</h5>
                                    <ul class="criteria-list">
                                        ${m.criteria.map(c => `<li>${c}</li>`).join('')}
                                    </ul>
                                ` : ''}

                                ${m.hints ? `
                                    <h5 style="margin-top: 20px; font-size: 13px; color: var(--text-muted); text-transform: uppercase; letter-spacing: 0.5px;">Hints (Progressive)</h5>
                                    <div class="hints-container">
                                        <div class="hint-level">
                                            <div class="hint-header">
                                                <span>üí° Level 1 - Gentle nudge</span>
                                                <span>‚ñº</span>
                                            </div>
                                            <div class="hint-content"><pre>${m.hints.level1}</pre></div>
                                        </div>
                                        <div class="hint-level">
                                            <div class="hint-header">
                                                <span>üí° Level 2 - More detail</span>
                                                <span>‚ñº</span>
                                            </div>
                                            <div class="hint-content"><pre>${m.hints.level2}</pre></div>
                                        </div>
                                        <div class="hint-level">
                                            <div class="hint-header">
                                                <span>üí° Level 3 - Full guidance</span>
                                                <span>‚ñº</span>
                                            </div>
                                            <div class="hint-content"><pre>${m.hints.level3}</pre></div>
                                        </div>
                                    </div>
                                ` : ''}

                                ${m.pitfalls && m.pitfalls.length > 0 ? `
                                    <h5 style="margin-top: 20px; font-size: 13px; color: var(--text-muted); text-transform: uppercase; letter-spacing: 0.5px;">Common Pitfalls</h5>
                                    <ul class="pitfalls-list">
                                        ${m.pitfalls.map(p => `<li>${p}</li>`).join('')}
                                    </ul>
                                ` : ''}

                                ${m.concepts && m.concepts.length > 0 ? `
                                    <h5 style="margin-top: 20px; font-size: 13px; color: var(--text-muted); text-transform: uppercase; letter-spacing: 0.5px;">Key Concepts</h5>
                                    <ul class="concepts-list">
                                        ${m.concepts.map(c => `<li>${c}</li>`).join('')}
                                    </ul>
                                ` : ''}
                            </div>
                        </div>
                    `).join('')}
                </div>
            </div>
        `;

        modalContent.innerHTML = content;

        // Add back button handler
        document.getElementById('backToList').addEventListener('click', () => {
            openDomainModal(selectedDomain);
        });

        // Add expand button handlers
        document.querySelectorAll('.expand-btn').forEach(btn => {
            btn.addEventListener('click', (e) => {
                const milestone = e.target.closest('.milestone');
                milestone.classList.toggle('expanded');
                btn.textContent = milestone.classList.contains('expanded') ? 'Hide details ‚ñ≤' : 'Show details ‚ñº';
            });
        });

        // Add hint toggle handlers
        document.querySelectorAll('.hint-header').forEach(header => {
            header.addEventListener('click', () => {
                header.parentElement.classList.toggle('open');
            });
        });
    }

    // Show/hide modal
    function showModal() {
        modalOverlay.classList.add('active');
        modal.classList.add('active');
        document.body.style.overflow = 'hidden';
    }

    function hideModal() {
        modalOverlay.classList.remove('active');
        modal.classList.remove('active');
        document.body.style.overflow = '';
    }

    // Event listeners
    closeModal.addEventListener('click', hideModal);
    modalOverlay.addEventListener('click', hideModal);

    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape') hideModal();
    });

    filterPills.forEach(pill => {
        pill.addEventListener('click', () => {
            filterPills.forEach(p => p.classList.remove('active'));
            pill.classList.add('active');
            currentFilter = pill.dataset.filter;
            renderDomains();
        });
    });

    searchInput.addEventListener('input', (e) => {
        searchQuery = e.target.value;
        renderDomains();
    });

    // Initialize
    calculateStats();
    renderDomains();
    </script>
</body>
</html>
